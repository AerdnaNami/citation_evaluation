{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70705fc5",
   "metadata": {},
   "source": [
    "Collecting the gold label data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e17934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "# keys.env with stored keys required \n",
    "key_path = 'keys.env'\n",
    "load_dotenv(dotenv_path=key_path)\n",
    "api_key = os.getenv(\"API_KEY_2\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def ask_gpt(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-5\",\n",
    "        messages=[\n",
    "            {\"role\": \"assistant\", \"content\": \"You are an expert in academic writing and citation analysis.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2589af9",
   "metadata": {},
   "source": [
    "### GPT span classifications\n",
    "- filter dataset to remove 'References' \n",
    "- create prompt for zero and few shot prompting\n",
    "- collect GPT responses\n",
    "\n",
    "checklist prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0bf0a",
   "metadata": {},
   "source": [
    "Prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bb30cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_def = f\"\"\"Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n",
    "    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n",
    "    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n",
    "    c) Formatting issues appear as either citations or parts of a citation.\n",
    "    Examples of formatting issues include:\n",
    "        i) Narrative citation missing year: “Vatswani et al.” -> should be “Vatswani et al. (2020)”\n",
    "        ii) Wrong citation style: “In (Vatswani et al., 2019)” -> should be “in Vatswani et al. (2019)”\n",
    "        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\"\"\"\n",
    "\n",
    "unsupp_def = f\"\"\"claim about prior work or statistics w/o citation or evidence. \n",
    "    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n",
    "    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n",
    "    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under ‘Unsupported claim’. For example:\n",
    "        i) “However, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.” -> it is obvious that creating a dataset is time consuming and mentally demanding.\n",
    "    d) Any mention of “recent works” should be backed up with citations to the works.\n",
    "    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n",
    "    Examples of unsupported claims include:\n",
    "        i) Missing citations for mentions of 'recent works': “and there are many recent works that explore this topic”,\n",
    "        ii) Mention of a previous work and claim without citation: “..., while in a previous study, the authors claim …”,\n",
    "        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \"\"\"\n",
    "\n",
    "lacksynth_def = f\"\"\"occurs when either:\n",
    "    a) The author describes or cites papers without connecting them to their own work/argument \n",
    "    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n",
    "    c) It does not articulate the author's perspective or motivation.\n",
    "    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n",
    "    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n",
    "    Examples of lack of synthesis include:\n",
    "        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n",
    "        ii)  No explanation of the cited works and relation to their own work: “Recently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).” \"\"\"\n",
    "\n",
    "coherence_def = f\"\"\"connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n",
    "    a) Sentences are not transitioned from one to another.\n",
    "    b) The relationship between sentences describing papers is implied but not explicitly stated.\n",
    "    c) Coherence issues appear only as multiple sentences.\n",
    "    Examples of coherence issues include:\n",
    "        i) Relation between mentioned works is not explicit: “Smith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).”\n",
    "        ii) Lack of transitions between sentences: “Recent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.” \n",
    "        iii) No explanation of the cited works and relation to their own work: “Recently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).” \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ea1c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import re\n",
    "\n",
    "def complete_prompt(text, full_text):\n",
    "    final = text + f\"\"\"\\n\n",
    "Respond ONLY in this json format:\n",
    "{{  \"span_text\": ... ,\n",
    "    \"gpt_label\": ... ,\n",
    "    \"reason\": ... }}\n",
    "Please have 'span_text' ONLY contain text. The text must be verbatim from the passage of text provided.\n",
    "\n",
    "Here is the passage of text you must identify spans in:\n",
    "{full_text}\"\"\"\n",
    "    return final \n",
    "\n",
    "def parse_list(response):\n",
    "    s = re.sub(r\"```(?:json)?\\s*\", \"\", response).replace(\"```\", \"\").strip()\n",
    "\n",
    "    start_candidates = [i for i in (s.find(\"[\"), s.find(\"{\")) if i != -1]\n",
    "    if not start_candidates:\n",
    "        raise ValueError(\"No JSON start token found ('[' or '{').\")\n",
    "\n",
    "    start = min(start_candidates)\n",
    "    s = s[start:].lstrip()\n",
    "\n",
    "    decoder = json.JSONDecoder()\n",
    "    obj, _end = decoder.raw_decode(s)  # parses first JSON value only\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        obj = [obj]\n",
    "\n",
    "    if not isinstance(obj, list):\n",
    "        raise ValueError(f\"Expected a JSON list, got {type(obj).__name__}\")\n",
    "\n",
    "    if not all(isinstance(item, dict) for item in obj):\n",
    "        bad_types = {type(item).__name__ for item in obj if not isinstance(item, dict)}\n",
    "        raise ValueError(f\"Expected list of dicts, but found non-dict items: {bad_types}\")\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd9fa793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_start_end_char(text, full_text):\n",
    "    text_norm = text.replace('.', '')\n",
    "    full_norm = full_text.replace('.', '')\n",
    "\n",
    "    if text_norm in full_norm:\n",
    "        start = full_norm.index(text_norm)\n",
    "        end = start + len(text_norm)\n",
    "    else:\n",
    "        start = end = -1\n",
    "\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3719d63d",
   "metadata": {},
   "source": [
    "GPT response collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d8eb603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting GPT responses: 100%|██████████| 1/1 [07:09<00:00, 429.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "from pathlib import Path\n",
    "\n",
    "prompt_strategy = ['zero', 'fewshot']\n",
    "zeroshot = []\n",
    "fewshot = []\n",
    "folder = '../data/to_annotate'\n",
    "categories_def = {\"Unsupported Claim\": unsupp_def, \"Format\": format_def, \"Coherence\": coherence_def, \"Lacks synthesis\": lacksynth_def}\n",
    "\n",
    "for i in tqdm(range(100,101), desc='Collecting GPT responses: '):\n",
    "    path = Path(f\"{folder}/paper_{i}.txt\")\n",
    "\n",
    "    if path.exists():\n",
    "        with open(f\"{folder}/paper_{i}.txt\", 'r') as f:\n",
    "            full_text = f.read()\n",
    "        full_text = full_text.split('References')[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    for strategy in prompt_strategy:\n",
    "        temp = []\n",
    "        for category, definition in categories_def.items():\n",
    "            if strategy == 'fewshot':\n",
    "                prompt = f\"\"\"\n",
    "You are given a passage of text that comes from an academic paper on the field of Natural Language Processing. You must highlight the spans that exhibit any of these issues:\n",
    "**{category}**: {definition}\n",
    "\"\"\"\n",
    "            else:\n",
    "                prompt = f\"\"\"\n",
    "You are given a passage of text that comes from an academic paper on the field of Natural Language Processing. You must highlight the spans that exhibit any of these issues:\n",
    "**{category}**: {definition.split(\"Examples of\")[0]}\n",
    "\"\"\"\n",
    "            prompt = complete_prompt(prompt, full_text)\n",
    "            response = ask_gpt(prompt)\n",
    "            if len(response) == 0:\n",
    "                continue\n",
    "\n",
    "            json_response = parse_list(response)\n",
    "            for resp in json_response:\n",
    "                start, end = extract_start_end_char(resp['span_text'], full_text)\n",
    "                resp['start'] = start\n",
    "                resp['end'] = end\n",
    "\n",
    "            if strategy == 'fewshot':\n",
    "                temp.append({\"filename\": f\"paper_{i}.txt\", \"full_text\": full_text, 'gpt_response': json_response})\n",
    "            else:\n",
    "                temp.append({\"filename\": f\"paper_{i}.txt\", \"full_text\": full_text, 'gpt_response': json_response})\n",
    "        \n",
    "        if strategy == 'fewshot':\n",
    "            fewshot.append(temp)\n",
    "        else:\n",
    "            zeroshot.append(temp)\n",
    "    \n",
    "    if i%10 == 0:\n",
    "        with open(f\"../experiments/fewshot_responses_{i}.json\", \"w\") as f:\n",
    "            json.dump(fewshot, f, indent=4)\n",
    "\n",
    "        with open(f\"../experiments/zero_responses_{i}.json\", \"w\") as f:\n",
    "            json.dump(zeroshot, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb9a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Unsupported Claim', 'Format', 'Coherence', 'Lacks synthesis'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "def _flatten(x):\n",
    "    \"\"\"Yield dict items from arbitrarily nested lists.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        for y in x:\n",
    "            yield from _flatten(y)\n",
    "    else:\n",
    "        yield x\n",
    "\n",
    "def merge_and_group_by_paper(data: Any, *, dedupe: bool = True) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Accepts either:\n",
    "      - list of dicts\n",
    "      - list of lists of dicts (nested)\n",
    "    and merges/group predictions by filename.\n",
    "    \"\"\"\n",
    "    merged: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    for rec in _flatten(data):\n",
    "        if not isinstance(rec, dict):\n",
    "            continue  # skip unexpected items safely\n",
    "\n",
    "        # handle both 'filename' and 'file' keys\n",
    "        fn = rec.get(\"filename\") or rec.get(\"file\")\n",
    "        if fn is None:\n",
    "            continue\n",
    "\n",
    "        full_text = rec.get(\"full_text\", \"\") or rec.get(\"text\", \"\")\n",
    "\n",
    "        if fn not in merged:\n",
    "            merged[fn] = {\n",
    "                \"filename\": fn,\n",
    "                \"full_text\": full_text,\n",
    "                \"all_spans\": [],\n",
    "                \"spans_by_label\": defaultdict(list),\n",
    "            }\n",
    "        else:\n",
    "            if not merged[fn][\"full_text\"] and full_text:\n",
    "                merged[fn][\"full_text\"] = full_text\n",
    "\n",
    "        for span in rec.get(\"gpt_response\", []) or []:\n",
    "            merged[fn][\"all_spans\"].append(span)\n",
    "            label = span.get(\"gpt_label\", \"UNKNOWN\")\n",
    "            merged[fn][\"spans_by_label\"][label].append(span)\n",
    "\n",
    "    # Optional dedupe\n",
    "    if dedupe:\n",
    "        for fn, obj in merged.items():\n",
    "            def uniq(spans):\n",
    "                seen = set()\n",
    "                out = []\n",
    "                for s in spans:\n",
    "                    key = (\n",
    "                        s.get(\"gpt_label\"),\n",
    "                        int(s.get(\"start\")) if s.get(\"start\") is not None else None,\n",
    "                        int(s.get(\"end\")) if s.get(\"end\") is not None else None,\n",
    "                        s.get(\"span_text\"),\n",
    "                    )\n",
    "                    if key in seen:\n",
    "                        continue\n",
    "                    seen.add(key)\n",
    "                    out.append(s)\n",
    "                return out\n",
    "\n",
    "            obj[\"all_spans\"] = uniq(obj[\"all_spans\"])\n",
    "            for lab in list(obj[\"spans_by_label\"].keys()):\n",
    "                obj[\"spans_by_label\"][lab] = uniq(obj[\"spans_by_label\"][lab])\n",
    "\n",
    "            obj[\"spans_by_label\"] = dict(obj[\"spans_by_label\"])\n",
    "\n",
    "    return merged\n",
    "\n",
    "# group all predictions based on paper \n",
    "fewshot = merge_and_group_by_paper(fewshot)\n",
    "zeroshot = merge_and_group_by_paper(zeroshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076bd86d",
   "metadata": {},
   "source": [
    "Some spans dont have a start and end index due to GPT over correcting grammar/spelling mistakes, so we do post-processing to fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56234d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block if youre resuming work after restarting your kernel, otherwise continue to the next block to save the responses after collection is done\n",
    "import json \n",
    "\n",
    "fewshot_path = \"../experiments/final/fewshot_preds.json\"\n",
    "zeroshot_path = \"../experiments/final/zeroshot_preds.json\"\n",
    "\n",
    "with open(fewshot_path, 'r') as f:\n",
    "    fewshot = json.load(f)\n",
    "\n",
    "with open(zeroshot_path, 'r') as f:\n",
    "    zeroshot = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "MISSING_SEMI_SPACE_RE = re.compile(r\";\\S\")  # semicolon not followed by space\n",
    "\n",
    "def remove_missing_spaces_format_spans(\n",
    "    in_path: str,\n",
    "    out_path: str,\n",
    "):\n",
    "    with open(in_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    removed = 0\n",
    "\n",
    "    for paper_key, paper_obj in data.items():\n",
    "        # ---- filter all_spans ----\n",
    "        new_all = []\n",
    "        for s in paper_obj.get(\"all_spans\", []):\n",
    "            if s.get(\"gpt_label\") == \"Format\":\n",
    "                reason = (s.get(\"reason\") or \"\").lower()\n",
    "                span_text = s.get(\"span_text\") or \"\"\n",
    "\n",
    "                if (\n",
    "                    \"missing space\" in reason\n",
    "                    or \"missing spaces\" in reason\n",
    "                    or MISSING_SEMI_SPACE_RE.search(span_text)\n",
    "                ):\n",
    "                    removed += 1\n",
    "                    continue\n",
    "\n",
    "            new_all.append(s)\n",
    "\n",
    "        paper_obj[\"all_spans\"] = new_all\n",
    "\n",
    "        # ---- keep spans_by_label in sync ----\n",
    "        sbl = paper_obj.get(\"spans_by_label\")\n",
    "        if isinstance(sbl, dict) and \"Format\" in sbl:\n",
    "            new_format = []\n",
    "            for s in sbl[\"Format\"]:\n",
    "                reason = (s.get(\"reason\") or \"\").lower()\n",
    "                span_text = s.get(\"span_text\") or \"\"\n",
    "\n",
    "                if (\n",
    "                    \"missing space\" in reason\n",
    "                    or \"missing spaces\" in reason\n",
    "                    or MISSING_SEMI_SPACE_RE.search(span_text)\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                new_format.append(s)\n",
    "\n",
    "            sbl[\"Format\"] = new_format\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Removed {removed} Format spans for missing semicolon spaces.\")\n",
    "    return removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ec4679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 173 Format spans for missing semicolon spaces.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_missing_spaces_format_spans(\n",
    "    in_path=\"../experiments/final/zeroshot_preds.json\",\n",
    "    out_path=\"../experiments/zeroshot_preds.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d42b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _normalize_with_map(s: str):\n",
    "    \"\"\"\n",
    "    Normalize in a way that often matches annotation tools:\n",
    "    - Unicode normalize (NFC)\n",
    "    - Convert CRLF/CR to LF\n",
    "    - Replace NBSP with space\n",
    "    - Remove common zero-width chars\n",
    "    Returns: (normalized_string, norm_index -> original_index map)\n",
    "    \"\"\"\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "\n",
    "    norm_chars = []\n",
    "    norm_to_orig = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(s):\n",
    "        ch = s[i]\n",
    "\n",
    "        # normalize newlines\n",
    "        if ch == \"\\r\":\n",
    "            # treat \\r\\n or \\r as a single \\n\n",
    "            if i + 1 < len(s) and s[i + 1] == \"\\n\":\n",
    "                # map the normalized '\\n' to the start of the pair\n",
    "                norm_chars.append(\"\\n\")\n",
    "                norm_to_orig.append(i)\n",
    "                i += 2\n",
    "                continue\n",
    "            else:\n",
    "                norm_chars.append(\"\\n\")\n",
    "                norm_to_orig.append(i)\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        # normalize NBSP\n",
    "        if ch == \"\\u00A0\":\n",
    "            ch = \" \"\n",
    "\n",
    "        # drop zero-width characters (common culprits)\n",
    "        if ch in (\"\\u200b\", \"\\u200c\", \"\\u200d\", \"\\ufeff\"):\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        norm_chars.append(ch)\n",
    "        norm_to_orig.append(i)\n",
    "        i += 1\n",
    "\n",
    "    return \"\".join(norm_chars), norm_to_orig\n",
    "\n",
    "\n",
    "def fuzzy_span_start_end_mapped(\n",
    "    span_text: str,\n",
    "    full_text: str,\n",
    "    threshold: float = 0.80,\n",
    "    window_slack: int = 20,\n",
    "    output_1_based_inclusive: bool = False,\n",
    "):\n",
    "    if not span_text or not full_text:\n",
    "        return -1, -1\n",
    "\n",
    "    norm_full, norm_map = _normalize_with_map(full_text)\n",
    "    norm_span, _ = _normalize_with_map(span_text)\n",
    "\n",
    "    target_len = len(norm_span)\n",
    "    best_score = 0.0\n",
    "    best_start = -1\n",
    "    best_end = -1  # exclusive in normalized coordinates\n",
    "\n",
    "    min_len = max(1, target_len - window_slack)\n",
    "    max_len = min(len(norm_full), target_len + window_slack)\n",
    "\n",
    "    for win_len in range(min_len, max_len + 1):\n",
    "        for i in range(0, len(norm_full) - win_len + 1):\n",
    "            candidate = norm_full[i:i + win_len]\n",
    "            score = SequenceMatcher(None, norm_span, candidate).ratio()\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_start = i\n",
    "                best_end = i + win_len\n",
    "\n",
    "    if best_score < threshold or best_start < 0:\n",
    "        return -1, -1\n",
    "\n",
    "    # Map normalized [best_start, best_end) back to ORIGINAL full_text indices.\n",
    "    # start maps directly; end maps via last char + 1 (to keep exclusive end).\n",
    "    orig_start = norm_map[best_start]\n",
    "    orig_end = norm_map[best_end - 1] + 1\n",
    "\n",
    "    if output_1_based_inclusive:\n",
    "        # convert [orig_start, orig_end) to 1-based inclusive\n",
    "        return orig_start + 1, orig_end  # end becomes inclusive in 1-based\n",
    "\n",
    "    return orig_start, orig_end\n",
    "\n",
    "\n",
    "for resp in tqdm(fewshot, desc='Fuzzy matching few shot samples: '):\n",
    "    span = resp['gpt_response'][0]['span_text']\n",
    "    full_text = resp['full_text']\n",
    "    s, e = fuzzy_span_start_end_mapped(span, full_text, output_1_based_inclusive=True)\n",
    "    resp['gpt_response'][0]['start'], resp['gpt_response'][0]['end'] = s, e\n",
    "\n",
    "for resp in tqdm(zeroshot, desc='Fuzzy matching zero shot samples: '):\n",
    "    span = resp['gpt_response'][0]['span_text']\n",
    "    full_text = resp['full_text']\n",
    "    s, e = fuzzy_span_start_end_mapped(span, full_text, output_1_based_inclusive=True)\n",
    "    resp['gpt_response'][0]['start'], resp['gpt_response'][0]['end'] = s, e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3967a1b0",
   "metadata": {},
   "source": [
    "Save to folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03469e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "filename = \"fewshot_preds.json\"\n",
    "folder = \"../experiments/final\"\n",
    "\n",
    "with open(f\"{folder}/{filename}\", 'w') as f:\n",
    "    json.dump(fewshot, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2652b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "filename = \"zeroshot_preds.json\"\n",
    "folder = \"../experiments/final\"\n",
    "\n",
    "with open(f\"{folder}/{filename}\", 'w') as f:\n",
    "    json.dump(zeroshot, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c99d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
