{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314fb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "HEADER_RE = re.compile(\n",
    "    r\"^\\s*\\[(?P<span>\\d+-\\d+)\\]\\s*(?P<label>[^,]+),\\s*File:\\s*(?P<filename>[^,]+)\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_ann_text(path, txt_path, encoding =\"utf-8\"):\n",
    "    with open(path, \"r\", encoding=encoding) as fh:\n",
    "        lines = fh.readlines()\n",
    "\n",
    "    # Find indices of header lines and capture groups\n",
    "    headers = []  # list of tuples (index, matchobj)\n",
    "    for i, line in enumerate(lines):\n",
    "        m = HEADER_RE.match(line)\n",
    "        if m:\n",
    "            headers.append((i, m))\n",
    "\n",
    "    results = []\n",
    "    if not headers:\n",
    "        return results\n",
    "\n",
    "    # For each found header, slice the following lines until next header (or EOF)\n",
    "    for idx, (line_idx, match) in enumerate(headers):\n",
    "        next_idx = headers[idx + 1][0] if (idx + 1) < len(headers) else len(lines)\n",
    "\n",
    "        # Extract body: lines after the header line up to next header index\n",
    "        body_lines = [ln.rstrip(\"\\n\") for ln in lines[line_idx + 1 : next_idx]]\n",
    "        body = \"\\n\".join(body_lines).strip() or None\n",
    "\n",
    "        span = match.group(\"span\").strip() if match.group(\"span\") else None\n",
    "        start_span = int(span.split(\"-\")[0])\n",
    "        end_span = int(span.split(\"-\")[1])\n",
    "        filename = match.group(\"filename\").strip() if match.group(\"filename\") else None\n",
    "        label = match.group(\"label\").strip() if match.group(\"label\") else None\n",
    "\n",
    "        with open(f\"{txt_path}/{filename}\", 'r') as f:\n",
    "            full_text = f.read()\n",
    "\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"start\": start_span,\n",
    "            \"end\": end_span,\n",
    "            \"label\": label,\n",
    "            \"text\": body, \n",
    "            \"full_text\": full_text,\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f1fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = \"../annotated_data/first_ten_agreed.txt\"\n",
    "\n",
    "txt_path = \"../data/to_annotate\"\n",
    "goldset = extract_ann_text(gold, txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a811d440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(goldset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629b9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import unicodedata\n",
    "\n",
    "def _normalize_with_map(s: str):\n",
    "    \"\"\"\n",
    "    Normalize in a way that often matches annotation tools:\n",
    "    - Unicode normalize (NFC)\n",
    "    - Convert CRLF/CR to LF\n",
    "    - Replace NBSP with space\n",
    "    - Remove common zero-width chars\n",
    "    Returns: (normalized_string, norm_index -> original_index map)\n",
    "    \"\"\"\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "\n",
    "    norm_chars = []\n",
    "    norm_to_orig = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(s):\n",
    "        ch = s[i]\n",
    "\n",
    "        # normalize newlines\n",
    "        if ch == \"\\r\":\n",
    "            # treat \\r\\n or \\r as a single \\n\n",
    "            if i + 1 < len(s) and s[i + 1] == \"\\n\":\n",
    "                # map the normalized '\\n' to the start of the pair\n",
    "                norm_chars.append(\"\\n\")\n",
    "                norm_to_orig.append(i)\n",
    "                i += 2\n",
    "                continue\n",
    "            else:\n",
    "                norm_chars.append(\"\\n\")\n",
    "                norm_to_orig.append(i)\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        # normalize NBSP\n",
    "        if ch == \"\\u00A0\":\n",
    "            ch = \" \"\n",
    "\n",
    "        # drop zero-width characters (common culprits)\n",
    "        if ch in (\"\\u200b\", \"\\u200c\", \"\\u200d\", \"\\ufeff\"):\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        norm_chars.append(ch)\n",
    "        norm_to_orig.append(i)\n",
    "        i += 1\n",
    "\n",
    "    return \"\".join(norm_chars), norm_to_orig\n",
    "\n",
    "def fuzzy_span_start_end_mapped(\n",
    "    span_text: str,\n",
    "    full_text: str,\n",
    "    threshold: float = 0.80,\n",
    "    window_slack: int = 20,\n",
    "    output_1_based_inclusive: bool = False,\n",
    "):\n",
    "    if not span_text or not full_text:\n",
    "        return -1, -1\n",
    "\n",
    "    norm_full, norm_map = _normalize_with_map(full_text)\n",
    "    norm_span, _ = _normalize_with_map(span_text)\n",
    "\n",
    "    target_len = len(norm_span)\n",
    "    best_score = 0.0\n",
    "    best_start = -1\n",
    "    best_end = -1  # exclusive in normalized coordinates\n",
    "\n",
    "    min_len = max(1, target_len - window_slack)\n",
    "    max_len = min(len(norm_full), target_len + window_slack)\n",
    "\n",
    "    for win_len in range(min_len, max_len + 1):\n",
    "        for i in range(0, len(norm_full) - win_len + 1):\n",
    "            candidate = norm_full[i:i + win_len]\n",
    "            score = SequenceMatcher(None, norm_span, candidate).ratio()\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_start = i\n",
    "                best_end = i + win_len\n",
    "\n",
    "    if best_score < threshold or best_start < 0:\n",
    "        return -1, -1\n",
    "\n",
    "    # Map normalized [best_start, best_end) back to ORIGINAL full_text indices.\n",
    "    # start maps directly; end maps via last char + 1 (to keep exclusive end).\n",
    "    orig_start = norm_map[best_start]\n",
    "    orig_end = norm_map[best_end - 1] + 1\n",
    "\n",
    "    if output_1_based_inclusive:\n",
    "        # convert [orig_start, orig_end) to 1-based inclusive\n",
    "        return orig_start + 1, orig_end  # end becomes inclusive in 1-based\n",
    "\n",
    "    return orig_start, orig_end\n",
    "\n",
    "\n",
    "\n",
    "def exact_span_start_end_mapped(\n",
    "    span_text: str,\n",
    "    full_text: str,\n",
    "    *,\n",
    "    output_1_based_inclusive: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Exact substring match in normalized space, then map back to original indices.\n",
    "\n",
    "    Uses the same _normalize_with_map() convention as fuzzy_span_start_end_mapped,\n",
    "    so indices are directly comparable.\n",
    "    \"\"\"\n",
    "    if not span_text or not full_text:\n",
    "        return -1, -1\n",
    "\n",
    "    norm_full, norm_map = _normalize_with_map(full_text)\n",
    "    norm_span, _ = _normalize_with_map(span_text)\n",
    "\n",
    "    idx = norm_full.find(norm_span)\n",
    "    if idx == -1:\n",
    "        return -1, -1\n",
    "\n",
    "    norm_start = idx\n",
    "    norm_end = idx + len(norm_span)  # exclusive in normalized space\n",
    "\n",
    "    # Map normalized [norm_start, norm_end) back to ORIGINAL full_text indices.\n",
    "    orig_start = norm_map[norm_start]\n",
    "    orig_end = norm_map[norm_end - 1] + 1  # exclusive end\n",
    "\n",
    "    if output_1_based_inclusive:\n",
    "        # convert [orig_start, orig_end) â†’ (1-based, inclusive)\n",
    "        return orig_start + 1, orig_end\n",
    "\n",
    "    return orig_start, orig_end\n",
    "\n",
    "def add_mapped_indices_to_records(\n",
    "    records,\n",
    "    *,\n",
    "    use_fuzzy_fallback: bool = True,\n",
    "    fuzzy_threshold: float = 0.80,\n",
    "    window_slack: int = 20,\n",
    "    output_1_based_inclusive: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each record in a list of dicts with keys 'text' and 'full_text',\n",
    "    compute start/end using the same normalization/mapping as\n",
    "    fuzzy_span_start_end_mapped.\n",
    "\n",
    "    Mutates records in-place and also returns them.\n",
    "    \"\"\"\n",
    "    for r in records:\n",
    "        span_text = r[\"text\"]\n",
    "        full_text = r[\"full_text\"]\n",
    "\n",
    "        # 1) Try exact normalized match\n",
    "        s, e = exact_span_start_end_mapped(\n",
    "            span_text,\n",
    "            full_text,\n",
    "            output_1_based_inclusive=output_1_based_inclusive,\n",
    "        )\n",
    "\n",
    "        # 2) Optionally fall back to fuzzy if exact fails\n",
    "        if use_fuzzy_fallback and s == -1:\n",
    "            s, e = fuzzy_span_start_end_mapped(\n",
    "                span_text,\n",
    "                full_text,\n",
    "                threshold=fuzzy_threshold,\n",
    "                window_slack=window_slack,\n",
    "                output_1_based_inclusive=output_1_based_inclusive,\n",
    "            )\n",
    "\n",
    "        r[\"start\"], r[\"end\"] = s, e\n",
    "\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb7d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "goldset = add_mapped_indices_to_records(goldset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b731ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess to remove 'References' section, keep only relevant text\n",
    "for text in goldset:\n",
    "    keep_text = text['full_text'].split(\"References\")[0]\n",
    "    text['full_text'] = keep_text\n",
    "\n",
    "goldset_sorted = []\n",
    "for i in range(1,11):\n",
    "    paper = []\n",
    "    for text in goldset:\n",
    "        if text['filename'] == f\"paper_{i}.txt\":\n",
    "            paper.append(text)\n",
    "    goldset_sorted.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9fe34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "folder = \"../annotated_data\"\n",
    "filename = \"goldset_sorted.json\"\n",
    "with open(f\"{folder}/{filename}\", 'w') as f:\n",
    "    json.dump(goldset_sorted, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e342b9",
   "metadata": {},
   "source": [
    "### Evaluation using token F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76add007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block if youre resuming work after restarting your kernel\n",
    "import json \n",
    "\n",
    "fewshot_path = \"../experiments/final/fewshot_preds.json\"\n",
    "zeroshot_path = \"../experiments/final/zeroshot_preds.json\"\n",
    "\n",
    "with open(fewshot_path, 'r') as f:\n",
    "    fewshot = json.load(f)\n",
    "\n",
    "with open(zeroshot_path, 'r') as f:\n",
    "    zeroshot = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540c9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "Span = Tuple[int, int]  # (start_char, end_char_inclusive)\n",
    "\n",
    "def span_f1_exact(gold_spans, pred_spans, dedupe=True):\n",
    "\n",
    "    def valid(span: Span):\n",
    "        s, e = span\n",
    "        return isinstance(s, int) and isinstance(e, int) and e >= s\n",
    "\n",
    "    gold = [sp for sp in gold_spans if valid(sp)]\n",
    "    pred = [sp for sp in pred_spans if valid(sp)]\n",
    "\n",
    "    if dedupe:\n",
    "        gold_set = set(gold)\n",
    "        pred_set = set(pred)\n",
    "\n",
    "        matched = sorted(gold_set & pred_set)\n",
    "        tp = len(matched)\n",
    "        fp = len(pred_set - gold_set)\n",
    "        fn = len(gold_set - pred_set)\n",
    "\n",
    "        unmatched_pred = sorted(pred_set - gold_set)\n",
    "        unmatched_gold = sorted(gold_set - pred_set)\n",
    "    else:\n",
    "        gold_c = Counter(gold)\n",
    "        pred_c = Counter(pred)\n",
    "\n",
    "        matched = []\n",
    "        tp = 0\n",
    "        for sp in (gold_c.keys() & pred_c.keys()):\n",
    "            k = min(gold_c[sp], pred_c[sp])\n",
    "            tp += k\n",
    "            matched.extend([sp] * k)\n",
    "\n",
    "        fp = sum((pred_c - gold_c).values())\n",
    "        fn = sum((gold_c - pred_c).values())\n",
    "\n",
    "        unmatched_pred = []\n",
    "        for sp, k in (pred_c - gold_c).items():\n",
    "            unmatched_pred.extend([sp] * k)\n",
    "\n",
    "        unmatched_gold = []\n",
    "        for sp, k in (gold_c - pred_c).items():\n",
    "            unmatched_gold.extend([sp] * k)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"matched_spans\": matched,\n",
    "        \"unmatched_pred_spans\": unmatched_pred,\n",
    "        \"unmatched_gold_spans\": unmatched_gold,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b25742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_predictions_by_paper(predictions, num_papers=10):\n",
    "    \"\"\"\n",
    "    predictions: dict keyed by filename (new format)\n",
    "    Returns: list of length num_papers, each a list of span dicts\n",
    "    \"\"\"\n",
    "    papers = [[] for _ in range(num_papers)]\n",
    "\n",
    "    for i in range(num_papers):\n",
    "        fn = f\"paper_{i+1}.txt\"\n",
    "        if fn not in predictions:\n",
    "            continue\n",
    "\n",
    "        paper_obj = predictions[fn]\n",
    "        for span in paper_obj.get(\"all_spans\", []):\n",
    "            if span.get(\"start\") == -1:\n",
    "                continue\n",
    "\n",
    "            papers[i].append({\n",
    "                \"filename\": fn,\n",
    "                \"span\": span.get(\"span_text\"),\n",
    "                \"start\": span.get(\"start\"),\n",
    "                \"end\": span.get(\"end\"),\n",
    "                \"label\": span.get(\"gpt_label\"),\n",
    "            })\n",
    "\n",
    "    return papers\n",
    "\n",
    "fewshot = flatten_predictions_by_paper(fewshot, num_papers=10)\n",
    "zeroshot = flatten_predictions_by_paper(zeroshot, num_papers=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b54728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold set:  [{'Coherence': [(742, 1101), (1508, 1713)], 'Format': [(1705, 1713)], 'Unsupported claim': [(468, 557), (993, 997), (1091, 1100), (1373, 1506)], 'Lacks synthesis': []}, {'Coherence': [(2037, 2506), (3462, 4014)], 'Format': [(1755, 1777), (2576, 2592), (4882, 4896)], 'Unsupported claim': [], 'Lacks synthesis': [(868, 1363), (1366, 2035)]}, {'Coherence': [], 'Format': [], 'Unsupported claim': [(716, 1194), (3148, 3155), (3191, 3232), (3242, 3248), (3451, 3458)], 'Lacks synthesis': []}, {'Coherence': [], 'Format': [], 'Unsupported claim': [(31, 180), (182, 386), (596, 726), (729, 1010), (1203, 1396), (1398, 1540), (1542, 1589), (4489, 4651)], 'Lacks synthesis': []}, {'Coherence': [], 'Format': [], 'Unsupported claim': [(760, 797), (803, 833), (2362, 2376)], 'Lacks synthesis': []}, {'Coherence': [(2028, 2462)], 'Format': [(369, 388), (2051, 2071)], 'Unsupported claim': [(62, 224), (1643, 1769)], 'Lacks synthesis': [(1791, 2026)]}, {'Coherence': [(394, 735), (1225, 1857)], 'Format': [(863, 882), (1914, 1924)], 'Unsupported claim': [(311, 392), (1719, 1727)], 'Lacks synthesis': [(263, 981), (1004, 1857)]}, {'Coherence': [], 'Format': [(4156, 4174)], 'Unsupported claim': [(15, 103), (752, 1063), (1429, 1496), (1605, 1706), (3751, 3787), (4418, 4455), (4460, 4473), (4503, 4537), (4611, 4613)], 'Lacks synthesis': []}, {'Coherence': [], 'Format': [], 'Unsupported claim': [(15, 260), (590, 631), (1540, 1570), (2780, 2900)], 'Lacks synthesis': [(263, 1627)]}, {'Coherence': [], 'Format': [(4305, 4341)], 'Unsupported claim': [(99, 293), (842, 938), (1786, 1992), (4029, 4054)], 'Lacks synthesis': []}]\n",
      "Few shot set:  [{'Coherence': [(731, 1083)], 'Format': [(1680, 1687)], 'Unsupported claim': [(462, 551), (977, 982), (1073, 1083), (1486, 1599), (1680, 1687), (1776, 1858)], 'Lacks synthesis': [(553, 1228)]}, {'Coherence': [(3303, 3982)], 'Format': [(4843, 4857), (2556, 2579)], 'Unsupported claim': [(277, 328), (4402, 4454)], 'Lacks synthesis': [(861, 1353), (4925, 5408)]}, {'Coherence': [(14, 697)], 'Format': [], 'Unsupported claim': [(773, 993), (999, 1175), (1176, 1357), (3159, 3201), (3674, 3822)], 'Lacks synthesis': [(316, 697)]}, {'Coherence': [(3061, 3833)], 'Format': [(778, 785)], 'Unsupported claim': [(30, 179), (180, 255), (576, 722), (724, 1005), (1196, 1389), (2732, 2784), (4459, 4621)], 'Lacks synthesis': [(3350, 3673)]}, {'Coherence': [(2300, 2939)], 'Format': [], 'Unsupported claim': [(749, 787), (792, 823), (2327, 2354), (2649, 2675)], 'Lacks synthesis': [(14, 206)]}, {'Coherence': [(1615, 1995)], 'Format': [(364, 382), (2022, 2039)], 'Unsupported claim': [(60, 222), (544, 671), (1615, 1741)], 'Lacks synthesis': [(223, 543)]}, {'Coherence': [(991, 1838)], 'Format': [(854, 873), (1894, 1904)], 'Unsupported claim': [(307, 388), (1526, 1646), (2434, 2484)], 'Lacks synthesis': [(260, 969), (991, 1838)]}, {'Coherence': [(103, 405)], 'Format': [], 'Unsupported claim': [(14, 102), (231, 297), (903, 1048), (1582, 1683), (3644, 3746), (4303, 4425), (4561, 4564)], 'Lacks synthesis': [(103, 297)]}, {'Coherence': [(2874, 3255)], 'Format': [], 'Unsupported claim': [(1522, 1612), (1838, 1989), (2874, 2983), (2984, 3138), (3139, 3255), (3256, 3416), (3417, 3573), (3574, 3777)], 'Lacks synthesis': [(261, 1612)]}, {'Coherence': [(2721, 3245)], 'Format': [(4275, 4313)], 'Unsupported claim': [(97, 291), (834, 930), (1387, 1595), (1596, 1769), (1770, 1976), (4000, 4026)], 'Lacks synthesis': [(1076, 1241)]}]\n",
      "Zero shot set:  [{'Coherence': [(731, 1083)], 'Format': [], 'Unsupported claim': [(462, 551), (1486, 1599), (855, 859), (973, 982), (1680, 1687), (1773, 1858)], 'Lacks synthesis': [(731, 1083)]}, {'Coherence': [(4455, 5055)], 'Format': [(1742, 1764), (4843, 4857)], 'Unsupported claim': [(277, 328), (3303, 3433), (4606, 4755), (4402, 4454)], 'Lacks synthesis': [(861, 1353)]}, {'Coherence': [(2979, 3421)], 'Format': [], 'Unsupported claim': [(769, 994), (995, 1175), (1176, 1357), (3125, 3217), (3674, 3822)], 'Lacks synthesis': [(14, 697)]}, {'Coherence': [(1390, 2491)], 'Format': [(3350, 3376)], 'Unsupported claim': [(180, 255), (576, 722), (833, 1005), (1215, 1389), (2732, 2784), (4128, 4258), (4459, 4621)], 'Lacks synthesis': [(1533, 2280)]}, {'Coherence': [(2300, 2939)], 'Format': [], 'Unsupported claim': [(749, 787), (792, 823), (2327, 2354)], 'Lacks synthesis': [(14, 206)]}, {'Coherence': [(1996, 2426)], 'Format': [(364, 382), (2022, 2039)], 'Unsupported claim': [(60, 222), (544, 671), (1615, 1741)], 'Lacks synthesis': [(223, 543)]}, {'Coherence': [(991, 1358)], 'Format': [(854, 873)], 'Unsupported claim': [(307, 388), (842, 846), (2438, 2484)], 'Lacks synthesis': [(1211, 1838)]}, {'Coherence': [(2528, 2773)], 'Format': [], 'Unsupported claim': [(14, 102), (103, 297), (739, 800), (884, 1048), (1582, 1683), (4561, 4564)], 'Lacks synthesis': []}, {'Coherence': [(2874, 3255)], 'Format': [], 'Unsupported claim': [(584, 629), (1016, 1114), (1522, 1612), (1838, 1989), (2956, 2983), (3168, 3185), (3186, 3255), (3256, 3416), (3417, 3573), (3574, 3777)], 'Lacks synthesis': [(261, 1612)]}, {'Coherence': [(1242, 1769)], 'Format': [(4275, 4313)], 'Unsupported claim': [(97, 291), (834, 930), (1387, 1595), (1596, 1769), (1770, 1976)], 'Lacks synthesis': [(1076, 1241)]}]\n"
     ]
    }
   ],
   "source": [
    "goldset_spans = []\n",
    "predset_fewshot_spans = []\n",
    "predset_zeroshot_spans = []\n",
    "categories = ['Unsupported claim', 'Format', 'Coherence', 'Lacks synthesis']\n",
    "\n",
    "for i in range(0,10):\n",
    "    papers = {\"Coherence\": [], \n",
    "                \"Format\": [], \n",
    "                \"Unsupported claim\": [], \n",
    "                \"Lacks synthesis\": []}\n",
    "    for text in goldset_sorted[i]:\n",
    "        for category in categories:\n",
    "            if text['label'].lower() == category.lower():\n",
    "                papers[category].append((text['start'], text['end']))\n",
    "    goldset_spans.append(papers)\n",
    "\n",
    "for i in range(0,10):\n",
    "    papers = {\"Coherence\": [], \n",
    "                \"Format\": [], \n",
    "                \"Unsupported claim\": [], \n",
    "                \"Lacks synthesis\": []}\n",
    "    for text in fewshot[i]:\n",
    "        for category in categories:\n",
    "            if text['label'].lower() == category.lower():\n",
    "                papers[category].append((text['start'], text['end']))\n",
    "    predset_fewshot_spans.append(papers)\n",
    "\n",
    "for i in range(0,10):\n",
    "    papers = {\"Coherence\": [], \n",
    "                \"Format\": [], \n",
    "                \"Unsupported claim\": [], \n",
    "                \"Lacks synthesis\": []}\n",
    "    for text in zeroshot[i]:\n",
    "        for category in categories:\n",
    "            if text['label'].lower() == category.lower():\n",
    "                papers[category].append((text['start'], text['end']))\n",
    "    predset_zeroshot_spans.append(papers)\n",
    "\n",
    "print(\"Gold set: \", goldset_spans)\n",
    "print(\"Few shot set: \", predset_fewshot_spans)\n",
    "print(\"Zero shot set: \", predset_zeroshot_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d07f8ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Coherence': [(742, 1101), (1508, 1713)], 'Format': [(1705, 1713)], 'Unsupported claim': [(468, 557), (993, 997), (1091, 1100), (1373, 1506)], 'Lacks synthesis': []}\n",
      "{'Coherence': [(731, 1083)], 'Format': [(1680, 1687)], 'Unsupported claim': [(462, 551), (977, 982), (1073, 1083), (1486, 1599), (1680, 1687), (1776, 1858)], 'Lacks synthesis': [(553, 1228)]}\n"
     ]
    }
   ],
   "source": [
    "print(goldset_spans[0])\n",
    "print(predset_fewshot_spans[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18cd4b",
   "metadata": {},
   "source": [
    "### Micro averaged scores per category:\n",
    "- For each category, gather all spans across papers and compute one single F1/precision/recall using total TP/FP/FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f88d8145",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Unsupported claim', 'Format', 'Coherence', 'Lacks synthesis']\n",
    "\n",
    "fewshot_results_per_category = {}\n",
    "\n",
    "for category in categories:\n",
    "    all_gold = []\n",
    "    all_pred = []\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        all_gold.extend(goldset_spans[i].get(category, []))\n",
    "        all_pred.extend(predset_fewshot_spans[i].get(category, []))\n",
    "\n",
    "    res = span_f1_exact(all_gold, all_pred, dedupe=False)\n",
    "\n",
    "    fewshot_results_per_category[category] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "079be432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for Unsupported claim: \n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Micro F1: 0.0\n",
      "True positives: 0\n",
      "False positives: 51\n",
      "False negatives: 41 \n",
      "\n",
      "Scores for Format: \n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Micro F1: 0.0\n",
      "True positives: 0\n",
      "False positives: 9\n",
      "False negatives: 10 \n",
      "\n",
      "Scores for Coherence: \n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Micro F1: 0.0\n",
      "True positives: 0\n",
      "False positives: 10\n",
      "False negatives: 7 \n",
      "\n",
      "Scores for Lacks synthesis: \n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Micro F1: 0.0\n",
      "True positives: 0\n",
      "False positives: 12\n",
      "False negatives: 6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save results to a json file \n",
    "import json\n",
    "\n",
    "folder = \"../experiments\"\n",
    "filename = \"fewshot_micro_avg_first_ten.json\"\n",
    "\n",
    "with open(f\"{folder}/{filename}\", 'w') as f:\n",
    "    json.dump(fewshot_results_per_category, f, indent=4)\n",
    "\n",
    "for category in fewshot_results_per_category:\n",
    "    print(f\"Scores for {category}: \")\n",
    "    print(f\"Precision: {fewshot_results_per_category[category]['precision']}\")\n",
    "    print(f\"Recall: {fewshot_results_per_category[category]['recall']}\")\n",
    "    print(f\"Micro F1: {fewshot_results_per_category[category]['f1']}\")\n",
    "    print(f\"True positives: {fewshot_results_per_category[category]['tp']}\")\n",
    "    print(f\"False positives: {fewshot_results_per_category[category]['fp']}\")\n",
    "    print(f\"False negatives: {fewshot_results_per_category[category]['fn']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ce3f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Unsupported claim', 'Format', 'Coherence', 'Lacks synthesis']\n",
    "\n",
    "zeroshot_results_per_category = {}\n",
    "\n",
    "for category in categories:\n",
    "    all_gold = []\n",
    "    all_pred = []\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        all_gold.extend(goldset_spans[i].get(category, []))\n",
    "        all_pred.extend(predset_zeroshot_spans[i].get(category, []))\n",
    "\n",
    "    res = span_f1_exact(all_gold, all_pred, dedupe=False)\n",
    "\n",
    "    zeroshot_results_per_category[category] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e46fd6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for Unsupported claim: \n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Micro F1: 0.0\n",
      "True positives: 0\n",
      "False positives: 52\n",
      "False negatives: 41 \n",
      "\n",
      "Scores for Format: \n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Micro F1: 0.0\n",
      "True positives: 0\n",
      "False positives: 7\n",
      "False negatives: 10 \n",
      "\n",
      "Scores for Coherence: \n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Micro F1: 0.0\n",
      "True positives: 0\n",
      "False positives: 10\n",
      "False negatives: 7 \n",
      "\n",
      "Scores for Lacks synthesis: \n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Micro F1: 0.0\n",
      "True positives: 0\n",
      "False positives: 9\n",
      "False negatives: 6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save results to a json file \n",
    "import json \n",
    "\n",
    "folder = \"../experiments\"\n",
    "filename = \"zeroshot_micro_avg_first_ten.json\"\n",
    "\n",
    "with open(f\"{folder}/{filename}\", 'w') as f:\n",
    "    json.dump(zeroshot_results_per_category, f, indent=4)\n",
    "\n",
    "for category in zeroshot_results_per_category:\n",
    "    print(f\"Scores for {category}: \")\n",
    "    print(f\"Precision: {zeroshot_results_per_category[category]['precision']}\")\n",
    "    print(f\"Recall: {zeroshot_results_per_category[category]['recall']}\")\n",
    "    print(f\"Micro F1: {zeroshot_results_per_category[category]['f1']}\")\n",
    "    print(f\"True positives: {zeroshot_results_per_category[category]['tp']}\")\n",
    "    print(f\"False positives: {zeroshot_results_per_category[category]['fp']}\")\n",
    "    print(f\"False negatives: {zeroshot_results_per_category[category]['fn']} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caacf15",
   "metadata": {},
   "source": [
    "### Macro average scores per category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f48269c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "Span = Tuple[int, int]\n",
    "\n",
    "def macro_average_span_f1(\n",
    "    goldset_spans, predset_spans, categories, dedupe = True, unit = \"category\",  # \"paper_category\" | \"paper\" | \"category\"\n",
    "    skip_empty_gold = True):\n",
    "    f1s = []\n",
    "    details = []\n",
    "\n",
    "    n = min(len(goldset_spans), len(predset_spans))\n",
    "\n",
    "    if unit == \"paper_category\":\n",
    "        for i in range(n):\n",
    "            for cat in categories:\n",
    "                gold = goldset_spans[i].get(cat, [])\n",
    "                pred = predset_spans[i].get(cat, [])\n",
    "                res = span_f1_exact(gold, pred, dedupe=dedupe)\n",
    "\n",
    "                if skip_empty_gold and res[\"tp\"] == 0 and res[\"fn\"] == 0:\n",
    "                    continue\n",
    "\n",
    "                f1s.append(res[\"f1\"])\n",
    "                details.append({\"paper\": i + 1, \"category\": cat, **res})\n",
    "\n",
    "    elif unit == \"paper\":\n",
    "        for i in range(n):\n",
    "            gold_all, pred_all = [], []\n",
    "            for cat in categories:\n",
    "                gold_all.extend(goldset_spans[i].get(cat, []))\n",
    "                pred_all.extend(predset_spans[i].get(cat, []))\n",
    "\n",
    "            res = span_f1_exact(gold_all, pred_all, dedupe=dedupe)\n",
    "\n",
    "            if skip_empty_gold and res[\"tp\"] == 0 and res[\"fn\"] == 0:\n",
    "                continue\n",
    "\n",
    "            f1s.append(res[\"f1\"])\n",
    "            details.append({\"paper\": i + 1, **res})\n",
    "\n",
    "    elif unit == \"category\":\n",
    "        for cat in categories:\n",
    "            gold_all, pred_all = [], []\n",
    "            for i in range(n):\n",
    "                gold_all.extend(goldset_spans[i].get(cat, []))\n",
    "                pred_all.extend(predset_spans[i].get(cat, []))\n",
    "\n",
    "            res = span_f1_exact(gold_all, pred_all, dedupe=dedupe)\n",
    "\n",
    "            if skip_empty_gold and res[\"tp\"] == 0 and res[\"fn\"] == 0:\n",
    "                continue\n",
    "\n",
    "            f1s.append(res[\"f1\"])\n",
    "            details.append({\"category\": cat, **res})\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"unit must be one of: 'paper_category', 'paper', 'category'\")\n",
    "\n",
    "    macro_f1 = sum(f1s) / len(f1s) if f1s else 0.0\n",
    "\n",
    "    return {\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"num_units_scored\": len(f1s),\n",
    "        \"unit\": unit,\n",
    "        \"skip_empty_gold\": skip_empty_gold,\n",
    "        \"details\": details,  # remove if too large\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4233bb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 4\n"
     ]
    }
   ],
   "source": [
    "categories = ['Unsupported claim', 'Format', 'Coherence', 'Lacks synthesis']\n",
    "\n",
    "out = macro_average_span_f1(\n",
    "    goldset_spans,\n",
    "    predset_fewshot_spans,\n",
    "    categories,\n",
    "    dedupe=True,\n",
    "    unit=\"category\",\n",
    "    skip_empty_gold=True,\n",
    ")\n",
    "\n",
    "print(out[\"macro_f1\"], out[\"num_units_scored\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc5bf01",
   "metadata": {},
   "source": [
    "### Token overlap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8895d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "Span = Tuple[int, int]  # (start_char, end_char) end-exclusive by default\n",
    "_WORD_RE = re.compile(r\"\\S+\")\n",
    "\n",
    "def whitespace_tokenize_with_offsets(text: str):\n",
    "    return [(m.group(0), m.start(), m.end()) for m in _WORD_RE.finditer(text)]\n",
    "\n",
    "def spans_to_token_indices(\n",
    "    text: str,\n",
    "    spans: List[Span],\n",
    "    token_offsets=None,\n",
    "    inclusive_end: bool = False,\n",
    "):\n",
    "    if token_offsets is None:\n",
    "        token_offsets = whitespace_tokenize_with_offsets(text)\n",
    "\n",
    "    covered = set()\n",
    "    for i, (_tok, tstart, tend) in enumerate(token_offsets):\n",
    "        for s, e in spans:\n",
    "            span_start = s\n",
    "            span_end = e + 1 if inclusive_end else e\n",
    "            if not (tend <= span_start or tstart >= span_end):\n",
    "                covered.add(i)\n",
    "                break\n",
    "    return covered\n",
    "\n",
    "def token_overlap_metrics_aggregate(\n",
    "    texts: List[str],\n",
    "    gold_spans_list: List[List[Span]],\n",
    "    pred_spans_list: List[List[Span]],\n",
    "    *,\n",
    "    method,\n",
    "    filenames: List[str],                 # <-- add this\n",
    "    tokenize_with_offsets=None,\n",
    "    inclusive_end: bool = False,\n",
    "    out_prefix: str = \"token_overlap\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Micro-averaged token overlap metrics across all texts + dumps TP/FP/FN token instances.\n",
    "\n",
    "    Produces:\n",
    "      {out_prefix}_tp.json\n",
    "      {out_prefix}_fp.json\n",
    "      {out_prefix}_fn.json\n",
    "    \"\"\"\n",
    "    if len(filenames) != len(texts):\n",
    "        raise ValueError(f\"filenames must be same length as texts: {len(filenames)} vs {len(texts)}\")\n",
    "\n",
    "    TP: List[Dict[str, Any]] = []\n",
    "    FP: List[Dict[str, Any]] = []\n",
    "    FN: List[Dict[str, Any]] = []\n",
    "\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "\n",
    "    for doc_id, (fn, text, gold_spans, pred_spans) in enumerate(\n",
    "        zip(filenames, texts, gold_spans_list, pred_spans_list)\n",
    "    ):\n",
    "        token_offsets = (\n",
    "            whitespace_tokenize_with_offsets(text)\n",
    "            if tokenize_with_offsets is None\n",
    "            else tokenize_with_offsets(text)\n",
    "        )\n",
    "\n",
    "        gold_idxs = spans_to_token_indices(text, gold_spans, token_offsets, inclusive_end)\n",
    "        pred_idxs = spans_to_token_indices(text, pred_spans, token_offsets, inclusive_end)\n",
    "\n",
    "        tp_idxs = gold_idxs & pred_idxs\n",
    "        fp_idxs = pred_idxs - gold_idxs\n",
    "        fn_idxs = gold_idxs - pred_idxs\n",
    "\n",
    "        total_tp += len(tp_idxs)\n",
    "        total_fp += len(fp_idxs)\n",
    "        total_fn += len(fn_idxs)\n",
    "\n",
    "        # Store token records (now includes filename)\n",
    "        for i in sorted(tp_idxs):\n",
    "            tok, s, e = token_offsets[i]\n",
    "            TP.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"filename\": fn,\n",
    "                \"token_index\": i,\n",
    "                \"token\": tok,\n",
    "                \"char_start\": s,\n",
    "                \"char_end\": e,\n",
    "            })\n",
    "\n",
    "        for i in sorted(fp_idxs):\n",
    "            tok, s, e = token_offsets[i]\n",
    "            FP.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"filename\": fn,\n",
    "                \"token_index\": i,\n",
    "                \"token\": tok,\n",
    "                \"char_start\": s,\n",
    "                \"char_end\": e,\n",
    "            })\n",
    "\n",
    "        for i in sorted(fn_idxs):\n",
    "            tok, s, e = token_offsets[i]\n",
    "            FN.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"filename\": fn,\n",
    "                \"token_index\": i,\n",
    "                \"token\": tok,\n",
    "                \"char_start\": s,\n",
    "                \"char_end\": e,\n",
    "            })\n",
    "\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) else 0.0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n",
    "\n",
    "    # Dump JSON files\n",
    "    with open(f\"../experiments/to_examine/{method}/{out_prefix}_tp.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(TP, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(f\"../experiments/to_examine/{method}/{out_prefix}_fp.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(FP, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(f\"../experiments/to_examine/{method}/{out_prefix}_fn.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(FN, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return {\n",
    "        \"tp\": total_tp,\n",
    "        \"fp\": total_fp,\n",
    "        \"fn\": total_fn,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"tp_file\": f\"{out_prefix}_tp.json\",\n",
    "        \"fp_file\": f\"{out_prefix}_fp.json\",\n",
    "        \"fn_file\": f\"{out_prefix}_fn.json\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76b1b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-level scores for Unsupported claim: \n",
      "Precision: 0.4681\n",
      "Recall: 0.6661\n",
      "Micro F1: 0.5498\n",
      "fewshot_unsupported_claim_tp.json fewshot_unsupported_claim_fp.json fewshot_unsupported_claim_fn.json\n",
      "Token-level scores for Format: \n",
      "Precision: 0.2667\n",
      "Recall: 0.1951\n",
      "Micro F1: 0.2254\n",
      "fewshot_format_tp.json fewshot_format_fp.json fewshot_format_fn.json\n",
      "Token-level scores for Coherence: \n",
      "Precision: 0.2745\n",
      "Recall: 0.4989\n",
      "Micro F1: 0.3542\n",
      "fewshot_coherence_tp.json fewshot_coherence_fp.json fewshot_coherence_fn.json\n",
      "Token-level scores for Lacks synthesis: \n",
      "Precision: 0.557\n",
      "Recall: 0.7924\n",
      "Micro F1: 0.6542\n",
      "fewshot_lacks_synthesis_tp.json fewshot_lacks_synthesis_fp.json fewshot_lacks_synthesis_fn.json\n"
     ]
    }
   ],
   "source": [
    "categories = ['Unsupported claim', 'Format', 'Coherence', 'Lacks synthesis']\n",
    "\n",
    "for category in categories:\n",
    "    all_texts = []\n",
    "    all_gold_spans = []\n",
    "    all_pred_spans = []\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        text_content = goldset_sorted[i][0]['full_text']  \n",
    "        all_texts.append(text_content)\n",
    "        gold_spans = goldset_spans[i].get(category, [])\n",
    "        pred_spans = predset_fewshot_spans[i].get(category, [])\n",
    "        all_gold_spans.append(gold_spans)\n",
    "        all_pred_spans.append(pred_spans)\n",
    "\n",
    "    token_metrics = token_overlap_metrics_aggregate(\n",
    "        all_texts,\n",
    "        all_gold_spans,\n",
    "        all_pred_spans,\n",
    "        tokenize_with_offsets=None,\n",
    "        inclusive_end=True, \n",
    "        method='fewshot',  \n",
    "        filenames=[f\"paper_{i+1}.txt\" for i in range(10)],  # <-- pass filenames\n",
    "        out_prefix=f\"fewshot_{category.replace(' ', '_').lower()}\"\n",
    "    )\n",
    "\n",
    "    print(f\"Token-level scores for {category}: \")\n",
    "    print(f\"Precision: {round(token_metrics['precision'], 4)}\")\n",
    "    print(f\"Recall: {round(token_metrics['recall'], 4)}\")\n",
    "    print(f\"Micro F1: {round(token_metrics['f1'], 4)}\")\n",
    "    print(token_metrics[\"tp_file\"], token_metrics[\"fp_file\"], token_metrics[\"fn_file\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "edc889cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-level scores for Unsupported claim: \n",
      "Precision: 0.4451\n",
      "Recall: 0.624\n",
      "Micro F1: 0.5196\n",
      "zeroshot_unsupported_claim_tp.json zeroshot_unsupported_claim_fp.json zeroshot_unsupported_claim_fn.json\n",
      "Token-level scores for Format: \n",
      "Precision: 0.3\n",
      "Recall: 0.2195\n",
      "Micro F1: 0.2535\n",
      "zeroshot_format_tp.json zeroshot_format_fp.json zeroshot_format_fn.json\n",
      "Token-level scores for Coherence: \n",
      "Precision: 0.1505\n",
      "Recall: 0.3045\n",
      "Micro F1: 0.2014\n",
      "zeroshot_coherence_tp.json zeroshot_coherence_fp.json zeroshot_coherence_fn.json\n",
      "Token-level scores for Lacks synthesis: \n",
      "Precision: 0.5027\n",
      "Recall: 0.5727\n",
      "Micro F1: 0.5354\n",
      "zeroshot_lacks_synthesis_tp.json zeroshot_lacks_synthesis_fp.json zeroshot_lacks_synthesis_fn.json\n"
     ]
    }
   ],
   "source": [
    "categories = ['Unsupported claim', 'Format', 'Coherence', 'Lacks synthesis']\n",
    "\n",
    "for category in categories:\n",
    "    all_texts = []\n",
    "    all_gold_spans = []\n",
    "    all_pred_spans = []\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        text_content = goldset_sorted[i][0]['full_text']  # assuming full_text is available in goldset_sorted\n",
    "        all_texts.append(text_content)\n",
    "\n",
    "        gold_spans = goldset_spans[i].get(category, [])\n",
    "        pred_spans = predset_zeroshot_spans[i].get(category, [])\n",
    "\n",
    "        all_gold_spans.append(gold_spans)\n",
    "        all_pred_spans.append(pred_spans)\n",
    "\n",
    "    token_metrics = token_overlap_metrics_aggregate(\n",
    "        all_texts,\n",
    "        all_gold_spans,\n",
    "        all_pred_spans,\n",
    "        tokenize_with_offsets=None,\n",
    "        inclusive_end=True, \n",
    "        method='zeroshot',  \n",
    "        filenames=[f\"paper_{i+1}.txt\" for i in range(10)],  # <-- pass filenames\n",
    "        out_prefix=f\"zeroshot_{category.replace(' ', '_').lower()}\"\n",
    "    )\n",
    "\n",
    "    print(f\"Token-level scores for {category}: \")\n",
    "    print(f\"Precision: {round(token_metrics['precision'], 4)}\")\n",
    "    print(f\"Recall: {round(token_metrics['recall'], 4)}\")\n",
    "    print(f\"Micro F1: {round(token_metrics['f1'], 4)}\")\n",
    "    print(token_metrics[\"tp_file\"], token_metrics[\"fp_file\"], token_metrics[\"fn_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506d5b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
