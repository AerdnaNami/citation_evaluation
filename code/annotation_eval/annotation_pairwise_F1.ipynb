{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867026af",
   "metadata": {},
   "source": [
    "### Prepping data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c302f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "HEADER_RE = re.compile(\n",
    "    r\"^\\s*\\[(?P<span>\\d+-\\d+)\\]\\s*(?P<label>[^,]+),\\s*File:\\s*(?P<filename>[^,]+)\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_ann_text(path: str, encoding: str = \"utf-8\"):\n",
    "    \"\"\"\n",
    "    Read `path` and return a list of dicts, each dict:\n",
    "      {\n",
    "        \"span\": \"836-1053\",\n",
    "        \"filename\": \"paper_16.txt\",\n",
    "        \"label\": \"Coherence\",\n",
    "        \"body\": \"the text after the header up to the next header (multi-line string)\"\n",
    "      }\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=encoding) as fh:\n",
    "        lines = fh.readlines()\n",
    "\n",
    "    # Find indices of header lines and capture groups\n",
    "    headers = []  # list of tuples (index, matchobj)\n",
    "    for i, line in enumerate(lines):\n",
    "        m = HEADER_RE.match(line)\n",
    "        if m:\n",
    "            headers.append((i, m))\n",
    "\n",
    "    results: List[Dict[str, Optional[str]]] = []\n",
    "    if not headers:\n",
    "        return results\n",
    "\n",
    "    # For each found header, slice the following lines until next header (or EOF)\n",
    "    for idx, (line_idx, match) in enumerate(headers):\n",
    "        next_idx = headers[idx + 1][0] if (idx + 1) < len(headers) else len(lines)\n",
    "\n",
    "        # Extract body: lines after the header line up to next header index\n",
    "        body_lines = [ln.rstrip(\"\\n\") for ln in lines[line_idx + 1 : next_idx]]\n",
    "        body = \"\\n\".join(body_lines).strip() or None\n",
    "\n",
    "        span = match.group(\"text\").strip() if match.group(\"text\") else None\n",
    "        start_span = int(span.split(\"-\")[0])\n",
    "        end_span = int(span.split(\"-\")[1])\n",
    "        filename = match.group(\"file\").strip() if match.group(\"file\") else None\n",
    "        label = match.group(\"label\").strip() if match.group(\"label\") else None\n",
    "\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"start\": start_span,\n",
    "            \"end\": end_span,\n",
    "            \"label\": label,\n",
    "            \"text\": body, \n",
    "        })\n",
    "        print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c92017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 36 annotations from Ed\n",
      "Extracted 46 annotations from Ekaterina\n",
      "Extracted 43 annotations from Iman\n",
      "Extracted 41 annotations from Iraa\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import json\n",
    "\n",
    "annotation_folder = \"../../annotated_data/citation_annotations/annotations\"\n",
    "total_anns = {}\n",
    "papers = [11, 13, 14, 15, 16, 17, 18, 19, 20, 100] # read the annotated overlapping paper IDs \n",
    "\n",
    "for ann in os.listdir(annotation_folder):\n",
    "    total_anns[ann] = []\n",
    "    for file in os.listdir(os.path.join(annotation_folder, ann)):\n",
    "        if file.endswith(\".json\") and int(file.split(\"_\")[1].split(\".\")[0]) in papers:\n",
    "            file_path = os.path.join(annotation_folder, ann, file)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                annotations = json.load(f)\n",
    "            total_anns[ann].extend(annotations)\n",
    "\n",
    "    print(f\"Extracted {len(total_anns[ann])} annotations from {ann}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd7cdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ed': [{'file': 'paper_100.txt',\n",
       "   'start': 594,\n",
       "   'end': 966,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ed',\n",
       "   'text': 'Zhang et al. (2019) improves an LSTM-\\nbased encoder-decoder model with online vocabulary adaptation. For abbreviated pinyin, CoCAT (Huang et al., 2015) uses machine translation technology to reduce the number of the typing letters. Huang and Zhao (2018) propose an LSTM-based encoder-decoder approach with the concatenation of context words and abbreviated pinyin as input'},\n",
       "  {'file': 'paper_100.txt',\n",
       "   'start': 1110,\n",
       "   'end': 1646,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ed',\n",
       "   'text': 'In addition, there are some works handling\\npinyin with typing errors. Chen and Lee (2000) investigate a typing model which handles spelling correction in sentence-based pinyin input method. CHIME (Zheng et al., 2011) is a error-tolerant Chinese pinyin input method. It finds similar pinyin which will be further ranked with Chinese specific features. Jia and Zhao (2014) propose a joint graph model to globally optimize the tasks of pinyin input method and typo correction. We leave error-tolerant pinyin input method as a future work. '},\n",
       "  {'file': 'paper_100.txt',\n",
       "   'start': 2088,\n",
       "   'end': 2529,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ed',\n",
       "   'text': 'Zhang et al. (2021a) add a pinyin embedding layer and learns to predict characters from similarly pronounced candidates. PLOME (Liu et al., 2021) add two embedding layers implemented with two GRU networks to inject both pinyin and shape of characters, respectively. Xu et al. (2021) add a hierarchical encoder to inject the pinyin letters at character and sentence levels, and add a ResNet encoder to use graphic features of character image.'},\n",
       "  {'file': 'paper_100.txt',\n",
       "   'start': 2471,\n",
       "   'end': 2478,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'ResNet '},\n",
       "  {'file': 'paper_100.txt',\n",
       "   'start': 1929,\n",
       "   'end': 1934,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'BERT '},\n",
       "  {'file': 'paper_100.txt',\n",
       "   'start': 1815,\n",
       "   'end': 1820,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'BERT '},\n",
       "  {'file': 'paper_11.txt',\n",
       "   'start': 797,\n",
       "   'end': 960,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': ' alternative end-to-end approach that can tackle the problem purely cross-lingually, i.e., without involving MT, would clearly be more efficient and cost-effective'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 3365,\n",
       "   'end': 3395,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'In contrast to most prior work'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 14,\n",
       "   'end': 105,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'Few-shot learning is the problem of learning classifiers with only a few training examples.'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 445,\n",
       "   'end': 932,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Ed',\n",
       "   'text': '\\nIn recent years, there has been a surge in zeroshot and few-shot approaches to text classification. One approach (Yin et al., 2019, 2020; Halder et al., 2020;Wang et al., 2021) makes use of entailment models. Textual entailment (Dagan et al., 2006), also known as natural language inference (NLI) (Bowman et al., 2015), is the problem of predicting whether a textual premise implies a textual hypothesis in a logical sense. For example, Emma loves apples implies that Emma likes apples.'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 933,\n",
       "   'end': 1190,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Ed',\n",
       "   'text': '\\nThe entailment approach for text classification sets the input text as the premise and the text repre-senting the label as the hypothesis. A NLI model is applied to each input pair and the entailment probability is used to identify the best matching label.'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 1713,\n",
       "   'end': 1881,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'In contrast, the models typically applied in the entailment approach are Cross Attention (CA) models which need to be executed for every combination of text and label. '},\n",
       "  {'file': 'paper_14.txt',\n",
       "   'start': 182,\n",
       "   'end': 310,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'Unfortunately, for many languages, and especially low-resource languages, such taskspecific labelled data is often not available'},\n",
       "  {'file': 'paper_14.txt',\n",
       "   'start': 2549,\n",
       "   'end': 2644,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'as this is the only task for which high-quality data is available in a large number of language'},\n",
       "  {'file': 'paper_14.txt',\n",
       "   'start': 2833,\n",
       "   'end': 2980,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'a base understanding of syntactic structure in both the source and target language is necessary for any meaningful natural language processing task'},\n",
       "  {'file': 'paper_15.txt',\n",
       "   'start': 897,\n",
       "   'end': 902,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ed',\n",
       "   'text': '2021)'},\n",
       "  {'file': 'paper_15.txt',\n",
       "   'start': 14,\n",
       "   'end': 105,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'Multimodal machine translation is a cross-domain task in the filed of machine translation. '},\n",
       "  {'file': 'paper_16.txt',\n",
       "   'start': 713,\n",
       "   'end': 1264,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Ed',\n",
       "   'text': 'Researchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AM-PERE  for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs. Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation. There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.'},\n",
       "  {'file': 'paper_16.txt',\n",
       "   'start': 13,\n",
       "   'end': 416,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ed',\n",
       "   'text': '\\nTo facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al., 2018). Datasets for long docu-ments include Sharma et al. (2019), Cohan et al. (2018), andFisas et al. (2016). '},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 14,\n",
       "   'end': 1286,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Ed',\n",
       "   'text': 'Fully supervised event extraction. Event extraction has been studied for over a decade (Ahn, 2006;Ji and Grishman, 2008) and most traditional event extraction works follow the fully supervised setting (Nguyen et al., 2016;Sha et al., 2018;Nguyen and Nguyen, 2019;Yang et al., 2019;Lin et al., 2020;Li et al., 2020). Many of them use classification-based models and use pipeline-style frameworks to extract events (Nguyen et al., 2016;Yang et al., 2019;Wadden et al., 2019). To better leverage shared knowledge in event triggers and arguments, some works propose to incorporate global features to jointly decide triggers and arguments (Lin et al., 2020;Li et al., 2013;Yang and Mitchell, 2016). Recently, few generation-based event extraction models have been proposed. TANL (Paolini et al., 2021) treats event extraction as translation tasks between augmented natural languages. Their predicted targetaugmented language embed labels into the input passage via using brackets and vertical bar symbols, hindering the model from fully leveraging label semantics. BART-Gen  is also a generation-based model focusing on documentlevel event argument extraction. Yet, similar to TANL, they solve event extraction with a pipeline, which prevents knowledge sharing across subtasks.'},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1726,\n",
       "   'end': 2044,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Ed',\n",
       "   'text': 'Liu et al. (2020) uses a machine reading comprehension formulation to conduct event extraction in a low-resource regime. Text2Event (Lu et al., 2021), a sequence-to-structure generation paradigm, first presents events in a linearized format, and then trains a generative model to generate the linearized event sequence'},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1726,\n",
       "   'end': 2044,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ed',\n",
       "   'text': 'Liu et al. (2020) uses a machine reading comprehension formulation to conduct event extraction in a low-resource regime. Text2Event (Lu et al., 2021), a sequence-to-structure generation paradigm, first presents events in a linearized format, and then trains a generative model to generate the linearized event sequence'},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1074,\n",
       "   'end': 1169,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'BART-Gen  is also a generation-based model focusing on documentlevel event argument extraction.'},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1397,\n",
       "   'end': 1587,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'However, their designs are not specific for low-resource scenarios, hence, these models can not enjoy all the benefits that DEGREE obtains for low-resource event extraction at the same time,'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 952,\n",
       "   'end': 970,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ed',\n",
       "   'text': '(Li et al., 2020a;'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 3495,\n",
       "   'end': 3574,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'three large-scale benchmark datasets (OntoNotes V4.0, OntoNotes V5.0, and MSRA)'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 3769,\n",
       "   'end': 3792,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'medical dataset (CBLUE)'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 1566,\n",
       "   'end': 1654,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': ' shown promising for AL in NLP due to its good qualitative and computational performance'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 1801,\n",
       "   'end': 1824,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ed',\n",
       "   'text': ' Shelmanov et al. (2021'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 252,\n",
       "   'end': 631,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ed',\n",
       "   'text': ' Following Chen et al. (2020c), other works adopt PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a). Kale and Rastogi (2020b) and Ribeiro et al. (2020) showed that PLMs using linearized representations of data can outperform graph neural networks on graph-to-text datasets, recently surpassed again by graph-based models (Ke et al., 2021;Chen et al., 2020a)'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 3514,\n",
       "   'end': 3533,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ed',\n",
       "   'text': 'Jiang et al., 2020)'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 1781,\n",
       "   'end': 1870,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ed',\n",
       "   'text': 'Recently,  have shown that using a content plan leads to improved quality of PLM outputs.'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 39,\n",
       "   'end': 631,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Ed',\n",
       "   'text': 'Large neural language models pretrained on self-supervised tasks (Lewis et al., 2020;Liu et al., 2019;Devlin et al., 2019) have recently gained a lot of traction in D2T generation research (Ferreira et al., 2020). Following Chen et al. (2020c), other works adopt PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a). Kale and Rastogi (2020b) and Ribeiro et al. (2020) showed that PLMs using linearized representations of data can outperform graph neural networks on graph-to-text datasets, recently surpassed again by graph-based models (Ke et al., 2021;Chen et al., 2020a)'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 1003,\n",
       "   'end': 1051,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ed',\n",
       "   'text': '(Heidari et al., 2021;Kale and Rastogi, 2020a;. '},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 2107,\n",
       "   'end': 2491,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Ed',\n",
       "   'text': 'Sentence ordering is the task of organizing a set of natural language sentences to increase the coherence of a text (Barzilay et al., 2001;Lapata, 2003). Several neural methods for this task were proposed, using either interactions between pairs of sentences Li and Jurafsky, 2017), global interactions (Gong et al., 2016;Wang and Wan, 2019), or combination of both (Cui et al., 2020)'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 4026,\n",
       "   'end': 4047,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ed',\n",
       "   'text': '(Botha et al., 2018;.'}],\n",
       " 'Ekaterina': [{'file': 'paper_100.txt',\n",
       "   'start': 594,\n",
       "   'end': 825,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Zhang et al. (2019) improves an LSTM-\\nbased encoder-decoder model with online vocabulary adaptation. For abbreviated pinyin, CoCAT (Huang et al., 2015) uses machine translation technology to reduce the number of the typing letters.'},\n",
       "  {'file': 'paper_100.txt',\n",
       "   'start': 968,\n",
       "   'end': 1179,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Our work differs from existing works in that we are the first one to exploit GPT and verify the pros and cons of GPT in different situations. In addition, there are some works handling\\npinyin with typing errors.'},\n",
       "  {'file': 'paper_100.txt',\n",
       "   'start': 1763,\n",
       "   'end': 2087,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Sun et al. (2021) propose a general-purpose Chinese BERT with\\nnew embedding layers to inject pinyin and glyph information of characters. There are also task-specific BERT models, especially for the task of grammatical error correction since an important type of error is caused by characters pronounced with the same pinyin.'},\n",
       "  {'file': 'paper_11.txt',\n",
       "   'start': 1025,\n",
       "   'end': 1042,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'multilingual BERT'},\n",
       "  {'file': 'paper_11.txt',\n",
       "   'start': 1485,\n",
       "   'end': 1487,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': ' 1'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 14,\n",
       "   'end': 105,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Few-shot learning is the problem of learning classifiers with only a few training examples.'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 267,\n",
       "   'end': 444,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'For text data, this is usually accomplished by representing the labels of the task in a textual form, which can either be the name of the label or a concise textual description.'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 1713,\n",
       "   'end': 1880,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'In contrast, the models typically applied in the entailment approach are Cross Attention (CA) models which need to be executed for every combination of text and label.'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 1881,\n",
       "   'end': 2119,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'On the other hand, they allow for interaction between the tokens of label and input, so that in theory they should be superior in classification accuracy. However, in this work we show that in practice, the difference in quality is small.'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 2121,\n",
       "   'end': 2383,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Both CA and SNs also support the few-shot learning setup by fine-tuning the models on a small number of labeled examples. This is usually done by updating all parameters of the model, which in turn makes it impossible to share the models between different tasks.'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 3365,\n",
       "   'end': 3483,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'In contrast to most prior work, we also show that these results can also be achieved for languages other than English.'},\n",
       "  {'file': 'paper_14.txt',\n",
       "   'start': 14,\n",
       "   'end': 181,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'At present, for a large majority of natural language processing tasks, the most successful approach is fine-tuning pre-trained models with task-specific labelled data.'},\n",
       "  {'file': 'paper_15.txt',\n",
       "   'start': 649,\n",
       "   'end': 715,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Researchers also realize that the vision modality maybe redundant.'},\n",
       "  {'file': 'paper_15.txt',\n",
       "   'start': 897,\n",
       "   'end': 902,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': '2021)'},\n",
       "  {'file': 'paper_15.txt',\n",
       "   'start': 864,\n",
       "   'end': 956,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Encouraging results appeared in  2021) proposed a cross-lingual visual pretraining approach.'},\n",
       "  {'file': 'paper_15.txt',\n",
       "   'start': 14,\n",
       "   'end': 1184,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Multimodal machine translation is a cross-domain task in the filed of machine translation. Early attempts mainly focused on enhancing the MMT model by better incorporation of the vision features (Calixto and Liu, 2017;Elliott and Kádár, 2017;Delbrouck and Dupont, 2017). However, directly encoding the whole image feature brings additional noise to the text (Yao and Wan, 2020;Liu et al., 2021a). To address the above issue, Yao and Wan (2020) proposed a multimodal self-attention to consider the relative difference of information between two modalities. Similarly, Liu et al. (2021a) used a Gumbel Softmax to achieve the same goal.\\n\\nResearchers also realize that the vision modality maybe redundant. Irrelevant images have little impact on the translation quality, and no significant BLEU drop is observed even the image is absent (Elliott, 2018). Encouraging results appeared in  2021) proposed a cross-lingual visual pretraining approach. In this work, we make a systematic study on whether stronger vision features are helpful. We also extend the research to enhanced features, such as object-detection and image captioning, which is complementary to previous work.'},\n",
       "  {'file': 'paper_16.txt',\n",
       "   'start': 856,\n",
       "   'end': 906,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'AM-PERE  for proposition classification in reviews'},\n",
       "  {'file': 'paper_16.txt',\n",
       "   'start': 1093,\n",
       "   'end': 1621,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.\\n\\nA wide range of control perspectives has been explored in controllable generation, including style control (e.g., sentiments (Duan et al., 2020), politeness (Madaan et al., 2020), formality , domains (Takeno et al., 2017) and persona ) and content control (e.g., length (Duan et al., 2020), entities (Fan et al., 2018a), and keywords (Tang et al., 2019)).'},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1074,\n",
       "   'end': 1169,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'BART-Gen  is also a generation-based model focusing on documentlevel event argument extraction.'},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1397,\n",
       "   'end': 1586,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'However, their designs are not specific for low-resource scenarios, hence, these models can not enjoy all the benefits that DEGREE obtains for low-resource event extraction at the same time'},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1726,\n",
       "   'end': 2044,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Liu et al. (2020) uses a machine reading comprehension formulation to conduct event extraction in a low-resource regime. Text2Event (Lu et al., 2021), a sequence-to-structure generation paradigm, first presents events in a linearized format, and then trains a generative model to generate the linearized event sequence'},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 2261,\n",
       "   'end': 2403,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Another thread of works are using meta-learning to deal with the less label challenge (Deng et al., 2020;Shen et al., 2021;Cong et al., 2021).'},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1619,\n",
       "   'end': 2259,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': \"Low-resource event extraction. It has been a rising interest in event extraction under less data scenario. Liu et al. (2020) uses a machine reading comprehension formulation to conduct event extraction in a low-resource regime. Text2Event (Lu et al., 2021), a sequence-to-structure generation paradigm, first presents events in a linearized format, and then trains a generative model to generate the linearized event sequence. Text2Event's unnatural output format hinders the model from fully leveraging pre-trained knowledge. Hence, their model falls short on the cases with only extremely low data being available (as shown in Section 3).\"},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 952,\n",
       "   'end': 970,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': '(Li et al., 2020a;'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 1391,\n",
       "   'end': 1496,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Nevertheless, building the lexicon is time-consuming and the quality of the lexicon may not be satisfied.'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 2131,\n",
       "   'end': 2253,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'However, too immersed regularity leads to unfavorable boundary detection of entities and disturbing character composition.'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 3533,\n",
       "   'end': 3547,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'OntoNotes V4.0'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 3549,\n",
       "   'end': 3563,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'OntoNotes V5.0'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 3569,\n",
       "   'end': 3573,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'MSRA'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 3576,\n",
       "   'end': 3729,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'The results show that RICON achieves considerable improvements compared to the state-of-the-art models, even outperforming existing lexicon-based models.'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 3757,\n",
       "   'end': 3792,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'a practical medical dataset (CBLUE)'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 14,\n",
       "   'end': 174,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Deep learning, to a large extent, has freed data scientists from doing feature engineering, which has been one of the essential obstacles to annotation with AL.'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 723,\n",
       "   'end': 900,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'In our work, we take MNLP as a query strategy for experiments on sequence tagging tasks since it has demonstrated a good trade-off between quality and computational performance.'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 1464,\n",
       "   'end': 1655,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'We continue this line of works by relying on pre-trained Transformers since this architecture has been shown promising for AL in NLP due to its good qualitative and computational performance.'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 1802,\n",
       "   'end': 1824,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Shelmanov et al. (2021'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 2873,\n",
       "   'end': 3303,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Recently proposed alternatives to uncertaintybased query strategies leverage reinforcement learning and imitation learning (Fang et al., 2017;Liu et al., 2018;Vu et al., 2019;Brantley et al., 2020). This series of works aims at constructing trainable policy-based query strategies. However, this requires an excessive amount of computation while the transferability of learned policies across domains and tasks is underresearched.'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 3692,\n",
       "   'end': 3717,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': '(Shelmanov et al., 2021).'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 3422,\n",
       "   'end': 3433,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'ASM problem'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 1003,\n",
       "   'end': 1049,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': '(Heidari et al., 2021;Kale and Rastogi, 2020a;'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 791,\n",
       "   'end': 1331,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Generation Using simple handcrafted templates for individual keys or predicates is an efficient way of introducing domain knowledge while preventing text-to-text models from overfitting to a specific data format (Heidari et al., 2021;Kale and Rastogi, 2020a;. Transforming individual triples to text is also used in Laha et al. (2020) whose work is the most similar to ours. They also build a three-step pipeline for zero-shot D2T generation, but they use handcrafted rules for producing the output text and do not address content planning.'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 14,\n",
       "   'end': 760,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'D2T Generation with PLMs Large neural language models pretrained on self-supervised tasks (Lewis et al., 2020;Liu et al., 2019;Devlin et al., 2019) have recently gained a lot of traction in D2T generation research (Ferreira et al., 2020). Following Chen et al. (2020c), other works adopt PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a). Kale and Rastogi (2020b) and Ribeiro et al. (2020) showed that PLMs using linearized representations of data can outperform graph neural networks on graph-to-text datasets, recently surpassed again by graph-based models (Ke et al., 2021;Chen et al., 2020a). Although the models make use of general-domain pretraining tasks, all of them are eventually finetuned on domain-specific data.'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 1583,\n",
       "   'end': 1870,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'As previously demonstrated, using a content plan in neural D2T generation has important impact on the overall text quality (Moryossef et al., 2019a,b;Puduppully et al., 2019;Trisedya et al., 2020). Recently,  have shown that using a content plan leads to improved quality of PLM outputs.'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 2366,\n",
       "   'end': 2388,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Li and Jurafsky, 2017)'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 3514,\n",
       "   'end': 3533,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'Jiang et al., 2020)'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 4026,\n",
       "   'end': 4046,\n",
       "   'label': 'Format',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': '(Botha et al., 2018;'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 3615,\n",
       "   'end': 4047,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Ekaterina',\n",
       "   'text': 'In contrast to sentence fusion (Geva et al., 2019;Barzilay and McKeown, 2005) or sentence compression (Filippova and Altun, 2013), we operate in the context of multiple sentences in a paragraph. The task is the central focus of our WIKIFLUENT corpus ( §4), which we synthesize using a model for the reverse task, split-andrephrase, i.e. splitting a complex sentence into simpler ones while preserving semantics (Botha et al., 2018;.'}],\n",
       " 'Iman': [{'file': 'paper_100.txt',\n",
       "   'start': 1684,\n",
       "   'end': 2529,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iman',\n",
       "   'text': 'Our methodology also relates to pretrained models that use pinyin information. Sun et al. (2021) propose a general-purpose Chinese BERT with\\nnew embedding layers to inject pinyin and glyph information of characters. There are also task-specific BERT models, especially for the task of grammatical error correction since an important type of error is caused by characters pronounced with the same pinyin. Zhang et al. (2021a) add a pinyin embedding layer and learns to predict characters from similarly pronounced candidates. PLOME (Liu et al., 2021) add two embedding layers implemented with two GRU networks to inject both pinyin and shape of characters, respectively. Xu et al. (2021) add a hierarchical encoder to inject the pinyin letters at character and sentence levels, and add a ResNet encoder to use graphic features of character image.'},\n",
       "  {'file': 'paper_100.txt',\n",
       "   'start': 218,\n",
       "   'end': 1646,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iman',\n",
       "   'text': 'A majority of existing works focus on perfect pinyin. Traditional models are typically based on statistical language models (Chen and Lee, 2000) and statistical machine translation (Yang et al., 2012). Recent works are usually built with neural network. For example, Moon IME (Huang et al., 2018) integrates attention-based neural network and an information retrieval module. Zhang et al. (2019) improves an LSTM-\\nbased encoder-decoder model with online vocabulary adaptation. For abbreviated pinyin, CoCAT (Huang et al., 2015) uses machine translation technology to reduce the number of the typing letters. Huang and Zhao (2018) propose an LSTM-based encoder-decoder approach with the concatenation of context words and abbreviated pinyin as input. Our work differs from existing works in that we are the first one to exploit GPT and verify the pros and cons of GPT in different situations. In addition, there are some works handling\\npinyin with typing errors. Chen and Lee (2000) investigate a typing model which handles spelling correction in sentence-based pinyin input method. CHIME (Zheng et al., 2011) is a error-tolerant Chinese pinyin input method. It finds similar pinyin which will be further ranked with Chinese specific features. Jia and Zhao (2014) propose a joint graph model to globally optimize the tasks of pinyin input method and typo correction. We leave error-tolerant pinyin input method as a future work. '},\n",
       "  {'file': 'paper_100.txt',\n",
       "   'start': 694,\n",
       "   'end': 1646,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Iman',\n",
       "   'text': ' For abbreviated pinyin, CoCAT (Huang et al., 2015) uses machine translation technology to reduce the number of the typing letters. Huang and Zhao (2018) propose an LSTM-based encoder-decoder approach with the concatenation of context words and abbreviated pinyin as input. Our work differs from existing works in that we are the first one to exploit GPT and verify the pros and cons of GPT in different situations. In addition, there are some works handling\\npinyin with typing errors. Chen and Lee (2000) investigate a typing model which handles spelling correction in sentence-based pinyin input method. CHIME (Zheng et al., 2011) is a error-tolerant Chinese pinyin input method. It finds similar pinyin which will be further ranked with Chinese specific features. Jia and Zhao (2014) propose a joint graph model to globally optimize the tasks of pinyin input method and typo correction. We leave error-tolerant pinyin input method as a future work. '},\n",
       "  {'file': 'paper_100.txt',\n",
       "   'start': 1763,\n",
       "   'end': 2529,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Iman',\n",
       "   'text': 'Sun et al. (2021) propose a general-purpose Chinese BERT with\\nnew embedding layers to inject pinyin and glyph information of characters. There are also task-specific BERT models, especially for the task of grammatical error correction since an important type of error is caused by characters pronounced with the same pinyin. Zhang et al. (2021a) add a pinyin embedding layer and learns to predict characters from similarly pronounced candidates. PLOME (Liu et al., 2021) add two embedding layers implemented with two GRU networks to inject both pinyin and shape of characters, respectively. Xu et al. (2021) add a hierarchical encoder to inject the pinyin letters at character and sentence levels, and add a ResNet encoder to use graphic features of character image.'},\n",
       "  {'file': 'paper_11.txt',\n",
       "   'start': 1025,\n",
       "   'end': 1043,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'multilingual BERT '},\n",
       "  {'file': 'paper_11.txt',\n",
       "   'start': 2855,\n",
       "   'end': 2911,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': ' CLIR data: parallel questions (English and non-English)'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 14,\n",
       "   'end': 105,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'Few-shot learning is the problem of learning classifiers with only a few training examples.'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 1713,\n",
       "   'end': 1881,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'In contrast, the models typically applied in the entailment approach are Cross Attention (CA) models which need to be executed for every combination of text and label. '},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 1785,\n",
       "   'end': 1814,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': ' Cross Attention (CA) models '},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 446,\n",
       "   'end': 932,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iman',\n",
       "   'text': 'In recent years, there has been a surge in zeroshot and few-shot approaches to text classification. One approach (Yin et al., 2019, 2020; Halder et al., 2020;Wang et al., 2021) makes use of entailment models. Textual entailment (Dagan et al., 2006), also known as natural language inference (NLI) (Bowman et al., 2015), is the problem of predicting whether a textual premise implies a textual hypothesis in a logical sense. For example, Emma loves apples implies that Emma likes apples.'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 445,\n",
       "   'end': 870,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Iman',\n",
       "   'text': '\\nIn recent years, there has been a surge in zeroshot and few-shot approaches to text classification. One approach (Yin et al., 2019, 2020; Halder et al., 2020;Wang et al., 2021) makes use of entailment models. Textual entailment (Dagan et al., 2006), also known as natural language inference (NLI) (Bowman et al., 2015), is the problem of predicting whether a textual premise implies a textual hypothesis in a logical sense. '},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 2121,\n",
       "   'end': 2242,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'Both CA and SNs also support the few-shot learning setup by fine-tuning the models on a small number of labeled examples.'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 3365,\n",
       "   'end': 3397,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'In contrast to most prior work, '},\n",
       "  {'file': 'paper_14.txt',\n",
       "   'start': 1993,\n",
       "   'end': 2185,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'The language selection does, however, obfuscate the fact that for most non-Indo-European and low-resource languages no data is available for semantically rich tasks such as question answering.'},\n",
       "  {'file': 'paper_14.txt',\n",
       "   'start': 181,\n",
       "   'end': 311,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': ' Unfortunately, for many languages, and especially low-resource languages, such taskspecific labelled data is often not available.'},\n",
       "  {'file': 'paper_14.txt',\n",
       "   'start': 2549,\n",
       "   'end': 2646,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'as this is the only task for which high-quality data is available in a large number of languages,'},\n",
       "  {'file': 'paper_15.txt',\n",
       "   'start': 896,\n",
       "   'end': 902,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iman',\n",
       "   'text': ' 2021)'},\n",
       "  {'file': 'paper_15.txt',\n",
       "   'start': 649,\n",
       "   'end': 1046,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Iman',\n",
       "   'text': 'Researchers also realize that the vision modality maybe redundant. Irrelevant images have little impact on the translation quality, and no significant BLEU drop is observed even the image is absent (Elliott, 2018). Encouraging results appeared in  2021) proposed a cross-lingual visual pretraining approach. In this work, we make a systematic study on whether stronger vision features are helpful.'},\n",
       "  {'file': 'paper_15.txt',\n",
       "   'start': 897,\n",
       "   'end': 902,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iman',\n",
       "   'text': '2021)'},\n",
       "  {'file': 'paper_15.txt',\n",
       "   'start': 649,\n",
       "   'end': 714,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'Researchers also realize that the vision modality maybe redundant'},\n",
       "  {'file': 'paper_16.txt',\n",
       "   'start': 855,\n",
       "   'end': 906,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': ' AM-PERE  for proposition classification in reviews'},\n",
       "  {'file': 'paper_16.txt',\n",
       "   'start': 713,\n",
       "   'end': 1264,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iman',\n",
       "   'text': 'Researchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AM-PERE  for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs. Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation. There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.'},\n",
       "  {'file': 'paper_16.txt',\n",
       "   'start': 1883,\n",
       "   'end': 1926,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iman',\n",
       "   'text': '(Reiter and Dale, 1997;Shao et al., 2019;, '},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1074,\n",
       "   'end': 1083,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'BART-Gen '},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1650,\n",
       "   'end': 2144,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Iman',\n",
       "   'text': \"It has been a rising interest in event extraction under less data scenario. Liu et al. (2020) uses a machine reading comprehension formulation to conduct event extraction in a low-resource regime. Text2Event (Lu et al., 2021), a sequence-to-structure generation paradigm, first presents events in a linearized format, and then trains a generative model to generate the linearized event sequence. Text2Event's unnatural output format hinders the model from fully leveraging pre-trained knowledge\"},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1287,\n",
       "   'end': 1397,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'All these fully supervised methods can achieve substantial performance with a large amount of annotated data. '},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1521,\n",
       "   'end': 1528,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'DEGREE '},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1649,\n",
       "   'end': 2259,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iman',\n",
       "   'text': \" It has been a rising interest in event extraction under less data scenario. Liu et al. (2020) uses a machine reading comprehension formulation to conduct event extraction in a low-resource regime. Text2Event (Lu et al., 2021), a sequence-to-structure generation paradigm, first presents events in a linearized format, and then trains a generative model to generate the linearized event sequence. Text2Event's unnatural output format hinders the model from fully leveraging pre-trained knowledge. Hence, their model falls short on the cases with only extremely low data being available (as shown in Section 3).\"},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 3532,\n",
       "   'end': 3574,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': '(OntoNotes V4.0, OntoNotes V5.0, and MSRA)'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 951,\n",
       "   'end': 971,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iman',\n",
       "   'text': ' (Li et al., 2020a;,'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 2130,\n",
       "   'end': 2253,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': ' However, too immersed regularity leads to unfavorable boundary detection of entities and disturbing character composition.'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 3740,\n",
       "   'end': 3793,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'we experiment on a practical medical dataset (CBLUE) '},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 171,\n",
       "   'end': 173,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'AL'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 3422,\n",
       "   'end': 3425,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'ASM'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 14,\n",
       "   'end': 174,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'Deep learning, to a large extent, has freed data scientists from doing feature engineering, which has been one of the essential obstacles to annotation with AL.'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 2873,\n",
       "   'end': 3303,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iman',\n",
       "   'text': 'Recently proposed alternatives to uncertaintybased query strategies leverage reinforcement learning and imitation learning (Fang et al., 2017;Liu et al., 2018;Vu et al., 2019;Brantley et al., 2020). This series of works aims at constructing trainable policy-based query strategies. However, this requires an excessive amount of computation while the transferability of learned policies across domains and tasks is underresearched.'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 762,\n",
       "   'end': 1331,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iman',\n",
       "   'text': 'Templates in Data-Driven D2T Generation Using simple handcrafted templates for individual keys or predicates is an efficient way of introducing domain knowledge while preventing text-to-text models from overfitting to a specific data format (Heidari et al., 2021;Kale and Rastogi, 2020a;. Transforming individual triples to text is also used in Laha et al. (2020) whose work is the most similar to ours. They also build a three-step pipeline for zero-shot D2T generation, but they use handcrafted rules for producing the output text and do not address content planning.'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 2087,\n",
       "   'end': 2660,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iman',\n",
       "   'text': '\\n\\nSentence Ordering Sentence ordering is the task of organizing a set of natural language sentences to increase the coherence of a text (Barzilay et al., 2001;Lapata, 2003). Several neural methods for this task were proposed, using either interactions between pairs of sentences Li and Jurafsky, 2017), global interactions (Gong et al., 2016;Wang and Wan, 2019), or combination of both (Cui et al., 2020). We base our ordering module ( §5.1) on the recent work of Calizzano et al. (2021), who use a pointer network (Wang and Wan, 2019;Vinyals et al., 2015) on top of a PLM.'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 14,\n",
       "   'end': 760,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iman',\n",
       "   'text': 'D2T Generation with PLMs Large neural language models pretrained on self-supervised tasks (Lewis et al., 2020;Liu et al., 2019;Devlin et al., 2019) have recently gained a lot of traction in D2T generation research (Ferreira et al., 2020). Following Chen et al. (2020c), other works adopt PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a). Kale and Rastogi (2020b) and Ribeiro et al. (2020) showed that PLMs using linearized representations of data can outperform graph neural networks on graph-to-text datasets, recently surpassed again by graph-based models (Ke et al., 2021;Chen et al., 2020a). Although the models make use of general-domain pretraining tasks, all of them are eventually finetuned on domain-specific data.'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 14,\n",
       "   'end': 760,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Iman',\n",
       "   'text': 'D2T Generation with PLMs Large neural language models pretrained on self-supervised tasks (Lewis et al., 2020;Liu et al., 2019;Devlin et al., 2019) have recently gained a lot of traction in D2T generation research (Ferreira et al., 2020). Following Chen et al. (2020c), other works adopt PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a). Kale and Rastogi (2020b) and Ribeiro et al. (2020) showed that PLMs using linearized representations of data can outperform graph neural networks on graph-to-text datasets, recently surpassed again by graph-based models (Ke et al., 2021;Chen et al., 2020a). Although the models make use of general-domain pretraining tasks, all of them are eventually finetuned on domain-specific data.'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 1003,\n",
       "   'end': 1049,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iman',\n",
       "   'text': '(Heidari et al., 2021;Kale and Rastogi, 2020a;'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 1208,\n",
       "   'end': 1232,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iman',\n",
       "   'text': 'zero-shot D2T generation'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 4026,\n",
       "   'end': 4047,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iman',\n",
       "   'text': '(Botha et al., 2018;.'}],\n",
       " 'Iraa': [{'file': 'paper_100.txt',\n",
       "   'start': 1900,\n",
       "   'end': 2087,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'There are also task-specific BERT models, especially for the task of grammatical error correction since an important type of error is caused by characters pronounced with the same pinyin.'},\n",
       "  {'file': 'paper_100.txt',\n",
       "   'start': 968,\n",
       "   'end': 1179,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Our work differs from existing works in that we are the first one to exploit GPT and verify the pros and cons of GPT in different situations. In addition, there are some works handling\\npinyin with typing errors.'},\n",
       "  {'file': 'paper_100.txt',\n",
       "   'start': 1684,\n",
       "   'end': 2088,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Our methodology also relates to pretrained models that use pinyin information. Sun et al. (2021) propose a general-purpose Chinese BERT with\\nnew embedding layers to inject pinyin and glyph information of characters. There are also task-specific BERT models, especially for the task of grammatical error correction since an important type of error is caused by characters pronounced with the same pinyin. '},\n",
       "  {'file': 'paper_11.txt',\n",
       "   'start': 720,\n",
       "   'end': 962,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'While this two-stage process is capable of providing accurate predictions, an alternative end-to-end approach that can tackle the problem purely cross-lingually, i.e., without involving MT, would clearly be more efficient and cost-effective. '},\n",
       "  {'file': 'paper_11.txt',\n",
       "   'start': 1025,\n",
       "   'end': 1042,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'multilingual BERT'},\n",
       "  {'file': 'paper_11.txt',\n",
       "   'start': 1961,\n",
       "   'end': 2746,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Can a highperformance CLIR model be trained that can operate without having to rely on MT? To answer the question, instead of viewing the MT-based approach as a competing one, we propose to leverage its strength via knowledge distillation (KD) into an end-to-end CLIR model. KD (Hinton et al., 2014) is a powerful supervision technique typically used to distill the knowledge of a large teacher model about some task into a smaller student model (Mukherjee and Awadallah, 2020;Turc et al., 2020). Here we propose to use it in a slightly different context, where the teacher and the student retriever are identical in size, but the former has superior performance simply due to utilizing MT output and consequently operating in a high-resource and lowdifficulty monolingual environment.'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 3365,\n",
       "   'end': 3483,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'In contrast to most prior work, we also show that these results can also be achieved for languages other than English.'},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 1713,\n",
       "   'end': 1881,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'In contrast, the models typically applied in the entailment approach are Cross Attention (CA) models which need to be executed for every combination of text and label. '},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 14,\n",
       "   'end': 106,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Few-shot learning is the problem of learning classifiers with only a few training examples. '},\n",
       "  {'file': 'paper_13.txt',\n",
       "   'start': 446,\n",
       "   'end': 932,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'In recent years, there has been a surge in zeroshot and few-shot approaches to text classification. One approach (Yin et al., 2019, 2020; Halder et al., 2020;Wang et al., 2021) makes use of entailment models. Textual entailment (Dagan et al., 2006), also known as natural language inference (NLI) (Bowman et al., 2015), is the problem of predicting whether a textual premise implies a textual hypothesis in a logical sense. For example, Emma loves apples implies that Emma likes apples.'},\n",
       "  {'file': 'paper_14.txt',\n",
       "   'start': 14,\n",
       "   'end': 182,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'At present, for a large majority of natural language processing tasks, the most successful approach is fine-tuning pre-trained models with task-specific labelled data. '},\n",
       "  {'file': 'paper_14.txt',\n",
       "   'start': 182,\n",
       "   'end': 311,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Unfortunately, for many languages, and especially low-resource languages, such taskspecific labelled data is often not available.'},\n",
       "  {'file': 'paper_14.txt',\n",
       "   'start': 1993,\n",
       "   'end': 2185,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'The language selection does, however, obfuscate the fact that for most non-Indo-European and low-resource languages no data is available for semantically rich tasks such as question answering.'},\n",
       "  {'file': 'paper_14.txt',\n",
       "   'start': 2494,\n",
       "   'end': 2714,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'We evaluate this via partof-speech (POS) tagging data, as this is the only task for which high-quality data is available in a large number of languages, including low-resource languages from different language families. '},\n",
       "  {'file': 'paper_14.txt',\n",
       "   'start': 2714,\n",
       "   'end': 2981,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Also, high cross-lingual POS tagging performance may be seen as a precondition for more semantically complex tasks, as a base understanding of syntactic structure in both the source and target language is necessary for any meaningful natural language processing task.'},\n",
       "  {'file': 'paper_15.txt',\n",
       "   'start': 649,\n",
       "   'end': 1046,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Researchers also realize that the vision modality maybe redundant. Irrelevant images have little impact on the translation quality, and no significant BLEU drop is observed even the image is absent (Elliott, 2018). Encouraging results appeared in  2021) proposed a cross-lingual visual pretraining approach. In this work, we make a systematic study on whether stronger vision features are helpful.'},\n",
       "  {'file': 'paper_15.txt',\n",
       "   'start': 897,\n",
       "   'end': 902,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iraa',\n",
       "   'text': '2021)'},\n",
       "  {'file': 'paper_16.txt',\n",
       "   'start': 14,\n",
       "   'end': 415,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'To facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al., 2018). Datasets for long docu-ments include Sharma et al. (2019), Cohan et al. (2018), andFisas et al. (2016).'},\n",
       "  {'file': 'paper_16.txt',\n",
       "   'start': 713,\n",
       "   'end': 1265,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Researchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AM-PERE  for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs. Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation. There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.\\n'},\n",
       "  {'file': 'paper_16.txt',\n",
       "   'start': 1883,\n",
       "   'end': 1926,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iraa',\n",
       "   'text': '(Reiter and Dale, 1997;Shao et al., 2019;, '},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1073,\n",
       "   'end': 1082,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': ' BART-Gen'},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1287,\n",
       "   'end': 1397,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'All these fully supervised methods can achieve substantial performance with a large amount of annotated data. '},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1521,\n",
       "   'end': 1527,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'DEGREE'},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 14,\n",
       "   'end': 893,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Fully supervised event extraction. Event extraction has been studied for over a decade (Ahn, 2006;Ji and Grishman, 2008) and most traditional event extraction works follow the fully supervised setting (Nguyen et al., 2016;Sha et al., 2018;Nguyen and Nguyen, 2019;Yang et al., 2019;Lin et al., 2020;Li et al., 2020). Many of them use classification-based models and use pipeline-style frameworks to extract events (Nguyen et al., 2016;Yang et al., 2019;Wadden et al., 2019). To better leverage shared knowledge in event triggers and arguments, some works propose to incorporate global features to jointly decide triggers and arguments (Lin et al., 2020;Li et al., 2013;Yang and Mitchell, 2016). Recently, few generation-based event extraction models have been proposed. TANL (Paolini et al., 2021) treats event extraction as translation tasks between augmented natural languages. '},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1650,\n",
       "   'end': 1726,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'It has been a rising interest in event extraction under less data scenario. '},\n",
       "  {'file': 'paper_17.txt',\n",
       "   'start': 1726,\n",
       "   'end': 2146,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Iraa',\n",
       "   'text': \"Liu et al. (2020) uses a machine reading comprehension formulation to conduct event extraction in a low-resource regime. Text2Event (Lu et al., 2021), a sequence-to-structure generation paradigm, first presents events in a linearized format, and then trains a generative model to generate the linearized event sequence. Text2Event's unnatural output format hinders the model from fully leveraging pre-trained knowledge. \"},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 952,\n",
       "   'end': 972,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iraa',\n",
       "   'text': '(Li et al., 2020a;, '},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 3786,\n",
       "   'end': 3791,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'CBLUE'},\n",
       "  {'file': 'paper_18.txt',\n",
       "   'start': 3416,\n",
       "   'end': 3574,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'To verify the effectiveness of our method, we conduct extensive experiments on three large-scale benchmark datasets (OntoNotes V4.0, OntoNotes V5.0, and MSRA)'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 14,\n",
       "   'end': 173,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Deep learning, to a large extent, has freed data scientists from doing feature engineering, which has been one of the essential obstacles to annotation with AL'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 175,\n",
       "   'end': 286,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'This advantage has sparked a series of works on deep active learning (DAL) in natural language processing (NLP)'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 1464,\n",
       "   'end': 1655,\n",
       "   'label': 'Unsupported claim',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'We continue this line of works by relying on pre-trained Transformers since this architecture has been shown promising for AL in NLP due to its good qualitative and computational performance.'},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 1802,\n",
       "   'end': 1881,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Shelmanov et al. (2021 leverage Monte Carlo dropout (Gal and Ghahramani, 2016) '},\n",
       "  {'file': 'paper_19.txt',\n",
       "   'start': 2873,\n",
       "   'end': 3303,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Recently proposed alternatives to uncertaintybased query strategies leverage reinforcement learning and imitation learning (Fang et al., 2017;Liu et al., 2018;Vu et al., 2019;Brantley et al., 2020). This series of works aims at constructing trainable policy-based query strategies. However, this requires an excessive amount of computation while the transferability of learned policies across domains and tasks is underresearched.'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 253,\n",
       "   'end': 633,\n",
       "   'label': 'Coherence',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Following Chen et al. (2020c), other works adopt PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a). Kale and Rastogi (2020b) and Ribeiro et al. (2020) showed that PLMs using linearized representations of data can outperform graph neural networks on graph-to-text datasets, recently surpassed again by graph-based models (Ke et al., 2021;Chen et al., 2020a). '},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 1003,\n",
       "   'end': 1050,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iraa',\n",
       "   'text': '(Heidari et al., 2021;Kale and Rastogi, 2020a;.'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 2365,\n",
       "   'end': 2390,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iraa',\n",
       "   'text': ' Li and Jurafsky, 2017), '},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 2089,\n",
       "   'end': 2660,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Sentence Ordering Sentence ordering is the task of organizing a set of natural language sentences to increase the coherence of a text (Barzilay et al., 2001;Lapata, 2003). Several neural methods for this task were proposed, using either interactions between pairs of sentences Li and Jurafsky, 2017), global interactions (Gong et al., 2016;Wang and Wan, 2019), or combination of both (Cui et al., 2020). We base our ordering module ( §5.1) on the recent work of Calizzano et al. (2021), who use a pointer network (Wang and Wan, 2019;Vinyals et al., 2015) on top of a PLM.'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 762,\n",
       "   'end': 1331,\n",
       "   'label': 'Lacks synthesis',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Templates in Data-Driven D2T Generation Using simple handcrafted templates for individual keys or predicates is an efficient way of introducing domain knowledge while preventing text-to-text models from overfitting to a specific data format (Heidari et al., 2021;Kale and Rastogi, 2020a;. Transforming individual triples to text is also used in Laha et al. (2020) whose work is the most similar to ours. They also build a three-step pipeline for zero-shot D2T generation, but they use handcrafted rules for producing the output text and do not address content planning.'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 3514,\n",
       "   'end': 3534,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iraa',\n",
       "   'text': 'Jiang et al., 2020),'},\n",
       "  {'file': 'paper_20.txt',\n",
       "   'start': 4026,\n",
       "   'end': 4047,\n",
       "   'label': 'Format',\n",
       "   'user': 'Iraa',\n",
       "   'text': '(Botha et al., 2018;.'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_anns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb638d6c",
   "metadata": {},
   "source": [
    "Grouping based on category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeec3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "def group_by_paper_and_category(\n",
    "    annotator_data: Dict[str, List[Dict[str, Any]]]\n",
    "):\n",
    "    \"\"\"\n",
    "    annotator_data:\n",
    "      {\n",
    "        \"Ed\": [ {file, start, end, label, ...}, ... ],\n",
    "        \"Iman\": [...],\n",
    "        ...\n",
    "      }\n",
    "\n",
    "    Returns:\n",
    "      grouped[paper][category][annotator] -> list of annotation dicts\n",
    "    \"\"\"\n",
    "    grouped = defaultdict(\n",
    "        lambda: defaultdict(lambda: defaultdict(list))\n",
    "    )\n",
    "\n",
    "    for annotator, anns in annotator_data.items():\n",
    "        for ann in anns:\n",
    "            paper = ann[\"file\"]\n",
    "            category = ann[\"label\"]\n",
    "            grouped[paper][category][annotator].append(ann)\n",
    "\n",
    "    return grouped\n",
    "\n",
    "grouped = group_by_paper_and_category(total_anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d944cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../annotated_data/overlapping_papers'\n",
    "\n",
    "for paper, categories in grouped.items():\n",
    "    for category, ann_by_annotator in categories.items():\n",
    "        with open(os.path.join(path, f\"{paper.split('.txt')[0]}_{category}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(ann_by_annotator, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabdde22",
   "metadata": {},
   "source": [
    "### Pairwise F1 token overlap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35eff670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Any\n",
    "from itertools import permutations\n",
    "\n",
    "Span = Tuple[int, int]  # (start, end), end-exclusive\n",
    "\n",
    "\n",
    "def span_iou(a: Span, b: Span) -> float:\n",
    "    inter_start = max(a[0], b[0])\n",
    "    inter_end = min(a[1], b[1])\n",
    "    inter = max(0, inter_end - inter_start)\n",
    "    if inter == 0:\n",
    "        return 0.0\n",
    "    union = (a[1] - a[0]) + (b[1] - b[0]) - inter\n",
    "    return inter / union\n",
    "\n",
    "\n",
    "def match_spans_overlap(\n",
    "    gold: List[Span],\n",
    "    pred: List[Span],\n",
    "    iou_threshold: float = 0.5,\n",
    ") -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    One-directional matching:\n",
    "      gold = ground truth\n",
    "      pred = predictions\n",
    "\n",
    "    Returns (tp, fp, fn)\n",
    "    \"\"\"\n",
    "    used_gold = set()\n",
    "    tp = 0\n",
    "\n",
    "    for p in pred:\n",
    "        best_iou = 0.0\n",
    "        best_idx = None\n",
    "        for i, g in enumerate(gold):\n",
    "            if i in used_gold:\n",
    "                continue\n",
    "            iou = span_iou(g, p)\n",
    "            if iou >= iou_threshold and iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_idx = i\n",
    "\n",
    "        if best_idx is not None:\n",
    "            tp += 1\n",
    "            used_gold.add(best_idx)\n",
    "\n",
    "    fp = len(pred) - tp\n",
    "    fn = len(gold) - tp\n",
    "    return tp, fp, fn\n",
    "\n",
    "def pairwise_micro_f1_across_annotators(\n",
    "    annotator_spans: Dict[str, Dict[str, List[Span]]],\n",
    "    *,\n",
    "    iou_threshold: float = 0.5,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    annotator_spans:\n",
    "      annotator -> { doc_id -> [spans] }\n",
    "\n",
    "    Returns:\n",
    "      micro-averaged precision/recall/F1 across all ordered annotator pairs\n",
    "    \"\"\"\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "    pair_details = {}\n",
    "\n",
    "    annotators = list(annotator_spans.keys())\n",
    "\n",
    "    for gold_ann, pred_ann in permutations(annotators, 2):\n",
    "        tp = fp = fn = 0\n",
    "\n",
    "        gold_docs = annotator_spans[gold_ann]\n",
    "        pred_docs = annotator_spans[pred_ann]\n",
    "        doc_ids = set(gold_docs.keys()) | set(pred_docs.keys())\n",
    "\n",
    "        for d in doc_ids:\n",
    "            gold_spans = gold_docs.get(d, [])\n",
    "            pred_spans = pred_docs.get(d, [])\n",
    "            t, f_p, f_n = match_spans_overlap(\n",
    "                gold_spans,\n",
    "                pred_spans,\n",
    "                iou_threshold=iou_threshold,\n",
    "            )\n",
    "            tp += t\n",
    "            fp += f_p\n",
    "            fn += f_n\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "        f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n",
    "\n",
    "        pair_details[(gold_ann, pred_ann)] = {\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "\n",
    "    micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) else 0.0\n",
    "    micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) else 0.0\n",
    "    micro_f1 = (\n",
    "        2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
    "        if (micro_precision + micro_recall)\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"micro_precision\": micro_precision,\n",
    "        \"micro_recall\": micro_recall,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"total_tp\": total_tp,\n",
    "        \"total_fp\": total_fp,\n",
    "        \"total_fn\": total_fn,\n",
    "        \"pairwise_details\": pair_details,\n",
    "        \"num_ordered_pairs\": len(pair_details),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f8175d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Unsupported claim\n",
      "0.3333\n",
      "Category: Lacks synthesis\n",
      "0.4872\n",
      "Category: Format\n",
      "0.6897\n",
      "Category: Coherence\n",
      "0.3678\n"
     ]
    }
   ],
   "source": [
    "categories = ['Unsupported claim', 'Lacks synthesis', 'Format', 'Coherence']\n",
    "pairwise_f1_total = {'Unsupported claim': 0, 'Lacks synthesis': 0, 'Format': 0, 'Coherence': 0}\n",
    "\n",
    "def process_ann_files(ann_data, label):\n",
    "    spans_by_file = {}\n",
    "    for d in ann_data:\n",
    "        if d[\"label\"] == label:\n",
    "            fn = d[\"file\"]\n",
    "            s = int(d[\"start\"])\n",
    "            e = int(d[\"end\"])\n",
    "            \n",
    "            if fn not in spans_by_file:\n",
    "                spans_by_file[fn] = []\n",
    "            spans_by_file[fn].append((s, e))\n",
    "\n",
    "    return spans_by_file\n",
    "\n",
    "for category in categories:\n",
    "    ann1_spans = process_ann_files(total_anns['Iman'], category)\n",
    "    ann2_spans = process_ann_files(total_anns['Ekaterina'], category)\n",
    "    ann3_spans = process_ann_files(total_anns['Iraa'], category)\n",
    "    ann4_spans = process_ann_files(total_anns['Ed'], category)\n",
    "\n",
    "    ann_spans = {\n",
    "        \"ann1\": ann1_spans,\n",
    "        \"ann2\": ann2_spans,\n",
    "        \"ann3\": ann3_spans,\n",
    "        \"ann4\": ann4_spans,\n",
    "    }\n",
    "\n",
    "    pres = pairwise_micro_f1_across_annotators(\n",
    "    ann_spans,\n",
    "    iou_threshold=0.5,\n",
    ")\n",
    "    pairwise_f1_total[category] = pres\n",
    "\n",
    "for category in pairwise_f1_total:\n",
    "    print(f\"Category: {category}\")\n",
    "    pairwise_results = pairwise_f1_total[category]\n",
    "    print(round(pairwise_results[\"micro_f1\"],4))\n",
    "    # for (a, b), stats in pairwise_results.items():\n",
    "    #     print(a, b, round(stats[\"f1\"], 4))\n",
    "    # print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb6ade",
   "metadata": {},
   "source": [
    "### Krippendorf's alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec447e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "_WORD_RE = re.compile(r\"\\S+\")\n",
    "\n",
    "def tokenize_with_offsets(text):\n",
    "    return [(m.start(), m.end()) for m in _WORD_RE.finditer(text)]\n",
    "\n",
    "def spans_to_token_labels(text, spans, inclusive_end=False):\n",
    "    \"\"\"\n",
    "    Returns a list of 0/1 labels per token.\n",
    "    1 = token overlaps any span\n",
    "    \"\"\"\n",
    "    tokens = tokenize_with_offsets(text)\n",
    "    labels = [0] * len(tokens)\n",
    "\n",
    "    for s, e in spans:\n",
    "        if inclusive_end:\n",
    "            e += 1\n",
    "        for i, (ts, te) in enumerate(tokens):\n",
    "            if not (te <= s or ts >= e):\n",
    "                labels[i] = 1\n",
    "    return labels\n",
    "\n",
    "def build_annotation_matrix(texts, annotator_spans, text_key=\"text\", file_key=\"filename\", inclusive_end=False):\n",
    "    \"\"\"\n",
    "    texts: list of dicts with at least {file_key: filename, text_key: text}\n",
    "    annotator_spans: dict[ann_id][filename] -> list of (start,end) spans\n",
    "    \"\"\"\n",
    "    annotators = list(annotator_spans.keys())\n",
    "    matrix = []\n",
    "\n",
    "    for d in texts:\n",
    "        fn = d[file_key]\n",
    "        text = d[text_key]          # <-- string\n",
    "        tokens = tokenize_with_offsets(text)\n",
    "\n",
    "        # precompute per-annotator token labels (or None if missing)\n",
    "        per_ann_labels = {}\n",
    "        for ann in annotators:\n",
    "            spans = annotator_spans[ann].get(fn)  # filename key\n",
    "            if spans is None:\n",
    "                per_ann_labels[ann] = None\n",
    "            else:\n",
    "                per_ann_labels[ann] = spans_to_token_labels(text, spans, inclusive_end=inclusive_end)\n",
    "\n",
    "        # one row per token\n",
    "        for t in range(len(tokens)):\n",
    "            row = []\n",
    "            for ann in annotators:\n",
    "                lbls = per_ann_labels[ann]\n",
    "                row.append(None if lbls is None else lbls[t])\n",
    "            matrix.append(row)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def krippendorffs_alpha_nominal(annotation_matrix):\n",
    "    \"\"\"\n",
    "    annotation_matrix: list of lists\n",
    "    Each row = unit, each column = annotator\n",
    "    Labels are categorical (0/1), None allowed\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove units with fewer than 2 annotations\n",
    "    units = [\n",
    "        [v for v in row if v is not None]\n",
    "        for row in annotation_matrix\n",
    "        if sum(v is not None for v in row) >= 2\n",
    "    ]\n",
    "\n",
    "    if not units:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    # Observed disagreement\n",
    "    Do = 0.0\n",
    "    n_pairs = 0\n",
    "\n",
    "    for u in units:\n",
    "        for i in range(len(u)):\n",
    "            for j in range(i + 1, len(u)):\n",
    "                n_pairs += 1\n",
    "                Do += 0 if u[i] == u[j] else 1\n",
    "\n",
    "    Do /= n_pairs\n",
    "\n",
    "    # Expected disagreement\n",
    "    all_labels = [v for u in units for v in u]\n",
    "    label_counts = Counter(all_labels)\n",
    "    N = sum(label_counts.values())\n",
    "\n",
    "    De = 0.0\n",
    "    for l1, c1 in label_counts.items():\n",
    "        for l2, c2 in label_counts.items():\n",
    "            if l1 != l2:\n",
    "                De += c1 * c2\n",
    "\n",
    "    De /= (N * (N - 1))\n",
    "\n",
    "    if De == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return 1 - (Do / De)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf00bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "texts = []\n",
    "\n",
    "path = \"../../to_annotate\"\n",
    "for text in os.listdir(path):\n",
    "    if 'paper' in text and text.endswith(\".txt\"):\n",
    "        with open(os.path.join(path, text), \"r\", encoding=\"utf-8\") as f:\n",
    "            texts.append({\"filename\": text, 'text': (f.read()).split(\"References\")[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "475cd221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ann_files(ann_data, label):\n",
    "    spans_by_file = {}\n",
    "    for d in ann_data:\n",
    "        if d[\"label\"] == label:\n",
    "            fn = d[\"file\"]\n",
    "            s = int(d[\"start\"])\n",
    "            e = int(d[\"end\"])\n",
    "            \n",
    "            if fn not in spans_by_file:\n",
    "                spans_by_file[fn] = []\n",
    "            spans_by_file[fn].append((s, e))\n",
    "\n",
    "    return spans_by_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df39596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's α for each category:\n",
      "Unsupported claim: 0.3551\n",
      "Lacks synthesis: 0.4375\n",
      "Format: 0.8347\n",
      "Coherence: 0.305\n"
     ]
    }
   ],
   "source": [
    "categories = ['Unsupported claim', 'Lacks synthesis', 'Format', 'Coherence']\n",
    "kripp_total = {'Unsupported claim': 0, 'Lacks synthesis': 0, 'Format': 0, 'Coherence': 0}\n",
    "\n",
    "for category in categories:\n",
    "    ann_spans_unsupp = {\n",
    "        \"ann1\": process_ann_files(total_anns['Iman'], category),\n",
    "        \"ann2\": process_ann_files(total_anns['Ekaterina'], category),\n",
    "        \"ann3\": process_ann_files(total_anns['Iraa'], category),\n",
    "        \"ann4\": process_ann_files(total_anns['Ed'], category),\n",
    "    }\n",
    "\n",
    "    matrix = build_annotation_matrix(texts, ann_spans_unsupp)\n",
    "    alpha = krippendorffs_alpha_nominal(matrix)\n",
    "    kripp_total[category] = alpha\n",
    "\n",
    "print(\"Krippendorff's α for each category:\")\n",
    "for category, alpha in kripp_total.items():\n",
    "    print(f\"{category}: {round(alpha, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5188bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "Span = Tuple[int, int]  # (start, end) end-exclusive by default\n",
    "_WORD_RE = re.compile(r\"\\S+\")\n",
    "\n",
    "\n",
    "def tokenize_with_offsets(text: str) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Whitespace tokens with (start,end_exclusive) offsets.\"\"\"\n",
    "    return [(m.start(), m.end()) for m in _WORD_RE.finditer(text)]\n",
    "\n",
    "\n",
    "def spans_to_token_labels(\n",
    "    text: str,\n",
    "    spans: List[Span],\n",
    "    inclusive_end: bool = False,\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Convert spans -> token-level binary labels.\n",
    "    Label = 1 if token overlaps any span, else 0.\n",
    "    \"\"\"\n",
    "    toks = tokenize_with_offsets(text)\n",
    "    labels = [0] * len(toks)\n",
    "\n",
    "    for s, e in spans:\n",
    "        if inclusive_end:\n",
    "            e = e + 1\n",
    "        for i, (ts, te) in enumerate(toks):\n",
    "            if not (te <= s or ts >= e):  # overlap\n",
    "                labels[i] = 1\n",
    "    return labels\n",
    "\n",
    "\n",
    "def fleiss_kappa(counts: List[List[int]]) -> float:\n",
    "    \"\"\"\n",
    "    Fleiss' kappa from a matrix of counts per unit:\n",
    "      counts[u] = [n_cat0, n_cat1, ..., n_catK-1]\n",
    "    Requires constant number of raters per unit.\n",
    "\n",
    "    Returns kappa in [-1, 1].\n",
    "    \"\"\"\n",
    "    if not counts:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    N = len(counts)          # units\n",
    "    k = len(counts[0])       # categories\n",
    "    n = sum(counts[0])       # raters per unit\n",
    "\n",
    "    # Validate constant n and shape\n",
    "    for row in counts:\n",
    "        if len(row) != k:\n",
    "            raise ValueError(\"All rows must have same number of categories\")\n",
    "        if sum(row) != n:\n",
    "            raise ValueError(\"Fleiss requires same number of ratings per unit\")\n",
    "\n",
    "    # p_j = proportion of all assignments to category j\n",
    "    total = N * n\n",
    "    p = [0.0] * k\n",
    "    for row in counts:\n",
    "        for j in range(k):\n",
    "            p[j] += row[j]\n",
    "    p = [pj / total for pj in p]\n",
    "\n",
    "    # P_i = extent of agreement for unit i\n",
    "    P = []\n",
    "    for row in counts:\n",
    "        s = sum(c * (c - 1) for c in row)\n",
    "        P.append(s / (n * (n - 1)))\n",
    "\n",
    "    P_bar = sum(P) / N\n",
    "    P_e = sum(pj * pj for pj in p)\n",
    "\n",
    "    if P_e == 1.0:\n",
    "        return 1.0\n",
    "\n",
    "    return (P_bar - P_e) / (1.0 - P_e)\n",
    "\n",
    "\n",
    "def build_fleiss_counts_from_spans(\n",
    "    texts: List[Dict[str, Any]],\n",
    "    annotator_spans: Dict[str, Dict[str, List[Span]]],\n",
    "    *,\n",
    "    file_key: str = \"file\",\n",
    "    text_key: str = \"text\",\n",
    "    inclusive_end: bool = False,\n",
    "    require_all: bool = True,\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Build Fleiss count matrix from span annotations.\n",
    "\n",
    "    texts: list of dicts with {file_key, text_key}\n",
    "    annotator_spans: ann_id -> { filename -> [(s,e), ...] }\n",
    "\n",
    "    Returns counts per token-unit: [n_out, n_in] for each token.\n",
    "\n",
    "    If require_all=True: only include docs where ALL annotators have spans (possibly empty list).\n",
    "      Missing doc for any annotator => doc skipped.\n",
    "    If require_all=False: tokens with missing labels are skipped (not strict Fleiss).\n",
    "    \"\"\"\n",
    "    annotators = list(annotator_spans.keys())\n",
    "    counts = []\n",
    "\n",
    "    for d in texts:\n",
    "        fn = d[file_key]\n",
    "        text = d[text_key]\n",
    "\n",
    "        # Check doc availability\n",
    "        if require_all:\n",
    "            if any(fn not in annotator_spans[ann] for ann in annotators):\n",
    "                continue\n",
    "\n",
    "        # Build token labels per annotator (or None if missing)\n",
    "        per_ann_labels: Dict[str, Optional[List[int]]] = {}\n",
    "        for ann in annotators:\n",
    "            spans = annotator_spans[ann].get(fn)\n",
    "            if spans is None:\n",
    "                per_ann_labels[ann] = None\n",
    "            else:\n",
    "                per_ann_labels[ann] = spans_to_token_labels(\n",
    "                    text, spans, inclusive_end=inclusive_end\n",
    "                )\n",
    "\n",
    "        # number of tokens (use any available annotator or tokenize directly)\n",
    "        tok_offsets = tokenize_with_offsets(text)\n",
    "        T = len(tok_offsets)\n",
    "\n",
    "        for t in range(T):\n",
    "            labels_t = []\n",
    "            for ann in annotators:\n",
    "                lbls = per_ann_labels[ann]\n",
    "                if lbls is None:\n",
    "                    labels_t.append(None)\n",
    "                else:\n",
    "                    labels_t.append(lbls[t])\n",
    "\n",
    "            if require_all:\n",
    "                # all 4 are present by construction\n",
    "                n_in = sum(labels_t)  # labels are 0/1\n",
    "                n_out = len(labels_t) - n_in\n",
    "                counts.append([n_out, n_in])\n",
    "            else:\n",
    "                # drop this token if any missing label\n",
    "                if any(v is None for v in labels_t):\n",
    "                    continue\n",
    "                n_in = sum(labels_t)\n",
    "                n_out = len(labels_t) - n_in\n",
    "                counts.append([n_out, n_in])\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "def fleiss_kappa_from_spans(\n",
    "    texts: List[Dict[str, Any]],\n",
    "    annotator_spans: Dict[str, Dict[str, List[Span]]],\n",
    "    *,\n",
    "    file_key: str = \"filename\",\n",
    "    text_key: str = \"text\",\n",
    "    inclusive_end: bool = False,\n",
    "    require_all: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: builds counts and computes Fleiss' kappa.\n",
    "    \"\"\"\n",
    "    counts = build_fleiss_counts_from_spans(\n",
    "        texts,\n",
    "        annotator_spans,\n",
    "        file_key=file_key,\n",
    "        text_key=text_key,\n",
    "        inclusive_end=inclusive_end,\n",
    "        require_all=require_all,\n",
    "    )\n",
    "    kappa = fleiss_kappa(counts)\n",
    "    return {\n",
    "        \"fleiss_kappa\": kappa,\n",
    "        \"num_token_units\": len(counts),\n",
    "        \"require_all\": require_all,\n",
    "        \"num_annotators\": len(annotator_spans),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3196b44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Unsupported claim: \n",
      "Fleiss' kappa: 0.4091\n",
      "Number of token units: 3492\n",
      "\n",
      "\n",
      "Category: Lacks synthesis: \n",
      "Fleiss' kappa: 0.2222\n",
      "Number of token units: 998\n",
      "\n",
      "\n",
      "Category: Format: \n",
      "Fleiss' kappa: 0.8561\n",
      "Number of token units: 1432\n",
      "\n",
      "\n",
      "Category: Coherence: \n",
      "Fleiss' kappa: 0.3154\n",
      "Number of token units: 1385\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_fleiss_kappa = {}\n",
    "\n",
    "for category in categories:\n",
    "    ann_spans_unsupp = {\n",
    "        \"ann1\": process_ann_files(total_anns['Iman'], category),\n",
    "        \"ann2\": process_ann_files(total_anns['Ekaterina'], category),\n",
    "        \"ann3\": process_ann_files(total_anns['Iraa'], category),\n",
    "        \"ann4\": process_ann_files(total_anns['Ed'], category),\n",
    "    }\n",
    "    res = fleiss_kappa_from_spans(\n",
    "        texts,\n",
    "        ann_spans_unsupp,\n",
    "        require_all=True,      # strict Fleiss\n",
    "        inclusive_end=True,   # set True if your end indices are inclusive\n",
    "    )\n",
    "\n",
    "    total_fleiss_kappa[category] = res\n",
    "\n",
    "for category in total_fleiss_kappa:\n",
    "    print(f\"Category: {category}: \")\n",
    "    print(f\"Fleiss' kappa: {round(total_fleiss_kappa[category]['fleiss_kappa'], 4)}\")\n",
    "    print(f\"Number of token units: {total_fleiss_kappa[category]['num_token_units']}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e235a97b",
   "metadata": {},
   "source": [
    "### Cohens Kappa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92cca4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "Span = Tuple[int, int]\n",
    "_WORD_RE = re.compile(r\"\\S+\")\n",
    "\n",
    "def tokenize_with_offsets(text: str):\n",
    "    return [(m.start(), m.end()) for m in _WORD_RE.finditer(text)]\n",
    "\n",
    "def spans_to_token_labels(text: str, spans: List[Span], inclusive_end: bool = False) -> List[int]:\n",
    "    tokens = tokenize_with_offsets(text)\n",
    "    labels = [0] * len(tokens)\n",
    "    for s, e in spans:\n",
    "        if inclusive_end:\n",
    "            e += 1\n",
    "        for i, (ts, te) in enumerate(tokens):\n",
    "            if not (te <= s or ts >= e):\n",
    "                labels[i] = 1\n",
    "    return labels\n",
    "\n",
    "def group_spans_by_file_for_label(\n",
    "    ann_list: List[Dict[str, Any]],\n",
    "    label: str,\n",
    ") -> Dict[str, List[Span]]:\n",
    "    out = defaultdict(list)\n",
    "    for d in ann_list:\n",
    "        if d.get(\"label\") != label:\n",
    "            continue\n",
    "        s, e = int(d[\"start\"]), int(d[\"end\"])\n",
    "        if s >= 0 and e > s:\n",
    "            out[d[\"file\"]].append((s, e))\n",
    "    return dict(out)\n",
    "\n",
    "def mean_pairwise_cohens_kappa_by_category(\n",
    "    texts: List[Dict[str, Any]],                 # list of {\"filename\",\"text\"}\n",
    "    total_anns: Dict[str, List[Dict[str, Any]]], # {\"Ed\":[...], ...}\n",
    "    categories: List[str],\n",
    "    *,\n",
    "    inclusive_end: bool = False,\n",
    "    filename_key: str = \"filename\",\n",
    "    text_key: str = \"text\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      {\n",
    "        category: {\n",
    "          \"pairwise_kappa\": {(annA,annB): kappa, ...},\n",
    "          \"mean_kappa\": float,\n",
    "          \"token_counts\": {(annA,annB): N_tokens, ...},\n",
    "          \"num_pairs\": int\n",
    "        },\n",
    "        ...\n",
    "      }\n",
    "    \"\"\"\n",
    "    annotators = list(total_anns.keys())\n",
    "    results = {}\n",
    "\n",
    "    for category in categories:\n",
    "        # spans_by_ann[ann][filename] -> spans for this category\n",
    "        spans_by_ann = {\n",
    "            ann: group_spans_by_file_for_label(anns, category)\n",
    "            for ann, anns in total_anns.items()\n",
    "        }\n",
    "\n",
    "        pairwise_kappa = {}\n",
    "        token_counts = {}\n",
    "\n",
    "        for a, b in combinations(annotators, 2):\n",
    "            y_a, y_b = [], []\n",
    "\n",
    "            for rec in texts:\n",
    "                fn = rec[filename_key]\n",
    "                text = rec[text_key]\n",
    "\n",
    "                if fn not in spans_by_ann[a] or fn not in spans_by_ann[b]:\n",
    "                    continue  # only docs both annotated (for this category)\n",
    "\n",
    "                labels_a = spans_to_token_labels(text, spans_by_ann[a][fn], inclusive_end=inclusive_end)\n",
    "                labels_b = spans_to_token_labels(text, spans_by_ann[b][fn], inclusive_end=inclusive_end)\n",
    "\n",
    "                y_a.extend(labels_a)\n",
    "                y_b.extend(labels_b)\n",
    "\n",
    "            kappa = float(\"nan\") if len(y_a) == 0 else cohen_kappa_score(y_a, y_b)\n",
    "            pairwise_kappa[(a, b)] = kappa\n",
    "            token_counts[(a, b)] = len(y_a)\n",
    "\n",
    "        valid = [k for k in pairwise_kappa.values() if k == k]  # drop NaNs\n",
    "        mean_kappa = sum(valid) / len(valid) if valid else float(\"nan\")\n",
    "\n",
    "        results[category] = {\n",
    "            \"pairwise_kappa\": pairwise_kappa,\n",
    "            \"mean_kappa\": mean_kappa,\n",
    "            \"token_counts\": token_counts,\n",
    "            \"num_pairs\": len(pairwise_kappa),\n",
    "        }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dc2c919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported claim mean κ = 0.4042\n",
      "  ('Ed', 'Ekaterina') 0.3129 tokens: 3668\n",
      "  ('Ed', 'Iman') 0.3825 tokens: 4292\n",
      "  ('Ed', 'Iraa') 0.4832 tokens: 3879\n",
      "  ('Ekaterina', 'Iman') 0.3951 tokens: 3995\n",
      "  ('Ekaterina', 'Iraa') 0.3281 tokens: 3492\n",
      "  ('Iman', 'Iraa') 0.5235 tokens: 3492\n",
      "Format mean κ = 0.8345\n",
      "  ('Ed', 'Ekaterina') 0.8552 tokens: 2044\n",
      "  ('Ed', 'Iman') 0.8875 tokens: 1432\n",
      "  ('Ed', 'Iraa') 0.7971 tokens: 2044\n",
      "  ('Ekaterina', 'Iman') 0.7973 tokens: 1432\n",
      "  ('Ekaterina', 'Iraa') 0.8206 tokens: 2044\n",
      "  ('Iman', 'Iraa') 0.8496 tokens: 1759\n",
      "Coherence mean κ = 0.3254\n",
      "  ('Ed', 'Ekaterina') -0.0391 tokens: 1712\n",
      "  ('Ed', 'Iman') 0.6962 tokens: 1385\n",
      "  ('Ed', 'Iraa') 0.441 tokens: 1712\n",
      "  ('Ekaterina', 'Iman') 0.1045 tokens: 1561\n",
      "  ('Ekaterina', 'Iraa') 0.183 tokens: 1888\n",
      "  ('Iman', 'Iraa') 0.5669 tokens: 1561\n",
      "Lacks synthesis mean κ = 0.4305\n",
      "  ('Ed', 'Ekaterina') 0.271 tokens: 998\n",
      "  ('Ed', 'Iman') 0.488 tokens: 1979\n",
      "  ('Ed', 'Iraa') 0.5421 tokens: 1979\n",
      "  ('Ekaterina', 'Iman') 0.6693 tokens: 1610\n",
      "  ('Ekaterina', 'Iraa') -0.0131 tokens: 1610\n",
      "  ('Iman', 'Iraa') 0.6257 tokens: 2591\n"
     ]
    }
   ],
   "source": [
    "categories = [\"Unsupported claim\", \"Format\", \"Coherence\", \"Lacks synthesis\"]\n",
    "\n",
    "res_by_cat = mean_pairwise_cohens_kappa_by_category(\n",
    "    texts,\n",
    "    total_anns,\n",
    "    categories=categories,\n",
    "    inclusive_end=False,   # set True if your end offsets are inclusive\n",
    ")\n",
    "\n",
    "for cat in categories:\n",
    "    print(cat, \"mean κ =\", round(res_by_cat[cat][\"mean_kappa\"], 4))\n",
    "    for pair, k in res_by_cat[cat][\"pairwise_kappa\"].items():\n",
    "        print(\" \", pair, round(k, 4), \"tokens:\", res_by_cat[cat][\"token_counts\"][pair])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c489a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
