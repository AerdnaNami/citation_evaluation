{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f28b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "\n",
    "HEADER_RE = re.compile(\n",
    "    r\"^\\s*\\[(?P<span>\\d+-\\d+)\\]\\s*(?P<label>[^,]+),\\s*File:\\s*(?P<filename>[^,]+)\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_ann_text(path: str, txt_path, encoding: str = \"utf-8\"):\n",
    "    \"\"\"\n",
    "    Read `path` and return a list of dicts, each dict:\n",
    "      {\n",
    "        \"span\": \"836-1053\",\n",
    "        \"filename\": \"paper_16.txt\",\n",
    "        \"label\": \"Coherence\",\n",
    "        \"body\": \"the text after the header up to the next header (multi-line string)\"\n",
    "      }\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=encoding) as fh:\n",
    "        lines = fh.readlines()\n",
    "\n",
    "    # Find indices of header lines and capture groups\n",
    "    headers = []  # list of tuples (index, matchobj)\n",
    "    for i, line in enumerate(lines):\n",
    "        m = HEADER_RE.match(line)\n",
    "        if m:\n",
    "            headers.append((i, m))\n",
    "\n",
    "    results: List[Dict[str, Optional[str]]] = []\n",
    "    if not headers:\n",
    "        return results\n",
    "\n",
    "    # For each found header, slice the following lines until next header (or EOF)\n",
    "    for idx, (line_idx, match) in enumerate(headers):\n",
    "        next_idx = headers[idx + 1][0] if (idx + 1) < len(headers) else len(lines)\n",
    "\n",
    "        # Extract body: lines after the header line up to next header index\n",
    "        body_lines = [ln.rstrip(\"\\n\") for ln in lines[line_idx + 1 : next_idx]]\n",
    "        body = \"\\n\".join(body_lines).strip() or None\n",
    "\n",
    "        span = match.group(\"span\").strip() if match.group(\"span\") else None\n",
    "        start_span = int(span.split(\"-\")[0])\n",
    "        end_span = int(span.split(\"-\")[1])\n",
    "        filename = match.group(\"filename\").strip() if match.group(\"filename\") else None\n",
    "        label = match.group(\"label\").strip() if match.group(\"label\") else None\n",
    "\n",
    "        with open(f\"{txt_path}/{filename}\", 'r') as f:\n",
    "            full_text = f.read()\n",
    "\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"start\": start_span,\n",
    "            \"end\": end_span,\n",
    "            \"label\": label,\n",
    "            \"text\": body, \n",
    "            \"full_text\": full_text,\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1_file = \"../../annotated_data/annotations/ann1/ann_edd_1.txt\"\n",
    "ann2_file = \"../../annotated_data/annotations/ann2/ann_iman_2.txt\"\n",
    "ann3_file = \"../../annotated_data/annotations/ann3/ann_Ekaterina_2.txt\"\n",
    "ann4_file = \"../../annotated_data/annotations/ann4/kaushal_annotations.txt\"\n",
    "\n",
    "txt_path = \"../../to_annotate\"\n",
    "ann1 = extract_ann_text(ann1_file, txt_path)\n",
    "ann2 = extract_ann_text(ann2_file, txt_path)\n",
    "ann3 = extract_ann_text(ann3_file, txt_path)\n",
    "ann4 = extract_ann_text(ann4_file, txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454eb398",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreed_ten = \"../../annotated_data/first_ten_agreed.txt\"\n",
    "ann_agrd_ten = extract_ann_text(agreed_ten, txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9cebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "path = \"../../annotated_data/annotations\"\n",
    "for i in range(1, 11):\n",
    "    ann1_paper = [d for d in ann1 if d.get(\"filename\") == f\"paper_{i}.txt\"]\n",
    "    ann2_paper = [d for d in ann2 if d.get(\"filename\") == f\"paper_{i}.txt\"]\n",
    "    ann3_paper = [d for d in ann3 if d.get(\"filename\") == f\"paper_{i}.txt\"]\n",
    "    ann4_paper = [d for d in ann4 if d.get(\"filename\") == f\"paper_{i}.txt\"]\n",
    "    agreed_ten = [d for d in ann_agrd_ten if d.get(\"filename\") == f\"paper_{i}.txt\"]\n",
    "\n",
    "    with open(f\"{path}/ann1/ann1_paper_{i}.json\", 'w') as f:\n",
    "        json.dump(ann1_paper, f, indent=4)\n",
    "\n",
    "    with open(f\"{path}  /ann2/ann2_paper_{i}.json\", 'w') as f:\n",
    "        json.dump(ann2_paper, f, indent=4)\n",
    "\n",
    "    with open(f\"{path}/ann3/ann3_paper_{i}.json\", 'w') as f:\n",
    "        json.dump(ann3_paper, f, indent=4)\n",
    "\n",
    "    with open(f\"{path}/ann4/ann4_paper_{i}.json\", 'w') as f:\n",
    "        json.dump(ann4_paper, f, indent=4)\n",
    "\n",
    "    with open(f\"{path}/first_ten_agreed/first_ten_agreed_paper_{i}.json\", 'w') as f:\n",
    "        json.dump(agreed_ten, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95bf91fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "Span = Tuple[int, int]  # [start, end)\n",
    "\n",
    "\n",
    "def intersect(a, b):\n",
    "    s = max(a[0], b[0])\n",
    "    e = min(a[1], b[1])\n",
    "    return (s, e) if e > s else None\n",
    "\n",
    "\n",
    "def group_by_label(items, required_label):\n",
    "    grouped = defaultdict(list)\n",
    "    for d in items:\n",
    "        if d[\"label\"] == required_label:\n",
    "            fn = d[\"filename\"]\n",
    "            lab = d[\"label\"]\n",
    "            grouped[(fn, lab)].append(d)\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def dedup_span_dicts(lst):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for d in lst:\n",
    "        key = (d.get(\"filename\"), int(d.get(\"start\")), int(d.get(\"end\")), d.get(\"label\"))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(d)\n",
    "    return out\n",
    "\n",
    "def consensus_overlaps_2(ann1, ann2, category,):\n",
    "    g1 = group_by_label(ann1, category)\n",
    "    g2 = group_by_label(ann2, category)\n",
    "    \n",
    "    keys = set(g1.keys()) | set(g2.keys())\n",
    "    \n",
    "    raw = []\n",
    "\n",
    "    for (fn, lab) in sorted(keys):\n",
    "        s1 = g1.get((fn, lab), [])\n",
    "        s2 = g2.get((fn, lab), [])\n",
    "        if not s1 or not s2:\n",
    "            continue\n",
    "\n",
    "        for a in s1:\n",
    "            if category == 'Format':\n",
    "                min_overlap_chars_a = int(len(a['text']) * 0.90)\n",
    "            elif category == 'Unsupported claim':\n",
    "                min_overlap_chars_a = int(len(a['text']) * 0.8)\n",
    "            else:\n",
    "                min_overlap_chars_a = int(len(a['text']) * 0.7)\n",
    "            \n",
    "            A = (int(a[\"start\"]), int(a[\"end\"]))\n",
    "            for b in s2:\n",
    "                if category == 'Format':\n",
    "                    min_overlap_chars_b = int(len(a['text']) * 0.90)\n",
    "                elif category == 'Unsupported claim':\n",
    "                    min_overlap_chars_b = int(len(a['text']) * 0.8)\n",
    "                else:\n",
    "                    min_overlap_chars_b = int(len(a['text']) * 0.7)\n",
    "\n",
    "                # smallest min overlapping chars from A or B will be used to determine how many characters must be present in each others annotations \n",
    "                if min_overlap_chars_a < min_overlap_chars_b:\n",
    "                    min_overlap_chars = min_overlap_chars_a\n",
    "                else:\n",
    "                    min_overlap_chars = min_overlap_chars_b\n",
    "\n",
    "                B = (int(b[\"start\"]), int(b[\"end\"]))\n",
    "                ab = intersect(A, B) # calculate interesect between annotator A and annotator B \n",
    "\n",
    "                if not ab:\n",
    "                    continue\n",
    "                if (ab[1] - ab[0]) < min_overlap_chars: # only consider spans that satisfy minimum overlapping chars\n",
    "                    continue\n",
    "                \n",
    "                raw.append(\n",
    "                    {\n",
    "                        \"filename\": fn,\n",
    "                        \"label\": a[\"label\"],\n",
    "                        \"text\": a['text'],\n",
    "                        \"start\": ab[0],\n",
    "                        \"end\": ab[1],\n",
    "                        \"spans_a\": [a],\n",
    "                        \"spans_b\": [b],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Merge duplicates: multiple pairs can yield the same intersection.\n",
    "    merged = {}\n",
    "    for r in raw:\n",
    "        k = (r[\"filename\"], r[\"label\"], r['text'], r[\"start\"], r[\"end\"])\n",
    "        if k not in merged:\n",
    "            merged[k] = {\n",
    "                \"filename\": r[\"filename\"],\n",
    "                \"label\": r[\"label\"],\n",
    "                \"text\": r['text'],\n",
    "                \"start\": r[\"start\"],\n",
    "                \"end\": r[\"end\"],\n",
    "                \"spans_a\": [],  # spans from annotator A that overlap with span from B\n",
    "                \"spans_b\": [],  # spans from annotator B that overlap with span from A\n",
    "            }\n",
    "        merged[k][\"spans_a\"].extend(r[\"spans_a\"])\n",
    "        merged[k][\"spans_b\"].extend(r[\"spans_b\"])\n",
    "\n",
    "    final = []\n",
    "    for v in merged.values():\n",
    "        v[\"spans_a\"] = dedup_span_dicts(v[\"spans_a\"])\n",
    "        v[\"spans_b\"] = dedup_span_dicts(v[\"spans_b\"])\n",
    "        final.append(v)\n",
    "\n",
    "    final.sort(key=lambda x: (x[\"filename\"], x[\"label\"], x['text'], x[\"start\"], x[\"end\"]))\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5576d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus_overlaps_3(\n",
    "    ann1,\n",
    "    ann2,\n",
    "    ann3,\n",
    "    *,\n",
    "    category,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns consensus overlaps where all three annotators overlap.\n",
    "\n",
    "    Output dict fields:\n",
    "      filename, label, start, end,\n",
    "      spans_a / spans_b / spans_c  (the contributing original spans)\n",
    "    \"\"\"\n",
    "    g1 = group_by_label(ann1, category)\n",
    "    g2 = group_by_label(ann2, category)\n",
    "    g3 = group_by_label(ann3, category)\n",
    "\n",
    "    keys = set(g1.keys()) | set(g2.keys()) | set(g3.keys())\n",
    "    out = []\n",
    "\n",
    "    for (fn, lab) in sorted(keys):\n",
    "        s1 = g1.get((fn, lab), [])\n",
    "        s2 = g2.get((fn, lab), [])\n",
    "        s3 = g3.get((fn, lab), [])\n",
    "\n",
    "        # If any annotator has no spans for this file/label, there can't be 3-way overlap\n",
    "        if not s1 or not s2 or not s3:\n",
    "            continue\n",
    "\n",
    "        # Brute force is usually fine for typical annotation sizes.\n",
    "        # If you have huge numbers of spans per file, we can sweep-line optimize.\n",
    "        for a in s1:\n",
    "            if category == 'Format':\n",
    "                min_overlap_chars = int(len(a['text']) * 0.90)\n",
    "            elif category == 'Unsupported claim':\n",
    "                min_overlap_chars = int(len(a['text']) * 0.8)\n",
    "            else:\n",
    "                min_overlap_chars = int(len(a['text']) * 0.7)\n",
    "\n",
    "            A = (int(a[\"start\"]), int(a[\"end\"]))\n",
    "            for b in s2:\n",
    "                B = (int(b[\"start\"]), int(b[\"end\"]))\n",
    "                ab = intersect(A, B)\n",
    "                if not ab or (ab[1] - ab[0]) < min_overlap_chars:\n",
    "                    continue\n",
    "                for c in s3:\n",
    "                    C = (int(c[\"start\"]), int(c[\"end\"]))\n",
    "                    abc = intersect(ab, C)\n",
    "                    if not abc or (abc[1] - abc[0]) < min_overlap_chars:\n",
    "                        continue\n",
    "\n",
    "                    out.append(\n",
    "                        {\n",
    "                            \"filename\": fn,\n",
    "                            \"label\": a[\"label\"],\n",
    "                            \"start\": abc[0],\n",
    "                            \"end\": abc[1],\n",
    "                            \"spans_a\": [a],\n",
    "                            \"spans_b\": [b],\n",
    "                            \"spans_c\": [c],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    # Merge duplicates: multiple triples can yield the same intersection range.\n",
    "    # We'll merge by (filename,label,start,end) and collect contributing spans.\n",
    "    merged: Dict[Tuple[str, str, int, int], Dict[str, Any]] = {}\n",
    "    for r in out:\n",
    "        k = (r[\"filename\"], r[\"label\"], r[\"start\"], r[\"end\"])\n",
    "        if k not in merged:\n",
    "            merged[k] = {\n",
    "                \"filename\": r[\"filename\"],\n",
    "                \"label\": r[\"label\"],\n",
    "                \"start\": r[\"start\"],\n",
    "                \"end\": r[\"end\"],\n",
    "                \"spans_a\": [],\n",
    "                \"spans_b\": [],\n",
    "                \"spans_c\": [],\n",
    "            }\n",
    "        merged[k][\"spans_a\"].extend(r[\"spans_a\"])\n",
    "        merged[k][\"spans_b\"].extend(r[\"spans_b\"])\n",
    "        merged[k][\"spans_c\"].extend(r[\"spans_c\"])\n",
    "\n",
    "    # Optional: de-duplicate identical contributing spans\n",
    "    def dedup_span_dicts(lst: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        seen = set()\n",
    "        out2 = []\n",
    "        for d in lst:\n",
    "            key = (d.get(\"filename\"), int(d.get(\"start\")), int(d.get(\"end\")), d.get(\"label\"))\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            out2.append(d)\n",
    "        return out2\n",
    "\n",
    "    final = []\n",
    "    for k, v in merged.items():\n",
    "        v[\"spans_a\"] = dedup_span_dicts(v[\"spans_a\"])\n",
    "        v[\"spans_b\"] = dedup_span_dicts(v[\"spans_b\"])\n",
    "        v[\"spans_c\"] = dedup_span_dicts(v[\"spans_c\"])\n",
    "        final.append(v)\n",
    "\n",
    "    final.sort(key=lambda x: (x[\"filename\"], x[\"label\"], x[\"start\"], x[\"end\"]))\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f352712",
   "metadata": {},
   "source": [
    "#### Consensus among:\n",
    "- annotators 1 & 2 \n",
    "- annotators 1 & 3\n",
    "- annotators 2 & 3\n",
    "- annotator 4 with final agreed first ten samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab5c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing annotation consensus: 100%|██████████| 10/10 [00:00<00:00, 42.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus agreements saved in folder:../annotated_data/annotations/agreements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm \n",
    "\n",
    "path = \"../../annotated_data/annotations\"\n",
    "categories = ['Unsupported claim', 'Format', 'Coherence', 'Lacks synthesis']\n",
    "\n",
    "total_consensus_12 = []\n",
    "total_consensus_13 = []\n",
    "total_consensus_23 = []\n",
    "total_consensus_ann4 = []\n",
    "\n",
    "for i in tqdm(range(1,11), desc=\"Processing annotation consensus\"):\n",
    "    ann1_paper = json.load(open(f\"{path}/ann1/ann1_paper_{i}.json\"))\n",
    "    ann2_paper = json.load(open(f\"{path}/ann2/ann2_paper_{i}.json\"))\n",
    "    ann3_paper = json.load(open(f\"{path}/ann3/ann3_paper_{i}.json\"))\n",
    "    ann4_paper = json.load(open(f\"{path}/ann4/ann4_paper_{i}.json\"))\n",
    "    first_ten_paper = json.load(open(f\"{path}/first_ten_agreed/first_ten_agreed_paper_{i}.json\"))\n",
    "    \n",
    "    for category in categories:\n",
    "        consensus_12 = consensus_overlaps_2(\n",
    "            ann1_paper, ann2_paper,\n",
    "            category=category\n",
    "        )\n",
    "        consensus_13 = consensus_overlaps_2(\n",
    "            ann1_paper, ann3_paper,\n",
    "            category=category\n",
    "        )\n",
    "        consensus_23 = consensus_overlaps_2(\n",
    "            ann2_paper, ann3_paper,\n",
    "            category=category\n",
    "        )\n",
    "        ann4_comparison = consensus_overlaps_2(\n",
    "            first_ten_paper, ann4_paper,\n",
    "            category=category\n",
    "        )\n",
    "\n",
    "        total_consensus_12.extend(consensus_12)\n",
    "        total_consensus_13.extend(consensus_13)\n",
    "        total_consensus_23.extend(consensus_23)\n",
    "        # consensus between 4th annotator and final agreements of first ten papers\n",
    "        total_consensus_ann4.extend(ann4_comparison)\n",
    " \n",
    "folder = f\"{path}/agreements\"\n",
    "\n",
    "filename = \"consensus_agreement_12.json\"\n",
    "with open(f\"{folder}/{filename}\", 'w') as f:\n",
    "    json.dump(total_consensus_12, f, indent=4)\n",
    "\n",
    "filename = f\"consensus_agreement_13.json\"\n",
    "with open(f\"{folder}/{filename}\", 'w') as f:\n",
    "    json.dump(total_consensus_13, f, indent=4)\n",
    "\n",
    "filename = f\"consensus_agreement_23.json\"\n",
    "with open(f\"{folder}/{filename}\", 'w') as f:\n",
    "    json.dump(total_consensus_23, f, indent=4)\n",
    "\n",
    "filename = f\"consensus_ann4_first_10.json\"\n",
    "with open(f\"{folder}/{filename}\", 'w') as f:\n",
    "    json.dump(total_consensus_ann4, f, indent=4)\n",
    "\n",
    "print(f\"Consensus agreements saved in folder:{folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b5b98",
   "metadata": {},
   "source": [
    "### Consensus among all 3 annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b8b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing annotation consensus among all 3: 100%|██████████| 10/10 [00:00<00:00, 57.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus agreements saved in folder:../annotated_data/annotations/agreements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm \n",
    "\n",
    "path = \"../../annotated_data/annotations\"\n",
    "categories = ['Unsupported claim', 'Format', 'Coherence', 'Lacks synthesis']\n",
    "\n",
    "total_consensus = []\n",
    "\n",
    "for i in tqdm(range(1,11), desc=\"Processing annotation consensus among all 3\"):\n",
    "    ann1_paper = json.load(open(f\"{path}/ann1/ann1_paper_{i}.json\"))\n",
    "    ann2_paper = json.load(open(f\"{path}/ann2/ann2_paper_{i}.json\"))\n",
    "    ann3_paper = json.load(open(f\"{path}/ann3/ann3_paper_{i}.json\"))\n",
    "    ann4_paper = json.load(open(f\"{path}/ann4/ann4_paper_{i}.json\"))\n",
    "    \n",
    "    for category in categories:\n",
    "        consensus = consensus_overlaps_3(\n",
    "            ann2_paper, ann3_paper, ann1_paper,\n",
    "            category=category\n",
    "        )\n",
    "\n",
    "        total_consensus.append(consensus)\n",
    "\n",
    "folder = f\"{path}/agreements\"\n",
    "\n",
    "filename = \"consensus_agreement_all_3.json\"\n",
    "with open(f\"{folder}/{filename}\", 'w') as f:\n",
    "    json.dump(total_consensus, f, indent=4)\n",
    "\n",
    "print(f\"Consensus agreements saved in folder:{folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fffe8a4",
   "metadata": {},
   "source": [
    "### Measuring Krippendorf's alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "001955e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Any, Optional, Set\n",
    "\n",
    "Token = Tuple[str, int, int]  # (token_text, start_char, end_char)\n",
    "\n",
    "\n",
    "def regex_tokenize_with_offsets(text: str) -> List[Token]:\n",
    "    return [(m.group(0), m.start(), m.end()) for m in re.finditer(r\"\\w+\", text, flags=re.UNICODE)]\n",
    "\n",
    "\n",
    "def overlaps(a_start: int, a_end: int, b_start: int, b_end: int) -> bool:\n",
    "    return not (a_end <= b_start or b_end <= a_start)\n",
    "\n",
    "\n",
    "def spans_for_label(spans: List[Dict[str, Any]], label: str) -> List[Tuple[int, int]]:\n",
    "    out: List[Tuple[int, int]] = []\n",
    "    for sp in spans:\n",
    "        if sp.get(\"label\") == label:\n",
    "            s, e = int(sp[\"start\"]), int(sp[\"end\"])\n",
    "            if e > s:\n",
    "                out.append((s, e))\n",
    "    return sorted(out)\n",
    "\n",
    "\n",
    "def token_binary_labels_from_spans(tokens: List[Token], pos_spans: List[Tuple[int, int]]) -> List[int]:\n",
    "    y = [0] * len(tokens)\n",
    "    if not pos_spans:\n",
    "        return y\n",
    "\n",
    "    for i, (_tok, ts, te) in enumerate(tokens):\n",
    "        for ss, se in pos_spans:\n",
    "            if overlaps(ts, te, ss, se):\n",
    "                y[i] = 1\n",
    "                break\n",
    "            if ss >= te:\n",
    "                break\n",
    "    return y\n",
    "\n",
    "\n",
    "def char_binary_labels_from_spans(\n",
    "    text: str,\n",
    "    pos_spans: List[Tuple[int, int]],\n",
    "    *,\n",
    "    ignore_whitespace: bool = False,\n",
    ") -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - labels: 0/1 per character index in `text` (or per non-whitespace char if ignore_whitespace=True)\n",
    "      - char_indices: mapping from label index -> original character index in `text`\n",
    "        (so unit_ids can be stable even when whitespace ignored)\n",
    "    \"\"\"\n",
    "    n = len(text)\n",
    "    if n == 0:\n",
    "        return [], []\n",
    "\n",
    "    # Which character positions are included as units\n",
    "    if ignore_whitespace:\n",
    "        char_indices = [i for i, ch in enumerate(text) if not ch.isspace()]\n",
    "    else:\n",
    "        char_indices = list(range(n))\n",
    "\n",
    "    labels = [0] * len(char_indices)\n",
    "    if not pos_spans:\n",
    "        return labels, char_indices\n",
    "\n",
    "    # Mark characters as positive if they fall inside ANY positive span.\n",
    "    # Spans are assumed half-open [start, end), like Python slicing.\n",
    "    span_i = 0\n",
    "    spans_sorted = sorted(pos_spans)\n",
    "\n",
    "    for j, ci in enumerate(char_indices):\n",
    "        # Advance span pointer until span might include ci\n",
    "        while span_i < len(spans_sorted) and spans_sorted[span_i][1] <= ci:\n",
    "            span_i += 1\n",
    "        if span_i >= len(spans_sorted):\n",
    "            break\n",
    "        ss, se = spans_sorted[span_i]\n",
    "        if ss <= ci < se:\n",
    "            labels[j] = 1\n",
    "\n",
    "    return labels, char_indices\n",
    "\n",
    "\n",
    "def krippendorff_alpha_nominal(units: Dict[Any, Dict[Any, Optional[int]]]) -> float:\n",
    "    all_ratings: List[int] = []\n",
    "    for ratings in units.values():\n",
    "        for v in ratings.values():\n",
    "            if v is not None:\n",
    "                all_ratings.append(v)\n",
    "\n",
    "    if len(all_ratings) < 2:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    freq = defaultdict(int)\n",
    "    for v in all_ratings:\n",
    "        freq[v] += 1\n",
    "    N = len(all_ratings)\n",
    "    sum_p2 = sum((cnt / N) ** 2 for cnt in freq.values())\n",
    "    De = 1.0 - sum_p2\n",
    "    if De == 0.0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    total_disagree_pairs = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for ratings in units.values():\n",
    "        vals = [v for v in ratings.values() if v is not None]\n",
    "        m = len(vals)\n",
    "        if m < 2:\n",
    "            continue\n",
    "        pairs = m * (m - 1) // 2\n",
    "        disagree = 0\n",
    "        for i in range(m):\n",
    "            for j in range(i + 1, m):\n",
    "                if vals[i] != vals[j]:\n",
    "                    disagree += 1\n",
    "        total_disagree_pairs += disagree\n",
    "        total_pairs += pairs\n",
    "\n",
    "    if total_pairs == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    Do = total_disagree_pairs / total_pairs\n",
    "    return 1.0 - (Do / De)\n",
    "\n",
    "\n",
    "def build_units_for_category(\n",
    "    text_by_doc: Dict[str, str],\n",
    "    spans_by_doc: Dict[str, Dict[str, List[Dict[str, Any]]]],\n",
    "    category: str,\n",
    "    *,\n",
    "    annotators: Optional[List[str]] = None,\n",
    "    require_all: bool = False,\n",
    "    min_annotators_per_unit: int = 2,\n",
    "    missing_means_no_spans: bool = True,\n",
    "    granularity: str = \"char\",  # \"char\" or \"token\"\n",
    "    ignore_whitespace_chars: bool = True,  # only used when granularity=\"char\"\n",
    ") -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Builds units for ONE category as binary (0/1).\n",
    "\n",
    "    If granularity=\"token\": tokens are units (coarser; overlap length approximated by token count).\n",
    "    If granularity=\"char\": characters are units (finer; overlap length weighted by char count).\n",
    "    \"\"\"\n",
    "    if min_annotators_per_unit < 2:\n",
    "        raise ValueError(\"min_annotators_per_unit must be >= 2\")\n",
    "    if granularity not in {\"token\", \"char\"}:\n",
    "        raise ValueError(\"granularity must be 'token' or 'char'\")\n",
    "\n",
    "    if annotators is None:\n",
    "        ann_set: Set[str] = set()\n",
    "        for doc_map in spans_by_doc.values():\n",
    "            ann_set.update(doc_map.keys())\n",
    "        annotators = sorted(ann_set)\n",
    "\n",
    "    if len(annotators) < 2:\n",
    "        raise ValueError(f\"Need at least 2 annotators, got {len(annotators)}: {annotators}\")\n",
    "\n",
    "    units: Dict[str, Dict[str, int]] = {}\n",
    "\n",
    "    for doc_id, text in text_by_doc.items():\n",
    "        ann_map = spans_by_doc.get(doc_id, {})\n",
    "\n",
    "        if granularity == \"token\":\n",
    "            tokens = regex_tokenize_with_offsets(text)\n",
    "            n_units = len(tokens)\n",
    "\n",
    "            per_ann_labels: Dict[str, List[int]] = {}\n",
    "            for ann in annotators:\n",
    "                if ann in ann_map:\n",
    "                    spans = ann_map[ann]\n",
    "                else:\n",
    "                    if not missing_means_no_spans:\n",
    "                        continue\n",
    "                    spans = []\n",
    "                pos_spans = spans_for_label(spans, category)\n",
    "                per_ann_labels[ann] = token_binary_labels_from_spans(tokens, pos_spans)\n",
    "\n",
    "            for ui in range(n_units):\n",
    "                unit_id = f\"{doc_id}:tok:{ui}\"\n",
    "                ratings: Dict[str, int] = {ann: labels[ui] for ann, labels in per_ann_labels.items()}\n",
    "\n",
    "                if require_all:\n",
    "                    if len(ratings) != len(annotators):\n",
    "                        continue\n",
    "                else:\n",
    "                    if len(ratings) < min_annotators_per_unit:\n",
    "                        continue\n",
    "                units[unit_id] = ratings\n",
    "\n",
    "        else:  # granularity == \"char\"\n",
    "            # Build a stable set of character-unit indices (optionally skipping whitespace)\n",
    "            # We compute it once per doc and reuse for all annotators.\n",
    "            base_labels, char_indices = char_binary_labels_from_spans(\n",
    "                text, [], ignore_whitespace=ignore_whitespace_chars\n",
    "            )\n",
    "            n_units = len(char_indices)\n",
    "            if n_units == 0:\n",
    "                continue\n",
    "\n",
    "            per_ann_labels: Dict[str, List[int]] = {}\n",
    "            for ann in annotators:\n",
    "                if ann in ann_map:\n",
    "                    spans = ann_map[ann]\n",
    "                else:\n",
    "                    if not missing_means_no_spans:\n",
    "                        continue\n",
    "                    spans = []\n",
    "                pos_spans = spans_for_label(spans, category)\n",
    "                labels, _ = char_binary_labels_from_spans(\n",
    "                    text, pos_spans, ignore_whitespace=ignore_whitespace_chars\n",
    "                )\n",
    "                per_ann_labels[ann] = labels\n",
    "\n",
    "            for ui, ci in enumerate(char_indices):\n",
    "                # include original char index in unit_id for debugging / traceability\n",
    "                unit_id = f\"{doc_id}:ch:{ci}\"\n",
    "                ratings: Dict[str, int] = {ann: labels[ui] for ann, labels in per_ann_labels.items()}\n",
    "\n",
    "                if require_all:\n",
    "                    if len(ratings) != len(annotators):\n",
    "                        continue\n",
    "                else:\n",
    "                    if len(ratings) < min_annotators_per_unit:\n",
    "                        continue\n",
    "                units[unit_id] = ratings\n",
    "\n",
    "    return units\n",
    "\n",
    "\n",
    "def krippendorff_alpha_per_category(\n",
    "    text_by_doc,\n",
    "    spans_by_doc,\n",
    "    categories,\n",
    "    *,\n",
    "    annotators,\n",
    "    require_all= False,\n",
    "    min_annotators_per_unit = 2,\n",
    "    missing_means_no_spans = True,\n",
    "    granularity = \"char\",          # <- set \"char\" to weight by overlap length\n",
    "    ignore_whitespace_chars = True):\n",
    "    out = {}\n",
    "    for cat in categories:\n",
    "        units = build_units_for_category(\n",
    "            text_by_doc,\n",
    "            spans_by_doc,\n",
    "            cat,\n",
    "            annotators=annotators,\n",
    "            require_all=require_all,\n",
    "            min_annotators_per_unit=min_annotators_per_unit,\n",
    "            missing_means_no_spans=missing_means_no_spans,\n",
    "            granularity=granularity,\n",
    "            ignore_whitespace_chars=ignore_whitespace_chars,\n",
    "        )\n",
    "        out[cat] = krippendorff_alpha_nominal(units)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecee412",
   "metadata": {},
   "source": [
    "Krippendorf's alpha between ann1, ann2, and ann3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1029e9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "krippendorf_3_anns.json saved in folder: ../annotated_data/annotations/agreements\n"
     ]
    }
   ],
   "source": [
    "txt_path = \"../../to_annotate\"\n",
    "categories = ['Unsupported claim', 'Format', 'Coherence', 'Lacks synthesis']\n",
    "total = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    ann1_paper = [d for d in ann1 if d.get(\"filename\") == f\"paper_{i}.txt\"]\n",
    "    ann2_paper = [d for d in ann2 if d.get(\"filename\") == f\"paper_{i}.txt\"]\n",
    "    ann3_paper = [d for d in ann3 if d.get(\"filename\") == f\"paper_{i}.txt\"]\n",
    "    \n",
    "    file = f\"{txt_path}/paper_{i}.txt\"\n",
    "    with open(file, 'r') as f:\n",
    "        file_text = f.read()\n",
    "    \n",
    "    text_by_doc = {\"doc1\": file_text}\n",
    "    spans_by_doc = {\n",
    "        \"doc1\": {\n",
    "            \"annA\": ann1_paper,\n",
    "            \"annB\": ann2_paper,\n",
    "            \"annC\": ann3_paper,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    alphas = krippendorff_alpha_per_category(\n",
    "        text_by_doc,\n",
    "        spans_by_doc,\n",
    "        categories,\n",
    "        annotators=[\"annB\", \"annC\"],\n",
    "        require_all=False,\n",
    "        min_annotators_per_unit=2,  # tokens need >=2 ratings\n",
    "        missing_means_no_spans=True \n",
    "    )\n",
    "\n",
    "    total.append({'filename': f\"paper_{i}.txt\", 'labels': alphas})\n",
    "\n",
    "folder = \"../annotated_data/annotations/agreements\"\n",
    "filename = \"krippendorf_3_anns.json\"\n",
    "with open(f\"{folder}/{filename}\", 'w') as f:\n",
    "    json.dump(total, f, indent=4)\n",
    "\n",
    "print(f\"{filename} saved in folder: {folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc9f71b",
   "metadata": {},
   "source": [
    "Krippendorf's alpha between ann4 and final ten agreed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53421b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "krippendorf_ann4_first_ten.json saved in folder: ../annotated_data/annotations/agreements\n"
     ]
    }
   ],
   "source": [
    "txt_path = \"../../to_annotate\"\n",
    "categories = ['Unsupported claim', 'Format', 'Coherence', 'Lacks synthesis']\n",
    "total = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    ann4_paper = [d for d in ann4 if d.get(\"filename\") == f\"paper_{i}.txt\"]\n",
    "    first_ten_paper = [d for d in first_ten_paper if d.get(\"filename\") == f\"paper_{i}.txt\"]\\\n",
    "    \n",
    "    file = f\"{txt_path}/paper_{i}.txt\"\n",
    "    with open(file, 'r') as f:\n",
    "        file_text = f.read()\n",
    "    \n",
    "    text_by_doc = {\"doc1\": file_text}\n",
    "    spans_by_doc = {\n",
    "        \"doc1\": {\n",
    "            \"annA\": ann4_paper,\n",
    "            \"annB\": first_ten_paper,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    alphas = krippendorff_alpha_per_category(\n",
    "        text_by_doc,\n",
    "        spans_by_doc,\n",
    "        categories,\n",
    "        annotators=[\"annA\", \"annB\"],\n",
    "        require_all=False,\n",
    "        min_annotators_per_unit=2,\n",
    "        missing_means_no_spans=True \n",
    "    )\n",
    "\n",
    "    total.append({'filename': f\"paper_{i}.txt\", 'labels': alphas})\n",
    "\n",
    "folder = f\"{path}/agreements\"\n",
    "filename = \"krippendorf_ann4_first_ten.json\"\n",
    "with open(f\"{folder}/{filename}\", 'w') as f:\n",
    "    json.dump(total, f, indent=4)\n",
    "\n",
    "print(f\"{filename} saved in folder: {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba746936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
