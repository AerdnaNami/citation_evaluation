nohup: ignoring input
Training: Format
Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Settings (from config):
  Category: Format
  Model: meta-llama/Llama-3.1-8B-Instruct
  Train data: format/format_training_data.json
  Dev/Eval used during training: format/format_eval_data.json
  Eval file for post-train scoring: format/format_eval_data.json
  Output dir: /home/iman.alsikaiti/citation_evaluation/citation_evaluation/code/training/format/llama3.1_8b_sft
  max_length: 2048
  train_bs: 1, eval_bs: 1, grad_accum: 16
  lr: 5e-05, epochs: 3.0, warmup_ratio: 0.03
  bf16: True, fp16: False
  LoRA: r=32, alpha=16, dropout=0.1
  flash_attn: False, torch_compile: False
  num_workers: 4, max_grad_norm: 1.0
  prompt_key: prompt, completion_key: completion
  eval generation: max_new_tokens=128, temperature=0.0, top_p=1.0
  eval debug: eval_debug_k=25, eval_debug_chars=600
  eval token-IoU threshold: iou_threshold=0.5
Tokenizer loaded.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:38<01:55, 38.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:59<00:56, 28.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:15<00:22, 22.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 18.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.69s/it]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
/home/iman.alsikaiti/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Model loaded.
Prepping for kbit training and adding LoRA adapters...
trainable params: 83,886,080 || all params: 8,114,147,328 || trainable%: 1.0338
Loading datasets...
[SANITY] seq_len=433, num_label_tokens=18
[SANITY] forward loss=0.9444942474365234
No checkpoint found; starting fresh.
  0%|          | 0/153 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Token indices sequence length is longer than the specified maximum sequence length for this model (134664 > 131072). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (134094 > 131072). Running this sequence through the model will result in indexing errors
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Token indices sequence length is longer than the specified maximum sequence length for this model (132696 > 131072). Running this sequence through the model will result in indexing errors
  1%|          | 1/153 [00:43<1:49:44, 43.32s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (141654 > 131072). Running this sequence through the model will result in indexing errors
  1%|▏         | 2/153 [01:12<1:27:40, 34.84s/it]  2%|▏         | 3/153 [01:40<1:19:39, 31.86s/it]  3%|▎         | 4/153 [02:09<1:16:05, 30.64s/it]  3%|▎         | 5/153 [02:38<1:14:15, 30.11s/it]  4%|▍         | 6/153 [03:07<1:12:35, 29.63s/it]  5%|▍         | 7/153 [03:36<1:11:54, 29.55s/it]  5%|▌         | 8/153 [04:06<1:11:20, 29.52s/it]  6%|▌         | 9/153 [04:35<1:10:27, 29.36s/it]  7%|▋         | 10/153 [05:03<1:09:37, 29.21s/it]  7%|▋         | 11/153 [05:33<1:09:04, 29.19s/it]  8%|▊         | 12/153 [06:02<1:08:40, 29.22s/it]  8%|▊         | 13/153 [06:31<1:08:09, 29.21s/it]  9%|▉         | 14/153 [07:00<1:07:40, 29.21s/it] 10%|▉         | 15/153 [07:28<1:06:09, 28.76s/it] 10%|█         | 16/153 [07:57<1:05:55, 28.87s/it] 11%|█         | 17/153 [08:25<1:04:41, 28.54s/it] 12%|█▏        | 18/153 [08:54<1:04:40, 28.74s/it] 12%|█▏        | 19/153 [09:23<1:04:28, 28.87s/it] 13%|█▎        | 20/153 [09:53<1:04:24, 29.06s/it] 14%|█▎        | 21/153 [10:22<1:04:08, 29.15s/it] 14%|█▍        | 22/153 [10:52<1:04:04, 29.34s/it] 15%|█▌        | 23/153 [11:21<1:03:22, 29.25s/it] 16%|█▌        | 24/153 [11:49<1:02:08, 28.90s/it] 16%|█▋        | 25/153 [12:17<1:01:13, 28.70s/it]                                                   16%|█▋        | 25/153 [12:17<1:01:13, 28.70s/it] 17%|█▋        | 26/153 [12:46<1:01:02, 28.84s/it] 18%|█▊        | 27/153 [13:16<1:00:57, 29.02s/it] 18%|█▊        | 28/153 [13:45<1:00:29, 29.03s/it] 19%|█▉        | 29/153 [14:14<1:00:17, 29.17s/it] 20%|█▉        | 30/153 [14:44<59:58, 29.26s/it]   20%|██        | 31/153 [15:13<59:31, 29.27s/it] 21%|██        | 32/153 [15:42<59:03, 29.29s/it] 22%|██▏       | 33/153 [16:12<58:54, 29.45s/it] 22%|██▏       | 34/153 [16:41<58:01, 29.25s/it] 23%|██▎       | 35/153 [17:11<57:40, 29.32s/it] 24%|██▎       | 36/153 [17:40<57:13, 29.34s/it] 24%|██▍       | 37/153 [18:09<56:45, 29.36s/it] 25%|██▍       | 38/153 [18:39<56:14, 29.35s/it] 25%|██▌       | 39/153 [19:08<55:57, 29.45s/it] 26%|██▌       | 40/153 [19:38<55:30, 29.47s/it] 27%|██▋       | 41/153 [20:07<54:56, 29.43s/it] 27%|██▋       | 42/153 [20:36<54:08, 29.27s/it] 28%|██▊       | 43/153 [21:05<53:39, 29.27s/it] 29%|██▉       | 44/153 [21:35<53:05, 29.22s/it] 29%|██▉       | 45/153 [22:04<52:39, 29.25s/it] 30%|███       | 46/153 [22:33<51:59, 29.15s/it] 31%|███       | 47/153 [23:02<51:34, 29.19s/it] 31%|███▏      | 48/153 [23:31<50:43, 28.98s/it] 32%|███▏      | 49/153 [24:00<50:21, 29.05s/it] 33%|███▎      | 50/153 [24:29<49:57, 29.11s/it]                                                 33%|███▎      | 50/153 [24:29<49:57, 29.11s/it] 33%|███▎      | 51/153 [24:57<48:57, 28.80s/it] 34%|███▍      | 52/153 [25:13<42:01, 24.97s/it]{'loss': 0.8367, 'grad_norm': 1.1037235260009766, 'learning_rate': 4.799414918094347e-05, 'epoch': 0.48}
{'loss': 0.1055, 'grad_norm': 1.411563515663147, 'learning_rate': 3.986582940760717e-05, 'epoch': 0.97}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 35%|███▍      | 53/153 [25:43<44:17, 26.58s/it] 35%|███▌      | 54/153 [26:13<45:07, 27.35s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (143316 > 131072). Running this sequence through the model will result in indexing errors
 36%|███▌      | 55/153 [26:42<45:29, 27.85s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (138856 > 131072). Running this sequence through the model will result in indexing errors
 37%|███▋      | 56/153 [27:11<45:38, 28.23s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (135040 > 131072). Running this sequence through the model will result in indexing errors
 37%|███▋      | 57/153 [27:40<45:41, 28.56s/it] 38%|███▊      | 58/153 [28:09<45:30, 28.75s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (136965 > 131072). Running this sequence through the model will result in indexing errors
 39%|███▊      | 59/153 [28:39<45:17, 28.91s/it] 39%|███▉      | 60/153 [29:08<45:12, 29.16s/it] 40%|███▉      | 61/153 [29:38<44:48, 29.22s/it] 41%|████      | 62/153 [30:07<44:11, 29.14s/it] 41%|████      | 63/153 [30:34<43:04, 28.72s/it] 42%|████▏     | 64/153 [31:04<42:50, 28.89s/it] 42%|████▏     | 65/153 [31:32<42:19, 28.86s/it] 43%|████▎     | 66/153 [31:59<41:02, 28.31s/it] 44%|████▍     | 67/153 [32:28<40:48, 28.47s/it] 44%|████▍     | 68/153 [32:58<40:45, 28.77s/it] 45%|████▌     | 69/153 [33:27<40:25, 28.88s/it] 46%|████▌     | 70/153 [33:55<39:27, 28.52s/it] 46%|████▋     | 71/153 [34:22<38:41, 28.31s/it] 47%|████▋     | 72/153 [34:52<38:34, 28.58s/it] 48%|████▊     | 73/153 [35:21<38:30, 28.88s/it] 48%|████▊     | 74/153 [35:50<38:06, 28.95s/it] 49%|████▉     | 75/153 [36:20<37:45, 29.05s/it]                                                 49%|████▉     | 75/153 [36:20<37:45, 29.05s/it] 50%|████▉     | 76/153 [36:49<37:22, 29.13s/it] 50%|█████     | 77/153 [37:17<36:39, 28.94s/it] 51%|█████     | 78/153 [37:47<36:25, 29.14s/it] 52%|█████▏    | 79/153 [38:16<36:02, 29.23s/it] 52%|█████▏    | 80/153 [38:46<35:40, 29.32s/it] 53%|█████▎    | 81/153 [39:15<35:11, 29.32s/it] 54%|█████▎    | 82/153 [39:45<34:43, 29.34s/it] 54%|█████▍    | 83/153 [40:14<34:18, 29.41s/it] 55%|█████▍    | 84/153 [40:43<33:46, 29.36s/it] 56%|█████▌    | 85/153 [41:13<33:17, 29.37s/it] 56%|█████▌    | 86/153 [41:42<32:38, 29.23s/it] 57%|█████▋    | 87/153 [42:11<32:03, 29.14s/it] 58%|█████▊    | 88/153 [42:40<31:34, 29.14s/it] 58%|█████▊    | 89/153 [43:09<31:07, 29.18s/it] 59%|█████▉    | 90/153 [43:37<30:17, 28.85s/it] 59%|█████▉    | 91/153 [44:07<29:58, 29.01s/it] 60%|██████    | 92/153 [44:36<29:38, 29.15s/it] 61%|██████    | 93/153 [45:05<28:57, 28.95s/it] 61%|██████▏   | 94/153 [45:34<28:33, 29.04s/it] 62%|██████▏   | 95/153 [46:03<28:04, 29.05s/it] 63%|██████▎   | 96/153 [46:32<27:35, 29.05s/it] 63%|██████▎   | 97/153 [47:01<27:09, 29.10s/it] 64%|██████▍   | 98/153 [47:31<26:49, 29.27s/it] 65%|██████▍   | 99/153 [48:00<26:21, 29.28s/it] 65%|██████▌   | 100/153 [48:29<25:47, 29.21s/it]                                                  65%|██████▌   | 100/153 [48:29<25:47, 29.21s/it] 66%|██████▌   | 101/153 [48:58<25:12, 29.08s/it] 67%|██████▋   | 102/153 [49:27<24:41, 29.05s/it] 67%|██████▋   | 103/153 [49:56<24:18, 29.17s/it] 68%|██████▊   | 104/153 [50:12<20:36, 25.22s/it]{'loss': 0.0501, 'grad_norm': 0.5072911977767944, 'learning_rate': 2.7648393442985403e-05, 'epoch': 1.45}
{'loss': 0.0485, 'grad_norm': 0.7442910075187683, 'learning_rate': 1.4702468793900188e-05, 'epoch': 1.93}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Token indices sequence length is longer than the specified maximum sequence length for this model (142958 > 131072). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (141479 > 131072). Running this sequence through the model will result in indexing errors
 69%|██████▊   | 105/153 [50:43<21:22, 26.72s/it] 69%|██████▉   | 106/153 [51:10<21:11, 27.05s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (133274 > 131072). Running this sequence through the model will result in indexing errors
 70%|██████▉   | 107/153 [51:38<20:57, 27.35s/it] 71%|███████   | 108/153 [52:07<20:40, 27.57s/it] 71%|███████   | 109/153 [52:35<20:30, 27.97s/it] 72%|███████▏  | 110/153 [53:04<20:14, 28.24s/it] 73%|███████▎  | 111/153 [53:32<19:41, 28.12s/it] 73%|███████▎  | 112/153 [54:01<19:24, 28.40s/it] 74%|███████▍  | 113/153 [54:30<19:01, 28.54s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (134664 > 131072). Running this sequence through the model will result in indexing errors
 75%|███████▍  | 114/153 [54:58<18:26, 28.38s/it] 75%|███████▌  | 115/153 [55:28<18:11, 28.72s/it] 76%|███████▌  | 116/153 [55:57<17:47, 28.85s/it] 76%|███████▋  | 117/153 [56:26<17:22, 28.95s/it] 77%|███████▋  | 118/153 [56:55<16:56, 29.04s/it] 78%|███████▊  | 119/153 [57:24<16:27, 29.05s/it] 78%|███████▊  | 120/153 [57:53<15:58, 29.04s/it] 79%|███████▉  | 121/153 [58:23<15:32, 29.15s/it] 80%|███████▉  | 122/153 [58:52<15:03, 29.13s/it] 80%|████████  | 123/153 [59:21<14:38, 29.28s/it] 81%|████████  | 124/153 [59:51<14:09, 29.29s/it] 82%|████████▏ | 125/153 [1:00:19<13:34, 29.10s/it]                                                    82%|████████▏ | 125/153 [1:00:19<13:34, 29.10s/it] 82%|████████▏ | 126/153 [1:00:49<13:06, 29.13s/it] 83%|████████▎ | 127/153 [1:01:18<12:40, 29.25s/it] 84%|████████▎ | 128/153 [1:01:47<12:06, 29.06s/it] 84%|████████▍ | 129/153 [1:02:16<11:39, 29.16s/it] 85%|████████▍ | 130/153 [1:02:46<11:12, 29.25s/it] 86%|████████▌ | 131/153 [1:03:15<10:43, 29.24s/it] 86%|████████▋ | 132/153 [1:03:44<10:13, 29.21s/it] 87%|████████▋ | 133/153 [1:04:13<09:46, 29.31s/it] 88%|████████▊ | 134/153 [1:04:43<09:17, 29.37s/it] 88%|████████▊ | 135/153 [1:05:12<08:48, 29.39s/it] 89%|████████▉ | 136/153 [1:05:41<08:17, 29.28s/it] 90%|████████▉ | 137/153 [1:06:11<07:48, 29.29s/it] 90%|█████████ | 138/153 [1:06:40<07:19, 29.29s/it] 91%|█████████ | 139/153 [1:07:09<06:50, 29.29s/it] 92%|█████████▏| 140/153 [1:07:39<06:21, 29.38s/it] 92%|█████████▏| 141/153 [1:08:08<05:51, 29.32s/it] 93%|█████████▎| 142/153 [1:08:37<05:22, 29.33s/it] 93%|█████████▎| 143/153 [1:09:07<04:53, 29.35s/it] 94%|█████████▍| 144/153 [1:09:36<04:23, 29.26s/it] 95%|█████████▍| 145/153 [1:10:05<03:53, 29.21s/it] 95%|█████████▌| 146/153 [1:10:33<03:22, 28.90s/it] 96%|█████████▌| 147/153 [1:11:03<02:54, 29.06s/it] 97%|█████████▋| 148/153 [1:11:33<02:26, 29.34s/it] 97%|█████████▋| 149/153 [1:12:02<01:57, 29.32s/it] 98%|█████████▊| 150/153 [1:12:31<01:27, 29.32s/it]                                                    98%|█████████▊| 150/153 [1:12:31<01:27, 29.32s/it] 99%|█████████▊| 151/153 [1:13:00<00:58, 29.09s/it] 99%|█████████▉| 152/153 [1:13:29<00:29, 29.01s/it]100%|██████████| 153/153 [1:13:58<00:00, 29.07s/it]                                                   100%|██████████| 153/153 [1:14:05<00:00, 29.07s/it]100%|██████████| 153/153 [1:14:05<00:00, 29.06s/it]
{'loss': 0.0289, 'grad_norm': 0.1768171787261963, 'learning_rate': 4.58906700180286e-06, 'epoch': 2.41}
{'loss': 0.0177, 'grad_norm': 0.05713629722595215, 'learning_rate': 9.006278643683696e-08, 'epoch': 2.89}
{'train_runtime': 4446.6016, 'train_samples_per_second': 0.557, 'train_steps_per_second': 0.034, 'train_loss': 0.17875972360956902, 'epoch': 2.95}
Running post-train evaluation on eval_path: format/format_eval_data.json
[EVAL] pairwise_micro: {'precision': 0.21487603305785125, 'recall': 0.21487603305785125, 'f1': 0.21487603305785125}
[EVAL] model_vs_gold_micro: {'precision': 0.5977011494252874, 'recall': 0.1309823677581864, 'f1': 0.21487603305785122}
[EVAL] debug_summary: {'num_empty_pred_examples': 0, 'avg_pred_spans_per_example': 4.563218390804598, 'debug_k': 25, 'debug_chars': 600, 'iou_threshold': 0.5}
[EVAL] Wrote eval metrics to: /home/iman.alsikaiti/citation_evaluation/citation_evaluation/code/training/format/llama3.1_8b_sft/eval_pairwise_f1.json
[EVAL] Wrote predictions debug to: /home/iman.alsikaiti/citation_evaluation/citation_evaluation/code/training/format/llama3.1_8b_sft/eval_predictions_debug.json
Saved (LoRA adapters + tokenizer) to: /home/iman.alsikaiti/citation_evaluation/citation_evaluation/code/training/format/llama3.1_8b_sft
Wrote save reasons to: /home/iman.alsikaiti/citation_evaluation/citation_evaluation/code/training/format/llama3.1_8b_sft/save_reasons.json
Training: Unsupported_claim
Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
Settings (from config):
  Category: Unsupported_claim
  Model: meta-llama/Llama-3.1-8B-Instruct
  Train data: unsupported_claim/unsupported_claim_training_data.json
  Dev/Eval used during training: unsupported_claim/unsupported_claim_eval_data.json
  Eval file for post-train scoring: unsupported_claim/unsupported_claim_eval_data.json
  Output dir: /home/iman.alsikaiti/citation_evaluation/citation_evaluation/code/training/unsupported_claim/llama3.1_8b_sft
  max_length: 2048
  train_bs: 1, eval_bs: 1, grad_accum: 16
  lr: 5e-05, epochs: 3.0, warmup_ratio: 0.03
  bf16: True, fp16: False
  LoRA: r=32, alpha=16, dropout=0.1
  flash_attn: False, torch_compile: False
  num_workers: 4, max_grad_norm: 1.0
  prompt_key: prompt, completion_key: completion
  eval generation: max_new_tokens=128, temperature=0.0, top_p=1.0
  eval debug: eval_debug_k=25, eval_debug_chars=600
  eval token-IoU threshold: iou_threshold=0.5
Tokenizer loaded.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:34<01:42, 34.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:54<00:52, 26.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:14<00:23, 23.25s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 19.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 21.92s/it]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
/home/iman.alsikaiti/miniconda3/envs/thesis/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Model loaded.
Prepping for kbit training and adding LoRA adapters...
trainable params: 83,886,080 || all params: 8,114,147,328 || trainable%: 1.0338
Loading datasets...
[SANITY] seq_len=453, num_label_tokens=12
[SANITY] forward loss=1.2593988180160522
No checkpoint found; starting fresh.
  0%|          | 0/153 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
Token indices sequence length is longer than the specified maximum sequence length for this model (134313 > 131072). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (135458 > 131072). Running this sequence through the model will result in indexing errors
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  1%|          | 1/153 [00:45<1:55:35, 45.63s/it]  1%|▏         | 2/153 [01:22<1:41:40, 40.40s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (131876 > 131072). Running this sequence through the model will result in indexing errors
  2%|▏         | 3/153 [01:59<1:37:44, 39.09s/it]  3%|▎         | 4/153 [02:31<1:30:12, 36.33s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (133583 > 131072). Running this sequence through the model will result in indexing errors
  3%|▎         | 5/153 [03:01<1:23:52, 34.00s/it]  4%|▍         | 6/153 [03:30<1:19:03, 32.27s/it]  5%|▍         | 7/153 [04:02<1:17:57, 32.04s/it]  5%|▌         | 8/153 [04:51<1:30:47, 37.57s/it]  6%|▌         | 9/153 [05:35<1:34:57, 39.57s/it]  7%|▋         | 10/153 [06:14<1:33:24, 39.19s/it]  7%|▋         | 11/153 [06:45<1:27:17, 36.88s/it]  8%|▊         | 12/153 [07:15<1:21:36, 34.73s/it]  8%|▊         | 13/153 [07:45<1:17:37, 33.27s/it]  9%|▉         | 14/153 [08:19<1:17:54, 33.63s/it] 10%|▉         | 15/153 [08:49<1:14:16, 32.30s/it] 10%|█         | 16/153 [09:18<1:11:43, 31.41s/it] 11%|█         | 17/153 [09:46<1:08:49, 30.36s/it]