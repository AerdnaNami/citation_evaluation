[
  {
    "span": "Multiple studies have shown that debiasing hurts downstream accuracy on coreference resolution.",
    "document": "Related Work\n\nAlgorithmic fairness in NLP has examined representational harms in word embeddings and contextualized encoders, motivating debiasing techniques and evaluation protocols (Bolukbasi et al., 2016; Caliskan et al., 2017). Beyond intrinsic measures, recent work emphasizes task-level impacts and trade-offs between fairness and utility.\n\nMultiple studies have shown that debiasing hurts downstream accuracy on coreference resolution. However, the extent of the trade-off depends on dataset characteristics and model capacity. We investigate causal mediation strategies to target specific sources of bias while preserving task-relevant information.",
    "reason": "Asserts empirical findings across multiple studies without citing any of them; such task-specific performance claims must be supported by references.",
    "start": 347,
    "end": 442,
    "label": "Unsupported_claim"
  },
  {
    "span": "Miller et al. 1",
    "document": "Introduction\n\nFairness in recommender systems has garnered increasing attention due to societal impact and regulatory considerations (Ekstrand et al., 2018; Burke, 2017). Approaches target both provider-side and consumer-side fairness by modifying training objectives or post-processing rankings (Beutel et al., 2019; Singh and Joachims, 2018).\n\nMiller et al. 1 introduce a dataset of user-item interactions annotated with sensitive attributes, enabling evaluation of disparate impact across demographics. However, existing evaluation protocols often conflate calibration with exposure fairness (Sapiezynski et al., 2019; Biega et al., 2018). We propose exposure-aware calibration metrics that explicitly account for position bias and interaction propensities (Joachims et al., 2017; Schnabel et al., 2016).\n\nOur contributions include a counterfactual risk minimization framework with fairness constraints, plus an open-source simulator for controlled policy evaluation (Swaminathan and Joachims, 2015; Dwork et al., 2012).",
    "reason": "Wrong use of footnotes: trailing '1' suggests a footnote but the citation lacks a year; should be formatted as a proper author–year citation (e.g., 'Miller et al. (2020)') or as a clearly defined footnote.",
    "start": 346,
    "end": 361,
    "label": "Format"
  },
  {
    "span": "Most public recommender benchmarks rely on implicit feedback such as clicks and views, rather than explicit ratings.",
    "document": "Introduction\n\nOffline evaluation remains the default for recommender systems due to ease of reproducibility and cost considerations. However, dataset bias and feedback loops complicate generalization to online performance. Most public recommender benchmarks rely on implicit feedback such as clicks and views, rather than explicit ratings. This mismatch can distort loss design and calibration for user satisfaction objectives. We propose an evaluation protocol that augments logged implicit signals with counterfactual reweighting and calibrated propensity estimation to better approximate online outcomes.",
    "reason": "Makes a general claim about the composition of public benchmarks without citing supporting surveys or datasets; per rule (b) this domain-specific assertion needs evidence.",
    "start": 223,
    "end": 339,
    "label": "Unsupported_claim"
  },
  {
    "span": "There are numerous recent works demonstrating that adapter-based fine-tuning outperforms full fine-tuning for multilingual NER.",
    "document": "Related Work\n\nParameter-efficient transfer learning methods aim to reduce the number of trainable parameters while preserving downstream accuracy. Among these, adapters, low-rank adaptation (LoRA), and prompt-based tuning have emerged as popular strategies. There are numerous recent works demonstrating that adapter-based fine-tuning outperforms full fine-tuning for multilingual NER. Despite the growing interest, the reported gains depend heavily on the language coverage and the annotation density of the target domains. Prior studies also highlight that adapter stacks can be composed to capture language- and task-specific variations with minimal interference, but the extent to which such compositionality transfers to low-resource settings remains unclear. In this paper, we focus on multilingual NER and evaluate several parameter-efficient configurations under controlled annotation budgets.\n",
    "reason": "Mentions 'recent works' and a comparative performance claim without providing citations to support it.",
    "start": 258,
    "end": 385,
    "label": "Unsupported_claim"
  },
  {
    "span": "(LeCun et al., 2015",
    "document": "Introduction\n\nConvolutional neural networks (CNNs) have transformed visual recognition by enabling hierarchical feature learning directly from pixels (Krizhevsky et al., 2012; He et al., 2016). As shown in (LeCun et al., 2015 the integration of depth, regularization, and large-scale data is crucial for robust performance. Later innovations such as residual connections (He et al., 2016) and normalization layers (Ioffe and Szegedy, 2015) further improved optimization stability. Despite these advances, deploying CNNs under tight latency constraints remains challenging (Howard et al., 2019; Tan and Le, 2019), motivating our study of compact architectures.",
    "reason": "Missing closing parenthesis in the parenthetical citation; it should be '(LeCun et al., 2015)'.",
    "start": 206,
    "end": 225,
    "label": "Format"
  },
  {
    "span": "Existing atlases rarely include metadata on ethnicity or environmental exposure.",
    "document": "Related Work\n\nSingle-cell atlases map cellular diversity across tissues and developmental stages, enabling comparative analyses and cross-study integration (Regev et al., 2017; Han et al., 2020; Cao et al., 2020). Integration methods address batch effects through mutual nearest neighbors, variational harmonization, or graph alignment (Haghverdi et al., 2018; Stuart et al., 2019; Lopez et al., 2018). Recent efforts expand to spatial and multi-omic modalities, complicating harmonization but providing richer context (Ståhl et al., 2016; Cable et al., 2022).\n\nExisting atlases rarely include metadata on ethnicity or environmental exposure. We introduce a schema and curation pipeline for harmonizing demographic and exposure covariates across consortia, and demonstrate downstream gains in differential abundance and trajectory analyses.",
    "reason": "Asserts a field-wide data limitation without supporting citations or evidence (rule b).",
    "start": 562,
    "end": 642,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Liu et al., 2018)",
    "document": "Introduction\n\nActive learning (AL) aims to reduce annotation cost by selecting the most informative instances for labeling (Settles, 2010). In (Liu et al., 2018), the authors propose a reinforcement learning approach to learn query strategies that generalize across tasks. Subsequent work studies uncertainty sampling and diversity-based selection with pretrained encoders (Sener and Savarese, 2018; Ash et al., 2020; Ein-Dor et al., 2020).\n\nRecent AL for Transformers leverages predictive entropy and gradient embeddings to balance informativeness and redundancy (Yuan et al., 2020; Shelmanov et al., 2021). Nevertheless, computational overhead at selection time and distribution shift between acquisition and deployment remain practical concerns (Lowell et al., 2019).",
    "reason": "Wrong citation style: narrative construction should not place the author-year inside parentheses after 'In'; correct form is \"In Liu et al. (2018), ...\".",
    "start": 140,
    "end": 161,
    "label": "Format"
  },
  {
    "span": "[Zhao et al., 2020]",
    "document": "Introduction\n\nCross-modal retrieval aligns visual and textual representations in a shared embedding space (Frome et al., 2013; Kiros et al., 2014). Transformers with dual encoders and late interaction have improved scalability (Johnson et al., 2021), while cross-attention models excel at fine-grained alignment (Li et al., 2019). We adhere to an author–year style with parenthetical citations, e.g., (Karpathy and Fei-Fei, 2015). Yet some benchmarks report that lightweight models remain competitive [Zhao et al., 2020], motivating our latency-accuracy analysis under strict serving budgets. Our contributions include a new evaluation protocol and a calibration method for retrieval scores.",
    "reason": "Wrong bracket type for author–year style: should use parentheses '(Zhao et al., 2020)' instead of square brackets.",
    "start": 501,
    "end": 520,
    "label": "Format"
  },
  {
    "span": "Klein and Murray 1",
    "document": "Related Work\n\nFew-shot meta-learning proposes to optimize for rapid adaptation from limited data (Finn et al., 2017; Snell et al., 2017). Klein and Murray 1 show that episodic training benefits from task grouping in NLP, inspiring structured episode construction. Contemporary approaches leverage large language models for in-context learning (Brown et al., 2020) while incorporating metric-learning objectives to stabilize adaptation (Oreshkin et al., 2018).",
    "reason": "Wrong use of footnotes: the numeric \"1\" appears after the authors without a year or proper footnote formatting.",
    "start": 138,
    "end": 156,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claimed that byte-level models outperform phoneme-based models on low-resource languages.",
    "document": "Related Work\n\nMultilingual automatic speech recognition (ASR) seeks to train a single model that generalizes across many languages, often with highly imbalanced data and diverse phonotactics (Conneau et al., 2021; Baevski et al., 2020). A central design choice is the modeling unit: phonemes, graphemes, subword wordpieces, or bytes. Self-supervised pretraining has significantly reduced the reliance on transcribed audio and improved cross-lingual transfer (Zhang et al., 2022). In a previous study, the authors claimed that byte-level models outperform phoneme-based models on low-resource languages. Other work emphasizes the importance of lexicon-free decoding and language model fusion in improving robustness under domain shift (Pratap et al., 2021; Watanabe et al., 2017).\n\nOur work focuses on unit selection for tiny-footprint ASR. We show that a carefully regularized subword tokenizer with cross-lingual sharing can match or exceed byte-level performance while enabling faster inference on embedded hardware.",
    "reason": "Refers to a 'previous study' and a performance claim without citing the study (violates rule b and e-ii).",
    "start": 480,
    "end": 602,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works pretrain on billions of image–text pairs scraped from the web.",
    "document": "Related Work\n\nVision–language pretraining (VLP) aligns visual representations with natural language to enable transfer across tasks such as retrieval, captioning, and VQA. Early VLP models combined object-centric features with masked language modeling, while more recent methods perform end-to-end contrastive pretraining. Recent works pretrain on billions of image–text pairs scraped from the web. Despite impressive zero-shot capabilities, concerns remain about dataset curation, duplicated content, and demographic biases. Our study investigates data filtering strategies and their downstream impacts on fairness and robustness in multilingual settings.\n",
    "reason": "Mentions 'recent works' and a specific scale of data without providing citations (violates rule d and b).",
    "start": 323,
    "end": 398,
    "label": "Unsupported_claim"
  },
  {
    "span": "Electronic health record phenotyping with transformers has become the de facto standard in hospital informatics.",
    "document": "Introduction\n\nAccurate computational phenotyping from electronic health records (EHRs) is essential for clinical research, cohort discovery, and decision support. The heterogeneity of EHR data—including unstructured notes, time-stamped events, and irregular sampling—poses significant modeling challenges. Neural architectures have been proposed to integrate structured and unstructured modalities, often emphasizing temporal reasoning and domain adaptation.\n\nElectronic health record phenotyping with transformers has become the de facto standard in hospital informatics. Nevertheless, questions remain regarding reproducibility, calibration under dataset shift, and fairness across demographic subgroups. We present a systematic study comparing transformer-based phenotyping to strong non-transformer baselines under identical preprocessing and evaluation settings, highlighting where architectural capacity, pretraining, and data volume most affect outcomes.",
    "reason": "Declares a field-wide 'de facto standard' without citing surveys, benchmarks, or adoption statistics to substantiate the claim.",
    "start": 460,
    "end": 572,
    "label": "Unsupported_claim"
  },
  {
    "span": "Our model achieves new state-of-the-art on SQuAD, HotpotQA, and Natural Questions.",
    "document": "Introduction\n\nOpen-domain question answering has advanced rapidly with the advent of pre-trained language models and retrieval-augmented generation. Despite progress, existing systems still struggle to consolidate multi-hop evidence and remain brittle under adversarially phrased queries. In this work, we present a lightweight retriever-reader pipeline that unifies dense retrieval with an instruction-tuned generator to better exploit cross-paragraph evidence.\n\nWe focus on benchmarks that test single-hop and multi-hop reasoning. Our method learns to select compact evidence sets that are fed to a constrained generator, improving factual consistency while reducing latency. Our model achieves new state-of-the-art on SQuAD, HotpotQA, and Natural Questions. We further analyze the contribution of retrieval compression and constrained decoding, and we conduct an error analysis on ambiguous questions to understand failure modes.\n\nFinally, we release our training code and inference scripts to encourage reproducibility and facilitate further research on efficient evidence selection for open-domain QA.",
    "reason": "Claims SOTA and mentions specific datasets at first mention without any citations or comparative evidence (violates a and b).",
    "start": 678,
    "end": 760,
    "label": "Unsupported_claim"
  },
  {
    "span": "wav2vec 2.0 leveraged contrastive pretraining on raw audio (Baevski et al., 2020). HuBERT masked prediction improved representation quality (Hsu et al., 2021). Data2vec unified modalities (Baevski et al., 2022). LARGE-scale supervision with pseudo-labels has also been used (Kahn et al., 2020).",
    "document": "Related Work\n\nSelf-Supervised Learning for Speech\n\nSelf-supervised pretraining on unlabeled audio has transformed ASR by learning robust representations transferable to low-resource settings. Objectives vary from contrastive learning to masked prediction and teacher-student distillation.\n\nwav2vec 2.0 leveraged contrastive pretraining on raw audio (Baevski et al., 2020). HuBERT masked prediction improved representation quality (Hsu et al., 2021). Data2vec unified modalities (Baevski et al., 2022). LARGE-scale supervision with pseudo-labels has also been used (Kahn et al., 2020).\n\nHowever, most methods assume stationarity across channels and environments. We propose a channel-invariant pretraining scheme that explicitly regularizes cross-microphone consistency.",
    "reason": "The span lists several methods without transitions indicating contrasts (e.g., contrastive vs. masked objectives) or building relations. The connection among sentences is implied but not stated, producing a coherence issue (criteria a and b).",
    "start": 290,
    "end": 584,
    "label": "Coherence"
  },
  {
    "span": "Recent works combine graph neural networks with transformers (Ying et al., 2021; Kreuzer et al., 2021; Chen et al., 2022).",
    "document": "Related Work\nMolecular property prediction often relies on graph representations of molecules, where atoms and bonds define nodes and edges. Graph neural networks (GNNs) have achieved strong results by leveraging message passing to encode local chemical structures.\n\nTransformer architectures have advanced sequence and vision tasks through self-attention, prompting adaptations for graphs. Recent works combine graph neural networks with transformers (Ying et al., 2021; Kreuzer et al., 2021; Chen et al., 2022). Hybrid approaches modify positional encodings (Dwivedi et al., 2021), introduce edge-aware attention (Hussain et al., 2022), or learn global context tokens (Wu et al., 2022). We evaluate our model on standard benchmarks with scaffold splits.",
    "reason": "The span merely catalogs a trend and citations without clarifying how these hybrid models connect to the paper's approach, what gaps persist, or why a new method is needed.",
    "start": 391,
    "end": 513,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT was used in an AES task trained on essays from ASAP.",
    "document": "Related Work\n\nAutomated essay scoring (AES) has progressed from handcrafted features and linear models to neural encoders that learn holistic representations of writing quality. Earlier approaches relied on surface features such as length and lexical diversity, while recent methods use pretrained language models to capture content and coherence. BERT was used in an AES task trained on essays from ASAP. Complementary research explores prompt-aware modeling, domain adaptation, and robustness to adversarial edits.",
    "reason": "Mentions a specific setup (model and dataset) from prior work without any citation to that work.",
    "start": 348,
    "end": 405,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from nine prompts to predict holistic scores.",
    "document": "Related Work\n\nAutomatic Essay Scoring (AES) has progressed from handcrafted features and linear models to neural architectures that learn representations directly from text. Early systems engineered lexical, syntactic, and discourse features to approximate rubric criteria. Subsequent work introduced convolutional and recurrent encoders to capture global coherence and local evidence. BERT was used in an AES task trained on essays from nine prompts to predict holistic scores. More recent approaches explore pairwise ranking objectives, prompt-aware conditioning, and multi-trait scoring to align with rubric dimensions. Despite these advances, domain shift across prompts and institutions continues to reduce model reliability, motivating robust transfer and calibration methods.\n",
    "reason": "This is a specific claim about a particular model setup and dataset configuration (nine prompts) without a citation to the study that did it (rule b and example iii).",
    "start": 386,
    "end": 478,
    "label": "Unsupported_claim"
  },
  {
    "span": "Explainers modulate user trust and reliance in decision support systems (Kizilcec, 2016; Buçinca et al., 2021). Users form mental models through iterative feedback during interaction (Lai et al., 2020). Interactive visualizations facilitate sensemaking in complex analytics tasks (Amershi et al., 2014).",
    "document": "Introduction\n\nHuman-centered AI research investigates how explanations, interfaces, and training protocols influence end-user outcomes. Appropriate reliance and calibrated trust are critical for safety and effectiveness in high-stakes domains.\n\nDesigning explanation modalities requires understanding user goals, cognitive load, and context of use. Iterative, user-in-the-loop workflows can align system outputs with human expectations but may also introduce biases.\n\nExplainers modulate user trust and reliance in decision support systems (Kizilcec, 2016; Buçinca et al., 2021). Users form mental models through iterative feedback during interaction (Lai et al., 2020). Interactive visualizations facilitate sensemaking in complex analytics tasks (Amershi et al., 2014).\n\nWe present a mixed-methods study of adaptive explanations that personalize content based on observed interaction signals. Quantitative metrics of reliance are paired with qualitative analyses of mental model shifts.",
    "reason": "The span lists three HCI topics with citations but lacks transitions or explicit statements linking them, making the relationship between the works unclear and abrupt (a, b).",
    "start": 468,
    "end": 771,
    "label": "Coherence"
  },
  {
    "span": "[Chen et al., 2017)",
    "document": "Related Work\n\nBiomedical NLP leverages domain-specific pretraining and structured knowledge. Models trained on PubMed and clinical notes yield large gains over general-domain encoders (Lee et al., 2020; Alsentzer et al., 2019). Prior work [Chen et al., 2017) also emphasizes the importance of entity normalization to link mentions to ontologies (Wang et al., 2018), and recent benchmarks measure cross-institution generalization (Peng et al., 2019).",
    "reason": "Mismatched brackets/parentheses in a citation; should be “(Chen et al., 2017)” or “[Chen et al., 2017]” consistently.",
    "start": 239,
    "end": 258,
    "label": "Format"
  },
  {
    "span": "the widely adopted augmentation strategy SpecAugment++",
    "document": "Related Work\n\nSupervised speech recognition benefits greatly from data augmentation, which improves robustness to channel variation and speaker diversity. Techniques such as time warping, frequency masking, and noise mixing are popular due to their simplicity and effectiveness across architectures. Beyond simple heuristics, policy learning approaches attempt to discover augmentation schedules automatically.\n\nIn end-to-end ASR, the widely adopted augmentation strategy SpecAugment++ extends masking with time-dynamic policies and adaptive ranges to better match the training curriculum. Several studies have combined such augmentations with self-supervised pretraining, reporting additive gains on noisy benchmarks. Our approach complements these efforts by conditioning augmentation strength on online loss signals to avoid over-perturbation.",
    "reason": "The method \"SpecAugment++\" is introduced as a widely adopted strategy without any citation to the original paper or source (rules a and b).",
    "start": 431,
    "end": 485,
    "label": "Unsupported_claim"
  },
  {
    "span": "WWM",
    "document": "Introduction\n\nBERT (Devlin et al., 2018) is a Transformer-based pretrained model, whose prosperity starts from English language and gradually spreads to many other languages. The original BERT model is trained with character-level masking (CLM). 2 A certain percentage (e.g. 15%) of tokens in the input sequence is masked and the model is learned to predict the masked tokens.\n\nIt is helpful to note that a word in the input sequence of BERT can be broken into multiple wordpiece tokens (Wu et al., 2016). 3 For example, the input sentence \"She is undeniably brilliant\" is converted to a wordpiece sequence \"She is un ##deni ##ably brilliant\", where \"##\" is a special prefix added to indicate that the token should be attached to the previous one. In this case the word \"undeniably\" is broken into three wordpieces {\"un\", \"##deni\", \"##ably\"}. In standard masked language modeling, CLM may mask any one of them. In this case, if the token \"##ably\" is masked, it is easier for the model to complete the prediction task because \"un\" and \"##deni\" are informative prompts. To address this, Whole word masking (WWM) masks all three subtokens (i.e., {\"un\", \"##deni\", \"##ably\"}) within a word at once. For Chinese, however, each token is an atomic character that cannot be broken into smaller pieces. Many Chinese words are compounds that consisting of multiple characters (Wood and Connelly, 2009). 4 For example, \"手机\" (cellphone) is a word consisting of two characters \"手\" (hand) and \"机\" (machine). Here, learning with WWM would lose the association among characters corresponding to a word.\n\nIn this work, we introduce two probing tasks to study Chinese BERT model's ability on characterlevel understanding. The first probing task is character replacement. Given a sentence and a position where the corresponding character is erroneous, the task is to replace the erroneous character with the correct one. The second probing task is character insertion. Given a sentence and the positions where a given number of characters should be inserted, the task is to insert the correct characters. We leverage the benchmark dataset on grammatical error correction (Rao et al., 2020a) and create a dataset including labels for 19,075 tokens in 10,448 sentences.\n\nWe train three baseline models based on the same text corpus of 80B characters using CLM, WWM, and both CLM and WWM, separately. We have the following major findings. (1) When one character needs to be inserted or replaced, the model trained with CLM performs the best. Moreover, the model initialized from RoBERTa (Cui et al., 2019) and trained with WWM gets worse gradually with more training steps. (2) When more than one character needs to be handled, WWM is the key to better performance. (3) When evaluating sentence-level downstream tasks, the impact of these masking strategies is minimal and the model trained with them performs comparably.\n\n ",
    "start": 2339,
    "end": 2342,
    "label": "Unsupported_claim"
  },
  {
    "span": "Garcia et al. 3",
    "document": "Related Work\n\nData augmentation is a cornerstone for improving generalization in vision and NLP. Classical transformations such as flips and crops remain competitive for images (Shorten and Khoshgoftaar, 2019), while back-translation aids machine translation (Sennrich et al., 2016). As argued by Garcia et al. 3, mixup-style strategies can be unified under vicinal risk minimization. Subsequent works propose learned augmentation policies (Cubuk et al., 2019) and manifold-preserving perturbations (Hendrycks et al., 2020). We study how augmentation interacts with long-tailed distributions, building on insights from Buda et al. (2018).",
    "reason": "Wrong use of footnote-style marker; should include a year or be formatted as a proper citation (e.g., 'Garcia et al. (2019)' or '[3]').",
    "start": 297,
    "end": 312,
    "label": "Format"
  },
  {
    "span": "For causal representation learning, invariant risk minimization (Arjovsky et al., 2020), domain adversarial training (Ganin et al., 2016), and counterfactual data augmentation (Kocaoglu et al., 2018) have been proposed.",
    "document": "Introduction\n\nCausal representation learning seeks features that are stable across interventions and environments, enabling models to generalize under distribution shift. Identifying invariant mechanisms is central to this goal.\n\nFor causal representation learning, invariant risk minimization (Arjovsky et al., 2020), domain adversarial training (Ganin et al., 2016), and counterfactual data augmentation (Kocaoglu et al., 2018) have been proposed. Recent works also examine sparsity-inducing priors and structure learning over latent variables.\n\nDespite progress in benchmarks, measuring invariance at scale remains challenging due to limited availability of interventional data.",
    "reason": "The sentence cites prominent approaches but does not articulate how they relate to the authors' problem or what gap motivates the new method (criterion a and c).",
    "start": 230,
    "end": 449,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The MIMIC-IV dataset is the de facto benchmark for ICU mortality prediction.",
    "document": "Introduction\n\nFederated learning has emerged as a promising paradigm for privacy-preserving predictive modeling in healthcare. By keeping patient data local and sharing only model updates, hospitals can collaborate without exposing sensitive information. Despite this promise, reproducing strong baselines across institutions is challenging due to heterogeneous coding practices, missingness patterns, and differing care protocols. The MIMIC-IV dataset is the de facto benchmark for ICU mortality prediction. Yet models tuned on a single site often fail to generalize to unseen hospitals, underscoring the importance of federated and domain adaptation techniques. In this paper, we present a rigorous evaluation of cross-site generalization under realistic communication and privacy constraints, and we analyze how common normalization and calibration strategies behave under federated aggregation.",
    "reason": "Asserts a dataset’s benchmark status without citing any source or evidence.",
    "start": 432,
    "end": 508,
    "label": "Unsupported_claim"
  },
  {
    "span": "Machine learning for climate downscaling encompasses statistical bias correction (Teutschbein and Seibert, 2012), analog and stochastic weather generators (Zorita and von Storch, 1999; Ailliot et al., 2015), convolutional super-resolution models (Vandal et al., 2017; Pan et al., 2019), and conditional generative models for precipitation fields (Ravuri et al., 2021; Leinonen et al., 2020). Temporal alignment and physical constraints have been incorporated through loss shaping and multi-task objectives (Beusch et al., 2020; Weyn et al., 2020).",
    "document": "Related Work\n\nAccurate downscaling translates coarse-resolution climate projections into localized variables needed for impact assessment. Methods vary in their fidelity to physical constraints and their robustness under nonstationary climates.\n\nMachine learning for climate downscaling encompasses statistical bias correction (Teutschbein and Seibert, 2012), analog and stochastic weather generators (Zorita and von Storch, 1999; Ailliot et al., 2015), convolutional super-resolution models (Vandal et al., 2017; Pan et al., 2019), and conditional generative models for precipitation fields (Ravuri et al., 2021; Leinonen et al., 2020). Temporal alignment and physical constraints have been incorporated through loss shaping and multi-task objectives (Beusch et al., 2020; Weyn et al., 2020).\n\nWe introduce a physics-guided diffusion model that enforces conservation laws via differentiable operators and demonstrate improved extrapolation to unseen warming scenarios.",
    "reason": "The span lists categories of methods with citations but does not relate them to the proposed physics-guided diffusion approach or specify the gap, meeting (a) and (b).",
    "start": 246,
    "end": 793,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent genome-wide association studies have established that attention-based models consistently outperform CNNs.",
    "document": "Related Work\n\nMachine learning for genomics has benefited from architectures that capture long-range dependencies in DNA and RNA sequences. Convolutional neural networks (CNNs) have been effective for motif discovery, while transformers promise better modeling of distal regulatory interactions. Hybrid architectures and pretraining on unlabeled sequences are active areas of exploration.\n\nRecent genome-wide association studies have established that attention-based models consistently outperform CNNs. Nevertheless, reported gains depend strongly on linkage disequilibrium correction, cohort size, and phenotype prevalence. In this study, we re-examine architectural trade-offs under matched training regimes and unified evaluation metrics, emphasizing calibration and out-of-cohort generalization.",
    "reason": "Mentions 'recent' studies and a comparative finding without any citations, violating rule (d) and requiring evidence for a domain-specific claim per rule (b).",
    "start": 390,
    "end": 503,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Morris et. al., 2017)",
    "document": "Introduction\n\nEvaluation of text generation has increasingly relied on reference-free metrics to better capture semantic adequacy (Novikova et al., 2017; Sellam et al., 2020). While token overlap remains informative, it often misses paraphrases and pragmatic nuances (Pham and Li, 2021).\n\nAdversarial test suites reveal brittleness in metrics when surface forms change despite preserved meaning (Morris et. al., 2017) and motivate embedding-based measures that align with human judgments (Zhang et al., 2020). We extend this line with a multi-facet scorer that jointly models entailment and fluency (Riley and Sun, 2022).",
    "reason": "Incorrect abbreviation 'et. al.'; should be 'et al.' without a period after 'et'.",
    "start": 395,
    "end": 417,
    "label": "Format"
  },
  {
    "span": "Transformer-based segmentation in medical imaging builds on hybrid CNN-Transformer encoders, hierarchical windows, and axial attention mechanisms (Chen et al., 2021; Hatamizadeh et al., 2022; Cao et al., 2021; Wang et al., 2022; Zhou et al., 2021).",
    "document": "Related Work\n\nMedical image segmentation has traditionally relied on encoder-decoder CNNs with skip connections, achieving strong results across modalities like CT and MRI. Recent advances adapt Transformer architectures to better capture long-range dependencies and global context, often improving robustness to anatomical variability.\n\nTransformer-based segmentation in medical imaging builds on hybrid CNN-Transformer encoders, hierarchical windows, and axial attention mechanisms (Chen et al., 2021; Hatamizadeh et al., 2022; Cao et al., 2021; Wang et al., 2022; Zhou et al., 2021). Parallel efforts integrate multi-scale tokenization, deformable attention, and lightweight adapters for resource-constrained settings.\n\nWe introduce a structure-aware token mixer that imposes organ-specific spatial priors via implicit shape fields, facilitating better boundary adherence under limited supervision.",
    "reason": "The sentence lists prior Transformer-based approaches without connecting them to the proposed method or stating the gap, thus lacking synthesis.",
    "start": 338,
    "end": 586,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Message passing neural networks aggregate neighbor features with learned transformations and non-linearities (Gilmer et al., 2017; Xu et al., 2019; Hu et al., 2020). Recent work explores positional encodings and spectral filters to enrich structure awareness (Dwivedi et al., 2020; Kreusen et al., 2021; Rampasek et al., 2022).",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become the de facto architecture for property prediction on molecular and materials graphs. Despite rapid progress, accurately modeling long-range interactions and stereochemistry remains challenging, particularly under scarce labeled data.\n\nSelf-supervised learning on graphs has shown promise by pretraining encoders with masking, contrastive, or generative objectives before fine-tuning on downstream tasks. However, the benefits vary significantly across datasets and are sensitive to augmentation choices.\n\nMessage passing neural networks aggregate neighbor features with learned transformations and non-linearities (Gilmer et al., 2017; Xu et al., 2019; Hu et al., 2020). Recent work explores positional encodings and spectral filters to enrich structure awareness (Dwivedi et al., 2020; Kreusen et al., 2021; Rampasek et al., 2022).\n\nWe propose a unified pretraining framework that couples distance-aware positional channels with chirality-sensitive subgraph denoising. Our design targets long-range coupling while remaining data-efficient.",
    "reason": "The span summarizes categories of prior GNN methods but does not connect them to the current work, delineate shortcomings, or synthesize insights about when or why they succeed; no explicit gap is articulated.",
    "start": 560,
    "end": 887,
    "label": "Lacks_synthesis"
  },
  {
    "span": "the SemEval-2019 Hyperpartisan News Detection shared task",
    "document": "Related Work\n\nResearch on misinformation has encompassed stance detection, fact verification, and stylistic analysis of biased content (Rashkin et al., 2017; Thorne et al., 2018). A related line targets hyperpartisan news, aiming to flag articles with extreme ideological slant. Notably, the SemEval-2019 Hyperpartisan News Detection shared task galvanized interest by providing a common evaluation protocol and dataset. While methods range from handcrafted linguistic cues to transformer-based encoders, questions remain about cross-source generalization and topic confounds (Schuster et al., 2019). Our approach addresses these issues with source-invariant adversarial training and calibrated thresholding.",
    "reason": "First mention of a specific shared task should include a citation to the task description; none is provided.",
    "start": 288,
    "end": 345,
    "label": "Unsupported_claim"
  },
  {
    "span": "In the context of signed languages, phonology typically distinguishes between manual features, such as usage, position and movement of hands and fingers, and non-manual features, such as facial expression",
    "document": "Introduction\n\nAround 200 languages in the world are signed rather than spoken, featuring their own vocabulary and grammatical structures. For example the American Sign Language (ASL) is not a mere translation of English into signs and is unrelated to the British Sign Language (BSL). This introduces many novel challenges to their automated processing. Research on Sign Language Processing (SLP) encompasses tasks such as sign language detection, i.e. recognising if and which signed language is performed (Moryossef et al., 2020) and sign language recognition (SLR) (Koller, 2020), i.e. the identification of signs either in isolation or in continuous speech. Other tasks concern the translation from signed to spoken (or written) (Camgoz et al.,2018) language or the production of signs from text (Rastgoo et al., 2021). With the recent success of deep learning-based approaches in computer vision (CV), as well as advancements in —from the CV perspective—related tasks of action and gesture recognition (Asadi-Aghbolaghi et al., 2017), SLP is gaining more attention in the CV community (Zheng et al., 2017).\n\nSome recent approaches to various SLP tasks rely on phonological features, perhaps due to the complexity of the tasks (Tornay, 2021; Metaxas et al., 2018; Gebre et al., 2013; avella et al., 2021). Surprisingly, however, little work has been carried out on explicitly modelling the phonology of signed languages. This presents a timely opportunity to investigate signed languages from a linguist’s perspective (Yin et al., 2021). In the context of signed languages, phonology typically distinguishes between manual features, such as usage, position and movement of hands and fingers, and non-manual features, such as facial expression. Sign language phonology is a matured field with well-developed\ntheoretical frameworks (Liddell and Johnson, 1989; Fenlon et al., 2017; Sandler, 2012). These phonological features, or phonemes, are drawn from a fixed inventory of possible configurations which is typically much smaller than the vocabulary of signed languages (Borg and Camilleri, 2020). For example, there is only a limited number of fingers\nthat can be used to perform a sign due to anatomical constraints. Hence, different signs share phonological properties and well performing classifiers can be used to predict those properties for signs unseen during training. This potentially holds even across different languages, because, while different languages may dictate different combinations\nof phonemes, there are also significant overlaps (Tornay et al., 2020).\n\nFinally, these phonological properties have a strong discriminatory power when determining signs. For example, in ASL-Lex (Caselli et al., 2017), a lexicon which also captures phonology information, the authors report that more than 50% of its 994 described signs have a unique combination of only six phonological properties and more than 80% of the signs share their combination with at most two other signs. By relying on additional (i.e., phonological) information from resources such as ASL-Lex, many signs can be determined from (predicted) phonological properties alone, without encountering them in training data. This is a capability that current data-driven approaches to SLR lack by design (Koller, 2020). Thus, in combination, mature approaches to phonology recognition can facilitate the development of sign language resources. This is an important task for both documenting low-resource sign languages as well as rapid developing of large-scale datasets, to fully harness data-driven CV approaches.\n\nTo spur research in this direction, we extend the preliminary work by Tavella et al. (2021) and introduce the task of Phonological Property Recognition (PPR). More specifically, this paper contributes (i) WLASLLex2001, a large-scale, automatically constructed PPR dataset, (ii) an analysis of the dataset quality, and (iii) an empirical study of the performance of different deep-learning based baselines thereon.\n\n ",
    "start": 1541,
    "end": 1745,
    "label": "Unsupported_claim"
  },
  {
    "span": "Several recent works use unit tests as implicit supervision for code generation.",
    "document": "Related Work\n\nPretrained language models specialized for source code have advanced program synthesis by learning from large corpora of repositories and issue discussions (Chen et al., 2021; Wang et al., 2021; Li et al., 2022). Beyond next-token prediction, constrained decoding and execution feedback have been explored to ensure semantic correctness and adherence to specifications (Le et al., 2016; Austin et al., 2021).\n\nSeveral recent works use unit tests as implicit supervision for code generation. This paradigm frames test execution signals as weak labels that guide search in the program space, often combined with iterative repair and self-refinement strategies.\n\nOur contribution unifies test-driven repair with contrastive alignment between natural language specifications and execution traces, improving generalization to unseen APIs and stricter test suites.",
    "reason": "The mention of 'several recent works' lacks any citations to those works, violating the requirement to cite prior studies when first mentioned.",
    "start": 424,
    "end": 504,
    "label": "Unsupported_claim"
  },
  {
    "span": "(2020, Zhao et al.)",
    "document": "Introduction\n\nProgram repair techniques increasingly leverage neural models to suggest patches from large code corpora. Early work framed the task as sequence-to-sequence translation (Tufano et al., 2019), while later systems integrate structural constraints from ASTs to improve syntactic correctness (Dinella et al., 2020). Beyond syntax, incorporating semantic signals from type checkers and tests further improves repair validity (Jiang et al., 2021). Prior studies (2020, Zhao et al.) find that training on mined bug–fix pairs yields strong in-distribution performance but struggles with out-of-project generalization.\n\nWe address this gap by proposing contrastive pretraining on mutation-generated negatives, which strengthens representations of error patterns and boosts cross-project robustness.",
    "reason": "Incorrect author–year order inside parentheses. It should be \"(Zhao et al., 2020)\" rather than \"(2020, Zhao et al.)\".",
    "start": 470,
    "end": 489,
    "label": "Format"
  },
  {
    "span": "(Miller et al., 2016 Smith et al., 2017)",
    "document": "Related Work\n\nSentiment analysis has evolved from lexicon-based methods to contextual representation learning (Pang and Lee, 2008; Socher et al., 2013; Devlin et al., 2019). Datasets such as movie and product reviews support benchmarking and domain adaptation (Blitzer et al., 2007). Recent datasets (Miller et al., 2016 Smith et al., 2017) expand coverage to multimodal and aspect-based settings. Our work investigates cross-domain generalization with robust training objectives (Sun et al., 2019; Hendrycks et al., 2020).",
    "reason": "Missing separator between multiple citations inside the same parentheses. A semicolon should separate them: '(Miller et al., 2016; Smith et al., 2017)'.",
    "start": 300,
    "end": 340,
    "label": "Format"
  },
  {
    "span": "Prior work has established that knowledge distillation consistently improves small transformer performance by 2–3 BLEU on translation.",
    "document": "Related Work\n\nModel compression for neural machine translation (NMT) aims to maintain quality while reducing inference costs. Knowledge distillation transfers behavior from a larger teacher to a smaller student, often enabling aggressive downsizing with limited degradation. Prior work has established that knowledge distillation consistently improves small transformer performance by 2–3 BLEU on translation. Complementary techniques, such as vocabulary pruning and quantization, further accelerate decoding but can interact non-trivially with distillation objectives, necessitating careful tuning.\n",
    "reason": "Presents a quantitative performance claim attributed to prior work without providing citations.",
    "start": 275,
    "end": 409,
    "label": "Unsupported_claim"
  },
  {
    "span": "Previous competitions have consistently reported that small object recall is the main bottleneck.",
    "document": "Related Work\n\nObject detection in aerial imagery presents unique challenges, including extreme scale variation, dense layouts, and high background clutter. Methods adapted from natural image detection often struggle to localize tiny objects and to suppress false positives in textured regions. Community benchmarks and challenges have spurred rapid progress by standardizing evaluation protocols and releasing large-scale aerial datasets. Previous competitions have consistently reported that small object recall is the main bottleneck. Approaches to mitigate this issue include multi-scale feature aggregation, super-resolution pre-processing, and specialized label assignment strategies for dense scenes. Nonetheless, detector calibration and uncertainty estimation for safety-critical applications remain underexplored.\n",
    "reason": "This refers to findings from unspecified competitions without providing citations to those events or reports (rule a regarding shared tasks/competitions).",
    "start": 439,
    "end": 536,
    "label": "Unsupported_claim"
  },
  {
    "span": "Fairness in recommender systems has been studied through exposure-aware ranking that balances utility with item provider parity. Post-processing re-ranking optimizes fairness metrics subject to minimal utility loss. Calibration techniques align recommendation distributions with user interests to reduce popularity bias. Graph-based debiasing adjusts embeddings to mitigate confounding from interaction logs. We address fairness at the platform level using a budgeted scheduler.",
    "document": "Related Work\n\nAs platforms mediate access to information and opportunity, fairness in recommendation has emerged as a multifaceted objective spanning users, items, and providers. Approaches differ in where fairness constraints are imposed and how they trade off with accuracy.\n\nFairness in recommender systems has been studied through exposure-aware ranking that balances utility with item provider parity. Post-processing re-ranking optimizes fairness metrics subject to minimal utility loss. Calibration techniques align recommendation distributions with user interests to reduce popularity bias. Graph-based debiasing adjusts embeddings to mitigate confounding from interaction logs. We address fairness at the platform level using a budgeted scheduler.\n\nWe study the policy impact on long-term exposure and cold-start creators in a simulated marketplace and with A/B tests.",
    "reason": "This span lists strands of fairness work and shifts to the authors' approach without explaining how platform-level budgeting fills a specific gap or synthesizing the differences relative to prior paradigms.",
    "start": 278,
    "end": 756,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Physics-informed neural networks enforce PDE residuals in the loss (Raissi et al., 2019; Karniadakis et al., 2021).",
    "document": "Related Work\nNeural surrogates for partial differential equations (PDEs) aim to reduce simulation costs in scientific computing. Methods vary in how they encode physical structure and boundary conditions.\n\nPhysics-informed neural networks enforce PDE residuals in the loss (Raissi et al., 2019; Karniadakis et al., 2021). Operator-learning approaches, such as neural operators and Fourier networks, learn solution maps across geometries (Li et al., 2020; Kovachki et al., 2021). Mesh-based graph networks incorporate discretization structure explicitly (Sanchez-Gonzalez et al., 2020; Pfaff et al., 2021). We evaluate models on elliptic and Navier–Stokes benchmarks.",
    "reason": "The sentence describes a method class with citations but does not connect it to the paper’s objectives, articulate limitations, or motivate the authors’ chosen approach.",
    "start": 206,
    "end": 321,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In the context of text generation, differential privacy has been enforced via gradient perturbation, private aggregation, and randomized response (Abadi et al., 2016; McMahan et al., 2018; Sheffet, 2017).",
    "document": "Related Work\n\nPrivacy in language technologies is essential when models memorize sensitive information present in training corpora. Differential privacy (DP) offers formal guarantees but often degrades utility when applied naively to high-capacity models.\n\nDP mechanisms for generation. In the context of text generation, differential privacy has been enforced via gradient perturbation, private aggregation, and randomized response (Abadi et al., 2016; McMahan et al., 2018; Sheffet, 2017). Complementary strategies consider post-hoc filtering, canary testing, and membership inference audits to assess leakage (Carlini et al., 2019; Shokri et al., 2017; Jagielski et al., 2020).\n\nWe examine privacy-utility trade-offs in instruction-tuned language models, proposing a calibration scheme that adapts privacy budgets to prompt sensitivity while preserving downstream helpfulness.",
    "reason": "The span lists DP techniques without explaining their trade-offs in text generation or connecting them to the authors’ calibration scheme or identified gap, showing a lack of synthesis.",
    "start": 287,
    "end": 491,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The SNLI dataset is the de facto benchmark for measuring pragmatic inference in dialog.",
    "document": "Related Work\n\nNatural language inference (NLI) has been proposed as a framework for reasoning over conversational content. The SNLI dataset is the de facto benchmark for measuring pragmatic inference in dialog. Prior efforts have also explored adapting entailment models to multi-turn contexts and dialog acts. However, dialog introduces phenomena such as coreference across turns, elliptical responses, and implicatures that are not fully captured by standard sentence-level NLI resources.\n\nRecent approaches extend NLI representations to dialog by incorporating speaker roles and discourse markers, but a comprehensive evaluation protocol for pragmatic inference in conversation remains under-specified.",
    "reason": "Asserts a specific role of a named dataset in a niche application (dialog pragmatic inference) without citing sources (rules a and b).",
    "start": 123,
    "end": 210,
    "label": "Unsupported_claim"
  },
  {
    "span": "Vastwani et al. 1",
    "document": "Introduction\n\nVision Transformers (ViTs) have demonstrated state-of-the-art performance on image classification by scaling data and compute. However, their data-hungry nature has spurred research on inductive biases and training protocols tailored to medium-sized datasets. Vastwani et al. 1 propose hierarchical token pooling to improve sample efficiency, while others incorporate convolutional stems and locality-aware attention (Park and Srinivas, 2021; Huang et al., 2022). In contrast, we study curriculum-based token pruning that adapts the compute budget to the difficulty of the input.",
    "reason": "Wrong use of footnotes/numbering within a citation: includes a superscript-like number without a proper citation format or year. Should include the year or be reformatted as a proper footnote or standard author-year citation (e.g., \"Vastwani et al. (2021)\").",
    "start": 274,
    "end": 291,
    "label": "Format"
  },
  {
    "span": "Wang et al. 1",
    "document": "Related Work\n\nSelf-training. Prior work by Wang et al. 1 proposes iterative pseudo-labeling for sequence tagging under limited supervision. Follow-up studies introduced confidence-aware filtering to reduce noise (Arazo et al., 2020) and teacher–student consistency to stabilize training (Tarvainen and Valpola, 2017). However, pseudo-label drift can still degrade performance over long horizons.\n\nConsistency regularization. Approaches based on perturbation invariance (Sohn et al., 2020) have proven effective, but their performance depends on high-quality augmentations that preserve semantics.",
    "reason": "Wrong use of footnotes: '1' appears after the author name without year or proper footnote formatting; should include the year or be formatted as a proper footnote/citation.",
    "start": 43,
    "end": 56,
    "label": "Format"
  },
  {
    "span": "Garcia et al. 1",
    "document": "Introduction\n\nEstimating heterogeneous treatment effects (HTE) is central to personalized decision making (Athey and Imbens, 2016; Wager and Athey, 2018). Representation learning for counterfactual inference aims to balance covariates across treatment groups (Johansson et al., 2016; Shalit et al., 2017). Propensity-weighted objectives remain sensitive to model misspecification (Austin, 2011), while doubly robust estimators mitigate bias by combining outcome and propensity models (Funk et al., 2011). Garcia et al. 1 show that monotonicity constraints improve identifiability in observational studies, and subsequent work extends these ideas to instrumental variables (Hartford et al., 2017). We build on this by introducing structured sparsity that captures shared effect modifiers across related interventions, improving sample efficiency in the small-n regime.",
    "reason": "Improper use of a footnote-style marker without a year; the citation should include a year or be formatted as a proper footnote/reference.",
    "start": 505,
    "end": 520,
    "label": "Format"
  },
  {
    "span": "Kim et al.",
    "document": "Introduction\n\nNeural program synthesis aims to induce programs from examples or natural language specifications. Early neural systems struggled with compositional generalization and data scarcity (Jain and Roy, 2017; Alvarez and Stone, 2018). Building on these insights, Kim et al. propose a decoder architecture with explicit copying from the specification, enabling better alignment with input constraints. Subsequent work incorporated symbolic executors to validate partial programs during decoding (Lu and Patel, 2019) and leveraged contrastive objectives to separate spurious from correct traces (Morgan and Xu, 2020). In this paper, we introduce a modular pretraining scheme that unifies specification parsing and sketch prediction (Lin and Torres, 2021).",
    "reason": "Narrative citation is missing the year; it should appear as 'Kim et al. (YEAR)' in author–year style, e.g., 'Kim et al. (2018)'.",
    "start": 271,
    "end": 281,
    "label": "Format"
  },
  {
    "span": "(Brown 2020)",
    "document": "Related Work\n\nLarge-scale language models pre-trained on diverse corpora demonstrate strong few-shot performance via in-context learning. Early transformer architectures scaled depth and width to improve perplexity and downstream accuracy, while later work emphasized data curation and instruction tuning to reduce hallucinations and toxicity. Several studies examine emergent capabilities as a function of scale (Brown 2020) and analyze the trade-offs between parameter count and training tokens (Kaplan et al., 2020; Hoffmann et al., 2022). Safety-focused research proposes reinforcement learning from human feedback to align model behavior with user preferences (Ouyang et al., 2022).",
    "reason": "Missing comma between author and year in a parenthetical citation; it should be “(Brown, 2020)”.",
    "start": 413,
    "end": 425,
    "label": "Format"
  },
  {
    "span": "Unsupervised domain adaptation for medical segmentation employs adversarial feature alignment, self-ensembling, and test-time adaptation to mitigate domain shift (Kamnitsas et al., 2017; French et al., 2018; Wang et al., 2021). Style transfer and image-to-image translation further reduce appearance gaps across scanners and protocols (Zhu et al., 2017; Chartsias et al., 2017).",
    "document": "Introduction\n\nMedical image segmentation models often fail to generalize across institutions due to scanner differences, protocols, and population shift. Overcoming this domain shift is critical to ensuring reliable clinical performance without exhaustive relabeling.\n\nUnsupervised domain adaptation for medical segmentation employs adversarial feature alignment, self-ensembling, and test-time adaptation to mitigate domain shift (Kamnitsas et al., 2017; French et al., 2018; Wang et al., 2021). Style transfer and image-to-image translation further reduce appearance gaps across scanners and protocols (Zhu et al., 2017; Chartsias et al., 2017).\n\nWe propose Anatomy-Preserved Adaptation (APA), which constrains adaptation with shape priors and cycle-consistent geometry, maintaining anatomical fidelity while aligning low-level appearance. APA integrates uncertainty-aware pseudo-labeling to filter unreliable targets.\n\nAcross MRI and CT benchmarks with cross-site shifts, APA improves Dice and boundary F1 while preserving anatomical plausibility. We include qualitative analyses of failure cases and sensitivity to prior strength.",
    "reason": "The span lists prior adaptation techniques and style transfer methods without clarifying how they motivate or differ from the proposed APA, providing no explicit gap or author stance, hence lacking synthesis (criteria a and c).",
    "start": 269,
    "end": 647,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Our method aligns with standard practices in educational measurement, which recommend four levels of reliability analysis.",
    "document": "Related Work\n\nAssessment design demands both validity and reliability considerations to ensure interpretable scores. In NLP for education, researchers often adopt psychometric frameworks to evaluate consistency and fairness of automated judgments. Our method aligns with standard practices in educational measurement, which recommend four levels of reliability analysis. Prior computational studies have adapted these practices unevenly, emphasizing internal consistency while overlooking test–retest stability and inter-rater calibration drift. We synthesize these dimensions into a unified evaluation suite for language-based assessments.",
    "reason": "Asserts a field-standard guideline ('four levels of reliability analysis') without citing authoritative sources from educational measurement.",
    "start": 248,
    "end": 370,
    "label": "Unsupported_claim"
  },
  {
    "span": "Our dataset is the first open benchmark for sub-seasonal to seasonal precipitation forecasting.",
    "document": "Introduction\n\nSub-seasonal to seasonal (S2S) precipitation forecasting sits between weather and climate scales and is crucial for water resource planning, agriculture, and disaster preparedness. Traditional dynamical models struggle with initialization errors and model bias, motivating hybrid and data-driven approaches that incorporate teleconnection indices and reanalysis fields.\n\nOur dataset is the first open benchmark for sub-seasonal to seasonal precipitation forecasting. It standardizes input variables, lead times, and evaluation metrics across multiple regions, enabling reproducible comparison of statistical, machine learning, and dynamical baselines. We further provide baseline implementations and a unified scoring toolkit to facilitate community adoption and longitudinal tracking of progress.",
    "reason": "Asserts 'first' status for a benchmark without citing surveys or existing resources to justify the novelty claim.",
    "start": 385,
    "end": 480,
    "label": "Unsupported_claim"
  },
  {
    "span": "LoRA has become the de facto standard for parameter-efficient finetuning",
    "document": "Related Work\n\nParameter-efficient finetuning (PEFT) seeks to adapt large language models with minimal trainable parameters, enabling rapid iteration and deployment (Houlsby et al., 2019; Zaken et al., 2022). Techniques include adapters, prefix/prompt tuning, and low-rank updates, each with trade-offs in capacity, inference overhead, and stability (Li and Liang, 2021; Lester et al., 2021). LoRA has become the de facto standard for parameter-efficient finetuning due to its favorable compute/quality trade-offs and compatibility with modern transformer stacks.\n\nRecent analyses investigate interactions between PEFT methods and scaling laws, as well as their robustness under distribution shift and catastrophic forgetting (He et al., 2022; Dettmers et al., 2023). Our study complements this line of work with a systematic comparison across instruction-following, retrieval-augmented generation, and safety alignment tasks.",
    "reason": "Asserts a field-wide status ('de facto standard') for LoRA without any citation to support the claim; first mention should include evidence per rule (b).",
    "start": 392,
    "end": 464,
    "label": "Unsupported_claim"
  },
  {
    "span": " (O'Neil et al 2021)",
    "document": "Introduction\n\nFairness in ranking has been studied through notions such as demographic parity and exposure fairness (Dwork et al., 2012; Biega et al., 2018) as well as equalized error rates (Hardt et al., 2016). Feedback loops in recommender systems can amplify disparities (Chaney et al., 2018). For comprehensive audits, see (O'Neil et al 2021) for a multi-stakeholder framework that emphasizes documentation and process accountability.",
    "reason": "Improper author–date formatting: missing period after \"al.\" and missing comma before the year; should be \"(O'Neil et al., 2021)\".",
    "start": 326,
    "end": 346,
    "label": "Format"
  },
  {
    "span": "Patel et al. 2020",
    "document": "Related Work\n\nNeural recommender systems have evolved from shallow matrix factorization to deep architectures that capture complex user-item interactions (He et al., 2017; Zhang et al., 2019). Context-aware models enrich representations with side information such as time and location (Xiang et al., 2010; Hidasi et al., 2016). Several works (O'Neil, 2017; Patel et al. 2020; Zhang and Li, 2021) examine the role of calibration and uncertainty, arguing that confidence-aware ranking improves long-term satisfaction. Sequence-based recommenders model evolving preferences using attention over historical behaviors (Kang and McAuley, 2018; Sun et al., 2019).",
    "reason": "Missing comma before the year inside a parenthetical citation; should be Patel et al., 2020.",
    "start": 357,
    "end": 374,
    "label": "Format"
  },
  {
    "span": "It is well-known that non-IID data severely degrades FedAvg convergence in real-world deployments.",
    "document": "Introduction\n\nFederated learning enables on-device training while preserving data locality, but heterogeneity in client data and compute complicates optimization and evaluation. It is well-known that non-IID data severely degrades FedAvg convergence in real-world deployments. Numerous strategies address heterogeneity, including server-side regularization, client clustering, adaptive aggregation, and personalized objectives. Despite progress, current algorithms often assume stable client participation and overlook resource-constrained scheduling. We propose a stratified aggregation scheme with adaptive client sampling that explicitly models availability and statistical skew, improving both convergence speed and fairness.",
    "reason": "Asserts a commonly cited effect in the literature without providing any supporting references (rule b and e).",
    "start": 178,
    "end": 276,
    "label": "Unsupported_claim"
  },
  {
    "span": "Garcia et al.",
    "document": "Related Work\n\nEstimating causal effects from observational data commonly relies on unconfoundedness and overlap, operationalized via matching, weighting, or doubly robust estimation (Rosenbaum and Rubin, 1983; Robins et al., 1994; Imbens and Rubin, 2015). Building on propensity score methods, Garcia et al. introduce balancing representations that minimize covariate shift between treated and control groups (Johansson et al., 2016; Shalit et al., 2017). Recent work leverages invariances across environments to reduce sensitivity to spurious correlations (Peters et al., 2016; Magliacane et al., 2018).",
    "reason": "Narrative citation missing year; should be Garcia et al. (YEAR).",
    "start": 294,
    "end": 307,
    "label": "Format"
  },
  {
    "span": "Matrix factorization decomposes the user-item matrix into latent factors (Koren et al., 2009). Session-based recommenders model short-term intent with recurrent networks (Hidasi et al., 2015). Graph-based recommenders propagate signals across interaction graphs (Wu et al., 2019). Causal inference controls for exposure bias using propensity scores (Schnabel et al., 2016).",
    "document": "Related Work\n\nRecommendation research spans collaborative filtering, sequence modeling of sessions, and graph-based methods that leverage higher-order connectivity. Another line examines selection bias and confounding in logged interaction data.\n\nMatrix factorization decomposes the user-item matrix into latent factors (Koren et al., 2009). Session-based recommenders model short-term intent with recurrent networks (Hidasi et al., 2015). Graph-based recommenders propagate signals across interaction graphs (Wu et al., 2019). Causal inference controls for exposure bias using propensity scores (Schnabel et al., 2016).\n\nWe integrate user-intent modeling with debiasing to improve ranking under covariate shift.",
    "reason": "The paragraph lists disparate recommender approaches and a causal inference method without transitions or an explanation of how they relate, resulting in abrupt shifts between topics.",
    "start": 247,
    "end": 620,
    "label": "Coherence"
  },
  {
    "span": "Dosovitskiy et al. (2021) propose Vision Transformers trained on large datasets. He et al. (2016) introduce residual networks that enable very deep convolutional models. Touvron et al. (2021) refine data-efficient vision transformers. Tan and Le (2019) scale CNNs with compound scaling.",
    "document": "Related Work\n\nTransformers and Convolutional Networks for Vision\n\nWe review architectures relevant to our hybrid design. Dosovitskiy et al. (2021) propose Vision Transformers trained on large datasets. He et al. (2016) introduce residual networks that enable very deep convolutional models. Touvron et al. (2021) refine data-efficient vision transformers. Tan and Le (2019) scale CNNs with compound scaling. Beyond architectures, training strategies such as strong augmentation and regularization are known to matter (Cubuk et al., 2019; Zhang et al., 2018).\n\nWe concentrate on resource-limited regimes and study how token sparsity interacts with multi-scale features.",
    "reason": "The paragraph lists disparate architectures with no connective explanations or transitions, leaving the relation between ViTs and CNNs implicit and unclear.",
    "start": 121,
    "end": 407,
    "label": "Coherence"
  },
  {
    "span": "The most widely used augmentation pipeline includes Gaussian blur with probability 0.5 and solarization.",
    "document": "Related Work\n\nSelf-supervised representation learning in vision has been driven by contrastive and non-contrastive objectives that leverage data augmentations to define invariances (He et al., 2020; Chen et al., 2020; Grill et al., 2020). Augmentations such as random cropping, color jitter, and horizontal flipping have proven critical for learning useful invariances, while architectural choices and temperature parameters strongly affect downstream transfer.\n\nBeyond these basics, practitioners have proposed more complex transformations to improve invariance to illumination and sensor artifacts. The most widely used augmentation pipeline includes Gaussian blur with probability 0.5 and solarization. However, the effect sizes of each component are not consistently reported across studies, making it difficult to isolate which augmentations are essential.",
    "reason": "Asserts a field-wide practice and specific probabilities about augmentation without citing any source (rule b: niche, specific detail; also d if implying 'widely used' by recent works).",
    "start": 601,
    "end": 705,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works explore prompt design, in-context learning strategies, and lightweight adapters for code generation (Brown et al., 2020; Chen et al., 2021; Austin et al., 2021; Liu et al., 2022; Fried et al., 2023).",
    "document": "Related Work\n\nLarge language models have demonstrated strong performance on code synthesis benchmarks by leveraging natural language intent and learned program patterns. Despite these advances, consistency and reliability across tasks and repositories remain challenging.\n\nRecent works explore prompt design, in-context learning strategies, and lightweight adapters for code generation (Brown et al., 2020; Chen et al., 2021; Austin et al., 2021; Liu et al., 2022; Fried et al., 2023).\n\nBenchmarks such as HumanEval and MBPP emphasize short problems, while real-world repositories exhibit longer dependencies and tool interaction. We focus on code generation with tool-use constraints and longitudinal dependency tracking.",
    "reason": "Provides a citation list about prompting and adapters without clarifying their relationship to the current study or articulating a gap in that passage (criteria a and c).",
    "start": 273,
    "end": 485,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The majority of open-source VLMs cannot follow multi-turn instructions involving both chart images and text.",
    "document": "Related Work\n\nVision-language models (VLMs) increasingly leverage large language models to perform instruction following with visual inputs (Li et al., 2023; Liu et al., 2023). Training recipes combine image-text pretraining, synthetic instruction tuning, and step-wise reasoning (Zhang et al., 2023; Dai et al., 2023). Specialized datasets target document and chart understanding with structured outputs (Mathew et al., 2021; Kantharaj et al., 2022).\n\nThe majority of open-source VLMs cannot follow multi-turn instructions involving both chart images and text. We address this gap with a hierarchical dialogue curriculum that composes chart parsing, entailment, and tool-use skills under a unified conversational interface.",
    "reason": "Claims a majority limitation across open-source models without citations or empirical evidence (rule b/d).",
    "start": 453,
    "end": 561,
    "label": "Unsupported_claim"
  },
  {
    "span": "(O'Connor 2015)",
    "document": "Related Work\n\nDebate summarization has leveraged argument mining to identify claims and premises (Lippi and Torroni, 2016). Early extractive systems optimized coverage and non-redundancy (Carbonell and Goldstein, 1998), while neural models capture discourse cues (Xu et al., 2020). Stance-aware summarizers integrate sentiment and topic modeling to balance perspectives (Chen et al., 2019). The role of speaker turns in coherence was highlighted by (O'Connor 2015), inspiring segmentation strategies that preserve rhetorical flow. We build on these insights with a hierarchical encoder that models speaker transitions explicitly.",
    "reason": "Missing comma between author and year in an author–year parenthetical citation; should be '(O'Connor, 2015)'.",
    "start": 449,
    "end": 464,
    "label": "Format"
  },
  {
    "span": "Graph-based approaches have become popular for traffic forecasting, including diffusion convolutional models (Li et al., 2018), spatio-temporal graph convolutional networks (Yu et al., 2018; Wu et al., 2019), attention-based graph models (Zheng et al., 2020), and adaptive adjacency methods that learn the graph structure (Bai et al., 2020; Song et al., 2020). Other lines incorporate external factors such as weather and events (Pan et al., 2019; Jia et al., 2021) or exploit multi-resolution temporal patterns (Guo et al., 2019; Cai et al., 2020).",
    "document": "Related Work\n\nUrban traffic forecasting has evolved from classical autoregressive models to deep spatio-temporal neural architectures that exploit the topology of road networks and the non-stationary nature of demand. Public benchmarks and sensor networks have catalyzed interest in methods that can scale and adapt to changing conditions.\n\nGraph-based approaches have become popular for traffic forecasting, including diffusion convolutional models (Li et al., 2018), spatio-temporal graph convolutional networks (Yu et al., 2018; Wu et al., 2019), attention-based graph models (Zheng et al., 2020), and adaptive adjacency methods that learn the graph structure (Bai et al., 2020; Song et al., 2020). Other lines incorporate external factors such as weather and events (Pan et al., 2019; Jia et al., 2021) or exploit multi-resolution temporal patterns (Guo et al., 2019; Cai et al., 2020).\n\nBeyond these neural methods, probabilistic state-space models and hybrid systems have also been proposed to capture uncertainty and regime shifts (Salinas et al., 2020; Rangapuram et al., 2018). In this work, we study topology adaptation under distribution shift with a lightweight meta-learning scheme.\n",
    "reason": "Violates (a) and (c): the paragraph lists prior works without explaining how they relate to the authors' objectives or perspective, nor does it articulate the motivation or connection to the proposed approach.",
    "start": 341,
    "end": 890,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Smith et al.",
    "document": "Related Work\n\nSemi-supervised learning has long sought to exploit the structure of unlabeled data to improve predictive performance when labeled examples are scarce (Zhu, 2005). Early consistency-regularization methods such as the Π-model and Mean Teacher demonstrated the benefits of enforcing prediction stability under input perturbations (Laine and Aila, 2017; Tarvainen and Valpola, 2017). Following Smith et al., we adopt a consistency-based objective with strong data augmentation to encourage invariance across transformations. Later works like MixMatch and FixMatch further integrated pseudo-labeling and sharpening to achieve state-of-the-art results on standard benchmarks (Berthelot et al., 2019; Sohn et al., 2020). Recent advances also leverage stronger augmentations and distribution alignment to reduce confirmation bias in pseudo-labeling (Pham et al., 2021).",
    "reason": "Narrative citation missing year; should be formatted as “Smith et al. (YEAR)” rather than omitting the year.",
    "start": 405,
    "end": 417,
    "label": "Format"
  },
  {
    "span": "Recent works have shown that automatic short answer grading can rival human raters on several datasets.",
    "document": "Introduction\n\nAutomating the evaluation of constructed responses promises faster feedback cycles and reduced instructor workload in large-scale courses (Burstein et al., 2001; Dikli, 2006). Short answer grading (SAG) poses unique challenges due to lexical variability and the need to assess partial correctness beyond exact matches. Recent works have shown that automatic short answer grading can rival human raters on several datasets. However, reliability across prompts and fairness across student populations remain open concerns, motivating models that incorporate semantic reasoning and calibration. We present a contrastive learning framework that aligns reference-based and rubric-based signals to improve generalization across prompts.",
    "reason": "Claims findings from unspecified 'recent works' without providing citations, which is required for such mentions.",
    "start": 333,
    "end": 436,
    "label": "Unsupported_claim"
  },
  {
    "span": "Secure aggregation prevents servers from accessing individual updates (Bonawitz et al., 2017). Differentially private SGD introduces calibrated noise to gradients (Abadi et al., 2016). Personalization layers adapt global models to client heterogeneity (Arivazhagan et al., 2019).",
    "document": "Introduction\n\nFederated learning enables on-device training with privacy and scalability benefits. Achieving strong accuracy while preserving privacy and accommodating heterogeneity is an open problem.\n\nSecure aggregation prevents servers from accessing individual updates (Bonawitz et al., 2017). Differentially private SGD introduces calibrated noise to gradients (Abadi et al., 2016). Personalization layers adapt global models to client heterogeneity (Arivazhagan et al., 2019). We present a method that balances privacy budgets and personalization through adaptive clipping.\n\nOur analysis characterizes privacy-utility trade-offs under skewed participation patterns.",
    "reason": "These sentences cover secure aggregation, DP-SGD, and personalization without explaining how they relate; the transitions are missing and coherence relies on implicit association.",
    "start": 203,
    "end": 482,
    "label": "Coherence"
  },
  {
    "span": "Time-series anomaly detection approaches can be categorized into reconstruction-based autoencoders, prediction-based models, and density estimation techniques. Recent advances introduce transformers for long-range dependencies, graph neural networks for multivariate correlations, and contrastive objectives for representation learning. Streaming scenarios motivate memory modules and dynamic thresholds for online detection.",
    "document": "Introduction\nDetecting anomalies in multivariate sensor streams is critical for monitoring industrial systems and IoT deployments. Non-stationarity, noise, and scarce labels complicate supervised learning, motivating self-supervised and unsupervised methods.\n\nRelated Work\nClassical methods relied on statistical control charts and ARIMA residual analysis, while deep learning expanded model capacity and flexibility.\nTime-series anomaly detection approaches can be categorized into reconstruction-based autoencoders, prediction-based models, and density estimation techniques. Recent advances introduce transformers for long-range dependencies, graph neural networks for multivariate correlations, and contrastive objectives for representation learning. Streaming scenarios motivate memory modules and dynamic thresholds for online detection.\nBenchmarking practices vary widely in windowing, scoring, and delay penalties, making cross-paper comparisons difficult.\n\nOur Approach\nWe present an online detector with adaptive context windows and calibrated scoring for delayed anomalies.",
    "reason": "The span lists categories and trends but does not synthesize how they relate to the paper's online detector or articulate remaining gaps, fulfilling (a) and (c).",
    "start": 418,
    "end": 843,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Nguyen and Patel, 2018.",
    "document": "Introduction\n\nKnowledge distillation transfers supervision from a large teacher to a compact student model (Hinton et al., 2015). Numerous variants adjust the distillation target, temperature, or feature matching (Romero et al., 2015; Zagoruyko and Komodakis, 2017). While cross-entropy with softened logits is standard, recent work considers task-agnostic distillers for multitask settings (Clark et al., 2019). However, the stability of online distillation with co-training remains underexplored, especially under distribution shift as noted in (Nguyen and Patel, 2018. We investigate this setting with curriculum schedules and analyze gradient interference between tasks.",
    "reason": "Missing closing parenthesis in the citation; should be (Nguyen and Patel, 2018).",
    "start": 547,
    "end": 571,
    "label": "Format"
  },
  {
    "span": "we follow the protocol of the GLUE diagnostic set for probing linguistic phenomena",
    "document": "Introduction\n\nEvaluating generalization in natural language understanding requires diverse tasks and targeted diagnostics. Composite benchmarks aggregate multiple classification and inference datasets to assess broad capabilities. In addition to aggregate accuracy, we examine model behavior on controlled tests of linguistic competence; specifically, we follow the protocol of the GLUE diagnostic set for probing linguistic phenomena. Our goal is to identify training strategies that improve both average performance and targeted robustness without increasing model capacity.\n",
    "reason": "Mentions a specific evaluation dataset and protocol (GLUE diagnostic set) without citing the benchmark’s original paper or documentation.",
    "start": 352,
    "end": 434,
    "label": "Unsupported_claim"
  },
  {
    "span": "The FLORES-200 benchmark has become the de facto standard for evaluating massively multilingual systems.",
    "document": "Introduction\n\nLow-resource machine translation (MT) must cope with scarce parallel data and diverse typologies. Techniques include transfer learning from high-resource languages, multilingual training, and back-translation (Zoph et al., 2016; Johnson et al., 2017; Sennrich et al., 2016). Pretrained sequence-to-sequence models such as mBART and mT5 provide strong initialization for many language pairs (Liu et al., 2020; Xue et al., 2021).\n\nCommunity benchmarks are critical for fair comparison across languages. The FLORES-200 benchmark has become the de facto standard for evaluating massively multilingual systems. Nonetheless, reporting practices still vary, with some works omitting trust-region confidence intervals in BLEU.\n\nWe advocate standardized reporting with language-level uncertainty and propose a robust averaging metric sensitive to low-resource variance.",
    "reason": "Introduces a specific benchmark at first mention and asserts its status without citing its release or overview paper (rule a and b).",
    "start": 515,
    "end": 619,
    "label": "Unsupported_claim"
  },
  {
    "span": "Earlier shared tasks on code-mixed POS tagging focused primarily on Hindi–English and Spanish–English.",
    "document": "Related Work\n\nCode-mixed language processing presents unique challenges due to orthographic variation, transliteration, and rapid language switching. Sequence labeling methods leveraging character-level CNNs and BiLSTMs have been effective for POS tagging under data scarcity (Plank et al., 2016; Aguilar et al., 2018). Subword-aware transformers further improve robustness to mixed scripts. Earlier shared tasks on code-mixed POS tagging focused primarily on Hindi–English and Spanish–English. However, comparatively less attention has been paid to typologically distant pairs and low-resource scripts, creating a skew in model generalization.\n",
    "reason": "Mentions earlier shared tasks and specific language pairs without any citations to the tasks or datasets.",
    "start": 392,
    "end": 494,
    "label": "Unsupported_claim"
  },
  {
    "span": "The MIMIC-IV Notes subset has become the de facto benchmark for clinical summarization.",
    "document": "Introduction\n\nClinical note summarization seeks to condense lengthy electronic health record (EHR) narratives into concise, actionable synopses for downstream care. The task presents unique challenges due to heterogeneous note types, domain-specific terminology, and substantial variability across institutions. The MIMIC-IV Notes subset has become the de facto benchmark for clinical summarization. However, models trained on single-institution data often overfit to idiosyncratic documentation styles, limiting generalizability. To address this, we investigate cross-hospital adaptation and evaluate whether domain-adaptive pretraining on heterogeneous corpora can mitigate style overfitting while preserving faithfulness to the source notes.\n",
    "reason": "Declares a dataset as the de facto benchmark without citing any sources or evidence.",
    "start": 312,
    "end": 399,
    "label": "Unsupported_claim"
  },
  {
    "span": "Vision Transformers have been applied to medical image classification, segmentation, and detection across X-ray, CT, and MRI modalities (Dosovitskiy et al., 2021; Chen et al., 2021; Hatamizadeh et al., 2022; Tang et al., 2022; Cao et al., 2022). Hybrid CNN–Transformer backbones, hierarchical tokens, and windowed attention improve efficiency and resolution handling (Liu et al., 2021; Wang et al., 2021; Li et al., 2022).",
    "document": "Related Work\n\nMedical imaging models must reconcile limited labeled data with high-resolution, modality-specific structures. Transformers offer long-range context modeling but face efficiency and data scarcity challenges in clinical settings.\n\nVision Transformers have been applied to medical image classification, segmentation, and detection across X-ray, CT, and MRI modalities (Dosovitskiy et al., 2021; Chen et al., 2021; Hatamizadeh et al., 2022; Tang et al., 2022; Cao et al., 2022). Hybrid CNN–Transformer backbones, hierarchical tokens, and windowed attention improve efficiency and resolution handling (Liu et al., 2021; Wang et al., 2021; Li et al., 2022).\n\nWe present a token-pruning strategy guided by uncertainty estimates for 3D volumes. Experiments on lung nodule detection and brain tumor segmentation demonstrate improved latency–accuracy trade-offs.",
    "reason": "The span summarizes prior applications of Vision Transformers but does not connect them to the proposed token-pruning approach or identify a concrete gap they fail to address; it lacks explicit synthesis.",
    "start": 244,
    "end": 666,
    "label": "Lacks_synthesis"
  },
  {
    "span": "To the best of our knowledge, no publicly available dataset exists for low-resource sign-to-text translation in Yoruba.",
    "document": "Introduction\n\nSign language processing has advanced rapidly with the advent of deep neural architectures and larger video corpora for recognition and translation tasks (Koller et al., 2019; Camgoz et al., 2018). While progress has been notable in high-resource languages and standardized sign varieties, research on African sign languages remains limited and fragmented (Saunders et al., 2020). To the best of our knowledge, no publicly available dataset exists for low-resource sign-to-text translation in Yoruba. This gap complicates benchmarking and hinders reproducibility for models targeting regional sign varieties. In this paper, we introduce a new collection effort and propose a baseline for sign-to-text mapping with curriculum learning and domain adaptation techniques inspired by prior cross-lingual transfer methods (Conneau et al., 2020). Our contributions include a pilot corpus, an evaluation suite with linguistically informed gloss annotations, and analyses of transfer from related sign varieties.",
    "reason": "Claims a non-existence of datasets in a specific niche area without providing any citation or evidence to substantiate the survey of prior work.",
    "start": 395,
    "end": 514,
    "label": "Unsupported_claim"
  },
  {
    "span": "The KDD'99 dataset suffers from severe class imbalance.",
    "document": "Introduction\n\nIntrusion detection systems (IDS) increasingly leverage machine learning to detect anomalous network behaviors beyond signature-based rules (Sommer and Paxson, 2010). Supervised detectors require representative training data, yet network traffic exhibits non-stationarity and heavy-tailed distributions that complicate modeling.\n\nThe KDD'99 dataset suffers from severe class imbalance. Moreover, evaluation practices often ignore temporal leakage and overlapping flows, inflating reported accuracy. To address these issues, we introduce a temporally aware benchmarking protocol and rebalanced training objectives tailored to rare attack types.\n\nOur contributions include a leakage-free split, a cost-sensitive loss calibrated to attack priors, and a comprehensive comparison across classical and deep IDS models.",
    "reason": "A claim about specific dataset properties (class imbalance) should be supported with a citation or evidence (violates rule b).",
    "start": 344,
    "end": 399,
    "label": "Unsupported_claim"
  },
  {
    "span": "Exploration in reinforcement learning has been approached via intrinsic rewards, optimism in the face of uncertainty, count-based bonuses, and curiosity-driven objectives (Bellemare et al., 2016; Pathak et al., 2017; Osband et al., 2016). In this paper, we present a new exploration bonus based on density models.",
    "document": "Related Work\n\nSparse-reward environments demand efficient exploration strategies that avoid excessive dithering and reduce sample complexity.\n\nExploration in reinforcement learning has been approached via intrinsic rewards, optimism in the face of uncertainty, count-based bonuses, and curiosity-driven objectives (Bellemare et al., 2016; Pathak et al., 2017; Osband et al., 2016). In this paper, we present a new exploration bonus based on density models.\n\nWe instantiate our idea with a lightweight kernel density surrogate and integrate it into an off-policy actor-critic algorithm. Empirically, our method yields faster discovery of reward states on hard Atari games and continuous-control tasks.",
    "reason": "The span transitions from a list of prior approaches directly to the authors’ method without identifying a concrete gap or explaining why a density-based bonus is needed.",
    "start": 143,
    "end": 456,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Dosovitskiy et al. (2021) show that image classification can work with pure transformers. UNet remains a strong baseline for biomedical segmentation (Ronneberger et al., 2015). Swin Transformer adapts hierarchical windows for dense prediction (Liu et al., 2021). Radiology datasets exhibit domain shift across scanners (Zech et al., 2018).",
    "document": "Related Work\n\nMedical image segmentation has benefited from convolutional architectures and, more recently, transformer-based designs that capture long-range dependencies. However, domain shift across institutions, scanners, and protocols challenges the robustness and calibration of these models in clinical practice.\n\nDosovitskiy et al. (2021) show that image classification can work with pure transformers. UNet remains a strong baseline for biomedical segmentation (Ronneberger et al., 2015). Swin Transformer adapts hierarchical windows for dense prediction (Liu et al., 2021). Radiology datasets exhibit domain shift across scanners (Zech et al., 2018).\n\nOur work builds a domain-aware segmentation transformer with lightweight adapters, targeting generalization under cross-site shifts while preserving data efficiency.",
    "reason": "The span lists multiple works back-to-back without transitions or clarification of how a classification result, UNet baseline, Swin, and dataset shift findings relate to each other, creating abrupt, unconnected statements across several sentences.",
    "start": 320,
    "end": 659,
    "label": "Coherence"
  },
  {
    "span": "The widely used MIMIC-III dataset contains over 60,000 ICU stays.",
    "document": "Introduction\n\nPredictive modeling with electronic health records (EHR) enables early warning systems for critical outcomes such as in-hospital mortality, prolonged length of stay, and readmission (Rajkomar et al., 2018; Harutyunyan et al., 2019). Benchmark datasets from intensive care units (ICUs) have catalyzed research in representation learning and clinical sequence modeling (Johnson et al., 2016; Pollard et al., 2018). Despite progress, label sparsity, missingness patterns, and distribution shift across hospitals remain persistent challenges (Shickel et al., 2018; Purushotham et al., 2018).\n\nThe widely used MIMIC-III dataset contains over 60,000 ICU stays. We focus on early mortality prediction within the first 24 hours using heterogeneous time series of vitals, labs, and interventions, and we compare calibrated uncertainty estimates across architectures. Our results highlight gains from self-supervised pretraining and per-variable imputation strategies.",
    "reason": "This is a quantitative claim about a specific dataset’s size and should be supported with a citation to the dataset paper or documentation.",
    "start": 603,
    "end": 668,
    "label": "Unsupported_claim"
  },
  {
    "span": "STGCN models spatial-temporal correlations on fixed road graphs (Yu et al., 2018). DCRNN employs diffusion convolution within seq2seq forecasting (Li et al., 2018). Graph WaveNet adds adaptive adjacency to capture dynamic connectivity (Wu et al., 2019). T-GCN combines GCN with GRU for joint modeling (Zhao et al., 2019). Transformer-based approaches capture long-range temporal dependencies (Cai et al., 2020).",
    "document": "Related Work\n\nGraph Neural Networks for Traffic Forecasting\n\nTraffic forecasting requires modeling complex spatial dependencies among sensors and nonstationary temporal dynamics. Graph neural networks (GNNs) have emerged as a dominant paradigm due to their capacity to encode structure while scaling to real-world sensor deployments. Techniques differ in how they construct or learn the graph, how they handle time, and how they trade off accuracy with latency in streaming settings.\n\nSTGCN models spatial-temporal correlations on fixed road graphs (Yu et al., 2018). DCRNN employs diffusion convolution within seq2seq forecasting (Li et al., 2018). Graph WaveNet adds adaptive adjacency to capture dynamic connectivity (Wu et al., 2019). T-GCN combines GCN with GRU for joint modeling (Zhao et al., 2019). Transformer-based approaches capture long-range temporal dependencies (Cai et al., 2020).\n\nLearning Graph Structure\n\nRecent research learns graphs from data rather than relying on physical topology. Adaptive Graph Learning (AGL) methods infer edges from sensor correlations (Zheng et al., 2020), while spatiotemporal attention models derive time-varying graphs (Pan et al., 2021). These methods often outperform fixed graphs under distribution shifts.\n\nOur Focus\n\nWe target robustness to sensor failures and road closures by introducing a topology-aware augmentation and an uncertainty-gated diffusion module that mitigates error propagation from corrupted nodes.",
    "reason": "Papers are enumerated with no transitions or explicit connections among them; the relationships (e.g., how each advances or differs from the others) are implied rather than stated, causing abrupt shifts between sentences.",
    "start": 485,
    "end": 896,
    "label": "Coherence"
  },
  {
    "span": "McMahan et al. (2017) introduce federated averaging to aggregate client updates. Smith et al. (2017) explore multi-task federated optimization. Meta-learning has been used to adapt client models rapidly (Fallah et al., 2020). Differential privacy bounds were derived for noisy aggregation (Kairouz et al., 2021).",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative model training without centralizing raw data. Personalization in FL addresses client heterogeneity, data imbalance, and non-IID distributions by tailoring models or parameters to local needs while respecting privacy constraints.\n\nMcMahan et al. (2017) introduce federated averaging to aggregate client updates. Smith et al. (2017) explore multi-task federated optimization. Meta-learning has been used to adapt client models rapidly (Fallah et al., 2020). Differential privacy bounds were derived for noisy aggregation (Kairouz et al., 2021).\n\nWhile these efforts target aggregation, optimization, adaptation, or privacy in isolation, our method jointly optimizes a client-adaptive head with an information bottleneck regularizer, providing principled control of personalization–privacy tradeoffs.",
    "reason": "The sentences present four disjoint works without transitions or explanation of how each is relevant to the others or to personalization, leaving the relationships implied rather than explicit across multiple sentences.",
    "start": 290,
    "end": 602,
    "label": "Coherence"
  },
  {
    "span": "several datasets now exist for red teaming instruction-tuned models",
    "document": "Introduction\n\nLarge language models have demonstrated impressive capabilities but also present safety risks, including the generation of harmful, biased, or privacy-sensitive content. Evaluation protocols increasingly incorporate adversarial probing to assess and improve model robustness under worst-case prompts.\n\nTo facilitate reproducible assessment, several datasets now exist for red teaming instruction-tuned models, spanning jailbreak prompts, unsafe content categories, and multi-turn attacks. However, differences in taxonomy and scoring make cross-benchmark comparison difficult.\n\nWe propose a unifying schema for categorizing adversarial intent and outcome severity, and we release a consolidated evaluation suite with standardized annotations and metrics.",
    "reason": "References the existence of multiple specific datasets (“several datasets now exist…”) without citing any of them.",
    "start": 355,
    "end": 422,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Clark, 2015, Davis, 2016)",
    "document": "Related Work\n\nMulti-task learning (MTL) can act as an inductive bias that improves generalization by sharing representations across related tasks (Caruana, 1997). Several approaches explore hard and soft parameter sharing for NLP encoders (Ruder, 2017; Hashimoto et al., 2017). In sequence labeling, jointly training with auxiliary syntactic tasks has yielded consistent gains (Søgaard and Goldberg, 2016). Several approaches explore multi-task training (Clark, 2015, Davis, 2016) to mitigate data sparsity and improve robustness in low-resource scenarios. Recent frameworks further introduce task routing and dynamic loss balancing to prevent negative transfer (Chen et al., 2018; Kendall et al., 2018).\n\nOur contribution lies in a curriculum-aware scheduler that adjusts task sampling based on per-task gradient conflicts, improving both stability and final accuracy.",
    "reason": "Incorrect separator for multiple citations in APA style. Use a semicolon between sources: \"(Clark, 2015; Davis, 2016)\" instead of \"(Clark, 2015, Davis, 2016)\".",
    "start": 454,
    "end": 480,
    "label": "Format"
  },
  {
    "span": "Several studies report that fine-tuning only the last layer yields the best trade-off between accuracy and communication cost.",
    "document": "Introduction\n\nPersonalized federated learning (pFL) aims to tailor global models to heterogeneous clients while preserving privacy and reducing communication overhead. A central question is how much of a shared model to adapt locally. Strategies range from fully local fine-tuning to partial adaptation of selected layers, adapters, or normalization parameters.\n\nSeveral studies report that fine-tuning only the last layer yields the best trade-off between accuracy and communication cost. Despite its simplicity, this approach may underfit clients with substantial covariate shift and overfit clients with very limited data. We revisit this design space and analyze layer-wise sensitivity to distribution shift under standard non-IID partitions. Our analysis reveals regimes where deeper personalization is necessary and others where shallow updates suffice.\n\nWe propose a budget-aware scheduling method that dynamically allocates personalization depth based on gradient similarity and client-specific validation metrics, achieving improved accuracy without increasing the average communication cost.",
    "reason": "This sentence references findings from 'several studies' but provides no citations, making the claim about prior work unsupported.",
    "start": 363,
    "end": 489,
    "label": "Unsupported_claim"
  },
  {
    "span": "In the last two years, several large-scale benchmarks have demonstrated that transformer-based spatiotemporal models consistently outperform graph convolutional networks by 5–10% MAE.",
    "document": "Introduction\n\nAccurate traffic forecasting underpins intelligent transportation systems, enabling proactive congestion management, route planning, and emission reduction. Early approaches relied on statistical time series models and shallow regressors, but the advent of deep learning brought sequence models and graph neural networks (GNNs) to the forefront for modeling spatiotemporal dependencies across road networks. Graph convolutional architectures have proven effective at capturing spatial correlations, while recurrent and attention-based encoders process temporal dynamics. Nonetheless, real-world traffic exhibits abrupt regime shifts, sensor outages, and non-stationary interactions that strain static graph assumptions.\n\nRecent advances in attention mechanisms and dynamic graph learning motivate revisiting the architectural trade-offs for spatiotemporal forecasting. In the last two years, several large-scale benchmarks have demonstrated that transformer-based spatiotemporal models consistently outperform graph convolutional networks by 5–10% MAE. Yet most of these methods either rely on dense attention with quadratic cost or treat the spatial structure as a token sequence, potentially discarding inductive biases that benefit generalization.\n\nWe propose a hybrid architecture that integrates localized graph filters with sparse temporal attention and adaptive topology refinement. Our method preserves spatial inductive structure while allocating attention capacity to rare events and long-range dependencies. We evaluate on multiple public datasets and under distribution shift scenarios, and we analyze robustness to missing data and sensor failures.\n\nContributions: (1) a hybrid spatiotemporal model that unifies graph filtering with efficient attention, (2) a dynamic edge refinement mechanism guided by uncertainty-aware signals, and (3) extensive experiments and ablations demonstrating improved accuracy and robustness under realistic perturbations.",
    "reason": "Claims specific performance margins on benchmarks and asserts a trend about model classes without providing any citations or evidence (b, d).",
    "start": 883,
    "end": 1066,
    "label": "Unsupported_claim"
  },
  {
    "span": "Thus, while the distributional hypothesis for phonology is well-established, one notable issue is the fact that the empirical evidence to study sound change is relatively inaccessible since it requires recorded speech or phonologically transcribed data. ",
    "document": "Related work\n\nThe application of NLP methods to automatic LSC detection is already a rather well-developed subfield of NLP research (Tahmasebi et al., 2018;Kutuzov et al., 2018). In particular, the emergence of word embeddings as a viable way to model the distributional hypothesis in semantics (Firth, 1957) has paved the way for an application of word embeddings to LSC modeling (Kim et al., 2014;Hamilton et al., 2016b;Eger and Mehler, 2016;Yao et al., 2018). Synchronically, the meaning of a word is characterized by word embeddings in terms of the contexts it appears in. LSC is captured by training word embeddings at different time points and comparing these distributions typically using cosine distance.\n\nThe main issues in this comparison is the alignment of temporal embeddings spaces, especially for neural embeddings as these are initialized and trained stochastically, which means that separate runs -on even the same data -will yield different embeddings spaces. Thus, work has focused on the development of methods to perform alignments to make embedding spaces comparable across time (see Kutuzov et al. (2018) for an overview). As an alternative to neural embeddings, scholars have also used purely count-based measures, which are naturally aligned across dimensions. Normalisation techniques are also applied, e.g. based on positive pointwise mutual information (PPMI) (Hamilton et al., 2016b;Yao et al., 2018).\n\nMost studies of LSC do not rely on a control dataset against which to validate their conclusions. In Dubossarsky et al. (2017), on the contrary, it is argued that any claims about putative laws of semantic change in diachronic corpora must be evaluated against a relevant control condition. The authors propose a methodology in which a control condition is created artificially from the original diachronic text collection by reshuffling the data.\n\nNo systematic LSC is expected in the artificially developed control dataset.\n\nThe distributional hypothesis has also been proposed as an explanatory model within the domain of phonology suggesting that phonological classes are acquired through distributional information (Chomsky and Halle, 1968;Mielke, 2008). Driven by this hypothesis, recent work has focused on testing how distributional properties can be learned by phoneme embeddings (see Mayer 2020 for an overview). Silfverberg et al. (2018) investigated to what extent learned vector representations of phonemes align with their respective representations in a feature space in which dimensions are articulatory descriptors (e.g., ±plosive). Recently, Mayer (2020) has shown that phonological classes, such as long and short vowels, can be deduced from phoneme embeddings normalised using PPMI by iteratively performing PCA on candidate classes.\n\nThus, while the distributional hypothesis for phonology is well-established, one notable issue is the fact that the empirical evidence to study sound change is relatively inaccessible since it requires recorded speech or phonologically transcribed data. Simulation is therefore used as a tool for studying the underlying mechanisms of sound change by creating computational models based on linguistic theory (Wedel, 2015). Through simulation, questions pertaining to e.g., what factors influence the (in)stability of vowel systems across generations (de Boer, 2003) can be modeled by controlling the assumptions made by the model. Work on simulation ranges from implementing theoretical approaches using mathematical models (Pierrehumbert, 2001;Blythe and Croft, 2012) to iterated learning and neural networks (Hare and Elman, 1995;Beguš, 2021).\n\nWhile the output of such models can be tested empirically on what we observe at a synchronic level, they are primarily theoretically driven. In this paper, we wish to take a data-driven approach and utilize some of the methods reviewed above to track historical sound change in writing. Rather than using word embeddings as done to model lexical change, we will use character embeddings, that are better suited to the task of sound change modeling.\n\n ",
    "start": 2787,
    "end": 3041,
    "label": "Unsupported_claim"
  },
  {
    "span": "Graph-based neural solvers are the de facto standard for molecule property prediction.",
    "document": "Related Work\n\nPredicting molecular properties from structure underpins drug discovery and materials science. Classical cheminformatics relied on engineered descriptors such as fingerprints and handcrafted physicochemical features. With the rise of deep learning, models operating directly on molecular graphs became prevalent due to their ability to learn from relational inductive biases.\n\nGraph-based neural solvers are the de facto standard for molecule property prediction. Several message-passing variants have been proposed to better capture stereochemistry and long-range interactions, while pretraining strategies aim to leverage large unlabeled compound libraries.\n\nOur work revisits pooling and positional encodings for graph transformers, demonstrating improved performance on both small-molecule and polymer datasets without domain-specific augmentations.\n",
    "reason": "Makes a field-wide claim about the prevailing standard without any supporting citations.",
    "start": 391,
    "end": 477,
    "label": "Unsupported_claim"
  },
  {
    "span": "Nguyen et al., (2021)",
    "document": "Introduction\n\nPre-trained language models have been adapted to event extraction by casting structured outputs as constrained sequences (Paolini et al., 2021; Lu et al., 2021). Nguyen et al., (2021) propose a span-centric decoder that conditions arguments on predicted triggers, but subsequent analyses highlight exposure bias during generation (Hsu et al., 2021). We address this by introducing a contrastive calibration objective aligned with schema constraints.\n",
    "reason": "Comma inserted before the year in a narrative citation; should be “Nguyen et al. (2021)”.",
    "start": 176,
    "end": 197,
    "label": "Format"
  },
  {
    "span": "Anderson et al. (2018) used bottom-up and top-down attention for grounding in images. Hudson and Manning (2019) developed a compositional model for reasoning over visual scenes. Lu et al. (2019) adopted a unified transformer for multimodal fusion. Yi et al. (2018) introduced neuro-symbolic execution for visual question answering.",
    "document": "Related Work\n\nVisual Question Answering\n\nVQA requires joint understanding of images and text, often combining visual grounding with compositional reasoning. Methods vary in their reliance on attention mechanisms, symbolic structures, and large-scale pretraining. Dataset biases and evaluation artifacts further complicate comparisons across architectures.\n\nAnderson et al. (2018) used bottom-up and top-down attention for grounding in images. Hudson and Manning (2019) developed a compositional model for reasoning over visual scenes. Lu et al. (2019) adopted a unified transformer for multimodal fusion. Yi et al. (2018) introduced neuro-symbolic execution for visual question answering.\n\nOur framework disentangles perception from reasoning by aligning object-centric representations with program-like operators, while maintaining end-to-end differentiability.",
    "reason": "The span strings together four VQA approaches without transitions or stated relationships; readers are not told how attention, compositional reasoning, transformers, and neuro-symbolic methods relate or compare.",
    "start": 357,
    "end": 688,
    "label": "Coherence"
  },
  {
    "span": "Garcia 2017",
    "document": "Introduction\n\nTopic modeling aims to uncover latent semantic structure in document collections, enabling summarization and exploratory analysis (Blei et al., 2003; Griffiths & Steyvers, 2004). Neural variants combine amortized inference with flexible priors, improving scalability and coherence (Miao et al., 2017; Srivastava & Sutton, 2017). According to Garcia 2017, anchor-based topic discovery offers deterministic guarantees for identifiability while remaining efficient for large corpora. Recent work augments topic models with pretrained language model features to better capture semantics and rare words (Dieng et al., 2020; Bianchi et al., 2021). We extend hybrid neural–probabilistic models with curvature-aware optimization to stabilize training across batch sizes (Martens, 2010; Zhang & He, 2020).",
    "reason": "Narrative citation missing the proper author–year format; should be 'Garcia (2017)'.",
    "start": 356,
    "end": 367,
    "label": "Format"
  },
  {
    "span": "(2021, Brown et al.)",
    "document": "Related Work\n\nCurriculum learning organizes training examples to improve convergence and generalization. Evidence from controlled trials (2021, Brown et al.) indicates benefits in low-data regimes, complementing classic results by Bengio et al. (2009). Swayamdipta et al. (2020) further show that example difficulty can be estimated via training dynamics. However, choosing a curriculum automatically remains nontrivial and task dependent.",
    "reason": "Year placed before authors in a parenthetical citation; should be '(Brown et al., 2021)'.",
    "start": 137,
    "end": 157,
    "label": "Format"
  },
  {
    "span": "For low-resource speech recognition, semi-supervised learning with pseudo-labels, multilingual pretraining, and CTC/attention hybrids have been applied to leverage unlabeled audio (Kahn et al., 2020; Watanabe et al., 2017; Conneau et al., 2021). We propose an approach that combines teacher re-weighting with iterative decoding for improved label quality.",
    "document": "Related Work\n\nLabel scarcity hinders the deployment of automatic speech recognition (ASR) in emerging languages and domains, where collecting transcriptions is costly and slow.\n\nFor low-resource speech recognition, semi-supervised learning with pseudo-labels, multilingual pretraining, and CTC/attention hybrids have been applied to leverage unlabeled audio (Kahn et al., 2020; Watanabe et al., 2017; Conneau et al., 2021). We propose an approach that combines teacher re-weighting with iterative decoding for improved label quality.\n\nOur experiments cover three typologically diverse languages, reporting word error rates across varying amounts of labeled seed data.",
    "reason": "The span moves from listing prior techniques straight to the authors’ proposal without identifying a specific gap or deficiency in existing methods.",
    "start": 178,
    "end": 533,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Several personalization strategies have been proposed, including meta-learning, fine-tuning, and clustered aggregation (Fallah et al., 2020; Arivazhagan et al., 2019; Sattler et al., 2020).",
    "document": "Introduction\n\nFederated learning aims to train models collaboratively without centralizing data, but client heterogeneity in objectives, data distributions, and resource constraints undermines global convergence and local utility. Personalization attempts to tailor models to individual clients while preserving collective benefits.\n\nMethodological landscape. Several personalization strategies have been proposed, including meta-learning, fine-tuning, and clustered aggregation (Fallah et al., 2020; Arivazhagan et al., 2019; Sattler et al., 2020). Other works adjust optimization dynamics, proximal terms, or representation layers to mitigate client drift (Li et al., 2020; Dinh et al., 2020; Collins et al., 2021).\n\nThis paper studies personalization under strict communication budgets and intermittent participation, focusing on mechanisms that decouple representation sharing from head adaptation. We present a protocol that reduces uplink bandwidth while maintaining strong on-device adaptation in the presence of non-stationary data.",
    "reason": "The span enumerates prior strategies without articulating their limitations or clarifying how they inform the paper’s approach; it neither identifies a gap nor connects the cited work to the authors’ motivation.",
    "start": 360,
    "end": 549,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Lee, 2020; Park, 2021",
    "document": "Introduction\n\nAlgorithmic fairness research spans definitions of group and individual fairness and their trade-offs under distribution shift (Hardy and Cole, 2019; Mendes et al., 2021). For comprehensive surveys, see Lee, 2020; Park, 2021, which catalog auditing methods for ranking and classification and highlight measurement challenges in practice. Recent approaches propose post-hoc calibration to mitigate disparities in false positive rates (Ibrahim and Zhu, 2022), while others embed fairness constraints directly into the learning objective (Garcia and Noor, 2020).\n\nOur contribution focuses on robust equalization under covariate shift, introducing a plug-in correction that controls subgroup error inflation without retraining base models, complementing prior work on distributionally robust optimization (Wei and Thomas, 2021).",
    "reason": "Multiple citations are presented without enclosing parentheses in an author–year style. Should be written as a parenthetical citation, e.g., \"(Lee, 2020; Park, 2021)\" or integrated as narrative citations with years.",
    "start": 217,
    "end": 238,
    "label": "Format"
  },
  {
    "span": "(et al., 2018)",
    "document": "Introduction\n\nModel-based reinforcement learning (MBRL) aims to improve sample efficiency by learning a dynamics model and planning with it (Sutton, 1991; Deisenroth and Rasmussen, 2011). Recent advances in stochastic latent dynamics and uncertainty-aware planning have yielded competitive performance on continuous control (Chua et al., 2018; Hafner et al., 2019). However, compounding model bias remains a key limitation when rollouts extend far into the future (Janner et al., 2019).\n\nTo mitigate bias, short-horizon value expansion and conservative policy evaluation have been proposed (Kumar et al., 2020; Kidambi et al., 2020). Additionally, ensembles and epistemic uncertainty estimates help avoid overconfident predictions in poorly explored regions (et al., 2018; Lakshminarayanan et al., 2017). Our work complements these approaches by introducing a risk-sensitive planner that adaptively truncates imagined rollouts based on calibrated uncertainty thresholds.",
    "reason": "Missing author names in the parenthetical citation; \"(et al., 2018)\" is incomplete and should include the leading author (e.g., \"(Chua et al., 2018)\").",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "In real-world call center conversations, overlapping speech accounts for nearly 40% of frames.",
    "document": "Introduction\n\nRobust emotion recognition in the wild must contend with noise, channel variability, and conversational dynamics. In real-world call center conversations, overlapping speech accounts for nearly 40% of frames. This prevalence of overlap degrades performance for models trained on clean, single-speaker segments. We explore multi-stream architectures that jointly leverage diarization cues and spectral masks to recover speaker-specific affective signals under overlap conditions.",
    "reason": "Presents a specific statistic without any citation or empirical evidence (definition b and e).",
    "start": 128,
    "end": 222,
    "label": "Unsupported_claim"
  },
  {
    "span": "Multimodal sarcasm detection has been studied using image-text fusion with attention, graph-based interactions, and sentiment-incongruity features (Cao et al., 2019; Pal and Saha, 2020; Reddy et al., 2021). Pretrained vision-language models have also been adapted to sarcasm classification (Kundu and Roy, 2022; Silva et al., 2023).",
    "document": "Introduction\nInterpreting sarcasm in social media is challenging due to implicit sentiment and pragmatic cues that span both text and accompanying images. Accurate detection benefits downstream moderation, toxicity mitigation, and brand monitoring. We hypothesize that modeling pragmatic contrast between literal and intended meanings can improve robustness across domains.\n\nRelated Work\nMultimodal sarcasm detection has been studied using image-text fusion with attention, graph-based interactions, and sentiment-incongruity features (Cao et al., 2019; Pal and Saha, 2020; Reddy et al., 2021). Pretrained vision-language models have also been adapted to sarcasm classification (Kundu and Roy, 2022; Silva et al., 2023). Prior work on pragmatic phenomena has explored irony detection and metaphor identification with lexical and contextual signals (Bhardwaj et al., 2018; Zhao and Lin, 2019).\n\nWe propose a counterfactual contrast framework that constructs literal paraphrases of captions and measures disagreement with visual entailment signals, enabling explicit modeling of pragmatic flip without relying on platform-specific heuristics.",
    "reason": "The span lists prior multimodal approaches and models without articulating how they relate to the problem the paper addresses, what their limitations are, or how the authors' method differs; there is no explicit synthesis.",
    "start": 388,
    "end": 720,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Fine-tuning on in-domain data remains a common strategy (Luong and Manning, 2015). Back-translation augments target-side monolingual data (Sennrich et al., 2016). Vocabulary expansion for domain specificity was explored (Kothur et al., 2018). Multi-domain mixture-of-experts has also been proposed (Gu et al., 2019).",
    "document": "Related Work\n\nDomain Adaptation for Neural Machine Translation\n\nNMT systems trained on heterogeneous corpora degrade when deployed in specialized domains due to lexical drift and style mismatch. Domain adaptation techniques aim to bridge this gap by incorporating domain signals into training or decoding.\n\nFine-tuning on in-domain data remains a common strategy (Luong and Manning, 2015). Back-translation augments target-side monolingual data (Sennrich et al., 2016). Vocabulary expansion for domain specificity was explored (Kothur et al., 2018). Multi-domain mixture-of-experts has also been proposed (Gu et al., 2019).\n\nWhile these approaches operate at different levels—data augmentation, parameter adaptation, and architectural modularization—robust selection of domain indicators at sentence level remains underexplored. We address this by learning a lightweight domain router that conditions both encoder and decoder representations.",
    "reason": "The span presents four adaptation strategies as isolated statements without connecting phrases or explanation of how each builds on or contrasts with the others. The abrupt, list-like style reduces coherence between sentences (criteria a and b).",
    "start": 307,
    "end": 623,
    "label": "Coherence"
  },
  {
    "span": "It is well known that node classification benchmarks like Cora and Citeseer are saturated and no longer challenging.",
    "document": "Related Work\nGraph neural networks (GNNs) have achieved strong performance on citation networks, social graphs, and molecular property prediction. Despite architectural innovations, concerns remain about evaluation practices, including small datasets, fixed splits, and tuning on test sets.\n\nIt is well known that node classification benchmarks like Cora and Citeseer are saturated and no longer challenging. Yet, reported gains often hinge on specific splits and feature preprocessing. We advocate for larger, temporally split benchmarks and cross-graph validation to better reflect real-world deployment.",
    "reason": "This is a literature claim about the state of benchmarks that requires citations to surveys or empirical studies; none are given (violates b and d).",
    "start": 292,
    "end": 408,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Lee et al., 2018))",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) has benefited from attention-based encoders and CTC objectives (Graves et al., 2006; Chan et al., 2016). Multitask learning with auxiliary phonetic targets improves data efficiency (Rao and Sak, 2017), and SpecAugment remains a strong baseline for robustness (Park et al., 2019). Streaming architectures leverage chunked attention for low-latency decoding (Zhang et al., 2020). Robustness to domain shift is often addressed by adversarial training (Shinohara, 2016) and test-time adaptation (Huang et al., 2020). Recent work explores self-supervised pretraining on unlabeled audio (Baevski et al., 2020; Hsu et al., 2021) and domain-matched fine-tuning (Li et al., 2020). Our approach complements confidence-based filtering (Lee et al., 2018)) with uncertainty-aware pseudo-label selection for noisy web-scale corpora.",
    "reason": "Extra closing parenthesis in the parenthetical citation; it should be '(Lee et al., 2018)'.",
    "start": 784,
    "end": 803,
    "label": "Format"
  },
  {
    "span": "Back-translation synthesizes source sentences from target monolingual data (Sennrich et al., 2016). Multilingual pretraining shares lexical knowledge across languages (Conneau and Lample, 2019). SentencePiece builds subword vocabularies (Kudo and Richardson, 2018). BLEU remains the de facto metric for translation quality (Papineni et al., 2002).",
    "document": "Introduction\n\nLow-Resource Neural Machine Translation\n\nImproving NMT for low-resource language pairs relies on exploiting monolingual data, transfer from related languages, and better tokenization. Despite advances, robustness to domain shifts and morphology-rich targets is limited.\n\nKey Techniques and Practices\n\nBack-translation synthesizes source sentences from target monolingual data (Sennrich et al., 2016). Multilingual pretraining shares lexical knowledge across languages (Conneau and Lample, 2019). SentencePiece builds subword vocabularies (Kudo and Richardson, 2018). BLEU remains the de facto metric for translation quality (Papineni et al., 2002).\n\nOur Contribution\n\nWe introduce a morphology-aware tokenizer with contrastive pretraining that improves transfer in extremely low-resource settings, and we evaluate with both n-gram and learned metrics to assess adequacy and fluency.",
    "reason": "The span chains together methods, preprocessing, and evaluation metrics without transitions or explanation of how they relate. The move from modeling strategies to tokenization and then to BLEU is abrupt, leaving the connections between sentences unclear.",
    "start": 315,
    "end": 662,
    "label": "Coherence"
  },
  {
    "span": "Dehghani et al. (2023) investigated long-context transformers for extended inputs. Zaheer et al. (2020) proposed sparse attention patterns for scalability. Beltagy et al. (2020) introduced Longformer with sliding-window attention. Tay et al. (2020) surveyed efficient transformers.",
    "document": "Related Work\n\nHandling long sequences in transformer architectures requires balancing memory, compute, and fidelity of global context. Many approaches approximate attention to reduce quadratic complexity while preserving key dependencies, and others alter positional encodings or recurrence to extend context windows. Despite progress, stability and calibration under extreme lengths remain open problems, especially for retrieval-augmented generation.\n\nDehghani et al. (2023) investigated long-context transformers for extended inputs. Zaheer et al. (2020) proposed sparse attention patterns for scalability. Beltagy et al. (2020) introduced Longformer with sliding-window attention. Tay et al. (2020) surveyed efficient transformers.\n\nOur contribution is a unified kernelized attention layer with adaptive bandwidths tuned by validation-time probes, enabling consistent performance from 4K to 128K tokens and integrating seamlessly with retrieval modules.",
    "reason": "The span lists four references without connecting them or explaining how each method relates to the others. The relationship between a survey and specific architectures is not made explicit, resulting in abrupt transitions.",
    "start": 454,
    "end": 735,
    "label": "Coherence"
  },
  {
    "span": "(Lee et al. 2021)",
    "document": "Related Work\n\nProbabilistic generative models such as variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014) enable amortized inference for complex latent-variable structures. Normalizing flows provide exact likelihoods and flexible densities (Dinh et al., 2017; Kingma and Dhariwal, 2018). Extensions include hierarchical priors (Lee et al. 2021) and discrete latent variables trained with gradient estimators (Maddison et al., 2017; Jang et al., 2017). Semi-supervised variants leverage small labeled sets to guide representation learning (Sohn et al., 2015). We contribute a unified objective that blends contrastive alignment with variational bounds, improving sample efficiency on multi-view datasets.\n",
    "reason": "Missing comma before the year in a parenthetical author–year citation; should be “(Lee et al., 2021)”.",
    "start": 349,
    "end": 366,
    "label": "Format"
  },
  {
    "span": "Recent works have shown that contrastive pretraining consistently improves molecular property prediction across small-molecule benchmarks.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a dominant approach for molecular representation learning due to their ability to encode topology and chemistry-aware features. Self-supervised methods augment limited labeled data by leveraging structural invariances, augmentations, and predictive tasks on subgraphs. Recent works have shown that contrastive pretraining consistently improves molecular property prediction across small-molecule benchmarks. Despite these advances, transfer to out-of-distribution scaffolds remains challenging. We propose augmentation curricula guided by uncertainty to reduce shortcut features and improve scaffold split generalization.",
    "reason": "Uses the phrase 'Recent works have shown' without providing any citations, which is an unsupported claim under rule (d).",
    "start": 324,
    "end": 462,
    "label": "Unsupported_claim"
  },
  {
    "span": "Message passing neural networks aggregate neighbor features to produce node embeddings (Gilmer et al., 2017). Oversmoothing degrades discriminative power as depth increases (Li et al., 2018; Oono and Suzuki, 2019). Positional encodings introduce global structure via spectral or random features (Dwivedi et al., 2021).",
    "document": "Related Work\n\nGraph representation learning has progressed rapidly with message passing architectures and spectral techniques. Design choices in aggregation, normalization, and depth crucially influence expressivity and generalization.\n\nMitigating the limitations of local aggregation has inspired methods that incorporate positional information, subgraph sampling, and multi-scale processing. Scalability on large graphs further motivates sparse operators and memory-efficient training.\n\nMessage passing neural networks aggregate neighbor features to produce node embeddings (Gilmer et al., 2017). Oversmoothing degrades discriminative power as depth increases (Li et al., 2018; Oono and Suzuki, 2019). Positional encodings introduce global structure via spectral or random features (Dwivedi et al., 2021).\n\nWe propose a depth-aware normalization scheme that counteracts oversmoothing while preserving stability. Our analysis connects spectral gaps to feature variance propagation and informs the design of positional terms.",
    "reason": "The sentences move between message passing, oversmoothing, and positional encodings without transitions or explaining their relationships, resulting in an abrupt and implied connection (a, b).",
    "start": 489,
    "end": 807,
    "label": "Coherence"
  },
  {
    "span": "Wang et al., 2020)",
    "document": "Introduction\n\nData augmentation for NLP has progressed from simple noising (Xie et al., 2017) to semantically aware transformations (Kumar et al., 2020; Kobayashi, 2018). Interpolation-based methods such as mixup have been adapted to text via embedding-space operations (Guo et al., 2020). A parallel line examines compositional perturbations to improve robustness. For sequence classification, recent extensions claim consistent improvements; however, Wang et al., 2020) report that naive mixup can destabilize fine-tuning on small datasets. We introduce a curvature-aware interpolation that mitigates this issue.\n\nOur study complements consistency regularization approaches (Sohn et al., 2020) and back-translation augmentation (Sennrich et al., 2016).",
    "reason": "Missing opening parenthesis for a parenthetical citation; it appears as 'Wang et al., 2020)' but should be '(Wang et al., 2020)'.",
    "start": 453,
    "end": 471,
    "label": "Format"
  },
  {
    "span": "Deep reinforcement learning for traffic signal control includes value-based methods such as DQN and Double DQN (Mnih et al., 2015; Hasselt et al., 2016), policy gradient and actor-critic approaches like A3C/A2C and PPO (Mnih et al., 2016; Schulman et al., 2017), and model-based or hybrid methods that incorporate planning (Tampuu et al., 2017; Chen et al., 2020). Multi-agent coordination is commonly handled via independent learners, parameter sharing, or centralized training with decentralized execution (Foerster et al., 2016; Rashid et al., 2018).",
    "document": "Related Work\n\nAdaptive traffic signal control has been framed as an RL problem where agents must balance throughput, delay, and fairness under stochastic demand. Coordination across intersections introduces non-stationarity and partial observability.\n\nDeep reinforcement learning for traffic signal control includes value-based methods such as DQN and Double DQN (Mnih et al., 2015; Hasselt et al., 2016), policy gradient and actor-critic approaches like A3C/A2C and PPO (Mnih et al., 2016; Schulman et al., 2017), and model-based or hybrid methods that incorporate planning (Tampuu et al., 2017; Chen et al., 2020). Multi-agent coordination is commonly handled via independent learners, parameter sharing, or centralized training with decentralized execution (Foerster et al., 2016; Rashid et al., 2018).\n\nRecent benchmarks emphasize generalization to unseen networks and demand patterns, yet evaluation protocols vary widely. Reward shaping and state design also differ, complicating comparison.\n\nWe introduce MARL-LiteFlow, a scalable CTDE framework with sparse communication and demand-invariant representations, evaluated on real-city traces under rigorous train/test splits.",
    "reason": "The span summarizes families of DRL and MARL techniques without relating them to the paper’s problem setting, highlighting limitations, or stating how the proposed approach differs.",
    "start": 252,
    "end": 805,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent shared tasks on bias detection in news and social media highlight the need for multilingual resources.",
    "document": "Related Work\n\nAlgorithmic bias in language technologies has motivated research on detection, mitigation, and evaluation frameworks. Recent shared tasks on bias detection in news and social media highlight the need for multilingual resources. Parallel efforts have explored stereotype probing, debiasing embeddings, and fairness-aware fine-tuning of large language models.\n\nA persistent challenge is the scarcity of labeled data that captures subtle forms of bias such as framing and presupposition. Moreover, cross-lingual transfer remains underexplored, as most datasets focus on English.\n\nWe introduce a multilingual corpus annotated for targeted bias categories and propose a contrastive learning objective that leverages weak signals from machine-translated content. Our benchmarks cover zero-shot and few-shot transfer settings across five languages.",
    "reason": "Mentions 'recent shared tasks' but provides no citations to those tasks.",
    "start": 132,
    "end": 241,
    "label": "Unsupported_claim"
  },
  {
    "span": "User-level differential privacy has been widely adopted in production systems.",
    "document": "Related Work\n\nFederated learning (FL) enables on-device training without centralizing raw user data (McMahan et al., 2017; Kairouz et al., 2021). Practical FL for mobile text input has explored compression, partial participation, and personalization (Bonawitz et al., 2019; Kang et al., 2020). Differential privacy (DP) provides rigorous protection by randomizing contributions during training (Dwork et al., 2014; Abadi et al., 2016).\n\nUser-level differential privacy has been widely adopted in production systems. However, balancing privacy budgets with utility remains challenging for skewed participation and non-IID user distributions.\n\nWe propose an adaptive privacy mechanism that targets per-user contribution bounds and show improved utility at the same privacy guarantees on two keyboard datasets.",
    "reason": "Asserts widespread production adoption without any supporting citation or evidence (rule b).",
    "start": 437,
    "end": 515,
    "label": "Unsupported_claim"
  },
  {
    "span": "Self-attention captures sequential user behavior in session-based recommenders (Kang and McAuley, 2018). Exposure bias skews observed feedback distributions (Schnabel et al., 2016). Knowledge graphs inject structured relations between items (Wang et al., 2019).",
    "document": "Related Work\n\nRecommender Systems\n\nModern recommender systems leverage implicit feedback, sequence modeling, and side information. Session-based recommenders use RNNs or self-attention to capture short-term dynamics (Hidasi et al., 2016; Kang and McAuley, 2018). Long-term preference modeling integrates user attributes and knowledge graphs (Wang et al., 2019; Zhang et al., 2016). Self-attention captures sequential user behavior in session-based recommenders (Kang and McAuley, 2018). Exposure bias skews observed feedback distributions (Schnabel et al., 2016). Knowledge graphs inject structured relations between items (Wang et al., 2019). Debiasing approaches use inverse propensity scoring and counterfactual estimation (Schnabel et al., 2016; Wang et al., 2020).",
    "reason": "The span lists three separate topics—sequence modeling, exposure bias, and knowledge graphs—without transitions or explanation of how they connect, resulting in unclear relationships between the cited works.",
    "start": 382,
    "end": 643,
    "label": "Coherence"
  },
  {
    "span": "Most publicly available emotion corpora include at least 100 distinct speakers.",
    "document": "Related Work\n\nSpeech emotion recognition (SER) research spans acted, elicited, and naturalistic corpora, each presenting distinct challenges for label reliability and domain generalization. Modeling approaches range from handcrafted prosodic descriptors to self-supervised acoustic representations fine-tuned for affective states.\n\nMost publicly available emotion corpora include at least 100 distinct speakers. Nevertheless, demographic skew and recording conditions often limit cross-corpus transferability. To address these issues, recent approaches incorporate domain adversarial training and class-balanced sampling to mitigate confounds.\n\nIn this work, we study label uncertainty and propose a curriculum that leverages confidence-aware training to improve robustness across heterogeneous SER datasets.",
    "reason": "Presents a quantitative claim about dataset composition (speaker counts) without any citation or evidence (rule b).",
    "start": 332,
    "end": 411,
    "label": "Unsupported_claim"
  },
  {
    "span": "Nguyen et al. (2019",
    "document": "Introduction\n\nSemi-supervised text classification seeks to leverage unlabeled data to reduce annotation costs while maintaining accuracy. Early approaches relied on consistency regularization and pseudo-labeling (Miyato et al., 2017; Lee, 2013), while more recent methods exploit pre-trained language models to propagate label information across similar examples (Clark et al., 2020). Nguyen et al. (2019 explored neighborhood consistency in embedding space to stabilize training, showing that local perturbations can improve robustness without extensive supervision. Despite these advances, selecting reliable unlabeled instances remains challenging, particularly under domain shift (Wei and Zou, 2019). In this work, we propose a confidence-calibrated objective that aligns uncertainty estimates with downstream selection criteria.",
    "reason": "Narrative citation is missing the closing parenthesis after the year; should be “Nguyen et al. (2019)”.",
    "start": 385,
    "end": 404,
    "label": "Format"
  },
  {
    "span": "To the best of our knowledge, this is the first work to jointly pretrain layout and knowledge signals for long-document question answering.",
    "document": "Introduction\n\nLong-document question answering requires systems to read, retrieve, and reason over multi-page inputs such as reports, manuals, and scientific articles. Unlike short-context QA, models must integrate textual content with layout cues, figures, and tables while maintaining consistency across sections. Prior efforts typically treat layout-aware modeling and knowledge-aware modeling as independent components, which can lead to fragmented representations and brittle reasoning when cross-referencing is needed.\n\nWe propose a pretraining framework that unifies layout structure and external knowledge into a single encoder suitable for long-document question answering. The approach learns to align hierarchical document structure with entity-linked knowledge units while preserving token-level semantics. To the best of our knowledge, this is the first work to jointly pretrain layout and knowledge signals for long-document question answering. We evaluate our model on multi-page QA benchmarks and analyze its ability to locate evidence spans and reconcile conflicting mentions.\n\nOur contributions include a document-knowledge alignment objective, a segment-level layout contrastive task, and a multi-hop supervision signal derived from distant links. We further provide an ablation study on each component and discuss practical considerations for training with long sequences.",
    "reason": "This is a novelty claim about prior work that asserts being the first to do something but provides no citation or survey evidence.",
    "start": 819,
    "end": 958,
    "label": "Unsupported_claim"
  },
  {
    "span": "Self-supervised pretraining for speech has advanced through contrastive predictive coding, masked prediction, and multi-task objectives (van den Oord et al., 2018; Schneider et al., 2019; Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2021).",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) has benefited substantially from self-supervised learning (SSL), which reduces dependence on labeled transcriptions by leveraging structure in raw waveforms. SSL methods provide robust encoders that transfer across domains and languages, especially in low-resource settings.\n\nSelf-supervised pretraining for speech has advanced through contrastive predictive coding, masked prediction, and multi-task objectives (van den Oord et al., 2018; Schneider et al., 2019; Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2021). Large-scale pretraining has also prompted work on efficient fine-tuning, layer selection, and adapter-based specialization to minimize compute.\n\nWe focus on streaming ASR with tight latency budgets and propose a chunk-aware masking scheme that preserves causal receptive fields while retaining SSL gains under streaming constraints.",
    "reason": "The sentence summarizes prior SSL advances but does not connect them to the paper’s focus or articulate a specific limitation motivating the new approach, hence lacking synthesis.",
    "start": 325,
    "end": 572,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Graph convolutional networks have been adopted to model spatial dependencies in traffic sensors (Yu et al., 2018; Li et al., 2018). Diffusion and attention-based operators further refine spatial propagation (Li et al., 2018b; Wu et al., 2019). Temporal dynamics are captured using RNNs, temporal convolutions, or Transformers (Zhao et al., 2019; Bai et al., 2018; Xu et al., 2020).",
    "document": "Related Work\n\nSpatio-temporal traffic forecasting seeks to predict future states of a road network given historical sensor measurements. Challenges arise from complex, evolving topologies, non-stationary demand patterns, and exogenous events. Recent deep architectures attempt to jointly learn spatial and temporal dependencies from large-scale data.\n\nGraph convolutional networks have been adopted to model spatial dependencies in traffic sensors (Yu et al., 2018; Li et al., 2018). Diffusion and attention-based operators further refine spatial propagation (Li et al., 2018b; Wu et al., 2019). Temporal dynamics are captured using RNNs, temporal convolutions, or Transformers (Zhao et al., 2019; Bai et al., 2018; Xu et al., 2020).\n\nAlthough many models integrate space and time, training stability and robustness to graph perturbations remain open issues. We revisit spatio-temporal coupling with a stability-regularized architecture that enforces Lipschitz constraints on both spatial operators and temporal recurrences, demonstrating improved robustness to sensor failures and topology drift.",
    "reason": "The span summarizes categories of prior work without synthesizing them into an argument, identifying a specific gap, or positioning how these works motivate the authors' method.",
    "start": 352,
    "end": 733,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Graph WaveNet remains the state-of-the-art on PEMS-BAY.",
    "document": "Related Work\n\nTraffic forecasting has been advanced by spatiotemporal graph neural networks that capture both spatial dependencies and temporal dynamics (Li et al., 2018; Yu et al., 2018). Approaches differ in how they construct adaptive adjacency matrices, incorporate dynamic relations, and model long-range temporal patterns (Wu et al., 2019; Bai et al., 2020). Benchmarks such as METR-LA and PEMS-BAY have become standard for evaluation across horizons (Jain et al., 2017).\n\nGraph WaveNet remains the state-of-the-art on PEMS-BAY. Other methods explore diffusion convolutions, attention mechanisms, and physics-informed constraints to enhance generalization under distribution shift (Pan et al., 2019; Cini et al., 2022). Our method introduces horizon-conditioned graph kernels that adaptively trade off locality and globality.",
    "reason": "Makes a state-of-the-art performance claim on a specific dataset without citing empirical evidence.",
    "start": 479,
    "end": 534,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most prior open-domain QA systems used TF-IDF or BM25 retrievers.",
    "document": "Related Work\n\nOpen-domain question answering (ODQA) typically decomposes the task into retrieval and reading. Early neural readers were paired with sparse retrievers, while recent work has shown the effectiveness of dense retrieval based on learned representations. Hybrid approaches combine sparse and dense signals for improved recall.\n\nMost prior open-domain QA systems used TF-IDF or BM25 retrievers. Subsequent advances introduced dual-encoder architectures and contrastive learning to close the gap to knowledge-intensive tasks.\n\nOur approach revisits sparse retrieval with learned expansion, showing that careful query reformulation can rival dense methods under strict latency constraints.",
    "reason": "Summarizes the landscape of prior systems without citing representative systems or studies.",
    "start": 339,
    "end": 404,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works show that cross-lingual pretraining dramatically improves summarization quality on low-resource languages.",
    "document": "Introduction\n\nMultilingual abstractive summarization has benefited substantially from advances in large pretrained sequence-to-sequence models. While early approaches relied on language-specific pipelines and parallel corpora, more recent transformer-based methods share parameters across languages to enable transfer (Johnson et al., 2017; Conneau and Lample, 2019). At the same time, evaluation practices have shifted toward reference-free metrics and human judgments to better capture faithfulness and coherence (Zhong et al., 2020; Fabbri et al., 2021).\n\nRecent works show that cross-lingual pretraining dramatically improves summarization quality on low-resource languages. Despite these advances, most existing benchmarks focus on high-resource European languages, leaving a gap for typologically diverse and morphologically rich settings. In this paper, we present PanSum, a multilingual benchmark targeting 18 low-resource languages with varied scripts and morphologies. We also introduce a constrained training protocol that isolates the contribution of target-language monolingual data. Our experiments reveal substantial variance in transfer effectiveness across language families and underline the importance of script-aware tokenization.",
    "reason": "Mentions 'recent works' and a specific outcome without providing citations at the first mention.",
    "start": 559,
    "end": 678,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Miller and Davis, 2021)",
    "document": "Related Work\n\nTask-oriented dialogue systems increasingly adopt pre-trained language models for belief tracking and policy learning (Henderson et al., 2019; Hosseini-Asl et al., 2020). End-to-end frameworks couple retrieval with generation to ground responses in external knowledge (Dinan et al., 2019; Komeili et al., 2021). User simulation is often used to reduce data collection costs (Schatzmann et al., 2007). Prior work shows that incorporating uncertainty improves robustness (Miller and Davis, 2021), particularly under ASR noise. Our method extends this line with a contrastive objective for intent disambiguation.",
    "reason": "Incorrect conjunction inside parentheses for APA style; should use an ampersand: (Miller & Davis, 2021).",
    "start": 483,
    "end": 507,
    "label": "Format"
  },
  {
    "span": "[Garcia et al., 2016)",
    "document": "Introduction\n\nSelf-supervised visual representation learning (SSL) reduces reliance on labels by solving pretext tasks such as contrastive instance discrimination (Chen et al., 2020) and clustering-based assignments (Caron et al., 2020). Architectural choices and augmentation recipes are crucial to performance (Grill et al., 2020). For an overview of early metric learning and descriptor methods, see [Garcia et al., 2016). Recent works also investigate multi-modal SSL by aligning images with text (Radford et al., 2021).\n\nWe study invariances induced by diverse augmentations and propose a curriculum over views that balances invariance and equivariance. We evaluate linear probing and low-shot transfer on standard benchmarks.",
    "reason": "Mismatched brackets in the citation; it opens with '[' and closes with ')'. It should consistently use parentheses '(Garcia et al., 2016)' or brackets '[Garcia et al., 2016]'.",
    "start": 403,
    "end": 424,
    "label": "Format"
  },
  {
    "span": "Prior work shows that graph neural networks outperform convolutional networks on citation networks.",
    "document": "Related Work\n\nNode classification on graphs has evolved from shallow embedding methods to architectures that propagate and transform neighborhood information. Graph neural networks (GNNs) leverage message passing to capture local structure and attribute interactions.\n\nPrior work shows that graph neural networks outperform convolutional networks on citation networks. Subsequent research introduced attention mechanisms, residual connections, and normalization schemes tailored to graphs. Despite these advancements, robustness to heterophily and distribution shift remains an open problem.",
    "reason": "Asserts a comparative result attributed to 'prior work' without citing specific studies (definition b/ii).",
    "start": 269,
    "end": 368,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent large-scale code generation benchmarks emphasize pass@k metrics.",
    "document": "Related Work\n\nEvaluating code generation has moved beyond n-gram similarity to functional correctness via unit tests and execution-based metrics. Early datasets used BLEU-style scores, which correlate weakly with semantic validity in code (Papineni et al., 2002; Ren et al., 2020). Recent large-scale code generation benchmarks emphasize pass@k metrics. These protocols reflect realistic developer workflows but also introduce variance due to stochastic sampling, prompting work on standardized seeds and test de-duplication (Chen et al., 2021; Austin et al., 2021).",
    "reason": "Refers to 'recent large-scale benchmarks' and their emphasis without citing any specific benchmarks, violating rule (d).",
    "start": 282,
    "end": 353,
    "label": "Unsupported_claim"
  },
  {
    "span": "Gao et al.",
    "document": "Related Work\n\nPre-trained encoder adaptation. Masked language models (Devlin et al., 2019; Liu et al., 2019) provide strong initializations for downstream tasks, but require careful adaptation to avoid catastrophic forgetting (Kirkpatrick et al., 2017; Chen et al., 2020). Prompt-based tuning aligns pre-training and inference objectives, improving data efficiency (Schick and Schütze, 2021; Lester et al., 2021). Gao et al. propose calibrating verbalizers to mitigate label bias, showing gains across few-shot settings. Subsequent work extends this idea with soft prompts and mixture-of-verbalizers (Zhong et al., 2021; Hambardzumyan et al., 2021), while others integrate demonstration-based in-context examples (Min et al., 2022; Liu et al., 2022).\n\nCross-task transfer. Multi-task learning exploits shared structure to regularize representations (Caruana, 1997; Liu et al., 2019). Auxiliary objectives, such as entailment or paraphrase detection, provide useful inductive biases for classification tasks (Clark et al., 2019; Phang et al., 2018). Recent methods adaptively weight tasks to avoid negative transfer (Chen et al., 2018; Pilault et al., 2021).",
    "reason": "Narrative citation missing year; should be formatted with year, e.g., 'Gao et al. (2021)'.",
    "start": 414,
    "end": 424,
    "label": "Format"
  },
  {
    "span": "(Brown et al. 2020 Clark et al., 2020)",
    "document": "Related Work\n\nVision Transformers adapt self-attention to image patches and achieve strong results across classification, detection, and segmentation (Dosovitskiy et al., 2021;Liu et al., 2021). Data augmentation and regularization are crucial for training stability (Touvron et al., 2021), and self-supervised objectives provide robust representations for transfer (Caron et al., 2021).\n\nScaling both model size and data leads to emergent capabilities and improved sample efficiency (Kolesnikov et al., 2020). Foundational studies demonstrate predictable scaling laws for loss and downstream accuracy (Rosenfeld et al., 2020) and motivate training pipelines leveraging large web corpora (Radford et al., 2021;Jia et al., 2021). We adopt a frozen backbone with lightweight adapters, inspired by findings in (Brown et al. 2020 Clark et al., 2020) on parameter-efficient transfer.",
    "reason": "Multiple-citation formatting error: missing separator between two citations and missing comma after 'et al.' for the first. Should be '(Brown et al., 2020; Clark et al., 2020)'.",
    "start": 807,
    "end": 845,
    "label": "Format"
  },
  {
    "span": "Normalization reduces technical variability in single-cell RNA-seq counts (Garcia et al., 2018). Graph-based clustering identifies cell types from neighborhood structure (Kwon et al., 2019). Differential expression highlights marker genes across conditions (Stein and Rao, 2020).",
    "document": "Introduction\n\nSingle-cell transcriptomics enables high-resolution profiling of heterogeneous tissues but introduces noise and sparsity that complicate inference. Analyses typically proceed through preprocessing, dimensionality reduction, and downstream tasks.\n\nNormalization reduces technical variability in single-cell RNA-seq counts (Garcia et al., 2018). Graph-based clustering identifies cell types from neighborhood structure (Kwon et al., 2019). Differential expression highlights marker genes across conditions (Stein and Rao, 2020). Integration methods align batches with partially overlapping cell states (Li and Zhou, 2021), yet evaluation under compositional shifts is limited.\n\nWe present a unified objective that couples normalization with structure-preserving embeddings to improve robustness across donors and platforms.",
    "reason": "The span comprises three separate sentences that list tasks with citations but fail to articulate how they relate to each other or to the overarching workflow; no transitions are provided, leading to coherence problems across multiple sentences.",
    "start": 261,
    "end": 540,
    "label": "Coherence"
  },
  {
    "span": "Existing multilingual ToD datasets rarely include low-resource languages spoken by migrant communities.",
    "document": "Introduction\n\nTask-oriented dialogue (ToD) datasets have enabled rapid progress in dialogue state tracking, policy learning, and response generation, especially in English and high-resource languages (Wen et al., 2017; Budzianowski et al., 2018). Cross-lingual transfer techniques have further reduced annotation costs by leveraging shared semantics across languages (Zhang et al., 2020; Lin et al., 2021). However, coverage across diverse linguistic communities remains limited.\n\nExisting multilingual ToD datasets rarely include low-resource languages spoken by migrant communities. This gap limits the external validity of current models in practical deployment scenarios and hinders progress on fairness and inclusivity in conversational AI. We introduce a curated benchmark spanning service-oriented domains with controlled slot schemas across seven underrepresented languages and provide strong baselines for cross-lingual generalization.",
    "reason": "Asserts a property of existing datasets and their language coverage without providing citations or evidence; dataset coverage claims should be supported.",
    "start": 481,
    "end": 584,
    "label": "Unsupported_claim"
  },
  {
    "span": "Transformer-based forecasters capture long-range dependencies (Zhou et al., 2021). Seasonal-trend decomposition reduces nonstationary variance (Hyndman et al., 2008). Probabilistic forecasting evaluates distributional accuracy via CRPS (Matheson and Winkler, 1976).",
    "document": "Introduction\n\nAccurate time-series forecasting often requires combining inductive biases for seasonality with flexible models capable of learning long-term dependencies. However, best practices for integrating classical decomposition with modern deep architectures are unclear.\n\nTransformer-based forecasters capture long-range dependencies (Zhou et al., 2021). Seasonal-trend decomposition reduces nonstationary variance (Hyndman et al., 2008). Probabilistic forecasting evaluates distributional accuracy via CRPS (Matheson and Winkler, 1976). Recent studies propose hybrid models that mix decomposition with attention (Wu et al., 2021), but results vary widely across datasets.\n\nWe provide a controlled benchmark that factors out leakage and mis-specified horizons, showing when decomposition aids attention and when it hurts calibration.",
    "reason": "The span abruptly shifts from modeling approaches to preprocessing and then to evaluation metrics without transitions or an explicit narrative connecting the cited works, resulting in low coherence.",
    "start": 279,
    "end": 544,
    "label": "Coherence"
  },
  {
    "span": "Bolukbasi et al. (2016) quantified bias in word embeddings. Zhao et al. (2017) analyzed gender bias in coreference systems. Counterfactual data augmentation was proposed by Zhao et al. (2018). Adversarial debiasing has been studied for classification (Zhang et al., 2018).",
    "document": "Related Work\n\nFairness and Bias Mitigation in NLP\n\nNLP models can propagate and amplify social biases present in text corpora. Prior work spans measurement, analysis, and mitigation, with varying assumptions about data generating processes and downstream constraints.\n\nBolukbasi et al. (2016) quantified bias in word embeddings. Zhao et al. (2017) analyzed gender bias in coreference systems. Counterfactual data augmentation was proposed by Zhao et al. (2018). Adversarial debiasing has been studied for classification (Zhang et al., 2018).\n\nRecent surveys systematize these interventions but emphasize the need for task-aware evaluation and causal reasoning. We follow this direction by integrating counterfactual interventions with constrained optimization to preserve utility on minority subpopulations.",
    "reason": "Each sentence cites a different strand (measurement, analysis, data augmentation, adversarial training) without transitions explaining their relation or progression. The connections are implied but not explicit, causing coherence issues across multiple sentences (criteria a and b).",
    "start": 269,
    "end": 541,
    "label": "Coherence"
  },
  {
    "span": "there are many recent works that explore retrieval-augmented code LLMs",
    "document": "Related Work\n\nCode summarization has benefited from pre-trained sequence-to-sequence models trained on large corpora of code and natural language descriptions. While early approaches emphasized syntax trees and path-based encodings, current models leverage transformer architectures and in-context learning for adaptation. In addition to purely parametric models, there is growing interest in hybrid systems that integrate external codebases, APIs, or documentation to ground generation. Specifically, there are many recent works that explore retrieval-augmented code LLMs that fetch relevant snippets, comments, or usage examples to inform the decoder.\n\nParallel strands of research have investigated contrastive pretraining on code–docstring pairs, multi-task training with bug-fixing and code translation, and prompt engineering for summarization quality. However, how retrieval granularity (file-level vs. function-level) and index freshness impact summary faithfulness remains under-explored.",
    "reason": "Mentions 'many recent works' on retrieval-augmented code LLMs without providing any citations to support the claim.",
    "start": 502,
    "end": 572,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Liu et al., 2021)",
    "document": "Related Work\n\nPrompt-based learning. In (Liu et al., 2021), prompting methods are categorized by whether the model or the prompt is tuned, and by the nature of the verbalizers. Subsequent work adopts both discrete and continuous prompts to bridge the gap between pre-training tasks and downstream objectives (Schick and Schütze, 2021; Gao et al., 2021). For multilingual settings, prompt transfer remains underexplored, with most studies focusing on English-only tasks (Winata et al., 2021; Zhao and Schütze, 2021).\n\nOur method uses a frozen prompt with parameter-efficient adapters, targeting label-semantic alignment in low-resource scenarios.",
    "reason": "Wrong citation style: capitalized 'In' preceding a parenthetical citation is ungrammatical; it should be \"In Liu et al. (2021)\" for narrative or the 'in' should be lowercase and outside the parentheses.",
    "start": 37,
    "end": 58,
    "label": "Format"
  },
  {
    "span": "Privacy-preserving analytics across institutions are often framed under federated learning, where model updates are aggregated centrally without sharing raw data (McMahan et al., 2017; Li et al., 2020; Kairouz et al., 2021; Rieke et al., 2020). Differential privacy and secure aggregation mitigate leakage risks (Abadi et al., 2016; Bonawitz et al., 2017; Truex et al., 2019).",
    "document": "Introduction\n\nHealthcare institutions increasingly seek collaborative training to improve predictive models for rare diseases and long-tail outcomes. Centralized pooling of data is hampered by regulatory, ethical, and technical constraints, creating demand for methods that learn across silos without raw data exchange.\n\nDespite progress, practical deployments face strong heterogeneity in patient populations, devices, and clinical workflows, as well as strict privacy guarantees that can degrade utility if naively applied.\n\nPrivacy-preserving analytics across institutions are often framed under federated learning, where model updates are aggregated centrally without sharing raw data (McMahan et al., 2017; Li et al., 2020; Kairouz et al., 2021; Rieke et al., 2020). Differential privacy and secure aggregation mitigate leakage risks (Abadi et al., 2016; Bonawitz et al., 2017; Truex et al., 2019).\n\nWe present a heterogeneity-aware federated objective with cohort calibration and adaptive clipping that maintains privacy budgets while aligning gradients across disparate sites, yielding improved calibration and robustness.",
    "reason": "The span summarizes standard frameworks and tools without articulating their shortcomings in the healthcare context or how the proposed approach builds upon or differs from them; it lacks synthesis.",
    "start": 527,
    "end": 903,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In intrusion detection, classical machine learning models such as SVMs and random forests, as well as deep networks including CNNs and RNNs, have been widely adopted (Lee et al., 2000; Tavallaee et al., 2009; Yin et al., 2017; Kim et al., 2018).",
    "document": "Related Work\n\nNetwork intrusion detection systems (IDS) must detect evolving threats with limited labeled data and shifting traffic patterns.\n\nIn intrusion detection, classical machine learning models such as SVMs and random forests, as well as deep networks including CNNs and RNNs, have been widely adopted (Lee et al., 2000; Tavallaee et al., 2009; Yin et al., 2017; Kim et al., 2018). Recent datasets incorporate flow-level and packet-level features, enabling both coarse and fine-grained detection.\n\nOur method employs contrastive pretraining on raw flows followed by a lightweight classifier, aiming to reduce false positives while retaining recall on rare attack types.",
    "reason": "The span summarizes prior models with citations but does not articulate how they relate to the authors’ contrastive approach or the specific shortcomings being addressed.",
    "start": 143,
    "end": 388,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Antol et al. (2015) introduced the visual question answering task. Lu et al. (2019) proposed ViLBERT for two-stream pretraining. Chen et al. (2020) presented UNITER with unified cross-modal objectives. Radford et al. (2021) trained CLIP with contrastive image-text pretraining.",
    "document": "Related Work\n\nVision-Language Pretraining\n\nPretraining on paired image-text data has advanced multimodal understanding and generation, yet differences in architecture, objectives, and data curation complicate unified comparisons. Works vary in how they align modalities and what supervision signals they exploit.\n\nAntol et al. (2015) introduced the visual question answering task. Lu et al. (2019) proposed ViLBERT for two-stream pretraining. Chen et al. (2020) presented UNITER with unified cross-modal objectives. Radford et al. (2021) trained CLIP with contrastive image-text pretraining.\n\nDownstream Transfer and Few-Shot Adaptation\n\nSubsequent research explores adapter-based finetuning and prompt-based conditioning for downstream tasks under limited labels, but cross-dataset generalization still hinges on pretraining distribution alignment.\n\nOur Work\n\nWe develop a contrastive-distillation hybrid that aligns region-level and sentence-level features under curriculum sampling, improving few-shot VQA and retrieval.",
    "reason": "The span provides a sequence of citations without transitions or explicit statements of how they relate, resulting in an abrupt, incoherent flow between works.",
    "start": 314,
    "end": 591,
    "label": "Coherence"
  },
  {
    "span": "Multilingual ASR has benefited from self-supervised pre-training such as wav2vec 2.0 (Baevski et al., 2020), HuBERT (Hsu et al., 2021), and XLS-R (Babu et al., 2021), as well as transducer and attention-based encoder-decoder architectures (Graves, 2012; Chan et al., 2016; Pratap et al., 2020).",
    "document": "Related Work\n\nMultilingual automatic speech recognition. Training a single ASR system that serves many languages is attractive for low-resource coverage and maintenance. Multilingual ASR has benefited from self-supervised pre-training such as wav2vec 2.0 (Baevski et al., 2020), HuBERT (Hsu et al., 2021), and XLS-R (Babu et al., 2021), as well as transducer and attention-based encoder-decoder architectures (Graves, 2012; Chan et al., 2016; Pratap et al., 2020). Additional lines of work consider pronunciation lexica, multilingual tokenization, and language identification coupling.\n\nAdaptation and transfer. Parameter-efficient tuning, adapters, and layer-wise language specialization have been investigated to balance sharing and per-language adaptation (Pfeiffer et al., 2020; Houlsby et al., 2019). Data balancing strategies and sampling schedules mitigate overfitting to high-resource languages (Conneau et al., 2020).\n\nEvaluation. Benchmarks like MLS, FLEURS, and VoxPopuli facilitate standardized evaluation across typologically diverse languages and domains (Pratap et al., 2020; Conneau et al., 2022; Wang et al., 2022).\n",
    "reason": "This span enumerates methods and models but does not explain their limitations or how the current study interacts with them, leaving the author’s perspective and gap unspecified (criteria a and c).",
    "start": 170,
    "end": 464,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Neural image and video compression models employ variational autoencoders, hyperpriors, autoregressive entropy models, and learned transforms to approach or surpass traditional codecs (Ballé et al., 2017; Minnen et al., 2018; Agustsson et al., 2020; Mentzer et al., 2020; Yang et al., 2021).",
    "document": "Related Work\n\nLearned compression seeks to replace hand-engineered transforms with data-driven representations optimized for rate–distortion trade-offs. Recent progress has narrowed or exceeded the gap to classical codecs across diverse content.\n\nNeural image and video compression models employ variational autoencoders, hyperpriors, autoregressive entropy models, and learned transforms to approach or surpass traditional codecs (Ballé et al., 2017; Minnen et al., 2018; Agustsson et al., 2020; Mentzer et al., 2020; Yang et al., 2021).\n\nWe study content-adaptive bitrate allocation under strict latency constraints, introducing a test-time modulation scheme that adjusts hyperprior bandwidth without full re-encoding.",
    "reason": "The span catalogs methods and citations but does not explain their relation to the present study or identify what is missing, thus lacking synthesis.",
    "start": 247,
    "end": 538,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Nguyen, 2020",
    "document": "Related Work\n\nModern object detection balances accuracy and efficiency via feature pyramids and anchor-free designs (Lin et al., 2017; Tian et al., 2019). One-stage detectors simplify pipelines but may struggle with small objects without multi-scale attention (Zhang et al., 2020). Prior work (Nguyen, 2020 introduced an uncertainty-aware loss that down-weights ambiguous boxes during training, improving average precision under occlusion. Detection transformers further unify detection and set prediction with bipartite matching (Carion et al., 2020), while recent efforts reduce computational cost through sparse attention and dynamic token pruning (Sun et al., 2021). We build on these ideas by coupling uncertainty estimation with cascaded refinement to improve localization.",
    "reason": "Missing closing parenthesis in a parenthetical citation; it should be “(Nguyen, 2020)” with both opening and closing parentheses.",
    "start": 293,
    "end": 306,
    "label": "Format"
  },
  {
    "span": "A number of methods address privacy in federated learning using secure aggregation, differential privacy, or compression (Bonawitz et al., 2017; Geyer et al., 2018; Kairouz et al., 2021; Sattler et al., 2019). We build on this line with a new algorithm.",
    "document": "Introduction\n\nFederated learning enables collaborative model training without centralizing user data, reducing the risk of raw data exposure while still benefiting from cross-device or cross-silo information sharing. However, communication constraints, system heterogeneity, and privacy risks remain significant obstacles in practice.\n\nA number of methods address privacy in federated learning using secure aggregation, differential privacy, or compression (Bonawitz et al., 2017; Geyer et al., 2018; Kairouz et al., 2021; Sattler et al., 2019). We build on this line with a new algorithm.\n\nFrom a systems perspective, the tension between convergence speed and resource constraints motivates algorithmic choices that reduce communication rounds, client dropouts, and straggler effects, especially in mobile and healthcare settings.",
    "reason": "Summarizes previous work and immediately states an own contribution without articulating the gap or motivation the new algorithm addresses (definition b/c).",
    "start": 336,
    "end": 589,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There are many recent works that explore domain adaptation for neural machine translation.",
    "document": "Related Work\n\nNeural machine translation (NMT) systems often degrade when tested on domains that differ from their training data. To mitigate this, research has investigated adaptation strategies such as fine-tuning on in-domain corpora, instance reweighting, and data selection. There are many recent works that explore domain adaptation for neural machine translation. Nevertheless, it remains unclear how to balance stability and plasticity during adaptation, especially under limited in-domain resources. We study regularization strategies that constrain parameter drift while enabling targeted improvements.",
    "reason": "The phrase “many recent works” makes a prior work claim without any citations, violating rule (d).",
    "start": 280,
    "end": 370,
    "label": "Unsupported_claim"
  },
  {
    "span": "Exposure-based fairness constrains ranking to balance visibility across groups (Singh and Joachims, 2018). Graph embedding methods learn low-dimensional user-item representations (Perozzi et al., 2014). Negative sampling strategies affect training dynamics and bias (Rendle et al., 2009).",
    "document": "Related Work\n\nBias and fairness in recommender systems have been studied from the perspectives of metrics, optimization, and data collection. Debiasing methods operate on logs with selection bias and intervene on the recommender to reduce disparate outcomes across user groups and item providers.\n\nExposure-based fairness constrains ranking to balance visibility across groups (Singh and Joachims, 2018). Graph embedding methods learn low-dimensional user-item representations (Perozzi et al., 2014). Negative sampling strategies affect training dynamics and bias (Rendle et al., 2009). Other work examines counterfactual estimators for off-policy evaluation (Swaminathan and Joachims, 2015) and calibrates exposure via constrained bandits (Celis et al., 2018).\n\nWe contribute a counterfactual training objective that optimizes recommendation quality subject to exposure parity constraints, using a differentiable proxy for group-level visibility.",
    "reason": "Three back-to-back sentences cite different areas (fairness constraints, embeddings, sampling) without explicit links or transitions, making their connection and relevance to each other unclear.",
    "start": 298,
    "end": 586,
    "label": "Coherence"
  },
  {
    "span": "Recent works show that contrastive learning dominates supervised approaches on small speech datasets.",
    "document": "Related Work\n\nSelf-supervised acoustic representation learning has reduced labeled data requirements through predictive or masked objectives (Oord et al., 2018; Schneider et al., 2019; Baevski et al., 2020; Hsu et al., 2021). These methods yield features that transfer well to ASR, speaker verification, and emotion recognition, often outperforming log-mel baselines when fine-tuned with limited labels (Kahn et al., 2020; Mohamed et al., 2022). Data augmentation and curriculum strategies further stabilize low-resource training (Park et al., 2019; Ko et al., 2015).\n\nRecent works show that contrastive learning dominates supervised approaches on small speech datasets.\n\nIn this paper, we probe the limit regimes of labeled data scarcity and domain mismatch, comparing contrastive, masked-prediction, and hybrid objectives under controlled pretraining corpora and target tasks.",
    "reason": "Mentions unspecified 'recent works' as evidence without citing any sources; per guideline (d), references are required for such claims.",
    "start": 569,
    "end": 670,
    "label": "Unsupported_claim"
  },
  {
    "span": "Hundman et al. (2018) used LSTMs for spacecraft telemetry anomalies. Ren et al. (2019) proposed DAGMM combining autoencoders and Gaussian mixtures. Schölkopf et al. (2001) introduced one-class SVM for novelty detection. Xu et al. (2018) applied matrix profiles for discord discovery.",
    "document": "Related Work\n\nTime Series Anomaly Detection\nIndustrial monitoring applications require early and robust detection of rare deviations under non-stationary conditions (Laptev et al., 2015). Methods vary across statistical tests, distance-based approaches, and representation learning.\n\nDeep and Classical Methods\nNeural methods learn latent representations that capture normal behavior, while classical techniques provide interpretable scores or guarantees. Hundman et al. (2018) used LSTMs for spacecraft telemetry anomalies. Ren et al. (2019) proposed DAGMM combining autoencoders and Gaussian mixtures. Schölkopf et al. (2001) introduced one-class SVM for novelty detection. Xu et al. (2018) applied matrix profiles for discord discovery.\n\nMultivariate Dependencies and Streaming\nRecent work addresses high-dimensional correlations and streaming constraints with graph structure and sketching (Malhotra et al., 2016; Shen et al., 2020). Our approach unifies reconstruction and forecasting with adaptive thresholds.",
    "reason": "The span abruptly enumerates disparate approaches (recurrent models, hybrid density models, kernel methods, and matrix profiles) without transitions that clarify their relationships or comparative roles. The connection between these citations is not explicitly stated.",
    "start": 456,
    "end": 739,
    "label": "Coherence"
  },
  {
    "span": "Lopez & Kim (2017)",
    "document": "Related Work\n\nData augmentation strategies such as back-translation, paraphrasing, and mixup improve generalization in low-resource settings (Sennrich et al., 2016; Fadaee et al., 2017; Zhang et al., 2018). Lopez & Kim (2017) propose token-level noising that preserves label semantics while regularizing the encoder. Subsequent work explores task-specific perturbations that maintain meaning constraints (Wei and Zou, 2019; Kobayashi, 2018). Our method unifies semantic constraints with distributional priors to automatically select augmentations that target model weaknesses.\n",
    "reason": "Incorrect use of ampersand in a narrative citation. In narrative form, use \"and\": \"Lopez and Kim (2017)\". Ampersands are reserved for parenthetical citations in many author–year styles.",
    "start": 207,
    "end": 225,
    "label": "Format"
  },
  {
    "span": "the current state-of-the-art on the CoNLL 2003 NER benchmark is 94.6 F1.",
    "document": "Introduction\n\nNamed entity recognition (NER) is a foundational task in information extraction. Advances in pretraining, span classification, and constrained decoding have steadily pushed benchmark results upward.\n\nDespite substantial progress, the current state-of-the-art on the CoNLL 2003 NER benchmark is 94.6 F1. Yet these models often rely on heavy ensembling and task-specific heuristics that limit deployability.\n\nWe present a lightweight architecture that achieves competitive performance without task-specific features, enabling faster inference and simpler deployment.",
    "reason": "Reports a specific SOTA statistic and benchmark result without citing the source (rule b).",
    "start": 244,
    "end": 316,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Ahmed et al, 2020)",
    "document": "Introduction\n\nCausal discovery from observational data has advanced through score-based search and constraint-based testing (Spirtes et al., 2000; Chickering, 2002). Continuous relaxations enable gradient-based optimization over DAGs (Zheng et al., 2018; Yu et al., 2019). Interventional equivalence classes and conditional independence tests remain central to identifiability (Peters et al., 2017). Recent works incorporate priors from temporal structure and sparsity (Runge, 2018; Ahmed et al, 2020). We propose a modular objective that jointly enforces acyclicity and stability under environment changes (Rojas-Carulla et al., 2018).",
    "reason": "Punctuation errors in citation: missing period after 'al.' and missing comma after 'al.'; should be '(Ahmed et al., 2020)'.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "Vision-language models have become the de facto standard for multimodal retrieval.",
    "document": "Related Work\n\nMultimodal retrieval connects images and text through a shared embedding space, enabling cross-modal search and captioning. Early pipelines relied on separately trained visual and textual features combined with metric learning. End-to-end pretraining on aligned corpora has improved transfer to downstream datasets and zero-shot settings.\n\nVision-language models have become the de facto standard for multimodal retrieval. Despite their strong performance, sensitivity to domain shifts (e.g., sketches, medical imagery) and annotation artifacts persists. Recent work investigates debiasing objectives and fine-grained alignment losses to improve robustness and interpretability.",
    "reason": "Broad claim about the dominance of a class of models is made without any supporting references.",
    "start": 354,
    "end": 436,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith et al.",
    "document": "Introduction\n\nDomain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled or sparsely labeled target domain with distributional shift (Ben-David et al., 2010). Adversarial approaches reduce discrepancy by aligning feature distributions across domains (Ganin et al., 2016), while moment-matching techniques minimize distances such as MMD or CORAL (Long et al., 2015; Sun and Saenko, 2016). Building on Smith et al., we propose a curriculum that progressively adapts high-variance features before low-variance features, thereby stabilizing training and improving target performance. Unlike self-training methods that rely on pseudo labels (Chen et al., 2020), our approach jointly optimizes an uncertainty-aware sampler and a domain-invariant encoder. We evaluate on VisDA and Office-Home, following established protocols (Tzeng et al., 2017; Saito et al., 2018), and report consistent gains over strong baselines.\n\nRelated Work\n\nAdversarial domain adaptation uses a domain discriminator to encourage indistinguishable representations (Ganin et al., 2016; Tzeng et al., 2017). Other lines of work leverage semi-supervised learning (Saito et al., 2018) or contrastive alignment (Kang et al., 2019). Our work is complementary and can be integrated with these techniques.",
    "reason": "Narrative citation missing year; in APA style, narrative form should include the year immediately after the authors (e.g., 'Smith et al. (2019)').",
    "start": 431,
    "end": 443,
    "label": "Format"
  },
  {
    "span": "In (Lopez and Chen, 2021)",
    "document": "Introduction\n\nScaling retrieval-augmented generation (RAG) requires efficient indexing and robust fusion of retrieved contexts (Lewis et al., 2020; Izacard and Grave, 2021). In (Lopez and Chen, 2021) we investigate sparse-dense hybrids that adaptively allocate compute across passages. Prior work has emphasized retrieval quality, but the interaction between retrieval uncertainty and decoder calibration is less understood (Kadavath et al., 2022; Lin et al., 2022).\n\nWe propose a calibration-aware retriever that estimates passage utility and modulates decoding entropy accordingly. Experiments on open-domain QA and fact verification show consistent gains over strong RAG baselines (Karpukhin et al., 2020; Petroni et al., 2021).",
    "reason": "Uses the wrong citation style by placing the citation in parentheses after “In”; it should be narrative, e.g., “In Lopez and Chen (2021) we …”.",
    "start": 174,
    "end": 199,
    "label": "Format"
  },
  {
    "span": "Although summarization is a prominent NLP task, summarization uncertainty has not been widely studied.  is the only work that focuses on uncertainty for summarization, but their work does not make use of Bayesian methods. They define a generated summary's uncertainty based on the entropy of each token generated by the model during the decoding phase. Their study includes experiments on CNN/DM and XSum using the PEGASUS and BART summarization models. Their main focus is on understanding different properties of uncertainty during the decoding phase, and their work is not directly comparable to ours.",
    "document": "Related work\n\nUncertainty estimation in deep learning is a topic that has been studied extensively. Bayesian deep learning includes a family of methods that attempt to capture the notion of uncertainty in deep neural networks. Such methods have gained increased popularity in the deep learning literature and there exist multiple applications in subfields such as Computer Vision (Kendall and Gal, 2017;Litjens et al., 2017; and Natural Language Processing (NLP) (Siddhant and Lipton, 2020;Lyu et al., 2020;.\n\nDespite their obvious advantage of modeling uncertainty, the main problem with Bayesian deep learning methods is the computational cost of full Bayesian inference. To tackle this problem, Gal and Ghahramani (2016) propose using standard dropout (Srivastava et al., 2014) as a practical approximation of Bayesian inference in deep neural networks and call this method Monte Carlo dropout.  use a convolutional neural network with Monte Carlo dropout in order to obtain an uncertainty estimate for active learning in the task of image classification. Houlsby et al. (2011) sample many networks with Monte Carlo simulation and propose an objective function that takes into account the disagreement and confidence of the predictions coming from these networks.\n\nSimilar methods have also been applied to NLP. In machine translation,  extend the Transformer architecture with MC dropout to get a Variational Transformer, and use it to sample multiple translations from the approximate posterior distribution. They also introduce BLEUVar, an uncertainty metric based on the BLEU score (Papineni et al., 2002) between pairs of the generated translations. Lyu et al. (2020) extend the work of  to question answering and propose an active learning approach based on a modified BLEUVar version. Similarly,  use a conditional random field to obtain uncertainty estimates for active learning and apply their method to named entity recognition.\n\nAlthough summarization is a prominent NLP task, summarization uncertainty has not been widely studied.  is the only work that focuses on uncertainty for summarization, but their work does not make use of Bayesian methods. They define a generated summary's uncertainty based on the entropy of each token generated by the model during the decoding phase. Their study includes experiments on CNN/DM and XSum using the PEGASUS and BART summarization models. Their main focus is on understanding different properties of uncertainty during the decoding phase, and their work is not directly comparable to ours.\n\n ",
    "start": 1943,
    "end": 2547,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Classical methods for anomaly detection in time series include ARIMA residual analysis and control charts (Box and Jenkins, 1970; Montgomery, 2007), while recent neural methods use LSTMs, TCNs, VAEs, GANs, and Transformers (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Lai et al., 2018; Wu et al., 2021).",
    "document": "Introduction\n\nDetecting anomalies in univariate and multivariate time series is crucial for monitoring industrial systems, cloud services, and healthcare signals. Recent advances in deep learning have broadened the modeling toolkit for capturing complex temporal dependencies.\n\nClassical methods for anomaly detection in time series include ARIMA residual analysis and control charts (Box and Jenkins, 1970; Montgomery, 2007), while recent neural methods use LSTMs, TCNs, VAEs, GANs, and Transformers (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Lai et al., 2018; Wu et al., 2021).\n\nWe propose a simple forecasting-then-thresholding baseline with calibration and evaluate it on widely used benchmarks.",
    "reason": "The span provides a survey-like list without relating these methods to the authors’ approach or identifying limitations, indicating a lack of synthesis.",
    "start": 278,
    "end": 599,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Exploration in reinforcement learning has been studied via intrinsic motivation, count-based bonuses, posterior sampling, and entropy regularization in both model-free and model-based settings (Bellemare et al., 2016; Houthooft et al., 2016; Osband et al., 2013; Pathak et al., 2017; Haarnoja et al., 2018).",
    "document": "Related Work\n\nSparse-reward environments demand effective exploration strategies to avoid myopic behaviors. However, exploration bonuses may introduce bias, while purely stochastic policies can be inefficient. We investigate exploration under strict interaction budgets.\n\nExploration in reinforcement learning has been studied via intrinsic motivation, count-based bonuses, posterior sampling, and entropy regularization in both model-free and model-based settings (Bellemare et al., 2016; Houthooft et al., 2016; Osband et al., 2013; Pathak et al., 2017; Haarnoja et al., 2018).\n\nOur method couples posterior sampling with conservative policy improvement to limit catastrophic trials while maintaining sufficient novelty in visited states.",
    "reason": "The span provides a generic list of exploration approaches without tying them to the low-interaction-budget scenario or highlighting why existing methods are inadequate (criteria a and c).",
    "start": 272,
    "end": 579,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Johnson and Lee, 2019)",
    "document": "Related Work\n\nLong-context sequence modeling has progressed through segment-level recurrence and memory augmentation, as exemplified by Transformer-XL and related variants (Dai et al., 2019; Beltagy et al., 2020). In (Johnson and Lee, 2019) a hierarchical attention mechanism is proposed to pool coarse-grained context across segments. Later, sparse attention patterns reduced complexity for longer inputs (Child et al., 2019; Zaheer et al., 2020). Beyond architecture, pretraining objectives tailored for long documents improved downstream summarization and QA (Lewis et al., 2020; Guo et al., 2021). Our approach unifies hierarchical pooling with learned memory compaction, extending observations in (Kitaev et al., 2020).",
    "reason": "Wrong citation style: narrative form should be written as 'In Johnson and Lee (2019) ...' without parentheses around the authors and year.",
    "start": 214,
    "end": 240,
    "label": "Format"
  },
  {
    "span": "Manabe and Wetherald (1967) established early general circulation models. Emanuel (1986) studied moist convection parameterization. Rasp et al. (2018) explored data-driven parameterizations with neural networks. The CMIP6 archive provides multi-model outputs (Eyring et al., 2016). Regional downscaling techniques produce fine-scale fields (Maraun et al., 2010).",
    "document": "Introduction\n\nData-Driven Parameterizations in Climate Models\n\nSubgrid-scale processes in climate models are typically represented by parameterizations that introduce systematic uncertainties. We examine whether machine learning can reduce biases while preserving physical constraints.\n\nManabe and Wetherald (1967) established early general circulation models. Emanuel (1986) studied moist convection parameterization. Rasp et al. (2018) explored data-driven parameterizations with neural networks. The CMIP6 archive provides multi-model outputs (Eyring et al., 2016). Regional downscaling techniques produce fine-scale fields (Maraun et al., 2010).\n\nOur experiments couple learned closures with an energy-conserving integrator and evaluate skill across multiple forcing scenarios.",
    "reason": "The sequence moves from parameterizations to a dataset archive (CMIP6) and then to downscaling without transitions; the connections among these topics are implied but not made explicit.",
    "start": 287,
    "end": 649,
    "label": "Coherence"
  },
  {
    "span": "Data augmentation for automatic speech recognition (ASR) has included speed perturbation, additive noise, reverberation, vocal tract length perturbation, and SpecAugment-style masking (Ko et al., 2015; Cui et al., 2015; Park et al., 2019; Hannun et al., 2014; Kim et al., 2019). Text-based augmentation through TTS and back-translation has also been studied (Rosenberg et al., 2019; Jia et al., 2019).",
    "document": "Related Work\n\nEnd-to-end ASR has advanced through encoder-decoder architectures with attention or transducers, benefiting from large-scale labeled corpora and self-supervised pretraining (Graves, 2012; Chan et al., 2016; Baevski et al., 2020). However, robustness to domain and acoustic variability remains a key challenge.\n\nData augmentation for automatic speech recognition (ASR) has included speed perturbation, additive noise, reverberation, vocal tract length perturbation, and SpecAugment-style masking (Ko et al., 2015; Cui et al., 2015; Park et al., 2019; Hannun et al., 2014; Kim et al., 2019). Text-based augmentation through TTS and back-translation has also been studied (Rosenberg et al., 2019; Jia et al., 2019).\n\nUnlabeled audio pretraining with contrastive or masked objectives has improved downstream accuracy and data efficiency (Baevski et al., 2020; Hsu et al., 2021). Domain adaptation efforts explore adversarial training, feature alignment, and meta-learning (Shinohara, 2016; Sun et al., 2017; Karpov et al., 2022).\n\nWe target low-resource conversational ASR with severe channel mismatch, introducing a controllable augmentation policy that conditions transformations on learned acoustic difficulty. Our experiments demonstrate consistent gains over fixed augmentations while maintaining recognition latency.",
    "reason": "The span lists augmentation techniques with citations but does not relate them to the paper’s aims or specify why another method is needed.",
    "start": 325,
    "end": 726,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Most robotics competitions now evaluate embodied agents on instruction following with natural language maps.",
    "document": "Related Work\n\nEmbodied AI investigates how agents perceive, plan, and act in simulated or real environments while grounded in language (Anderson et al., 2018; Savva et al., 2019). Benchmarks have expanded from navigation-only tasks to interactive object manipulation and instruction following.\n\nMost robotics competitions now evaluate embodied agents on instruction following with natural language maps. Parallel lines of work propose modular policies, language-conditioned planners, and neural mapping to connect perception with goal-directed behavior (Chaplot et al., 2020; Das et al., 2018).\n\nWe propose a bilingual instruction grounding dataset with aligned symbolic and free-form directives, paired with a hierarchical policy that separates language parsing from visuomotor control. We report generalization to unseen environments and novel instruction compositions.",
    "reason": "Claims a trend across competitions without citing any specific competitions or sources.",
    "start": 295,
    "end": 403,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior multimodal MT systems integrate visual features using concatenation, attention fusion, or gating over text encoders (Caglayan et al., 2016; Calixto et al., 2017; Elliott and Kádár, 2017; Ive et al., 2019).",
    "document": "Introduction\nMultimodal machine translation (MMT) seeks to improve translation quality by leveraging aligned images alongside source text. Visual context can disambiguate references and resolve ellipses but can also introduce noise if misaligned.\nPrior multimodal MT systems integrate visual features using concatenation, attention fusion, or gating over text encoders (Caglayan et al., 2016; Calixto et al., 2017; Elliott and Kádár, 2017; Ive et al., 2019). Data augmentation methods synthesize image–text pairs or exploit object tags to mitigate limited supervision (Grönroos et al., 2018; Sennrich et al., 2016; Elliott et al., 2017). Evaluation commonly uses Multi30K and Flickr30K with METEOR and BLEU.\nWe revisit the role of visual grounding under distribution shift and propose a selective fusion mechanism triggered by uncertainty. Our analysis isolates cases where images aid or harm translation and quantifies the trade-off between robustness and reliance on visual cues.",
    "reason": "The span lists technique categories and citations without clarifying how they relate to the new approach or identifying a concrete gap, demonstrating a lack of synthesis.",
    "start": 247,
    "end": 458,
    "label": "Lacks_synthesis"
  },
  {
    "span": "LibriSpeech test-clean has around 5.4 hours of audio",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) systems map acoustic features directly to token sequences using CTC, attention-based encoder-decoders, or transducer objectives. Self-supervised pretraining on large unlabeled corpora has substantially reduced labeled data requirements, and conformer encoders have become a strong default backbone.\n\nCommon evaluation datasets include LibriSpeech, TED-LIUM, and Common Voice, with word error rate as the primary metric. LibriSpeech test-clean has around 5.4 hours of audio, whereas test-other is more challenging due to accented speech and background noise. Researchers frequently report both greedy and beam search decoding to separate model quality from language model benefits.\n\nOur work addresses streaming constraints by introducing a chunkwise monotonic alignment mechanism that maintains low latency without sacrificing accuracy. We additionally propose a noise-conditioned layer normalization to stabilize training under domain mismatch.\n\nWe present results on multiple corpora with strict real-time factors and analyze trade-offs between context size, latency, and robustness to reverberation.",
    "reason": "Provides a specific dataset statistic without a citation or source for the stated duration.",
    "start": 480,
    "end": 532,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prompt-based learning reformulates tasks as cloze questions to better exploit pretrained knowledge (Gao et al., 2021; Liu et al., 2021). Instruction tuning finetunes models on a mixture of natural language tasks described with task instructions (Sanh et al., 2022; Wei et al., 2022). Prefix-tuning prepends learnable continuous vectors to condition generation without updating model weights (Li and Liang, 2021). Chain-of-thought prompting elicits intermediate reasoning steps (Wei et al., 2023; Kojima et al., 2022).",
    "document": "Related Work\n\nPrompting and Parameter-Efficient Adaptation for Large Language Models\nLarge pretrained language models can be adapted to downstream tasks through prompting, fine-tuning, or lightweight parameterization strategies (Brown et al., 2020; Houlsby et al., 2019). These approaches trade off data efficiency, controllability, and computational cost.\n\nPrompting and Tuning Methods\nPrompt-based learning reformulates tasks as cloze questions to better exploit pretrained knowledge (Gao et al., 2021; Liu et al., 2021). Instruction tuning finetunes models on a mixture of natural language tasks described with task instructions (Sanh et al., 2022; Wei et al., 2022). Prefix-tuning prepends learnable continuous vectors to condition generation without updating model weights (Li and Liang, 2021). Chain-of-thought prompting elicits intermediate reasoning steps (Wei et al., 2023; Kojima et al., 2022). Recent work studies emergent generalization under mixtures of demonstrations and rationales (Magister et al., 2022).\n\nEvaluation\nBenchmarks like SuperGLUE, BIG-bench, and reasoning-heavy datasets test few-shot and zero-shot capabilities (Wang et al., 2019; Srivastava et al., 2022). We evaluate controllability and robustness across varying prompt templates and tuning budgets.",
    "reason": "The span lists several prompting and tuning approaches as isolated statements with no transitions or explicit comparisons, resulting in unclear coherence among the cited works.",
    "start": 387,
    "end": 904,
    "label": "Coherence"
  },
  {
    "span": " AM-PERE  for proposition classification in reviews",
    "document": "Related Work\n\nTo facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al., 2018). Datasets for long docu-ments include Sharma et al. (2019), Cohan et al. (2018), andFisas et al. (2016). In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely structurecontrollable text generation.\n\nResearchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AM-PERE  for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs. Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation. There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.\n\nA wide range of control perspectives has been explored in controllable generation, including style control (e.g., sentiments (Duan et al., 2020), politeness (Madaan et al., 2020), formality , domains (Takeno et al., 2017) and persona ) and content control (e.g., length (Duan et al., 2020), entities (Fan et al., 2018a), and keywords (Tang et al., 2019)). Our structure-controlled generation differs from these works as we control the high-level output structure, rather than the specific styles or the surface details of which keywords to include in the generated output. Our task also differs from content planning (Reiter and Dale, 1997;Shao et al., 2019;, which involves explicitly selecting and arranging the input content. Instead, we provide the model with the high-level control labels, and let the model decide on its own the relevant styles and contents.\n\n ",
    "start": 855,
    "end": 906,
    "label": "Unsupported_claim"
  },
  {
    "span": " (Reiter and Dale, 1997;Shao et al., 2019;,",
    "document": "Related Work\n\nTo facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al., 2018). Datasets for long docu-ments include Sharma et al. (2019), Cohan et al. (2018), andFisas et al. (2016). In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely structurecontrollable text generation.\n\nResearchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AM-PERE  for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs. Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation. There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.\n\nA wide range of control perspectives has been explored in controllable generation, including style control (e.g., sentiments (Duan et al., 2020), politeness (Madaan et al., 2020), formality , domains (Takeno et al., 2017) and persona ) and content control (e.g., length (Duan et al., 2020), entities (Fan et al., 2018a), and keywords (Tang et al., 2019)). Our structure-controlled generation differs from these works as we control the high-level output structure, rather than the specific styles or the surface details of which keywords to include in the generated output. Our task also differs from content planning (Reiter and Dale, 1997;Shao et al., 2019;, which involves explicitly selecting and arranging the input content. Instead, we provide the model with the high-level control labels, and let the model decide on its own the relevant styles and contents.\n\n ",
    "start": 1882,
    "end": 1925,
    "label": "Format"
  },
  {
    "span": "To facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al., 2018). Datasets for long docu-ments include Sharma et al. (2019), Cohan et al. (2018), andFisas et al. (2016).",
    "document": "Related Work\n\nTo facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al., 2018). Datasets for long docu-ments include Sharma et al. (2019), Cohan et al. (2018), andFisas et al. (2016). In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely structurecontrollable text generation.\n\nResearchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AM-PERE  for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs. Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation. There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.\n\nA wide range of control perspectives has been explored in controllable generation, including style control (e.g., sentiments (Duan et al., 2020), politeness (Madaan et al., 2020), formality , domains (Takeno et al., 2017) and persona ) and content control (e.g., length (Duan et al., 2020), entities (Fan et al., 2018a), and keywords (Tang et al., 2019)). Our structure-controlled generation differs from these works as we control the high-level output structure, rather than the specific styles or the surface details of which keywords to include in the generated output. Our task also differs from content planning (Reiter and Dale, 1997;Shao et al., 2019;, which involves explicitly selecting and arranging the input content. Instead, we provide the model with the high-level control labels, and let the model decide on its own the relevant styles and contents.\n\n ",
    "start": 14,
    "end": 415,
    "label": "Coherence"
  },
  {
    "span": "Researchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AM-PERE  for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs. Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation. There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.",
    "document": "Related Work\n\nTo facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al., 2018). Datasets for long docu-ments include Sharma et al. (2019), Cohan et al. (2018), andFisas et al. (2016). In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely structurecontrollable text generation.\n\nResearchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AM-PERE  for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs. Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation. There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.\n\nA wide range of control perspectives has been explored in controllable generation, including style control (e.g., sentiments (Duan et al., 2020), politeness (Madaan et al., 2020), formality , domains (Takeno et al., 2017) and persona ) and content control (e.g., length (Duan et al., 2020), entities (Fan et al., 2018a), and keywords (Tang et al., 2019)). Our structure-controlled generation differs from these works as we control the high-level output structure, rather than the specific styles or the surface details of which keywords to include in the generated output. Our task also differs from content planning (Reiter and Dale, 1997;Shao et al., 2019;, which involves explicitly selecting and arranging the input content. Instead, we provide the model with the high-level control labels, and let the model decide on its own the relevant styles and contents.\n\n ",
    "start": 713,
    "end": 1264,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Contrastive learning maximizes instance discrimination (Chen et al., 2020). Clustering-based self-supervision groups semantically similar samples (Caron et al., 2020). Masked autoencoding reconstructs occluded content (He et al., 2022).",
    "document": "Related Work\n\nSelf-supervised learning (SSL) for video has advanced rapidly, yet the design space spans multiple objectives and data augmentations that complicate method comparison. Understanding how objectives exploit temporal and spatial structure is especially important for downstream tasks such as retrieval and action recognition.\n\nContrastive learning maximizes instance discrimination (Chen et al., 2020). Clustering-based self-supervision groups semantically similar samples (Caron et al., 2020). Masked autoencoding reconstructs occluded content (He et al., 2022). Multi-modal objectives further align video and audio streams to leverage cross-modal cues (Alayrac et al., 2020). Yet, how these families of methods complement or compete with each other under limited labels remains unclear.\n\nWe analyze objective synergies by factorizing temporal stride, masking ratio, and negative sampling density, revealing settings where combining local reconstruction with global discrimination yields consistent gains.",
    "reason": "The span lists three different SSL paradigms back-to-back with no transition phrases or explanation of their relationships. The link between each cited work and the preceding sentence is implied rather than explicit, creating abrupt topic shifts.",
    "start": 338,
    "end": 574,
    "label": "Coherence"
  },
  {
    "span": "Smith 2019 [12]",
    "document": "Related Work\n\nPersonalized recommender systems leverage user context and temporal signals to improve ranking quality. Early methods used factorization with time-decay and sequence-aware models, while recent approaches incorporate self-attention for long-range preferences. Context-aware ranking was introduced by Smith 2019 [12] and extended with hierarchical attention to capture session dynamics (Lee and Park, 2020; Gupta et al., 2021). Our method unifies context and intent signals via a contrastive pretraining objective on interaction graphs.",
    "reason": "Mixed citation styles in a single reference: combines author–year (\"Smith 2019\") with numeric style (\"[12]\"). It should use a consistent format, e.g., \"Smith (2019)\" or \"[12]\" alone, depending on the chosen style.",
    "start": 313,
    "end": 328,
    "label": "Format"
  },
  {
    "span": "Bellemare et al. (2016) introduced pseudo-counts for exploration in high-dimensional spaces. Pathak et al. (2017) proposed curiosity-driven exploration using prediction error as intrinsic reward. Osband et al. (2016) investigated Bootstrapped DQN for deep exploration. Entropy regularization in policy gradients can also encourage broader state visitation (Mnih et al., 2016).",
    "document": "Related Work\n\nExploration in reinforcement learning (RL) addresses the fundamental challenge of efficiently discovering rewarding behaviors in sparse or deceptive environments. Approaches vary from intrinsic motivation and uncertainty estimation to optimism and posterior sampling.\n\nBellemare et al. (2016) introduced pseudo-counts for exploration in high-dimensional spaces. Pathak et al. (2017) proposed curiosity-driven exploration using prediction error as intrinsic reward. Osband et al. (2016) investigated Bootstrapped DQN for deep exploration. Entropy regularization in policy gradients can also encourage broader state visitation (Mnih et al., 2016).\n\nRecent work integrates model-based rollouts, latent space novelty, and structured priors to better balance exploration and exploitation (Sekar et al., 2020; Henaff et al., 2022). Our study analyzes the sample complexity of combining intrinsic rewards with distributional value functions.",
    "reason": "Multiple papers are listed back-to-back without transitions or explicit comparisons, leaving unclear how one approach relates to the next or why they are grouped together.",
    "start": 283,
    "end": 659,
    "label": "Coherence"
  },
  {
    "span": "(Omar and Lin, 2016; Wu, 2018;.",
    "document": "Related Work\n\nOpen-domain information extraction (OpenIE) relaxes schema constraints to harvest tuples from unstructured text (Banko et al., 2007; Mausam, 2016). Early systems relied on pattern learning and clause-based segmentation (Fader et al., 2011; Del Corro and Gemulla, 2013), while neural models improved context sensitivity and boundary detection (Stanovsky et al., 2018; Kolluru et al., 2020). Distant supervision scales relation extraction but introduces noise due to alignment errors (Mintz et al., 2009; Riedel et al., 2010; Zeng et al., 2015).\n\nJoint modeling of entities and relations reduces error propagation inherent in pipeline systems (Miwa and Bansal, 2016; Bekoulis et al., 2018). Span-based encoders and biaffine classifiers have demonstrated state-of-the-art performance on benchmark corpora (Luan et al., 2019; Wadden et al., 2019). Recent work explores document-level context and coreference to capture long-range dependencies (Yao et al., 2019; Nan et al., 2020).\n\nOur study focuses on calibration and confidence estimation for OpenIE outputs, complementing prior efforts on noise-robust training (Zheng et al., 2017; Jia et al., 2019). We compare thresholding strategies and post-hoc calibration across domains (News, Wiki, Web), and evaluate transfer with domain-adversarial objectives (Ganin et al., 2016). We also analyze error categories following prior taxonomies (Omar and Lin, 2016; Wu, 2018;. Our findings suggest that calibration substantially affects downstream KB population, especially under class imbalance.",
    "reason": "Malformed multi-citation punctuation with a trailing semicolon and period inside the parentheses; it should be '(Omar and Lin, 2016; Wu, 2018)'.",
    "start": 1397,
    "end": 1428,
    "label": "Format"
  },
  {
    "span": "(O'Neil 2017)",
    "document": "Related Work\n\nPolicy gradient methods optimize expected return via stochastic gradients (Williams, 1992; Sutton et al., 2000). Trust region and proximal constraints stabilize updates (Schulman et al., 2015; Schulman et al., 2017). Off-policy evaluation remains challenging due to distribution shift (Precup et al., 2000). Recent analyses of variance reduction in importance sampling include baselines and control variates (Greensmith et al., 2004). Empirical studies in healthcare simulators have highlighted identifiability concerns (O'Neil 2017). We extend these results with model-based estimators and doubly robust objectives.",
    "reason": "Missing comma between author and year in a parenthetical citation; should be '(O'Neil, 2017)'.",
    "start": 534,
    "end": 547,
    "label": "Format"
  },
  {
    "span": "Recent works have shown that discrete prompts outperform continuous prompts for low-resource NER.",
    "document": "Introduction\n\nPrompt-based learning has emerged as a promising approach for adapting pretrained language models to sequence labeling tasks under limited supervision. In named entity recognition (NER), prompt formulations can steer a model to produce label tokens without adding large task-specific heads, which is attractive when annotation budgets are small or domains shift rapidly. Despite these advantages, there remains uncertainty about when to prefer discrete over continuous prompts and how to design templates that remain robust across domains and languages.\n\nRecent works have shown that discrete prompts outperform continuous prompts for low-resource NER. However, it is unclear whether these gains arise from better alignment with token-level supervision, improved calibration, or differences in optimization dynamics. To address these questions, we conduct a systematic study of prompt types, verbalizers, and calibration strategies across a suite of low-resource settings. We focus on regimes with as few as 20 labeled sentences per entity type and evaluate domain transfer to biomedical and social media text.\n\nOur contributions are threefold: we provide a unified evaluation protocol for prompt-based NER, analyze error patterns to disentangle calibration versus representation effects, and introduce a lightweight tuning strategy that reduces variance across random seeds.",
    "reason": "The sentence generalizes findings of 'recent works' without providing citations to those studies, violating the requirement that mentions of prior work be supported by references.",
    "start": 569,
    "end": 666,
    "label": "Unsupported_claim"
  },
  {
    "span": "Melis et al. (2019) demonstrated gradient leakage attacks on collaborative learning. Truex et al. (2019) analyzed membership inference in federated settings. McMahan et al. (2017) proposed FedAvg for decentralized optimization. Differential privacy can mitigate leakage (Geyer et al., 2017).",
    "document": "Related Work\n\nFederated learning (FL) enables model training across decentralized data silos while aiming to protect user privacy. However, attacks exploiting updates or auxiliary information can reveal sensitive attributes or membership, necessitating rigorous defenses.\n\nMelis et al. (2019) demonstrated gradient leakage attacks on collaborative learning. Truex et al. (2019) analyzed membership inference in federated settings. McMahan et al. (2017) proposed FedAvg for decentralized optimization. Differential privacy can mitigate leakage (Geyer et al., 2017).\n\nSubsequent research explored secure aggregation, compression for communication efficiency, and robust aggregation against poisoning (Bonawitz et al., 2017; Yin et al., 2018). Our work evaluates privacy-utility trade-offs under non-iid client distributions.",
    "reason": "The sequence jumps between attacks, core optimization algorithms, and defenses without stating how each relates to the previous, lacking transitions and explicit connections.",
    "start": 273,
    "end": 564,
    "label": "Coherence"
  },
  {
    "span": "(et al., 2022)",
    "document": "Related Work\n\nComputational genomics increasingly relies on deep models to predict regulatory activity from sequence (Alipanahi et al., 2015; Zhou and Troyanskaya, 2015). Multi-task architectures capture shared patterns across assays, improving data efficiency (Kelley et al., 2018; Avsec et al., 2021). Recent contrastive pretraining approaches leverage paired modalities such as sequence and accessibility profiles to learn transferable features (Schreiber and Durham, 2020). However, (et al., 2022) report that model calibration degrades on rare motifs, complicating variant effect prediction.\n\nOur method introduces uncertainty-aware ensembling with motif-level temperature scaling to improve calibration without sacrificing accuracy.",
    "reason": "Incomplete citation lacking author names; “(et al., 2022)” is invalid and should include at least one author surname.",
    "start": 487,
    "end": 501,
    "label": "Format"
  },
  {
    "span": "Deep knowledge tracing variants include RNN-based, attention-based, and graph-based formulations (Piech et al., 2015; Pandey and Karypis, 2019; Ghosh et al., 2020; Zhang et al., 2020b).",
    "document": "Related Work\nKnowledge tracing models estimate a learner’s evolving mastery of skills from interaction logs. Classical Bayesian models assume factorized skills, whereas neural approaches capture complex temporal dependencies.\nDeep knowledge tracing variants include RNN-based, attention-based, and graph-based formulations (Piech et al., 2015; Pandey and Karypis, 2019; Ghosh et al., 2020; Zhang et al., 2020b). Auxiliary signals such as exercise text and concept tags have been incorporated to improve generalization (Liu et al., 2019; Wang et al., 2020a). Recent benchmarks standardize splits and address data leakage (Choi et al., 2020; Gervet et al., 2020).\nOur work studies calibration and early-warning detection of forgetting in low-interaction regimes. We propose a confidence-aware predictor and evaluate its utility for adaptive assessment design.",
    "reason": "The span merely catalogs prior variants with citations and does not explain their relevance to the authors’ objectives or the specific gap addressed, hence lacking synthesis.",
    "start": 226,
    "end": 411,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent surveys estimate that 65% of clinical notes contain at least one medication mention.",
    "document": "Introduction\n\nClinical NLP seeks to unlock information in EHR notes for downstream applications such as pharmacovigilance and cohort selection (Wang et al., 2018; Johnson et al., 2016). Named entity recognition of problems, tests, and treatments has been standardized through shared tasks (Uzuner et al., 2011; Henry et al., 2020). Recent surveys estimate that 65% of clinical notes contain at least one medication mention. This motivates robust medication extraction models that perform across institutions and note types (Jagannatha and Yu, 2016; Si et al., 2019).",
    "reason": "Presents a precise statistic (65%) without citing the source.",
    "start": 332,
    "end": 423,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior work on multimodal summarization explores hierarchical encoders for text (Nallapati et al., 2016), visual attention over keyframes (Li et al., 2017; Sun et al., 2019), and audio-aware segment scoring (Cohen et al., 2020; Kim et al., 2021). Approaches range from extractive pipelines (Zhang et al., 2019) to end-to-end abstractive models that fuse modalities using cross-attention or gating (Hori et al., 2017; Liu et al., 2020).",
    "document": "Introduction\n\nMultimodal summarization aims to produce concise synopses of long videos, lectures, or meetings by integrating signals from text, vision, and audio. Despite the prevalence of single-modality summarizers, practical scenarios such as broadcast news and instructional videos demand models that can align dialogues, slide content, and visual context. The challenge lies in how to effectively fuse heterogeneous signals at different temporal scales while controlling redundancy and improving factuality.\n\nPrior work on multimodal summarization explores hierarchical encoders for text (Nallapati et al., 2016), visual attention over keyframes (Li et al., 2017; Sun et al., 2019), and audio-aware segment scoring (Cohen et al., 2020; Kim et al., 2021). Approaches range from extractive pipelines (Zhang et al., 2019) to end-to-end abstractive models that fuse modalities using cross-attention or gating (Hori et al., 2017; Liu et al., 2020).\n\nIn this paper, we present a lightweight fusion architecture that learns modality-specific confidences and calibrates them dynamically during decoding. We introduce a curriculum that progressively increases cross-modal interactions and a factuality-aware loss that penalizes content not grounded in any modality. Experiments on two public benchmarks show consistent improvements over strong baselines while reducing inference cost.",
    "reason": "Lists prior works and techniques without explaining how they relate to the proposed approach or what specific gap remains; no author perspective or motivation is articulated.",
    "start": 514,
    "end": 948,
    "label": "Lacks_synthesis"
  },
  {
    "span": "UCI Adult dataset",
    "document": "Related Work\n\nFairness in machine learning often centers on tabular classification tasks where sensitive attributes correlate with outcomes. A large body of work proposes pre-processing, in-processing, and post-processing techniques to mitigate disparate impact while maintaining utility.\n\nTo compare with prior art, we report results on the UCI Adult dataset and COMPAS recidivism predictions, focusing on demographic parity and equalized odds trade-offs.\n\nWe further analyze thresholding effects and calibration under label shift to understand practical deployment constraints.",
    "reason": "First mention of a specific dataset (UCI Adult) requires a citation to its source or description (guideline a).",
    "start": 342,
    "end": 359,
    "label": "Unsupported_claim"
  },
  {
    "span": "A previous study found that language models trained on social media outperform broadcast transcripts for code-switched ASR.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) for code-switched speech remains challenging due to rapid language alternations, phonotactic interference, and limited labeled corpora (Yilmaz et al., 2018; Li and Vu, 2019). Advances include multilingual acoustic modeling, pronunciation lexicon unification, and subword tokenization strategies tailored to mixed-language data (Toshniwal et al., 2018; Ye et al., 2021).\n\nLanguage modeling plays a crucial role in resolving ambiguities at switch points and capturing mixed-language syntax. A previous study found that language models trained on social media outperform broadcast transcripts for code-switched ASR. Nevertheless, domain mismatch and noisy orthography can degrade ASR when user-generated text deviates significantly from spoken forms.\n\nWe introduce a contrastive domain adaptation framework that aligns textual pretraining with acoustic evidence using pseudo-transcripts derived from self-training, improving robustness across conversational domains.",
    "reason": "This references a 'previous study' and reports a comparative finding without any citation, which is an unsupported claim about prior work.",
    "start": 538,
    "end": 661,
    "label": "Unsupported_claim"
  },
  {
    "span": "In a previous study, the authors showed that pruning 90% of weights hardly affects BLEU on machine translation",
    "document": "Related Work\n\nModel compression for neural machine translation spans pruning, quantization, and knowledge distillation. Unstructured pruning removes individual weights based on magnitude or sensitivity, while structured pruning targets channels or attention heads. In a previous study, the authors showed that pruning 90% of weights hardly affects BLEU on machine translation, motivating aggressive sparsification in deployment settings. Quantization further reduces memory footprint, and distillation can recover accuracy lost during compression.\n\nDespite these advances, most evaluations focus on English-centric benchmarks and overlook low-resource languages where sparsity may interact with data scarcity.",
    "reason": "Mentions a specific prior result with numerical detail but provides no citation to the study (rule b).",
    "start": 265,
    "end": 375,
    "label": "Unsupported_claim"
  },
  {
    "span": "In a previous study, the authors claim that adversarial training degrades calibration on natural images.",
    "document": "Related Work\n\nAdversarial robustness research has produced a spectrum of defenses, from gradient obfuscation to certified robustness, with adversarial training emerging as the most reliable empirical method. Despite improving worst-case accuracy, questions remain about its impact on generalization, uncertainty, and calibration. In a previous study, the authors claim that adversarial training degrades calibration on natural images. Subsequent efforts have proposed temperature scaling and margin-aware losses to mitigate this effect, but a comprehensive picture across architectures and datasets is still lacking. We contribute a systematic evaluation with unified metrics and training protocols.",
    "reason": "Refers to a specific 'previous study' and its claim without providing a citation.",
    "start": 330,
    "end": 434,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Lopez et al., 2018)",
    "document": "Related Work\n\nMultimodal fusion has progressed from simple early-fusion concatenation to more expressive cross-attention mechanisms (Tsai et al., 2019; Sun et al., 2020). In (Lopez et al., 2018) a probabilistic gating unit was proposed to adaptively weight visual features given linguistic context, showing gains on VQA benchmarks. However, subsequent analyses indicated that such gains were dataset-specific and sensitive to spurious correlations (Goyal et al., 2019). To mitigate these issues, contrastive objectives aligning modalities at multiple granularities have become common (Radford et al., 2021; Jia et al., 2021). Our method extends these ideas by introducing a structure-aware alignment head that preserves relational cues across modalities.\n\nWe situate our work within the trend toward stronger inductive biases in fusion modules while maintaining scalability.",
    "reason": "Wrong citation style: preposition directly before a parenthetical citation; should be 'In Lopez et al. (2018) ...' for narrative usage.",
    "start": 171,
    "end": 194,
    "label": "Format"
  },
  {
    "span": "Two-stage detectors with region proposal networks reduce search space (Ren et al., 2015). Vision Transformers model long-range dependencies (Dosovitskiy et al., 2021). Anchor-free designs avoid predefined priors (Tian et al., 2019). Self-training improves low-data detectors (Rosenfeld et al., 2020).",
    "document": "Related Work\n\nObject detection research spans two-stage proposal-based methods and one-stage dense prediction approaches. Two-stage systems such as Faster R-CNN refine a small set of candidate regions to achieve high accuracy (Ren et al., 2015), while one-stage models like RetinaNet trade some precision for speed using focal loss (Lin et al., 2017). Recent improvements also consider label assignment strategies and multi-scale features (Zhu et al., 2020; Liu et al., 2016).\n\nTwo-stage detectors with region proposal networks reduce search space (Ren et al., 2015). Vision Transformers model long-range dependencies (Dosovitskiy et al., 2021). Anchor-free designs avoid predefined priors (Tian et al., 2019). Self-training improves low-data detectors (Rosenfeld et al., 2020).\n\nWhile our method adopts a dense architecture, we focus on uncertainty-guided target assignment rather than architectural changes or pretraining alone.",
    "reason": "The span lists disparate contributions (RPNs, ViTs, anchor-free designs, and self-training) back-to-back without transitions or an explicit connective thread, making the relationship among these works unclear.",
    "start": 478,
    "end": 778,
    "label": "Coherence"
  },
  {
    "span": "It is well known that adding pink noise at 10 dB SNR reduces WER by roughly 15%.",
    "document": "Related Work: Robust ASR\nAutomatic speech recognition (ASR) systems degrade under background noise, reverberation, and channel mismatch. Techniques such as multi-condition training, speech enhancement, and self-supervised pretraining have improved robustness but do not eliminate brittleness to unseen acoustic conditions.\nIt is well known that adding pink noise at 10 dB SNR reduces WER by roughly 15%. Such rules of thumb motivate standardized corruption benchmarks that report performance across signal-to-noise ratios and noise types.\nIn this study, we systematize evaluation by constructing a controlled suite of noise conditions and report a detailed breakdown of error types across acoustic perturbations.",
    "reason": "Asserts a specific quantitative effect size without providing any supporting citation or evidence.",
    "start": 323,
    "end": 403,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most previous clinical NER work ignores abbreviations and shorthand used by practitioners",
    "document": "Introduction\n\nClinical named entity recognition (NER) supports downstream tasks such as cohort selection and adverse event detection. Domain-specific challenges include nonstandard orthography, deidentification artifacts, and frequent use of abbreviations.\n\nMost previous clinical NER work ignores abbreviations and shorthand used by practitioners, leading to systematic recall errors in critical settings. Addressing this gap requires models that integrate contextual disambiguation with lexicons tailored to clinical note styles.\n\nWe propose a hybrid approach combining contextual encoders with abbreviation expansion modules trained on weak supervision from clinical dictionaries.",
    "reason": "Broad claim about prior work practices without any citations to surveys or representative studies (rule a and b).",
    "start": 258,
    "end": 347,
    "label": "Unsupported_claim"
  },
  {
    "span": "Hinton et al.",
    "document": "Introduction\n\nKnowledge distillation compresses a large teacher into a smaller student by transferring soft targets and intermediate representations. Hinton et al. introduced temperature scaling to reveal dark knowledge in output distributions, inspiring a broad line of work on feature- and relation-based transfers. Subsequent studies explored self-distillation without an external teacher (Zhang et al., 2019) and multi-teacher ensembles (You et al., 2017). Distillation has also been applied to sequence models in speech and translation to reduce latency while maintaining accuracy (Kim and Rush, 2016).\n\nWe investigate curriculum temperature schedules that adapt to the student's learning phase. Our approach integrates layer-wise alignment with a teacher assistant and examines stability under distribution shift. We provide a systematic comparison across CIFAR, ImageNet, and WMT benchmarks.",
    "reason": "Narrative citation is missing the year; it should be 'Hinton et al. (2015)' (or the appropriate year) when used narratively.",
    "start": 150,
    "end": 163,
    "label": "Format"
  },
  {
    "span": "Nguyen, T., et.al., (2019)",
    "document": "Related Work\n\nEnd-to-end speech recognition has progressed from CTC-based systems to attention-driven encoder–decoder architectures (Harris and Venkataraman, 2018; Ortega and Bloom, 2020). Building on Nguyen, T., et.al., (2019) and Ortega et al. (2020), joint CTC/attention training has emerged as a strong baseline, while self-supervised pretraining on unlabeled audio further narrows the gap to fully supervised methods (Qin and Moss, 2021; Li and Pereira, 2022).\n\nWe extend conformer-based encoders with multi-resolution feature fusion to improve robustness in noisy environments, following insights from data augmentation and spectrogram masking (Rossi et al., 2020; Ahmed and Kwon, 2021).",
    "reason": "Incorrect formatting of \"et al.\" and punctuation. It should be \"Nguyen et al. (2019)\" in narrative style (no initials, no extra commas, and \"et al.\" with a space and period after \"al\").",
    "start": 201,
    "end": 227,
    "label": "Format"
  },
  {
    "span": "GNNs are susceptible to poisoning attacks on graphs (Zügner et al., 2018). Shapley-based explanations can attribute node importance (Yuan et al., 2021). Spectral normalization stabilizes training (Miyato et al., 2018).",
    "document": "Related Work\n\nGraph Neural Networks. Message passing neural networks achieve strong performance on node and graph classification by aggregating neighborhood information. However, they often struggle with oversmoothing and distribution shifts.\n\nRobustness and Explainability. Robust training and faithful explanations are critical for high-stakes domains such as fraud detection and recommendation. Defense mechanisms attempt to mitigate adversarial perturbations and spurious correlations.\n\nStability and Security. GNNs are susceptible to poisoning attacks on graphs (Zügner et al., 2018). Shapley-based explanations can attribute node importance (Yuan et al., 2021). Spectral normalization stabilizes training (Miyato et al., 2018).\n\nCalibration and Uncertainty. Additional work explores Bayesian variants and post-hoc calibration to quantify uncertainty in predictions under distribution shift.",
    "reason": "The span abruptly moves from adversarial attacks to explanation methods to spectral normalization without clarifying their connections or providing transitions, making the relationships between cited works unclear.",
    "start": 515,
    "end": 733,
    "label": "Coherence"
  },
  {
    "span": "(Klein et al., 2016",
    "document": "Introduction\n\nNeural machine translation (NMT) benefits from subword segmentation, attention, and large-scale pretraining (Sennrich et al., 2016; Bahdanau et al., 2015; Conneau and Lample, 2019). Decoding strategies such as beam search and nucleus sampling trade off fluency and diversity (Sutskever et al., 2014; Holtzman et al., 2020). Efficient training leverages mixed precision and gradient accumulation to fit larger batches (Micikevicius et al., 2018; You et al., 2020).\n\nTooling ecosystems have accelerated research progress by standardizing datasets, tokenizers, and evaluation (Ott et al., 2019; Kudo and Richardson, 2018). Fairseq popularized modular experimentation for sequence modeling (Ott et al., 2019), while OpenNMT focused on production readiness (Klein et al., 2016 across diverse environments). Our work integrates constrained decoding into a state-of-the-art NMT stack with minimal overhead.\n\nWe evaluate on WMT benchmarks and show consistent improvements under terminology constraints without sacrificing BLEU or COMET.",
    "reason": "Missing closing parenthesis in a parenthetical citation; the citation is not properly bracketed.",
    "start": 766,
    "end": 785,
    "label": "Format"
  },
  {
    "span": "CLIP aligns images and texts with a contrastive objective (Radford et al., 2021). Object detectors such as Faster R-CNN pretrain region encoders (Ren et al., 2015). Masked language modeling improves text encoders (Devlin et al., 2019).",
    "document": "Related Work\n\nMultimodal Pretraining. Jointly learning from images and text has enabled zero-shot transfer and improved downstream performance across retrieval, captioning, and VQA.\n\nAlignment and Representation Learning. Contrastive and generative objectives both attempt to align modalities while preserving fine-grained semantics and compositionality.\n\nTechniques. CLIP aligns images and texts with a contrastive objective (Radford et al., 2021). Object detectors such as Faster R-CNN pretrain region encoders (Ren et al., 2015). Masked language modeling improves text encoders (Devlin et al., 2019).\n\nCompositional and Fine-Grained Reasoning. Recent work explores region–phrase alignment, cross-attention fusion, and grounding to improve localization and compositional generalization.",
    "reason": "The span abruptly shifts from contrastive vision–language alignment to object detection pretraining to masked language modeling without stating how these components connect, causing a coherence gap.",
    "start": 368,
    "end": 603,
    "label": "Coherence"
  },
  {
    "span": "Smith et al., 2020.)",
    "document": "Related Work\n\nPretrained word and sentence representations underpin many transfer learning pipelines (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019). Domain adaptation techniques align feature spaces and mitigate distribution shift via adversarial learning, self-training, and robust optimization (Ganin et al., 2016; Shu et al., 2018; Ben-David et al., 2020). Recent studies highlight the role of intermediate-task fine-tuning for stable transfer across tasks (Phang et al., 2018) and domains (Smith et al., 2020.)\n",
    "reason": "Extraneous period inside the parenthetical citation before the sentence-ending period. It should be formatted as \"(Smith et al., 2020).\"",
    "start": 537,
    "end": 557,
    "label": "Format"
  },
  {
    "span": "Matching Networks (Vinyals et al., 2016) used attention and memory. Prototypical Networks (Snell et al., 2017) learned class prototypes in embedding space. MAML (Finn et al., 2017) optimized for fast adaptation. Chen et al. (2019) revisited simple baselines.",
    "document": "Related Work\n\nFew-Shot Image Recognition\n\nFew-shot learning aims to generalize to novel classes with limited labeled examples by leveraging inductive biases and prior knowledge. Methods vary from metric learning and meta-learning to data augmentation and self-supervision. Despite substantial progress, performance remains sensitive to domain shift and choice of embedding.\n\nMatching Networks (Vinyals et al., 2016) used attention and memory. Prototypical Networks (Snell et al., 2017) learned class prototypes in embedding space. MAML (Finn et al., 2017) optimized for fast adaptation. Chen et al. (2019) revisited simple baselines.\n\nTransductive inference and task-adaptive pretraining further improve robustness by tailoring representations to the episode structure. We extend this line by aligning support–query distributions via feature calibration while preserving intra-class compactness.",
    "reason": "The span enumerates several seminal methods but does not articulate how they connect (e.g., differences between metric-based and optimization-based meta-learning) or transition from one to another. The relationship is only implied, producing coherence issues across multiple sentences (criteria a and b).",
    "start": 375,
    "end": 633,
    "label": "Coherence"
  },
  {
    "span": "Ren et al. (2015) presented Faster R-CNN with region proposals. Redmon et al. (2016) unified detection in a single-stage YOLO pipeline. Lin et al. (2017) developed Feature Pyramid Networks. Carion et al. (2020) used transformers in DETR.",
    "document": "Related Work\n\nModern object detection has evolved from region-based pipelines to end-to-end architectures. Progress is often driven by improvements in backbone representations, multiscale reasoning, and training objectives.\n\nRen et al. (2015) presented Faster R-CNN with region proposals. Redmon et al. (2016) unified detection in a single-stage YOLO pipeline. Lin et al. (2017) developed Feature Pyramid Networks. Carion et al. (2020) used transformers in DETR.\n\nOur approach revisits proposal generation under limited annotations, orthogonal to the model families summarized above.",
    "reason": "The span lists detection approaches back-to-back without transitions or explanations that clarify how they build on or contrast with one another, undermining coherence.",
    "start": 225,
    "end": 462,
    "label": "Coherence"
  },
  {
    "span": "However, both methods require the pretraining data to be fully annotated for all TOD sub-tasks (i.e., DST, POL, and NLG) which greatly limits the amount of data they can use.",
    "document": "Related Work\n\nTask-Oriented Dialogue. Task-oriented dialogue aims at accomplishing user's goal. Traditional systems (Williams and Young, 2007;Young et al., 2013) adopt a pipelined approach that requires dialogue state tracking for understanding user's goal, dialogue policy learning for deciding which system action to take, and natural language generation for generating dialogue responses.\n\nRecently, to simplify the modelling effort, researchers have shifted their attention to building neural network models that address the TOD subtasks Eric et al., 2017;Lei et al., 2018;Liang et al., 2020). With the advances in pretrained language models (PLMs), Budzianowski and Vulić (2019) first applied the GPT-2 model for the NLG task. Lin et al. (2020) and  moved one step forward and utilized pretrained language models to solve all TOD sub-tasks conditioned on the history of oracle belief states. Based on the GPT-2 model, Hosseini-Asl et al. (2020) proposed a cascaded model, SimpleTOD, that addresses all TOD sub-tasks without using the oracle information. To improve the system performance, Peng et al. (2021) and Liu et al. (2021) applied dialogue pre-training over external dialogue corpora. However, both methods require the pretraining data to be fully annotated for all TOD sub-tasks (i.e., DST, POL, and NLG) which greatly limits the amount of data they can use. Additionally, Liu et al. (2021) achieved better results with noisy chanel model that requires two additional language models for outputs re-scoring. Unlike their approach, we address the task of task-oriented dialogue with a single unified model.\n\nLanguage Model Pre-training. The research community has witnessed remarkable progress of pre-training methods in a wide range of NLP tasks, including language understanding (Peters et al., 2018;Devlin et al., 2019;Yang et al., 2019) and text generation (Radford et al., 2019;Lewis et al., 2020;Raffel et al., 2020).\n\nIn the dialogue domain, many models are pretrained on open-domain conversational data like Reddit. Based on GPT-2, Transfertransfo (Wolf et al., 2019b) achieves good results on ConvAI-2 competition. As another extension of GPT-2, Di-aloGPT (Zhang et al., 2020c) performs well in generating open-domain dialogue response. ConveRT ) is a language model with dual-encoder built for the task of response selection. PLATO (Bao et al., 2020) pre-trains a model with discrete latent variable structure for the response generation task.  adapts BERT with TOD pre-training and achieves strong performances on four dialogue understanding tasks.\n\nPre-training on Supplementary Data. Recent work (Phang et al., 2018;Aghajanyan et al., 2021) found that supplementary training on the tasks with intermediate-labelled data improves the performance of the fine-tuned models on GLUE natural language understanding benchmark (Wang et al., 2018). Our work studies a similar supplementary training setup with intermediate-labelled data for task-oriented dialogue systems. Unlike previous work, we use a single multi-task model for all relevant sub-tasks in task-oriented dialogue systems.\n\n ",
    "start": 1197,
    "end": 1371,
    "label": "Unsupported_claim"
  },
  {
    "span": "Transformer-based architectures have recently been adapted for long-horizon time series forecasting (Li et al., 2019; Zhou et al., 2021; Wu et al., 2021; Zeng et al., 2023).",
    "document": "Introduction\n\nLong-Horizon Time Series Forecasting. Accurate multi-step forecasting under long contexts is crucial in energy, finance, and supply chains. Transformer-based architectures have recently been adapted for long-horizon time series forecasting (Li et al., 2019; Zhou et al., 2021; Wu et al., 2021; Zeng et al., 2023). Architectural variations target efficiency via sparse attention, seasonal-trend decomposition, and frequency-domain operations. Concurrently, probabilistic objectives and quantile losses aim to capture uncertainty in volatile regimes (Salinas et al., 2020; Rangapuram et al., 2018).\n\nChallenges. Real-world series often exhibit nonstationarity, covariate shift, and calendar effects that complicate stable training and evaluation.\n\nWe propose a forecasting approach that combines frequency-aware embeddings with hierarchical decoders.",
    "reason": "The span catalogs prior transformer approaches but does not articulate how they fall short for the identified challenges or how the new approach addresses a specific gap (definition b and c).",
    "start": 154,
    "end": 327,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Park and Zhao 1",
    "document": "Introduction\n\nStatic analysis for security has advanced with abstract interpretation and model checking to detect information leaks and privilege escalations in mobile apps (Barthe et al., 2012; Ernst et al., 2014). Demand-driven analyses reduce overhead by focusing on slices relevant to a query (Sridharan and Fink, 2009). A concurrent line of work Park and Zhao 1 explores hybrid techniques that combine lightweight taint tracking with summarization of library code to improve precision on large codebases. Our method complements these approaches by learning probabilistic summaries from examples and using them to guide symbolic execution, yielding improved scalability without sacrificing soundness guarantees.\n",
    "reason": "Wrong use of a footnote indicator appended to an author citation; it should include a publication year (e.g., “Park and Zhao (2019)”) or be formatted as a proper footnote with corresponding markup.",
    "start": 351,
    "end": 366,
    "label": "Format"
  },
  {
    "span": "Prior temporal KG benchmarks predominantly use synthetic or discretized timestamps that do not reflect real-world reporting latencies.",
    "document": "Related Work\n\nTemporal knowledge graph (TKG) reasoning extends static link prediction by incorporating time-aware embeddings and relational dynamics (Leblay and Chekol, 2018; Goel et al., 2020; Xu et al., 2020). Benchmarks such as ICEWS and GDELT have catalyzed research into event forecasting and temporal inference with large-scale time-stamped triples (Boschee et al., 2015; Leetaru and Schrodt, 2013). Models differ in how they encode time, from sinusoidal features to learned temporal points and intervals.\n\nPrior temporal KG benchmarks predominantly use synthetic or discretized timestamps that do not reflect real-world reporting latencies. This mismatch can bias evaluations toward simplified temporal patterns and obscure delays inherent to news and intelligence feeds. We introduce a benchmark with raw ingestion times and document-level provenance to study latency-robust reasoning.",
    "reason": "This claim characterizes existing benchmarks without citing supporting analyses or surveys; it requires references to substantiate the generalization.",
    "start": 513,
    "end": 647,
    "label": "Unsupported_claim"
  },
  {
    "span": "Baker et al., 2019)",
    "document": "Related Work\n\nTime series forecasting methods range from classical ARIMA models to deep sequence architectures (Box and Jenkins, 1976; Salinas et al., 2020). Global models that share parameters across series improve data efficiency (Wen et al., 2017; Lim et al., 2021). For intermittent demand, probabilistic approaches have proven effective, as discussed in Baker et al., 2019) and extended by hierarchical reconciliation techniques (Hyndman et al., 2011). Our method integrates calendar effects with graph-based dependencies across related series.\n\nWe benchmark on retail, traffic, and energy datasets with multiple forecast horizons (Makridakis et al., 2020).",
    "reason": "Missing opening parenthesis; should be formatted as “(Baker et al., 2019)”.",
    "start": 359,
    "end": 378,
    "label": "Format"
  },
  {
    "span": "Prompting strategies such as few-shot prompting, chain-of-thought, and self-consistency have been applied to code generation (Brown et al., 2020; Wei et al., 2022; Wang et al., 2023).",
    "document": "Related Work\n\nProgram synthesis with large language models. Recent foundation models demonstrate strong zero- and few-shot code generation abilities. Prompting strategies such as few-shot prompting, chain-of-thought, and self-consistency have been applied to code generation (Brown et al., 2020; Wei et al., 2022; Wang et al., 2023). In addition, execution-guided decoding, unit-test feedback, and repair loops have been proposed to improve functional correctness (Chen et al., 2021; Le et al., 2022; Austin et al., 2021).\n\nPositioning. We target tasks requiring multi-file, stateful reasoning with tool use, where static prompting is insufficient.\n\nOur approach. We present an iterative planner-verifier that couples symbolic environment traces with natural language plans, improving correctness and compositional generalization.",
    "reason": "The span merely lists prompting techniques and citations without relating them to the proposed planner-verifier or clarifying shortcomings, thus lacking synthesis.",
    "start": 150,
    "end": 333,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Liu et al., 2019)",
    "document": "Related Work\n\nDomain adaptation for sentiment analysis has progressed from feature augmentation and pivot-based selection (Blitzer et al., 2007) to adversarial representation learning (Ganin et al., 2016) and moment-matching approaches (Zellinger et al., 2017). With the advent of large-scale pretraining, contextual encoders have become the de facto backbone for cross-domain text classification (Howard and Ruder, 2018; Devlin et al., 2019; Peters et al., 2018).\n\nPretrained encoders such as BERT (Devlin et al., 2019) and RoBERTa Liu et al., 2019) underpin many recent advances in robust sentiment transfer, enabling models to separate domain-specific lexical variation from domain-invariant sentiment cues (Gururangan et al., 2020; Ben-David et al., 2010). On top of these backbones, adapters and lightweight finetuning techniques have further reduced the cost of deploying models across multiple domains (Houlsby et al., 2019; Pfeiffer et al., 2021). Despite these advances, identifying which layers capture transferable sentiment remains an open problem (Mosbach et al., 2021), motivating our analysis-oriented approach.",
    "reason": "Missing opening parenthesis for a parenthetical citation; should be RoBERTa (Liu et al., 2019).",
    "start": 533,
    "end": 550,
    "label": "Format"
  },
  {
    "span": "Temporal Fusion Transformers incorporate static covariates and gating (Lim et al., 2021). N-BEATS uses backward and forward residual stacks for univariate forecasting (Oreshkin et al., 2020). Informer employs ProbSparse attention for long sequences (Zhou et al., 2021). ARIMA models seasonal patterns with differencing (Box and Jenkins, 1970).",
    "document": "Related Work\n\nNeural forecasting combines sequence modeling with inductive biases for temporal dynamics, handling covariates, seasonality, and long horizons. Transformer variants and deep residual architectures have advanced accuracy on many benchmarks.\n\nTemporal Fusion Transformers incorporate static covariates and gating (Lim et al., 2021). N-BEATS uses backward and forward residual stacks for univariate forecasting (Oreshkin et al., 2020). Informer employs ProbSparse attention for long sequences (Zhou et al., 2021). ARIMA models seasonal patterns with differencing (Box and Jenkins, 1970).\n\nRecent work studies hierarchical reconciliation (Wickramasuriya et al., 2019) and probabilistic forecasting with quantile and distributional losses (Salinas et al., 2020). Our method introduces a multi-resolution attention mechanism with calendar-aware priors to improve long-horizon calibration.",
    "reason": "The span abruptly moves among different architectures and even a classical ARIMA baseline without transitions or explicit links, making their relationships and relevance to each other unclear.",
    "start": 255,
    "end": 598,
    "label": "Coherence"
  },
  {
    "span": "Prior work on multilingual neural machine translation has explored shared subword vocabularies, language-specific adapters, and universal encoders. Techniques for low-resource settings include back-translation, pivot translation, iterative self-training, and transfer from high-resource auxiliaries. We adopt back-translation and multilingual pretraining to improve baseline performance.",
    "document": "Introduction\n\nMultilingual neural machine translation (MNMT) enables parameter sharing across languages and reduces deployment costs by maintaining a single model for many directions. Leveraging cross-lingual transfer can be especially beneficial for low-resource languages.\n\nPrior work on multilingual neural machine translation has explored shared subword vocabularies, language-specific adapters, and universal encoders. Techniques for low-resource settings include back-translation, pivot translation, iterative self-training, and transfer from high-resource auxiliaries. We adopt back-translation and multilingual pretraining to improve baseline performance.\n\nOur experiments focus on several underrepresented language pairs with limited parallel data and diverse morphology.",
    "reason": "The paragraph lists prior approaches and states the authors' use of similar techniques without explaining why existing methods are insufficient or what specific gap their approach addresses.",
    "start": 276,
    "end": 663,
    "label": "Lacks_synthesis"
  },
  {
    "span": "To the best of our knowledge, no prior study has systematically evaluated bias in morphological inflection datasets across languages.",
    "document": "Introduction\n\nMorphological inflection tasks have become a standard benchmark for evaluating character-level sequence modeling and low-resource learning in NLP. Shared tasks have catalyzed progress by curating multilingual datasets and standardized evaluation scripts (e.g., Cotterell et al., 2016; McCarthy et al., 2019; Vylomova et al., 2020). Recent neural architectures leverage inductive biases such as copy mechanisms and alignment constraints to improve generalization in extremely low-resource settings (Kann and Schütze, 2017; Peters and Nicolai, 2018; Makarov and Clematide, 2018).\n\nDespite these advances, concerns remain about dataset imbalance across typological families and the potential for systematic performance disparities tied to script, morphological complexity, or annotation conventions. To the best of our knowledge, no prior study has systematically evaluated bias in morphological inflection datasets across languages. We address this gap by proposing a controlled evaluation protocol and a suite of bias-sensitive metrics that disentangle morphological complexity from resource availability.\n\nOur contributions are threefold: (1) a typology-aware sampling strategy for constructing balanced evaluation splits, (2) a metric family that isolates affixal productivity effects, and (3) a multilingual audit of widely used inflection benchmarks.",
    "reason": "Claims the absence of prior systematic evaluations without citing literature or providing evidence (rule a/b).",
    "start": 811,
    "end": 944,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith et al., (2019)",
    "document": "Introduction\n\nProgram synthesis from natural language aims to map textual specifications to executable code (Yin and Neubig, 2017; Rabinovich et al., 2017). Smith et al., (2019) propose a sequence-to-tree decoder that enforces grammar constraints, improving syntactic validity. Follow-up work integrates retrieval to ground generation in similar code snippets (Hashimoto et al., 2018; Parvez et al., 2021).\n\nWe instead introduce a verifier-guided decoding scheme that jointly reasons over execution traces and natural language constraints.\n",
    "reason": "Extraneous comma before the year in a narrative citation; should be 'Smith et al. (2019)'.",
    "start": 157,
    "end": 177,
    "label": "Format"
  },
  {
    "span": "Dosovitskiy et al. (2020) introduced Vision Transformers for image classification. U-Net remains a strong baseline for medical image segmentation (Ronneberger et al., 2015). Self-supervised pretraining improves performance on small labeled datasets (He et al., 2020). Annotation costs are high in radiology (Irvin et al., 2019).",
    "document": "Introduction\n\nTransformers in Medical Imaging. Transformer-based architectures have begun to rival convolutional networks in vision tasks by modeling long-range dependencies with self-attention (Dosovitskiy et al., 2020; Liu et al., 2021). In medical imaging, limited labels and domain shift pose additional challenges that motivate self-supervised learning and domain adaptation (Azizi et al., 2021; Tajbakhsh et al., 2020).\n\nArchitectures and Data Constraints. Dosovitskiy et al. (2020) introduced Vision Transformers for image classification. U-Net remains a strong baseline for medical image segmentation (Ronneberger et al., 2015). Self-supervised pretraining improves performance on small labeled datasets (He et al., 2020). Annotation costs are high in radiology (Irvin et al., 2019). We propose a hybrid CNN-Transformer with masked image modeling tailored to volumetric scans, reducing annotation requirements via cross-study pseudo-labeling.",
    "reason": "The paragraph strings together ViTs, U-Net, self-supervision, and annotation cost without explaining their relationships or providing transitions, making the connection between architecture choices and data constraints implicit.",
    "start": 463,
    "end": 791,
    "label": "Coherence"
  },
  {
    "span": "In a previous MOOC dropout study, the authors showed that forum activity explains 80% of the variance.",
    "document": "Introduction\nPredicting student dropout in massive open online courses (MOOCs) supports targeted interventions and personalized learning pathways. Prior work has explored behavioral signals such as video interactions, assignment submissions, and forum engagement.\n\nRelated Work\nFeature engineering for dropout prediction often centers on weekly aggregates and temporal trends. In a previous MOOC dropout study, the authors showed that forum activity explains 80% of the variance. While discussion participation is a compelling indicator of engagement, such a large effect size could reflect dataset bias, course design, or label leakage. We therefore re-assess the contribution of forum features under rigorous temporal splitting and across multiple platforms.",
    "reason": "This mentions a specific 'previous study' and a numerical result without citing the source (violates a and b).",
    "start": 377,
    "end": 479,
    "label": "Unsupported_claim"
  },
  {
    "span": "The BraTS 2021 dataset provides over 10,000 annotated volumes.",
    "document": "Related Work\n\nBrain tumor segmentation has been dominated by encoder–decoder architectures such as U-Net and its 3D variants (Ronneberger et al., 2015; Çiçek et al., 2016; Milletari et al., 2016). Attention mechanisms and cascaded decoders further refine boundaries and reduce false positives (Oktay et al., 2018; Isensee et al., 2021). Self-supervised pretraining on large unlabeled MRI corpora has also improved data efficiency (Zhuang et al., 2019).\n\nBenchmarking commonly uses the BraTS challenges, which standardize modalities and evaluation metrics. The BraTS 2021 dataset provides over 10,000 annotated volumes. Many recent entries incorporate uncertainty estimation to better handle domain shift between centers.\n\nOur method builds on a 3D U-Net backbone with test-time augmentation to improve robustness to acquisition variability.",
    "reason": "Introduces a specific dataset and a quantitative claim at first mention without any citation (rule a and b).",
    "start": 556,
    "end": 618,
    "label": "Unsupported_claim"
  },
  {
    "span": "Chen (2020",
    "document": "Introduction\n\nSelf-supervised pretraining has transformed NLP by leveraging large unlabeled corpora through masked token recovery and related objectives (Devlin et al., 2019; Liu et al., 2019). Contrastive learning complements masked modeling by enforcing representation consistency across augmentations (Giorgi et al., 2021; Gao et al., 2021). Building on momentum encoders, Chen (2020 introduced a temperature-scaled contrastive objective with large negative banks. Subsequent work adapted these ideas to sentence and document encoders with efficient in-batch negatives (Karpukhin et al., 2020; Luan et al., 2021). We study augmentation choice and sampling bias in low-resource regimes, extending analyses in (Gunel et al., 2021).",
    "reason": "Missing closing parenthesis in narrative citation; should be Chen (2020) introduced ...",
    "start": 376,
    "end": 386,
    "label": "Format"
  },
  {
    "span": "(Zhang et al., 2020; Li et al., 2021;.",
    "document": "Related Work\n\nGraph neural networks (GNNs) extend deep learning to non-Euclidean domains, enabling message passing over nodes and edges (Kipf and Welling, 2017; Hamilton et al., 2017). Variants address over-smoothing and scalability via residual connections and sampling (Xu et al., 2019; Chen et al., 2018). Applications include molecular property prediction (Gilmer et al., 2017) and knowledge graph completion (Bordes et al., 2013). For graph pooling and structure learning, several surveys synthesize recent progress (Zhang et al., 2020; Li et al., 2021;. Our work complements these by evaluating uncertainty-aware GNNs in dynamic settings (Gal and Ghahramani, 2016; Sun et al., 2020).",
    "reason": "Dangling semicolon and period inside the parenthetical citation; the list ends with ';.' which is incorrect. It should end cleanly with ')', e.g., '(Zhang et al., 2020; Li et al., 2021)'.",
    "start": 521,
    "end": 559,
    "label": "Format"
  },
  {
    "span": "Smith et al., 2019)",
    "document": "Related Work\n\nGraph-based recommendation systems leverage user–item interaction graphs to capture higher-order connectivity (Ying et al., 2018; Wang et al., 2019). Message passing schemes propagate preferences along the graph to improve cold-start performance (He et al., 2020; Klicpera et al., 2019). A line of work integrates side information such as textual reviews and images (Zhang et al., 2017; Chen et al., 2019). Smith et al., 2019) introduced a sampling strategy to reduce over-smoothing in deep graph stacks, while other methods regularize embeddings via contrastive objectives (Wu et al., 2021; Sun et al., 2020). However, most approaches assume stationary interaction patterns and struggle under temporal drifts.",
    "reason": "Missing opening parenthesis: the parenthetical citation is incomplete. It should be '(Smith et al., 2019)' rather than 'Smith et al., 2019)'.",
    "start": 421,
    "end": 440,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on essays from the ASAP competition.",
    "document": "Introduction\n\nAutomated Essay Scoring (AES) aims to estimate writing quality with machine learning models, enabling scalable feedback and assessment. Earlier AES systems relied on handcrafted features capturing grammar, style, and discourse, whereas neural models now learn representations directly from text. BERT was used in an AES task trained on essays from the ASAP competition. Despite strong performance, the generalization of such models to new prompts and genres remains underexplored. We investigate cross-prompt robustness by evaluating transfer across multiple essay sets and prompt types.\n",
    "reason": "Mentions a specific model usage and a named competition/dataset (ASAP) without citing the corresponding study or dataset source at first mention.",
    "start": 310,
    "end": 383,
    "label": "Unsupported_claim"
  },
  {
    "span": "A widely used baseline in news summarization is the Lead-3 heuristic.",
    "document": "Introduction\n\nNeural abstractive summarization has benefited from encoder–decoder architectures with attention and pre-training objectives aligned with document-level compression (See et al., 2017; Liu and Lapata, 2019). Evaluation remains challenging, as automatic metrics correlate imperfectly with human judgments and can incentivize extractive copying (Fabbri et al., 2021). A widely used baseline in news summarization is the Lead-3 heuristic. While simple, its strong performance on inverted-pyramid news articles motivates models that exploit discourse cues without overfitting to positional bias.\n",
    "reason": "States a common baseline practice in a specific domain without providing a supporting citation.",
    "start": 379,
    "end": 448,
    "label": "Unsupported_claim"
  },
  {
    "span": "There is a consensus that group fairness metrics cannot be directly applied to high-stakes medical triage.",
    "document": "Introduction\n\nFairness in decision-support systems requires aligning statistical definitions with domain-specific ethical and operational constraints. There is a consensus that group fairness metrics cannot be directly applied to high-stakes medical triage. Instead, practitioners must consider resource constraints, triage protocols, and dynamic patient outcomes that complicate standard parity criteria.\n\nWe formalize triage-aware fairness objectives and present a simulation framework to study trade-offs under varying patient arrival patterns and capacity limits.",
    "reason": "Asserts field-wide consensus on a specialized topic without providing evidence or citations (rule b).",
    "start": 151,
    "end": 257,
    "label": "Unsupported_claim"
  },
  {
    "span": "According to (Nguyen, 2017)",
    "document": "Introduction\n\nDialogue state tracking estimates user goals over the course of a conversation and is central to task-oriented dialogue systems. Early pipeline systems relied on semantic frames and rule-based updates (Williams and Young, 2007), whereas recent neural approaches maintain distributions over slot values with recurrent or transformer encoders (Mrkšić et al., 2017;Henderson et al., 2014;Zhang et al., 2020).\n\nAccording to (Nguyen, 2017) robust tracking benefits from delexicalization and ontology-driven constraints, but scalability remains a challenge in open settings. We address this by using contrastive objectives that separate conflicting evidence while preserving uncertainty.",
    "reason": "Wrong citation style in a narrative context: the citation should be integrated as 'According to Nguyen (2017)' rather than a parenthetical immediately after 'According to'.",
    "start": 421,
    "end": 448,
    "label": "Format"
  },
  {
    "span": "Multimodal sentiment analysis has progressed from early feature concatenation baselines to sophisticated fusion schemes. Tensor fusion captures multiplicative interactions among modalities, graph-based models propagate cross-modal cues, and attention mechanisms align audio, visual, and textual streams. Pretrained vision–language models and audio-text encoders have further improved benchmarks through large-scale pretraining.",
    "document": "Introduction\nUnderstanding human sentiment requires integrating cues from language, prosody, and facial expressions. Robust multimodal models must reconcile asynchronous, noisy, and partially missing signals.\n\nRelated Work\nInitial multimodal sentiment systems combined hand-crafted features, while deep learning enabled end-to-end fusion.\nMultimodal sentiment analysis has progressed from early feature concatenation baselines to sophisticated fusion schemes. Tensor fusion captures multiplicative interactions among modalities, graph-based models propagate cross-modal cues, and attention mechanisms align audio, visual, and textual streams. Pretrained vision–language models and audio-text encoders have further improved benchmarks through large-scale pretraining.\nMissing-modality handling includes modality dropout and generative imputation, and temporal modeling leverages sequence encoders for long-context reasoning.\n\nOur Focus\nWe study robustness under cross-dataset shift with a lightweight fusion module designed to be resilient to missing modalities.",
    "reason": "The span lists prior fusion techniques and pretraining advances without relating them to the paper's robustness objective or identifying shortcomings, meeting (a) and (c).",
    "start": 339,
    "end": 766,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There has been a surge of attacks exploiting prompt injection in LLM agents",
    "document": "Related Work\n\nLarge language model (LLM) agents are increasingly deployed to operate tools, browse the web, and execute multi-step plans. This expanded capability surface introduces new security risks beyond traditional model misuse.\n\nThere has been a surge of attacks exploiting prompt injection in LLM agents. Defenses span input sanitization, policy training, and tool access mediation, yet empirical evaluations are fragmented and often limited to narrow threat models.\n\nOur work consolidates a unified threat taxonomy and presents a standardized red-teaming protocol that measures attack success and collateral task degradation across popular agent frameworks.",
    "reason": "Uses 'surge of attacks' (recent works) without citing any papers, reports, or datasets documenting such attacks (rule d and a).",
    "start": 235,
    "end": 310,
    "label": "Unsupported_claim"
  },
  {
    "span": "Hyndman and Athanasopoulos (2018) discuss classical statistical forecasting methods such as ARIMA and ETS. Hochreiter and Schmidhuber (1997) introduced LSTMs for sequence modeling. Taylor and Letham (2018) presented Prophet for decomposable time series. Zerveas et al. (2021) used transformers for multivariate forecasting.",
    "document": "Related Work\n\nTime series forecasting encompasses classical statistical techniques, neural sequence models, and hybrid approaches aimed at capturing trend, seasonality, and exogenous drivers. Challenges include long-horizon accuracy, irregular sampling, and domain adaptation.\n\nHyndman and Athanasopoulos (2018) discuss classical statistical forecasting methods such as ARIMA and ETS. Hochreiter and Schmidhuber (1997) introduced LSTMs for sequence modeling. Taylor and Letham (2018) presented Prophet for decomposable time series. Zerveas et al. (2021) used transformers for multivariate forecasting.\n\nRecent advances integrate probabilistic calibration and uncertainty quantification with scalable architectures (Salinas et al., 2020; Rangapuram et al., 2018). Our method targets long-context representation with decomposed attention and seasonality priors, complementing prior neural and statistical models.",
    "reason": "The span lists disparate approaches without connecting them or explaining their relation to the earlier challenges. There are no transitions clarifying progression from classical to neural to transformer-based methods, undermining coherence.",
    "start": 278,
    "end": 601,
    "label": "Coherence"
  },
  {
    "span": "Representation learning for counterfactual inference has considered balancing covariates (Johansson et al., 2016), adversarial alignment (Shalit et al., 2017), and invariant risk minimization (Arjovsky et al., 2020).",
    "document": "Introduction\n\nEstimating individual treatment effects from observational data requires controlling for confounding while avoiding extrapolation outside the support of observed covariates. Representation learning has emerged as a promising approach to reduce selection bias and enable reliable counterfactual estimation.\n\nRepresentation learning for counterfactual inference has considered balancing covariates (Johansson et al., 2016), adversarial alignment (Shalit et al., 2017), and invariant risk minimization (Arjovsky et al., 2020).\n\nWe propose a transport-regularized encoder that aligns treated and control distributions through an optimal transport penalty and a local smoothness prior tied to outcome curvature. The method yields calibrated uplift estimates with explicit control over support overlap.\n\nWe evaluate on semi-synthetic and real-world datasets, reporting PEHE, ATE error, and policy value metrics, and analyze robustness under hidden confounding severity.",
    "reason": "The span recites prior approaches without stating how they fall short or how the current method responds, fulfilling (a) and (c).",
    "start": 321,
    "end": 537,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Two-stage detectors like Faster R-CNN and its variants leverage region proposals and feature pyramids, while one-stage detectors such as YOLO and RetinaNet prioritize speed with dense predictions (Ren et al., 2015; Lin et al., 2017; Redmon et al., 2016; Bochkovskiy et al., 2020). Extensions address small-object detection, anchor-free designs, and transformer backbones (Tian et al., 2019; Carion et al., 2020; Zhu et al., 2020; Wang et al., 2021).",
    "document": "Related Work\n\nObject detection research spans two-stage and one-stage paradigms, with a longstanding trade-off between accuracy and efficiency. Recent efforts also target robustness under distribution shift (weather, sensor noise) and label scarcity. Our study evaluates detection under extreme bandwidth constraints where feature compression and adaptive computation are central.\n\nTwo-stage detectors like Faster R-CNN and its variants leverage region proposals and feature pyramids, while one-stage detectors such as YOLO and RetinaNet prioritize speed with dense predictions (Ren et al., 2015; Lin et al., 2017; Redmon et al., 2016; Bochkovskiy et al., 2020). Extensions address small-object detection, anchor-free designs, and transformer backbones (Tian et al., 2019; Carion et al., 2020; Zhu et al., 2020; Wang et al., 2021).\n\nEdge-centric detection has explored feature caching, early exits, and dynamic networks to reduce computation. We build on dynamic backbones but introduce bitrate-aware adapters that jointly optimize feature fidelity and detection quality for streaming scenarios.",
    "reason": "The span only catalogs prior detectors and extensions without explaining their relation to the bandwidth-constrained setting or clarifying what is missing that the current work addresses (criteria a and b).",
    "start": 382,
    "end": 831,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Brown et al., (2021)",
    "document": "Related Work\n\nMultimodal fusion techniques combine language and vision signals for improved grounding and reasoning. Early methods concatenated modality-specific representations followed by shallow fusion (Kiros et al., 2015). Attention-based fusion aligns image regions with words or phrases for fine-grained grounding (Anderson et al., 2018). Brown et al., (2021) propose a gated bilinear pooling mechanism to improve robustness under occlusions. Alternative strategies learn cross-modal transformers with shared positional encodings (Li et al., 2020). For sample efficiency, contrastive pretraining on large image–text pairs has proven effective (Radford et al., 2021). We depart from prior work by introducing structure-preserving adapters that retain unimodal calibration while enhancing cross-modal interactions (Su and Deng, 2022).",
    "reason": "Extraneous comma before the parenthetical year in a narrative citation; should be 'Brown et al. (2021)'.",
    "start": 345,
    "end": 365,
    "label": "Format"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nVisual question answering (VQA) benchmarks probe multimodal reasoning by requiring models to answer natural language questions about images (Antol et al., 2015; Goyal et al., 2017). Transformer-based architectures that fuse vision and language have led to strong performance (Lu et al., 2019; Tan and Bansal, 2019; Chen et al., 2020).\n\nWhile large-scale pretraining is effective, models often exploit dataset biases rather than perform grounded reasoning (Agrawal et al., 2018; Cadene et al., 2019). To address this, several works propose counterfactual data augmentation and adversarial evaluation (Gokhale et al., 2020; Selvaraju et al., 2020). However, we observe limited gains in compositional generalization [12], motivating a diagnostic benchmark that targets specific reasoning skills (Hudson and Manning, 2019; Andreas et al., 2016).\n\nWe present a suite of structured probes for attribute binding, spatial relations, and numerical reasoning, accompanied by analyses of shortcut reliance under distribution shifts (Barrett et al., 2018; Subramanian et al., 2019).",
    "reason": "Wrong citation style: numeric bracket '[12]' appears in an author–year citation context; it should be replaced with an author–year citation.",
    "start": 727,
    "end": 731,
    "label": "Format"
  },
  {
    "span": "Liu et al., 2021)",
    "document": "Related Work\n\nNeural machine translation (NMT) has advanced rapidly with encoder–decoder architectures and attention mechanisms (Bahdanau et al., 2015; Vaswani et al., 2017). Transfer learning and multilingual pretraining have further improved low-resource performance (Johnson et al., 2017; Conneau and Lample, 2019). Recent work Liu et al., 2021) explores cross-modal pretraining for multimodal translation, while other studies investigate document-level context (Maruf and Haffari, 2018; Voita et al., 2019). We focus on robust decoding under domain shift, extending calibration techniques from Guo et al. (2017) to sequence generation.",
    "reason": "Parenthetical citation missing opening parenthesis; should be '(Liu et al., 2021)'.",
    "start": 331,
    "end": 348,
    "label": "Format"
  },
  {
    "span": "the VQA v2 dataset reduces language priors but still allows shortcut learning",
    "document": "Related Work\n\nVisual Question Answering (VQA) evaluates multimodal reasoning by requiring models to answer natural language questions about images. Early VQA benchmarks exhibited strong language priors, enabling models to guess answers without grounding in the image. To mitigate this, the VQA v2 dataset reduces language priors but still allows shortcut learning through dataset artifacts and annotation patterns.\n\nRecent research explores improved negative sampling, counterfactual data augmentation, and faithfulness metrics to encourage visual grounding. Nevertheless, spurious correlations persist, and evaluation protocols often fail to disentangle reasoning from recognition. Our work introduces a diagnostic suite that measures dependence on visual evidence versus textual heuristics.",
    "reason": "Claims specific properties of the VQA v2 dataset without citing sources that demonstrate reduced priors and remaining shortcuts.",
    "start": 286,
    "end": 363,
    "label": "Unsupported_claim"
  },
  {
    "span": "OntoNotes V4.0",
    "document": "Introduction\n\nNamed entity recognition (NER) aims at identifying text spans pertaining to specific entity types. It plays an important role in many downstream tasks such as relation extraction (Ji et al., 2017), entity linking (Sevgili et al., 2020), co-reference resolution (Clark and Manning, 2016), and knowledge graph (Ji et al., 2020). Due to the complex composition (Gui et al., 2019), character-level Chinese NER is more challenging compared to English NER. As shown in Figure 1 (a), the middle charac-  ter \"流\" can constitute words with the characters to both their left and their right, such as \"河流 (River)\" and \"流经 (flows)\", leading to ambiguous character boundaries.\n\nThere are two typical frameworks for NER. The first one conceptualizes NER as a sequence labeling task (Huang et al., 2015;Lample et al., 2016;Ma and Hovy, 2016), where each character is assigned to a special label (e.g., B-LOC, I-LOC). The second one is span-based method (Li et al., 2020a;, which classifies candidate spans based on their span-level representations. However, despite the success of these two types of methods, they do not explicitly take the complex composition of Chinese NER into consideration. Recently, several works (Zhang and Yang, 2018;Gui et al., 2019;Li et al., 2020b) utilize external lexicon knowledge to help connect related characters and promote capturing the local composition. Nevertheless, building the lexicon is time-consuming and the quality of the lexicon may not be satisfied.\n\nIn contrast to previous works, we observe that the regularity exists in the common NER types (e.g., ORG and LOC). As shown in Figure 1 (a), \"尼日尔河 (Niger River)\" follows the specific composition pattern \"XX+河 (XX + River)\" which ends with indicator character \"河\" and mostly belongs to location type, and the ambiguous character \"流\" can properly constitute \"流经\" with the right character \"经\". Thus, the regularity information serves as important clues for entity type recognition and identifying the character composition. Formally, we refer to regularity as specific internal patterns contained in a type of entity (Lin et al., 2020). However, too immersed regularity leads to unfavorable boundary detection of entities and disturbing character composition. As shown in Figure 1 (b), \"中国队 (Chinese team)\" conforms to the pattern \"XX+队 (XX + Team)\", but the correct entity boundary should be \"中国 (Chinese)\" and \"队员 (players)\" according to the context. Therefore, the context also plays a key role in determining the character boundary.\n\nIn this paper, we introduce a simple but effective method to explore the regularity information of entity spans for Chinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON). The proposed model consists of two branches named regularity-aware module and regularity-agnostic module, where each module has task-specific encoder and optimization object. Concretely, the regularity-aware module aims at analyzing the internal regularity of each span and integrates the significant regularity information into the corresponding span-level representation, leading to precise entity type prediction. Meanwhile, the regularityagnostic module is devised to capture context information and avoid excessive focus on intra-span regularity. Furthermore, we adopt an orthogonality space restriction to encourage two branches to extract different features with regard to the regularity. To verify the effectiveness of our method, we conduct extensive experiments on three large-scale benchmark datasets (OntoNotes V4.0, OntoNotes V5.0, and MSRA). The results show that RICON achieves considerable improvements compared to the state-of-the-art models, even outperforming existing lexicon-based models. Moreover, we experiment on a practical medical dataset (CBLUE) to further demonstrate the ability of RICON.\n\nOur contributions can be summarized as follows:\n\n• This is the first work that explicitly explores the internal regularity of entity mentions for Chinese NER.\n\n• We propose a simple but effective method for Chinese NER, which effectively utilizes regularity information while avoiding excessive focus on intra-span regularity.\n\n• Extensive experiments on three large-scale benchmark datasets and a practical medical dataset demonstrate the effectiveness of our proposed method.\n\n ",
    "start": 3533,
    "end": 3547,
    "label": "Unsupported_claim"
  },
  {
    "span": "OntoNotes V5.0",
    "document": "Introduction\n\nNamed entity recognition (NER) aims at identifying text spans pertaining to specific entity types. It plays an important role in many downstream tasks such as relation extraction (Ji et al., 2017), entity linking (Sevgili et al., 2020), co-reference resolution (Clark and Manning, 2016), and knowledge graph (Ji et al., 2020). Due to the complex composition (Gui et al., 2019), character-level Chinese NER is more challenging compared to English NER. As shown in Figure 1 (a), the middle charac-  ter \"流\" can constitute words with the characters to both their left and their right, such as \"河流 (River)\" and \"流经 (flows)\", leading to ambiguous character boundaries.\n\nThere are two typical frameworks for NER. The first one conceptualizes NER as a sequence labeling task (Huang et al., 2015;Lample et al., 2016;Ma and Hovy, 2016), where each character is assigned to a special label (e.g., B-LOC, I-LOC). The second one is span-based method (Li et al., 2020a;, which classifies candidate spans based on their span-level representations. However, despite the success of these two types of methods, they do not explicitly take the complex composition of Chinese NER into consideration. Recently, several works (Zhang and Yang, 2018;Gui et al., 2019;Li et al., 2020b) utilize external lexicon knowledge to help connect related characters and promote capturing the local composition. Nevertheless, building the lexicon is time-consuming and the quality of the lexicon may not be satisfied.\n\nIn contrast to previous works, we observe that the regularity exists in the common NER types (e.g., ORG and LOC). As shown in Figure 1 (a), \"尼日尔河 (Niger River)\" follows the specific composition pattern \"XX+河 (XX + River)\" which ends with indicator character \"河\" and mostly belongs to location type, and the ambiguous character \"流\" can properly constitute \"流经\" with the right character \"经\". Thus, the regularity information serves as important clues for entity type recognition and identifying the character composition. Formally, we refer to regularity as specific internal patterns contained in a type of entity (Lin et al., 2020). However, too immersed regularity leads to unfavorable boundary detection of entities and disturbing character composition. As shown in Figure 1 (b), \"中国队 (Chinese team)\" conforms to the pattern \"XX+队 (XX + Team)\", but the correct entity boundary should be \"中国 (Chinese)\" and \"队员 (players)\" according to the context. Therefore, the context also plays a key role in determining the character boundary.\n\nIn this paper, we introduce a simple but effective method to explore the regularity information of entity spans for Chinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON). The proposed model consists of two branches named regularity-aware module and regularity-agnostic module, where each module has task-specific encoder and optimization object. Concretely, the regularity-aware module aims at analyzing the internal regularity of each span and integrates the significant regularity information into the corresponding span-level representation, leading to precise entity type prediction. Meanwhile, the regularityagnostic module is devised to capture context information and avoid excessive focus on intra-span regularity. Furthermore, we adopt an orthogonality space restriction to encourage two branches to extract different features with regard to the regularity. To verify the effectiveness of our method, we conduct extensive experiments on three large-scale benchmark datasets (OntoNotes V4.0, OntoNotes V5.0, and MSRA). The results show that RICON achieves considerable improvements compared to the state-of-the-art models, even outperforming existing lexicon-based models. Moreover, we experiment on a practical medical dataset (CBLUE) to further demonstrate the ability of RICON.\n\nOur contributions can be summarized as follows:\n\n• This is the first work that explicitly explores the internal regularity of entity mentions for Chinese NER.\n\n• We propose a simple but effective method for Chinese NER, which effectively utilizes regularity information while avoiding excessive focus on intra-span regularity.\n\n• Extensive experiments on three large-scale benchmark datasets and a practical medical dataset demonstrate the effectiveness of our proposed method.\n\n ",
    "start": 3549,
    "end": 3563,
    "label": "Unsupported_claim"
  },
  {
    "span": "MSRA",
    "document": "Introduction\n\nNamed entity recognition (NER) aims at identifying text spans pertaining to specific entity types. It plays an important role in many downstream tasks such as relation extraction (Ji et al., 2017), entity linking (Sevgili et al., 2020), co-reference resolution (Clark and Manning, 2016), and knowledge graph (Ji et al., 2020). Due to the complex composition (Gui et al., 2019), character-level Chinese NER is more challenging compared to English NER. As shown in Figure 1 (a), the middle charac-  ter \"流\" can constitute words with the characters to both their left and their right, such as \"河流 (River)\" and \"流经 (flows)\", leading to ambiguous character boundaries.\n\nThere are two typical frameworks for NER. The first one conceptualizes NER as a sequence labeling task (Huang et al., 2015;Lample et al., 2016;Ma and Hovy, 2016), where each character is assigned to a special label (e.g., B-LOC, I-LOC). The second one is span-based method (Li et al., 2020a;, which classifies candidate spans based on their span-level representations. However, despite the success of these two types of methods, they do not explicitly take the complex composition of Chinese NER into consideration. Recently, several works (Zhang and Yang, 2018;Gui et al., 2019;Li et al., 2020b) utilize external lexicon knowledge to help connect related characters and promote capturing the local composition. Nevertheless, building the lexicon is time-consuming and the quality of the lexicon may not be satisfied.\n\nIn contrast to previous works, we observe that the regularity exists in the common NER types (e.g., ORG and LOC). As shown in Figure 1 (a), \"尼日尔河 (Niger River)\" follows the specific composition pattern \"XX+河 (XX + River)\" which ends with indicator character \"河\" and mostly belongs to location type, and the ambiguous character \"流\" can properly constitute \"流经\" with the right character \"经\". Thus, the regularity information serves as important clues for entity type recognition and identifying the character composition. Formally, we refer to regularity as specific internal patterns contained in a type of entity (Lin et al., 2020). However, too immersed regularity leads to unfavorable boundary detection of entities and disturbing character composition. As shown in Figure 1 (b), \"中国队 (Chinese team)\" conforms to the pattern \"XX+队 (XX + Team)\", but the correct entity boundary should be \"中国 (Chinese)\" and \"队员 (players)\" according to the context. Therefore, the context also plays a key role in determining the character boundary.\n\nIn this paper, we introduce a simple but effective method to explore the regularity information of entity spans for Chinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON). The proposed model consists of two branches named regularity-aware module and regularity-agnostic module, where each module has task-specific encoder and optimization object. Concretely, the regularity-aware module aims at analyzing the internal regularity of each span and integrates the significant regularity information into the corresponding span-level representation, leading to precise entity type prediction. Meanwhile, the regularityagnostic module is devised to capture context information and avoid excessive focus on intra-span regularity. Furthermore, we adopt an orthogonality space restriction to encourage two branches to extract different features with regard to the regularity. To verify the effectiveness of our method, we conduct extensive experiments on three large-scale benchmark datasets (OntoNotes V4.0, OntoNotes V5.0, and MSRA). The results show that RICON achieves considerable improvements compared to the state-of-the-art models, even outperforming existing lexicon-based models. Moreover, we experiment on a practical medical dataset (CBLUE) to further demonstrate the ability of RICON.\n\nOur contributions can be summarized as follows:\n\n• This is the first work that explicitly explores the internal regularity of entity mentions for Chinese NER.\n\n• We propose a simple but effective method for Chinese NER, which effectively utilizes regularity information while avoiding excessive focus on intra-span regularity.\n\n• Extensive experiments on three large-scale benchmark datasets and a practical medical dataset demonstrate the effectiveness of our proposed method.\n\n ",
    "start": 3569,
    "end": 3573,
    "label": "Unsupported_claim"
  },
  {
    "span": "medical dataset (CBLUE",
    "document": "Introduction\n\nNamed entity recognition (NER) aims at identifying text spans pertaining to specific entity types. It plays an important role in many downstream tasks such as relation extraction (Ji et al., 2017), entity linking (Sevgili et al., 2020), co-reference resolution (Clark and Manning, 2016), and knowledge graph (Ji et al., 2020). Due to the complex composition (Gui et al., 2019), character-level Chinese NER is more challenging compared to English NER. As shown in Figure 1 (a), the middle charac-  ter \"流\" can constitute words with the characters to both their left and their right, such as \"河流 (River)\" and \"流经 (flows)\", leading to ambiguous character boundaries.\n\nThere are two typical frameworks for NER. The first one conceptualizes NER as a sequence labeling task (Huang et al., 2015;Lample et al., 2016;Ma and Hovy, 2016), where each character is assigned to a special label (e.g., B-LOC, I-LOC). The second one is span-based method (Li et al., 2020a;, which classifies candidate spans based on their span-level representations. However, despite the success of these two types of methods, they do not explicitly take the complex composition of Chinese NER into consideration. Recently, several works (Zhang and Yang, 2018;Gui et al., 2019;Li et al., 2020b) utilize external lexicon knowledge to help connect related characters and promote capturing the local composition. Nevertheless, building the lexicon is time-consuming and the quality of the lexicon may not be satisfied.\n\nIn contrast to previous works, we observe that the regularity exists in the common NER types (e.g., ORG and LOC). As shown in Figure 1 (a), \"尼日尔河 (Niger River)\" follows the specific composition pattern \"XX+河 (XX + River)\" which ends with indicator character \"河\" and mostly belongs to location type, and the ambiguous character \"流\" can properly constitute \"流经\" with the right character \"经\". Thus, the regularity information serves as important clues for entity type recognition and identifying the character composition. Formally, we refer to regularity as specific internal patterns contained in a type of entity (Lin et al., 2020). However, too immersed regularity leads to unfavorable boundary detection of entities and disturbing character composition. As shown in Figure 1 (b), \"中国队 (Chinese team)\" conforms to the pattern \"XX+队 (XX + Team)\", but the correct entity boundary should be \"中国 (Chinese)\" and \"队员 (players)\" according to the context. Therefore, the context also plays a key role in determining the character boundary.\n\nIn this paper, we introduce a simple but effective method to explore the regularity information of entity spans for Chinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON). The proposed model consists of two branches named regularity-aware module and regularity-agnostic module, where each module has task-specific encoder and optimization object. Concretely, the regularity-aware module aims at analyzing the internal regularity of each span and integrates the significant regularity information into the corresponding span-level representation, leading to precise entity type prediction. Meanwhile, the regularityagnostic module is devised to capture context information and avoid excessive focus on intra-span regularity. Furthermore, we adopt an orthogonality space restriction to encourage two branches to extract different features with regard to the regularity. To verify the effectiveness of our method, we conduct extensive experiments on three large-scale benchmark datasets (OntoNotes V4.0, OntoNotes V5.0, and MSRA). The results show that RICON achieves considerable improvements compared to the state-of-the-art models, even outperforming existing lexicon-based models. Moreover, we experiment on a practical medical dataset (CBLUE) to further demonstrate the ability of RICON.\n\nOur contributions can be summarized as follows:\n\n• This is the first work that explicitly explores the internal regularity of entity mentions for Chinese NER.\n\n• We propose a simple but effective method for Chinese NER, which effectively utilizes regularity information while avoiding excessive focus on intra-span regularity.\n\n• Extensive experiments on three large-scale benchmark datasets and a practical medical dataset demonstrate the effectiveness of our proposed method.\n\n ",
    "start": 3769,
    "end": 3791,
    "label": "Unsupported_claim"
  },
  {
    "span": "However, too immersed regularity leads to unfavorable boundary detection of entities and disturbing character composition.",
    "document": "Introduction\n\nNamed entity recognition (NER) aims at identifying text spans pertaining to specific entity types. It plays an important role in many downstream tasks such as relation extraction (Ji et al., 2017), entity linking (Sevgili et al., 2020), co-reference resolution (Clark and Manning, 2016), and knowledge graph (Ji et al., 2020). Due to the complex composition (Gui et al., 2019), character-level Chinese NER is more challenging compared to English NER. As shown in Figure 1 (a), the middle charac-  ter \"流\" can constitute words with the characters to both their left and their right, such as \"河流 (River)\" and \"流经 (flows)\", leading to ambiguous character boundaries.\n\nThere are two typical frameworks for NER. The first one conceptualizes NER as a sequence labeling task (Huang et al., 2015;Lample et al., 2016;Ma and Hovy, 2016), where each character is assigned to a special label (e.g., B-LOC, I-LOC). The second one is span-based method (Li et al., 2020a;, which classifies candidate spans based on their span-level representations. However, despite the success of these two types of methods, they do not explicitly take the complex composition of Chinese NER into consideration. Recently, several works (Zhang and Yang, 2018;Gui et al., 2019;Li et al., 2020b) utilize external lexicon knowledge to help connect related characters and promote capturing the local composition. Nevertheless, building the lexicon is time-consuming and the quality of the lexicon may not be satisfied.\n\nIn contrast to previous works, we observe that the regularity exists in the common NER types (e.g., ORG and LOC). As shown in Figure 1 (a), \"尼日尔河 (Niger River)\" follows the specific composition pattern \"XX+河 (XX + River)\" which ends with indicator character \"河\" and mostly belongs to location type, and the ambiguous character \"流\" can properly constitute \"流经\" with the right character \"经\". Thus, the regularity information serves as important clues for entity type recognition and identifying the character composition. Formally, we refer to regularity as specific internal patterns contained in a type of entity (Lin et al., 2020). However, too immersed regularity leads to unfavorable boundary detection of entities and disturbing character composition. As shown in Figure 1 (b), \"中国队 (Chinese team)\" conforms to the pattern \"XX+队 (XX + Team)\", but the correct entity boundary should be \"中国 (Chinese)\" and \"队员 (players)\" according to the context. Therefore, the context also plays a key role in determining the character boundary.\n\nIn this paper, we introduce a simple but effective method to explore the regularity information of entity spans for Chinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON). The proposed model consists of two branches named regularity-aware module and regularity-agnostic module, where each module has task-specific encoder and optimization object. Concretely, the regularity-aware module aims at analyzing the internal regularity of each span and integrates the significant regularity information into the corresponding span-level representation, leading to precise entity type prediction. Meanwhile, the regularityagnostic module is devised to capture context information and avoid excessive focus on intra-span regularity. Furthermore, we adopt an orthogonality space restriction to encourage two branches to extract different features with regard to the regularity. To verify the effectiveness of our method, we conduct extensive experiments on three large-scale benchmark datasets (OntoNotes V4.0, OntoNotes V5.0, and MSRA). The results show that RICON achieves considerable improvements compared to the state-of-the-art models, even outperforming existing lexicon-based models. Moreover, we experiment on a practical medical dataset (CBLUE) to further demonstrate the ability of RICON.\n\nOur contributions can be summarized as follows:\n\n• This is the first work that explicitly explores the internal regularity of entity mentions for Chinese NER.\n\n• We propose a simple but effective method for Chinese NER, which effectively utilizes regularity information while avoiding excessive focus on intra-span regularity.\n\n• Extensive experiments on three large-scale benchmark datasets and a practical medical dataset demonstrate the effectiveness of our proposed method.\n\n ",
    "start": 2131,
    "end": 2253,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Li et al., 2020a;,",
    "document": "Introduction\n\nNamed entity recognition (NER) aims at identifying text spans pertaining to specific entity types. It plays an important role in many downstream tasks such as relation extraction (Ji et al., 2017), entity linking (Sevgili et al., 2020), co-reference resolution (Clark and Manning, 2016), and knowledge graph (Ji et al., 2020). Due to the complex composition (Gui et al., 2019), character-level Chinese NER is more challenging compared to English NER. As shown in Figure 1 (a), the middle charac-  ter \"流\" can constitute words with the characters to both their left and their right, such as \"河流 (River)\" and \"流经 (flows)\", leading to ambiguous character boundaries.\n\nThere are two typical frameworks for NER. The first one conceptualizes NER as a sequence labeling task (Huang et al., 2015;Lample et al., 2016;Ma and Hovy, 2016), where each character is assigned to a special label (e.g., B-LOC, I-LOC). The second one is span-based method (Li et al., 2020a;, which classifies candidate spans based on their span-level representations. However, despite the success of these two types of methods, they do not explicitly take the complex composition of Chinese NER into consideration. Recently, several works (Zhang and Yang, 2018;Gui et al., 2019;Li et al., 2020b) utilize external lexicon knowledge to help connect related characters and promote capturing the local composition. Nevertheless, building the lexicon is time-consuming and the quality of the lexicon may not be satisfied.\n\nIn contrast to previous works, we observe that the regularity exists in the common NER types (e.g., ORG and LOC). As shown in Figure 1 (a), \"尼日尔河 (Niger River)\" follows the specific composition pattern \"XX+河 (XX + River)\" which ends with indicator character \"河\" and mostly belongs to location type, and the ambiguous character \"流\" can properly constitute \"流经\" with the right character \"经\". Thus, the regularity information serves as important clues for entity type recognition and identifying the character composition. Formally, we refer to regularity as specific internal patterns contained in a type of entity (Lin et al., 2020). However, too immersed regularity leads to unfavorable boundary detection of entities and disturbing character composition. As shown in Figure 1 (b), \"中国队 (Chinese team)\" conforms to the pattern \"XX+队 (XX + Team)\", but the correct entity boundary should be \"中国 (Chinese)\" and \"队员 (players)\" according to the context. Therefore, the context also plays a key role in determining the character boundary.\n\nIn this paper, we introduce a simple but effective method to explore the regularity information of entity spans for Chinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON). The proposed model consists of two branches named regularity-aware module and regularity-agnostic module, where each module has task-specific encoder and optimization object. Concretely, the regularity-aware module aims at analyzing the internal regularity of each span and integrates the significant regularity information into the corresponding span-level representation, leading to precise entity type prediction. Meanwhile, the regularityagnostic module is devised to capture context information and avoid excessive focus on intra-span regularity. Furthermore, we adopt an orthogonality space restriction to encourage two branches to extract different features with regard to the regularity. To verify the effectiveness of our method, we conduct extensive experiments on three large-scale benchmark datasets (OntoNotes V4.0, OntoNotes V5.0, and MSRA). The results show that RICON achieves considerable improvements compared to the state-of-the-art models, even outperforming existing lexicon-based models. Moreover, we experiment on a practical medical dataset (CBLUE) to further demonstrate the ability of RICON.\n\nOur contributions can be summarized as follows:\n\n• This is the first work that explicitly explores the internal regularity of entity mentions for Chinese NER.\n\n• We propose a simple but effective method for Chinese NER, which effectively utilizes regularity information while avoiding excessive focus on intra-span regularity.\n\n• Extensive experiments on three large-scale benchmark datasets and a practical medical dataset demonstrate the effectiveness of our proposed method.\n\n ",
    "start": 952,
    "end": 971,
    "label": "Format"
  },
  {
    "span": "He et al. (2020) propose MoCo, a contrastive method that maintains a momentum queue of negatives. BYOL eliminates negatives by employing an online and target network (Grill et al., 2020). Vision Transformers benefit from masked autoencoding objectives (He et al., 2022).",
    "document": "Related Work\n\nSelf-Supervised Representation Learning in Vision\n\nSelf-supervised learning has advanced visual pretraining through contrastive objectives, redundancy reduction, and generative masking. Pretrained features improve downstream tasks such as detection and segmentation while reducing labeled data requirements. He et al. (2020) propose MoCo, a contrastive method that maintains a momentum queue of negatives. BYOL eliminates negatives by employing an online and target network (Grill et al., 2020). Vision Transformers benefit from masked autoencoding objectives (He et al., 2022). Despite these advances, how pretraining objectives interact with limited compute and varied image resolutions is underexplored.\n\nTransfer to Downstream Tasks\n\nRecent work studies linear probing and full fine-tuning under domain shift. We focus on data- and compute-efficient pretraining that preserves transferability across scales.",
    "reason": "The sequence presents three separate works with different objectives but no connective explanation or transition, leaving the relationship between them unstated.",
    "start": 322,
    "end": 592,
    "label": "Coherence"
  },
  {
    "span": "recent works have shown that reinforcement learning significantly improves factual consistency in abstractive summarization",
    "document": "Introduction\n\nAbstractive summarization seeks to generate concise digests of source documents while preserving salient content. Traditional sequence-to-sequence architectures with attention have been widely adopted, and subsequent advances in pretraining have further improved fluency and coverage. However, factual consistency—ensuring that generated summaries do not contradict the source—remains a core challenge. To address this, some approaches augment maximum likelihood training with auxiliary objectives or decoding constraints. Moreover, recent works have shown that reinforcement learning significantly improves factual consistency in abstractive summarization, yet there is no consensus on which reward formulations transfer across domains. In this paper, we revisit reward design under constrained decoding and propose a lightweight method that balances salience and factuality without additional supervision.",
    "reason": "Mentions 'recent works' and a specific outcome without providing citations to those works (rule d; claim about prior work requires evidence).",
    "start": 547,
    "end": 670,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent studies introduce personalization layers or meta-learning strategies to adapt global federated models to client-specific data (Smith et al., 2018; Arivazhagan et al., 2019; Fallah et al., 2020; Collins et al., 2021). Other lines of work modify aggregation rules to account for non-IID distributions or participation heterogeneity (Li et al., 2020; Karimireddy et al., 2020; Reddi et al., 2021). Several papers also explore multi-task formulations where each client solves a related but distinct problem (Shapiro et al., 2019; Dinh et al., 2020).",
    "document": "Introduction\n\nFederated learning enables collaborative model training without centralizing raw data, making it attractive for domains with sensitive or siloed information. However, heterogeneity across clients often leads to suboptimal global models and degraded personalization. The community has responded with algorithmic and systems-level advances targeting optimization stability, communication efficiency, and privacy.\n\nRecent studies introduce personalization layers or meta-learning strategies to adapt global federated models to client-specific data (Smith et al., 2018; Arivazhagan et al., 2019; Fallah et al., 2020; Collins et al., 2021). Other lines of work modify aggregation rules to account for non-IID distributions or participation heterogeneity (Li et al., 2020; Karimireddy et al., 2020; Reddi et al., 2021). Several papers also explore multi-task formulations where each client solves a related but distinct problem (Shapiro et al., 2019; Dinh et al., 2020).\n\nDespite progress, the interplay between constrained on-device computation and practical personalization remains underexplored at scale. This paper studies resource-aware adaptation in federated settings with strict memory and latency budgets.\n\nWe evaluate our approach across image and text benchmarks, emphasizing low-power edge devices and intermittent participation. Our experiments suggest that judicious modularization and selective training can provide competitive personalization with modest cost.",
    "reason": "The span lists prior work across several subareas without explaining how these works relate to the paper’s aims or what specific gap persists, satisfying (a) and (b) of the definition.",
    "start": 426,
    "end": 978,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Hyndman and Khandakar (2008) automated ARIMA selection and evaluation. Hochreiter and Schmidhuber (1997) introduced LSTMs for long-range dependencies. Salinas et al. (2020) proposed DeepAR for probabilistic forecasting. Oreshkin et al. (2020) developed N-BEATS for univariate series.",
    "document": "Related Work\n\nTime Series Forecasting\n\nForecasting methods span classical statistical models, deep sequence models, and hybrid approaches that incorporate exogenous variables and probabilistic outputs. Comparisons often depend on data preprocessing choices and horizon-specific metrics.\n\nHyndman and Khandakar (2008) automated ARIMA selection and evaluation. Hochreiter and Schmidhuber (1997) introduced LSTMs for long-range dependencies. Salinas et al. (2020) proposed DeepAR for probabilistic forecasting. Oreshkin et al. (2020) developed N-BEATS for univariate series.\n\nCross-Domain Transfer\n\nMeta-learning and representation learning across heterogeneous time series have emerged to mitigate data scarcity per entity, yet stability across seasonal regimes remains challenging.\n\nOur Contribution\n\nWe propose a regime-aware forecaster that disentangles trend, seasonality, and event effects via shared latent factors with entity-specific adapters.",
    "reason": "The span lists four disparate forecasting works sequentially with no transitions or explicit explanation of their relationships, creating a coherence gap between sentences.",
    "start": 288,
    "end": 571,
    "label": "Coherence"
  },
  {
    "span": "Most existing object detection benchmarks overreport mAP due to lenient IoU thresholds.",
    "document": "Introduction\n\nObject detection has seen tremendous advances with two-stage and one-stage detectors trained on large-scale datasets such as PASCAL VOC and MS COCO (Ren et al., 2015; Lin et al., 2014). Robust evaluation protocols have been critical to tracking progress, particularly standardized metrics like mean Average Precision (Everingham et al., 2010). Most existing object detection benchmarks overreport mAP due to lenient IoU thresholds. This observation motivates our proposal of a stricter, size-aware IoU schedule and an error taxonomy that separately accounts for localization imprecision and category confusion, extending analyses similar in spirit to prior diagnostic tools (Hoiem et al., 2012).",
    "reason": "Asserts a broad critique of existing benchmarks and their metrics without citing studies that demonstrate overreporting due to IoU thresholds.",
    "start": 358,
    "end": 445,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior work leverages speed perturbation, SpecAugment, noise mixing, and TTS synthesis to expand limited corpora (Ko et al., 2015; Park et al., 2019; Cui et al., 2015; Chen et al., 2020). In this paper, we propose a multilingual augmentation pipeline that combines phoneme-level perturbation with cross-lingual mixing.",
    "document": "Introduction\n\nBuilding robust automatic speech recognition (ASR) systems in low-resource settings is hindered by limited transcribed audio and domain mismatch. Data augmentation has emerged as a practical strategy to improve generalization without collecting new data.\n\nPrior work leverages speed perturbation, SpecAugment, noise mixing, and TTS synthesis to expand limited corpora (Ko et al., 2015; Park et al., 2019; Cui et al., 2015; Chen et al., 2020). In this paper, we propose a multilingual augmentation pipeline that combines phoneme-level perturbation with cross-lingual mixing.\n\nWe evaluate our approach on four languages with diverse phonotactics and report improvements over standard baselines, along with ablations isolating the contribution of each augmentation component.",
    "reason": "The span moves from a list of prior augmentations directly to announcing the proposed method without explicitly stating the gap or why existing methods are insufficient, which matches (b).",
    "start": 270,
    "end": 587,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Time-series anomaly detection techniques include reconstruction-based autoencoders, forecasting residual analysis, one-class classification, and change-point detection (Zong et al., 2018; Malhotra et al., 2016; Hundman et al., 2018; Siffer et al., 2017; Xu et al., 2018). Recent methods leverage transformers and graph structures for multivariate dependencies (Chen et al., 2021; Deng and Hooi, 2021).",
    "document": "Introduction\n\nDetecting anomalies in IoT sensor streams is crucial for maintaining service reliability and safety. Real-world deployments demand high precision under scarce labels and non-stationary conditions.\n\nTime-series anomaly detection techniques include reconstruction-based autoencoders, forecasting residual analysis, one-class classification, and change-point detection (Zong et al., 2018; Malhotra et al., 2016; Hundman et al., 2018; Siffer et al., 2017; Xu et al., 2018). Recent methods leverage transformers and graph structures for multivariate dependencies (Chen et al., 2021; Deng and Hooi, 2021).\n\nHowever, many models overfit spurious correlations and fail to separate predictable regime shifts from true anomalies. Label scarcity compounds threshold calibration issues.\n\nWe introduce a causality-aware forecasting-reconstruction hybrid with adaptive thresholds learned from synthetic counterfactuals, improving robustness under distribution shift and limited labels.",
    "reason": "The span summarizes techniques and cites works but does not synthesize how they relate to the paper’s aims or identify the gap that motivates the contribution.",
    "start": 212,
    "end": 613,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Automated program repair approaches span template- and pattern-based methods (Kim et al., 2013; Long and Rinard, 2016), generate-and-validate systems (Le Goues et al., 2012; Martinez and Monperrus, 2016), and neural techniques that learn to produce patches from data (Tufano et al., 2019; Jiang et al., 2021).",
    "document": "Related Work\n\nAutomated program repair. APR seeks to automatically generate patches that make failing tests pass while preserving intended behavior. Automated program repair approaches span template- and pattern-based methods (Kim et al., 2013; Long and Rinard, 2016), generate-and-validate systems (Le Goues et al., 2012; Martinez and Monperrus, 2016), and neural techniques that learn to produce patches from data (Tufano et al., 2019; Jiang et al., 2021). Recent works further explore large language models fine-tuned on code and bug-fix corpora to improve patch correctness.\n\nSearch space and correctness. Efforts to constrain the search space leverage program analysis, type constraints, and semantics-aware transformations (Qi et al., 2015; Xiong et al., 2017). Patch correctness is evaluated with additional tests, semantic equivalence checking, and human assessment, with varying success (Smith et al., 2015; Ye et al., 2021).\n\nBenchmarks and reproducibility. Datasets such as Defects4J, Bugs.jar, and QuixBugs provide common testbeds, though concerns remain about overfitting to benchmark idiosyncrasies and inconsistent experimental protocols (Just et al., 2014; Lin et al., 2017).\n",
    "reason": "The span summarizes categories of prior APR methods without connecting them to the paper’s goals or explicitly stating what gap remains, thus lacking synthesis (criteria a and b).",
    "start": 149,
    "end": 458,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Kumar et al. 1",
    "document": "Introduction\n\nAlgorithmic fairness research spans definitions of group and individual fairness, practical mitigation techniques, and evaluations across domains (Dwork et al., 2012; Barocas et al., 2019). Despite numerous metrics, there is increasing awareness of trade-offs and incompatibilities among fairness criteria (Kleinberg et al., 2017; Chouldechova, 2017). This has been noted by Kumar et al. 1 in their survey of fairness regularizers for classification. Recent methods emphasize distributional robustness and causal reasoning to better align interventions with societal goals (Sagawa et al., 2020; Kusner et al., 2017). We contribute a benchmark that systematically varies dataset shifts and base rates, building on evaluation gaps identified in (Mitchell et al., 2021).",
    "reason": "Improper footnote-style marker after an author string; should include the year (e.g., Kumar et al. (YEAR)) or be reformatted as a proper footnote with consistent citation style.",
    "start": 389,
    "end": 403,
    "label": "Format"
  },
  {
    "span": "In autonomous driving, 3D object detection has been advanced by LiDAR-based networks (Yan et al., 2018; Shi et al., 2019), camera-only detectors (Chen et al., 2016; Zhou et al., 2019), and multi-modal fusion frameworks (Ku et al., 2018; Liang et al., 2020).",
    "document": "Related Work\n\nPerception stacks for autonomous vehicles rely on robust 3D object detection across varying weather, lighting, and traffic densities. Advances span sensor hardware and algorithms for extracting geometric and semantic cues.\n\nIn autonomous driving, 3D object detection has been advanced by LiDAR-based networks (Yan et al., 2018; Shi et al., 2019), camera-only detectors (Chen et al., 2016; Zhou et al., 2019), and multi-modal fusion frameworks (Ku et al., 2018; Liang et al., 2020).\n\nDatasets such as KITTI, nuScenes, and Waymo Open have encouraged the development of benchmarks for long-tail categories and open-vocabulary detection. Our approach later incorporates temporal fusion with uncertainty-aware calibration.\n",
    "reason": "Violates (a): it only lists categories of prior work without connecting them to the authors' objectives or explaining how this literature motivates or contrasts with the present study.",
    "start": 238,
    "end": 495,
    "label": "Lacks_synthesis"
  },
  {
    "span": "According to a recent survey, 73% of users prefer explanations that highlight feature attributions over counterfactuals.",
    "document": "Introduction\n\nExplainable AI (XAI) methods are increasingly deployed to justify model outputs to end users and regulators. Post hoc approaches such as feature attributions and counterfactuals offer complementary views of model behavior, but user preferences and task context often determine which explanation type is most effective.\n\nAccording to a recent survey, 73% of users prefer explanations that highlight feature attributions over counterfactuals. Nevertheless, preferences can reverse in high-stakes settings where actionable recourse is prioritized over transparency alone.\n\nThis paper studies a mixed-initiative interface that adaptively switches between attribution and counterfactual narratives based on user goals and uncertainty estimates.",
    "reason": "Presents a precise statistic from a 'recent survey' without citing the source.",
    "start": 334,
    "end": 454,
    "label": "Unsupported_claim"
  },
  {
    "span": "Singh et al. 1",
    "document": "Introduction\n\nDataset bias can cause models to overfit spurious correlations, reducing robustness under distribution shift (Torralba and Efros, 2011; Geirhos et al., 2020). This observation was noted by Singh et al. 1 in a survey of visual cues that models exploit under weak supervision. Follow-up work recommends balancing reweighting with targeted data augmentation (Kumar and Zhang, 2021) and using causal interventions to decouple features (Peters and Schölkopf, 2018). We extend these ideas with an adaptive sampler that estimates bias gradients online (Wang and Ilyas, 2022).",
    "reason": "Wrong use of footnote/numeric marker with an author–year citation; include the year 'Singh et al. (2019)' or format as a proper footnote/superscript, not 'Singh et al. 1'.",
    "start": 203,
    "end": 217,
    "label": "Format"
  },
  {
    "span": "(Kumar et. al., 2022)",
    "document": "Introduction\n\nOffline reinforcement learning (RL) aims to learn policies solely from logged interaction data without further exploration (Levine et al., 2020). This setting is attractive for safety-critical domains but introduces distributional shift challenges between the behavior and learned policies.\n\nConservative value estimation and support constraints have emerged as effective remedies (Fujimoto et al., 2019; Kumar et. al., 2022). Parallel work explores model-based offline RL to improve sample efficiency via learned dynamics (Yu et al., 2021; Kidambi et al., 2020).\n\nWe contribute a simple uncertainty-aware policy improvement step that tightens performance guarantees under limited coverage.",
    "reason": "Incorrect 'et al.' formatting: 'et. al.' is wrong; it should be 'et al.' without a period after 'et' -> '(Kumar et al., 2022)'.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "We follow the standard 5-way 1-shot setup introduced by prior work.",
    "document": "Introduction\n\nFew-shot image classification evaluates the ability to learn new categories from a handful of labeled examples. Benchmarks typically episodically sample tasks that mimic test-time conditions by constructing support and query sets from a held-out label space. We follow the standard 5-way 1-shot setup introduced by prior work. Under this protocol, a model must correctly classify queries among five classes after seeing a single labeled example per class.\n\nWhile metric-based and adaptation-based methods have achieved strong results on curated benchmarks, their robustness to domain shift and label noise remains unclear. We study these factors by introducing cross-domain evaluation episodes and stability metrics that quantify performance variance across seeds and tasks.",
    "reason": "Refers to a 'standard' evaluation setup and its introduction by prior work without citing the originating papers.",
    "start": 273,
    "end": 340,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Mikolov et al. 2013)",
    "document": "Introduction\n\nDistributional word representations enable efficient transfer to downstream tasks and remain a strong baseline despite the surge of large language models (Pennington et al., 2014; Peters et al., 2018). Static embeddings such as word2vec (Mikolov et al. 2013) capture syntactic and semantic regularities but struggle with polysemy. Contextual encoders address this limitation by dynamically conditioning on surrounding tokens (Devlin et al., 2019; Liu et al., 2019). We revisit classic embedding objectives to analyze their compositional properties under sparse supervision.",
    "reason": "Missing comma between authors and year in parenthetical citation; should be “(Mikolov et al., 2013)”.",
    "start": 251,
    "end": 272,
    "label": "Format"
  },
  {
    "span": "In medical imaging, saliency maps, class activation maps, counterfactuals, and concept-based explanations have been explored to render model outputs more interpretable to clinicians (Selvaraju et al., 2017; Ghorbani et al., 2019; Koh et al., 2020; Singla et al., 2021). Uncertainty quantification techniques such as Monte Carlo dropout and deep ensembles are also used to convey confidence (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017).",
    "document": "Related Work\n\nClinically reliable AI systems require not only high accuracy but also transparency and calibrated uncertainty. The literature on explainability and uncertainty for medical imaging is extensive and diverse.\n\nIn medical imaging, saliency maps, class activation maps, counterfactuals, and concept-based explanations have been explored to render model outputs more interpretable to clinicians (Selvaraju et al., 2017; Ghorbani et al., 2019; Koh et al., 2020; Singla et al., 2021). Uncertainty quantification techniques such as Monte Carlo dropout and deep ensembles are also used to convey confidence (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017).\n\nWe focus on integrating structured priors from radiology reporting into both explanation and uncertainty pipelines. Our approach aligns explanatory concepts with report-derived entities while calibrating predictions with label noise models.",
    "reason": "The span inventories methods and citations but omits how they inform or contrast with the authors’ approach, thus lacking synthesis per (a) and not articulating motivation per (c).",
    "start": 222,
    "end": 670,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Chen and Rao, 2017)",
    "document": "Related Work\n\nMulti-task learning leverages shared representations to improve generalization across related tasks (Caruana, 1997). Hard parameter sharing reduces overfitting by constraining layers to be common across tasks, while soft sharing ties parameters via regularization (Ruder, 2017). For sequence labeling, conditional random fields and neural-CRF hybrids have been explored (Lafferty et al., 2001; Ma and Hovy, 2016). In joint intent and slot modeling, attention mechanisms enhance label dependencies (Goo et al., 2018; Chen and Rao, 2017). Our architecture introduces task adapters with uncertainty-based weighting (Kendall et al., 2018).",
    "reason": "Wrong conjunction style inside parenthetical citation for APA-like format; should use '&' instead of 'and' in '(Chen & Rao, 2017)'.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "To mitigate privacy leakage in federated learning, the literature explores secure aggregation (Bonawitz et al., 2017), differential privacy (Abadi et al., 2016; Geyer et al., 2017), homomorphic encryption (Aono et al., 2017), and trusted execution environments (Tramer and Boneh, 2018).",
    "document": "Related Work\n\nFederated learning enables collaborative model training across distributed clients without centralizing raw data. However, gradient updates can leak sensitive information, prompting the development of privacy-preserving mechanisms.\n\nTo mitigate privacy leakage in federated learning, the literature explores secure aggregation (Bonawitz et al., 2017), differential privacy (Abadi et al., 2016; Geyer et al., 2017), homomorphic encryption (Aono et al., 2017), and trusted execution environments (Tramer and Boneh, 2018).\n\nWe study a training pipeline for cross-device learning and assess accuracy-privacy trade-offs on image and text benchmarks.",
    "reason": "The span enumerates techniques without explaining their trade-offs, relevance, or how they relate to the presented work, thus lacking synthesis.",
    "start": 247,
    "end": 533,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Knowledge distillation in NLP has been studied with vanilla soft-target distillation, layer-wise mimicry, and task-specific student compression (Hinton et al., 2015; Sun et al., 2019; Jiao et al., 2020; Sanh et al., 2019).",
    "document": "Related Work\n\nModel compression. Deploying large language models on edge devices motivates distillation and pruning. Knowledge distillation in NLP has been studied with vanilla soft-target distillation, layer-wise mimicry, and task-specific student compression (Hinton et al., 2015; Sun et al., 2019; Jiao et al., 2020; Sanh et al., 2019). Other threads include multi-teacher aggregation and contrastive representation alignment.\n\nLatency-accuracy trade-offs. Practical constraints require preserving task accuracy under strict latency budgets.\n\nWe introduce a latency-aware distillation schedule that adapts temperature and sample difficulty to meet target budgets.\n",
    "reason": "The span merely catalogs prior distillation approaches without explaining limitations or how they motivate a latency-aware schedule; it provides no synthesis or explicit gap.",
    "start": 117,
    "end": 339,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The LibriSpeech corpus contains 1000 hours of read English speech.",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) has benefited from larger datasets and stronger encoders, yet robustness to channel and speaker variability remains challenging. The LibriSpeech corpus contains 1000 hours of read English speech. While widely used for benchmarking, its acoustic homogeneity limits conclusions about noisy and conversational conditions. We therefore complement it with additional corpora featuring spontaneous speech and background noise. Our approach focuses on representation learning with multi-condition training to improve generalization under domain shift.",
    "reason": "States a specific dataset statistic without citing the source dataset paper (criteria a and b).",
    "start": 189,
    "end": 255,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT-based AES systems have already achieved human-level agreement on the ASAP benchmark.",
    "document": "Related Work\n\nAutomated Essay Scoring (AES) has progressed from hand-engineered features to neural encoders that capture richer discourse signals. Recent efforts emphasize domain adaptation and robustness to adversarial writing.\n\nBERT-based AES systems have already achieved human-level agreement on the ASAP benchmark. Yet, such systems often rely on prompt-specific correlations, raising concerns about fairness and generalization beyond the original prompts.",
    "reason": "Makes a specific performance claim about a benchmark without any citation; also first mention of the dataset/benchmark lacks a citation (rules a and b).",
    "start": 230,
    "end": 319,
    "label": "Unsupported_claim"
  },
  {
    "span": "Several recent works propose transformer-based one-stage detectors",
    "document": "Introduction\n\nObject detection has transitioned from region-based pipelines to end-to-end architectures that jointly localize and classify instances. Transformers enable global context modeling but can introduce latency that impedes real-time applications.\n\nRelated Work\n\nAnchor-free designs simplify training by removing hand-crafted priors, while query-based methods treat detection as set prediction. Several recent works propose transformer-based one-stage detectors that improve small-object recall and simplify post-processing. However, their performance often depends on large-scale pretraining and heavy data augmentation, which may not be feasible in resource-constrained settings.",
    "reason": "Mentions 'recent works' without providing citations to those works, violating rule (d).",
    "start": 404,
    "end": 470,
    "label": "Unsupported_claim"
  },
  {
    "span": "(O'Connor et al 2016)",
    "document": "Related Work\n\nRobust training objectives for structured prediction have incorporated margin penalties, confidence regularization, and entropy constraints. Some studies report marginal gains (O'Connor et al 2016) when pretraining corpora are limited, while others find larger benefits with strong data augmentation (Perez & Huang, 2019). Complementary lines of work exploit agreement across perturbed inputs to improve calibration (Gal & Ghahramani, 2016; Lakshminarayanan et al., 2017).\n\nOur approach integrates confidence penalties with perturbation-based consistency, yielding improved accuracy and better-calibrated probabilities.",
    "reason": "Missing comma after \"et al\" and missing period in \"al.\" within a parenthetical citation; APA-style should be \"(O'Connor et al., 2016)\".",
    "start": 190,
    "end": 211,
    "label": "Format"
  },
  {
    "span": "Previous studies show that larger batch sizes always improve generalization in contrastive learning.",
    "document": "Related Work\n\nContrastive learning has emerged as a powerful paradigm for representation learning by pulling together positive pairs and pushing apart negatives (Hadsell et al., 2006; Chen et al., 2020). A key design dimension is the availability of negatives, which can be sourced from large batches, memory banks, or momentum encoders (Wu et al., 2018; He et al., 2020). Optimization dynamics and temperature scaling have also been shown to affect feature uniformity and alignment (Wang and Isola, 2020). Previous studies show that larger batch sizes always improve generalization in contrastive learning. Nonetheless, computational constraints and diminishing returns motivate strategies that recycle or synthesize informative negatives without inflating memory footprints.\n",
    "reason": "Asserts a universal conclusion about prior work without any citation; sweeping claim about literature requires evidence.",
    "start": 507,
    "end": 607,
    "label": "Unsupported_claim"
  },
  {
    "span": "Knowledge distillation for large language models spans logit matching and softened labels (Hinton et al., 2015), feature- and attention-level alignment (Romero et al., 2015; Zagoruyko and Komodakis, 2017), task-specific distillation for NLP (Sanh et al., 2019; Jiao et al., 2020), and data-free or synthetic-data distillation (Yin et al., 2020; Fang et al., 2022). Compression techniques such as pruning and quantization are often combined with KD (Han et al., 2015; Frantar and Alistarh, 2022).",
    "document": "Related Work\n\nScaling laws show that larger language models yield better performance but are costly to deploy. Distillation and compression aim to reduce latency and memory while retaining accuracy and alignment.\n\nKnowledge distillation for large language models spans logit matching and softened labels (Hinton et al., 2015), feature- and attention-level alignment (Romero et al., 2015; Zagoruyko and Komodakis, 2017), task-specific distillation for NLP (Sanh et al., 2019; Jiao et al., 2020), and data-free or synthetic-data distillation (Yin et al., 2020; Fang et al., 2022). Compression techniques such as pruning and quantization are often combined with KD (Han et al., 2015; Frantar and Alistarh, 2022).\n\nRecent alignment work incorporates preference modeling and safety constraints, but efficient student training under restricted data access remains underexplored.\n\nWe propose Constrained-KD, a privacy-aware distillation pipeline that integrates preference-aligned teachers with data curation under confidentiality constraints.",
    "reason": "The span lists KD variants and related compression without indicating how these relate to the paper’s constraints or what limitation the new method targets.",
    "start": 214,
    "end": 709,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Industry deployments typically implement demographic parity because it is easier to explain to stakeholders.",
    "document": "Introduction\n\nFair ranking seeks to allocate exposure while satisfying normative constraints across protected groups. Multiple definitions of fairness, including demographic parity and equal opportunity, yield different trade-offs between utility and equity.\n\nIndustry deployments typically implement demographic parity because it is easier to explain to stakeholders. Yet parity-based constraints can misallocate exposure under heterogeneous relevance, motivating context-aware formulations that account for uncertainty and position bias.\n\nWe contribute a calibration-aware exposure allocator that optimizes group exposure subject to uncertainty-adjusted constraints, enabling interpretable trade-offs in production-like settings.\n",
    "reason": "The sentence makes a claim about common industry practice without citing any studies or case reports, which requires evidence or citations.",
    "start": 260,
    "end": 368,
    "label": "Unsupported_claim"
  },
  {
    "span": "The FedScale Challenge established a standardized evaluation pipeline for cross-device FL.",
    "document": "Introduction\n\nFederated learning (FL) enables model training across decentralized devices without centralizing raw data, but heterogeneity in device availability, data distributions, and network conditions complicates reproducible evaluation. Benchmarks have emerged to characterize these dimensions and to compare aggregation strategies and personalization techniques.\n\nThe FedScale Challenge established a standardized evaluation pipeline for cross-device FL. Yet, disparate client sampling policies and resource simulators remain common, making it difficult to compare methods under consistent constraints. We present a reproducible simulator and protocol that align client availability, training budgets, and straggler behavior with real-world traces.\n\nOur experiments quantify the trade-offs between fairness and efficiency and provide guidance for choosing aggregation rules under skewed participation.",
    "reason": "Mentions a specific competition/benchmark and its contribution without citing it (rule a).",
    "start": 371,
    "end": 461,
    "label": "Unsupported_claim"
  },
  {
    "span": "Predictive uncertainty for medical image segmentation is often estimated via Monte Carlo dropout (Kendall and Gal, 2017), deep ensembles (Lakshminarayanan et al., 2017), and test-time augmentation (Wang et al., 2019). Calibration techniques such as temperature scaling and focal losses have also been investigated (Guo et al., 2017; Mukhoti et al., 2020). In this paper, we adopt similar uncertainty estimation strategies.",
    "document": "Related Work\n\nDeploying segmentation systems in clinical workflows requires reliable estimates of predictive uncertainty and well-calibrated probabilities. Prior art spans Bayesian approximations, ensembling, and calibration procedures.\n\nPredictive uncertainty for medical image segmentation is often estimated via Monte Carlo dropout (Kendall and Gal, 2017), deep ensembles (Lakshminarayanan et al., 2017), and test-time augmentation (Wang et al., 2019). Calibration techniques such as temperature scaling and focal losses have also been investigated (Guo et al., 2017; Mukhoti et al., 2020). In this paper, we adopt similar uncertainty estimation strategies.\n\nClinical datasets exhibit significant domain shift across scanners and sites. We evaluate our method across multiple cohorts.",
    "reason": "The span summarizes prior methods and then states the authors will adopt similar strategies without clarifying the gap or how their approach differs, failing to synthesize (criterion b).",
    "start": 238,
    "end": 660,
    "label": "Lacks_synthesis"
  },
  {
    "span": "the SQuAD dataset",
    "document": "Introduction\n\nExtractive question answering (QA) evaluates a model's ability to locate answer spans within a given passage. Early neural QA systems leveraged attention over bidirectional encoders to align questions and contexts (Seo et al., 2017; Wang and Jiang, 2017). Large-scale supervision from the SQuAD dataset enabled rapid progress and established strong baselines for span prediction. Despite these advances, models often exploit dataset artifacts and struggle with compositional generalization (Jia and Liang, 2017; Sugawara et al., 2020). We investigate structure-aware pretraining to improve compositional reasoning in extractive QA.",
    "reason": "First mention of a well-known dataset should be accompanied by a citation to its original paper; none is provided.",
    "start": 299,
    "end": 316,
    "label": "Unsupported_claim"
  },
  {
    "span": "Bayesian knowledge tracing models student mastery as latent variables updated after each response (Corbett and Anderson, 1995). Deep knowledge tracing replaces hand-crafted updates with recurrent networks (Piech et al., 2015). Attention-based knowledge tracing highlights informative interactions (Pandey and Karypis, 2019). Spaced repetition schedules practice to improve retention (Cepeda et al., 2006).",
    "document": "Related Work\n\nStudent modeling and personalization aim to estimate learner knowledge and adapt content accordingly. Predictive models of mastery and forgetting inform system decisions about what to present next.\n\nBayesian knowledge tracing models student mastery as latent variables updated after each response (Corbett and Anderson, 1995). Deep knowledge tracing replaces hand-crafted updates with recurrent networks (Piech et al., 2015). Attention-based knowledge tracing highlights informative interactions (Pandey and Karypis, 2019). Spaced repetition schedules practice to improve retention (Cepeda et al., 2006).\n\nWe propose a unified model that couples mastery estimation with scheduling under uncertainty.",
    "reason": "The sequence lists related but distinct areas without transitions or explicit explanation of how spaced repetition connects to knowledge tracing, causing an abrupt, implied relationship.",
    "start": 213,
    "end": 618,
    "label": "Coherence"
  },
  {
    "span": "In (Lopez et al., 2021)",
    "document": "Related Work\n\nGraph neural networks (GNNs) generalize convolution to non-Euclidean domains by propagating and transforming messages along edges. Spectral methods (Bruna et al., 2014) and spatial methods (Kipf and Welling, 2017) inaugurated the field, with subsequent advances in expressivity (Xu et al., 2019) and scalability (Chiang et al., 2019). In (Lopez et al., 2021) we see that residual connections and normalization substantially stabilize deeper GNNs. Attention mechanisms also improve performance on heterogeneous graphs (Velickovic et al., 2018), and sampling-based training reduces memory footprints for large networks (Hamilton et al., 2017).\n\nOur study examines curriculum-based neighborhood expansion for more stable convergence. We compare against baselines that use fixed-hop neighborhoods and adaptive depth selection, and we evaluate on citation and product networks under transductive and inductive settings.",
    "reason": "Wrong citation style for a narrative citation; it should be written as 'In Lopez et al. (2021) we see ...' rather than 'In (Lopez et al., 2021)'.",
    "start": 349,
    "end": 372,
    "label": "Format"
  },
  {
    "span": "SpecAugment distorts the spectrogram to regularize models (Park et al., 2019). Noise and reverberation simulation improves robustness (Ko et al., 2017). TTS-augmented semi-supervision scales to low-resource languages (Laptev et al., 2020). Pseudo-labeling leverages unlabeled speech (Kahn et al., 2020).",
    "document": "Related Work\n\nData augmentation for ASR. Augmentation reduces overfitting and improves robustness to acoustic variability by transforming inputs or synthesizing data (Cui et al., 2015; Ko et al., 2017). Recent semi- and self-supervised training regimes further exploit large unlabeled corpora (Kahn et al., 2020; Baevski et al., 2020).\n\nSpecAugment distorts the spectrogram to regularize models (Park et al., 2019). Noise and reverberation simulation improves robustness (Ko et al., 2017). TTS-augmented semi-supervision scales to low-resource languages (Laptev et al., 2020). Pseudo-labeling leverages unlabeled speech (Kahn et al., 2020). Our work unifies these ideas by learning augmentation policies that adapt to target-domain acoustics while calibrating confidence for pseudo-label selection.",
    "reason": "The span lists several techniques with citations but does not state how they relate to each other. There are no transitions, and the relationships among augmentation, simulation, TTS, and pseudo-labeling are left implicit.",
    "start": 337,
    "end": 640,
    "label": "Coherence"
  },
  {
    "span": "Work on QA over tables includes semantic parsers that map questions to logical forms (Pasupat and Liang, 2015; Zhong et al., 2017), neural rankers that retrieve relevant cells or rows (Chen et al., 2020; Krishnan et al., 2021), and pre-trained models adapted to semi-structured data (Herzig et al., 2020; Yin et al., 2020).",
    "document": "Introduction\n\nQuestion answering (QA) over tables requires models to understand schema, perform discrete reasoning, and ground operations in semi-structured formats. While large language models have improved generalization, they often struggle with compositional reasoning and unseen schemas.\n\nWork on QA over tables includes semantic parsers that map questions to logical forms (Pasupat and Liang, 2015; Zhong et al., 2017), neural rankers that retrieve relevant cells or rows (Chen et al., 2020; Krishnan et al., 2021), and pre-trained models adapted to semi-structured data (Herzig et al., 2020; Yin et al., 2020).\n\nWe introduce a schema-conditioned reasoning framework that disentangles operation planning from execution, using a planner to predict a sequence of typed actions and an executor to ground them in the table. This separation improves robustness to novel schemas and reduces spurious programs.",
    "reason": "Provides a catalogue of prior approaches without synthesizing their relationships, articulating a gap, or clarifying how the new framework addresses specific shortcomings.",
    "start": 294,
    "end": 617,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Kim et al., 2018; Rao and Singh, 2020;",
    "document": "Introduction\n\nPretrained language models have rapidly advanced document classification by leveraging large-scale unlabeled corpora for representation learning (Devlin et al., 2019; Liu et al., 2019). Early neural approaches (Kim et al., 2018; Rao and Singh, 2020; focus on shallow architectures, while later works incorporate hierarchical encoders to capture discourse-level signals (Yang et al., 2016; Guo et al., 2020). However, most methods still struggle when label distributions shift across domains, and their robustness under distribution shift remains underexplored (Sagawa et al., 2020; Hendrycks et al., 2020).\n\nIn this paper, we study domain-robust document classification with a simple augmentation strategy that encourages invariance to lexical perturbations. We compare our approach against strong adversarial and consistency-based baselines and show improvements on three benchmarks spanning newswire, reviews, and scientific abstracts (Howard and Ruder, 2018; Gururangan et al., 2020).",
    "reason": "The multi-citation is missing the closing parenthesis and ends with an extraneous semicolon; it should be closed properly, e.g., “(Kim et al., 2018; Rao and Singh, 2020)”.",
    "start": 224,
    "end": 263,
    "label": "Format"
  },
  {
    "span": "The ACE and TAC KBP benchmarks define canonical event types and roles for evaluation (Walker et al., 2006; Mitamura et al., 2017). Pretrained language models provide contextual features that improve trigger and argument identification (Peters et al., 2018; Devlin et al., 2019). Distant supervision aligns text with knowledge bases to expand labeled data for relation extraction (Mintz et al., 2009; Zeng et al., 2015). Graph neural networks capture dependencies among event mentions (Liu et al., 2020).",
    "document": "Related Work\n\nEvent and Relation Extraction\nInformation extraction systems aim to identify structured facts about entities, relations, and events from text. Progress has come from better representations, richer supervision signals, and more realistic evaluation datasets (Ji and Grishman, 2008; Nguyen et al., 2016).\n\nDatasets, Models, and Supervision\nThe ACE and TAC KBP benchmarks define canonical event types and roles for evaluation (Walker et al., 2006; Mitamura et al., 2017). Pretrained language models provide contextual features that improve trigger and argument identification (Peters et al., 2018; Devlin et al., 2019). Distant supervision aligns text with knowledge bases to expand labeled data for relation extraction (Mintz et al., 2009; Zeng et al., 2015). Graph neural networks capture dependencies among event mentions (Liu et al., 2020). Domain adaptation and cross-document modeling further improve coverage in low-resource settings (Subramanian et al., 2020; Du and Cardie, 2020).\n\nOur Perspective\nWe propose a unified framework that jointly models events and relations with shared span representations and constrained decoding, evaluated under consistent cross-domain splits.",
    "reason": "The span lists datasets, model architectures, supervision strategies, and graph methods in separate sentences without transitions or explicit connections, leaving their relationships implied and incoherent.",
    "start": 352,
    "end": 855,
    "label": "Coherence"
  },
  {
    "span": "Recent works have dramatically improved multi-step forecasting accuracy",
    "document": "Introduction\n\nAccurate traffic forecasting underpins intelligent transportation systems and urban planning. Graph neural networks (GNNs) model spatial dependencies between sensors, while temporal modules capture periodicity and non-stationarity. Yet, compounding errors in multi-step prediction and sensitivity to missing data remain open challenges.\n\nRelated Work\n\nTopology-aware GNNs learn sensor embeddings from road networks, and attention mechanisms adapt to dynamic congestion patterns. Recent works have dramatically improved multi-step forecasting accuracy, largely by integrating diffusion processes and residual connections into spatiotemporal architectures. Nevertheless, most methods assume fixed graphs and struggle when sensors fail or are newly deployed.",
    "reason": "The phrase 'Recent works' asserts progress without providing citations, violating rule (d).",
    "start": 493,
    "end": 564,
    "label": "Unsupported_claim"
  },
  {
    "span": "Lopez et al.",
    "document": "Introduction\n\nCross-lingual summarization aims to generate target-language summaries from source-language documents, reducing the need for expensive parallel datasets (Nallapati et al., 2016; See et al., 2017; Zhu et al., 2019). Early approaches adapt machine translation pipelines followed by monolingual summarization (Ladhak et al., 2020), while more recent work explores end-to-end multilingual encoders and decoders (Chen et al., 2020; Tang et al., 2021).\n\nNeural methods have benefited from pretrained multilingual language models that provide shared semantic spaces (Conneau and Lample, 2019; Liu et al., 2020). However, domain shifts and limited target-side supervision remain central challenges (Wei et al., 2021). Lopez et al. propose leveraging weak alignments via comparable corpora to mitigate coverage gaps, and hybridize extractive and abstractive objectives. In contrast, we introduce a constrained decoding scheme that explicitly balances content preservation and fluency while remaining robust to noisy alignments (Goyal et al., 2020; Xu et al., 2021).\n\nOur contributions are threefold: i) a cross-lingual training objective that reweights sentence-level salience signals using pseudo-parallel anchors (Cao et al., 2018); ii) a selective knowledge distillation strategy from teacher MT models (Kim and Rush, 2016; Jiao et al., 2020); and iii) comprehensive evaluation across low- and medium-resource settings (Narayan et al., 2018; Bhandari et al., 2020).",
    "reason": "Narrative citation missing year; in author–year style, a narrative mention should include the year, e.g., 'Lopez et al. (2019)'.",
    "start": 724,
    "end": 736,
    "label": "Format"
  },
  {
    "span": "The first benchmark for graph out-of-distribution detection was released in 2021.",
    "document": "Related Work\n\nOut-of-distribution (OOD) detection has been extensively studied in computer vision (Hendrycks and Gimpel, 2017; Liang et al., 2018) and more recently considered for graph-structured data with graph neural networks (Kipf and Welling, 2017; Veličković et al., 2018). Efforts to quantify uncertainty and calibration for GNNs have emerged in parallel (Guo et al., 2017; Mukhoti and Gal, 2018; Liu et al., 2020).\n\nThe first benchmark for graph out-of-distribution detection was released in 2021. Subsequent methods adapt energy scores and contrastive learning to node and graph classification settings, but the evaluation protocols remain inconsistent across studies.\n\nIn this paper, we introduce a unified evaluation suite and a simple baseline that improves detection under class- and structure-shifted OOD settings.",
    "reason": "Claims a specific historical first benchmark release without any citation to the benchmark or paper (rule a/b).",
    "start": 424,
    "end": 505,
    "label": "Unsupported_claim"
  },
  {
    "span": "Blei et al. (2003) introduced LDA for unsupervised topic discovery. Dieng et al. (2020) proposed dynamic embedded topic models that capture temporal drift. Neural variational inference with amortized encoders has also been explored (Miao et al., 2016). Sentence-BERT improves semantic similarity (Reimers and Gurevych, 2019).",
    "document": "Related Work\n\nNeural Topic Models and Contextual Representations\n\nClassic probabilistic topic models have provided a foundation for discovering latent themes in large text corpora, while recent neural approaches couple richer representations with scalable inference. Our method builds on this line of work by aligning topic discovery with sentence-level semantics.\n\nBlei et al. (2003) introduced LDA for unsupervised topic discovery. Dieng et al. (2020) proposed dynamic embedded topic models that capture temporal drift. Neural variational inference with amortized encoders has also been explored (Miao et al., 2016). Sentence-BERT improves semantic similarity (Reimers and Gurevych, 2019).\n\nWe focus on integrating topic structure with contextual encoders to support downstream document retrieval without supervision.",
    "reason": "Abrupt shift from topic models and variational inference to Sentence-BERT is not explained; there is no transition clarifying how sentence embeddings relate to the preceding topic modeling works.",
    "start": 366,
    "end": 691,
    "label": "Coherence"
  },
  {
    "span": "Lopez et al.",
    "document": "Related Work\n\nRecommender systems have progressed from latent factor models (Koren et al., 2009) to neural collaborative filtering (He et al., 2017) and graph-based recommenders that capture higher-order connectivity (Wang et al., 2019; Ying et al., 2018). Context-aware models incorporate temporal dynamics and side information to refine preference estimates (Rendle, 2010; Hidasi et al., 2016).\n\nBuilding on Lopez et al., we propose a hybrid objective that balances sequence modeling with user-item graph signals to address cold-start scenarios. Prior hybrid systems often overfit to popular items or suffer from exposure bias (Chen et al., 2020; Steck, 2011). Our approach regularizes popularity while preserving personalization.\n\nWe also compare against slate-aware ranking methods and counterfactual estimators that correct for logging policy biases (Swaminathan and Joachims, 2015; Ma et al., 2020).",
    "reason": "Narrative citation missing year: the narrative mention 'Lopez et al.' should include the year as 'Lopez et al. (YEAR)'.",
    "start": 410,
    "end": 422,
    "label": "Format"
  },
  {
    "span": "Privacy-preserving deep learning commonly applies differentially private SGD with per-sample clipping, advanced composition, Rényi accounting, and secure aggregation (Abadi et al., 2016; Mironov, 2017; Wang et al., 2019; Kairouz et al., 2021; Bonawitz et al., 2017).",
    "document": "Introduction\n\nTraining deep models with privacy guarantees is essential in sensitive domains such as health and finance. Differential privacy (DP) provides a principled framework to bound information leakage, yet practical deployments must balance utility, computation, and formal guarantees.\n\nPrivacy-preserving deep learning commonly applies differentially private SGD with per-sample clipping, advanced composition, Rényi accounting, and secure aggregation (Abadi et al., 2016; Mironov, 2017; Wang et al., 2019; Kairouz et al., 2021; Bonawitz et al., 2017). Recent work explores adaptive clipping, privacy amplification via subsampling, and tighter moment accounting to improve utility at fixed privacy budgets.\n\nWe propose a curvature-aware noise allocation scheme that dynamically rebalances noise across layers to reduce excess regularization while maintaining end-to-end DP guarantees.",
    "reason": "The span provides a list of techniques and citations but does not explain their relation to the paper’s contribution or the specific deficiency addressed, reflecting a lack of synthesis.",
    "start": 294,
    "end": 560,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The winning solutions of the Kaggle M5 competition demonstrated that simple gradient-boosted trees can outperform deep models for long-horizon retail forecasting.",
    "document": "Related Work\n\nDemand forecasting integrates statistical time-series models with feature-rich machine learning approaches to handle seasonality, promotions, and hierarchical aggregation. Classical methods such as exponential smoothing and ARIMA remain competitive in certain regimes due to their inductive biases and interpretability. Machine learning techniques—gradient-boosted trees, random forests, and deep sequence models—leverage covariates like calendar events, prices, and inventory constraints. The winning solutions of the Kaggle M5 competition demonstrated that simple gradient-boosted trees can outperform deep models for long-horizon retail forecasting. Subsequent studies investigated reconciliation for hierarchical consistency and probabilistic accuracy metrics aligned with inventory cost. Transfer learning across stores and items, as well as meta-learning for model selection, further improved accuracy under sparse sales histories. Our contribution revisits tree-based ensembles with calibrated probabilistic outputs and introduces a cross-series representation that preserves per-item seasonality while enabling information sharing.",
    "reason": "Makes a specific claim about results from a named competition without citation; such references need supporting sources (rule a).",
    "start": 504,
    "end": 666,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Wu et al., 2019;Chen, 2020;.",
    "document": "Related Work\n\nMultimodal pretraining aligns visual and textual representations via contrastive and masked modeling objectives (Radford et al., 2021; Li et al., 2021). Cross-modal fusion architectures include co-attention transformers and dual-encoder setups for retrieval (Lu et al., 2019; Tan and Bansal, 2019). Prior analyses report shortcut learning and unimodal collapse on biased datasets (Goyal et al., 2017; Agarwal et al., 2020). Several diagnostic benchmarks test compositionality and spatial reasoning (Hudson and Manning, 2019; Subramanian et al., 2020; Wu et al., 2020). To situate our contribution, we compare against strong baselines from instruction-tuned vision–language models and evaluate robustness under caption perturbations (Rosenberg et al., 2021). We also review dataset curation practices to reduce artifacts (Wu et al., 2019;Chen, 2020;. Our findings highlight the need for balanced negative sampling and stricter controls on lexical overlap.\n",
    "reason": "Extraneous punctuation and spacing in a citation list; there is a missing space after the semicolon and an unnecessary semicolon before the period. It should be formatted like “(Wu et al., 2019; Chen, 2020).”",
    "start": 834,
    "end": 863,
    "label": "Format"
  },
  {
    "span": "Iyer et al. (2016) used attention-based seq2seq models for code-to-text generation. LeClair et al. (2019) studied the role of structural information via AST encoders. Dataset curation practices differ across languages (Allamanis, 2018). Transformer architectures improve performance (Ahmad et al., 2020).",
    "document": "Related Work\n\nNeural code summarization aims to generate natural language descriptions of source code. Approaches vary in how they represent code structure, integrate lexical and semantic signals, and handle multilingual codebases.\n\nIyer et al. (2016) used attention-based seq2seq models for code-to-text generation. LeClair et al. (2019) studied the role of structural information via AST encoders. Dataset curation practices differ across languages (Allamanis, 2018). Transformer architectures improve performance (Ahmad et al., 2020).\n\nWe propose a structure-aware pretraining objective that aligns AST paths with subword spans to provide stronger inductive bias for summarization across repositories.",
    "reason": "The span mixes modeling approaches, dataset issues, and architectures without transitions or an explicit explanation of how these aspects influence each other.",
    "start": 233,
    "end": 537,
    "label": "Coherence"
  },
  {
    "span": "O'Neil et al., (2020)",
    "document": "Introduction\n\nPrivacy-preserving data publishing aims to share useful datasets while bounding disclosure risks (Dwork, 2006; Fung et al., 2010). Differential privacy (DP) provides rigorous guarantees by randomizing outputs to limit individual influence (Dwork et al., 2014). Practical deployments balance privacy budgets with utility through careful mechanism design (Abowd, 2018; Adams and Sisson, 2019).\n\nAt the model level, DP-SGD trains neural networks with per-example gradient clipping and noise (Abadi et al., 2016), while posterior sampling and PATE provide alternatives for classification (Papernot et al., 2017; Wang et al., 2019). O'Neil et al., (2020) survey pitfalls in applying nominal k-anonymity to high-dimensional records, aligning with follow-up analyses on linkage attacks (Sweeney, 2002; Narayanan and Shmatikov, 2008).\n\nWe revisit release mechanisms for sparse count data, proposing a Poisson subsampling scheme that improves accuracy under tight privacy budgets.",
    "reason": "Incorrect punctuation/style for narrative citation; the comma before the parenthetical year is not standard. Should be “O'Neil et al. (2020)”.",
    "start": 642,
    "end": 663,
    "label": "Format"
  },
  {
    "span": "Vatswani et al.",
    "document": "Related Work\n\nNeural text simplification has evolved from rule-based pipelines to end-to-end neural architectures that leverage large pretrained encoders (Wang et al., 2020; Sato and Aizawa, 2019). Prior work by Vatswani et al. investigates few-shot adaptation in multilingual settings, while Rahimi et al. (2019) examine domain shift and robustness under sparse supervision.\n\nAdaptive decoding and controllable generation have also been explored to balance simplicity and adequacy (Martin et al., 2020; Nishihara et al., 2021). More recently, constrained decoding and post-editing strategies have been paired with uncertainty estimation to minimize meaning drift (Kumar and Talukdar, 2022; Zhao et al., 2023). Despite these advances, domain portability and faithfulness remain open challenges.",
    "reason": "Narrative citation missing year; should be in the form 'Vatswani et al. (YEAR)'.",
    "start": 212,
    "end": 227,
    "label": "Format"
  },
  {
    "span": "BERT has been used in AES to score essays with human-level accuracy.",
    "document": "Introduction\n\nAutomated essay scoring (AES) seeks to predict rubric-aligned scores from student writing. Modern neural approaches leverage pretraining and attention to capture coherence, argumentation, and surface features beyond n-grams. BERT has been used in AES to score essays with human-level accuracy. However, confounds such as prompt leakage, adversarial superficial cues, and distributional shift across prompts complicate fair assessment. We propose a cross-prompt evaluation protocol and investigate calibration techniques that maintain validity across genres and grade levels.",
    "reason": "Specific claim about BERT achieving 'human-level accuracy' in a niche task requires citation to the study demonstrating it.",
    "start": 239,
    "end": 307,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Garcia 2017)",
    "document": "Related Work\n\nData augmentation has been key to improving generalization in low-resource NLP tasks (Fadaee et al., 2017; Kobayashi, 2018). Techniques range from token-level substitutions to paraphrasing with back-translation (Sennrich et al., 2016; Xie et al., 2020).\n\nSemantic-preserving edits can reduce overfitting while maintaining label consistency (Wei and Zou, 2019; Kumar et al., 2020). However, simple heuristics may introduce distribution shifts that hurt downstream performance (Prabhumoye et al., 2018). To address this, some works learn augmentation policies conditioned on task loss (Garcia 2017; Ratner et al., 2017), while others leverage pretrained LMs to produce label-aware variants (Anaby-Tavor et al., 2020; Yoo et al., 2021).\n\nOur approach jointly optimizes augmentation and classifier parameters via a bilevel objective that enforces consistency under perturbations and improves calibration (Thulasidasan et al., 2019; Guo et al., 2017).",
    "reason": "Missing comma between author and year in a parenthetical citation; should be '(Garcia, 2017)'.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "We follow the setup of the CoNaLa dataset with manual paraphrases and abstracted API calls.",
    "document": "Introduction\n\nProgram synthesis from natural language aims to generate executable code that satisfies a user intent, often specified through short utterances and examples. Recent progress leverages pretrained code-language models and retrieval of similar problems to improve grounding. We follow the setup of the CoNaLa dataset with manual paraphrases and abstracted API calls. While this configuration facilitates evaluation of intent understanding, it may underrepresent real-world ambiguity. We therefore introduce a harder split with noisy paraphrases and partial specifications to test robustness.",
    "reason": "Mentions a specific dataset and experimental setup without providing a citation to CoNaLa.",
    "start": 286,
    "end": 377,
    "label": "Unsupported_claim"
  },
  {
    "span": "According to recent industry surveys, 62% of users rely on recommendations for content discovery.",
    "document": "Introduction\n\nRecommender systems are central to user engagement on content platforms, yet optimizing for long-term satisfaction rather than short-term clicks remains an open challenge. According to recent industry surveys, 62% of users rely on recommendations for content discovery. This underscores the need to balance novelty, diversity, and personalization under evolving preferences. We propose a counterfactual evaluation framework that estimates long-horizon value while respecting platform constraints on latency and inventory exposure.",
    "reason": "Presents a precise statistic attributed to 'recent industry surveys' without citing the source survey(s) (criterion b).",
    "start": 186,
    "end": 283,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Köhler et. al., 2016)",
    "document": "Introduction\n\nAutomatic evaluation of text generation has traditionally relied on n-gram overlap metrics such as BLEU and ROUGE (Papineni et al., 2002; Lin, 2004). However, these metrics correlate imperfectly with human judgments of adequacy and fluency, especially for abstractive tasks (Novikova et al., 2017; Chaganty et al., 2018). Learned evaluators based on pretrained language models have emerged to capture semantic similarity beyond surface forms (Zhang et al., 2020; Sellam et al., 2020). Calibration and robustness of these evaluators remain active research topics, with concerns about domain shift and prompt sensitivity (Gehrmann et al., 2023). Prior work on reference-free metrics aims to reduce reliance on costly human references while maintaining reliability (Kryscinski et al., 2020; (Köhler et. al., 2016) surveyed early attempts at readability and grammaticality estimation. We build upon these insights by proposing a dual-view evaluator that jointly models semantic entailment and factual consistency using multi-task training.\n",
    "reason": "Incorrect abbreviation 'et. al.'; should be 'et al.' without a period after 'et'.",
    "start": 802,
    "end": 824,
    "label": "Format"
  },
  {
    "span": "the SemEval-2020 sarcasm detection shared task",
    "document": "Introduction\n\nSarcasm detection is a long-standing challenge in computational social science and NLP due to its reliance on pragmatic cues, context, and world knowledge. Benchmarks have emerged to standardize evaluation, most notably the SemEval-2020 sarcasm detection shared task, which catalyzed research interest in modeling irony in social media. However, current methods often overfit to superficial lexical markers and struggle with cross-domain transfer. Our work addresses this gap by incorporating user- and thread-level context alongside contrastive pretraining objectives designed to distinguish literal from sarcastic intent.",
    "reason": "First mention of a shared task requires a citation to the official task description or proceedings; none is provided.",
    "start": 234,
    "end": 280,
    "label": "Unsupported_claim"
  },
  {
    "span": "We evaluate on the StackOverflow next-word prediction dataset and the LEAF benchmark.",
    "document": "Introduction\n\nFederated learning enables collaborative model training across user devices without centralizing raw data, which is important for privacy-preserving mobile text entry. Next-word prediction models must cope with heterogeneous data distributions, intermittent connectivity, and strict resource budgets. Benchmarks standardize evaluation to compare approaches under realistic constraints.\n\nWe evaluate on the StackOverflow next-word prediction dataset and the LEAF benchmark.\n\nTo handle heterogeneity, we introduce an adaptive client sampling strategy coupled with proximal regularization that stabilizes updates across diverse users while maintaining efficiency.",
    "reason": "First mentions of specific datasets/benchmarks lack citations to their sources or documentation (violates rule a).",
    "start": 401,
    "end": 486,
    "label": "Unsupported_claim"
  },
  {
    "span": "[Baker et al., 2015]",
    "document": "Introduction\n\nSyntactic priors for parsing. Classic probabilistic parsers leverage lexicalized grammars and head rules to improve attachment decisions (Collins, 1999). Neural encoders further enhance feature extraction, but the role of explicit grammar remains debated. Early evidence [Baker et al., 2015] suggests that lightly supervised constraints can regularize learning in low-data settings.\n",
    "reason": "Wrong bracket style for an author–date citation: uses square brackets. In APA-style author–date, it should be parentheses: \"(Baker et al., 2015)\".",
    "start": 285,
    "end": 305,
    "label": "Format"
  },
  {
    "span": "(Singh and Patel, 2019",
    "document": "Related Work\n\nMultimodal grounding evaluates how language aligns with visual context (Ishida and Rao, 2019; Mendes et al., 2021). Early attention models fused region proposals with sentence encoders (Campos, 2020). More recent methods learn cross-modal alignments using transformers (Fischer and Wang, 2022) and large-scale web data (Liu et al., 2023).\n\nDespite progress, evaluation often overestimates grounding due to dataset shortcuts (Harper and Singh, 2020). To mitigate this, challenge sets perturb captions while holding images constant (Ospina et al., 2021; Li and Noor, 2022), following recommendations in (Singh and Patel, 2019 for artifact control and balance.",
    "reason": "Missing closing parenthesis in the parenthetical citation.",
    "start": 615,
    "end": 637,
    "label": "Format"
  },
  {
    "span": "In (Patel et al., 2019)",
    "document": "Related Work\n\nGraph neural networks (GNNs) have been widely adopted for semi-supervised node classification (Kipf and Welling, 2017) and inductive representation learning (Hamilton et al., 2017). In (Patel et al., 2019) a fairness-aware message passing scheme is introduced to mitigate bias propagation across the graph. Subsequent work explores counterfactual augmentations to disentangle sensitive attributes (Dai and Wang, 2021), and regularization based on demographic parity constraints (Kang et al., 2020). Self-supervised pretraining via contrastive objectives improves label efficiency (Velickovic et al., 2019; You et al., 2020). Beyond homogeneous graphs, heterogeneity introduces schema-level bias, which is tackled using relation-specific normalization (Hu et al., 2020). We extend this line by incorporating group-conditional calibration during training and evaluation to align error rates across subpopulations.",
    "reason": "Wrong citation style: the preposition 'In' should not precede a parenthetical citation; use 'in Patel et al. (2019)' for a narrative citation.",
    "start": 196,
    "end": 219,
    "label": "Format"
  },
  {
    "span": "Classical link prediction models include translational distance and bilinear factorization approaches (Bordes et al., 2013; Yang et al., 2015; Trouillon et al., 2016). Recent methods leverage graph neural networks and path-based reasoning (Schlichtkrull et al., 2018; Lin et al., 2015; Xiong et al., 2017).",
    "document": "Related Work\n\nKnowledge graph completion aims to infer missing facts by modeling interactions among entities and relations. Approaches balance expressivity, scalability, and inductive bias to generalize across sparse, incomplete graphs.\n\nClassical link prediction models include translational distance and bilinear factorization approaches (Bordes et al., 2013; Yang et al., 2015; Trouillon et al., 2016). Recent methods leverage graph neural networks and path-based reasoning (Schlichtkrull et al., 2018; Lin et al., 2015; Xiong et al., 2017).\n\nWe propose a compositional scoring function that reuses relation substructures and show that it enables parameter sharing across relation patterns. Empirically, our method improves tail generalization on inductive benchmarks.",
    "reason": "The span aggregates prior categories with citations but does not explain their trade-offs, limitations, or relevance to the proposed method, hence it lacks synthesis and author perspective.",
    "start": 238,
    "end": 544,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In a previous study, the authors claim that contextual embeddings eliminate the need for feature engineering.",
    "document": "Related Work\n\nAspect-based Sentiment Analysis. ABSA has evolved from pipeline models to end-to-end neural architectures (Pontiki et al., 2014; Li et al., 2019). Target-aware attention mechanisms improved extraction (Ma et al., 2017; Fan et al., 2018). In a previous study, the authors claim that contextual embeddings eliminate the need for feature engineering. Recent models fine-tune BERT-like encoders to jointly predict aspects and sentiments (Sun et al., 2019; He et al., 2019).",
    "reason": "References an unspecified \"previous study\" and its claim without providing a citation.",
    "start": 252,
    "end": 361,
    "label": "Unsupported_claim"
  },
  {
    "span": "The M4 competition demonstrated that simple statistical ensembles outperform complex machine learning models.",
    "document": "Related Work\n\nTime series forecasting spans statistical, machine learning, and hybrid methods, with classical approaches such as exponential smoothing and ARIMA remaining competitive in many settings (Hyndman and Athanasopoulos, 2018). Global models trained across series have shown promise for cross-learning and cold-start generalization (Smyl, 2020; Salinas et al., 2020).\n\nThe M4 competition demonstrated that simple statistical ensembles outperform complex machine learning models. Subsequent efforts investigated reconciliation, cross-series features, and hierarchical structures to further boost accuracy in real-world applications.\n\nWe build on these insights by proposing a lightweight ensemble distillation method that transfers accuracy into a single deployable model.",
    "reason": "A specific claim about a competition's findings requires a citation to the competition report or analyses (violates rule a and b).",
    "start": 377,
    "end": 486,
    "label": "Unsupported_claim"
  },
  {
    "span": "((Oliveira, 2022))",
    "document": "Related Work\n\nEvaluation protocols. There is growing recognition that leaderboard metrics alone are insufficient to capture robustness and usability. Recent surveys compile best practices across domains; see ((Oliveira, 2022)) for a taxonomy of stress tests, and (Kaur et al., 2023) for human-centered evaluation frameworks. We adopt a suite of perturbation-based checks to assess stability.\n",
    "reason": "Duplicate parentheses around a single citation. It should be a single parenthetical citation: \"(Oliveira, 2022)\".",
    "start": 208,
    "end": 226,
    "label": "Format"
  },
  {
    "span": "Van der Pol and Oliehoek (2016) apply deep reinforcement learning to traffic signal control. Coogan et al. (2017) study stability in transportation networks. Multi-agent credit assignment is addressed with counterfactual baselines (Foerster et al., 2018). Queue length estimation can be improved with camera-based sensing (Li et al., 2020).",
    "document": "Related Work\n\nAdaptive traffic signal control (ATSC) aims to reduce congestion via dynamic policies that respond to real-time conditions. Recent work explores deep reinforcement learning (RL), decentralized coordination, and robust sensing to handle partial observability and nonstationarity in urban networks.\n\nVan der Pol and Oliehoek (2016) apply deep reinforcement learning to traffic signal control. Coogan et al. (2017) study stability in transportation networks. Multi-agent credit assignment is addressed with counterfactual baselines (Foerster et al., 2018). Queue length estimation can be improved with camera-based sensing (Li et al., 2020).\n\nOur approach unifies sensing and control by fusing learned queue estimates with a credit-assigned multi-agent RL objective that respects network-level constraints.",
    "reason": "The sentences cite RL for ATSC, network stability, multi-agent credit assignment, and sensing as separate points with no transitions or explanation of how each connects to the others, leading to incoherence across multiple sentences.",
    "start": 312,
    "end": 652,
    "label": "Coherence"
  },
  {
    "span": "(Kim, 2019. Park, 2020)",
    "document": "Introduction\n\nPersonalized recommendation balances accuracy with diversity and fairness to avoid echo chambers (Ziegler et al., 2005; Steck, 2018). Recent approaches incorporate exposure constraints and calibrated diversity to improve long-term user satisfaction (Abdollahpouri et al., 2020; Ekstrand et al., 2021).\n\nContext-aware models leverage temporal and session signals to capture shifting intents (Kim, 2019. Park, 2020), yet they often overlook catalog churn, leading to cold-start issues (Wang and Luo, 2021). We tackle this by coupling sequence models with item lifecycle features to stabilize exploration (Chen and Gupta, 2022).",
    "reason": "Multiple citations separated by a period inside parentheses; should use a semicolon: (Kim, 2019; Park, 2020).",
    "start": 404,
    "end": 427,
    "label": "Format"
  },
  {
    "span": "Transformer-based approaches now dominate machine translation benchmarks",
    "document": "Introduction\n\nNeural machine translation (NMT) has transitioned from recurrent architectures to attention-driven models that better capture long-range dependencies and enable efficient parallelization. Transformer-based approaches now dominate machine translation benchmarks, achieving strong performance across high-resource language pairs and demonstrating solid transfer to mid-resource settings. Yet, data sparsity and domain shift remain persistent challenges. We propose a compact adapter framework that mitigates catastrophic forgetting during domain adaptation while preserving general-domain quality.",
    "reason": "Field-wide claim about prior work/trends is made without supporting citations (rule b/d).",
    "start": 202,
    "end": 274,
    "label": "Unsupported_claim"
  },
  {
    "span": "[Klein et al., 2016)",
    "document": "Introduction\n\nNeural machine translation (NMT) models have evolved from recurrent architectures to Transformer-based systems with improved parallelism and long-range modeling (Bahdanau et al., 2015; Vaswani et al., 2017). Subword segmentation and vocabulary sharing reduce out-of-vocabulary rates and improve low-resource performance (Sennrich et al., 2016; Kudo, 2018). Decoding strategies such as beam search with length normalization and coverage penalties enhance adequacy and fluency (Wu et al., 2016; Yang et al., 2018). Prior work [Klein et al., 2016) also demonstrated the benefits of open-source toolkits for reproducibility and rapid experimentation.\n\nIn this paper, we focus on calibration and selective generation, proposing a confidence-aware reranking scheme that improves adequacy under tight latency constraints. We evaluate on WMT benchmarks and domain-shifted test sets with source-target uncertainty decomposition.",
    "reason": "Mismatched brackets combining a square opening bracket with a round closing parenthesis; the citation should use consistent parentheses or brackets, e.g., \"(Klein et al., 2016)\".",
    "start": 538,
    "end": 558,
    "label": "Format"
  },
  {
    "span": "Intrinsic motivation assigns novelty-based rewards to encourage exploration (Pathak et al., 2017). Model-based rollouts reduce sample complexity by leveraging learned dynamics (Janner et al., 2019). Offline reinforcement learning stabilizes training from logged datasets (Levine et al., 2020).",
    "document": "Introduction\n\nEfficient exploration remains a key bottleneck in reinforcement learning (RL), especially in sparse-reward environments. Curiosity-driven and count-based methods shape rewards to guide agents toward informative states (Bellemare et al., 2016; Pathak et al., 2017). Complementary research improves data efficiency using model-based planning and imaginative rollouts (Schrittwieser et al., 2020; Janner et al., 2019).\n\nIntrinsic motivation assigns novelty-based rewards to encourage exploration (Pathak et al., 2017). Model-based rollouts reduce sample complexity by leveraging learned dynamics (Janner et al., 2019). Offline reinforcement learning stabilizes training from logged datasets (Levine et al., 2020). Recent approaches combine uncertainty estimation with exploration bonuses to better balance exploitation (Ostrovski et al., 2017; Liu et al., 2021). We propose a unified objective that adaptively interpolates between intrinsic bonuses and model-based value expansions.",
    "reason": "The span lists three distinct RL threads in adjacent sentences without clarifying their relationships or transitions, making the connection between the cited works abrupt and implicit.",
    "start": 431,
    "end": 724,
    "label": "Coherence"
  },
  {
    "span": "Privacy-preserving federated learning has drawn on differential privacy (Dwork et al., 2014), secure aggregation (Bonawitz et al., 2017), homomorphic encryption (Acar et al., 2018), and split learning (Gupta and Raskar, 2018) to mitigate information leakage.",
    "document": "Introduction\n\nFederated learning. Federated learning enables collaborative model training across clients without centralizing raw data. By transmitting updates rather than examples, it aims to reduce privacy risks while leveraging distributed data heterogeneity (McMahan et al., 2017). Privacy-preserving federated learning has drawn on differential privacy (Dwork et al., 2014), secure aggregation (Bonawitz et al., 2017), homomorphic encryption (Acar et al., 2018), and split learning (Gupta and Raskar, 2018) to mitigate information leakage.\n\nSystems and heterogeneity. FL systems must contend with stragglers, intermittent connectivity, and non-iid data across devices. Approaches for client selection, compression, and personalized modeling address these challenges to varying degrees (Chen et al., 2020; Hsu et al., 2019; Li et al., 2020).\n\nAttacks and auditing. Recent studies document gradient leakage attacks and membership inference in FL settings, prompting work on auditing tools and formal guarantees to assess privacy risk under realistic threat models (Zhu et al., 2019; Nasr et al., 2019; Carlini et al., 2022).\n",
    "reason": "The sentence lists techniques without articulating which weaknesses remain or how the present work positions itself relative to them, thus lacking synthesis with the paper's aims (criteria a and c).",
    "start": 286,
    "end": 544,
    "label": "Lacks_synthesis"
  },
  {
    "span": "For molecular property prediction, numerous message-passing variants have been proposed, such as GCN (Kipf and Welling, 2017), GAT (Veličković et al., 2018), MPNN (Gilmer et al., 2017), DimeNet (Klicpera et al., 2020), SchNet (Schütt et al., 2018), and Graphormer (Ying et al., 2021), trained on datasets like QM9, ZINC, MoleculeNet benchmarks (Wu et al., 2018), and Open Graph Benchmark (Hu et al., 2020).",
    "document": "Introduction\n\nPredicting molecular properties from structure is a central task in computational chemistry that supports virtual screening and lead optimization. Despite advances in graph neural networks (GNNs), generalization across scaffolds and robustness to distribution shift remain open challenges. We investigate uncertainty-aware pretraining and calibration for property prediction in low-data regimes.\n\nFor molecular property prediction, numerous message-passing variants have been proposed, such as GCN (Kipf and Welling, 2017), GAT (Veličković et al., 2018), MPNN (Gilmer et al., 2017), DimeNet (Klicpera et al., 2020), SchNet (Schütt et al., 2018), and Graphormer (Ying et al., 2021), trained on datasets like QM9, ZINC, MoleculeNet benchmarks (Wu et al., 2018), and Open Graph Benchmark (Hu et al., 2020).\n\nIn contrast to architectural complexity, we focus on simple message passing paired with principled uncertainty estimation and temperature scaling. We evaluate scaffold split performance and calibration under resource constraints typical of early-stage discovery.",
    "reason": "The span enumerates models and datasets without explaining how they relate to the paper’s uncertainty-aware approach or what specific gap these methods leave, matching (a) and (c).",
    "start": 411,
    "end": 817,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Transformers have been introduced to medical image segmentation through hybrid CNN-Transformer backbones and pure Transformer decoders (Chen et al., 2021; Hatamizadeh et al., 2022; Cao et al., 2021). Variants leverage windowed attention, hierarchical feature pyramids, or axial attention to balance cost and capacity (Liu et al., 2021; Wang et al., 2021; Chen et al., 2022). In this paper, we propose a lightweight hybrid encoder-decoder that achieves strong performance on multi-organ CT segmentation.",
    "document": "Introduction\n\nAccurate medical image segmentation underpins diagnosis, treatment planning, and population studies. While convolutional networks dominate, long-range dependencies remain difficult to capture efficiently. Transformers promise global context modeling but often impose prohibitive memory and data requirements for clinical-scale 3D volumes.\n\nTransformers have been introduced to medical image segmentation through hybrid CNN-Transformer backbones and pure Transformer decoders (Chen et al., 2021; Hatamizadeh et al., 2022; Cao et al., 2021). Variants leverage windowed attention, hierarchical feature pyramids, or axial attention to balance cost and capacity (Liu et al., 2021; Wang et al., 2021; Chen et al., 2022). In this paper, we propose a lightweight hybrid encoder-decoder that achieves strong performance on multi-organ CT segmentation.\n\nOur approach constrains attention to topology-preserving neighborhoods learned from organ-aware priors while employing low-rank projections to reduce memory. We provide theoretical analysis of approximation error and ablations on data scales from 5% to 100%, showing robustness in low-data regimes.",
    "reason": "The span moves from a literature summary directly to stating the contribution without explicitly articulating the gap or limitation the new method addresses, thus lacking synthesis and motivation.",
    "start": 354,
    "end": 856,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The BC5CDR dataset contains 1,500 annotated abstracts covering chemicals and diseases.",
    "document": "Introduction\n\nBiomedical named entity recognition (BioNER) underpins downstream tasks such as relation extraction and knowledge base curation. While general-domain models have improved, domain adaptation is necessary to handle specialized terminology and frequent abbreviations. The BC5CDR dataset contains 1,500 annotated abstracts covering chemicals and diseases. Prior research has explored span-level pretraining, dictionary augmentation, and multi-task learning to improve recognition robustness. However, performance still degrades on out-of-corpus evaluation due to annotation schema and domain shift.",
    "reason": "Introduces a specific dataset and its statistics at first mention without providing a citation.",
    "start": 279,
    "end": 365,
    "label": "Unsupported_claim"
  },
  {
    "span": "Kipf and Welling (2017) formalized spectral graph convolution in GCNs. Hamilton et al. (2017) proposed GraphSAGE with neighborhood sampling. Veličković et al. (2018) introduced attention mechanisms in GAT. Rong et al. (2020) applied DropEdge for regularization.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become the dominant paradigm for learning on relational data. A large body of research studies architectural choices, scalability, and generalization under distribution shift.\n\nKipf and Welling (2017) formalized spectral graph convolution in GCNs. Hamilton et al. (2017) proposed GraphSAGE with neighborhood sampling. Veličković et al. (2018) introduced attention mechanisms in GAT. Rong et al. (2020) applied DropEdge for regularization.\n\nWe build on message-passing foundations but focus specifically on stability under subgraph perturbations, a dimension underexplored in the above works.",
    "reason": "The span enumerates four GNN methods without articulating their relationships, differences, or a narrative linking them, resulting in an abrupt sequence lacking explicit connections.",
    "start": 225,
    "end": 486,
    "label": "Coherence"
  },
  {
    "span": "The CHEMDNER task defines strict boundary criteria that exclude nested mentions.",
    "document": "Introduction\n\nChemical named entity recognition underpins downstream tasks such as reaction extraction and patent mining. Benchmarks vary in annotation scope, boundary rules, and handling of ambiguous nomenclature. The CHEMDNER task defines strict boundary criteria that exclude nested mentions. These choices have significant implications for model design, particularly when using span-based decoders and post-processing heuristics.\n\nWe investigate boundary sensitivity by comparing nested-capable models against flat decoders across multiple chemical corpora and present an error analysis focused on ligand complexes and salt forms.",
    "reason": "Describes a specific rule of a named shared task without citing the task description or guidelines (definition a).",
    "start": 215,
    "end": 295,
    "label": "Unsupported_claim"
  },
  {
    "span": "CLIP aligns images and text using a contrastive objective (Radford et al., 2021). AudioSet provides weakly labeled audio clips for sound event recognition (Gemmeke et al., 2017). Cross-modal transformers fuse video and language (Lei et al., 2021).",
    "document": "Related Work\n\nMultimodal Representation Learning. Aligning signals across modalities enables zero-shot transfer and robust grounding for downstream tasks spanning vision, audio, and language (Arandjelovic and Zisserman, 2017; Alayrac et al., 2020).\n\nImage–Text and Beyond. Large-scale image–text pretraining with contrastive losses has led to powerful zero-shot classifiers and flexible encoders for retrieval and captioning (Radford et al., 2021; Jia et al., 2021). CLIP aligns images and text using a contrastive objective (Radford et al., 2021). AudioSet provides weakly labeled audio clips for sound event recognition (Gemmeke et al., 2017). Cross-modal transformers fuse video and language (Lei et al., 2021). We focus on tri-modal alignment across audio, image, and text.\n\nTri-Modal Fusion. Prior work extends dual encoders with shared latent spaces and modality-specific projections, but consistent semantic grounding across three modalities remains challenging (Alayrac et al., 2020; Akbari et al., 2021). Our method introduces a curriculum for alignment that incrementally couples modalities.",
    "reason": "The span jumps from CLIP (image–text) to AudioSet (audio dataset) to video–language transformers without transitions or an explicit connection among them, making the relations unclear.",
    "start": 467,
    "end": 714,
    "label": "Coherence"
  },
  {
    "span": "Vinyals et al. (2015) presented Show and Tell with CNN–RNN captioning. Anderson et al. (2018) introduced bottom-up and top-down attention using region features. Cornia et al. (2020) applied transformer decoders for visual caption generation. Li et al. (2020) proposed OSCAR to incorporate object tags as anchor points.",
    "document": "Related Work\n\nImage captioning couples visual understanding with natural language generation. Progress has evolved from CNN–RNN pipelines to attention mechanisms and, more recently, transformer-based architectures that better exploit region semantics and multimodal alignment.\n\nVinyals et al. (2015) presented Show and Tell with CNN–RNN captioning. Anderson et al. (2018) introduced bottom-up and top-down attention using region features. Cornia et al. (2020) applied transformer decoders for visual caption generation. Li et al. (2020) proposed OSCAR to incorporate object tags as anchor points.\n\nWe extend vision-language pretraining by injecting scene-graph constraints during decoding, aiming to reduce relational hallucinations while preserving fluency.",
    "reason": "The cited works are listed sequentially without transitions or explicit comparative framing, making the connections between them unclear and abrupt (issues a and b).",
    "start": 278,
    "end": 596,
    "label": "Coherence"
  },
  {
    "span": "We evaluate on LibriSpeech, Switchboard, and TED-LIUM to assess generalization across read and conversational speech.",
    "document": "Experimental Setup\n\nOur end-to-end ASR model is trained using a hybrid CTC/attention objective with subword units. To test robustness, we consider both read and spontaneous speech conditions.\n\nWe evaluate on LibriSpeech, Switchboard, and TED-LIUM to assess generalization across read and conversational speech. Models are selected based on development set WER and hyperparameters are tuned using early stopping.",
    "reason": "First mention of established datasets lacks citations; per rule (a), datasets should be cited at first mention.",
    "start": 193,
    "end": 310,
    "label": "Unsupported_claim"
  },
  {
    "span": "ablation studies have shown that forum participation is the strongest predictor of completion",
    "document": "Related Work\n\nPredicting learner success in massive open online courses (MOOCs) is a long-standing challenge with implications for early intervention and support. Features considered include clickstream behaviors, assignment submissions, forum activity, and temporal engagement patterns.\n\nWhile many models incorporate social features, ablation studies have shown that forum participation is the strongest predictor of completion. However, forum usage is highly skewed and can be confounded with prior motivation, suggesting that raw counts may overestimate causal importance.\n\nWe introduce counterfactual feature evaluation combining propensity matching with temporal bootstrapping to assess the incremental utility of social signals for early-risk detection.",
    "reason": "Claims results from 'ablation studies' without providing citations to the specific studies (rule a and b).",
    "start": 336,
    "end": 429,
    "label": "Unsupported_claim"
  },
  {
    "span": "There has been a recent surge of works on multi-hop question generation.",
    "document": "Related Work\n\nQuestion generation (QG) seeks to automatically construct questions conditioned on text, knowledge bases, or reasoning chains. Early methods focused on rule-based transformations, while neural encoder–decoder models improved fluency and relevance by learning end-to-end mappings from passages to questions. Beyond single-sentence cues, reasoning over multiple evidence spans is crucial for producing challenging and educationally meaningful questions.\n\nThere has been a recent surge of works on multi-hop question generation. Approaches differ in how they select evidence chains, encode intermediate rationales, and enforce answerability. Despite promising results, robust evaluation remains difficult due to annotation sparsity, limited negative controls, and confounds arising from answer leakage. Our work introduces a rationale-anchored training objective that explicitly supervises hop selection and question formation.",
    "reason": "Mentions 'recent surge of works' without providing citations to any of those works, violating the requirement to cite recent literature.",
    "start": 467,
    "end": 539,
    "label": "Unsupported_claim"
  },
  {
    "span": "The SQuAD dataset contains over 100,000 question–answer pairs curated from Wikipedia.",
    "document": "Introduction\n\nMachine reading comprehension benchmarks have catalyzed rapid progress in extractive and abstractive question answering. The SQuAD dataset contains over 100,000 question–answer pairs curated from Wikipedia. Subsequent benchmarks expanded to closed-book question answering, open-domain retrieval, and multi-hop reasoning across documents. Despite strong leaderboard results, models can struggle to calibrate uncertainty and generalize beyond the training distribution. We investigate whether simple training-time regularizers and minimal data augmentation can yield gains on in-domain and out-of-domain evaluation sets without increasing model size.\n",
    "reason": "Mentions a specific dataset (SQuAD) and provides a specific statistic about its size without citing the original dataset paper.",
    "start": 135,
    "end": 220,
    "label": "Unsupported_claim"
  },
  {
    "span": "There are many recent works that explore retrieval-augmented generation for factual QA.",
    "document": "Related Work\n\nKnowledge-intensive NLP tasks such as open-domain question answering and fact verification often require models to access external information beyond their parametric memory. Early approaches focused on pipeline retrieval followed by reading comprehension, while more recent neural architectures integrate retrieval into the generation process. There are many recent works that explore retrieval-augmented generation for factual QA. In parallel, hybrid systems that combine parametric and non-parametric memory have become a common paradigm for improving factuality and reducing hallucinations. Our work builds on this line by studying how retrieval choices affect calibration and answer faithfulness.\n",
    "reason": "Claims the existence of 'many recent works' without providing citations to any of those works (missing citations for 'recent works').",
    "start": 359,
    "end": 446,
    "label": "Unsupported_claim"
  },
  {
    "span": "Lee et al., (2019)",
    "document": "Introduction\n\nEnd-to-end speech recognition replaces hand-engineered pipelines with neural acoustic and language models (Graves et al., 2014; Chan et al., 2016). Attention-based encoder–decoders and transducer models have demonstrated strong performance across languages and domains (Prabhavalkar et al., 2018; Chiu et al., 2018). Building on CTC regularization, Lee et al., (2019) introduced a hybrid objective that stabilizes alignment learning in low-resource settings, while subsequent work explored self-training with pseudo-labels (Park et al., 2020).\n\nWe investigate curriculum-based self-training with alignment-aware filtering, showing gains on noisy far-field datasets.",
    "reason": "Extraneous comma before the year in a narrative citation; should be “Lee et al. (2019)” without the comma.",
    "start": 363,
    "end": 381,
    "label": "Format"
  },
  {
    "span": "Ying et al. (2018) modeled user–item interactions with PinSage. Wang et al. (2019) incorporated knowledge graphs into collaborative filtering. He et al. (2020) proposed LightGCN to simplify message passing. Sun et al. (2020) leveraged self-supervision with contrastive objectives.",
    "document": "Related Work\n\nGraph-based recommenders extend collaborative filtering by propagating signals over user–item graphs. Recent research explores message passing, side information integration, and training objectives for sparse implicit feedback.\n\nYing et al. (2018) modeled user–item interactions with PinSage. Wang et al. (2019) incorporated knowledge graphs into collaborative filtering. He et al. (2020) proposed LightGCN to simplify message passing. Sun et al. (2020) leveraged self-supervision with contrastive objectives.\n\nIn contrast to these architectures, our method focuses on debiasing exposure while retaining the efficiency of lightweight propagation.",
    "reason": "The paragraph enumerates works with no connective tissue or explanation of how each approach relates, leading to abrupt topic shifts between citations.",
    "start": 243,
    "end": 523,
    "label": "Coherence"
  },
  {
    "span": "In a previous study, the authors claim that dependency distance predicts reading time.",
    "document": "Related Work\n\nSyntactic predictors of processing difficulty. Psycholinguistic research has examined how syntactic structure influences real-time comprehension, proposing features such as surprisal, integration cost, and memory constraints. Eye-tracking and self-paced reading paradigms have provided empirical proxies for processing load.\n\nDependency metrics. In a previous study, the authors claim that dependency distance predicts reading time. Other work investigates the role of headedness, center-embedding, and long-distance dependencies, with mixed evidence across languages and genres.\n\nNeural language models as cognitive models. Recent efforts compare surprisal estimates from language models against human reading measures, exploring how training data and architectural choices affect correlations with behavioral data.",
    "reason": "The sentence refers to a specific previous study and its claim but does not include a citation to identify the study.",
    "start": 360,
    "end": 446,
    "label": "Unsupported_claim"
  },
  {
    "span": "Demographic parity equalizes positive classification rates across groups (Dwork et al., 2012). Counterfactual fairness requires predictions to be invariant under interventions on protected attributes (Kusner et al., 2017). Actionable recourse provides feasible user-level interventions to flip model outcomes (Ustun et al., 2019).",
    "document": "Related Work\n\nFairness in machine learning has been formalized through multiple, sometimes incompatible, criteria. Statistical definitions focus on parity of predictions or errors across protected groups (Dwork et al., 2012; Hardt et al., 2016). Causal perspectives reinterpret fairness as invariance under interventions or path-specific constraints (Kusner et al., 2017; Kilbertus et al., 2017). A complementary line of work emphasizes individual-level remedies via recourse and transparency (Ustun et al., 2019; Karimi et al., 2020).\n\nDemographic parity equalizes positive classification rates across groups (Dwork et al., 2012). Counterfactual fairness requires predictions to be invariant under interventions on protected attributes (Kusner et al., 2017). Actionable recourse provides feasible user-level interventions to flip model outcomes (Ustun et al., 2019). Despite extensive theory, practical trade-offs under deployment constraints remain underexplored. We study allocation under capacity limits with fairness-utility frontiers.",
    "reason": "The span presents three different fairness notions in sequence without transitions or explanation of their relationships, leading to an abrupt and unclear connection among the cited works.",
    "start": 537,
    "end": 867,
    "label": "Coherence"
  },
  {
    "span": "A wide range of domain adaptation techniques for sentiment analysis has been proposed, including pivot-based feature selection (Blitzer et al., 2006), structural correspondence learning (Ando and Zhang, 2005), instance reweighting (Jiang and Zhai, 2007), adversarial feature alignment (Ganin and Lempitsky, 2015; Shen et al., 2018), moment matching (Tzeng et al., 2014; Long et al., 2015), self-training and pseudo-labeling (Grandvalet and Bengio, 2005; Zou et al., 2019), and contrastive representation learning (Sohn, 2020; Gunel et al., 2021).",
    "document": "Introduction\n\nCross-domain sentiment analysis remains challenging due to distribution shifts in vocabulary, writing style, and label priors across domains such as product reviews, tweets, and app feedback. While large pre-trained language models reduce annotation costs, their out-of-domain performance can still lag substantially without careful adaptation.\n\nA wide range of domain adaptation techniques for sentiment analysis has been proposed, including pivot-based feature selection (Blitzer et al., 2006), structural correspondence learning (Ando and Zhang, 2005), instance reweighting (Jiang and Zhai, 2007), adversarial feature alignment (Ganin and Lempitsky, 2015; Shen et al., 2018), moment matching (Tzeng et al., 2014; Long et al., 2015), self-training and pseudo-labeling (Grandvalet and Bengio, 2005; Zou et al., 2019), and contrastive representation learning (Sohn, 2020; Gunel et al., 2021).\n\nHowever, adaptation quality varies widely depending on label shift and the presence of domain-specific sentiment lexicons. We propose a label-prior calibrated adversarial adaptation method that explicitly estimates and corrects label shift while aligning conditional feature distributions. Our approach combines black-box prior estimation with a conditional discriminator and demonstrates robust gains across four sentiment benchmarks under synthetic and natural shifts.",
    "reason": "Only catalogs prior domain adaptation methods without stating how they are insufficient for the presented problem or how the new method differs; lacks articulation of the paper’s perspective (criterion a/c).",
    "start": 360,
    "end": 906,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The Atari 57 benchmark is the standard testbed for general Atari agents.",
    "document": "Introduction\n\nReinforcement learning (RL) has made significant strides on discrete-control problems through advances in value-based and policy-gradient methods. The Atari 57 benchmark is the standard testbed for general Atari agents. Despite progress, performance remains brittle under small visual perturbations and stochastic frame skipping. We investigate representation regularization and data augmentation schemes that improve generalization without additional environment interactions.",
    "reason": "Asserts that a particular benchmark is the 'standard testbed' without citing foundational benchmark papers or surveys (criteria a and b).",
    "start": 161,
    "end": 233,
    "label": "Unsupported_claim"
  },
  {
    "span": "Classical analytic grasp metrics and data-driven grasp detection with convolutional networks have both been extensively studied for robotic manipulation (Ferrari and Canny, 1992; Redmon and Angelova, 2015; Mahler et al., 2017; Morrison et al., 2018).",
    "document": "Introduction\n\nReliable grasping in open-world settings requires handling clutter, occlusion, and novel object geometries, while maintaining real-time inference on embedded hardware.\n\nClassical analytic grasp metrics and data-driven grasp detection with convolutional networks have both been extensively studied for robotic manipulation (Ferrari and Canny, 1992; Redmon and Angelova, 2015; Mahler et al., 2017; Morrison et al., 2018). Recent work combines grasp quality predictors with depth sensing and closed-loop control to improve success rates.\n\nWe introduce LiteGrasp, a two-stage pipeline that first prunes grasp candidates using a geometry-aware filter and then refines poses with a tiny transformer. LiteGrasp runs at 30 Hz on low-power devices and improves success in bin-picking tasks.",
    "reason": "The span recites prior lines of work without articulating how they inform or contrast with the proposed lightweight pipeline.",
    "start": 183,
    "end": 433,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The UrbanScenes-12K dataset contains exactly 12,437 annotated frames and is the de facto benchmark.",
    "document": "Related Work\n\nSemantic segmentation in urban environments has been driven by benchmarks that couple dense pixel annotations with diverse driving scenarios. Prior efforts have investigated stronger backbones, multi-scale decoding, and self-training with pseudo-labels to reduce the dependence on manual annotation. Domain adaptation and test-time adaptation have further improved transfer across cities and weather conditions.\n\nThe UrbanScenes-12K dataset contains exactly 12,437 annotated frames and is the de facto benchmark. Recent methods often report mIoU on four canonical splits and compare latency on an embedded GPU to assess deployment feasibility. However, inconsistent pre-processing and label remapping complicate cross-paper comparisons. To address these issues, we standardize the input resolution, class taxonomy, and evaluation scripts, and we release a set of reproducible training recipes.",
    "reason": "Asserts specific dataset size and benchmark status without any citation, which requires evidence per rules (a) and (b).",
    "start": 427,
    "end": 526,
    "label": "Unsupported_claim"
  },
  {
    "span": "ERA5 reanalysis",
    "document": "Related Work\n\nAccurate renewable generation forecasting depends critically on high-quality weather inputs. Numerical weather prediction models provide the basis for many forecasting pipelines, while reanalysis products offer consistent, multidecadal datasets useful for training data-driven models. The ERA5 reanalysis has become a de facto standard for wind and solar forecasting studies due to its hourly global fields and improved resolution over earlier products. Nevertheless, biases and spatiotemporal misalignments persist, prompting research on bias correction and downscaling techniques. Our study evaluates a multi-task learning approach that jointly corrects irradiance and wind fields to improve site-level forecasts.\n",
    "reason": "Mentions a specific dataset/resource without citing its source at first mention.",
    "start": 303,
    "end": 318,
    "label": "Unsupported_claim"
  },
  {
    "span": " In other words, there exists a mapping from one's personal memory to its selection of knowledge.",
    "document": "Introduction\n\nOpen-domain dialogue system often suffers from safe response (Li et al., 2015; problem as they could only refer to the context when generating a response. To alleviate this, Knowledge-grounded conversation (KGC) is proposed to introduce external fact and real-world commonsense as prior knowledge (Zhou et al., 2018a;Dinan et al., 2019;Zhao et al., 2020a), such that a dialogue system is able to ground the conversation with the provided knowledge and therefore generate informative and engaging responses. As external knowledge supplements the background to the inputs and decides what to say, knowledge selection is a key ingredient in KGC.\n\nNumerous methods have been developed to tackle the knowledge selection problem by sequential latent variables (Kim et al., 2020;Meng et al., 2020), reinforcement learning (Zhao et al., 2020b), or expectation maximization algorithm (Li et al., 2020). In spite of the progress in this task, knowledge selection remains an unsolved problem as the precision is still far from satisfactory in Wizard of Wikipedia (Dinan et al., 2019) and other benchmarks in KGC (Gopalakrishnan et al., 2019), which also hinders the optimization of subsequent response generation models. A crucial point is, they often make assumption that the golden knowledge is distinguishable as long as the dialogue context is known, yet this is not always held true because there exists a one-to-many relationship in conversation and the past utterance history in a dialogue session is insufficient to decide the knowledge selection or the future trend of a dialogue.\n\nAs is shown in Figure 1, personalization is a key to success in the task because knowledge selection is a personal or subjective process in nature. When people communicate with each other, their perception of dialogue context will evoke their past memory about relevant life experience, taste and values, which we refer to as personal memory. The aroused fragment of personal memory further guides their interest and preference for different knowledge. In other words, there exists a mapping from one's personal memory to its selection of knowledge.\n\nImporting persona memory into knowledge selection is a non-trivial task. One of the challenge is concretization of personal memory. Personal memory is an abstract concept related to user-specific experience, which is difficult to depict or model. Though it has been discussed in open-domain dialogue (Li et al., 2016;Zhang et al., 2018), no previous research sheds light on the personalization issue in KGC and there exists no dialogue dataset featured with external facts and personal memory at the same time. Besides, there is no annotated label to indicate which knowledge candidate a person will choose based on his or her personal memory. Namely, the mapping between personal memory and knowledge selection is highly unconstrained without golden label.\n\nTo address the above issue, we construct a KGC dataset featured with personalized memory repository, collecting user-specific utterance history under multiple types of context, which is a reflection of one's personal memory. And to discover the underlying relationship between the dialogue context, personal memory and knowledge, we propose a variational method and introduce two latent variables Z p and Z k to indicate the fragment of personal memory to evoke and the knowledge candidate to select respectively. And to model the mapping from Z p to Z k , we introduce an inverse mapping as a dual task and employ dual learning to allow the two mappings to teach each other. The motivation behind this is intuitive: The reconstruction of personal memory from selected knowledge candidate is natural and easy if the mapping from personal memory to knowledge is accurate. Extensive experiment shows that our methods outperform competitive baselines in both automatic evaluation and human evaluation, justifying the importance of introducing personal memory and the effect of the dual learning mechanism empirically.\n\nThe contributions of this work are three-fold:\n\n(1) We explore the personalization issue of the knowledge selection task in KGC and construct a dataset featured with user-specific personal mem-ory to benefit relevant research in the future. We are the first to explore the possibility of introducing personal memory into KGC.\n\n(2) We propose a novel variational method and introduce two latent variables to model the interdependency between the persona and knowledge. Besides, we employ dual learning to optimize the relationship between the dialogue context, personal memory and knowledge in a unified framework.\n\n(3) We conduct extensive experiments and verify the proposed methods empirically. Both the automatic and human evaluation evidence the efficacy of our proposed method.\n\n ",
    "start": 2046,
    "end": 2143,
    "label": "Unsupported_claim"
  },
  {
    "span": "see (Narayan et al., 2018)",
    "document": "Introduction\n\nAbstractive summarization seeks to generate concise paraphrases of source documents while preserving key content (See et al., 2017; Rush et al., 2015). Prior work has explored coverage mechanisms and reinforcement learning to reduce repetition and improve factuality (Paulus et al., 2018; Chen and Bansal, 2018). For long inputs, hierarchical encoders and segment-aware attention are effective; see (Narayan et al., 2018) for early evidence on extreme summarization with limited supervision. We build on these insights by introducing discourse-anchored planning before generation.",
    "reason": "Wrong citation style after a narrative cue; should be 'see Narayan et al. (2018)' rather than a parenthetical immediately after 'see'.",
    "start": 409,
    "end": 435,
    "label": "Format"
  },
  {
    "span": "Intrinsic motivation and count-based bonuses are widely used to encourage exploration in sparse-reward environments (Bellemare et al., 2016; Pathak et al., 2017; Burda et al., 2019).",
    "document": "Related Work\n\nExploration in Reinforcement Learning. Exploration strategies seek to reduce sample complexity and avoid local optima in sparse-reward tasks. Intrinsic motivation and count-based bonuses are widely used to encourage exploration in sparse-reward environments (Bellemare et al., 2016; Pathak et al., 2017; Burda et al., 2019). Other approaches use posterior uncertainty and bootstrapped value functions to guide exploration (Osband et al., 2016; O'Donoghue et al., 2018). Model-based methods leverage dynamics learning to plan informative trajectories (Chua et al., 2018; Schrittwieser et al., 2020).\n\nBenchmarking. Standard benchmarks include grid worlds, Atari, and continuous-control suites, with varying degrees of sparsity and stochasticity.\n\nWe study a policy regularization technique that stabilizes learning under noisy rewards.",
    "reason": "The span presents a list of exploration techniques with no discussion of their limitations or relationship to the proposed regularization method, lacking synthesis (definition a and c).",
    "start": 156,
    "end": 338,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The SEAME corpus is typically split by speaker identity to prevent leakage.",
    "document": "Introduction\n\nAutomatic speech recognition for code-switched speech faces unique challenges due to linguistic mixing and limited labeled resources. Prior work has examined language modeling with mixed-language subwords and adaptation strategies to handle intra-sentential switches. The SEAME corpus is typically split by speaker identity to prevent leakage. However, evaluation protocols vary widely across papers, making direct comparison difficult.\n\nWe propose a standardized evaluation for Mandarin–English code-switching with carefully controlled splits and report results across accent groups. Our analysis highlights the role of lexical coverage and pronunciation modeling in reducing errors at switch points.",
    "reason": "Claims a specific dataset practice that should be supported by a citation to the dataset guidelines or prior work (definition a and b).",
    "start": 282,
    "end": 357,
    "label": "Unsupported_claim"
  },
  {
    "span": "Transformer-XL remains the strongest baseline on character-level language modeling for enwik8.",
    "document": "Introduction\n\nCharacter-level language modeling serves as a stress test for long-range dependency modeling and efficient context utilization. While Byte Pair Encoding and other subword methods dominate downstream NLP tasks, character-level models avoid tokenization errors and can be more robust to noise.\n\nTransformer-XL remains the strongest baseline on character-level language modeling for enwik8. We propose a multi-scale state space model that complements attention with linear-time recurrence, aiming to combine long-context capacity with competitive throughput on standard benchmarks.",
    "reason": "Makes a comparative performance claim about prior work (a specific baseline on a specific dataset) without any citation (definition b).",
    "start": 307,
    "end": 401,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent work investigates large language models for tutoring, feedback generation, and grading assistance, often incorporating chain-of-thought prompting and retrieval augmentation (OpenAI, 2023; Kojima et al., 2022; Zhang et al., 2023; Liu et al., 2023).",
    "document": "Introduction\n\nLarge language models are increasingly used as educational assistants that scaffold problem-solving and provide formative feedback. Ensuring pedagogical soundness and controllability remains challenging.\n\nRecent work investigates large language models for tutoring, feedback generation, and grading assistance, often incorporating chain-of-thought prompting and retrieval augmentation (OpenAI, 2023; Kojima et al., 2022; Zhang et al., 2023; Liu et al., 2023).\n\nWe introduce a rubric-aligned critique-and-revision protocol that conditions LLM feedback on explicit learning objectives and verifies alignment via automated pedagogical rubrics.",
    "reason": "The span lists areas and techniques with citations but provides no linkage to the authors’ problem statement, gap, or perspective, reflecting a lack of synthesis.",
    "start": 219,
    "end": 473,
    "label": "Lacks_synthesis"
  }
]