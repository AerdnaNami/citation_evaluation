[
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\n",
        "completion": "[Klein et al., 2016]"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\n",
        "completion": "In (Smith et al., 2018)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\n",
        "completion": "(Davis et al.,, 2017)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\n",
        "completion": "Klein et al."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\n",
        "completion": "(Nguyen et al. 2016)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\n",
        "completion": "[see (Li, 2018]"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\n",
        "completion": "(Kumar 2020)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\n",
        "completion": "RoBERTa Liu et al., 2019)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\n",
        "completion": "Huang et al."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\n",
        "completion": "Dawson et al. 2"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\n",
        "completion": "[Miller et al., 2020]"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\nRelated Work\n\nMeta-learning aims to learn how to learn, enabling rapid adaptation from limited data (Finn et al., 2017; Nichol et al., 2018). Optimization-based methods focus on fast inner-loop updates, while metric-based methods leverage embedding spaces for nearest-neighbor decisions (Snell et al., 2017). Early advances in few-shot classification [Kumar et al., 2017) highlighted the importance of episodic training to match evaluation conditions.\n",
        "completion": "[Kumar et al., 2017)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\nRelated Work\n\nMeta-learning aims to learn how to learn, enabling rapid adaptation from limited data (Finn et al., 2017; Nichol et al., 2018). Optimization-based methods focus on fast inner-loop updates, while metric-based methods leverage embedding spaces for nearest-neighbor decisions (Snell et al., 2017). Early advances in few-shot classification [Kumar et al., 2017) highlighted the importance of episodic training to match evaluation conditions.\nIntroduction\n\nSelf-supervised objectives have narrowed the gap between supervised and unsupervised pretraining for speech recognition (Baevski et al., 2020; Hsu et al., 2021). As shown by (Nguyen, 2021), augmentations play a critical role in representation robustness, yet their interaction with model depth remains unclear (Chen et al., 2020).\n\nWe analyze augmentation schedules across depths and report consistent gains when pairing time-masking with frequency perturbations (Park et al., 2019).\n",
        "completion": "(Nguyen, 2021)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\nRelated Work\n\nMeta-learning aims to learn how to learn, enabling rapid adaptation from limited data (Finn et al., 2017; Nichol et al., 2018). Optimization-based methods focus on fast inner-loop updates, while metric-based methods leverage embedding spaces for nearest-neighbor decisions (Snell et al., 2017). Early advances in few-shot classification [Kumar et al., 2017) highlighted the importance of episodic training to match evaluation conditions.\nIntroduction\n\nSelf-supervised objectives have narrowed the gap between supervised and unsupervised pretraining for speech recognition (Baevski et al., 2020; Hsu et al., 2021). As shown by (Nguyen, 2021), augmentations play a critical role in representation robustness, yet their interaction with model depth remains unclear (Chen et al., 2020).\n\nWe analyze augmentation schedules across depths and report consistent gains when pairing time-masking with frequency perturbations (Park et al., 2019).\nRelated Work\n\nTransformer-based object detection dispenses with anchors and NMS by formulating set prediction (Carion et al., 2020; Zhu et al., 2020). Hybrid approaches integrate deformable attention and multi-scale features to improve convergence (Zhu et al., 2021; Dai et al., 2021). We also compare with attention-guided alignment (Al-Rawi and Chen 2020; which uses dynamic feature gating for small objects, and with query decoupling methods (Gao et al., 2022).\n",
        "completion": " (Al-Rawi and Chen 2020;"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\nRelated Work\n\nMeta-learning aims to learn how to learn, enabling rapid adaptation from limited data (Finn et al., 2017; Nichol et al., 2018). Optimization-based methods focus on fast inner-loop updates, while metric-based methods leverage embedding spaces for nearest-neighbor decisions (Snell et al., 2017). Early advances in few-shot classification [Kumar et al., 2017) highlighted the importance of episodic training to match evaluation conditions.\nIntroduction\n\nSelf-supervised objectives have narrowed the gap between supervised and unsupervised pretraining for speech recognition (Baevski et al., 2020; Hsu et al., 2021). As shown by (Nguyen, 2021), augmentations play a critical role in representation robustness, yet their interaction with model depth remains unclear (Chen et al., 2020).\n\nWe analyze augmentation schedules across depths and report consistent gains when pairing time-masking with frequency perturbations (Park et al., 2019).\nRelated Work\n\nTransformer-based object detection dispenses with anchors and NMS by formulating set prediction (Carion et al., 2020; Zhu et al., 2020). Hybrid approaches integrate deformable attention and multi-scale features to improve convergence (Zhu et al., 2021; Dai et al., 2021). We also compare with attention-guided alignment (Al-Rawi and Chen 2020; which uses dynamic feature gating for small objects, and with query decoupling methods (Gao et al., 2022).\nRelated Work\n\nDocument classification benchmarks fostered rapid development of neural architectures that capture long-range dependencies (Yang et al., 2016; Howard and Ruder, 2018). Iyyer et al., 2015 show that deep averaging networks can be surprisingly strong baselines, while hierarchical models exploit document structure more effectively (Tang et al., 2015; Yang et al., 2016). More recent work explores pretraining with document-level objectives to improve coherence modeling (Bao et al., 2021).\n",
        "completion": "Iyyer et al., 2015"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\nRelated Work\n\nMeta-learning aims to learn how to learn, enabling rapid adaptation from limited data (Finn et al., 2017; Nichol et al., 2018). Optimization-based methods focus on fast inner-loop updates, while metric-based methods leverage embedding spaces for nearest-neighbor decisions (Snell et al., 2017). Early advances in few-shot classification [Kumar et al., 2017) highlighted the importance of episodic training to match evaluation conditions.\nIntroduction\n\nSelf-supervised objectives have narrowed the gap between supervised and unsupervised pretraining for speech recognition (Baevski et al., 2020; Hsu et al., 2021). As shown by (Nguyen, 2021), augmentations play a critical role in representation robustness, yet their interaction with model depth remains unclear (Chen et al., 2020).\n\nWe analyze augmentation schedules across depths and report consistent gains when pairing time-masking with frequency perturbations (Park et al., 2019).\nRelated Work\n\nTransformer-based object detection dispenses with anchors and NMS by formulating set prediction (Carion et al., 2020; Zhu et al., 2020). Hybrid approaches integrate deformable attention and multi-scale features to improve convergence (Zhu et al., 2021; Dai et al., 2021). We also compare with attention-guided alignment (Al-Rawi and Chen 2020; which uses dynamic feature gating for small objects, and with query decoupling methods (Gao et al., 2022).\nRelated Work\n\nDocument classification benchmarks fostered rapid development of neural architectures that capture long-range dependencies (Yang et al., 2016; Howard and Ruder, 2018). Iyyer et al., 2015 show that deep averaging networks can be surprisingly strong baselines, while hierarchical models exploit document structure more effectively (Tang et al., 2015; Yang et al., 2016). More recent work explores pretraining with document-level objectives to improve coherence modeling (Bao et al., 2021).\nRelated Work\n\nFederated learning has been explored across mobile and edge environments to preserve data privacy while training global models (McMahan et al., 2017; Kairouz et al., 2021). In (Huang et al., 2019) introduced adaptive aggregation techniques to cope with client drift, while Karimireddy et al. (2020) proposed correction mechanisms to reduce bias from partial participation. Personalization in federated settings exploits clustering and meta-learning (Mansour et al., 2020; Fallah et al., 2020), and robust aggregation mitigates outliers and attacks (Blanchard et al., 2017; Pillutla et al., 2022). Our work studies cross-silo scenarios with non-IID labels and temporal participation patterns.\n",
        "completion": "In (Huang et al., 2019)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\nRelated Work\n\nMeta-learning aims to learn how to learn, enabling rapid adaptation from limited data (Finn et al., 2017; Nichol et al., 2018). Optimization-based methods focus on fast inner-loop updates, while metric-based methods leverage embedding spaces for nearest-neighbor decisions (Snell et al., 2017). Early advances in few-shot classification [Kumar et al., 2017) highlighted the importance of episodic training to match evaluation conditions.\nIntroduction\n\nSelf-supervised objectives have narrowed the gap between supervised and unsupervised pretraining for speech recognition (Baevski et al., 2020; Hsu et al., 2021). As shown by (Nguyen, 2021), augmentations play a critical role in representation robustness, yet their interaction with model depth remains unclear (Chen et al., 2020).\n\nWe analyze augmentation schedules across depths and report consistent gains when pairing time-masking with frequency perturbations (Park et al., 2019).\nRelated Work\n\nTransformer-based object detection dispenses with anchors and NMS by formulating set prediction (Carion et al., 2020; Zhu et al., 2020). Hybrid approaches integrate deformable attention and multi-scale features to improve convergence (Zhu et al., 2021; Dai et al., 2021). We also compare with attention-guided alignment (Al-Rawi and Chen 2020; which uses dynamic feature gating for small objects, and with query decoupling methods (Gao et al., 2022).\nRelated Work\n\nDocument classification benchmarks fostered rapid development of neural architectures that capture long-range dependencies (Yang et al., 2016; Howard and Ruder, 2018). Iyyer et al., 2015 show that deep averaging networks can be surprisingly strong baselines, while hierarchical models exploit document structure more effectively (Tang et al., 2015; Yang et al., 2016). More recent work explores pretraining with document-level objectives to improve coherence modeling (Bao et al., 2021).\nRelated Work\n\nFederated learning has been explored across mobile and edge environments to preserve data privacy while training global models (McMahan et al., 2017; Kairouz et al., 2021). In (Huang et al., 2019) introduced adaptive aggregation techniques to cope with client drift, while Karimireddy et al. (2020) proposed correction mechanisms to reduce bias from partial participation. Personalization in federated settings exploits clustering and meta-learning (Mansour et al., 2020; Fallah et al., 2020), and robust aggregation mitigates outliers and attacks (Blanchard et al., 2017; Pillutla et al., 2022). Our work studies cross-silo scenarios with non-IID labels and temporal participation patterns.\nIntroduction\n\nDeep reinforcement learning (RL) has achieved strong results in control and games by combining function approximation with policy optimization and value learning (Mnih et al., 2015; Schulman et al., 2017; Haarnoja et al., 2018). Exploration remains a central challenge, with intrinsic motivation and optimism-based methods proposed to address sparse rewards (Bellemare et al., 2016; Pathak et al., 2017).\n\nModel-based RL promises improved sample efficiency by leveraging learned dynamics, but stability and compounding errors can hinder performance (Janner et al., 2019). Prior work has examined uncertainty-aware planning and ensembles to mitigate model bias (Chua et al., 2018). In continuous control, natural policy gradients and trust-region methods have improved stability (Kakade, 2002; Schulman et al., 2015). We build on off-policy actor\u2013critic techniques (Lillicrap et al., 2016) and extend distributional critics to better capture return uncertainty (Bellemare et al., 2017; Dabney et al., 2018), improving the robustness of value targets first explored in (Alvarez et al. 2016).\n",
        "completion": "(Alvarez et al. 2016)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\nRelated Work\n\nMeta-learning aims to learn how to learn, enabling rapid adaptation from limited data (Finn et al., 2017; Nichol et al., 2018). Optimization-based methods focus on fast inner-loop updates, while metric-based methods leverage embedding spaces for nearest-neighbor decisions (Snell et al., 2017). Early advances in few-shot classification [Kumar et al., 2017) highlighted the importance of episodic training to match evaluation conditions.\nIntroduction\n\nSelf-supervised objectives have narrowed the gap between supervised and unsupervised pretraining for speech recognition (Baevski et al., 2020; Hsu et al., 2021). As shown by (Nguyen, 2021), augmentations play a critical role in representation robustness, yet their interaction with model depth remains unclear (Chen et al., 2020).\n\nWe analyze augmentation schedules across depths and report consistent gains when pairing time-masking with frequency perturbations (Park et al., 2019).\nRelated Work\n\nTransformer-based object detection dispenses with anchors and NMS by formulating set prediction (Carion et al., 2020; Zhu et al., 2020). Hybrid approaches integrate deformable attention and multi-scale features to improve convergence (Zhu et al., 2021; Dai et al., 2021). We also compare with attention-guided alignment (Al-Rawi and Chen 2020; which uses dynamic feature gating for small objects, and with query decoupling methods (Gao et al., 2022).\nRelated Work\n\nDocument classification benchmarks fostered rapid development of neural architectures that capture long-range dependencies (Yang et al., 2016; Howard and Ruder, 2018). Iyyer et al., 2015 show that deep averaging networks can be surprisingly strong baselines, while hierarchical models exploit document structure more effectively (Tang et al., 2015; Yang et al., 2016). More recent work explores pretraining with document-level objectives to improve coherence modeling (Bao et al., 2021).\nRelated Work\n\nFederated learning has been explored across mobile and edge environments to preserve data privacy while training global models (McMahan et al., 2017; Kairouz et al., 2021). In (Huang et al., 2019) introduced adaptive aggregation techniques to cope with client drift, while Karimireddy et al. (2020) proposed correction mechanisms to reduce bias from partial participation. Personalization in federated settings exploits clustering and meta-learning (Mansour et al., 2020; Fallah et al., 2020), and robust aggregation mitigates outliers and attacks (Blanchard et al., 2017; Pillutla et al., 2022). Our work studies cross-silo scenarios with non-IID labels and temporal participation patterns.\nIntroduction\n\nDeep reinforcement learning (RL) has achieved strong results in control and games by combining function approximation with policy optimization and value learning (Mnih et al., 2015; Schulman et al., 2017; Haarnoja et al., 2018). Exploration remains a central challenge, with intrinsic motivation and optimism-based methods proposed to address sparse rewards (Bellemare et al., 2016; Pathak et al., 2017).\n\nModel-based RL promises improved sample efficiency by leveraging learned dynamics, but stability and compounding errors can hinder performance (Janner et al., 2019). Prior work has examined uncertainty-aware planning and ensembles to mitigate model bias (Chua et al., 2018). In continuous control, natural policy gradients and trust-region methods have improved stability (Kakade, 2002; Schulman et al., 2015). We build on off-policy actor\u2013critic techniques (Lillicrap et al., 2016) and extend distributional critics to better capture return uncertainty (Bellemare et al., 2017; Dabney et al., 2018), improving the robustness of value targets first explored in (Alvarez et al. 2016).\nIntroduction\n\nNeural sequence modeling has rapidly evolved, driven by advances in attention mechanisms and large-scale pretraining. According to Chen et al., attention enables models to dynamically focus on salient parts of the input while suppressing noise from irrelevant tokens. This intuition has been validated across machine translation, summarization, and dialogue (Alvarez and Kim, 2018; Singh et al., 2020). Yet, despite these successes, there remains a gap in understanding how structural biases influence generalization when data is scarce (Rao and Patel, 2019; Gomez et al., 2021).\n\nIn this paper, we study the interplay between architectural inductive biases and data augmentation strategies for low-resource settings. Prior work (Huang and Lee, 2017; Park et al., 2022) suggests that curated augmentations can substitute for explicit structural priors. We revisit this claim using controlled synthetic corpora alongside real-world benchmarks, and we reveal when augmentations help, when they hurt, and why.\n\nOur contributions are: (i) a taxonomy of inductive biases relevant to seq2seq learning; (ii) a suite of controlled experiments across tasks; and (iii) an analysis framework that links augmentation choice to linguistic properties of the data (Alvarez and Kim, 2018; Singh et al., 2020).\n",
        "completion": "Chen et al."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\nRelated Work\n\nMeta-learning aims to learn how to learn, enabling rapid adaptation from limited data (Finn et al., 2017; Nichol et al., 2018). Optimization-based methods focus on fast inner-loop updates, while metric-based methods leverage embedding spaces for nearest-neighbor decisions (Snell et al., 2017). Early advances in few-shot classification [Kumar et al., 2017) highlighted the importance of episodic training to match evaluation conditions.\nIntroduction\n\nSelf-supervised objectives have narrowed the gap between supervised and unsupervised pretraining for speech recognition (Baevski et al., 2020; Hsu et al., 2021). As shown by (Nguyen, 2021), augmentations play a critical role in representation robustness, yet their interaction with model depth remains unclear (Chen et al., 2020).\n\nWe analyze augmentation schedules across depths and report consistent gains when pairing time-masking with frequency perturbations (Park et al., 2019).\nRelated Work\n\nTransformer-based object detection dispenses with anchors and NMS by formulating set prediction (Carion et al., 2020; Zhu et al., 2020). Hybrid approaches integrate deformable attention and multi-scale features to improve convergence (Zhu et al., 2021; Dai et al., 2021). We also compare with attention-guided alignment (Al-Rawi and Chen 2020; which uses dynamic feature gating for small objects, and with query decoupling methods (Gao et al., 2022).\nRelated Work\n\nDocument classification benchmarks fostered rapid development of neural architectures that capture long-range dependencies (Yang et al., 2016; Howard and Ruder, 2018). Iyyer et al., 2015 show that deep averaging networks can be surprisingly strong baselines, while hierarchical models exploit document structure more effectively (Tang et al., 2015; Yang et al., 2016). More recent work explores pretraining with document-level objectives to improve coherence modeling (Bao et al., 2021).\nRelated Work\n\nFederated learning has been explored across mobile and edge environments to preserve data privacy while training global models (McMahan et al., 2017; Kairouz et al., 2021). In (Huang et al., 2019) introduced adaptive aggregation techniques to cope with client drift, while Karimireddy et al. (2020) proposed correction mechanisms to reduce bias from partial participation. Personalization in federated settings exploits clustering and meta-learning (Mansour et al., 2020; Fallah et al., 2020), and robust aggregation mitigates outliers and attacks (Blanchard et al., 2017; Pillutla et al., 2022). Our work studies cross-silo scenarios with non-IID labels and temporal participation patterns.\nIntroduction\n\nDeep reinforcement learning (RL) has achieved strong results in control and games by combining function approximation with policy optimization and value learning (Mnih et al., 2015; Schulman et al., 2017; Haarnoja et al., 2018). Exploration remains a central challenge, with intrinsic motivation and optimism-based methods proposed to address sparse rewards (Bellemare et al., 2016; Pathak et al., 2017).\n\nModel-based RL promises improved sample efficiency by leveraging learned dynamics, but stability and compounding errors can hinder performance (Janner et al., 2019). Prior work has examined uncertainty-aware planning and ensembles to mitigate model bias (Chua et al., 2018). In continuous control, natural policy gradients and trust-region methods have improved stability (Kakade, 2002; Schulman et al., 2015). We build on off-policy actor\u2013critic techniques (Lillicrap et al., 2016) and extend distributional critics to better capture return uncertainty (Bellemare et al., 2017; Dabney et al., 2018), improving the robustness of value targets first explored in (Alvarez et al. 2016).\nIntroduction\n\nNeural sequence modeling has rapidly evolved, driven by advances in attention mechanisms and large-scale pretraining. According to Chen et al., attention enables models to dynamically focus on salient parts of the input while suppressing noise from irrelevant tokens. This intuition has been validated across machine translation, summarization, and dialogue (Alvarez and Kim, 2018; Singh et al., 2020). Yet, despite these successes, there remains a gap in understanding how structural biases influence generalization when data is scarce (Rao and Patel, 2019; Gomez et al., 2021).\n\nIn this paper, we study the interplay between architectural inductive biases and data augmentation strategies for low-resource settings. Prior work (Huang and Lee, 2017; Park et al., 2022) suggests that curated augmentations can substitute for explicit structural priors. We revisit this claim using controlled synthetic corpora alongside real-world benchmarks, and we reveal when augmentations help, when they hurt, and why.\n\nOur contributions are: (i) a taxonomy of inductive biases relevant to seq2seq learning; (ii) a suite of controlled experiments across tasks; and (iii) an analysis framework that links augmentation choice to linguistic properties of the data (Alvarez and Kim, 2018; Singh et al., 2020).\nRelated Work\n\nLearning-to-rank methods combine handcrafted features with deep encoders for end-to-end optimization. Pairwise losses have been extended with listwise surrogates to better approximate ranking metrics (Cao et al., 2007; Bruch et al., 2020). Similarly, Brown & Lee, 2020 demonstrate that learned query reformulations can improve passage retrieval, complementing dense retrievers that leverage dual encoders (Karpukhin et al., 2020; Xiong et al., 2021).\n",
        "completion": "Brown & Lee, 2020"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\nRelated Work\n\nMeta-learning aims to learn how to learn, enabling rapid adaptation from limited data (Finn et al., 2017; Nichol et al., 2018). Optimization-based methods focus on fast inner-loop updates, while metric-based methods leverage embedding spaces for nearest-neighbor decisions (Snell et al., 2017). Early advances in few-shot classification [Kumar et al., 2017) highlighted the importance of episodic training to match evaluation conditions.\nIntroduction\n\nSelf-supervised objectives have narrowed the gap between supervised and unsupervised pretraining for speech recognition (Baevski et al., 2020; Hsu et al., 2021). As shown by (Nguyen, 2021), augmentations play a critical role in representation robustness, yet their interaction with model depth remains unclear (Chen et al., 2020).\n\nWe analyze augmentation schedules across depths and report consistent gains when pairing time-masking with frequency perturbations (Park et al., 2019).\nRelated Work\n\nTransformer-based object detection dispenses with anchors and NMS by formulating set prediction (Carion et al., 2020; Zhu et al., 2020). Hybrid approaches integrate deformable attention and multi-scale features to improve convergence (Zhu et al., 2021; Dai et al., 2021). We also compare with attention-guided alignment (Al-Rawi and Chen 2020; which uses dynamic feature gating for small objects, and with query decoupling methods (Gao et al., 2022).\nRelated Work\n\nDocument classification benchmarks fostered rapid development of neural architectures that capture long-range dependencies (Yang et al., 2016; Howard and Ruder, 2018). Iyyer et al., 2015 show that deep averaging networks can be surprisingly strong baselines, while hierarchical models exploit document structure more effectively (Tang et al., 2015; Yang et al., 2016). More recent work explores pretraining with document-level objectives to improve coherence modeling (Bao et al., 2021).\nRelated Work\n\nFederated learning has been explored across mobile and edge environments to preserve data privacy while training global models (McMahan et al., 2017; Kairouz et al., 2021). In (Huang et al., 2019) introduced adaptive aggregation techniques to cope with client drift, while Karimireddy et al. (2020) proposed correction mechanisms to reduce bias from partial participation. Personalization in federated settings exploits clustering and meta-learning (Mansour et al., 2020; Fallah et al., 2020), and robust aggregation mitigates outliers and attacks (Blanchard et al., 2017; Pillutla et al., 2022). Our work studies cross-silo scenarios with non-IID labels and temporal participation patterns.\nIntroduction\n\nDeep reinforcement learning (RL) has achieved strong results in control and games by combining function approximation with policy optimization and value learning (Mnih et al., 2015; Schulman et al., 2017; Haarnoja et al., 2018). Exploration remains a central challenge, with intrinsic motivation and optimism-based methods proposed to address sparse rewards (Bellemare et al., 2016; Pathak et al., 2017).\n\nModel-based RL promises improved sample efficiency by leveraging learned dynamics, but stability and compounding errors can hinder performance (Janner et al., 2019). Prior work has examined uncertainty-aware planning and ensembles to mitigate model bias (Chua et al., 2018). In continuous control, natural policy gradients and trust-region methods have improved stability (Kakade, 2002; Schulman et al., 2015). We build on off-policy actor\u2013critic techniques (Lillicrap et al., 2016) and extend distributional critics to better capture return uncertainty (Bellemare et al., 2017; Dabney et al., 2018), improving the robustness of value targets first explored in (Alvarez et al. 2016).\nIntroduction\n\nNeural sequence modeling has rapidly evolved, driven by advances in attention mechanisms and large-scale pretraining. According to Chen et al., attention enables models to dynamically focus on salient parts of the input while suppressing noise from irrelevant tokens. This intuition has been validated across machine translation, summarization, and dialogue (Alvarez and Kim, 2018; Singh et al., 2020). Yet, despite these successes, there remains a gap in understanding how structural biases influence generalization when data is scarce (Rao and Patel, 2019; Gomez et al., 2021).\n\nIn this paper, we study the interplay between architectural inductive biases and data augmentation strategies for low-resource settings. Prior work (Huang and Lee, 2017; Park et al., 2022) suggests that curated augmentations can substitute for explicit structural priors. We revisit this claim using controlled synthetic corpora alongside real-world benchmarks, and we reveal when augmentations help, when they hurt, and why.\n\nOur contributions are: (i) a taxonomy of inductive biases relevant to seq2seq learning; (ii) a suite of controlled experiments across tasks; and (iii) an analysis framework that links augmentation choice to linguistic properties of the data (Alvarez and Kim, 2018; Singh et al., 2020).\nRelated Work\n\nLearning-to-rank methods combine handcrafted features with deep encoders for end-to-end optimization. Pairwise losses have been extended with listwise surrogates to better approximate ranking metrics (Cao et al., 2007; Bruch et al., 2020). Similarly, Brown & Lee, 2020 demonstrate that learned query reformulations can improve passage retrieval, complementing dense retrievers that leverage dual encoders (Karpukhin et al., 2020; Xiong et al., 2021).\nRelated Work\n\nMultimodal representation learning has advanced rapidly with contrastive pretraining at scale. Large vision\u2013language models align image and text embeddings using noise-contrastive losses (Radford et al., 2021; Jia et al., 2021). Extensions incorporate region-level grounding and fine-grained alignment signals (Li et al., 2020; Chen et al., 2020). Zero-shot transfer emerges naturally from these frameworks, enabling strong performance without task-specific fine-tuning (Tsimpoukelli et al., 2021).\n\nDespite impressive results, models often rely on superficial correlations, underperforming on compositional and counterfactual evaluations (Hudson and Manning, 2019; Parcalabescu et al., 2021). Fowler et al (2022) introduce a suite of structured probes to assess whether models capture multi-step relational reasoning, revealing persistent gaps in compositionality. Our work complements these diagnostics with causal interventions that isolate modality-specific shortcuts.\n",
        "completion": "Fowler et al (2022)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\nRelated Work\n\nMeta-learning aims to learn how to learn, enabling rapid adaptation from limited data (Finn et al., 2017; Nichol et al., 2018). Optimization-based methods focus on fast inner-loop updates, while metric-based methods leverage embedding spaces for nearest-neighbor decisions (Snell et al., 2017). Early advances in few-shot classification [Kumar et al., 2017) highlighted the importance of episodic training to match evaluation conditions.\nIntroduction\n\nSelf-supervised objectives have narrowed the gap between supervised and unsupervised pretraining for speech recognition (Baevski et al., 2020; Hsu et al., 2021). As shown by (Nguyen, 2021), augmentations play a critical role in representation robustness, yet their interaction with model depth remains unclear (Chen et al., 2020).\n\nWe analyze augmentation schedules across depths and report consistent gains when pairing time-masking with frequency perturbations (Park et al., 2019).\nRelated Work\n\nTransformer-based object detection dispenses with anchors and NMS by formulating set prediction (Carion et al., 2020; Zhu et al., 2020). Hybrid approaches integrate deformable attention and multi-scale features to improve convergence (Zhu et al., 2021; Dai et al., 2021). We also compare with attention-guided alignment (Al-Rawi and Chen 2020; which uses dynamic feature gating for small objects, and with query decoupling methods (Gao et al., 2022).\nRelated Work\n\nDocument classification benchmarks fostered rapid development of neural architectures that capture long-range dependencies (Yang et al., 2016; Howard and Ruder, 2018). Iyyer et al., 2015 show that deep averaging networks can be surprisingly strong baselines, while hierarchical models exploit document structure more effectively (Tang et al., 2015; Yang et al., 2016). More recent work explores pretraining with document-level objectives to improve coherence modeling (Bao et al., 2021).\nRelated Work\n\nFederated learning has been explored across mobile and edge environments to preserve data privacy while training global models (McMahan et al., 2017; Kairouz et al., 2021). In (Huang et al., 2019) introduced adaptive aggregation techniques to cope with client drift, while Karimireddy et al. (2020) proposed correction mechanisms to reduce bias from partial participation. Personalization in federated settings exploits clustering and meta-learning (Mansour et al., 2020; Fallah et al., 2020), and robust aggregation mitigates outliers and attacks (Blanchard et al., 2017; Pillutla et al., 2022). Our work studies cross-silo scenarios with non-IID labels and temporal participation patterns.\nIntroduction\n\nDeep reinforcement learning (RL) has achieved strong results in control and games by combining function approximation with policy optimization and value learning (Mnih et al., 2015; Schulman et al., 2017; Haarnoja et al., 2018). Exploration remains a central challenge, with intrinsic motivation and optimism-based methods proposed to address sparse rewards (Bellemare et al., 2016; Pathak et al., 2017).\n\nModel-based RL promises improved sample efficiency by leveraging learned dynamics, but stability and compounding errors can hinder performance (Janner et al., 2019). Prior work has examined uncertainty-aware planning and ensembles to mitigate model bias (Chua et al., 2018). In continuous control, natural policy gradients and trust-region methods have improved stability (Kakade, 2002; Schulman et al., 2015). We build on off-policy actor\u2013critic techniques (Lillicrap et al., 2016) and extend distributional critics to better capture return uncertainty (Bellemare et al., 2017; Dabney et al., 2018), improving the robustness of value targets first explored in (Alvarez et al. 2016).\nIntroduction\n\nNeural sequence modeling has rapidly evolved, driven by advances in attention mechanisms and large-scale pretraining. According to Chen et al., attention enables models to dynamically focus on salient parts of the input while suppressing noise from irrelevant tokens. This intuition has been validated across machine translation, summarization, and dialogue (Alvarez and Kim, 2018; Singh et al., 2020). Yet, despite these successes, there remains a gap in understanding how structural biases influence generalization when data is scarce (Rao and Patel, 2019; Gomez et al., 2021).\n\nIn this paper, we study the interplay between architectural inductive biases and data augmentation strategies for low-resource settings. Prior work (Huang and Lee, 2017; Park et al., 2022) suggests that curated augmentations can substitute for explicit structural priors. We revisit this claim using controlled synthetic corpora alongside real-world benchmarks, and we reveal when augmentations help, when they hurt, and why.\n\nOur contributions are: (i) a taxonomy of inductive biases relevant to seq2seq learning; (ii) a suite of controlled experiments across tasks; and (iii) an analysis framework that links augmentation choice to linguistic properties of the data (Alvarez and Kim, 2018; Singh et al., 2020).\nRelated Work\n\nLearning-to-rank methods combine handcrafted features with deep encoders for end-to-end optimization. Pairwise losses have been extended with listwise surrogates to better approximate ranking metrics (Cao et al., 2007; Bruch et al., 2020). Similarly, Brown & Lee, 2020 demonstrate that learned query reformulations can improve passage retrieval, complementing dense retrievers that leverage dual encoders (Karpukhin et al., 2020; Xiong et al., 2021).\nRelated Work\n\nMultimodal representation learning has advanced rapidly with contrastive pretraining at scale. Large vision\u2013language models align image and text embeddings using noise-contrastive losses (Radford et al., 2021; Jia et al., 2021). Extensions incorporate region-level grounding and fine-grained alignment signals (Li et al., 2020; Chen et al., 2020). Zero-shot transfer emerges naturally from these frameworks, enabling strong performance without task-specific fine-tuning (Tsimpoukelli et al., 2021).\n\nDespite impressive results, models often rely on superficial correlations, underperforming on compositional and counterfactual evaluations (Hudson and Manning, 2019; Parcalabescu et al., 2021). Fowler et al (2022) introduce a suite of structured probes to assess whether models capture multi-step relational reasoning, revealing persistent gaps in compositionality. Our work complements these diagnostics with causal interventions that isolate modality-specific shortcuts.\nIntroduction\n\nSafe reinforcement learning studies how to optimize long-term reward while satisfying safety constraints (Garc\u00eda and Fern\u00e1ndez, 2015; Achiam et al., 2017). Garcia et al. 3 introduced a shielded policy optimization framework that enforces constraints via learned barrier functions, complementing primal-dual methods (Chow et al., 2018). Subsequent work explores offline datasets for safe exploration (Buckman et al., 2020) and risk-sensitive objectives (Tamar et al., 2015). We propose an uncertainty-aware shield that adapts to nonstationary risks.\n",
        "completion": "Garcia et al. 3"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\nRelated Work\n\nMeta-learning aims to learn how to learn, enabling rapid adaptation from limited data (Finn et al., 2017; Nichol et al., 2018). Optimization-based methods focus on fast inner-loop updates, while metric-based methods leverage embedding spaces for nearest-neighbor decisions (Snell et al., 2017). Early advances in few-shot classification [Kumar et al., 2017) highlighted the importance of episodic training to match evaluation conditions.\nIntroduction\n\nSelf-supervised objectives have narrowed the gap between supervised and unsupervised pretraining for speech recognition (Baevski et al., 2020; Hsu et al., 2021). As shown by (Nguyen, 2021), augmentations play a critical role in representation robustness, yet their interaction with model depth remains unclear (Chen et al., 2020).\n\nWe analyze augmentation schedules across depths and report consistent gains when pairing time-masking with frequency perturbations (Park et al., 2019).\nRelated Work\n\nTransformer-based object detection dispenses with anchors and NMS by formulating set prediction (Carion et al., 2020; Zhu et al., 2020). Hybrid approaches integrate deformable attention and multi-scale features to improve convergence (Zhu et al., 2021; Dai et al., 2021). We also compare with attention-guided alignment (Al-Rawi and Chen 2020; which uses dynamic feature gating for small objects, and with query decoupling methods (Gao et al., 2022).\nRelated Work\n\nDocument classification benchmarks fostered rapid development of neural architectures that capture long-range dependencies (Yang et al., 2016; Howard and Ruder, 2018). Iyyer et al., 2015 show that deep averaging networks can be surprisingly strong baselines, while hierarchical models exploit document structure more effectively (Tang et al., 2015; Yang et al., 2016). More recent work explores pretraining with document-level objectives to improve coherence modeling (Bao et al., 2021).\nRelated Work\n\nFederated learning has been explored across mobile and edge environments to preserve data privacy while training global models (McMahan et al., 2017; Kairouz et al., 2021). In (Huang et al., 2019) introduced adaptive aggregation techniques to cope with client drift, while Karimireddy et al. (2020) proposed correction mechanisms to reduce bias from partial participation. Personalization in federated settings exploits clustering and meta-learning (Mansour et al., 2020; Fallah et al., 2020), and robust aggregation mitigates outliers and attacks (Blanchard et al., 2017; Pillutla et al., 2022). Our work studies cross-silo scenarios with non-IID labels and temporal participation patterns.\nIntroduction\n\nDeep reinforcement learning (RL) has achieved strong results in control and games by combining function approximation with policy optimization and value learning (Mnih et al., 2015; Schulman et al., 2017; Haarnoja et al., 2018). Exploration remains a central challenge, with intrinsic motivation and optimism-based methods proposed to address sparse rewards (Bellemare et al., 2016; Pathak et al., 2017).\n\nModel-based RL promises improved sample efficiency by leveraging learned dynamics, but stability and compounding errors can hinder performance (Janner et al., 2019). Prior work has examined uncertainty-aware planning and ensembles to mitigate model bias (Chua et al., 2018). In continuous control, natural policy gradients and trust-region methods have improved stability (Kakade, 2002; Schulman et al., 2015). We build on off-policy actor\u2013critic techniques (Lillicrap et al., 2016) and extend distributional critics to better capture return uncertainty (Bellemare et al., 2017; Dabney et al., 2018), improving the robustness of value targets first explored in (Alvarez et al. 2016).\nIntroduction\n\nNeural sequence modeling has rapidly evolved, driven by advances in attention mechanisms and large-scale pretraining. According to Chen et al., attention enables models to dynamically focus on salient parts of the input while suppressing noise from irrelevant tokens. This intuition has been validated across machine translation, summarization, and dialogue (Alvarez and Kim, 2018; Singh et al., 2020). Yet, despite these successes, there remains a gap in understanding how structural biases influence generalization when data is scarce (Rao and Patel, 2019; Gomez et al., 2021).\n\nIn this paper, we study the interplay between architectural inductive biases and data augmentation strategies for low-resource settings. Prior work (Huang and Lee, 2017; Park et al., 2022) suggests that curated augmentations can substitute for explicit structural priors. We revisit this claim using controlled synthetic corpora alongside real-world benchmarks, and we reveal when augmentations help, when they hurt, and why.\n\nOur contributions are: (i) a taxonomy of inductive biases relevant to seq2seq learning; (ii) a suite of controlled experiments across tasks; and (iii) an analysis framework that links augmentation choice to linguistic properties of the data (Alvarez and Kim, 2018; Singh et al., 2020).\nRelated Work\n\nLearning-to-rank methods combine handcrafted features with deep encoders for end-to-end optimization. Pairwise losses have been extended with listwise surrogates to better approximate ranking metrics (Cao et al., 2007; Bruch et al., 2020). Similarly, Brown & Lee, 2020 demonstrate that learned query reformulations can improve passage retrieval, complementing dense retrievers that leverage dual encoders (Karpukhin et al., 2020; Xiong et al., 2021).\nRelated Work\n\nMultimodal representation learning has advanced rapidly with contrastive pretraining at scale. Large vision\u2013language models align image and text embeddings using noise-contrastive losses (Radford et al., 2021; Jia et al., 2021). Extensions incorporate region-level grounding and fine-grained alignment signals (Li et al., 2020; Chen et al., 2020). Zero-shot transfer emerges naturally from these frameworks, enabling strong performance without task-specific fine-tuning (Tsimpoukelli et al., 2021).\n\nDespite impressive results, models often rely on superficial correlations, underperforming on compositional and counterfactual evaluations (Hudson and Manning, 2019; Parcalabescu et al., 2021). Fowler et al (2022) introduce a suite of structured probes to assess whether models capture multi-step relational reasoning, revealing persistent gaps in compositionality. Our work complements these diagnostics with causal interventions that isolate modality-specific shortcuts.\nIntroduction\n\nSafe reinforcement learning studies how to optimize long-term reward while satisfying safety constraints (Garc\u00eda and Fern\u00e1ndez, 2015; Achiam et al., 2017). Garcia et al. 3 introduced a shielded policy optimization framework that enforces constraints via learned barrier functions, complementing primal-dual methods (Chow et al., 2018). Subsequent work explores offline datasets for safe exploration (Buckman et al., 2020) and risk-sensitive objectives (Tamar et al., 2015). We propose an uncertainty-aware shield that adapts to nonstationary risks.\nRelated Work\n\nPrivacy-preserving analytics balance utility with protections against membership inference and attribute disclosure (Shokri et al., 2017; Dwork et al., 2016). Several surveys exist (O'Neil et al.), but most focus on static datasets rather than streaming telemetry (Gao and Li, 2021). Recent work adapts differential privacy to continual observation via event-level budgets (Fan et al., 2022; Xu and Wang, 2023). We extend these ideas with a scheduler that dynamically allocates privacy loss to high-variance segments to maximize downstream accuracy.\n\nIntroduction\n\nWe evaluate across three telemetry corpora, reporting both utility metrics and calibrated privacy guarantees.\n",
        "completion": "(O'Neil et al.)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\nRelated Work\n\nMeta-learning aims to learn how to learn, enabling rapid adaptation from limited data (Finn et al., 2017; Nichol et al., 2018). Optimization-based methods focus on fast inner-loop updates, while metric-based methods leverage embedding spaces for nearest-neighbor decisions (Snell et al., 2017). Early advances in few-shot classification [Kumar et al., 2017) highlighted the importance of episodic training to match evaluation conditions.\nIntroduction\n\nSelf-supervised objectives have narrowed the gap between supervised and unsupervised pretraining for speech recognition (Baevski et al., 2020; Hsu et al., 2021). As shown by (Nguyen, 2021), augmentations play a critical role in representation robustness, yet their interaction with model depth remains unclear (Chen et al., 2020).\n\nWe analyze augmentation schedules across depths and report consistent gains when pairing time-masking with frequency perturbations (Park et al., 2019).\nRelated Work\n\nTransformer-based object detection dispenses with anchors and NMS by formulating set prediction (Carion et al., 2020; Zhu et al., 2020). Hybrid approaches integrate deformable attention and multi-scale features to improve convergence (Zhu et al., 2021; Dai et al., 2021). We also compare with attention-guided alignment (Al-Rawi and Chen 2020; which uses dynamic feature gating for small objects, and with query decoupling methods (Gao et al., 2022).\nRelated Work\n\nDocument classification benchmarks fostered rapid development of neural architectures that capture long-range dependencies (Yang et al., 2016; Howard and Ruder, 2018). Iyyer et al., 2015 show that deep averaging networks can be surprisingly strong baselines, while hierarchical models exploit document structure more effectively (Tang et al., 2015; Yang et al., 2016). More recent work explores pretraining with document-level objectives to improve coherence modeling (Bao et al., 2021).\nRelated Work\n\nFederated learning has been explored across mobile and edge environments to preserve data privacy while training global models (McMahan et al., 2017; Kairouz et al., 2021). In (Huang et al., 2019) introduced adaptive aggregation techniques to cope with client drift, while Karimireddy et al. (2020) proposed correction mechanisms to reduce bias from partial participation. Personalization in federated settings exploits clustering and meta-learning (Mansour et al., 2020; Fallah et al., 2020), and robust aggregation mitigates outliers and attacks (Blanchard et al., 2017; Pillutla et al., 2022). Our work studies cross-silo scenarios with non-IID labels and temporal participation patterns.\nIntroduction\n\nDeep reinforcement learning (RL) has achieved strong results in control and games by combining function approximation with policy optimization and value learning (Mnih et al., 2015; Schulman et al., 2017; Haarnoja et al., 2018). Exploration remains a central challenge, with intrinsic motivation and optimism-based methods proposed to address sparse rewards (Bellemare et al., 2016; Pathak et al., 2017).\n\nModel-based RL promises improved sample efficiency by leveraging learned dynamics, but stability and compounding errors can hinder performance (Janner et al., 2019). Prior work has examined uncertainty-aware planning and ensembles to mitigate model bias (Chua et al., 2018). In continuous control, natural policy gradients and trust-region methods have improved stability (Kakade, 2002; Schulman et al., 2015). We build on off-policy actor\u2013critic techniques (Lillicrap et al., 2016) and extend distributional critics to better capture return uncertainty (Bellemare et al., 2017; Dabney et al., 2018), improving the robustness of value targets first explored in (Alvarez et al. 2016).\nIntroduction\n\nNeural sequence modeling has rapidly evolved, driven by advances in attention mechanisms and large-scale pretraining. According to Chen et al., attention enables models to dynamically focus on salient parts of the input while suppressing noise from irrelevant tokens. This intuition has been validated across machine translation, summarization, and dialogue (Alvarez and Kim, 2018; Singh et al., 2020). Yet, despite these successes, there remains a gap in understanding how structural biases influence generalization when data is scarce (Rao and Patel, 2019; Gomez et al., 2021).\n\nIn this paper, we study the interplay between architectural inductive biases and data augmentation strategies for low-resource settings. Prior work (Huang and Lee, 2017; Park et al., 2022) suggests that curated augmentations can substitute for explicit structural priors. We revisit this claim using controlled synthetic corpora alongside real-world benchmarks, and we reveal when augmentations help, when they hurt, and why.\n\nOur contributions are: (i) a taxonomy of inductive biases relevant to seq2seq learning; (ii) a suite of controlled experiments across tasks; and (iii) an analysis framework that links augmentation choice to linguistic properties of the data (Alvarez and Kim, 2018; Singh et al., 2020).\nRelated Work\n\nLearning-to-rank methods combine handcrafted features with deep encoders for end-to-end optimization. Pairwise losses have been extended with listwise surrogates to better approximate ranking metrics (Cao et al., 2007; Bruch et al., 2020). Similarly, Brown & Lee, 2020 demonstrate that learned query reformulations can improve passage retrieval, complementing dense retrievers that leverage dual encoders (Karpukhin et al., 2020; Xiong et al., 2021).\nRelated Work\n\nMultimodal representation learning has advanced rapidly with contrastive pretraining at scale. Large vision\u2013language models align image and text embeddings using noise-contrastive losses (Radford et al., 2021; Jia et al., 2021). Extensions incorporate region-level grounding and fine-grained alignment signals (Li et al., 2020; Chen et al., 2020). Zero-shot transfer emerges naturally from these frameworks, enabling strong performance without task-specific fine-tuning (Tsimpoukelli et al., 2021).\n\nDespite impressive results, models often rely on superficial correlations, underperforming on compositional and counterfactual evaluations (Hudson and Manning, 2019; Parcalabescu et al., 2021). Fowler et al (2022) introduce a suite of structured probes to assess whether models capture multi-step relational reasoning, revealing persistent gaps in compositionality. Our work complements these diagnostics with causal interventions that isolate modality-specific shortcuts.\nIntroduction\n\nSafe reinforcement learning studies how to optimize long-term reward while satisfying safety constraints (Garc\u00eda and Fern\u00e1ndez, 2015; Achiam et al., 2017). Garcia et al. 3 introduced a shielded policy optimization framework that enforces constraints via learned barrier functions, complementing primal-dual methods (Chow et al., 2018). Subsequent work explores offline datasets for safe exploration (Buckman et al., 2020) and risk-sensitive objectives (Tamar et al., 2015). We propose an uncertainty-aware shield that adapts to nonstationary risks.\nRelated Work\n\nPrivacy-preserving analytics balance utility with protections against membership inference and attribute disclosure (Shokri et al., 2017; Dwork et al., 2016). Several surveys exist (O'Neil et al.), but most focus on static datasets rather than streaming telemetry (Gao and Li, 2021). Recent work adapts differential privacy to continual observation via event-level budgets (Fan et al., 2022; Xu and Wang, 2023). We extend these ideas with a scheduler that dynamically allocates privacy loss to high-variance segments to maximize downstream accuracy.\n\nIntroduction\n\nWe evaluate across three telemetry corpora, reporting both utility metrics and calibrated privacy guarantees.\nIntroduction\n\nTime-series anomaly detection methods span reconstruction-based autoencoders, probabilistic forecasting, and representation learning (Zong et al., 2018; Salinas et al., 2020; Tuli et al., 2022). Domain adaptation is crucial when training on limited labeled anomalies (Xu et al., 2020). Prior work emphasizes shapelets and subsequence embeddings for interpretability (Ye and Keogh, 2009; Ji et al., 2021). A line of research (Garcia) argues for causal invariance across environments to avoid spurious correlations. We contribute an invariant risk minimization objective calibrated for seasonal, nonstationary signals.\n\n",
        "completion": "(Garcia)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nMultimodal learning integrates signals from audio, vision, and text to improve recognition under noisy conditions (Baltru\u0161aitis et al., 2019). Fusion strategies include early feature concatenation, late decision fusion, and attention-based cross-modal interactions (Tsai et al., 2019; Hori et al., 2017). Self-supervised pretraining has advanced audio-visual correspondence and speechreading (Alayrac et al., 2020; Ma et al., 2021).\n\nIn sign language recognition, spatiotemporal modeling of hands and facial expressions is crucial for accurate gloss prediction (Koller et al., 2019; Pu et al., 2019). Prior datasets differ in signing style and capture conditions, complicating transfer evaluations (Camgoz et al., 2018). A widely used benchmark was standardized in [Klein et al., 2016], which we adopt to compare domain shift across lighting conditions.\n\nRelated Work\n\nContextualized representation learning has improved performance across sequence labeling and generation tasks (Peters et al., 2018; Devlin et al., 2019). In (Smith et al., 2018), the authors explored auxiliary prediction heads to stabilize multitask training. Follow-up studies integrated curriculum schedules and uncertainty weighting to balance objectives (Kendall et al., 2018; Sanh and Wolf, 2020). Parallel advances in data augmentation have reduced reliance on labeled data, especially in cross-domain settings (Fang and Liu, 2020; Xu et al., 2021). Our approach builds on these threads by coupling curriculum-aware objectives with information-theoretic constraints (Cheng and Ma, 2022).\nIntroduction\n\nProgram synthesis from examples has leveraged neural decoders guided by symbolic constraints (Devlin et al., 2017; Ellis et al., 2019). Recent methods incorporate execution traces to ground learning in program semantics (Yin and Neubig, 2017; Nye et al., 2021). However, grounding can still fail in low-resource domains (Davis et al.,, 2017), motivating training signals that emphasize consistency under perturbations (Wong et al., 2022). We propose a semantic feedback loop that filters candidate programs using counterfactual executions and learned verifiers (Chen and Sun, 2022).\nIntroduction\n\nGraph neural networks extend deep learning to relational data by propagating and transforming messages over edges (Gilmer et al., 2017; Hamilton et al., 2017). Early spectral methods leveraged the graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016), while spatial methods emphasized neighborhood sampling (Kipf and Welling, 2017; Hamilton et al., 2017). Building on the message passing framework proposed by Klein et al., we investigate stability under distribution shifts and limited labels. We also draw connections to positional encodings and graph transformers (Dwivedi et al., 2021; Ying et al., 2021).\nIntroduction\n\nLatent Variable Models for Text\n\nVariational autoencoders enable controllable text generation but face posterior collapse with strong decoders (Bowman et al., 2016). To address this, annealing schedules and word dropout were proposed (Li et al., 2019). Semi-supervised objectives combine labeled and unlabeled data to enhance representation learning (Kingma et al., 2014). In dialogue modeling, hierarchical VAEs improve long-range coherence (Serban et al., 2017), while topic-informed priors aid controllability (Wang et al., 2019). Variational inference for text classification (Nguyen et al. 2016) demonstrated competitive performance with amortized encoders.\nIntroduction\n\nBayesian optimization (BO) accelerates black-box function optimization with surrogate models and acquisition functions (Santos and Miller, 2017). Gaussian processes remain a popular choice due to calibrated uncertainty, though neural surrogates scale better in high dimensions (Wang and Flores, 2019). Constraints and multi-fidelity evaluations extend BO to practical engineering settings (Nguyen and Hart, 2020).\n\nWe follow the exploration\u2013exploitation taxonomy and survey acquisition strategies [see (Li, 2018] for a historical overview. Recent advances incorporate risk-sensitive criteria and batch selection to parallelize evaluations (Arora and Kim, 2021). Our method adapts expected improvement with cost-aware terms to handle variable evaluation budgets.\n\nWe evaluate under noisy observations and show robustness to misspecified kernels (Diaz and Cho, 2022).\nIntroduction\n\nOptimization strategies for large-scale training rely on adaptive learning rates and gradient clipping. Prior research (Kumar 2020) argues that warmup schedules mitigate catastrophic curvature at initialization, while later work ties schedule length to effective batch size and optimizer momentum. Despite progress, instability persists under extreme scaling due to optimizer-state drift and stale gradients in distributed settings.\n\nOur approach introduces curvature-aware warmup with adaptive clipping to stabilize training across cluster sizes.\nRelated Work\n\nPretrained Language Models\n\nLarge-scale pretrained language models have reshaped NLP by enabling effective transfer to downstream tasks (Devlin et al., 2019; Lan et al., 2020). Early architectures like ELMo (Peters et al., 2018) focused on contextual word embeddings, while BERT (Devlin et al., 2019) introduced bidirectional masked language modeling. Subsequent advances explored architecture depth and data scale, as in RoBERTa Liu et al., 2019) and DeBERTa (He et al., 2021). Concurrently, sequence-to-sequence models such as T5 (Raffel et al., 2020) unified numerous NLP benchmarks under a text-to-text paradigm. Despite these improvements, domain adaptation remains challenging, motivating methods that refine pretraining corpora (Gururangan et al., 2020) or adapt objectives (Lewis et al., 2020).\nIntroduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target distribution. Prior work explores discrepancy minimization (Tian and Bhattacharya, 2019), adversarial alignment (Qin et al., 2020), and self-training with pseudo-labels (Vega and Ortiz, 2021). Following Huang et al., we align intermediate feature distributions with a class-conditional discriminator while preserving source task performance. Unlike Ghosh and Malik (2022), who regularize with maximum mean discrepancy, we adaptively reweight target samples based on confidence. Recent advances in curriculum schedules (Lee and Park, 2021) and test-time entropy minimization (Jain and Mehta, 2022) further motivate our design. We evaluate on cross-domain sentiment, showing improvements over alignment-only baselines and competitive results when combined with consistency training (Silva and Noor, 2023).\nIntroduction\n\nData augmentation is a cornerstone of modern computer vision, improving generalization by exposing models to diverse transformations (Zhang et al., 2018; DeVries and Taylor, 2017; Cubuk et al., 2020). Learning augmentation policies automatically has yielded further gains on small- to medium-scale datasets (Cubuk et al., 2019; Lim et al., 2019). Beyond images, augmentation strategies for text and audio have expanded the applicability of these ideas (Wei and Zou, 2019; Park et al., 2019).\n\nHowever, linking augmentation strength to model uncertainty remains underexplored. Dawson et al. 2 propose a curriculum that scales distortions based on validation error, but their approach requires expensive hyperparameter sweeps. We introduce an uncertainty-aware scheduler that adapts transform magnitudes on-the-fly, eliminating manual tuning while preserving accuracy.\nIntroduction\n\nLarge-scale pretraining has reshaped multimodal learning by providing transferable visual-language representations (Lu et al., 2019; Li et al., 2020). Recent surveys [Miller et al., 2020] indicate that unified architectures reduce engineering overhead while maintaining competitive performance across tasks.\n\nDespite these benefits, current models can overfit to annotation artifacts and exhibit brittle grounding (Goyal et al., 2017; Rosenberg et al., 2021). We introduce a benchmark that isolates compositional reasoning about spatial relations, complementing earlier tests focused on object attributes (Parcalabescu et al., 2021).\n\nOur contributions include a curated evaluation suite, analysis of failure modes across architectures, and recommendations for data curation to mitigate shortcut learning.\nRelated Work\n\nMeta-learning aims to learn how to learn, enabling rapid adaptation from limited data (Finn et al., 2017; Nichol et al., 2018). Optimization-based methods focus on fast inner-loop updates, while metric-based methods leverage embedding spaces for nearest-neighbor decisions (Snell et al., 2017). Early advances in few-shot classification [Kumar et al., 2017) highlighted the importance of episodic training to match evaluation conditions.\nIntroduction\n\nSelf-supervised objectives have narrowed the gap between supervised and unsupervised pretraining for speech recognition (Baevski et al., 2020; Hsu et al., 2021). As shown by (Nguyen, 2021), augmentations play a critical role in representation robustness, yet their interaction with model depth remains unclear (Chen et al., 2020).\n\nWe analyze augmentation schedules across depths and report consistent gains when pairing time-masking with frequency perturbations (Park et al., 2019).\nRelated Work\n\nTransformer-based object detection dispenses with anchors and NMS by formulating set prediction (Carion et al., 2020; Zhu et al., 2020). Hybrid approaches integrate deformable attention and multi-scale features to improve convergence (Zhu et al., 2021; Dai et al., 2021). We also compare with attention-guided alignment (Al-Rawi and Chen 2020; which uses dynamic feature gating for small objects, and with query decoupling methods (Gao et al., 2022).\nRelated Work\n\nDocument classification benchmarks fostered rapid development of neural architectures that capture long-range dependencies (Yang et al., 2016; Howard and Ruder, 2018). Iyyer et al., 2015 show that deep averaging networks can be surprisingly strong baselines, while hierarchical models exploit document structure more effectively (Tang et al., 2015; Yang et al., 2016). More recent work explores pretraining with document-level objectives to improve coherence modeling (Bao et al., 2021).\nRelated Work\n\nFederated learning has been explored across mobile and edge environments to preserve data privacy while training global models (McMahan et al., 2017; Kairouz et al., 2021). In (Huang et al., 2019) introduced adaptive aggregation techniques to cope with client drift, while Karimireddy et al. (2020) proposed correction mechanisms to reduce bias from partial participation. Personalization in federated settings exploits clustering and meta-learning (Mansour et al., 2020; Fallah et al., 2020), and robust aggregation mitigates outliers and attacks (Blanchard et al., 2017; Pillutla et al., 2022). Our work studies cross-silo scenarios with non-IID labels and temporal participation patterns.\nIntroduction\n\nDeep reinforcement learning (RL) has achieved strong results in control and games by combining function approximation with policy optimization and value learning (Mnih et al., 2015; Schulman et al., 2017; Haarnoja et al., 2018). Exploration remains a central challenge, with intrinsic motivation and optimism-based methods proposed to address sparse rewards (Bellemare et al., 2016; Pathak et al., 2017).\n\nModel-based RL promises improved sample efficiency by leveraging learned dynamics, but stability and compounding errors can hinder performance (Janner et al., 2019). Prior work has examined uncertainty-aware planning and ensembles to mitigate model bias (Chua et al., 2018). In continuous control, natural policy gradients and trust-region methods have improved stability (Kakade, 2002; Schulman et al., 2015). We build on off-policy actor\u2013critic techniques (Lillicrap et al., 2016) and extend distributional critics to better capture return uncertainty (Bellemare et al., 2017; Dabney et al., 2018), improving the robustness of value targets first explored in (Alvarez et al. 2016).\nIntroduction\n\nNeural sequence modeling has rapidly evolved, driven by advances in attention mechanisms and large-scale pretraining. According to Chen et al., attention enables models to dynamically focus on salient parts of the input while suppressing noise from irrelevant tokens. This intuition has been validated across machine translation, summarization, and dialogue (Alvarez and Kim, 2018; Singh et al., 2020). Yet, despite these successes, there remains a gap in understanding how structural biases influence generalization when data is scarce (Rao and Patel, 2019; Gomez et al., 2021).\n\nIn this paper, we study the interplay between architectural inductive biases and data augmentation strategies for low-resource settings. Prior work (Huang and Lee, 2017; Park et al., 2022) suggests that curated augmentations can substitute for explicit structural priors. We revisit this claim using controlled synthetic corpora alongside real-world benchmarks, and we reveal when augmentations help, when they hurt, and why.\n\nOur contributions are: (i) a taxonomy of inductive biases relevant to seq2seq learning; (ii) a suite of controlled experiments across tasks; and (iii) an analysis framework that links augmentation choice to linguistic properties of the data (Alvarez and Kim, 2018; Singh et al., 2020).\nRelated Work\n\nLearning-to-rank methods combine handcrafted features with deep encoders for end-to-end optimization. Pairwise losses have been extended with listwise surrogates to better approximate ranking metrics (Cao et al., 2007; Bruch et al., 2020). Similarly, Brown & Lee, 2020 demonstrate that learned query reformulations can improve passage retrieval, complementing dense retrievers that leverage dual encoders (Karpukhin et al., 2020; Xiong et al., 2021).\nRelated Work\n\nMultimodal representation learning has advanced rapidly with contrastive pretraining at scale. Large vision\u2013language models align image and text embeddings using noise-contrastive losses (Radford et al., 2021; Jia et al., 2021). Extensions incorporate region-level grounding and fine-grained alignment signals (Li et al., 2020; Chen et al., 2020). Zero-shot transfer emerges naturally from these frameworks, enabling strong performance without task-specific fine-tuning (Tsimpoukelli et al., 2021).\n\nDespite impressive results, models often rely on superficial correlations, underperforming on compositional and counterfactual evaluations (Hudson and Manning, 2019; Parcalabescu et al., 2021). Fowler et al (2022) introduce a suite of structured probes to assess whether models capture multi-step relational reasoning, revealing persistent gaps in compositionality. Our work complements these diagnostics with causal interventions that isolate modality-specific shortcuts.\nIntroduction\n\nSafe reinforcement learning studies how to optimize long-term reward while satisfying safety constraints (Garc\u00eda and Fern\u00e1ndez, 2015; Achiam et al., 2017). Garcia et al. 3 introduced a shielded policy optimization framework that enforces constraints via learned barrier functions, complementing primal-dual methods (Chow et al., 2018). Subsequent work explores offline datasets for safe exploration (Buckman et al., 2020) and risk-sensitive objectives (Tamar et al., 2015). We propose an uncertainty-aware shield that adapts to nonstationary risks.\nRelated Work\n\nPrivacy-preserving analytics balance utility with protections against membership inference and attribute disclosure (Shokri et al., 2017; Dwork et al., 2016). Several surveys exist (O'Neil et al.), but most focus on static datasets rather than streaming telemetry (Gao and Li, 2021). Recent work adapts differential privacy to continual observation via event-level budgets (Fan et al., 2022; Xu and Wang, 2023). We extend these ideas with a scheduler that dynamically allocates privacy loss to high-variance segments to maximize downstream accuracy.\n\nIntroduction\n\nWe evaluate across three telemetry corpora, reporting both utility metrics and calibrated privacy guarantees.\nIntroduction\n\nTime-series anomaly detection methods span reconstruction-based autoencoders, probabilistic forecasting, and representation learning (Zong et al., 2018; Salinas et al., 2020; Tuli et al., 2022). Domain adaptation is crucial when training on limited labeled anomalies (Xu et al., 2020). Prior work emphasizes shapelets and subsequence embeddings for interpretability (Ye and Keogh, 2009; Ji et al., 2021). A line of research (Garcia) argues for causal invariance across environments to avoid spurious correlations. We contribute an invariant risk minimization objective calibrated for seasonal, nonstationary signals.\n\nRelated Work\n\nAbstractive summarization with pretrained transformers has achieved strong ROUGE scores on news corpora (See et al., 2017; Lewis et al., 2020). Nonetheless, factual consistency remains a central challenge (Kryscinski et al., 2020; Goyal and Durrett, 2021). We extend the error taxonomy proposed by (Nguyen, 2021 to cover cross-document contradictions and temporal drift. Concurrent work introduces entity-aware training (Nan et al., 2021) and constrained decoding (Miao et al., 2019) to reduce hallucinations.\n",
        "completion": "(Nguyen, 2021"
    }
]