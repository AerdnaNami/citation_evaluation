[
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\n",
        "completion": "(Smith et al., 2019)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\n",
        "completion": "Lee & Kim (2021)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\n",
        "completion": "(Basu, 2014 and Ortega et al., 2015)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\n",
        "completion": "Klein and Murray 3"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\n",
        "completion": "(O'Neil, 2015 Kearns and Roth, 2019)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\n",
        "completion": "(Lopez et al. 2022)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\n",
        "completion": "(Taylor and Kim, 2020)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\n",
        "completion": "Liang et al. 2"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\n",
        "completion": "(Olsen, 2017, Patel, 2019)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\nRelated Work\n\nMultimodal learning integrates signals from text, vision, and audio to improve downstream performance. Early fusion methods concatenate features learned independently (Park and Huang, 2018), whereas late fusion aggregates predictions from unimodal experts (Ahmed and Krishnan, 2019). Joint training approaches learn shared representations that align modalities (Wu et al., 2020).\n\nFollowing Johnson et al., we consider cross-modal attention as a mechanism to mediate information exchange between streams while preserving modality-specific information. Subsequent work demonstrated that aligning contrastive objectives with task losses yields further gains (Chen and Duarte, 2021; Lin et al., 2022). Our work differs by introducing a calibration loss that attenuates modality dominance under class imbalance.\n",
        "completion": "Johnson et al."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\nRelated Work\n\nMultimodal learning integrates signals from text, vision, and audio to improve downstream performance. Early fusion methods concatenate features learned independently (Park and Huang, 2018), whereas late fusion aggregates predictions from unimodal experts (Ahmed and Krishnan, 2019). Joint training approaches learn shared representations that align modalities (Wu et al., 2020).\n\nFollowing Johnson et al., we consider cross-modal attention as a mechanism to mediate information exchange between streams while preserving modality-specific information. Subsequent work demonstrated that aligning contrastive objectives with task losses yields further gains (Chen and Duarte, 2021; Lin et al., 2022). Our work differs by introducing a calibration loss that attenuates modality dominance under class imbalance.\nRelated Work\n\nNeural machine translation (NMT) benefits from multilingual transfer and shared subword vocabularies (Johnson et al., 2017; Conneau et al., 2020). Context-aware NMT uses document history to resolve coreference and lexical consistency (Voita et al., 2018; Maruf et al., 2019). Some works also incorporate visual cues for multimodal translation [12], though evidence of consistent gains is mixed (Calixto and Liu, 2017; Elliott, 2018). We revisit context modeling with lightweight cache mechanisms.\n",
        "completion": "[12]"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\nRelated Work\n\nMultimodal learning integrates signals from text, vision, and audio to improve downstream performance. Early fusion methods concatenate features learned independently (Park and Huang, 2018), whereas late fusion aggregates predictions from unimodal experts (Ahmed and Krishnan, 2019). Joint training approaches learn shared representations that align modalities (Wu et al., 2020).\n\nFollowing Johnson et al., we consider cross-modal attention as a mechanism to mediate information exchange between streams while preserving modality-specific information. Subsequent work demonstrated that aligning contrastive objectives with task losses yields further gains (Chen and Duarte, 2021; Lin et al., 2022). Our work differs by introducing a calibration loss that attenuates modality dominance under class imbalance.\nRelated Work\n\nNeural machine translation (NMT) benefits from multilingual transfer and shared subword vocabularies (Johnson et al., 2017; Conneau et al., 2020). Context-aware NMT uses document history to resolve coreference and lexical consistency (Voita et al., 2018; Maruf et al., 2019). Some works also incorporate visual cues for multimodal translation [12], though evidence of consistent gains is mixed (Calixto and Liu, 2017; Elliott, 2018). We revisit context modeling with lightweight cache mechanisms.\nRelated Work\n\nFairness in machine learning encompasses group fairness, individual fairness, and causal fairness perspectives (Dwork et al., 2012; Kusner et al., 2017; Barocas et al., 2019). Smith (2019; surveyed post-processing techniques that calibrate decision thresholds to equalize error rates across groups. Complementary approaches incorporate fairness constraints during training (Zafar et al., 2017) or redesign objectives via adversarial learning (Zhang et al., 2018).\n",
        "completion": "Smith (2019;"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\nRelated Work\n\nMultimodal learning integrates signals from text, vision, and audio to improve downstream performance. Early fusion methods concatenate features learned independently (Park and Huang, 2018), whereas late fusion aggregates predictions from unimodal experts (Ahmed and Krishnan, 2019). Joint training approaches learn shared representations that align modalities (Wu et al., 2020).\n\nFollowing Johnson et al., we consider cross-modal attention as a mechanism to mediate information exchange between streams while preserving modality-specific information. Subsequent work demonstrated that aligning contrastive objectives with task losses yields further gains (Chen and Duarte, 2021; Lin et al., 2022). Our work differs by introducing a calibration loss that attenuates modality dominance under class imbalance.\nRelated Work\n\nNeural machine translation (NMT) benefits from multilingual transfer and shared subword vocabularies (Johnson et al., 2017; Conneau et al., 2020). Context-aware NMT uses document history to resolve coreference and lexical consistency (Voita et al., 2018; Maruf et al., 2019). Some works also incorporate visual cues for multimodal translation [12], though evidence of consistent gains is mixed (Calixto and Liu, 2017; Elliott, 2018). We revisit context modeling with lightweight cache mechanisms.\nRelated Work\n\nFairness in machine learning encompasses group fairness, individual fairness, and causal fairness perspectives (Dwork et al., 2012; Kusner et al., 2017; Barocas et al., 2019). Smith (2019; surveyed post-processing techniques that calibrate decision thresholds to equalize error rates across groups. Complementary approaches incorporate fairness constraints during training (Zafar et al., 2017) or redesign objectives via adversarial learning (Zhang et al., 2018).\nRelated Work\n\nPolicy gradient methods have evolved from REINFORCE (Williams, 1992) to actor\u2013critic variants that stabilize learning with value baselines (Konda and Tsitsiklis, 2000; Mnih et al., 2016). Kaur et al. propose entropy regularization to encourage exploration, while other works develop trust region constraints to control policy updates (Schulman et al., 2015; Schulman et al., 2017). Off-policy corrections via importance sampling and variance reduction have also been investigated (Espeholt et al., 2018).\n\n",
        "completion": "Kaur et al."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\nRelated Work\n\nMultimodal learning integrates signals from text, vision, and audio to improve downstream performance. Early fusion methods concatenate features learned independently (Park and Huang, 2018), whereas late fusion aggregates predictions from unimodal experts (Ahmed and Krishnan, 2019). Joint training approaches learn shared representations that align modalities (Wu et al., 2020).\n\nFollowing Johnson et al., we consider cross-modal attention as a mechanism to mediate information exchange between streams while preserving modality-specific information. Subsequent work demonstrated that aligning contrastive objectives with task losses yields further gains (Chen and Duarte, 2021; Lin et al., 2022). Our work differs by introducing a calibration loss that attenuates modality dominance under class imbalance.\nRelated Work\n\nNeural machine translation (NMT) benefits from multilingual transfer and shared subword vocabularies (Johnson et al., 2017; Conneau et al., 2020). Context-aware NMT uses document history to resolve coreference and lexical consistency (Voita et al., 2018; Maruf et al., 2019). Some works also incorporate visual cues for multimodal translation [12], though evidence of consistent gains is mixed (Calixto and Liu, 2017; Elliott, 2018). We revisit context modeling with lightweight cache mechanisms.\nRelated Work\n\nFairness in machine learning encompasses group fairness, individual fairness, and causal fairness perspectives (Dwork et al., 2012; Kusner et al., 2017; Barocas et al., 2019). Smith (2019; surveyed post-processing techniques that calibrate decision thresholds to equalize error rates across groups. Complementary approaches incorporate fairness constraints during training (Zafar et al., 2017) or redesign objectives via adversarial learning (Zhang et al., 2018).\nRelated Work\n\nPolicy gradient methods have evolved from REINFORCE (Williams, 1992) to actor\u2013critic variants that stabilize learning with value baselines (Konda and Tsitsiklis, 2000; Mnih et al., 2016). Kaur et al. propose entropy regularization to encourage exploration, while other works develop trust region constraints to control policy updates (Schulman et al., 2015; Schulman et al., 2017). Off-policy corrections via importance sampling and variance reduction have also been investigated (Espeholt et al., 2018).\n\nIntroduction\n\nMultitask learning (MTL) can improve generalization by sharing representations across related tasks (Caruana, 1997; Ruder, 2017). Earlier sequence-to-sequence MTL frameworks (Johnson et al., 2017 demonstrated that multilingual models can leverage shared subword vocabularies to transfer knowledge across languages. Building on this, subsequent studies explored task-conditioned adapters to separate task-specific signals while keeping shared parameters compact (Singh and Roy, 2019). Our work extends these ideas to few-shot settings by aligning task prompts with adapter routing, enabling better transfer to unseen tasks (Vega and Park, 2021).\n\nDespite promising results, negative transfer remains a challenge when tasks are only weakly related (Pang and Li, 2020), motivating the need for routing mechanisms that detect and mitigate interference.\n",
        "completion": "(Johnson et al., 2017"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\nRelated Work\n\nMultimodal learning integrates signals from text, vision, and audio to improve downstream performance. Early fusion methods concatenate features learned independently (Park and Huang, 2018), whereas late fusion aggregates predictions from unimodal experts (Ahmed and Krishnan, 2019). Joint training approaches learn shared representations that align modalities (Wu et al., 2020).\n\nFollowing Johnson et al., we consider cross-modal attention as a mechanism to mediate information exchange between streams while preserving modality-specific information. Subsequent work demonstrated that aligning contrastive objectives with task losses yields further gains (Chen and Duarte, 2021; Lin et al., 2022). Our work differs by introducing a calibration loss that attenuates modality dominance under class imbalance.\nRelated Work\n\nNeural machine translation (NMT) benefits from multilingual transfer and shared subword vocabularies (Johnson et al., 2017; Conneau et al., 2020). Context-aware NMT uses document history to resolve coreference and lexical consistency (Voita et al., 2018; Maruf et al., 2019). Some works also incorporate visual cues for multimodal translation [12], though evidence of consistent gains is mixed (Calixto and Liu, 2017; Elliott, 2018). We revisit context modeling with lightweight cache mechanisms.\nRelated Work\n\nFairness in machine learning encompasses group fairness, individual fairness, and causal fairness perspectives (Dwork et al., 2012; Kusner et al., 2017; Barocas et al., 2019). Smith (2019; surveyed post-processing techniques that calibrate decision thresholds to equalize error rates across groups. Complementary approaches incorporate fairness constraints during training (Zafar et al., 2017) or redesign objectives via adversarial learning (Zhang et al., 2018).\nRelated Work\n\nPolicy gradient methods have evolved from REINFORCE (Williams, 1992) to actor\u2013critic variants that stabilize learning with value baselines (Konda and Tsitsiklis, 2000; Mnih et al., 2016). Kaur et al. propose entropy regularization to encourage exploration, while other works develop trust region constraints to control policy updates (Schulman et al., 2015; Schulman et al., 2017). Off-policy corrections via importance sampling and variance reduction have also been investigated (Espeholt et al., 2018).\n\nIntroduction\n\nMultitask learning (MTL) can improve generalization by sharing representations across related tasks (Caruana, 1997; Ruder, 2017). Earlier sequence-to-sequence MTL frameworks (Johnson et al., 2017 demonstrated that multilingual models can leverage shared subword vocabularies to transfer knowledge across languages. Building on this, subsequent studies explored task-conditioned adapters to separate task-specific signals while keeping shared parameters compact (Singh and Roy, 2019). Our work extends these ideas to few-shot settings by aligning task prompts with adapter routing, enabling better transfer to unseen tasks (Vega and Park, 2021).\n\nDespite promising results, negative transfer remains a challenge when tasks are only weakly related (Pang and Li, 2020), motivating the need for routing mechanisms that detect and mitigate interference.\nRelated Work\n\nAutomatic abusive language detection relies on robust representations that capture context and pragmatics (Waseem and Hovy, 2016; Nobata et al., 2016). Early datasets focused on lexicon cues, which led to spurious correlations and poor generalization (Schmidt and Wiegand, 2017). The introduction of crowd-annotated corpora improved coverage but also introduced annotator bias and label disagreements [Davidson et al., 2017]. Recent work addresses these issues via debiasing objectives and counterfactual augmentation (Park et al., 2018; Dinan et al., 2020).\n",
        "completion": "[Davidson et al., 2017]"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\nRelated Work\n\nMultimodal learning integrates signals from text, vision, and audio to improve downstream performance. Early fusion methods concatenate features learned independently (Park and Huang, 2018), whereas late fusion aggregates predictions from unimodal experts (Ahmed and Krishnan, 2019). Joint training approaches learn shared representations that align modalities (Wu et al., 2020).\n\nFollowing Johnson et al., we consider cross-modal attention as a mechanism to mediate information exchange between streams while preserving modality-specific information. Subsequent work demonstrated that aligning contrastive objectives with task losses yields further gains (Chen and Duarte, 2021; Lin et al., 2022). Our work differs by introducing a calibration loss that attenuates modality dominance under class imbalance.\nRelated Work\n\nNeural machine translation (NMT) benefits from multilingual transfer and shared subword vocabularies (Johnson et al., 2017; Conneau et al., 2020). Context-aware NMT uses document history to resolve coreference and lexical consistency (Voita et al., 2018; Maruf et al., 2019). Some works also incorporate visual cues for multimodal translation [12], though evidence of consistent gains is mixed (Calixto and Liu, 2017; Elliott, 2018). We revisit context modeling with lightweight cache mechanisms.\nRelated Work\n\nFairness in machine learning encompasses group fairness, individual fairness, and causal fairness perspectives (Dwork et al., 2012; Kusner et al., 2017; Barocas et al., 2019). Smith (2019; surveyed post-processing techniques that calibrate decision thresholds to equalize error rates across groups. Complementary approaches incorporate fairness constraints during training (Zafar et al., 2017) or redesign objectives via adversarial learning (Zhang et al., 2018).\nRelated Work\n\nPolicy gradient methods have evolved from REINFORCE (Williams, 1992) to actor\u2013critic variants that stabilize learning with value baselines (Konda and Tsitsiklis, 2000; Mnih et al., 2016). Kaur et al. propose entropy regularization to encourage exploration, while other works develop trust region constraints to control policy updates (Schulman et al., 2015; Schulman et al., 2017). Off-policy corrections via importance sampling and variance reduction have also been investigated (Espeholt et al., 2018).\n\nIntroduction\n\nMultitask learning (MTL) can improve generalization by sharing representations across related tasks (Caruana, 1997; Ruder, 2017). Earlier sequence-to-sequence MTL frameworks (Johnson et al., 2017 demonstrated that multilingual models can leverage shared subword vocabularies to transfer knowledge across languages. Building on this, subsequent studies explored task-conditioned adapters to separate task-specific signals while keeping shared parameters compact (Singh and Roy, 2019). Our work extends these ideas to few-shot settings by aligning task prompts with adapter routing, enabling better transfer to unseen tasks (Vega and Park, 2021).\n\nDespite promising results, negative transfer remains a challenge when tasks are only weakly related (Pang and Li, 2020), motivating the need for routing mechanisms that detect and mitigate interference.\nRelated Work\n\nAutomatic abusive language detection relies on robust representations that capture context and pragmatics (Waseem and Hovy, 2016; Nobata et al., 2016). Early datasets focused on lexicon cues, which led to spurious correlations and poor generalization (Schmidt and Wiegand, 2017). The introduction of crowd-annotated corpora improved coverage but also introduced annotator bias and label disagreements [Davidson et al., 2017]. Recent work addresses these issues via debiasing objectives and counterfactual augmentation (Park et al., 2018; Dinan et al., 2020).\nIntroduction\n\nDomain adaptation seeks to bridge distribution shifts between source and target tasks (Ben-David et al., 2010; Ganin and Lempitsky, 2015). According to Zhou (2019 the use of pseudo-labels with confidence thresholds can mitigate negative transfer by filtering unreliable targets. Adversarial alignment and moment matching offer complementary solutions (Long et al., 2015; Tzeng et al., 2017).\n\nRecent approaches combine self-training with uncertainty estimation to further improve target-domain accuracy (Sohn et al., 2020; Rizve et al., 2021). Data augmentation and curriculum schedules provide additional benefits in challenging shifts (Volpi et al., 2018; Shu et al., 2018).\n",
        "completion": "Zhou (2019"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\nRelated Work\n\nMultimodal learning integrates signals from text, vision, and audio to improve downstream performance. Early fusion methods concatenate features learned independently (Park and Huang, 2018), whereas late fusion aggregates predictions from unimodal experts (Ahmed and Krishnan, 2019). Joint training approaches learn shared representations that align modalities (Wu et al., 2020).\n\nFollowing Johnson et al., we consider cross-modal attention as a mechanism to mediate information exchange between streams while preserving modality-specific information. Subsequent work demonstrated that aligning contrastive objectives with task losses yields further gains (Chen and Duarte, 2021; Lin et al., 2022). Our work differs by introducing a calibration loss that attenuates modality dominance under class imbalance.\nRelated Work\n\nNeural machine translation (NMT) benefits from multilingual transfer and shared subword vocabularies (Johnson et al., 2017; Conneau et al., 2020). Context-aware NMT uses document history to resolve coreference and lexical consistency (Voita et al., 2018; Maruf et al., 2019). Some works also incorporate visual cues for multimodal translation [12], though evidence of consistent gains is mixed (Calixto and Liu, 2017; Elliott, 2018). We revisit context modeling with lightweight cache mechanisms.\nRelated Work\n\nFairness in machine learning encompasses group fairness, individual fairness, and causal fairness perspectives (Dwork et al., 2012; Kusner et al., 2017; Barocas et al., 2019). Smith (2019; surveyed post-processing techniques that calibrate decision thresholds to equalize error rates across groups. Complementary approaches incorporate fairness constraints during training (Zafar et al., 2017) or redesign objectives via adversarial learning (Zhang et al., 2018).\nRelated Work\n\nPolicy gradient methods have evolved from REINFORCE (Williams, 1992) to actor\u2013critic variants that stabilize learning with value baselines (Konda and Tsitsiklis, 2000; Mnih et al., 2016). Kaur et al. propose entropy regularization to encourage exploration, while other works develop trust region constraints to control policy updates (Schulman et al., 2015; Schulman et al., 2017). Off-policy corrections via importance sampling and variance reduction have also been investigated (Espeholt et al., 2018).\n\nIntroduction\n\nMultitask learning (MTL) can improve generalization by sharing representations across related tasks (Caruana, 1997; Ruder, 2017). Earlier sequence-to-sequence MTL frameworks (Johnson et al., 2017 demonstrated that multilingual models can leverage shared subword vocabularies to transfer knowledge across languages. Building on this, subsequent studies explored task-conditioned adapters to separate task-specific signals while keeping shared parameters compact (Singh and Roy, 2019). Our work extends these ideas to few-shot settings by aligning task prompts with adapter routing, enabling better transfer to unseen tasks (Vega and Park, 2021).\n\nDespite promising results, negative transfer remains a challenge when tasks are only weakly related (Pang and Li, 2020), motivating the need for routing mechanisms that detect and mitigate interference.\nRelated Work\n\nAutomatic abusive language detection relies on robust representations that capture context and pragmatics (Waseem and Hovy, 2016; Nobata et al., 2016). Early datasets focused on lexicon cues, which led to spurious correlations and poor generalization (Schmidt and Wiegand, 2017). The introduction of crowd-annotated corpora improved coverage but also introduced annotator bias and label disagreements [Davidson et al., 2017]. Recent work addresses these issues via debiasing objectives and counterfactual augmentation (Park et al., 2018; Dinan et al., 2020).\nIntroduction\n\nDomain adaptation seeks to bridge distribution shifts between source and target tasks (Ben-David et al., 2010; Ganin and Lempitsky, 2015). According to Zhou (2019 the use of pseudo-labels with confidence thresholds can mitigate negative transfer by filtering unreliable targets. Adversarial alignment and moment matching offer complementary solutions (Long et al., 2015; Tzeng et al., 2017).\n\nRecent approaches combine self-training with uncertainty estimation to further improve target-domain accuracy (Sohn et al., 2020; Rizve et al., 2021). Data augmentation and curriculum schedules provide additional benefits in challenging shifts (Volpi et al., 2018; Shu et al., 2018).\nIntroduction\n\nPersonalized recommendation has evolved from neighborhood methods to deep architectures that capture complex user\u2013item interactions (Koren et al., 2009; He et al., 2017). Context-aware models further improve click-through prediction by incorporating temporal and session features (Quadrana et al., 2018). Despite their success, such models can overfit short-term patterns and neglect long-term preferences (Zhou et al., 2019). We revisit sequence-aware recommendation and introduce a regularizer that stabilizes training while preserving recency signals (Nguyen et al., 2018\n",
        "completion": " (Nguyen et al., 2018"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\nRelated Work\n\nMultimodal learning integrates signals from text, vision, and audio to improve downstream performance. Early fusion methods concatenate features learned independently (Park and Huang, 2018), whereas late fusion aggregates predictions from unimodal experts (Ahmed and Krishnan, 2019). Joint training approaches learn shared representations that align modalities (Wu et al., 2020).\n\nFollowing Johnson et al., we consider cross-modal attention as a mechanism to mediate information exchange between streams while preserving modality-specific information. Subsequent work demonstrated that aligning contrastive objectives with task losses yields further gains (Chen and Duarte, 2021; Lin et al., 2022). Our work differs by introducing a calibration loss that attenuates modality dominance under class imbalance.\nRelated Work\n\nNeural machine translation (NMT) benefits from multilingual transfer and shared subword vocabularies (Johnson et al., 2017; Conneau et al., 2020). Context-aware NMT uses document history to resolve coreference and lexical consistency (Voita et al., 2018; Maruf et al., 2019). Some works also incorporate visual cues for multimodal translation [12], though evidence of consistent gains is mixed (Calixto and Liu, 2017; Elliott, 2018). We revisit context modeling with lightweight cache mechanisms.\nRelated Work\n\nFairness in machine learning encompasses group fairness, individual fairness, and causal fairness perspectives (Dwork et al., 2012; Kusner et al., 2017; Barocas et al., 2019). Smith (2019; surveyed post-processing techniques that calibrate decision thresholds to equalize error rates across groups. Complementary approaches incorporate fairness constraints during training (Zafar et al., 2017) or redesign objectives via adversarial learning (Zhang et al., 2018).\nRelated Work\n\nPolicy gradient methods have evolved from REINFORCE (Williams, 1992) to actor\u2013critic variants that stabilize learning with value baselines (Konda and Tsitsiklis, 2000; Mnih et al., 2016). Kaur et al. propose entropy regularization to encourage exploration, while other works develop trust region constraints to control policy updates (Schulman et al., 2015; Schulman et al., 2017). Off-policy corrections via importance sampling and variance reduction have also been investigated (Espeholt et al., 2018).\n\nIntroduction\n\nMultitask learning (MTL) can improve generalization by sharing representations across related tasks (Caruana, 1997; Ruder, 2017). Earlier sequence-to-sequence MTL frameworks (Johnson et al., 2017 demonstrated that multilingual models can leverage shared subword vocabularies to transfer knowledge across languages. Building on this, subsequent studies explored task-conditioned adapters to separate task-specific signals while keeping shared parameters compact (Singh and Roy, 2019). Our work extends these ideas to few-shot settings by aligning task prompts with adapter routing, enabling better transfer to unseen tasks (Vega and Park, 2021).\n\nDespite promising results, negative transfer remains a challenge when tasks are only weakly related (Pang and Li, 2020), motivating the need for routing mechanisms that detect and mitigate interference.\nRelated Work\n\nAutomatic abusive language detection relies on robust representations that capture context and pragmatics (Waseem and Hovy, 2016; Nobata et al., 2016). Early datasets focused on lexicon cues, which led to spurious correlations and poor generalization (Schmidt and Wiegand, 2017). The introduction of crowd-annotated corpora improved coverage but also introduced annotator bias and label disagreements [Davidson et al., 2017]. Recent work addresses these issues via debiasing objectives and counterfactual augmentation (Park et al., 2018; Dinan et al., 2020).\nIntroduction\n\nDomain adaptation seeks to bridge distribution shifts between source and target tasks (Ben-David et al., 2010; Ganin and Lempitsky, 2015). According to Zhou (2019 the use of pseudo-labels with confidence thresholds can mitigate negative transfer by filtering unreliable targets. Adversarial alignment and moment matching offer complementary solutions (Long et al., 2015; Tzeng et al., 2017).\n\nRecent approaches combine self-training with uncertainty estimation to further improve target-domain accuracy (Sohn et al., 2020; Rizve et al., 2021). Data augmentation and curriculum schedules provide additional benefits in challenging shifts (Volpi et al., 2018; Shu et al., 2018).\nIntroduction\n\nPersonalized recommendation has evolved from neighborhood methods to deep architectures that capture complex user\u2013item interactions (Koren et al., 2009; He et al., 2017). Context-aware models further improve click-through prediction by incorporating temporal and session features (Quadrana et al., 2018). Despite their success, such models can overfit short-term patterns and neglect long-term preferences (Zhou et al., 2019). We revisit sequence-aware recommendation and introduce a regularizer that stabilizes training while preserving recency signals (Nguyen et al., 2018\nRelated Work\n\nSelf-supervised pretraining has transformed speech recognition, enabling label-efficient fine-tuning (Baevski et al., 2020; Hsu et al., 2021). For streaming ASR, monotonic attention variants improve latency-accuracy trade-offs (Tsuchiya et al., 2019; Ma et al., 2021). Prior work suggests that layer-wise learning rates stabilize adaptation (Zhao 2020) and that data augmentation with noise profiles boosts robustness (Ko et al., 2015). Our study focuses on segment-level adapters that preserve causality.\n",
        "completion": "(Zhao 2020)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\nRelated Work\n\nMultimodal learning integrates signals from text, vision, and audio to improve downstream performance. Early fusion methods concatenate features learned independently (Park and Huang, 2018), whereas late fusion aggregates predictions from unimodal experts (Ahmed and Krishnan, 2019). Joint training approaches learn shared representations that align modalities (Wu et al., 2020).\n\nFollowing Johnson et al., we consider cross-modal attention as a mechanism to mediate information exchange between streams while preserving modality-specific information. Subsequent work demonstrated that aligning contrastive objectives with task losses yields further gains (Chen and Duarte, 2021; Lin et al., 2022). Our work differs by introducing a calibration loss that attenuates modality dominance under class imbalance.\nRelated Work\n\nNeural machine translation (NMT) benefits from multilingual transfer and shared subword vocabularies (Johnson et al., 2017; Conneau et al., 2020). Context-aware NMT uses document history to resolve coreference and lexical consistency (Voita et al., 2018; Maruf et al., 2019). Some works also incorporate visual cues for multimodal translation [12], though evidence of consistent gains is mixed (Calixto and Liu, 2017; Elliott, 2018). We revisit context modeling with lightweight cache mechanisms.\nRelated Work\n\nFairness in machine learning encompasses group fairness, individual fairness, and causal fairness perspectives (Dwork et al., 2012; Kusner et al., 2017; Barocas et al., 2019). Smith (2019; surveyed post-processing techniques that calibrate decision thresholds to equalize error rates across groups. Complementary approaches incorporate fairness constraints during training (Zafar et al., 2017) or redesign objectives via adversarial learning (Zhang et al., 2018).\nRelated Work\n\nPolicy gradient methods have evolved from REINFORCE (Williams, 1992) to actor\u2013critic variants that stabilize learning with value baselines (Konda and Tsitsiklis, 2000; Mnih et al., 2016). Kaur et al. propose entropy regularization to encourage exploration, while other works develop trust region constraints to control policy updates (Schulman et al., 2015; Schulman et al., 2017). Off-policy corrections via importance sampling and variance reduction have also been investigated (Espeholt et al., 2018).\n\nIntroduction\n\nMultitask learning (MTL) can improve generalization by sharing representations across related tasks (Caruana, 1997; Ruder, 2017). Earlier sequence-to-sequence MTL frameworks (Johnson et al., 2017 demonstrated that multilingual models can leverage shared subword vocabularies to transfer knowledge across languages. Building on this, subsequent studies explored task-conditioned adapters to separate task-specific signals while keeping shared parameters compact (Singh and Roy, 2019). Our work extends these ideas to few-shot settings by aligning task prompts with adapter routing, enabling better transfer to unseen tasks (Vega and Park, 2021).\n\nDespite promising results, negative transfer remains a challenge when tasks are only weakly related (Pang and Li, 2020), motivating the need for routing mechanisms that detect and mitigate interference.\nRelated Work\n\nAutomatic abusive language detection relies on robust representations that capture context and pragmatics (Waseem and Hovy, 2016; Nobata et al., 2016). Early datasets focused on lexicon cues, which led to spurious correlations and poor generalization (Schmidt and Wiegand, 2017). The introduction of crowd-annotated corpora improved coverage but also introduced annotator bias and label disagreements [Davidson et al., 2017]. Recent work addresses these issues via debiasing objectives and counterfactual augmentation (Park et al., 2018; Dinan et al., 2020).\nIntroduction\n\nDomain adaptation seeks to bridge distribution shifts between source and target tasks (Ben-David et al., 2010; Ganin and Lempitsky, 2015). According to Zhou (2019 the use of pseudo-labels with confidence thresholds can mitigate negative transfer by filtering unreliable targets. Adversarial alignment and moment matching offer complementary solutions (Long et al., 2015; Tzeng et al., 2017).\n\nRecent approaches combine self-training with uncertainty estimation to further improve target-domain accuracy (Sohn et al., 2020; Rizve et al., 2021). Data augmentation and curriculum schedules provide additional benefits in challenging shifts (Volpi et al., 2018; Shu et al., 2018).\nIntroduction\n\nPersonalized recommendation has evolved from neighborhood methods to deep architectures that capture complex user\u2013item interactions (Koren et al., 2009; He et al., 2017). Context-aware models further improve click-through prediction by incorporating temporal and session features (Quadrana et al., 2018). Despite their success, such models can overfit short-term patterns and neglect long-term preferences (Zhou et al., 2019). We revisit sequence-aware recommendation and introduce a regularizer that stabilizes training while preserving recency signals (Nguyen et al., 2018\nRelated Work\n\nSelf-supervised pretraining has transformed speech recognition, enabling label-efficient fine-tuning (Baevski et al., 2020; Hsu et al., 2021). For streaming ASR, monotonic attention variants improve latency-accuracy trade-offs (Tsuchiya et al., 2019; Ma et al., 2021). Prior work suggests that layer-wise learning rates stabilize adaptation (Zhao 2020) and that data augmentation with noise profiles boosts robustness (Ko et al., 2015). Our study focuses on segment-level adapters that preserve causality.\nIntroduction\n\nReinforcement learning for robotic manipulation combines model-free exploration with demonstrations and priors (Levine et al., 2016; Kalashnikov et al., 2018). Policy constraints and offline datasets stabilize training (Fujimoto et al., 2019; Kumar et al., 2020). More recent work by Zhou et al., 2021) extends implicit Q-learning to dexterous hands, highlighting the importance of conservative estimates under limited coverage.\n",
        "completion": "Zhou et al., 2021)"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\nRelated Work\n\nMultimodal learning integrates signals from text, vision, and audio to improve downstream performance. Early fusion methods concatenate features learned independently (Park and Huang, 2018), whereas late fusion aggregates predictions from unimodal experts (Ahmed and Krishnan, 2019). Joint training approaches learn shared representations that align modalities (Wu et al., 2020).\n\nFollowing Johnson et al., we consider cross-modal attention as a mechanism to mediate information exchange between streams while preserving modality-specific information. Subsequent work demonstrated that aligning contrastive objectives with task losses yields further gains (Chen and Duarte, 2021; Lin et al., 2022). Our work differs by introducing a calibration loss that attenuates modality dominance under class imbalance.\nRelated Work\n\nNeural machine translation (NMT) benefits from multilingual transfer and shared subword vocabularies (Johnson et al., 2017; Conneau et al., 2020). Context-aware NMT uses document history to resolve coreference and lexical consistency (Voita et al., 2018; Maruf et al., 2019). Some works also incorporate visual cues for multimodal translation [12], though evidence of consistent gains is mixed (Calixto and Liu, 2017; Elliott, 2018). We revisit context modeling with lightweight cache mechanisms.\nRelated Work\n\nFairness in machine learning encompasses group fairness, individual fairness, and causal fairness perspectives (Dwork et al., 2012; Kusner et al., 2017; Barocas et al., 2019). Smith (2019; surveyed post-processing techniques that calibrate decision thresholds to equalize error rates across groups. Complementary approaches incorporate fairness constraints during training (Zafar et al., 2017) or redesign objectives via adversarial learning (Zhang et al., 2018).\nRelated Work\n\nPolicy gradient methods have evolved from REINFORCE (Williams, 1992) to actor\u2013critic variants that stabilize learning with value baselines (Konda and Tsitsiklis, 2000; Mnih et al., 2016). Kaur et al. propose entropy regularization to encourage exploration, while other works develop trust region constraints to control policy updates (Schulman et al., 2015; Schulman et al., 2017). Off-policy corrections via importance sampling and variance reduction have also been investigated (Espeholt et al., 2018).\n\nIntroduction\n\nMultitask learning (MTL) can improve generalization by sharing representations across related tasks (Caruana, 1997; Ruder, 2017). Earlier sequence-to-sequence MTL frameworks (Johnson et al., 2017 demonstrated that multilingual models can leverage shared subword vocabularies to transfer knowledge across languages. Building on this, subsequent studies explored task-conditioned adapters to separate task-specific signals while keeping shared parameters compact (Singh and Roy, 2019). Our work extends these ideas to few-shot settings by aligning task prompts with adapter routing, enabling better transfer to unseen tasks (Vega and Park, 2021).\n\nDespite promising results, negative transfer remains a challenge when tasks are only weakly related (Pang and Li, 2020), motivating the need for routing mechanisms that detect and mitigate interference.\nRelated Work\n\nAutomatic abusive language detection relies on robust representations that capture context and pragmatics (Waseem and Hovy, 2016; Nobata et al., 2016). Early datasets focused on lexicon cues, which led to spurious correlations and poor generalization (Schmidt and Wiegand, 2017). The introduction of crowd-annotated corpora improved coverage but also introduced annotator bias and label disagreements [Davidson et al., 2017]. Recent work addresses these issues via debiasing objectives and counterfactual augmentation (Park et al., 2018; Dinan et al., 2020).\nIntroduction\n\nDomain adaptation seeks to bridge distribution shifts between source and target tasks (Ben-David et al., 2010; Ganin and Lempitsky, 2015). According to Zhou (2019 the use of pseudo-labels with confidence thresholds can mitigate negative transfer by filtering unreliable targets. Adversarial alignment and moment matching offer complementary solutions (Long et al., 2015; Tzeng et al., 2017).\n\nRecent approaches combine self-training with uncertainty estimation to further improve target-domain accuracy (Sohn et al., 2020; Rizve et al., 2021). Data augmentation and curriculum schedules provide additional benefits in challenging shifts (Volpi et al., 2018; Shu et al., 2018).\nIntroduction\n\nPersonalized recommendation has evolved from neighborhood methods to deep architectures that capture complex user\u2013item interactions (Koren et al., 2009; He et al., 2017). Context-aware models further improve click-through prediction by incorporating temporal and session features (Quadrana et al., 2018). Despite their success, such models can overfit short-term patterns and neglect long-term preferences (Zhou et al., 2019). We revisit sequence-aware recommendation and introduce a regularizer that stabilizes training while preserving recency signals (Nguyen et al., 2018\nRelated Work\n\nSelf-supervised pretraining has transformed speech recognition, enabling label-efficient fine-tuning (Baevski et al., 2020; Hsu et al., 2021). For streaming ASR, monotonic attention variants improve latency-accuracy trade-offs (Tsuchiya et al., 2019; Ma et al., 2021). Prior work suggests that layer-wise learning rates stabilize adaptation (Zhao 2020) and that data augmentation with noise profiles boosts robustness (Ko et al., 2015). Our study focuses on segment-level adapters that preserve causality.\nIntroduction\n\nReinforcement learning for robotic manipulation combines model-free exploration with demonstrations and priors (Levine et al., 2016; Kalashnikov et al., 2018). Policy constraints and offline datasets stabilize training (Fujimoto et al., 2019; Kumar et al., 2020). More recent work by Zhou et al., 2021) extends implicit Q-learning to dexterous hands, highlighting the importance of conservative estimates under limited coverage.\nRelated Work\n\nPrivacy-preserving learning techniques seek to protect user data while maintaining model utility. Differential privacy provides rigorous guarantees by bounding the contribution of any individual record (Dwork et al., 2014). Several studies adapt DP-SGD to large models, investigating the privacy\u2013utility trade-off at scale (Abadi et al., 2016; Bu et al., 2020). As shown in [Miller, 2020], gradient clipping strategies can significantly influence privacy accounting. We complement this line of work by analyzing optimizer choice under tight privacy budgets.\n",
        "completion": "[Miller, 2020]"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\nRelated Work\n\nMultimodal learning integrates signals from text, vision, and audio to improve downstream performance. Early fusion methods concatenate features learned independently (Park and Huang, 2018), whereas late fusion aggregates predictions from unimodal experts (Ahmed and Krishnan, 2019). Joint training approaches learn shared representations that align modalities (Wu et al., 2020).\n\nFollowing Johnson et al., we consider cross-modal attention as a mechanism to mediate information exchange between streams while preserving modality-specific information. Subsequent work demonstrated that aligning contrastive objectives with task losses yields further gains (Chen and Duarte, 2021; Lin et al., 2022). Our work differs by introducing a calibration loss that attenuates modality dominance under class imbalance.\nRelated Work\n\nNeural machine translation (NMT) benefits from multilingual transfer and shared subword vocabularies (Johnson et al., 2017; Conneau et al., 2020). Context-aware NMT uses document history to resolve coreference and lexical consistency (Voita et al., 2018; Maruf et al., 2019). Some works also incorporate visual cues for multimodal translation [12], though evidence of consistent gains is mixed (Calixto and Liu, 2017; Elliott, 2018). We revisit context modeling with lightweight cache mechanisms.\nRelated Work\n\nFairness in machine learning encompasses group fairness, individual fairness, and causal fairness perspectives (Dwork et al., 2012; Kusner et al., 2017; Barocas et al., 2019). Smith (2019; surveyed post-processing techniques that calibrate decision thresholds to equalize error rates across groups. Complementary approaches incorporate fairness constraints during training (Zafar et al., 2017) or redesign objectives via adversarial learning (Zhang et al., 2018).\nRelated Work\n\nPolicy gradient methods have evolved from REINFORCE (Williams, 1992) to actor\u2013critic variants that stabilize learning with value baselines (Konda and Tsitsiklis, 2000; Mnih et al., 2016). Kaur et al. propose entropy regularization to encourage exploration, while other works develop trust region constraints to control policy updates (Schulman et al., 2015; Schulman et al., 2017). Off-policy corrections via importance sampling and variance reduction have also been investigated (Espeholt et al., 2018).\n\nIntroduction\n\nMultitask learning (MTL) can improve generalization by sharing representations across related tasks (Caruana, 1997; Ruder, 2017). Earlier sequence-to-sequence MTL frameworks (Johnson et al., 2017 demonstrated that multilingual models can leverage shared subword vocabularies to transfer knowledge across languages. Building on this, subsequent studies explored task-conditioned adapters to separate task-specific signals while keeping shared parameters compact (Singh and Roy, 2019). Our work extends these ideas to few-shot settings by aligning task prompts with adapter routing, enabling better transfer to unseen tasks (Vega and Park, 2021).\n\nDespite promising results, negative transfer remains a challenge when tasks are only weakly related (Pang and Li, 2020), motivating the need for routing mechanisms that detect and mitigate interference.\nRelated Work\n\nAutomatic abusive language detection relies on robust representations that capture context and pragmatics (Waseem and Hovy, 2016; Nobata et al., 2016). Early datasets focused on lexicon cues, which led to spurious correlations and poor generalization (Schmidt and Wiegand, 2017). The introduction of crowd-annotated corpora improved coverage but also introduced annotator bias and label disagreements [Davidson et al., 2017]. Recent work addresses these issues via debiasing objectives and counterfactual augmentation (Park et al., 2018; Dinan et al., 2020).\nIntroduction\n\nDomain adaptation seeks to bridge distribution shifts between source and target tasks (Ben-David et al., 2010; Ganin and Lempitsky, 2015). According to Zhou (2019 the use of pseudo-labels with confidence thresholds can mitigate negative transfer by filtering unreliable targets. Adversarial alignment and moment matching offer complementary solutions (Long et al., 2015; Tzeng et al., 2017).\n\nRecent approaches combine self-training with uncertainty estimation to further improve target-domain accuracy (Sohn et al., 2020; Rizve et al., 2021). Data augmentation and curriculum schedules provide additional benefits in challenging shifts (Volpi et al., 2018; Shu et al., 2018).\nIntroduction\n\nPersonalized recommendation has evolved from neighborhood methods to deep architectures that capture complex user\u2013item interactions (Koren et al., 2009; He et al., 2017). Context-aware models further improve click-through prediction by incorporating temporal and session features (Quadrana et al., 2018). Despite their success, such models can overfit short-term patterns and neglect long-term preferences (Zhou et al., 2019). We revisit sequence-aware recommendation and introduce a regularizer that stabilizes training while preserving recency signals (Nguyen et al., 2018\nRelated Work\n\nSelf-supervised pretraining has transformed speech recognition, enabling label-efficient fine-tuning (Baevski et al., 2020; Hsu et al., 2021). For streaming ASR, monotonic attention variants improve latency-accuracy trade-offs (Tsuchiya et al., 2019; Ma et al., 2021). Prior work suggests that layer-wise learning rates stabilize adaptation (Zhao 2020) and that data augmentation with noise profiles boosts robustness (Ko et al., 2015). Our study focuses on segment-level adapters that preserve causality.\nIntroduction\n\nReinforcement learning for robotic manipulation combines model-free exploration with demonstrations and priors (Levine et al., 2016; Kalashnikov et al., 2018). Policy constraints and offline datasets stabilize training (Fujimoto et al., 2019; Kumar et al., 2020). More recent work by Zhou et al., 2021) extends implicit Q-learning to dexterous hands, highlighting the importance of conservative estimates under limited coverage.\nRelated Work\n\nPrivacy-preserving learning techniques seek to protect user data while maintaining model utility. Differential privacy provides rigorous guarantees by bounding the contribution of any individual record (Dwork et al., 2014). Several studies adapt DP-SGD to large models, investigating the privacy\u2013utility trade-off at scale (Abadi et al., 2016; Bu et al., 2020). As shown in [Miller, 2020], gradient clipping strategies can significantly influence privacy accounting. We complement this line of work by analyzing optimizer choice under tight privacy budgets.\nIntroduction\n\nSemantic segmentation assigns a class label to every pixel, enabling scene understanding for robotics and autonomous driving (Long et al., 2015; Chen et al., 2018). Encoder\u2013decoder networks with dilated convolutions capture multi-scale context efficiently (Yu and Koltun, 2016; Zhao et al., 2017). Datasets such as Cityscapes and ADE20K have standardized evaluation protocols, but domain shift across weather and sensors persists (Cordts et al., 2016; Zhou et al., 2017). We adopt a standard author\u2013year style throughout this work and reference the dataset splits in [12] for comparison to prior methods.\n",
        "completion": "[12]"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n    c) Formatting issues appear as either citations or parts of a citation.\n    Examples of formatting issues include:\n        i) Narrative citation missing year: \u201cVatswani et al.\u201d -> should be \u201cVatswani et al. (2020)\u201d\n        ii) Wrong citation style: \u201cIn (Vatswani et al., 2019)\u201d -> should be \u201cin Vatswani et al. (2019)\u201d\n        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\n\n        Example text: \n        Introduction\n\nRecent progress in multimodal learning has focused on aligning visual and textual representations under weak supervision. In (Smith et al., 2019), the authors demonstrate that contrastive objectives benefit from temperature scaling and large batch sizes. Building on this, follow-up work proposes momentum encoders and memory banks to expand the pool of negatives without increasing GPU memory. However, these methods can introduce bias when the queue distribution lags behind the current model state, leading to instability under rapid domain shifts.\n\nOur approach addresses this by coupling adaptive temperature with batch-aware debiasing, enabling robust training at scale while maintaining consistent downstream transfer.\nRelated Work\n\nTemporal representation learning benefits from self-supervised objectives such as contrastive prediction and masked modeling (Oord et al., 2018; Bai et al., 2018). In forecasting with limited labels, as shown by Lee & Kim (2021), pretraining on proxy tasks reduces error on long horizons. Recent advances also leverage frequency-domain augmentations to capture seasonality (Wu et al., 2021).\n\nOur approach combines masked forecasting with spectral consistency to improve robustness under distribution shift.\nRelated Work\n\nTopic modeling traditionally relies on probabilistic latent variable models such as LDA to uncover interpretable themes in text corpora (Blei et al., 2003). Nonnegative matrix factorization provides an alternative decomposition with competitive coherence in some settings (Lee and Seung, 2001; Arora et al., 2012). Dynamic topic models capture temporal evolution of topics (Blei and Lafferty, 2006), while neural variational approaches scale to large corpora with amortized inference (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nRecent work explores embedding-based topics by clustering contextualized representations from pretrained language models (Grootendorst, 2022; Angelov, 2020). For short texts, biterm and self-aggregation strategies mitigate sparsity (Yan et al., 2013; Zuo et al., 2016). Evaluation frameworks emphasize coherence and diversity, with human-in-the-loop curation to refine topic quality (Ares et al., 2019). Several studies (Basu, 2014 and Ortega et al., 2015) examine cross-domain adaptation but leave open questions about robustness under domain shift.\n\nWe propose a contrastive alignment objective that regularizes topic assignments across domains, improving both coherence and transfer accuracy.\nRelated Work\n\nOptimization tricks for stabilizing sequence-to-sequence training include label smoothing (Serrano and Cho, 2018) and variance reduction via gradient clipping (Ng and Patel, 2017). Klein and Murray 3 compare heuristic schedules for learning rates and conclude that adaptive warmup is competitive with cosine decay. Subsequent research integrates trust region updates into Transformer training to further reduce variance (Huang and Patel, 2020). Unlike these approaches, our method targets batch-level gradient noise by adaptively reweighting difficult examples (Ono and Li, 2021), which we show complements both label smoothing and clipping.\n\nFinally, we note that architectural modifications such as reversible layers (Foster and Wang, 2019) can reduce memory costs, but they do not directly address optimization instability under long sequences.\nRelated Work\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity and equalized odds (Hardt et al., 2016; Dwork et al., 2012). Individual fairness enforces Lipschitz-like constraints over similar individuals (Dwork et al., 2012). Auditing methods propose post hoc adjustments to decision thresholds (Pleiss et al., 2017) and counterfactual explanations to diagnose harms (Kusner et al., 2017). Popular texts highlight societal risks of unregulated models (O'Neil, 2015 Kearns and Roth, 2019), while recent work formalizes trade-offs between accuracy and fairness under distribution shift (Menon and Williamson, 2018; Zhao and Gordon, 2019). Our approach jointly optimizes a calibrated surrogate with constraints enforced via Lagrangian duality, improving Pareto efficiency across metrics.\nRelated Work\n\nPersonalized recommendation has leveraged sequential signals to capture evolving user preferences (Hidasi et al., 2016; Kang and McAuley, 2018). Hybrid models that integrate content features with sequences show improved robustness to cold starts (Cheng et al., 2016; Sun et al., 2019). Recent contrastive objectives have further enhanced representation quality for sparse interactions (Xie et al., 2020; (Lopez et al. 2022)).\n\nNevertheless, bias amplification remains a persistent issue, particularly for popularity-skewed catalogs (Chen et al., 2020; Abdollahpouri et al., 2019). We address this by introducing counterfactual evaluation protocols and debiasing strategies that trade off short-term engagement for long-term diversity.\nIntroduction\n\nMeasuring user experience requires integrating behavioral logs with subjective reports (Brooke, 1996; Sauro and Lewis, 2012). Prior work has emphasized subjective measures (Taylor and Kim, 2020) but often neglects temporal dynamics that shape satisfaction over sessions (Lopez and Hart, 2021). We propose a session-aware UX index that aggregates micro-interactions into interpretable segments aligned with task goals (Nakamura et al., 2022).\n\nRelated Work\n\nHybrid UX metrics combine surveys with interaction features (Zhang and Patel, 2019). Our approach is closest to engagement curves (Davis and Chen, 2020) but introduces causal controls to adjust for confounders like task difficulty.\nRelated Work\n\nMedical image segmentation has progressed through encoder-decoder networks (Rossi and Patel, 2016) and attention-enhanced decoders (Ng and Costa, 2019). Multi-task formulations regularize boundaries via auxiliary losses (Sato and Lin, 2020), while test-time adaptation mitigates scanner shift (Grewal et al., 2021). Public benchmarks such as MedContours and VascSet (Ortiz et al., 2020) catalyzed reproducibility. However, annotations remain scarce; semi-supervised consistency (Meyer and Zhao, 2021) and weak labels from reports (Qiao and Singh, 2022) address this gap. As noted by Liang et al. 2 during their dataset release, slice thickness variation complicates cross-center generalization. We address this by harmonizing volumetric spacing and introducing a span-aware loss that is robust to anisotropy.\nIntroduction\n\nProbabilistic forecasting in time series has shifted from classical ARIMA-based models to deep generative approaches that capture complex seasonality and covariate effects (Salinas et al., 2020; Rangapuram et al., 2018). Sequence-to-sequence and transformer-based models support multi-horizon prediction with covariate conditioning (Lim et al., 2021; Zhou et al., 2021). Calibration and sharpness are evaluated using proper scoring rules such as CRPS (Gneiting and Raftery, 2007).\n\nExogenous variables can be incorporated through embeddings and feature selection techniques to handle large-scale deployments (Sen et al., 2019). Recent surveys highlight the importance of probabilistic evaluation and reproducibility (Makridakis et al., 2022). Prior work (Olsen, 2017, Patel, 2019) discusses regime shifts and drift adaptation in retail demand data, motivating our covariate shift detector for intermittent series.\n\nRelated Work\n\nMultimodal learning integrates signals from text, vision, and audio to improve downstream performance. Early fusion methods concatenate features learned independently (Park and Huang, 2018), whereas late fusion aggregates predictions from unimodal experts (Ahmed and Krishnan, 2019). Joint training approaches learn shared representations that align modalities (Wu et al., 2020).\n\nFollowing Johnson et al., we consider cross-modal attention as a mechanism to mediate information exchange between streams while preserving modality-specific information. Subsequent work demonstrated that aligning contrastive objectives with task losses yields further gains (Chen and Duarte, 2021; Lin et al., 2022). Our work differs by introducing a calibration loss that attenuates modality dominance under class imbalance.\nRelated Work\n\nNeural machine translation (NMT) benefits from multilingual transfer and shared subword vocabularies (Johnson et al., 2017; Conneau et al., 2020). Context-aware NMT uses document history to resolve coreference and lexical consistency (Voita et al., 2018; Maruf et al., 2019). Some works also incorporate visual cues for multimodal translation [12], though evidence of consistent gains is mixed (Calixto and Liu, 2017; Elliott, 2018). We revisit context modeling with lightweight cache mechanisms.\nRelated Work\n\nFairness in machine learning encompasses group fairness, individual fairness, and causal fairness perspectives (Dwork et al., 2012; Kusner et al., 2017; Barocas et al., 2019). Smith (2019; surveyed post-processing techniques that calibrate decision thresholds to equalize error rates across groups. Complementary approaches incorporate fairness constraints during training (Zafar et al., 2017) or redesign objectives via adversarial learning (Zhang et al., 2018).\nRelated Work\n\nPolicy gradient methods have evolved from REINFORCE (Williams, 1992) to actor\u2013critic variants that stabilize learning with value baselines (Konda and Tsitsiklis, 2000; Mnih et al., 2016). Kaur et al. propose entropy regularization to encourage exploration, while other works develop trust region constraints to control policy updates (Schulman et al., 2015; Schulman et al., 2017). Off-policy corrections via importance sampling and variance reduction have also been investigated (Espeholt et al., 2018).\n\nIntroduction\n\nMultitask learning (MTL) can improve generalization by sharing representations across related tasks (Caruana, 1997; Ruder, 2017). Earlier sequence-to-sequence MTL frameworks (Johnson et al., 2017 demonstrated that multilingual models can leverage shared subword vocabularies to transfer knowledge across languages. Building on this, subsequent studies explored task-conditioned adapters to separate task-specific signals while keeping shared parameters compact (Singh and Roy, 2019). Our work extends these ideas to few-shot settings by aligning task prompts with adapter routing, enabling better transfer to unseen tasks (Vega and Park, 2021).\n\nDespite promising results, negative transfer remains a challenge when tasks are only weakly related (Pang and Li, 2020), motivating the need for routing mechanisms that detect and mitigate interference.\nRelated Work\n\nAutomatic abusive language detection relies on robust representations that capture context and pragmatics (Waseem and Hovy, 2016; Nobata et al., 2016). Early datasets focused on lexicon cues, which led to spurious correlations and poor generalization (Schmidt and Wiegand, 2017). The introduction of crowd-annotated corpora improved coverage but also introduced annotator bias and label disagreements [Davidson et al., 2017]. Recent work addresses these issues via debiasing objectives and counterfactual augmentation (Park et al., 2018; Dinan et al., 2020).\nIntroduction\n\nDomain adaptation seeks to bridge distribution shifts between source and target tasks (Ben-David et al., 2010; Ganin and Lempitsky, 2015). According to Zhou (2019 the use of pseudo-labels with confidence thresholds can mitigate negative transfer by filtering unreliable targets. Adversarial alignment and moment matching offer complementary solutions (Long et al., 2015; Tzeng et al., 2017).\n\nRecent approaches combine self-training with uncertainty estimation to further improve target-domain accuracy (Sohn et al., 2020; Rizve et al., 2021). Data augmentation and curriculum schedules provide additional benefits in challenging shifts (Volpi et al., 2018; Shu et al., 2018).\nIntroduction\n\nPersonalized recommendation has evolved from neighborhood methods to deep architectures that capture complex user\u2013item interactions (Koren et al., 2009; He et al., 2017). Context-aware models further improve click-through prediction by incorporating temporal and session features (Quadrana et al., 2018). Despite their success, such models can overfit short-term patterns and neglect long-term preferences (Zhou et al., 2019). We revisit sequence-aware recommendation and introduce a regularizer that stabilizes training while preserving recency signals (Nguyen et al., 2018\nRelated Work\n\nSelf-supervised pretraining has transformed speech recognition, enabling label-efficient fine-tuning (Baevski et al., 2020; Hsu et al., 2021). For streaming ASR, monotonic attention variants improve latency-accuracy trade-offs (Tsuchiya et al., 2019; Ma et al., 2021). Prior work suggests that layer-wise learning rates stabilize adaptation (Zhao 2020) and that data augmentation with noise profiles boosts robustness (Ko et al., 2015). Our study focuses on segment-level adapters that preserve causality.\nIntroduction\n\nReinforcement learning for robotic manipulation combines model-free exploration with demonstrations and priors (Levine et al., 2016; Kalashnikov et al., 2018). Policy constraints and offline datasets stabilize training (Fujimoto et al., 2019; Kumar et al., 2020). More recent work by Zhou et al., 2021) extends implicit Q-learning to dexterous hands, highlighting the importance of conservative estimates under limited coverage.\nRelated Work\n\nPrivacy-preserving learning techniques seek to protect user data while maintaining model utility. Differential privacy provides rigorous guarantees by bounding the contribution of any individual record (Dwork et al., 2014). Several studies adapt DP-SGD to large models, investigating the privacy\u2013utility trade-off at scale (Abadi et al., 2016; Bu et al., 2020). As shown in [Miller, 2020], gradient clipping strategies can significantly influence privacy accounting. We complement this line of work by analyzing optimizer choice under tight privacy budgets.\nIntroduction\n\nSemantic segmentation assigns a class label to every pixel, enabling scene understanding for robotics and autonomous driving (Long et al., 2015; Chen et al., 2018). Encoder\u2013decoder networks with dilated convolutions capture multi-scale context efficiently (Yu and Koltun, 2016; Zhao et al., 2017). Datasets such as Cityscapes and ADE20K have standardized evaluation protocols, but domain shift across weather and sensors persists (Cordts et al., 2016; Zhou et al., 2017). We adopt a standard author\u2013year style throughout this work and reference the dataset splits in [12] for comparison to prior methods.\nRelated Work\n\nTransformer-based pretraining has advanced a wide range of NLP tasks (Vaswani et al., 2017; Peters et al., 2018). Among encoder-only models, BERT Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are commonly adopted for representation learning, while XLNet (Yang et al., 2019) explores permutation-based objectives to capture bidirectional context. For generative modeling, T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) have shown strong results in sequence-to-sequence benchmarks.\n\nRecent work also emphasizes domain adaptation and task-specific fine-tuning strategies (Gururangan et al., 2020; Ruder et al., 2019). Concurrently, efficient training techniques such as knowledge distillation (Hinton et al., 2015) and parameter-efficient tuning (Houlsby et al., 2019; Lester et al., 2021) reduce compute while maintaining performance.\n",
        "completion": "Devlin et al., 2019)"
    }
]