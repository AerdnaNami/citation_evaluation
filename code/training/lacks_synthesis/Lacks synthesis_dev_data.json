[
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\n",
        "completion": "Classical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\n",
        "completion": "Graph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\n",
        "completion": "Molecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\n",
        "completion": "Classical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\n",
        "completion": "Attacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\n",
        "completion": "Recent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\n",
        "completion": "Deep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\n",
        "completion": "Early session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\n",
        "completion": "Post-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\n",
        "completion": "Classical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\n",
        "completion": "Recent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\n",
        "completion": "Network anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\nRelated Work\n\nTransformer-based vision models have reshaped perception tasks by scaling receptive fields and enabling flexible token-level interactions. Detection, in particular, benefits from global context but faces challenges with small objects and long-tailed categories.\n\nObject detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022).\n\nIn this paper we introduce TokenScope, a training framework that selectively amplifies informative tokens via gradient-guided masking during pretraining. We measure improvements on COCO and LVIS, focusing on rare categories and small object recall.\n",
        "completion": "Object detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\nRelated Work\n\nTransformer-based vision models have reshaped perception tasks by scaling receptive fields and enabling flexible token-level interactions. Detection, in particular, benefits from global context but faces challenges with small objects and long-tailed categories.\n\nObject detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022).\n\nIn this paper we introduce TokenScope, a training framework that selectively amplifies informative tokens via gradient-guided masking during pretraining. We measure improvements on COCO and LVIS, focusing on rare categories and small object recall.\nRelated Work\n\nForecasting energy demand underpins grid reliability and market operations. Models must accommodate seasonality, weather effects, and nonstationarities induced by evolving consumption patterns.\n\nShort-term load forecasting has been addressed with autoregressive models (Hyndman and Athanasopoulos, 2018), LSTM and GRU-based sequence models (Lai et al., 2018; Kong et al., 2019), temporal convolutional networks (Bai et al., 2018), and attention mechanisms for capturing long-range dependencies (Qin et al., 2017; Vaswani et al., 2017). Exogenous variables such as temperature, calendar effects, and economic indicators are commonly incorporated (Hong et al., 2016; Taieb and Hyndman, 2014).\n\nProbabilistic forecasting has further been explored through quantile regression (Taylor, 2000) and deep generative models (Salinas et al., 2020), focusing on calibrated uncertainty.\n\nIn contrast, our study examines adaptive context windows driven by distribution shift tests to improve robustness without increasing model size.\n\n",
        "completion": "Short-term load forecasting has been addressed with autoregressive models (Hyndman and Athanasopoulos, 2018), LSTM and GRU-based sequence models (Lai et al., 2018; Kong et al., 2019), temporal convolutional networks (Bai et al., 2018), and attention mechanisms for capturing long-range dependencies (Qin et al., 2017; Vaswani et al., 2017). Exogenous variables such as temperature, calendar effects, and economic indicators are commonly incorporated (Hong et al., 2016; Taieb and Hyndman, 2014)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\nRelated Work\n\nTransformer-based vision models have reshaped perception tasks by scaling receptive fields and enabling flexible token-level interactions. Detection, in particular, benefits from global context but faces challenges with small objects and long-tailed categories.\n\nObject detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022).\n\nIn this paper we introduce TokenScope, a training framework that selectively amplifies informative tokens via gradient-guided masking during pretraining. We measure improvements on COCO and LVIS, focusing on rare categories and small object recall.\nRelated Work\n\nForecasting energy demand underpins grid reliability and market operations. Models must accommodate seasonality, weather effects, and nonstationarities induced by evolving consumption patterns.\n\nShort-term load forecasting has been addressed with autoregressive models (Hyndman and Athanasopoulos, 2018), LSTM and GRU-based sequence models (Lai et al., 2018; Kong et al., 2019), temporal convolutional networks (Bai et al., 2018), and attention mechanisms for capturing long-range dependencies (Qin et al., 2017; Vaswani et al., 2017). Exogenous variables such as temperature, calendar effects, and economic indicators are commonly incorporated (Hong et al., 2016; Taieb and Hyndman, 2014).\n\nProbabilistic forecasting has further been explored through quantile regression (Taylor, 2000) and deep generative models (Salinas et al., 2020), focusing on calibrated uncertainty.\n\nIn contrast, our study examines adaptive context windows driven by distribution shift tests to improve robustness without increasing model size.\n\nRelated Work\n\nExploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward and long-horizon tasks. Prior methods can be grouped by their principles, such as optimism, information gain, and novelty seeking.\n\nClassic exploration strategies include epsilon-greedy and softmax action selection (Sutton and Barto, 2018). Optimism-driven approaches such as UCB and posterior sampling provide theoretical guarantees (Auer, 2002; Osband et al., 2013). Intrinsic motivation methods reward novelty via prediction error or state visitation counts (Pathak et al., 2017; Bellemare et al., 2016).\n\nWe investigate scalable, task-agnostic exploration bonuses derived from successor features, aiming to balance coverage with stability in deep RL benchmarks.\n",
        "completion": "Classic exploration strategies include epsilon-greedy and softmax action selection (Sutton and Barto, 2018). Optimism-driven approaches such as UCB and posterior sampling provide theoretical guarantees (Auer, 2002; Osband et al., 2013). Intrinsic motivation methods reward novelty via prediction error or state visitation counts (Pathak et al., 2017; Bellemare et al., 2016)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\nRelated Work\n\nTransformer-based vision models have reshaped perception tasks by scaling receptive fields and enabling flexible token-level interactions. Detection, in particular, benefits from global context but faces challenges with small objects and long-tailed categories.\n\nObject detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022).\n\nIn this paper we introduce TokenScope, a training framework that selectively amplifies informative tokens via gradient-guided masking during pretraining. We measure improvements on COCO and LVIS, focusing on rare categories and small object recall.\nRelated Work\n\nForecasting energy demand underpins grid reliability and market operations. Models must accommodate seasonality, weather effects, and nonstationarities induced by evolving consumption patterns.\n\nShort-term load forecasting has been addressed with autoregressive models (Hyndman and Athanasopoulos, 2018), LSTM and GRU-based sequence models (Lai et al., 2018; Kong et al., 2019), temporal convolutional networks (Bai et al., 2018), and attention mechanisms for capturing long-range dependencies (Qin et al., 2017; Vaswani et al., 2017). Exogenous variables such as temperature, calendar effects, and economic indicators are commonly incorporated (Hong et al., 2016; Taieb and Hyndman, 2014).\n\nProbabilistic forecasting has further been explored through quantile regression (Taylor, 2000) and deep generative models (Salinas et al., 2020), focusing on calibrated uncertainty.\n\nIn contrast, our study examines adaptive context windows driven by distribution shift tests to improve robustness without increasing model size.\n\nRelated Work\n\nExploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward and long-horizon tasks. Prior methods can be grouped by their principles, such as optimism, information gain, and novelty seeking.\n\nClassic exploration strategies include epsilon-greedy and softmax action selection (Sutton and Barto, 2018). Optimism-driven approaches such as UCB and posterior sampling provide theoretical guarantees (Auer, 2002; Osband et al., 2013). Intrinsic motivation methods reward novelty via prediction error or state visitation counts (Pathak et al., 2017; Bellemare et al., 2016).\n\nWe investigate scalable, task-agnostic exploration bonuses derived from successor features, aiming to balance coverage with stability in deep RL benchmarks.\nRelated Work\n\nAccessible interaction for blind and low-vision users often centers on screen readers that translate visual interfaces into auditory or tactile modalities. Prior work explores both input and output channels to improve efficiency and satisfaction.\n\nScreen reader interaction has been studied through command vocabularies (Perez and Green, 2016), gesture mappings on touch devices (Hussain et al., 2018), and auditory feedback designs (Nguyen and Hayes, 2019; Kim et al., 2020). Studies on blind users\u2019 navigation strategies report mixed preferences for linear and hierarchical traversal (Lopez et al., 2017; Sato and Ortiz, 2019).\n\nBeyond individual components, some work examines developer tooling for accessibility annotations (Barker and White, 2021) and automated audits (Ibrahim et al., 2020).\n",
        "completion": "Screen reader interaction has been studied through command vocabularies (Perez and Green, 2016), gesture mappings on touch devices (Hussain et al., 2018), and auditory feedback designs (Nguyen and Hayes, 2019; Kim et al., 2020). Studies on blind users\u2019 navigation strategies report mixed preferences for linear and hierarchical traversal (Lopez et al., 2017; Sato and Ortiz, 2019)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\nRelated Work\n\nTransformer-based vision models have reshaped perception tasks by scaling receptive fields and enabling flexible token-level interactions. Detection, in particular, benefits from global context but faces challenges with small objects and long-tailed categories.\n\nObject detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022).\n\nIn this paper we introduce TokenScope, a training framework that selectively amplifies informative tokens via gradient-guided masking during pretraining. We measure improvements on COCO and LVIS, focusing on rare categories and small object recall.\nRelated Work\n\nForecasting energy demand underpins grid reliability and market operations. Models must accommodate seasonality, weather effects, and nonstationarities induced by evolving consumption patterns.\n\nShort-term load forecasting has been addressed with autoregressive models (Hyndman and Athanasopoulos, 2018), LSTM and GRU-based sequence models (Lai et al., 2018; Kong et al., 2019), temporal convolutional networks (Bai et al., 2018), and attention mechanisms for capturing long-range dependencies (Qin et al., 2017; Vaswani et al., 2017). Exogenous variables such as temperature, calendar effects, and economic indicators are commonly incorporated (Hong et al., 2016; Taieb and Hyndman, 2014).\n\nProbabilistic forecasting has further been explored through quantile regression (Taylor, 2000) and deep generative models (Salinas et al., 2020), focusing on calibrated uncertainty.\n\nIn contrast, our study examines adaptive context windows driven by distribution shift tests to improve robustness without increasing model size.\n\nRelated Work\n\nExploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward and long-horizon tasks. Prior methods can be grouped by their principles, such as optimism, information gain, and novelty seeking.\n\nClassic exploration strategies include epsilon-greedy and softmax action selection (Sutton and Barto, 2018). Optimism-driven approaches such as UCB and posterior sampling provide theoretical guarantees (Auer, 2002; Osband et al., 2013). Intrinsic motivation methods reward novelty via prediction error or state visitation counts (Pathak et al., 2017; Bellemare et al., 2016).\n\nWe investigate scalable, task-agnostic exploration bonuses derived from successor features, aiming to balance coverage with stability in deep RL benchmarks.\nRelated Work\n\nAccessible interaction for blind and low-vision users often centers on screen readers that translate visual interfaces into auditory or tactile modalities. Prior work explores both input and output channels to improve efficiency and satisfaction.\n\nScreen reader interaction has been studied through command vocabularies (Perez and Green, 2016), gesture mappings on touch devices (Hussain et al., 2018), and auditory feedback designs (Nguyen and Hayes, 2019; Kim et al., 2020). Studies on blind users\u2019 navigation strategies report mixed preferences for linear and hierarchical traversal (Lopez et al., 2017; Sato and Ortiz, 2019).\n\nBeyond individual components, some work examines developer tooling for accessibility annotations (Barker and White, 2021) and automated audits (Ibrahim et al., 2020).\nRelated Work\n\nSelf-supervised representation learning in computer vision has rapidly progressed via contrastive objectives, clustering mechanisms, and generative masking formulations. These approaches reduce reliance on labeled data and serve as foundations for downstream tasks including classification and detection.\n\nContrastive pretraining methods such as instance discrimination and momentum contrast learn invariant visual features from large unlabeled corpora (Nguyen et al., 2020; Ramos and Ortega, 2021), while clustering-based approaches leverage online assignment to discover semantic groups (Li and Sun, 2021; Patel et al., 2022). Masked image modeling reconstructs missing patches to capture spatial structure (Kumar et al., 2022; Zhou and Fang, 2023).\n\nWe present a simple training recipe that combines lightweight augmentations with scalable batch schedules for improved linear probing performance.\n",
        "completion": "Contrastive pretraining methods such as instance discrimination and momentum contrast learn invariant visual features from large unlabeled corpora (Nguyen et al., 2020; Ramos and Ortega, 2021), while clustering-based approaches leverage online assignment to discover semantic groups (Li and Sun, 2021; Patel et al., 2022). Masked image modeling reconstructs missing patches to capture spatial structure (Kumar et al., 2022; Zhou and Fang, 2023)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\nRelated Work\n\nTransformer-based vision models have reshaped perception tasks by scaling receptive fields and enabling flexible token-level interactions. Detection, in particular, benefits from global context but faces challenges with small objects and long-tailed categories.\n\nObject detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022).\n\nIn this paper we introduce TokenScope, a training framework that selectively amplifies informative tokens via gradient-guided masking during pretraining. We measure improvements on COCO and LVIS, focusing on rare categories and small object recall.\nRelated Work\n\nForecasting energy demand underpins grid reliability and market operations. Models must accommodate seasonality, weather effects, and nonstationarities induced by evolving consumption patterns.\n\nShort-term load forecasting has been addressed with autoregressive models (Hyndman and Athanasopoulos, 2018), LSTM and GRU-based sequence models (Lai et al., 2018; Kong et al., 2019), temporal convolutional networks (Bai et al., 2018), and attention mechanisms for capturing long-range dependencies (Qin et al., 2017; Vaswani et al., 2017). Exogenous variables such as temperature, calendar effects, and economic indicators are commonly incorporated (Hong et al., 2016; Taieb and Hyndman, 2014).\n\nProbabilistic forecasting has further been explored through quantile regression (Taylor, 2000) and deep generative models (Salinas et al., 2020), focusing on calibrated uncertainty.\n\nIn contrast, our study examines adaptive context windows driven by distribution shift tests to improve robustness without increasing model size.\n\nRelated Work\n\nExploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward and long-horizon tasks. Prior methods can be grouped by their principles, such as optimism, information gain, and novelty seeking.\n\nClassic exploration strategies include epsilon-greedy and softmax action selection (Sutton and Barto, 2018). Optimism-driven approaches such as UCB and posterior sampling provide theoretical guarantees (Auer, 2002; Osband et al., 2013). Intrinsic motivation methods reward novelty via prediction error or state visitation counts (Pathak et al., 2017; Bellemare et al., 2016).\n\nWe investigate scalable, task-agnostic exploration bonuses derived from successor features, aiming to balance coverage with stability in deep RL benchmarks.\nRelated Work\n\nAccessible interaction for blind and low-vision users often centers on screen readers that translate visual interfaces into auditory or tactile modalities. Prior work explores both input and output channels to improve efficiency and satisfaction.\n\nScreen reader interaction has been studied through command vocabularies (Perez and Green, 2016), gesture mappings on touch devices (Hussain et al., 2018), and auditory feedback designs (Nguyen and Hayes, 2019; Kim et al., 2020). Studies on blind users\u2019 navigation strategies report mixed preferences for linear and hierarchical traversal (Lopez et al., 2017; Sato and Ortiz, 2019).\n\nBeyond individual components, some work examines developer tooling for accessibility annotations (Barker and White, 2021) and automated audits (Ibrahim et al., 2020).\nRelated Work\n\nSelf-supervised representation learning in computer vision has rapidly progressed via contrastive objectives, clustering mechanisms, and generative masking formulations. These approaches reduce reliance on labeled data and serve as foundations for downstream tasks including classification and detection.\n\nContrastive pretraining methods such as instance discrimination and momentum contrast learn invariant visual features from large unlabeled corpora (Nguyen et al., 2020; Ramos and Ortega, 2021), while clustering-based approaches leverage online assignment to discover semantic groups (Li and Sun, 2021; Patel et al., 2022). Masked image modeling reconstructs missing patches to capture spatial structure (Kumar et al., 2022; Zhou and Fang, 2023).\n\nWe present a simple training recipe that combines lightweight augmentations with scalable batch schedules for improved linear probing performance.\nIntroduction\n\nCredit scoring models risk entrenching existing disparities if sensitive attributes correlate with default risk via historical bias or structural inequities. Regulators demand transparency and non-discrimination, yet practical guidance on fair deployment remains limited. A large body of work formalizes group fairness through demographic parity, equalized odds, and calibration (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017). Individual fairness notions and causal criteria offer complementary perspectives but require strong assumptions (Dwork et al., 2012; Kusner et al., 2017).\n\nWe examine post-hoc thresholding under multiple regulatory constraints and propose an auditing toolkit that integrates counterfactual sensitivity analysis with disclosure-ready summaries.\n",
        "completion": "A large body of work formalizes group fairness through demographic parity, equalized odds, and calibration (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\nRelated Work\n\nTransformer-based vision models have reshaped perception tasks by scaling receptive fields and enabling flexible token-level interactions. Detection, in particular, benefits from global context but faces challenges with small objects and long-tailed categories.\n\nObject detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022).\n\nIn this paper we introduce TokenScope, a training framework that selectively amplifies informative tokens via gradient-guided masking during pretraining. We measure improvements on COCO and LVIS, focusing on rare categories and small object recall.\nRelated Work\n\nForecasting energy demand underpins grid reliability and market operations. Models must accommodate seasonality, weather effects, and nonstationarities induced by evolving consumption patterns.\n\nShort-term load forecasting has been addressed with autoregressive models (Hyndman and Athanasopoulos, 2018), LSTM and GRU-based sequence models (Lai et al., 2018; Kong et al., 2019), temporal convolutional networks (Bai et al., 2018), and attention mechanisms for capturing long-range dependencies (Qin et al., 2017; Vaswani et al., 2017). Exogenous variables such as temperature, calendar effects, and economic indicators are commonly incorporated (Hong et al., 2016; Taieb and Hyndman, 2014).\n\nProbabilistic forecasting has further been explored through quantile regression (Taylor, 2000) and deep generative models (Salinas et al., 2020), focusing on calibrated uncertainty.\n\nIn contrast, our study examines adaptive context windows driven by distribution shift tests to improve robustness without increasing model size.\n\nRelated Work\n\nExploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward and long-horizon tasks. Prior methods can be grouped by their principles, such as optimism, information gain, and novelty seeking.\n\nClassic exploration strategies include epsilon-greedy and softmax action selection (Sutton and Barto, 2018). Optimism-driven approaches such as UCB and posterior sampling provide theoretical guarantees (Auer, 2002; Osband et al., 2013). Intrinsic motivation methods reward novelty via prediction error or state visitation counts (Pathak et al., 2017; Bellemare et al., 2016).\n\nWe investigate scalable, task-agnostic exploration bonuses derived from successor features, aiming to balance coverage with stability in deep RL benchmarks.\nRelated Work\n\nAccessible interaction for blind and low-vision users often centers on screen readers that translate visual interfaces into auditory or tactile modalities. Prior work explores both input and output channels to improve efficiency and satisfaction.\n\nScreen reader interaction has been studied through command vocabularies (Perez and Green, 2016), gesture mappings on touch devices (Hussain et al., 2018), and auditory feedback designs (Nguyen and Hayes, 2019; Kim et al., 2020). Studies on blind users\u2019 navigation strategies report mixed preferences for linear and hierarchical traversal (Lopez et al., 2017; Sato and Ortiz, 2019).\n\nBeyond individual components, some work examines developer tooling for accessibility annotations (Barker and White, 2021) and automated audits (Ibrahim et al., 2020).\nRelated Work\n\nSelf-supervised representation learning in computer vision has rapidly progressed via contrastive objectives, clustering mechanisms, and generative masking formulations. These approaches reduce reliance on labeled data and serve as foundations for downstream tasks including classification and detection.\n\nContrastive pretraining methods such as instance discrimination and momentum contrast learn invariant visual features from large unlabeled corpora (Nguyen et al., 2020; Ramos and Ortega, 2021), while clustering-based approaches leverage online assignment to discover semantic groups (Li and Sun, 2021; Patel et al., 2022). Masked image modeling reconstructs missing patches to capture spatial structure (Kumar et al., 2022; Zhou and Fang, 2023).\n\nWe present a simple training recipe that combines lightweight augmentations with scalable batch schedules for improved linear probing performance.\nIntroduction\n\nCredit scoring models risk entrenching existing disparities if sensitive attributes correlate with default risk via historical bias or structural inequities. Regulators demand transparency and non-discrimination, yet practical guidance on fair deployment remains limited. A large body of work formalizes group fairness through demographic parity, equalized odds, and calibration (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017). Individual fairness notions and causal criteria offer complementary perspectives but require strong assumptions (Dwork et al., 2012; Kusner et al., 2017).\n\nWe examine post-hoc thresholding under multiple regulatory constraints and propose an auditing toolkit that integrates counterfactual sensitivity analysis with disclosure-ready summaries.\nIntroduction\n\nRecommender systems play a central role in modern content platforms, where the objective is to match users with relevant items under dynamic and sparse interactions. Recent advances in representation learning have enabled models to leverage higher-order structure and contextual signals at scale.\n\nGraph-based recommenders have attracted significant attention. Early methods propagated user and item signals over bipartite graphs (Yu et al., 2014; He et al., 2015), while more recent approaches use graph neural networks to perform message passing on user\u2013item interaction graphs (Ying et al., 2018; Wang et al., 2019; He et al., 2020; Wu et al., 2021). Session-based recommendation has also been modeled with gated and attention-enhanced GNNs to capture short-term dependencies (Li et al., 2017; Xu et al., 2019; Pan et al., 2020). Knowledge-graph-enhanced recommenders incorporate side relations to improve accuracy and explainability (Zhang et al., 2018; Wang et al., 2021; Chen et al., 2022).\n\nIn this paper, we investigate the tradeoffs between accuracy and latency for graph neural recommenders in high-throughput systems. We present a decoupled encoder architecture with cached item embeddings and on-demand lightweight user aggregation, enabling millisecond inference while preserving predictive power.\n",
        "completion": "Graph-based recommenders have attracted significant attention. Early methods propagated user and item signals over bipartite graphs (Yu et al., 2014; He et al., 2015), while more recent approaches use graph neural networks to perform message passing on user\u2013item interaction graphs (Ying et al., 2018; Wang et al., 2019; He et al., 2020; Wu et al., 2021). Session-based recommendation has also been modeled with gated and attention-enhanced GNNs to capture short-term dependencies (Li et al., 2017; Xu et al., 2019; Pan et al., 2020). Knowledge-graph-enhanced recommenders incorporate side relations to improve accuracy and explainability (Zhang et al., 2018; Wang et al., 2021; Chen et al., 2022)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\nRelated Work\n\nTransformer-based vision models have reshaped perception tasks by scaling receptive fields and enabling flexible token-level interactions. Detection, in particular, benefits from global context but faces challenges with small objects and long-tailed categories.\n\nObject detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022).\n\nIn this paper we introduce TokenScope, a training framework that selectively amplifies informative tokens via gradient-guided masking during pretraining. We measure improvements on COCO and LVIS, focusing on rare categories and small object recall.\nRelated Work\n\nForecasting energy demand underpins grid reliability and market operations. Models must accommodate seasonality, weather effects, and nonstationarities induced by evolving consumption patterns.\n\nShort-term load forecasting has been addressed with autoregressive models (Hyndman and Athanasopoulos, 2018), LSTM and GRU-based sequence models (Lai et al., 2018; Kong et al., 2019), temporal convolutional networks (Bai et al., 2018), and attention mechanisms for capturing long-range dependencies (Qin et al., 2017; Vaswani et al., 2017). Exogenous variables such as temperature, calendar effects, and economic indicators are commonly incorporated (Hong et al., 2016; Taieb and Hyndman, 2014).\n\nProbabilistic forecasting has further been explored through quantile regression (Taylor, 2000) and deep generative models (Salinas et al., 2020), focusing on calibrated uncertainty.\n\nIn contrast, our study examines adaptive context windows driven by distribution shift tests to improve robustness without increasing model size.\n\nRelated Work\n\nExploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward and long-horizon tasks. Prior methods can be grouped by their principles, such as optimism, information gain, and novelty seeking.\n\nClassic exploration strategies include epsilon-greedy and softmax action selection (Sutton and Barto, 2018). Optimism-driven approaches such as UCB and posterior sampling provide theoretical guarantees (Auer, 2002; Osband et al., 2013). Intrinsic motivation methods reward novelty via prediction error or state visitation counts (Pathak et al., 2017; Bellemare et al., 2016).\n\nWe investigate scalable, task-agnostic exploration bonuses derived from successor features, aiming to balance coverage with stability in deep RL benchmarks.\nRelated Work\n\nAccessible interaction for blind and low-vision users often centers on screen readers that translate visual interfaces into auditory or tactile modalities. Prior work explores both input and output channels to improve efficiency and satisfaction.\n\nScreen reader interaction has been studied through command vocabularies (Perez and Green, 2016), gesture mappings on touch devices (Hussain et al., 2018), and auditory feedback designs (Nguyen and Hayes, 2019; Kim et al., 2020). Studies on blind users\u2019 navigation strategies report mixed preferences for linear and hierarchical traversal (Lopez et al., 2017; Sato and Ortiz, 2019).\n\nBeyond individual components, some work examines developer tooling for accessibility annotations (Barker and White, 2021) and automated audits (Ibrahim et al., 2020).\nRelated Work\n\nSelf-supervised representation learning in computer vision has rapidly progressed via contrastive objectives, clustering mechanisms, and generative masking formulations. These approaches reduce reliance on labeled data and serve as foundations for downstream tasks including classification and detection.\n\nContrastive pretraining methods such as instance discrimination and momentum contrast learn invariant visual features from large unlabeled corpora (Nguyen et al., 2020; Ramos and Ortega, 2021), while clustering-based approaches leverage online assignment to discover semantic groups (Li and Sun, 2021; Patel et al., 2022). Masked image modeling reconstructs missing patches to capture spatial structure (Kumar et al., 2022; Zhou and Fang, 2023).\n\nWe present a simple training recipe that combines lightweight augmentations with scalable batch schedules for improved linear probing performance.\nIntroduction\n\nCredit scoring models risk entrenching existing disparities if sensitive attributes correlate with default risk via historical bias or structural inequities. Regulators demand transparency and non-discrimination, yet practical guidance on fair deployment remains limited. A large body of work formalizes group fairness through demographic parity, equalized odds, and calibration (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017). Individual fairness notions and causal criteria offer complementary perspectives but require strong assumptions (Dwork et al., 2012; Kusner et al., 2017).\n\nWe examine post-hoc thresholding under multiple regulatory constraints and propose an auditing toolkit that integrates counterfactual sensitivity analysis with disclosure-ready summaries.\nIntroduction\n\nRecommender systems play a central role in modern content platforms, where the objective is to match users with relevant items under dynamic and sparse interactions. Recent advances in representation learning have enabled models to leverage higher-order structure and contextual signals at scale.\n\nGraph-based recommenders have attracted significant attention. Early methods propagated user and item signals over bipartite graphs (Yu et al., 2014; He et al., 2015), while more recent approaches use graph neural networks to perform message passing on user\u2013item interaction graphs (Ying et al., 2018; Wang et al., 2019; He et al., 2020; Wu et al., 2021). Session-based recommendation has also been modeled with gated and attention-enhanced GNNs to capture short-term dependencies (Li et al., 2017; Xu et al., 2019; Pan et al., 2020). Knowledge-graph-enhanced recommenders incorporate side relations to improve accuracy and explainability (Zhang et al., 2018; Wang et al., 2021; Chen et al., 2022).\n\nIn this paper, we investigate the tradeoffs between accuracy and latency for graph neural recommenders in high-throughput systems. We present a decoupled encoder architecture with cached item embeddings and on-demand lightweight user aggregation, enabling millisecond inference while preserving predictive power.\nIntroduction\n\nOut-of-Distribution Detection for Reliable Perception\n\nMachine learning systems deployed in open-world settings must recognize when test inputs differ from training distributions. Failure to detect distribution shift can lead to catastrophic errors, undermining trust and safety.\n\nA large body of work detects out-of-distribution inputs via density models, energy scores, and calibrated confidence measures (Hendrycks and Gimpel, 2017; Liang et al., 2018; Liu et al., 2020; Huang et al., 2021). Another line trains with auxiliary outliers or synthetic perturbations to shape decision boundaries (Lee et al., 2018; Mohseni et al., 2020).\n\nBenchmarks increasingly evaluate fine-grained shifts, partial OOD, and semantic anomalies across modalities, but practical deployment remains challenging due to compute limits and label scarcity (Yang et al., 2021; Vaze et al., 2022).\n",
        "completion": "A large body of work detects out-of-distribution inputs via density models, energy scores, and calibrated confidence measures (Hendrycks and Gimpel, 2017; Liang et al., 2018; Liu et al., 2020; Huang et al., 2021). Another line trains with auxiliary outliers or synthetic perturbations to shape decision boundaries (Lee et al., 2018; Mohseni et al., 2020)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\nRelated Work\n\nTransformer-based vision models have reshaped perception tasks by scaling receptive fields and enabling flexible token-level interactions. Detection, in particular, benefits from global context but faces challenges with small objects and long-tailed categories.\n\nObject detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022).\n\nIn this paper we introduce TokenScope, a training framework that selectively amplifies informative tokens via gradient-guided masking during pretraining. We measure improvements on COCO and LVIS, focusing on rare categories and small object recall.\nRelated Work\n\nForecasting energy demand underpins grid reliability and market operations. Models must accommodate seasonality, weather effects, and nonstationarities induced by evolving consumption patterns.\n\nShort-term load forecasting has been addressed with autoregressive models (Hyndman and Athanasopoulos, 2018), LSTM and GRU-based sequence models (Lai et al., 2018; Kong et al., 2019), temporal convolutional networks (Bai et al., 2018), and attention mechanisms for capturing long-range dependencies (Qin et al., 2017; Vaswani et al., 2017). Exogenous variables such as temperature, calendar effects, and economic indicators are commonly incorporated (Hong et al., 2016; Taieb and Hyndman, 2014).\n\nProbabilistic forecasting has further been explored through quantile regression (Taylor, 2000) and deep generative models (Salinas et al., 2020), focusing on calibrated uncertainty.\n\nIn contrast, our study examines adaptive context windows driven by distribution shift tests to improve robustness without increasing model size.\n\nRelated Work\n\nExploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward and long-horizon tasks. Prior methods can be grouped by their principles, such as optimism, information gain, and novelty seeking.\n\nClassic exploration strategies include epsilon-greedy and softmax action selection (Sutton and Barto, 2018). Optimism-driven approaches such as UCB and posterior sampling provide theoretical guarantees (Auer, 2002; Osband et al., 2013). Intrinsic motivation methods reward novelty via prediction error or state visitation counts (Pathak et al., 2017; Bellemare et al., 2016).\n\nWe investigate scalable, task-agnostic exploration bonuses derived from successor features, aiming to balance coverage with stability in deep RL benchmarks.\nRelated Work\n\nAccessible interaction for blind and low-vision users often centers on screen readers that translate visual interfaces into auditory or tactile modalities. Prior work explores both input and output channels to improve efficiency and satisfaction.\n\nScreen reader interaction has been studied through command vocabularies (Perez and Green, 2016), gesture mappings on touch devices (Hussain et al., 2018), and auditory feedback designs (Nguyen and Hayes, 2019; Kim et al., 2020). Studies on blind users\u2019 navigation strategies report mixed preferences for linear and hierarchical traversal (Lopez et al., 2017; Sato and Ortiz, 2019).\n\nBeyond individual components, some work examines developer tooling for accessibility annotations (Barker and White, 2021) and automated audits (Ibrahim et al., 2020).\nRelated Work\n\nSelf-supervised representation learning in computer vision has rapidly progressed via contrastive objectives, clustering mechanisms, and generative masking formulations. These approaches reduce reliance on labeled data and serve as foundations for downstream tasks including classification and detection.\n\nContrastive pretraining methods such as instance discrimination and momentum contrast learn invariant visual features from large unlabeled corpora (Nguyen et al., 2020; Ramos and Ortega, 2021), while clustering-based approaches leverage online assignment to discover semantic groups (Li and Sun, 2021; Patel et al., 2022). Masked image modeling reconstructs missing patches to capture spatial structure (Kumar et al., 2022; Zhou and Fang, 2023).\n\nWe present a simple training recipe that combines lightweight augmentations with scalable batch schedules for improved linear probing performance.\nIntroduction\n\nCredit scoring models risk entrenching existing disparities if sensitive attributes correlate with default risk via historical bias or structural inequities. Regulators demand transparency and non-discrimination, yet practical guidance on fair deployment remains limited. A large body of work formalizes group fairness through demographic parity, equalized odds, and calibration (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017). Individual fairness notions and causal criteria offer complementary perspectives but require strong assumptions (Dwork et al., 2012; Kusner et al., 2017).\n\nWe examine post-hoc thresholding under multiple regulatory constraints and propose an auditing toolkit that integrates counterfactual sensitivity analysis with disclosure-ready summaries.\nIntroduction\n\nRecommender systems play a central role in modern content platforms, where the objective is to match users with relevant items under dynamic and sparse interactions. Recent advances in representation learning have enabled models to leverage higher-order structure and contextual signals at scale.\n\nGraph-based recommenders have attracted significant attention. Early methods propagated user and item signals over bipartite graphs (Yu et al., 2014; He et al., 2015), while more recent approaches use graph neural networks to perform message passing on user\u2013item interaction graphs (Ying et al., 2018; Wang et al., 2019; He et al., 2020; Wu et al., 2021). Session-based recommendation has also been modeled with gated and attention-enhanced GNNs to capture short-term dependencies (Li et al., 2017; Xu et al., 2019; Pan et al., 2020). Knowledge-graph-enhanced recommenders incorporate side relations to improve accuracy and explainability (Zhang et al., 2018; Wang et al., 2021; Chen et al., 2022).\n\nIn this paper, we investigate the tradeoffs between accuracy and latency for graph neural recommenders in high-throughput systems. We present a decoupled encoder architecture with cached item embeddings and on-demand lightweight user aggregation, enabling millisecond inference while preserving predictive power.\nIntroduction\n\nOut-of-Distribution Detection for Reliable Perception\n\nMachine learning systems deployed in open-world settings must recognize when test inputs differ from training distributions. Failure to detect distribution shift can lead to catastrophic errors, undermining trust and safety.\n\nA large body of work detects out-of-distribution inputs via density models, energy scores, and calibrated confidence measures (Hendrycks and Gimpel, 2017; Liang et al., 2018; Liu et al., 2020; Huang et al., 2021). Another line trains with auxiliary outliers or synthetic perturbations to shape decision boundaries (Lee et al., 2018; Mohseni et al., 2020).\n\nBenchmarks increasingly evaluate fine-grained shifts, partial OOD, and semantic anomalies across modalities, but practical deployment remains challenging due to compute limits and label scarcity (Yang et al., 2021; Vaze et al., 2022).\nRelated Work\n\nPrompting and In-Context Learning\nRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022). Prompt design strategies include manual templates, mined patterns, and learned soft prompts that are optimized with gradient-based methods. In-context learning methods analyze demonstration selection and ordering effects across tasks (Lu et al., 2022; Min et al., 2022; Zhao et al., 2021).\n\nCalibration and Control of Prompts\nOther work investigates prompt calibration and uncertainty estimation to stabilize performance across tasks (Zhao et al., 2021; Holtzman et al., 2021). Techniques include verbalizer tuning, prompt ensembling, and entropy regularization under distribution shift.\n\nTask-Specific Prompting\nDomain-specific prompting has been explored for information extraction, code generation, and chain-of-thought reasoning, often with task-adapted templates and few-shot exemplars (Schick and Sch\u00fctze, 2021; Kojima et al., 2022; Chowdhery et al., 2022).\n",
        "completion": "Recently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022). Prompt design strategies include manual templates, mined patterns, and learned soft prompts that are optimized with gradient-based methods. In-context learning methods analyze demonstration selection and ordering effects across tasks (Lu et al., 2022; Min et al., 2022; Zhao et al., 2021)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\nRelated Work\n\nTransformer-based vision models have reshaped perception tasks by scaling receptive fields and enabling flexible token-level interactions. Detection, in particular, benefits from global context but faces challenges with small objects and long-tailed categories.\n\nObject detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022).\n\nIn this paper we introduce TokenScope, a training framework that selectively amplifies informative tokens via gradient-guided masking during pretraining. We measure improvements on COCO and LVIS, focusing on rare categories and small object recall.\nRelated Work\n\nForecasting energy demand underpins grid reliability and market operations. Models must accommodate seasonality, weather effects, and nonstationarities induced by evolving consumption patterns.\n\nShort-term load forecasting has been addressed with autoregressive models (Hyndman and Athanasopoulos, 2018), LSTM and GRU-based sequence models (Lai et al., 2018; Kong et al., 2019), temporal convolutional networks (Bai et al., 2018), and attention mechanisms for capturing long-range dependencies (Qin et al., 2017; Vaswani et al., 2017). Exogenous variables such as temperature, calendar effects, and economic indicators are commonly incorporated (Hong et al., 2016; Taieb and Hyndman, 2014).\n\nProbabilistic forecasting has further been explored through quantile regression (Taylor, 2000) and deep generative models (Salinas et al., 2020), focusing on calibrated uncertainty.\n\nIn contrast, our study examines adaptive context windows driven by distribution shift tests to improve robustness without increasing model size.\n\nRelated Work\n\nExploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward and long-horizon tasks. Prior methods can be grouped by their principles, such as optimism, information gain, and novelty seeking.\n\nClassic exploration strategies include epsilon-greedy and softmax action selection (Sutton and Barto, 2018). Optimism-driven approaches such as UCB and posterior sampling provide theoretical guarantees (Auer, 2002; Osband et al., 2013). Intrinsic motivation methods reward novelty via prediction error or state visitation counts (Pathak et al., 2017; Bellemare et al., 2016).\n\nWe investigate scalable, task-agnostic exploration bonuses derived from successor features, aiming to balance coverage with stability in deep RL benchmarks.\nRelated Work\n\nAccessible interaction for blind and low-vision users often centers on screen readers that translate visual interfaces into auditory or tactile modalities. Prior work explores both input and output channels to improve efficiency and satisfaction.\n\nScreen reader interaction has been studied through command vocabularies (Perez and Green, 2016), gesture mappings on touch devices (Hussain et al., 2018), and auditory feedback designs (Nguyen and Hayes, 2019; Kim et al., 2020). Studies on blind users\u2019 navigation strategies report mixed preferences for linear and hierarchical traversal (Lopez et al., 2017; Sato and Ortiz, 2019).\n\nBeyond individual components, some work examines developer tooling for accessibility annotations (Barker and White, 2021) and automated audits (Ibrahim et al., 2020).\nRelated Work\n\nSelf-supervised representation learning in computer vision has rapidly progressed via contrastive objectives, clustering mechanisms, and generative masking formulations. These approaches reduce reliance on labeled data and serve as foundations for downstream tasks including classification and detection.\n\nContrastive pretraining methods such as instance discrimination and momentum contrast learn invariant visual features from large unlabeled corpora (Nguyen et al., 2020; Ramos and Ortega, 2021), while clustering-based approaches leverage online assignment to discover semantic groups (Li and Sun, 2021; Patel et al., 2022). Masked image modeling reconstructs missing patches to capture spatial structure (Kumar et al., 2022; Zhou and Fang, 2023).\n\nWe present a simple training recipe that combines lightweight augmentations with scalable batch schedules for improved linear probing performance.\nIntroduction\n\nCredit scoring models risk entrenching existing disparities if sensitive attributes correlate with default risk via historical bias or structural inequities. Regulators demand transparency and non-discrimination, yet practical guidance on fair deployment remains limited. A large body of work formalizes group fairness through demographic parity, equalized odds, and calibration (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017). Individual fairness notions and causal criteria offer complementary perspectives but require strong assumptions (Dwork et al., 2012; Kusner et al., 2017).\n\nWe examine post-hoc thresholding under multiple regulatory constraints and propose an auditing toolkit that integrates counterfactual sensitivity analysis with disclosure-ready summaries.\nIntroduction\n\nRecommender systems play a central role in modern content platforms, where the objective is to match users with relevant items under dynamic and sparse interactions. Recent advances in representation learning have enabled models to leverage higher-order structure and contextual signals at scale.\n\nGraph-based recommenders have attracted significant attention. Early methods propagated user and item signals over bipartite graphs (Yu et al., 2014; He et al., 2015), while more recent approaches use graph neural networks to perform message passing on user\u2013item interaction graphs (Ying et al., 2018; Wang et al., 2019; He et al., 2020; Wu et al., 2021). Session-based recommendation has also been modeled with gated and attention-enhanced GNNs to capture short-term dependencies (Li et al., 2017; Xu et al., 2019; Pan et al., 2020). Knowledge-graph-enhanced recommenders incorporate side relations to improve accuracy and explainability (Zhang et al., 2018; Wang et al., 2021; Chen et al., 2022).\n\nIn this paper, we investigate the tradeoffs between accuracy and latency for graph neural recommenders in high-throughput systems. We present a decoupled encoder architecture with cached item embeddings and on-demand lightweight user aggregation, enabling millisecond inference while preserving predictive power.\nIntroduction\n\nOut-of-Distribution Detection for Reliable Perception\n\nMachine learning systems deployed in open-world settings must recognize when test inputs differ from training distributions. Failure to detect distribution shift can lead to catastrophic errors, undermining trust and safety.\n\nA large body of work detects out-of-distribution inputs via density models, energy scores, and calibrated confidence measures (Hendrycks and Gimpel, 2017; Liang et al., 2018; Liu et al., 2020; Huang et al., 2021). Another line trains with auxiliary outliers or synthetic perturbations to shape decision boundaries (Lee et al., 2018; Mohseni et al., 2020).\n\nBenchmarks increasingly evaluate fine-grained shifts, partial OOD, and semantic anomalies across modalities, but practical deployment remains challenging due to compute limits and label scarcity (Yang et al., 2021; Vaze et al., 2022).\nRelated Work\n\nPrompting and In-Context Learning\nRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022). Prompt design strategies include manual templates, mined patterns, and learned soft prompts that are optimized with gradient-based methods. In-context learning methods analyze demonstration selection and ordering effects across tasks (Lu et al., 2022; Min et al., 2022; Zhao et al., 2021).\n\nCalibration and Control of Prompts\nOther work investigates prompt calibration and uncertainty estimation to stabilize performance across tasks (Zhao et al., 2021; Holtzman et al., 2021). Techniques include verbalizer tuning, prompt ensembling, and entropy regularization under distribution shift.\n\nTask-Specific Prompting\nDomain-specific prompting has been explored for information extraction, code generation, and chain-of-thought reasoning, often with task-adapted templates and few-shot exemplars (Schick and Sch\u00fctze, 2021; Kojima et al., 2022; Chowdhery et al., 2022).\nRelated Work\n\nMapping natural language intents to executable programs requires handling long-range dependencies, structured constraints, and ambiguous specifications. Approaches vary in their reliance on grammar constraints, retrieval, and large-scale pretraining.\n\nNeural program synthesis has evolved from sequence-to-sequence translation of natural language to code (Ling et al., 2016; Yin and Neubig, 2017) to grammar-constrained decoding and abstract syntax tree modeling (Dong and Lapata, 2018; Krishnamurthy et al., 2017). Retrieval-augmented and exemplarbased methods incorporate in-context examples to improve generalization (Hashimoto et al., 2018; Chen et al., 2021). Large pretrained code models leverage massive corpora and self-supervision (Feng et al., 2020; Austin et al., 2021; Nijkamp et al., 2022). Our model builds on sequence-to-sequence pretraining and constrained decoding.\n\nWe evaluate on compositional text-to-SQL and domain-specific configuration tasks, focusing on out-of-domain generalization and constraint satisfaction.\n",
        "completion": "Neural program synthesis has evolved from sequence-to-sequence translation of natural language to code (Ling et al., 2016; Yin and Neubig, 2017) to grammar-constrained decoding and abstract syntax tree modeling (Dong and Lapata, 2018; Krishnamurthy et al., 2017). Retrieval-augmented and exemplarbased methods incorporate in-context examples to improve generalization (Hashimoto et al., 2018; Chen et al., 2021). Large pretrained code models leverage massive corpora and self-supervision (Feng et al., 2020; Austin et al., 2021; Nijkamp et al., 2022). Our model builds on sequence-to-sequence pretraining and constrained decoding."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\nRelated Work\n\nTransformer-based vision models have reshaped perception tasks by scaling receptive fields and enabling flexible token-level interactions. Detection, in particular, benefits from global context but faces challenges with small objects and long-tailed categories.\n\nObject detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022).\n\nIn this paper we introduce TokenScope, a training framework that selectively amplifies informative tokens via gradient-guided masking during pretraining. We measure improvements on COCO and LVIS, focusing on rare categories and small object recall.\nRelated Work\n\nForecasting energy demand underpins grid reliability and market operations. Models must accommodate seasonality, weather effects, and nonstationarities induced by evolving consumption patterns.\n\nShort-term load forecasting has been addressed with autoregressive models (Hyndman and Athanasopoulos, 2018), LSTM and GRU-based sequence models (Lai et al., 2018; Kong et al., 2019), temporal convolutional networks (Bai et al., 2018), and attention mechanisms for capturing long-range dependencies (Qin et al., 2017; Vaswani et al., 2017). Exogenous variables such as temperature, calendar effects, and economic indicators are commonly incorporated (Hong et al., 2016; Taieb and Hyndman, 2014).\n\nProbabilistic forecasting has further been explored through quantile regression (Taylor, 2000) and deep generative models (Salinas et al., 2020), focusing on calibrated uncertainty.\n\nIn contrast, our study examines adaptive context windows driven by distribution shift tests to improve robustness without increasing model size.\n\nRelated Work\n\nExploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward and long-horizon tasks. Prior methods can be grouped by their principles, such as optimism, information gain, and novelty seeking.\n\nClassic exploration strategies include epsilon-greedy and softmax action selection (Sutton and Barto, 2018). Optimism-driven approaches such as UCB and posterior sampling provide theoretical guarantees (Auer, 2002; Osband et al., 2013). Intrinsic motivation methods reward novelty via prediction error or state visitation counts (Pathak et al., 2017; Bellemare et al., 2016).\n\nWe investigate scalable, task-agnostic exploration bonuses derived from successor features, aiming to balance coverage with stability in deep RL benchmarks.\nRelated Work\n\nAccessible interaction for blind and low-vision users often centers on screen readers that translate visual interfaces into auditory or tactile modalities. Prior work explores both input and output channels to improve efficiency and satisfaction.\n\nScreen reader interaction has been studied through command vocabularies (Perez and Green, 2016), gesture mappings on touch devices (Hussain et al., 2018), and auditory feedback designs (Nguyen and Hayes, 2019; Kim et al., 2020). Studies on blind users\u2019 navigation strategies report mixed preferences for linear and hierarchical traversal (Lopez et al., 2017; Sato and Ortiz, 2019).\n\nBeyond individual components, some work examines developer tooling for accessibility annotations (Barker and White, 2021) and automated audits (Ibrahim et al., 2020).\nRelated Work\n\nSelf-supervised representation learning in computer vision has rapidly progressed via contrastive objectives, clustering mechanisms, and generative masking formulations. These approaches reduce reliance on labeled data and serve as foundations for downstream tasks including classification and detection.\n\nContrastive pretraining methods such as instance discrimination and momentum contrast learn invariant visual features from large unlabeled corpora (Nguyen et al., 2020; Ramos and Ortega, 2021), while clustering-based approaches leverage online assignment to discover semantic groups (Li and Sun, 2021; Patel et al., 2022). Masked image modeling reconstructs missing patches to capture spatial structure (Kumar et al., 2022; Zhou and Fang, 2023).\n\nWe present a simple training recipe that combines lightweight augmentations with scalable batch schedules for improved linear probing performance.\nIntroduction\n\nCredit scoring models risk entrenching existing disparities if sensitive attributes correlate with default risk via historical bias or structural inequities. Regulators demand transparency and non-discrimination, yet practical guidance on fair deployment remains limited. A large body of work formalizes group fairness through demographic parity, equalized odds, and calibration (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017). Individual fairness notions and causal criteria offer complementary perspectives but require strong assumptions (Dwork et al., 2012; Kusner et al., 2017).\n\nWe examine post-hoc thresholding under multiple regulatory constraints and propose an auditing toolkit that integrates counterfactual sensitivity analysis with disclosure-ready summaries.\nIntroduction\n\nRecommender systems play a central role in modern content platforms, where the objective is to match users with relevant items under dynamic and sparse interactions. Recent advances in representation learning have enabled models to leverage higher-order structure and contextual signals at scale.\n\nGraph-based recommenders have attracted significant attention. Early methods propagated user and item signals over bipartite graphs (Yu et al., 2014; He et al., 2015), while more recent approaches use graph neural networks to perform message passing on user\u2013item interaction graphs (Ying et al., 2018; Wang et al., 2019; He et al., 2020; Wu et al., 2021). Session-based recommendation has also been modeled with gated and attention-enhanced GNNs to capture short-term dependencies (Li et al., 2017; Xu et al., 2019; Pan et al., 2020). Knowledge-graph-enhanced recommenders incorporate side relations to improve accuracy and explainability (Zhang et al., 2018; Wang et al., 2021; Chen et al., 2022).\n\nIn this paper, we investigate the tradeoffs between accuracy and latency for graph neural recommenders in high-throughput systems. We present a decoupled encoder architecture with cached item embeddings and on-demand lightweight user aggregation, enabling millisecond inference while preserving predictive power.\nIntroduction\n\nOut-of-Distribution Detection for Reliable Perception\n\nMachine learning systems deployed in open-world settings must recognize when test inputs differ from training distributions. Failure to detect distribution shift can lead to catastrophic errors, undermining trust and safety.\n\nA large body of work detects out-of-distribution inputs via density models, energy scores, and calibrated confidence measures (Hendrycks and Gimpel, 2017; Liang et al., 2018; Liu et al., 2020; Huang et al., 2021). Another line trains with auxiliary outliers or synthetic perturbations to shape decision boundaries (Lee et al., 2018; Mohseni et al., 2020).\n\nBenchmarks increasingly evaluate fine-grained shifts, partial OOD, and semantic anomalies across modalities, but practical deployment remains challenging due to compute limits and label scarcity (Yang et al., 2021; Vaze et al., 2022).\nRelated Work\n\nPrompting and In-Context Learning\nRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022). Prompt design strategies include manual templates, mined patterns, and learned soft prompts that are optimized with gradient-based methods. In-context learning methods analyze demonstration selection and ordering effects across tasks (Lu et al., 2022; Min et al., 2022; Zhao et al., 2021).\n\nCalibration and Control of Prompts\nOther work investigates prompt calibration and uncertainty estimation to stabilize performance across tasks (Zhao et al., 2021; Holtzman et al., 2021). Techniques include verbalizer tuning, prompt ensembling, and entropy regularization under distribution shift.\n\nTask-Specific Prompting\nDomain-specific prompting has been explored for information extraction, code generation, and chain-of-thought reasoning, often with task-adapted templates and few-shot exemplars (Schick and Sch\u00fctze, 2021; Kojima et al., 2022; Chowdhery et al., 2022).\nRelated Work\n\nMapping natural language intents to executable programs requires handling long-range dependencies, structured constraints, and ambiguous specifications. Approaches vary in their reliance on grammar constraints, retrieval, and large-scale pretraining.\n\nNeural program synthesis has evolved from sequence-to-sequence translation of natural language to code (Ling et al., 2016; Yin and Neubig, 2017) to grammar-constrained decoding and abstract syntax tree modeling (Dong and Lapata, 2018; Krishnamurthy et al., 2017). Retrieval-augmented and exemplarbased methods incorporate in-context examples to improve generalization (Hashimoto et al., 2018; Chen et al., 2021). Large pretrained code models leverage massive corpora and self-supervision (Feng et al., 2020; Austin et al., 2021; Nijkamp et al., 2022). Our model builds on sequence-to-sequence pretraining and constrained decoding.\n\nWe evaluate on compositional text-to-SQL and domain-specific configuration tasks, focusing on out-of-domain generalization and constraint satisfaction.\nIntroduction\n\nDomain shift remains a primary obstacle for machine translation (MT), where models trained on general corpora underperform in specialized domains such as biomedicine or law. Conventional finetuning adapts the entire network but is resource-intensive and prone to catastrophic forgetting.\n\nRecently, several studies explore adapter-based finetuning, prefix-tuning, and prompt learning for adapting large pretrained sequence-to-sequence models to new domains (Houlsby et al., 2019; Li and Liang, 2021; Lester et al., 2021; Qin and Eisner, 2021; Gu et al., 2021).\n\nWhile parameter-efficient methods reduce adaptation costs, stability and controllability across heterogeneous domains remain open challenges. We propose a calibration-aware prompting scheme that explicitly regularizes domain lexical choice and terminology preservation while maintaining fluency.\n\nOur method integrates confidence-calibrated lexical constraints into soft prompts and introduces a curriculum that gradually increases domain specificity. Experiments on four domain adaptation benchmarks show improvements in BLEU and term accuracy with minimal additional parameters.\n\nWe provide ablations on calibration strategies and release scripts for reproducibility.\n",
        "completion": "Recently, several studies explore adapter-based finetuning, prefix-tuning, and prompt learning for adapting large pretrained sequence-to-sequence models to new domains (Houlsby et al., 2019; Li and Liang, 2021; Lester et al., 2021; Qin and Eisner, 2021; Gu et al., 2021)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\nRelated Work\n\nTransformer-based vision models have reshaped perception tasks by scaling receptive fields and enabling flexible token-level interactions. Detection, in particular, benefits from global context but faces challenges with small objects and long-tailed categories.\n\nObject detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022).\n\nIn this paper we introduce TokenScope, a training framework that selectively amplifies informative tokens via gradient-guided masking during pretraining. We measure improvements on COCO and LVIS, focusing on rare categories and small object recall.\nRelated Work\n\nForecasting energy demand underpins grid reliability and market operations. Models must accommodate seasonality, weather effects, and nonstationarities induced by evolving consumption patterns.\n\nShort-term load forecasting has been addressed with autoregressive models (Hyndman and Athanasopoulos, 2018), LSTM and GRU-based sequence models (Lai et al., 2018; Kong et al., 2019), temporal convolutional networks (Bai et al., 2018), and attention mechanisms for capturing long-range dependencies (Qin et al., 2017; Vaswani et al., 2017). Exogenous variables such as temperature, calendar effects, and economic indicators are commonly incorporated (Hong et al., 2016; Taieb and Hyndman, 2014).\n\nProbabilistic forecasting has further been explored through quantile regression (Taylor, 2000) and deep generative models (Salinas et al., 2020), focusing on calibrated uncertainty.\n\nIn contrast, our study examines adaptive context windows driven by distribution shift tests to improve robustness without increasing model size.\n\nRelated Work\n\nExploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward and long-horizon tasks. Prior methods can be grouped by their principles, such as optimism, information gain, and novelty seeking.\n\nClassic exploration strategies include epsilon-greedy and softmax action selection (Sutton and Barto, 2018). Optimism-driven approaches such as UCB and posterior sampling provide theoretical guarantees (Auer, 2002; Osband et al., 2013). Intrinsic motivation methods reward novelty via prediction error or state visitation counts (Pathak et al., 2017; Bellemare et al., 2016).\n\nWe investigate scalable, task-agnostic exploration bonuses derived from successor features, aiming to balance coverage with stability in deep RL benchmarks.\nRelated Work\n\nAccessible interaction for blind and low-vision users often centers on screen readers that translate visual interfaces into auditory or tactile modalities. Prior work explores both input and output channels to improve efficiency and satisfaction.\n\nScreen reader interaction has been studied through command vocabularies (Perez and Green, 2016), gesture mappings on touch devices (Hussain et al., 2018), and auditory feedback designs (Nguyen and Hayes, 2019; Kim et al., 2020). Studies on blind users\u2019 navigation strategies report mixed preferences for linear and hierarchical traversal (Lopez et al., 2017; Sato and Ortiz, 2019).\n\nBeyond individual components, some work examines developer tooling for accessibility annotations (Barker and White, 2021) and automated audits (Ibrahim et al., 2020).\nRelated Work\n\nSelf-supervised representation learning in computer vision has rapidly progressed via contrastive objectives, clustering mechanisms, and generative masking formulations. These approaches reduce reliance on labeled data and serve as foundations for downstream tasks including classification and detection.\n\nContrastive pretraining methods such as instance discrimination and momentum contrast learn invariant visual features from large unlabeled corpora (Nguyen et al., 2020; Ramos and Ortega, 2021), while clustering-based approaches leverage online assignment to discover semantic groups (Li and Sun, 2021; Patel et al., 2022). Masked image modeling reconstructs missing patches to capture spatial structure (Kumar et al., 2022; Zhou and Fang, 2023).\n\nWe present a simple training recipe that combines lightweight augmentations with scalable batch schedules for improved linear probing performance.\nIntroduction\n\nCredit scoring models risk entrenching existing disparities if sensitive attributes correlate with default risk via historical bias or structural inequities. Regulators demand transparency and non-discrimination, yet practical guidance on fair deployment remains limited. A large body of work formalizes group fairness through demographic parity, equalized odds, and calibration (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017). Individual fairness notions and causal criteria offer complementary perspectives but require strong assumptions (Dwork et al., 2012; Kusner et al., 2017).\n\nWe examine post-hoc thresholding under multiple regulatory constraints and propose an auditing toolkit that integrates counterfactual sensitivity analysis with disclosure-ready summaries.\nIntroduction\n\nRecommender systems play a central role in modern content platforms, where the objective is to match users with relevant items under dynamic and sparse interactions. Recent advances in representation learning have enabled models to leverage higher-order structure and contextual signals at scale.\n\nGraph-based recommenders have attracted significant attention. Early methods propagated user and item signals over bipartite graphs (Yu et al., 2014; He et al., 2015), while more recent approaches use graph neural networks to perform message passing on user\u2013item interaction graphs (Ying et al., 2018; Wang et al., 2019; He et al., 2020; Wu et al., 2021). Session-based recommendation has also been modeled with gated and attention-enhanced GNNs to capture short-term dependencies (Li et al., 2017; Xu et al., 2019; Pan et al., 2020). Knowledge-graph-enhanced recommenders incorporate side relations to improve accuracy and explainability (Zhang et al., 2018; Wang et al., 2021; Chen et al., 2022).\n\nIn this paper, we investigate the tradeoffs between accuracy and latency for graph neural recommenders in high-throughput systems. We present a decoupled encoder architecture with cached item embeddings and on-demand lightweight user aggregation, enabling millisecond inference while preserving predictive power.\nIntroduction\n\nOut-of-Distribution Detection for Reliable Perception\n\nMachine learning systems deployed in open-world settings must recognize when test inputs differ from training distributions. Failure to detect distribution shift can lead to catastrophic errors, undermining trust and safety.\n\nA large body of work detects out-of-distribution inputs via density models, energy scores, and calibrated confidence measures (Hendrycks and Gimpel, 2017; Liang et al., 2018; Liu et al., 2020; Huang et al., 2021). Another line trains with auxiliary outliers or synthetic perturbations to shape decision boundaries (Lee et al., 2018; Mohseni et al., 2020).\n\nBenchmarks increasingly evaluate fine-grained shifts, partial OOD, and semantic anomalies across modalities, but practical deployment remains challenging due to compute limits and label scarcity (Yang et al., 2021; Vaze et al., 2022).\nRelated Work\n\nPrompting and In-Context Learning\nRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022). Prompt design strategies include manual templates, mined patterns, and learned soft prompts that are optimized with gradient-based methods. In-context learning methods analyze demonstration selection and ordering effects across tasks (Lu et al., 2022; Min et al., 2022; Zhao et al., 2021).\n\nCalibration and Control of Prompts\nOther work investigates prompt calibration and uncertainty estimation to stabilize performance across tasks (Zhao et al., 2021; Holtzman et al., 2021). Techniques include verbalizer tuning, prompt ensembling, and entropy regularization under distribution shift.\n\nTask-Specific Prompting\nDomain-specific prompting has been explored for information extraction, code generation, and chain-of-thought reasoning, often with task-adapted templates and few-shot exemplars (Schick and Sch\u00fctze, 2021; Kojima et al., 2022; Chowdhery et al., 2022).\nRelated Work\n\nMapping natural language intents to executable programs requires handling long-range dependencies, structured constraints, and ambiguous specifications. Approaches vary in their reliance on grammar constraints, retrieval, and large-scale pretraining.\n\nNeural program synthesis has evolved from sequence-to-sequence translation of natural language to code (Ling et al., 2016; Yin and Neubig, 2017) to grammar-constrained decoding and abstract syntax tree modeling (Dong and Lapata, 2018; Krishnamurthy et al., 2017). Retrieval-augmented and exemplarbased methods incorporate in-context examples to improve generalization (Hashimoto et al., 2018; Chen et al., 2021). Large pretrained code models leverage massive corpora and self-supervision (Feng et al., 2020; Austin et al., 2021; Nijkamp et al., 2022). Our model builds on sequence-to-sequence pretraining and constrained decoding.\n\nWe evaluate on compositional text-to-SQL and domain-specific configuration tasks, focusing on out-of-domain generalization and constraint satisfaction.\nIntroduction\n\nDomain shift remains a primary obstacle for machine translation (MT), where models trained on general corpora underperform in specialized domains such as biomedicine or law. Conventional finetuning adapts the entire network but is resource-intensive and prone to catastrophic forgetting.\n\nRecently, several studies explore adapter-based finetuning, prefix-tuning, and prompt learning for adapting large pretrained sequence-to-sequence models to new domains (Houlsby et al., 2019; Li and Liang, 2021; Lester et al., 2021; Qin and Eisner, 2021; Gu et al., 2021).\n\nWhile parameter-efficient methods reduce adaptation costs, stability and controllability across heterogeneous domains remain open challenges. We propose a calibration-aware prompting scheme that explicitly regularizes domain lexical choice and terminology preservation while maintaining fluency.\n\nOur method integrates confidence-calibrated lexical constraints into soft prompts and introduces a curriculum that gradually increases domain specificity. Experiments on four domain adaptation benchmarks show improvements in BLEU and term accuracy with minimal additional parameters.\n\nWe provide ablations on calibration strategies and release scripts for reproducibility.\nRelated Work\n\nAutomatic speech recognition (ASR) for low-resource languages remains challenging due to limited transcribed data, domain mismatch, and orthographic variation. Recent progress leverages self-supervision, cross-lingual transfer, and semi-supervised learning.\n\nUnsupervised pretraining on unlabelled speech, such as wav2vec and wav2vec 2.0, reduces labeled data needs (Schneider et al., 2019; Baevski et al., 2020). Multilingual transfer and shared subword units enable cross-lingual generalization (Pratap et al., 2020; Conneau et al., 2021). Data augmentation and pseudo-labeling further improve performance (Park et al., 2019; Kahn et al., 2020).\n\nWe investigate lexicon-free decoding with articulatory priors for tonal and morphologically rich languages, assessing robustness under extreme label scarcity.\n",
        "completion": "Unsupervised pretraining on unlabelled speech, such as wav2vec and wav2vec 2.0, reduces labeled data needs (Schneider et al., 2019; Baevski et al., 2020). Multilingual transfer and shared subword units enable cross-lingual generalization (Pratap et al., 2020; Conneau et al., 2021). Data augmentation and pseudo-labeling further improve performance (Park et al., 2019; Kahn et al., 2020)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        occurs when either:\n    a) The author describes or cites papers without connecting them to their own work/argument \n    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n    c) It does not articulate the author's perspective or motivation.\n    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n    Examples of lack of synthesis include:\n        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n        ii)  No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nDetecting anomalies in time series is critical for reliability in industrial and cyber-physical systems. Methods vary in how they model temporal dependence and accommodate nonstationarity.\n\nClassical anomaly detection in time series employs statistical tests, ARIMA residual analysis, and spectral methods (Box and Jenkins, 1970; Shumway and Stoffer, 2011). Deep learning approaches introduce autoencoders, LSTM forecasting, and normalizing flows to capture complex patterns (Malhotra et al., 2015; Zong et al., 2018; Tay et al., 2020).\n\nWe build on this landscape by proposing Spectral-Conformal, a conformal prediction scheme in the frequency domain that yields calibrated anomaly scores under distribution shift.\nRelated Work\n\nLearning expressive graph representations without labels has become a central focus in graph machine learning. Contrastive objectives are widely used to shape embeddings by enforcing consistency across perturbed views of the same graph.\n\nGraph contrastive learning has shown strong performance using augmentations such as edge dropping, feature masking, and subgraph sampling (You et al., 2020; Zhu et al., 2020; Qiu et al., 2020). Negative-sample-free approaches further reduce sampling bias by maximizing agreement across views (Thakoor et al., 2021; Hassani and Khasahmadi, 2020). In this paper, we propose GCL-X for semi-supervised node classification.\n\nWe evaluate on six benchmark datasets and perform extensive ablations on augmentation strength and encoder depth.\nRelated Work\n\nLearning structure\u2013property relationships from molecular graphs is a core problem in computational chemistry and drug discovery. Supervised deep models attempt to generalize across chemotypes while coping with label sparsity and scaffold shift.\n\nMolecular property prediction has employed graph convolutional networks (Duvenaud et al., 2015; Kipf and Welling, 2017), message passing neural networks (Gilmer et al., 2017), and attention-based graph models (Veli\u010dkovi\u0107 et al., 2018; Ying et al., 2021). Pretraining on large unlabeled molecular corpora via contrastive or generative objectives has shown benefits (Hu et al., 2019; Sun et al., 2020; Rong et al., 2020). Recent work also integrates 3D geometry through equivariant networks (Sch\u00fctt et al., 2017; Satorras et al., 2021) and quantum-inspired features (Stuke et al., 2019).\n\nWhile these trends illustrate rapid progress, challenges remain around data efficiency, out-of-distribution generalization, and limited assay overlap across datasets.\n\nWe investigate scaffold-aware pretraining with uncertainty-calibrated adapters to improve transfer under scaffold shift.\n\nIntroduction\n\nAccurate citywide traffic forecasting underpins intelligent transportation, dynamic tolling, and congestion mitigation. However, complex road topologies, non-stationary patterns, and exogenous events make the problem challenging for purely temporal or spatial models.\n\nClassical statistical baselines such as ARIMA and VAR have been applied to traffic forecasting (Nguyen, 2015; Zhou and Lin, 2016). Deep sequence models introduce LSTM/GRU encoders to capture temporal dynamics (Huang et al., 2017; Park and Lee, 2018). Graph-based approaches model road networks with spectral or spatial convolutions (Li et al., 2018; Wu et al., 2019; Bai and Chen, 2020). Attention-based spatio-temporal networks further enhance long-range dependencies (Guo et al., 2019; Zhang et al., 2020). Recently, diffusion convolution and adaptive adjacency have shown strong performance (Yu et al., 2018; Pan et al., 2021).\n\nIn this paper we present Adaptive Graph Co-Attention (AGCoA), a spatio-temporal architecture that jointly conditions edge weights and temporal gates on heterogeneous signals such as weather, events, and sensor reliability. We evaluate AGCoA on three metropolitan datasets and observe consistent improvements in short- and medium-term horizons while maintaining training efficiency.\nRelated Work\n\nThreats to Privacy in Federated Learning\nFederated learning (FL) shares model updates rather than raw data, but leakage can occur through gradients and intermediate activations. Adversaries may be malicious participants or external observers, and attacks vary by access patterns and objectives.\n\nAttacks and Defenses\nAttacks on federated learning have examined gradient inversion, property inference, and membership inference (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019; Yin et al., 2021). Defenses include secure aggregation, gradient perturbation, and robust aggregation rules (Bonawitz et al., 2017; Geyer et al., 2017; Kairouz et al., 2021). We follow this line of work by developing a defense that integrates noise and clipping.\n\nSystem Considerations\nDeployment constraints such as communication budgets, client heterogeneity, and partial participation further shape the practical efficacy of defenses and the choice of aggregation.\n\nOur Angle\nWe consider privacy-utility trade-offs under label-skewed clients with bounded compute, aiming for tunable guarantees.\nIntroduction\n\nProgram synthesis with large language models shows promise but remains brittle on complex tasks requiring long-range reasoning and precise control flow. Errors often stem from incomplete step-by-step reasoning and insufficient verification.\n\nRecent approaches enhance program synthesis with chain-of-thought prompting, self-consistency sampling, tool use, and execution-guided decoding (Wei et al., 2022; Wang et al., 2022; Schick et al., 2023; Chen et al., 2021; Gao et al., 2023).\n\nNevertheless, performance degrades when specifications are underspecified or contain hidden constraints not captured by unit tests. We investigate a planning-first synthesis framework that constructs a declarative sketch aligned to the specification before code generation.\n\nOur contributions include a specification-grounded planner, a sketch-conditioned decoder, and a verifier that adapts tests via counterexample-guided refinement. Experiments on competitive coding benchmarks show accuracy gains and fewer post-hoc fixes compared to strong baselines.\nRelated Work\n\nTime Series Anomaly Detection\nIndustrial monitoring and IT telemetry require early detection of rare and evolving anomalies. Methods differ in the modeling of normal behavior, handling of seasonality, and adaptation to drift.\n\nDeep Learning Approaches\nDeep anomaly detection for time series employs reconstruction-based autoencoders, forecasting-based residual models, and probabilistic density estimation (Malhotra et al., 2015; Hundman et al., 2018; Xu et al., 2018; Su et al., 2019). Multivariate extensions leverage graph structures or attention to capture dependencies (Audibert et al., 2020; Tuli et al., 2022; Deng and Hooi, 2021).\n\nLabel Efficiency and Drift\nFew-shot labeling and distribution shifts remain challenges. Some works use weak supervision or active learning, while others apply continual learning to reduce forgetting.\n\nScope of This Work\nWe focus on label-free detection under abrupt regime changes and propose a mechanism to distinguish contextual shifts from point anomalies.\nRelated Work\n\nGraph neural networks (GNNs) have become central to modern recommendation, offering a flexible framework to encode higher-order connectivity and session transitions. In session-based settings, items are nodes and observed transitions form edges; in user-item settings, bipartite graphs capture collaborative signals beyond immediate co-interactions.\n\nEarly session-based recommenders used recurrent architectures to capture short-term dynamics (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based methods later modeled item transitions as edges in a session graph, leading to SR-GNN (Wu et al., 2019) and extensions that incorporate edge weights or context (Wang et al., 2020; Li and Chen, 2021). For user-item bipartite graphs, LightGCN simplifies message passing by removing nonlinearities and feature transforms (He et al., 2020), while NGCF and its variants enhance signal propagation with higher-order connectivity (Wang et al., 2019; Sun et al., 2020). Recent work explores contrastive objectives to stabilize graph encoders under data sparsity (Yu et al., 2022; Zhou et al., 2023).\n\nOur approach uses cross-session augmentation and state-aware contrastive signals to improve robustness under cold-start and sparse-click regimes, but we maintain the computational simplicity of lightweight propagation.\nRelated Work\n\nGNN Explainability\nPost-hoc explainers for GNNs include gradient-based saliency, perturbation tests, and subgraph extraction (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2020).\n\nCounterfactual and Causal Perspectives\nRecent work explores counterfactual explanations and causal abstractions to provide instance-level rationales on graphs (Lucic et al., 2022; Pearl, 2009; Mothilal et al., 2020).\n\nFaithfulness and Stability\nMetrics for faithfulness, stability, and sparsity have been proposed to assess explanation quality, with varied conclusions across datasets (Agarwal et al., 2023; Jain and Wallace, 2019).\nIntroduction\n\nFast lookup under evolving data distributions is a core challenge for modern storage engines. Recent work explores whether learned components can complement or replace traditional index structures.\n\nClassical database indexing relies on B-trees and variants for ordered data (Comer, 1979; Graefe, 2011). Learned indexes treat indexing as a supervised learning problem to approximate cumulative distribution functions (Kraska et al., 2018; Ma and Tang, 2020). Hybrid approaches propose piecewise models combined with fallback structures (Wang et al., 2019; Li and Luo, 2021). In this work, we present a distribution-aware learned index for dynamic workloads.\n\nWe evaluate point and range queries on shifting key distributions and analyze memory/latency trade-offs.\nRelated Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across a wide range of NLP tasks, driven by scale, pretraining data, and new interfaces for conditioning model behavior. A central theme in this progress is how to elicit reliable behavior from the same base model under minimal supervision.\n\nRecent advances in prompting have explored instruction-tuning, chain-of-thought prompting, self-consistency, and tool-augmented prompting with large language models (Zhang et al., 2022; Wei et al., 2022; Kojima et al., 2022; Press et al., 2023). These studies investigate how different prompt formats and decoding strategies influence model outputs and reveal emergent abilities in few-shot settings.\n\nBeyond prompting, efforts in alignment via human feedback and preference modeling aim to steer LLMs toward safer, more helpful responses (Ouyang et al., 2022; Bai et al., 2022). Work on controllability explores soft prompts, adapters, and parameter-efficient finetuning to specialize models for domains without full retraining (Lester et al., 2021; Hu et al., 2022).\nIntroduction\n\nIdentifying abnormal network behavior is central to intrusion detection and operational monitoring. High throughput, nonstationary traffic, and scarce labels make unsupervised approaches attractive.\n\nNetwork anomaly detection has been approached with statistical baselines (Fei and Zhou, 2015), clustering over flow features (Gao et al., 2016), and deep autoencoders trained for reconstruction (Ruff et al., 2018; Lin and Peng, 2019). Temporal models capture sequence dependencies in traffic (Hsu and Chen, 2020; Wang and Li, 2021). In this study, we adopt an autoencoder-based framework.\n\nWe benchmark on public flow datasets and report precision\u2013recall under varying contamination rates.\nRelated Work\n\nTransformer-based vision models have reshaped perception tasks by scaling receptive fields and enabling flexible token-level interactions. Detection, in particular, benefits from global context but faces challenges with small objects and long-tailed categories.\n\nObject detection with transformers has been studied in multiple variants including encoder-decoder architectures, deformable attention, and multi-scale feature fusion (Carion et al., 2020; Zhu et al., 2021; Chen et al., 2022; Sun et al., 2021). Recent works also integrate knowledge distillation and multi-task pretraining to boost detection accuracy (Xie et al., 2022; Wang et al., 2022).\n\nIn this paper we introduce TokenScope, a training framework that selectively amplifies informative tokens via gradient-guided masking during pretraining. We measure improvements on COCO and LVIS, focusing on rare categories and small object recall.\nRelated Work\n\nForecasting energy demand underpins grid reliability and market operations. Models must accommodate seasonality, weather effects, and nonstationarities induced by evolving consumption patterns.\n\nShort-term load forecasting has been addressed with autoregressive models (Hyndman and Athanasopoulos, 2018), LSTM and GRU-based sequence models (Lai et al., 2018; Kong et al., 2019), temporal convolutional networks (Bai et al., 2018), and attention mechanisms for capturing long-range dependencies (Qin et al., 2017; Vaswani et al., 2017). Exogenous variables such as temperature, calendar effects, and economic indicators are commonly incorporated (Hong et al., 2016; Taieb and Hyndman, 2014).\n\nProbabilistic forecasting has further been explored through quantile regression (Taylor, 2000) and deep generative models (Salinas et al., 2020), focusing on calibrated uncertainty.\n\nIn contrast, our study examines adaptive context windows driven by distribution shift tests to improve robustness without increasing model size.\n\nRelated Work\n\nExploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward and long-horizon tasks. Prior methods can be grouped by their principles, such as optimism, information gain, and novelty seeking.\n\nClassic exploration strategies include epsilon-greedy and softmax action selection (Sutton and Barto, 2018). Optimism-driven approaches such as UCB and posterior sampling provide theoretical guarantees (Auer, 2002; Osband et al., 2013). Intrinsic motivation methods reward novelty via prediction error or state visitation counts (Pathak et al., 2017; Bellemare et al., 2016).\n\nWe investigate scalable, task-agnostic exploration bonuses derived from successor features, aiming to balance coverage with stability in deep RL benchmarks.\nRelated Work\n\nAccessible interaction for blind and low-vision users often centers on screen readers that translate visual interfaces into auditory or tactile modalities. Prior work explores both input and output channels to improve efficiency and satisfaction.\n\nScreen reader interaction has been studied through command vocabularies (Perez and Green, 2016), gesture mappings on touch devices (Hussain et al., 2018), and auditory feedback designs (Nguyen and Hayes, 2019; Kim et al., 2020). Studies on blind users\u2019 navigation strategies report mixed preferences for linear and hierarchical traversal (Lopez et al., 2017; Sato and Ortiz, 2019).\n\nBeyond individual components, some work examines developer tooling for accessibility annotations (Barker and White, 2021) and automated audits (Ibrahim et al., 2020).\nRelated Work\n\nSelf-supervised representation learning in computer vision has rapidly progressed via contrastive objectives, clustering mechanisms, and generative masking formulations. These approaches reduce reliance on labeled data and serve as foundations for downstream tasks including classification and detection.\n\nContrastive pretraining methods such as instance discrimination and momentum contrast learn invariant visual features from large unlabeled corpora (Nguyen et al., 2020; Ramos and Ortega, 2021), while clustering-based approaches leverage online assignment to discover semantic groups (Li and Sun, 2021; Patel et al., 2022). Masked image modeling reconstructs missing patches to capture spatial structure (Kumar et al., 2022; Zhou and Fang, 2023).\n\nWe present a simple training recipe that combines lightweight augmentations with scalable batch schedules for improved linear probing performance.\nIntroduction\n\nCredit scoring models risk entrenching existing disparities if sensitive attributes correlate with default risk via historical bias or structural inequities. Regulators demand transparency and non-discrimination, yet practical guidance on fair deployment remains limited. A large body of work formalizes group fairness through demographic parity, equalized odds, and calibration (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017). Individual fairness notions and causal criteria offer complementary perspectives but require strong assumptions (Dwork et al., 2012; Kusner et al., 2017).\n\nWe examine post-hoc thresholding under multiple regulatory constraints and propose an auditing toolkit that integrates counterfactual sensitivity analysis with disclosure-ready summaries.\nIntroduction\n\nRecommender systems play a central role in modern content platforms, where the objective is to match users with relevant items under dynamic and sparse interactions. Recent advances in representation learning have enabled models to leverage higher-order structure and contextual signals at scale.\n\nGraph-based recommenders have attracted significant attention. Early methods propagated user and item signals over bipartite graphs (Yu et al., 2014; He et al., 2015), while more recent approaches use graph neural networks to perform message passing on user\u2013item interaction graphs (Ying et al., 2018; Wang et al., 2019; He et al., 2020; Wu et al., 2021). Session-based recommendation has also been modeled with gated and attention-enhanced GNNs to capture short-term dependencies (Li et al., 2017; Xu et al., 2019; Pan et al., 2020). Knowledge-graph-enhanced recommenders incorporate side relations to improve accuracy and explainability (Zhang et al., 2018; Wang et al., 2021; Chen et al., 2022).\n\nIn this paper, we investigate the tradeoffs between accuracy and latency for graph neural recommenders in high-throughput systems. We present a decoupled encoder architecture with cached item embeddings and on-demand lightweight user aggregation, enabling millisecond inference while preserving predictive power.\nIntroduction\n\nOut-of-Distribution Detection for Reliable Perception\n\nMachine learning systems deployed in open-world settings must recognize when test inputs differ from training distributions. Failure to detect distribution shift can lead to catastrophic errors, undermining trust and safety.\n\nA large body of work detects out-of-distribution inputs via density models, energy scores, and calibrated confidence measures (Hendrycks and Gimpel, 2017; Liang et al., 2018; Liu et al., 2020; Huang et al., 2021). Another line trains with auxiliary outliers or synthetic perturbations to shape decision boundaries (Lee et al., 2018; Mohseni et al., 2020).\n\nBenchmarks increasingly evaluate fine-grained shifts, partial OOD, and semantic anomalies across modalities, but practical deployment remains challenging due to compute limits and label scarcity (Yang et al., 2021; Vaze et al., 2022).\nRelated Work\n\nPrompting and In-Context Learning\nRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022). Prompt design strategies include manual templates, mined patterns, and learned soft prompts that are optimized with gradient-based methods. In-context learning methods analyze demonstration selection and ordering effects across tasks (Lu et al., 2022; Min et al., 2022; Zhao et al., 2021).\n\nCalibration and Control of Prompts\nOther work investigates prompt calibration and uncertainty estimation to stabilize performance across tasks (Zhao et al., 2021; Holtzman et al., 2021). Techniques include verbalizer tuning, prompt ensembling, and entropy regularization under distribution shift.\n\nTask-Specific Prompting\nDomain-specific prompting has been explored for information extraction, code generation, and chain-of-thought reasoning, often with task-adapted templates and few-shot exemplars (Schick and Sch\u00fctze, 2021; Kojima et al., 2022; Chowdhery et al., 2022).\nRelated Work\n\nMapping natural language intents to executable programs requires handling long-range dependencies, structured constraints, and ambiguous specifications. Approaches vary in their reliance on grammar constraints, retrieval, and large-scale pretraining.\n\nNeural program synthesis has evolved from sequence-to-sequence translation of natural language to code (Ling et al., 2016; Yin and Neubig, 2017) to grammar-constrained decoding and abstract syntax tree modeling (Dong and Lapata, 2018; Krishnamurthy et al., 2017). Retrieval-augmented and exemplarbased methods incorporate in-context examples to improve generalization (Hashimoto et al., 2018; Chen et al., 2021). Large pretrained code models leverage massive corpora and self-supervision (Feng et al., 2020; Austin et al., 2021; Nijkamp et al., 2022). Our model builds on sequence-to-sequence pretraining and constrained decoding.\n\nWe evaluate on compositional text-to-SQL and domain-specific configuration tasks, focusing on out-of-domain generalization and constraint satisfaction.\nIntroduction\n\nDomain shift remains a primary obstacle for machine translation (MT), where models trained on general corpora underperform in specialized domains such as biomedicine or law. Conventional finetuning adapts the entire network but is resource-intensive and prone to catastrophic forgetting.\n\nRecently, several studies explore adapter-based finetuning, prefix-tuning, and prompt learning for adapting large pretrained sequence-to-sequence models to new domains (Houlsby et al., 2019; Li and Liang, 2021; Lester et al., 2021; Qin and Eisner, 2021; Gu et al., 2021).\n\nWhile parameter-efficient methods reduce adaptation costs, stability and controllability across heterogeneous domains remain open challenges. We propose a calibration-aware prompting scheme that explicitly regularizes domain lexical choice and terminology preservation while maintaining fluency.\n\nOur method integrates confidence-calibrated lexical constraints into soft prompts and introduces a curriculum that gradually increases domain specificity. Experiments on four domain adaptation benchmarks show improvements in BLEU and term accuracy with minimal additional parameters.\n\nWe provide ablations on calibration strategies and release scripts for reproducibility.\nRelated Work\n\nAutomatic speech recognition (ASR) for low-resource languages remains challenging due to limited transcribed data, domain mismatch, and orthographic variation. Recent progress leverages self-supervision, cross-lingual transfer, and semi-supervised learning.\n\nUnsupervised pretraining on unlabelled speech, such as wav2vec and wav2vec 2.0, reduces labeled data needs (Schneider et al., 2019; Baevski et al., 2020). Multilingual transfer and shared subword units enable cross-lingual generalization (Pratap et al., 2020; Conneau et al., 2021). Data augmentation and pseudo-labeling further improve performance (Park et al., 2019; Kahn et al., 2020).\n\nWe investigate lexicon-free decoding with articulatory priors for tonal and morphologically rich languages, assessing robustness under extreme label scarcity.\nRelated Work\n\nAutomated program repair (APR) has evolved from search-based techniques to neural approaches that learn fix patterns from data. Recent advances in pretrained code models and prompting have further expanded the design space for patch generation.\n\nNeural program repair has explored sequence-to-sequence models trained on bug-fix pairs (Tufano et al., 2019; Chen et al., 2019). Pretrained code models like CodeBERT and GraphCodeBERT have been adapted for patch generation (Feng et al., 2020; Guo et al., 2021). Large language models have recently been prompted for repair tasks with few-shot examples (Chen et al., 2021; Jiang et al., 2023).\n\nWe study constrained decoding with static analysis feedback to reduce overfitting to dataset artifacts, assessing generalization to out-of-distribution bug patterns in real repositories.\n",
        "completion": "Neural program repair has explored sequence-to-sequence models trained on bug-fix pairs (Tufano et al., 2019; Chen et al., 2019). Pretrained code models like CodeBERT and GraphCodeBERT have been adapted for patch generation (Feng et al., 2020; Guo et al., 2021). Large language models have recently been prompted for repair tasks with few-shot examples (Chen et al., 2021; Jiang et al., 2023)."
    }
]