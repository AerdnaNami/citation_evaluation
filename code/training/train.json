[
  {
    "span": "BERT has been applied to automated essay scoring on short-answer datasets.",
    "document": "Related Work\n\nAutomated Essay Scoring\n\nAutomated essay scoring (AES) evaluates writing quality using machine learning. Early AES systems relied on engineered lexical and syntactic features, while recent neural models leverage pretrained encoders. BERT has been applied to automated essay scoring on short-answer datasets. Beyond surface features, coherence and argument structure modeling have been explored to better capture holistic writing quality. Domain adaptation remains challenging due to prompt shift and rubric variation.",
    "reason": "Mentions prior applications of BERT to AES without citing specific studies (rule b).",
    "start": 247,
    "end": 321,
    "label": "Unsupported claim"
  },
  {
    "span": "The CommonVoice Swahili subset contains roughly 80 hours of transcribed speech",
    "document": "Introduction\n\nLow-Resource Automatic Speech Recognition Building robust ASR for low-resource languages remains challenging due to limited labeled data and domain variability (Besacier et al., 2014; Adams et al., 2019). Cross-lingual transfer and self-supervised pretraining from large unlabeled corpora have recently narrowed the gap with high-resource systems (Kahn et al., 2020; Conneau et al., 2021). The CommonVoice Swahili subset contains roughly 80 hours of transcribed speech, which highlights the data scarcity faced in many Bantu languages.\n\nOur Contributions We propose a multilingual wav2vec-based model with adapter layers that balances shared acoustic representations and language-specific phonotactics. We further introduce a pronunciation-lexicon augmentation strategy using grapheme-to-phoneme models adapted from related languages.",
    "reason": "This sentence provides a dataset-specific statistic at first mention without citing the dataset source, which requires a citation.",
    "start": 404,
    "end": 482,
    "label": "Unsupported claim"
  },
  {
    "span": "Secure aggregation prevents servers from seeing raw updates (Bonawitz et al., 2017). Personalization layers tailor models to client heterogeneity (Arivazhagan et al., 2019). Gradient compression reduces communication cost (Lin et al., 2018). Differential privacy imposes formal guarantees on release mechanisms (Abadi et al., 2016).",
    "document": "Related Work\n\nFederated learning (FL) enables training across decentralized data silos without centralizing raw data (McMahan et al., 2017). A central challenge in FL is the presence of non-iid client distributions, which can destabilize training and lead to biased global models (Zhao et al., 2018). System heterogeneity, including stragglers and variable connectivity, further complicates update aggregation and convergence guarantees (Li et al., 2020).\n\nSecure aggregation prevents servers from seeing raw updates (Bonawitz et al., 2017). Personalization layers tailor models to client heterogeneity (Arivazhagan et al., 2019). Gradient compression reduces communication cost (Lin et al., 2018). Differential privacy imposes formal guarantees on release mechanisms (Abadi et al., 2016). While each direction aims to address a distinct pain point in FL, recent works emphasize the necessity of jointly considering privacy, communication, and statistical efficiency (Kairouz et al., 2021).\n\nOur work investigates cross-device FL under severe non-iid settings, focusing on balancing privacy guarantees with personalization quality. We build on robust aggregation and client selection strategies while examining the interplay between compression and privacy noise on downstream accuracy.",
    "reason": "The span lists four separate lines of work with no transitions or explicit relationships between them, making the connection between privacy, personalization, compression, and differential privacy abrupt and unclear.",
    "start": 457,
    "end": 789,
    "label": "Coherence"
  },
  {
    "span": "Neural enumerative search interleaves learning and constraint solving (Balog et al., 2017). Type-directed pruning reduces the combinatorial space (Osera and Zdancewic, 2015). Few-shot prompting with large language models demonstrates in-context induction of code (Brown et al., 2020). Execution-guided decoding avoids semantic errors (Chen et al., 2018).",
    "document": "Related Work\n\nProgram synthesis approaches vary from symbolic search with deductive constraints to neural methods that learn from examples or specifications. Recent trends also incorporate execution feedback and leverage large pretrained code models.\n\nNeural enumerative search interleaves learning and constraint solving (Balog et al., 2017). Type-directed pruning reduces the combinatorial space (Osera and Zdancewic, 2015). Few-shot prompting with large language models demonstrates in-context induction of code (Brown et al., 2020). Execution-guided decoding avoids semantic errors (Chen et al., 2018).\n\nWhile these techniques address search efficiency, data efficiency, or semantic correctness independently, our framework integrates type constraints and execution signals within a prompted synthesis loop.",
    "reason": "The span lists four distinct techniques without transitional phrases or an explanation of how they build on or contrast with each other, resulting in abrupt shifts and weak coherence.",
    "start": 252,
    "end": 606,
    "label": "Coherence"
  },
  {
    "span": "Sequence-level knowledge distillation narrows the gap between teacher and student models in ASR (Kim and Rush, 2016; Hinton et al., 2015). SpecAugment perturbs time-frequency masks to reduce overfitting (Park et al., 2019). Pseudo-labeling boosts low-resource recognition by leveraging untranscribed audio (Kahn et al., 2020). CTC alignment smoothing further stabilizes training (Graves et al., 2006).",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) models increasingly rely on data scaling and regularization to achieve strong accuracy with practical latency. Beyond architectural innovations, training strategies such as distillation, augmentation, and semi-supervised learning have become standard tools for robust ASR.\n\nSequence-level knowledge distillation narrows the gap between teacher and student models in ASR (Kim and Rush, 2016; Hinton et al., 2015). SpecAugment perturbs time-frequency masks to reduce overfitting (Park et al., 2019). Pseudo-labeling boosts low-resource recognition by leveraging untranscribed audio (Kahn et al., 2020). CTC alignment smoothing further stabilizes training (Graves et al., 2006).\n\nWe build on semi-supervised ASR by combining consistency regularization with curriculum schedules, focusing on label quality estimation for noisy pseudo-labels.",
    "reason": "The span enumerates unrelated techniques (distillation, augmentation, pseudo-labeling, CTC smoothing) without transitions or explaining how they relate to each other, resulting in poor coherence.",
    "start": 334,
    "end": 735,
    "label": "Coherence"
  },
  {
    "span": "Nguyen 2015",
    "document": "Related Work\n\nTemporal forecasting models have evolved from ARIMA baselines (Box and Jenkins, 1976) to deep sequence models (Lai et al., 2018; Lim and Zohren, 2021). Several methods have been proposed (Nguyen 2015; Patel et al., 2019; Zhou, 2021) to address regime shifts via adaptive losses and data augmentation. While ensembling improves robustness (Gal and Ghahramani, 2016), it is computationally heavy for streaming scenarios (Ribeiro and Silva, 2020). We contribute a lightweight change-point aware adapter that updates only a small subset of parameters during drift.\n\nIntroduction\n\nWe test on four public benchmarks with synthetic shift injections and report accuracy–latency trade-offs against strong baselines.",
    "reason": "Missing comma between author and year in an author–year parenthetical citation; should be 'Nguyen, 2015'.",
    "start": 202,
    "end": 213,
    "label": "Format"
  },
  {
    "span": "Estimating treatment effects from observational health data has been studied with propensity scores, doubly robust estimators, and representation learning (Rosenbaum and Rubin, 1983; Robins et al., 1994; Johansson et al., 2016). Instrumental variable methods and front-door adjustment address unobserved confounding under additional assumptions (Angrist et al., 1996; Pearl, 2009). We examine electronic health records using deep instrumental variables.",
    "document": "Introduction\n\nReliable causal effect estimation from electronic health records can guide clinical decision-making when randomized trials are infeasible. However, observational data pose challenges such as selection bias, confounding, and missingness.\n\nEstimating treatment effects from observational health data has been studied with propensity scores, doubly robust estimators, and representation learning (Rosenbaum and Rubin, 1983; Robins et al., 1994; Johansson et al., 2016). Instrumental variable methods and front-door adjustment address unobserved confounding under additional assumptions (Angrist et al., 1996; Pearl, 2009). We examine electronic health records using deep instrumental variables.\n\nWe test on two therapies in cardiology and oncology, reporting both point estimates and uncertainty quantification under weak instrument conditions.",
    "reason": "The span jumps from listing prior methods to stating the authors’ approach without making explicit what gap exists or why deep IV is appropriate for the identified challenges (criterion b).",
    "start": 252,
    "end": 705,
    "label": "Lacks synthesis"
  },
  {
    "span": "we follow the standard train/dev/test split of the NewsRoom-Sum dataset",
    "document": "Introduction\n\nAbstractive summarization models benefit from consistent evaluation protocols to ensure fair comparison. In this study, we assess our model on news-style articles with an emphasis on factual consistency and compression. To align with prior work and facilitate reproducibility, we follow the standard train/dev/test split of the NewsRoom-Sum dataset and report ROUGE, factuality, and compression rates.\n\nOur contributions include a constrained decoder that enforces entity consistency and a lightweight verifier that reduces hallucinations. We further analyze performance on long-form articles and rare-entity cases, and provide a human evaluation of summary coherence.",
    "reason": "First mention of a specific dataset and a 'standard' split is asserted without any citation to the dataset or protocol defining the split.",
    "start": 291,
    "end": 362,
    "label": "Unsupported claim"
  },
  {
    "span": "Prior work has demonstrated that domain randomization alone is sufficient to transfer dexterous manipulation policies to the real world.",
    "document": "Introduction\n\nLearning control policies for dexterous manipulation typically requires bridging significant sim-to-real gaps. Approaches span dynamics randomization, privileged state estimation, and visual domain adaptation, often coupled with offline data aggregation from human demonstrations (Tobin et al., 2017; OpenAI et al., 2019; Peng et al., 2020). Prior work has demonstrated that domain randomization alone is sufficient to transfer dexterous manipulation policies to the real world. However, such strategies can be sample-inefficient and brittle under contact-rich variations. We propose contact-aware curriculum randomization with uncertainty-guided resets to accelerate transfer and improve robustness across object geometries.\n",
    "reason": "Asserts a strong conclusion about prior work and transfer sufficiency without citing sources, violating rule (a) and (b).",
    "start": 356,
    "end": 492,
    "label": "Unsupported claim"
  },
  {
    "span": "the widely-used ToxicComment corpus",
    "document": "Introduction\n\nDetecting toxic language online is essential for content moderation and digital well-being. Prior work has addressed hate speech, harassment, and offensive content using lexicon-based, classical machine learning, and neural approaches (Schmidt and Wiegand, 2017; Badjatiya et al., 2017; Park and Fung, 2017). Recent transformer-based classifiers have achieved strong results by leveraging contextual embeddings and label smoothing for imbalanced classes (Liu et al., 2019; Mozafari et al., 2020). In our experiments, we evaluate on the widely-used ToxicComment corpus and two social media datasets spanning multiple platforms. We further consider robustness to adversarial paraphrases and domain shift.\n",
    "reason": "Mentions a specific dataset by name and asserts its ubiquity without providing a citation to the dataset or its source (rule a).",
    "start": 546,
    "end": 581,
    "label": "Unsupported claim"
  },
  {
    "span": "MovieLens-1M contains 1,000,209 ratings by 6,040 users on 3,952 movies.",
    "document": "Introduction\n\nRecommender systems are typically evaluated on public benchmarks using implicit or explicit feedback (Hu et al., 2008; Rendle et al., 2009). Matrix factorization and neural collaborative filtering remain strong baselines despite the advent of graph-based and sequence-aware models (Koren et al., 2009; He et al., 2017; Sun et al., 2019). MovieLens-1M contains 1,000,209 ratings by 6,040 users on 3,952 movies. While ubiquitous, these datasets have popularity and selection biases that can distort offline metrics (Steck, 2011; Krishna et al., 2021).",
    "reason": "Provides specific dataset statistics without citing the dataset paper or official source (rule a/b).",
    "start": 352,
    "end": 423,
    "label": "Unsupported claim"
  },
  {
    "span": "Previous clinical studies have shown that early detection via retinal OCT reduces vision loss by 35%.",
    "document": "Introduction\n\nRetinal optical coherence tomography (OCT) enables noninvasive, micrometer-resolution imaging of retinal layers and is central to diagnosing macular pathologies (Adams et al., 2019). Automated segmentation of retinal structures can facilitate large-scale screening and monitoring (Bennett and Zhou, 2020).\n\nPrevious clinical studies have shown that early detection via retinal OCT reduces vision loss by 35%.\n\nDespite these advances, robust segmentation remains challenging due to device variability, motion artifacts, and co-morbid conditions. We propose a cross-domain adaptation framework that aligns feature distributions across scanners while preserving anatomical consistency.",
    "reason": "Reports a specific quantitative effect size from clinical studies without citing those studies.",
    "start": 321,
    "end": 422,
    "label": "Unsupported claim"
  },
  {
    "span": "IEMOCAP corpus contains roughly 12 hours of acted dialogue",
    "document": "Introduction\n\nSpeech emotion recognition (SER) seeks to infer affective states from acoustic and linguistic cues. Applications span call-center analytics, mental health screening, and social robotics. Despite progress with deep architectures, SER performance still lags in cross-speaker and cross-corpus scenarios due to domain shift and label ambiguity.\n\nBenchmark datasets vary widely in recording conditions, acting style, and annotation protocol. The IEMOCAP corpus contains roughly 12 hours of acted dialogue recorded in a studio setting, whereas other corpora capture spontaneous emotions in the wild. These discrepancies complicate model transfer and evaluation fairness.\n\nWe propose a domain-adversarial SER model with label smoothing calibrated by corpus-specific uncertainty, and we introduce a protocol for cross-corpus validation to better reflect real-world deployment.",
    "reason": "This sentence states a specific dataset statistic without providing a citation for the dataset at first mention.",
    "start": 455,
    "end": 513,
    "label": "Unsupported claim"
  },
  {
    "span": "BM25 has long been the default ranking function in academic search engines",
    "document": "Related Work\n\nClassic information retrieval (IR) pipelines rely on sparse lexical matching coupled with efficient inverted indexes. BM25 has long been the default ranking function in academic search engines, providing a strong baseline for document retrieval across scientific domains. More recently, neural rankers have complemented sparse methods with semantic matching, often using BM25 to seed candidate sets. Our work situates itself in this hybrid paradigm by introducing a calibration layer that reconciles lexical and semantic scores.",
    "reason": "Asserts widespread adoption of a specific method without any citation to back the claim, violating (b).",
    "start": 132,
    "end": 206,
    "label": "Unsupported claim"
  },
  {
    "span": "The majority of clinical NER systems rely on dictionary bootstrapping.",
    "document": "Related Work\n\nClinical named entity recognition (NER) underpins downstream tasks such as cohort selection, pharmacovigilance, and phenotyping. While pretrained language models have improved generalization across institutions, challenges persist due to abbreviations, negation, and domain shift.\n\nThe majority of clinical NER systems rely on dictionary bootstrapping. Alternative strategies include weak supervision from heuristics, cross-institutional silver labels, and task-adaptive pretraining, but consistent comparisons across HIPAA-compliant datasets remain rare.\n\nWe propose a distantly supervised framework that integrates ontology alignment with span-level confidence calibration. Evaluations across de-identified notes demonstrate competitive performance with reduced annotation costs.\n\nWe discuss ethical considerations and potential biases introduced by ontology incompleteness.",
    "reason": "This generalization about prevailing methods in a niche domain lacks citations to surveys or empirical studies supporting the claim (rule b).",
    "start": 296,
    "end": 366,
    "label": "Unsupported claim"
  },
  {
    "span": "Biomedical entity normalization has been approached with dictionary matching, candidate ranking over knowledge bases, and cross-encoder architectures (D’Souza and Ng, 2015; Murty et al., 2018; Sung et al., 2020). Contextualized encoders like BioBERT and SciBERT further boost linking accuracy (Lee et al., 2020; Beltagy et al., 2019).",
    "document": "Related Work\n\nEntity normalization maps diverse surface forms in biomedical text to canonical identifiers. Ambiguity, abbreviation, and domain shift between corpora and ontologies complicate the task.\n\nBiomedical entity normalization has been approached with dictionary matching, candidate ranking over knowledge bases, and cross-encoder architectures (D’Souza and Ng, 2015; Murty et al., 2018; Sung et al., 2020). Contextualized encoders like BioBERT and SciBERT further boost linking accuracy (Lee et al., 2020; Beltagy et al., 2019).\n\nDespite accuracy gains, many systems assume fixed ontologies and struggle with emerging concepts. We propose an incremental linker that composes definitions on the fly and calibrates uncertainty for open-world settings.",
    "reason": "The span enumerates approaches and pretrained encoders without relating them to open-world normalization or articulating the gap the paper targets, thus lacking synthesis per (a) and (b).",
    "start": 202,
    "end": 536,
    "label": "Lacks synthesis"
  },
  {
    "span": "In a previous study, the authors claim that dictionary-based methods remain competitive with modern transformers for rare diseases.",
    "document": "Introduction\n\nBiomedical named entity recognition (BioNER) has shifted from dictionary and CRF-based pipelines to transformer architectures, yielding large improvements on benchmark corpora (Habibi et al., 2017; Lee et al., 2020). Pretraining on in-domain biomedical text further enhances recognition of specialized terminology and abbreviations (Beltagy et al., 2019).\n\nIn a previous study, the authors claim that dictionary-based methods remain competitive with modern transformers for rare diseases. While lexicon coverage can help with sparsity, such assertions require careful evaluation under the same annotation schemes and boundary criteria. We therefore re-examine rare disease mentions across multiple corpora and compare dictionary bootstrapping to supervised transformers under identical preprocessing.",
    "reason": "Refers to a 'previous study' and its claim without providing a citation to identify the study.",
    "start": 371,
    "end": 502,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from three prompts",
    "document": "Introduction\n\nAutomated essay scoring (AES) aims to predict human-assigned scores from student writing. Prior work has explored feature-rich linear models, neural encoders, and cross-prompt adaptation. BERT was used in an AES task trained on essays from three prompts to demonstrate cross-prompt transfer, motivating further research on robustness to prompt shift. We extend this line by introducing a prompt-invariant contrastive loss that disentangles content relevance from stylistic signals.",
    "reason": "Describes a specific prior setup and result without citing the corresponding work, matching example (iii) and violating (a).",
    "start": 202,
    "end": 267,
    "label": "Unsupported claim"
  },
  {
    "span": "in (O'Neil and Park 2016)",
    "document": "Introduction\n\nMultivariate time-series forecasting benefits from architectures that capture both temporal and cross-variable dependencies (Salinas et al., 2020; Sen et al., 2019). Attention mechanisms improve long-horizon accuracy by focusing on relevant segments (Li et al., 2019). As demonstrated in (O'Neil and Park 2016), hybrid autoregressive–neural models can reduce variance under regime shifts.",
    "reason": "Wrong citation style and missing comma: a preposition should not precede a parenthetical citation; it should be narrative 'in O'Neil and Park (2016)', and within parentheses the author–year should be 'O'Neil and Park, 2016'.",
    "start": 299,
    "end": 324,
    "label": "Format"
  },
  {
    "span": "A large body of recent work reports that BLEU is weakly correlated with human judgments for dialogue.",
    "document": "Related Work\n\nEvaluating open-domain dialogue remains contentious, with automatic metrics often failing to capture appropriateness, engagement, and context maintenance. While word-overlap metrics are easy to compute, they do not account for the diversity of valid responses.\n\nA large body of recent work reports that BLEU is weakly correlated with human judgments for dialogue. Alternative metrics have been proposed, including learned evaluators and reference-free approaches, but their reliability and robustness are still under debate.\n\nWe contribute an evaluation framework that combines pairwise human preference with calibrated uncertainty estimates, enabling more reproducible comparisons across systems.",
    "reason": "Asserts a consensus from recent work without providing citations to those works, violating rule (d).",
    "start": 276,
    "end": 377,
    "label": "Unsupported claim"
  },
  {
    "span": "The SemEval Metaphor Detection shared task standardized evaluation metrics for this problem.",
    "document": "Related Work\n\nMetaphor detection. Traditional approaches used lexical resources and hand-engineered features with SVMs and CRFs (Shutova et al., 2010; Tsvetkov et al., 2014). Neural methods adopt contextual embeddings and multi-task learning to capture figurative usage (Gao et al., 2018; Mao et al., 2019). The SemEval Metaphor Detection shared task standardized evaluation metrics for this problem. More recent work explores cross-lingual transfer of figurative language understanding through multilingual transformers (Leong et al., 2020; Dankers et al., 2020).\n",
    "reason": "Mentions a specific shared task and its impact without providing a citation to the task; per (a) references to shared tasks must be cited at first mention.",
    "start": 308,
    "end": 400,
    "label": "Unsupported claim"
  },
  {
    "span": "Language grounding for robot instruction following includes symbolic planners that map utterances to logical forms (Tellex et al., 2011; Artzi and Zettlemoyer, 2013), neural end-to-end policies trained on demonstrations (Mei et al., 2016; Misra et al., 2017), and multimodal grounding that fuses vision, language, and proprioception (Shridhar et al., 2020; Blukis et al., 2021). Recent work explores interactive correction and subgoal decomposition (Thomason et al., 2019; Kim et al., 2022).",
    "document": "Related Work\n\nRobots that interpret and execute natural language commands must connect linguistic structure with perceptual context and action repertoires.\n\nLanguage grounding for robot instruction following includes symbolic planners that map utterances to logical forms (Tellex et al., 2011; Artzi and Zettlemoyer, 2013), neural end-to-end policies trained on demonstrations (Mei et al., 2016; Misra et al., 2017), and multimodal grounding that fuses vision, language, and proprioception (Shridhar et al., 2020; Blukis et al., 2021). Recent work explores interactive correction and subgoal decomposition (Thomason et al., 2019; Kim et al., 2022).\n\nOur approach targets novel-object instructions in cluttered scenes by leveraging open-vocabulary visual recognition and a language-conditioned affordance planner. We also introduce a benchmark with systematic noun-verb recombinations to evaluate compositional generalization.",
    "reason": "The span catalogs prior approaches and trends but does not relate them to the paper’s focus on novel-object instructions or specify a limitation that the proposed method overcomes, thus lacking synthesis (criterion a and c).",
    "start": 157,
    "end": 648,
    "label": "Lacks synthesis"
  },
  {
    "span": "BERT has been widely adopted for AES on the ASAP dataset.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) research has progressed from handcrafted features and linear models to neural architectures that capture syntax, semantics, and discourse structure. Early approaches emphasized surface features such as n-grams, length statistics, and prompt-specific heuristics, while subsequent methods incorporated syntactic parses and coherence modeling.\n\nRecent neural methods leverage pre-trained language models to better encode essay content and style. BERT has been widely adopted for AES on the ASAP dataset. Beyond simple regression over pooled representations, several systems integrate attention over sentence embeddings and incorporate prompt-aware conditioning to better handle topical relevance.\n\nDespite improvements in predictive accuracy, concerns remain about robustness to adversarial edits, fairness across demographic subpopulations, and calibration under distribution shift. To address these issues, current work explores data augmentation, multi-task learning with auxiliary linguistic signals, and training objectives that align predictions with interpretable rubric dimensions.\n\nOur study complements this line of work by revisiting representation pooling strategies in the context of long-form student writing and assessing transfer across prompts with scarce labels.",
    "reason": "Mentions a specific model usage on a named dataset without any citation to prior studies or the dataset itself, violating the requirement to cite studies/datasets at first mention.",
    "start": 487,
    "end": 544,
    "label": "Unsupported claim"
  },
  {
    "span": "MS MARCO v2 passage ranking",
    "document": "Introduction\n\nNeural retrieval has progressed from interaction-focused models to dual encoders and late-interaction architectures that scale to web-sized corpora (Guo et al., 2016; Karpukhin et al., 2020; Santhanam et al., 2022). Knowledge distillation from cross-encoders further improves first-stage retrieval quality (Hofstätter et al., 2021). Our experiments focus on MS MARCO v2 passage ranking and BEIR subsets to evaluate both in-domain and zero-shot generalization. We complement these with ablations on document-level reranking using cross-encoders.",
    "reason": "Introduces a specific dataset/benchmark without providing a citation, which should accompany first mention.",
    "start": 372,
    "end": 399,
    "label": "Unsupported claim"
  },
  {
    "span": "Sato et al., 2011; Kumar and Rao, 2018)",
    "document": "Introduction\n\nTemporal convolution and recurrence have been the dominant paradigms for acoustic modeling in ASR (Graves et al., 2013; Hannun et al., 2014). Self-attention further improved long-range context modeling and non-autoregressive decoding (Chan et al., 2020; Baevski et al., 2020). Prior work Sato et al., 2011; Kumar and Rao, 2018) examined pronunciation variation and robust training under noisy conditions, highlighting the importance of lexicon adaptation. In this paper, we revisit lexicon-aware encoders with constrained attention to better capture phonetic neighborhoods while preserving end-to-end learnability.\n",
    "reason": "Missing opening parenthesis before a parenthetical citation group; should be '(Sato et al., 2011; Kumar and Rao, 2018)'.",
    "start": 302,
    "end": 341,
    "label": "Format"
  },
  {
    "span": "Miller et al. [2022]",
    "document": "Related Work\n\nDomain adaptation for ASR has explored data augmentation, teacher–student learning, and meta-learning. Miller et al. [2022] propose a contrastive pretraining scheme that reduces word error rates under accent shift, while concurrent work refines self-training with confidence filtering (Park et al., 2020). Despite these gains, robustness to spontaneous speech remains a key bottleneck (Watanabe et al., 2017).",
    "reason": "Wrong bracket style for a narrative citation: the year is in square brackets. It should be Miller et al. (2022) for narrative style or (Miller et al., 2022) if parenthetical.",
    "start": 117,
    "end": 137,
    "label": "Format"
  },
  {
    "span": "Word embeddings encode gender and racial stereotypes (Bolukbasi et al., 2016). Counterfactual data augmentation mitigates spurious correlations (Zmigrod et al., 2019). Demographic parity and equalized odds formalize group fairness (Hardt et al., 2016). Calibration under distribution shift has also been studied (Kumar et al., 2019).",
    "document": "Related Work\n\nFairness in natural language processing spans measurement, mitigation, and evaluation under distribution shift. Methods target representational bias, dataset artifacts, and model behavior for both group and individual notions of fairness.\n\nWord embeddings encode gender and racial stereotypes (Bolukbasi et al., 2016). Counterfactual data augmentation mitigates spurious correlations (Zmigrod et al., 2019). Demographic parity and equalized odds formalize group fairness (Hardt et al., 2016). Calibration under distribution shift has also been studied (Kumar et al., 2019).\n\nOur study focuses on post-hoc calibration constrained by fairness criteria, connecting uncertainty estimation with group-level guarantees, which prior work typically considers separately.",
    "reason": "The span abruptly moves between representational bias, data augmentation, fairness metrics, and calibration without transitions or explanation of their interrelations, causing a coherence problem.",
    "start": 254,
    "end": 587,
    "label": "Coherence"
  },
  {
    "span": "It has been shown that FedAvg can diverge under highly non-IID data.",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative training over decentralized data while preserving client privacy. Optimization challenges arise from data heterogeneity, partial participation, and communication constraints. It has been shown that FedAvg can diverge under highly non-IID data. Subsequent methods introduce proximal terms, variance reduction, and client sampling strategies to mitigate drift, but trade-offs between stability and convergence speed persist.\n\nOur work complements this line by introducing adaptive mixing schedules that provably bound client drift while reducing communication overhead.",
    "reason": "The sentence attributes a known result about FedAvg divergence to prior work but does not cite any source, which is required for such a claim.",
    "start": 235,
    "end": 303,
    "label": "Unsupported claim"
  },
  {
    "span": "Most prior work evaluates solely on univariate electricity datasets.",
    "document": "Related Work\n\nTime Series Forecasting Benchmarks\n\nDeep forecasting models have progressed from sequence-to-sequence architectures toward transformers with long-horizon attention and frequency-domain components (Salinas et al., 2020; Lim et al., 2021). Benchmarks such as M4 and ETT have standardized comparisons across horizons, though differences in preprocessing remain a confounder (Makridakis et al., 2018). Most prior work evaluates solely on univariate electricity datasets. Recent efforts emphasize multivariate industrial sensors and traffic data to stress test cross-variable dependencies (Wu et al., 2021). We contribute a suite of heterogeneous datasets with unified protocols for fairer evaluation.",
    "reason": "Claims a trend in prior evaluations without citing any supporting studies, violating rule d and requiring evidence.",
    "start": 412,
    "end": 480,
    "label": "Unsupported claim"
  },
  {
    "span": "Prior work explores meta-learning for personalization (Fallah et al., 2020), clustering-based approaches (Ghosh et al., 2021), and data augmentation to mitigate heterogeneity (Yurochkin et al., 2019; Zhao et al., 2018).",
    "document": "Related Work\n\nFederated learning must contend with statistical heterogeneity across clients, leading to degraded global models. Personalization has emerged as a remedy, with methods adapting global knowledge to local preferences and data distributions. Prior work explores meta-learning for personalization (Fallah et al., 2020), clustering-based approaches (Ghosh et al., 2021), and data augmentation to mitigate heterogeneity (Yurochkin et al., 2019; Zhao et al., 2018). Additional lines investigate proximal regularization and control variates to stabilize updates under drift (Li et al., 2020; Karimireddy et al., 2020).\n\nWe focus on user-level adaptation under communication constraints.",
    "reason": "The span merely enumerates prior categories and citations with no explanation of how they relate to the present study or what gap remains (definition a), leaving the authors' stance and motivation implicit (definition c).",
    "start": 253,
    "end": 472,
    "label": "Lacks synthesis"
  },
  {
    "span": "Li et al.,",
    "document": "Related Work\n\nNeural dialogue modeling has evolved from recurrent encoder–decoders to large-scale pretraining. Foundational sequence learning used RNNs with attention (Sutskever et al., 2014; Cho et al., 2014), while recent work explores retrieval-augmented generation and controllable decoding. Several studies emphasize persona conditioning and style transfer to improve consistency (Zhang et al., 2018; Keskar et al., 2019). Encoder–decoder pretraining has become standard (Sutskever et al., 2014; Li et al., ; Cho et al., 2014), though alignment with human preferences remains an open problem (Stiennon et al., 2020).",
    "reason": "The citation fragment is missing the publication year within a parenthetical list; it should include a year, e.g., Li et al. (2016) in narrative form or (Li et al., 2016) in parenthetical form.",
    "start": 501,
    "end": 511,
    "label": "Format"
  },
  {
    "span": "The widely used BRATS dataset includes multi-institutional scans with consistent annotation protocols.",
    "document": "Related Work\n\nBrain tumor segmentation has benefited substantially from convolutional and transformer-based architectures that learn from multi-modal MRI. While early work emphasized patch-based CNNs, more recent studies leverage attention mechanisms and self-supervision to exploit 3D context and scarce labels. Semi-supervised and domain adaptation methods further mitigate disparities across scanners and protocols.\n\nThe widely used BRATS dataset includes multi-institutional scans with consistent annotation protocols. Many benchmarks therefore report validation metrics on BRATS-style splits and transfer those models to other clinical cohorts without explicit calibration. Nevertheless, differences in scanner vendors and acquisition parameters can lead to non-trivial performance drops, motivating robust adaptation strategies.\n\nOur work focuses on harmonization methods that disentangle anatomy from contrast, enabling reliable segmentation across heterogeneous sites.",
    "reason": "The sentence introduces a specific dataset (BRATS) and asserts properties about it without citing the dataset or a relevant description paper, violating the rule that datasets and specific details require citations at first mention.",
    "start": 420,
    "end": 522,
    "label": "Unsupported claim"
  },
  {
    "span": "Toxicity filters remove unsafe continuations using classifiers (Gehman et al., 2020). RLHF aligns models with human preferences (Ouyang et al., 2022). Constitutional AI uses synthetic feedback from principles (Bai et al., 2022). Jailbreak defenses harden safety layers against prompt attacks (Zou et al., 2023).",
    "document": "Related Work\n\nSafety for large language models encompasses detection, mitigation, and alignment strategies across training and inference stages. Methods differ in where and how constraints are imposed on model behaviors.\n\nToxicity filters remove unsafe continuations using classifiers (Gehman et al., 2020). RLHF aligns models with human preferences (Ouyang et al., 2022). Constitutional AI uses synthetic feedback from principles (Bai et al., 2022). Jailbreak defenses harden safety layers against prompt attacks (Zou et al., 2023).\n\nOur approach complements alignment with selective abstention under uncertainty-aware prompting.",
    "reason": "The sequence lists distinct safety techniques without transitions or explanation of their interrelations, yielding abrupt shifts and an implicit, unstated connection among the cited works.",
    "start": 222,
    "end": 533,
    "label": "Coherence"
  },
  {
    "span": "There are many recent works that explore character-level modeling for low-resource MT.",
    "document": "Related Work\n\nLow-Resource Neural Machine Translation. Neural machine translation (NMT) performance in low-resource settings has been improved by transfer learning and multilingual pretraining (Johnson et al., 2017; Aharoni et al., 2019; Conneau et al., 2020). Subword segmentation techniques such as BPE and unigram LM reduce sparsity and are now standard in low-resource pipelines (Sennrich et al., 2016; Kudo, 2018). There are many recent works that explore character-level modeling for low-resource MT. Orthographic normalization and byte-level modeling have also been investigated to mitigate data scarcity and noisy orthographies (Cherry et al., 2018; Salesky et al., 2021). Beyond architecture, data augmentation via back-translation and noising continues to offer consistent gains (Edunov et al., 2018; Fadaee et al., 2017). Our approach complements these strands by combining byte-level encoders with multilingual pretraining while maintaining a compact parameter budget.",
    "reason": "Claims the existence of 'many recent works' on a specific technique without providing any citations to support that prior literature.",
    "start": 420,
    "end": 506,
    "label": "Unsupported claim"
  },
  {
    "span": "Klein and Manning 2",
    "document": "Related Work\n\nSyntactic features remain useful for tasks requiring fine-grained structural cues. Classic parsers inspired a wave of neural models that distill tree information into continuous representations. This practice follows Klein and Manning 2 in advocating structure-aware training signals, later extended to multilingual settings with subword-aware encoders. Nevertheless, recent results suggest syntax provides complementary gains primarily under data scarcity or when fine-tuning budgets are constrained.\n\nOur method introduces selective syntax infusion via lightweight adapters, preserving efficiency while capturing structural regularities.",
    "reason": "Wrong use of a footnote number in place of a proper citation; the year is missing and the citation is not formatted according to author–year style.",
    "start": 231,
    "end": 250,
    "label": "Format"
  },
  {
    "span": "The dataset most commonly used in the literature is the HSD-1M corpus.",
    "document": "Related Work\n\nMultimodal Hate Speech Detection. Early work focused on text-only classification using social media corpora (Davidson et al., 2017; Founta et al., 2018). Recent studies integrate images and text via multimodal transformers (Kiela et al., 2020; Zia et al., 2021). The dataset most commonly used in the literature is the HSD-1M corpus. Yet labeling guidelines and prevalence of borderline content vary widely across sources.",
    "reason": "Introduces a specific dataset and claims prevalence without citing sources (violates rule a and b).",
    "start": 277,
    "end": 347,
    "label": "Unsupported claim"
  },
  {
    "span": "The FEVER dataset contains over 1 million evidence sentences.",
    "document": "Introduction\n\nAutomated fact verification requires retrieving relevant evidence and assessing the veracity of claims against trusted sources. Recent systems typically couple dense or sparse retrieval with entailment-style reasoning to aggregate support and refutation.\n\nBenchmarks have been crucial for progress by standardizing evaluation and promoting shared resources. The FEVER dataset contains over 1 million evidence sentences. Nevertheless, evidence incompleteness and annotation artifacts complicate generalization to claims outside the training distribution.",
    "reason": "Provides a specific dataset statistic without citing the dataset paper or documentation (rule a, b).",
    "start": 372,
    "end": 433,
    "label": "Unsupported claim"
  },
  {
    "span": "Fairness in recommendation has been formulated at the user, item, and platform levels with metrics such as exposure parity, calibration, and disparate impact (Yao and Huang, 2017; Burke, 2017; Singh and Joachims, 2018). Algorithmic approaches include re-ranking, regularization, and constrained optimization (Beutel et al., 2019; Zehlike et al., 2017; Narasimhan et al., 2020). In this paper, we present a framework for fair ranking under exposure constraints.",
    "document": "Related Work\n\nFairness Definitions in Recommender Systems\nRecommender systems mediate attention across users and items, motivating fairness notions for different stakeholders. Definitions vary in their handling of utility trade-offs and long-term dynamics.\n\nMethods for Fair Recommendation\nFairness in recommendation has been formulated at the user, item, and platform levels with metrics such as exposure parity, calibration, and disparate impact (Yao and Huang, 2017; Burke, 2017; Singh and Joachims, 2018). Algorithmic approaches include re-ranking, regularization, and constrained optimization (Beutel et al., 2019; Zehlike et al., 2017; Narasimhan et al., 2020). In this paper, we present a framework for fair ranking under exposure constraints.\n\nDynamics and Feedback Loops\nA growing body of work analyzes how interventions alter future data collection and feedback loops, complicating offline evaluation and policy selection.\n\nOur Contributions\nWe model exposure as a constrained resource and derive an online policy that balances short-term utility with long-horizon equity.",
    "reason": "The span lists metrics and methods and then introduces the authors' framework without explicitly stating the gap it fills or why prior approaches are inadequate, lacking synthesis per (b) and (c).",
    "start": 290,
    "end": 750,
    "label": "Lacks synthesis"
  },
  {
    "span": "[Zhang, 2016]",
    "document": "Related Work\n\nCausal inference for recommendation has received attention for debiasing logged feedback (Schnabel et al., 2016; Joachims et al., 2017). For an early survey, see [Zhang, 2016], with subsequent developments on propensity scoring (Swaminathan and Joachims, 2015) and counterfactual evaluation (Li et al., 2011).\n\nWe contribute a policy-learning objective that directly optimizes unbiased reward estimates under sparse feedback.",
    "reason": "Wrong bracket style for author–year citation; should use parentheses rather than square brackets, i.e., '(Zhang, 2016)'.",
    "start": 176,
    "end": 189,
    "label": "Format"
  },
  {
    "span": "Recent works have proposed equivariant architectures for molecular property prediction.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become the de facto approach for molecular property prediction by operating directly on atoms and bonds (Duvenaud et al., 2015; Gilmer et al., 2017). Advances include attention mechanisms for long-range interactions and message passing schemes that better capture quantum effects (Klicpera et al., 2020).\n\nRecent works have proposed equivariant architectures for molecular property prediction. These methods aim to respect SE(3) symmetries to improve generalization and data efficiency, but their computational overhead can hinder large-scale screening.",
    "reason": "The statement claims a body of 'recent works' without citing any, which requires references under rule (d).",
    "start": 354,
    "end": 441,
    "label": "Unsupported claim"
  },
  {
    "span": "Classical spectral subtraction and Wiener filtering laid the groundwork for single-channel denoising (Boll, 1979; Lim and Oppenheim, 1979). Deep learning approaches such as DNN mask estimation, LSTM regression, and time-domain separation networks further improved quality (Wang and Chen, 2018; Weninger et al., 2015; Luo and Mesgarani, 2019). Recent diffusion-based speech enhancement models generate clean waveforms via iterative denoising (Richter et al., 2021; Kim et al., 2022).",
    "document": "Related Work\n\nSpeech enhancement seeks to recover intelligible speech from noisy mixtures while preserving naturalness and low latency. Deployment scenarios such as teleconferencing and hearing aids impose strict computational budgets.\n\nClassical spectral subtraction and Wiener filtering laid the groundwork for single-channel denoising (Boll, 1979; Lim and Oppenheim, 1979). Deep learning approaches such as DNN mask estimation, LSTM regression, and time-domain separation networks further improved quality (Wang and Chen, 2018; Weninger et al., 2015; Luo and Mesgarani, 2019). Recent diffusion-based speech enhancement models generate clean waveforms via iterative denoising (Richter et al., 2021; Kim et al., 2022).\n\nWhile diffusion models achieve strong performance, their iterative sampling is often too slow for streaming use. We introduce a conditional distillation scheme with perceptual constraints that attains diffusion-level quality in a few steps.",
    "reason": "The span lists classical and modern methods without linking them to the streaming and latency constraints motivating the paper, thereby lacking synthesis per (a) and (c).",
    "start": 237,
    "end": 719,
    "label": "Lacks synthesis"
  },
  {
    "span": "(Nguyen 2016)",
    "document": "Related Work\n\nContext-aware recommenders incorporate temporal and situational signals to personalize rankings (Adomavicius and Tuzhilin, 2015). Factorization machines unify sparse feature interactions and have been widely adopted for CTR prediction (Rendle, 2010; Juan et al., 2016). Recent neural variants capture higher-order contexts via attention and sequence modeling (Kang and McAuley, 2018; Sun et al., 2019). Early work (Nguyen 2016) suggested that context drift can be mitigated through periodic recalibration of embedding spaces. We revisit this hypothesis with contrastively trained context encoders that align session and global preferences.\n",
    "reason": "Missing comma between author and year in a parenthetical citation; should be '(Nguyen, 2016)'.",
    "start": 428,
    "end": 441,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors showed that adding pitch features reduces WER by 12% on noisy speech.",
    "document": "Introduction\n\nRobust automatic speech recognition (ASR) in noisy environments remains difficult due to overlapping speakers, background sounds, and channel variability. Enhancing acoustic front-ends with prosodic and source features has been proposed as a complementary strategy to data augmentation and robust architectures. In a previous study, the authors showed that adding pitch features reduces WER by 12% on noisy speech. While encouraging, it is unclear how well these gains translate across architectures and noise types without standardized evaluation.\n\nWe provide a comprehensive analysis of prosodic feature integration across conformer and transducer backbones under controlled noise conditions.",
    "reason": "This sentence references a specific prior study and quantitative finding but omits a citation to identify the work, making it an unsupported claim.",
    "start": 326,
    "end": 428,
    "label": "Unsupported claim"
  },
  {
    "span": "Johnson et al., (2017)",
    "document": "Related Work\n\nDeep Reinforcement Learning\n\nPolicy gradient methods enable optimization in high-dimensional action spaces but suffer from high variance (Sutton et al., 2000). Johnson et al., (2017) proposed a trust-region modification to stabilize updates, while PPO simplified constraints via clipped objectives (Schulman et al., 2017). Off-policy algorithms such as DDPG (Lillicrap et al., 2016) and SAC (Haarnoja et al., 2018) further improved sample efficiency. More recently, model-based methods (Janner et al., 2019) combined learned dynamics with policy optimization, achieving competitive performance with fewer interactions.",
    "reason": "Punctuation error in narrative citation; the comma before the parenthetical year is incorrect. It should be Johnson et al. (2017).",
    "start": 174,
    "end": 196,
    "label": "Format"
  },
  {
    "span": "the RoboSumo challenge introduced a standardized multi-agent benchmark with reproducible opponents.",
    "document": "Introduction\n\nBenchmarking multi-agent reinforcement learning (MARL) requires standardized environments, reproducible opponents, and clear evaluation protocols (Lowe et al., 2017; Samvelyan et al., 2019). Competitions and open-source suites have accelerated progress by enabling head-to-head comparisons and ablation studies across algorithms and training regimes. Among these, the RoboSumo challenge introduced a standardized multi-agent benchmark with reproducible opponents. However, current leaderboards often report single-run results without confidence intervals, complicating meta-analyses of robustness and sample efficiency.",
    "reason": "This is a specific reference to a benchmark/competition without a corresponding citation, which should be provided at first mention (rule a).",
    "start": 378,
    "end": 477,
    "label": "Unsupported claim"
  },
  {
    "span": "It is well known that seasonal ARIMA outperforms deep models on electricity demand forecasting for short horizons",
    "document": "Related Work\n\nTime series forecasting methods span classical statistical models and modern deep learning approaches. While deep architectures excel at capturing complex nonlinear dependencies, classical models remain competitive for short horizons and limited data regimes due to strong inductive biases and parsimonious parameterization. It is well known that seasonal ARIMA outperforms deep models on electricity demand forecasting for short horizons, a pattern often attributed to stable seasonal structure and limited need for long-range context.\n\nRecent research explores hybrid models that integrate decomposition, exogenous variables, and probabilistic outputs to balance accuracy and interpretability. Nevertheless, systematic comparisons across domains and horizons are scarce, and evaluation protocols vary widely in data preprocessing and error aggregation.\n\nWe propose a standardized benchmark for multihorizon forecasting with consistent preprocessing and error normalization, enabling robust comparisons across statistical and deep baselines.\n",
    "reason": "Asserts a domain-specific prior finding ('well known') without any citation or evidence, violating rule b and rule a.",
    "start": 339,
    "end": 452,
    "label": "Unsupported claim"
  },
  {
    "span": "(Chen et al., 2018",
    "document": "Related Work\n\nNeural sequence modeling has advanced through improved architectures and pretraining techniques (Allen et al., 2020; Baker, 2019). Domain adaptation methods often rely on auxiliary objectives to bridge distribution gaps (Huang and Xu, 2021; Patel et al., 2020). Recent surveys (Chen et al., 2018 discuss cross-domain robustness and outline several open challenges, while later studies emphasize the role of data augmentation (Rao et al., 2022; Lin, 2021).\n\nIn contrast to earlier work on feature alignment (Kim and Park, 2017), our approach emphasizes task-consistent representations learned via contrastive signals (Nguyen et al., 2020).",
    "reason": "Missing closing parenthesis in the parenthetical citation; it should be '(Chen et al., 2018)'.",
    "start": 291,
    "end": 309,
    "label": "Format"
  },
  {
    "span": "The SNLI dataset has been extensively used to study annotation artifacts.",
    "document": "Related Work\n\nNatural language inference (NLI) benchmarks such as SNLI and MultiNLI catalyzed research on sentence-level reasoning and transfer learning (Bowman et al., 2015; Williams et al., 2018). Large pretrained encoders and hypothesis-only baselines revealed dataset biases and shallow heuristics (Poliak et al., 2018; McCoy et al., 2019). The SNLI dataset has been extensively used to study annotation artifacts. Subsequent work proposed diagnostic splits and adversarial evaluation to better measure reasoning capabilities (Kaushik et al., 2020; Nie et al., 2020).\n",
    "reason": "Claims a body of prior analysis on SNLI annotation artifacts without citing the specific works that conducted those studies (rule a).",
    "start": 345,
    "end": 418,
    "label": "Unsupported claim"
  },
  {
    "span": "Adversarial debiasing learns a representation that obfuscates protected attributes (Zhang et al., 2018). Counterfactual data augmentation has been used to reduce spurious correlations (Kaushik et al., 2020). Post-hoc explanations attribute predictions to input features (Ribeiro et al., 2016). Causal inference frameworks emphasize identification assumptions (Pearl, 2009).",
    "document": "Related Work\n\nFairness in machine learning encompasses bias detection, mitigation, and accountability mechanisms, with approaches spanning training-time interventions and post-hoc analyses. Recent surveys highlight tensions between utility and equity across demographic groups.\n\nAdversarial debiasing learns a representation that obfuscates protected attributes (Zhang et al., 2018). Counterfactual data augmentation has been used to reduce spurious correlations (Kaushik et al., 2020). Post-hoc explanations attribute predictions to input features (Ribeiro et al., 2016). Causal inference frameworks emphasize identification assumptions (Pearl, 2009).\n\nIn contrast to post-hoc explanations, we focus on training-time regularizers that enforce counterfactual invariance, integrating causal priors into the representation learning objective.",
    "reason": "Each sentence introduces a distinct area (adversarial debiasing, counterfactual augmentation, explanations, causal inference) without transitions or explicit links to each other, leaving their relationships and progression unclear.",
    "start": 279,
    "end": 652,
    "label": "Coherence"
  },
  {
    "span": "In (Perez et al., 2018)",
    "document": "Related Work\n\nPrompt-based learning reframes downstream tasks by conditioning language models on task instructions. In (Perez et al., 2018) gradient-guided prompts were explored to steer model behavior, while more recent approaches investigate discrete prompt search and tuning (Shin et al., 2020; Lester et al., 2021). Instruction-tuned models show strong generalization to unseen tasks (Sanh et al., 2022; Wei et al., 2022). However, prompt sensitivity remains a concern, motivating robust prompt optimization and calibration methods (Zhao et al., 2021).",
    "reason": "Wrong citation style: a preposition precedes a parenthetical citation. It should be narrative (Perez et al. (2018)) or the preposition removed with a purely parenthetical citation.",
    "start": 116,
    "end": 139,
    "label": "Format"
  },
  {
    "span": "Time-series causal discovery methods include Granger-based tests (Granger, 1969; Barnett and Seth, 2014), constraint-based algorithms (Spirtes et al., 2000; Runge, 2018), and representation learning approaches (Tank et al., 2018; Khanna and Tan, 2020).",
    "document": "Introduction\n\nIdentifying causal structure from multivariate time series enables reliable forecasting and what-if analysis under interventions. However, temporal dependencies, latent confounding, and nonstationarity complicate identification beyond static causal discovery settings.\n\nTime-series causal discovery methods include Granger-based tests (Granger, 1969; Barnett and Seth, 2014), constraint-based algorithms (Spirtes et al., 2000; Runge, 2018), and representation learning approaches (Tank et al., 2018; Khanna and Tan, 2020). Score-based formulations and neural additive models have also been explored (Peters et al., 2014; Pamfil et al., 2020).\n\nWe present a scalable estimator for time-varying causal graphs with adaptive regularization and demonstrate performance on synthetic and real-world datasets.",
    "reason": "The span lists method families but does not connect them to the authors' estimator or articulate why a new method is needed, satisfying criteria (a) and (c).",
    "start": 284,
    "end": 536,
    "label": "Lacks synthesis"
  },
  {
    "span": "We evaluate on the widely used OpenCity-Dialog dataset for restaurant booking.",
    "document": "Introduction\n\nTask-oriented dialogue (ToD) systems facilitate goal-directed interactions such as reservations and information lookup. A central challenge is modeling dialogue state updates under sparse supervision and noisy user inputs.\n\nWe evaluate on the widely used OpenCity-Dialog dataset for restaurant booking. Our contributions include a context-aware state tracker that leverages schema hints and a response generator conditioned on uncertainty estimates.\n\nBeyond single-domain scenarios, we also test transfer to unseen slots by synthesizing counterfactual dialogues. The results highlight the importance of modeling slot value uncertainty and leveraging schema structure for robust generalization.",
    "reason": "First mention of a specific dataset lacks a citation or source, violating rule (a) that datasets should be cited at first mention.",
    "start": 238,
    "end": 316,
    "label": "Unsupported claim"
  },
  {
    "span": "There are many recent works exploring spatio-temporal attention in traffic forecasting.",
    "document": "Related Work\n\nTraffic forecasting has evolved from classical statistical methods such as ARIMA and VAR to deep neural architectures that model graph structure and temporal dynamics. Diffusion-based recurrent models capture spatial dependencies via directed graph diffusion processes, improving over sequence-only baselines (Li et al., 2018). Spectral and localized graph convolutions with gated temporal units further enhance long-range forecasting (Yu et al., 2018; Wu et al., 2019). There are many recent works exploring spatio-temporal attention in traffic forecasting. Transformer-style encoders have also been adapted to road networks by injecting positional and relational biases to better capture non-Euclidean topology (Zhou et al., 2020). Beyond accuracy, several studies examine robustness to sensor failures and distribution shifts through adversarial training and data augmentation (Chen et al., 2021). Our study extends this line by comparing attention-driven graph architectures under strict latency constraints.\n",
    "reason": "Mentions 'recent works' without providing any citations, violating rule (d) and requiring evidence for the claim about prior work.",
    "start": 485,
    "end": 572,
    "label": "Unsupported claim"
  },
  {
    "span": "The Netflix Prize popularized matrix factorization techniques for collaborative filtering.",
    "document": "Related Work\n\nRecommender systems commonly rely on collaborative filtering, content-based methods, or hybrids to predict user–item affinities (Ricci et al., 2011). Matrix factorization (MF) provides a compact representation of users and items in a shared latent space, enabling scalable optimization and competitive accuracy (Koren et al., 2009). Implicit feedback techniques extend MF to handle click and viewing data via confidence-weighted objectives and negative sampling (Hu et al., 2008; Rendle et al., 2009).\n\nThe Netflix Prize popularized matrix factorization techniques for collaborative filtering. Subsequent work introduced neighborhood-aware and temporal dynamics models that capture evolving user tastes and seasonality effects (TimeSVD++) and improved cold-start handling with side information (Koren, 2010; Saveski and Mantrach, 2014). Deep learning approaches augment MF with non-linear interactions (He et al., 2017), while sequential recommenders model short-term dynamics with RNNs and attention (Hidasi et al., 2016; Kang and McAuley, 2018).\n\nOur work revisits MF under strict latency and memory budgets, proposing a low-rank adapter that enables on-device personalization with bounded update costs.",
    "reason": "This statement mentions a well-known competition and claims its influence on the field without citing the Netflix Prize or supporting sources; first mentions of well-known datasets/competitions should include citations.",
    "start": 517,
    "end": 607,
    "label": "Unsupported claim"
  },
  {
    "span": "Unsupervised anomaly detection in time series leverages autoencoders, variational models, and forecasting residuals to flag deviations from learned patterns (Malhotra et al., 2015; Lai et al., 2018; Audibert et al., 2020). Reconstruction-based detectors compare input and output discrepancies, while prediction-based detectors monitor one-step or multi-step errors. Hybrid methods combine reconstruction and prediction signals for robustness (Xu et al., 2018; Hundman et al., 2018).",
    "document": "Related Work\n\nDetecting anomalies in multivariate time series is critical for monitoring industrial systems, cloud infrastructure, and scientific instruments (Laptev et al., 2015; Hundman et al., 2018). The scarcity of labeled anomalies has driven interest in unsupervised and self-supervised methods.\n\nUnsupervised anomaly detection in time series leverages autoencoders, variational models, and forecasting residuals to flag deviations from learned patterns (Malhotra et al., 2015; Lai et al., 2018; Audibert et al., 2020). Reconstruction-based detectors compare input and output discrepancies, while prediction-based detectors monitor one-step or multi-step errors. Hybrid methods combine reconstruction and prediction signals for robustness (Xu et al., 2018; Hundman et al., 2018).\n\nRecent work further incorporates probabilistic calibration, conformal prediction, and change point estimation to reduce false alarms and provide uncertainty-aware alerts (Tibshirani et al., 2018; Feldman et al., 2021).",
    "reason": "The span only catalogs methods and categories without relating them to the paper's aims or stating what is missing in the literature, satisfying definition a and c.",
    "start": 303,
    "end": 785,
    "label": "Lacks synthesis"
  },
  {
    "span": "cf. (Baker, 2014)",
    "document": "Introduction\n\nCausal inference frameworks define counterfactuals and identification conditions for estimating treatment effects (Rubin, 1974; Pearl, 2009). Matching and weighting methods reduce confounding through balanced representations (Rosenbaum and Rubin, 1983; Austin, 2011). Some economists, cf. (Baker, 2014), argue for design-based strategies that prioritize quasi-experiments over complex models, while recent ML work explores doubly robust learners (Foster and Syrgkanis, 2019; Kallus and Uehara, 2020).",
    "reason": "Wrong citation style with 'cf.' placed before a parenthetical citation; should be narrative 'cf. Baker (2014)'.",
    "start": 299,
    "end": 316,
    "label": "Format"
  },
  {
    "span": "in a previous study, the authors claim that off-policy correction eliminates bias",
    "document": "Introduction\n\nReinforcement learning (RL) for recommender systems has emerged as a principled framework to optimize long-term user utility. Approaches based on contextual bandits and policy gradients aim to model sequential interactions and delayed feedback, moving beyond myopic click-through optimization. However, logging policies, exposure constraints, and selection bias complicate counterfactual evaluation and learning.\n\nA central challenge is correcting for distribution shift between the logging policy and the learned policy. While inverse propensity scoring and doubly robust estimators are common, they can suffer from high variance if propensities are small. Notably, in a previous study, the authors claim that off-policy correction eliminates bias, but practical deployments remain sensitive to model misspecification and clipping heuristics.\n\nWe introduce a calibrated estimator that regularizes importance weights with user-level uncertainty, improving stability under severe exposure skew.",
    "reason": "This span references a 'previous study' and makes a claim about its findings without citing the study, which is an unsupported mention of prior work.",
    "start": 681,
    "end": 762,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent recommender systems increasingly leverage graph neural networks to model user–item interactions, learning from message passing over bipartite graphs and session graphs (Wu et al., 2019; Ying et al., 2018; Wang et al., 2019; Xu et al., 2020). Extensions incorporate attention, higher-order connectivity, and contrastive learning to enhance representations (Velickovic et al., 2018; Zhou et al., 2020; He et al., 2020).",
    "document": "Introduction\n\nRecommender systems must infer dynamic user interests from sparse and rapidly evolving interactions. Session-based recommendation, in particular, emphasizes short-lived intents that emerge from sequences of clicks or views within a single session. Classic approaches rely on Markov chains or recurrent networks to model sequential dependencies, but these often struggle to capture rich relational structure and cross-session signals at scale.\n\nRecent recommender systems increasingly leverage graph neural networks to model user–item interactions, learning from message passing over bipartite graphs and session graphs (Wu et al., 2019; Ying et al., 2018; Wang et al., 2019; Xu et al., 2020). Extensions incorporate attention, higher-order connectivity, and contrastive learning to enhance representations (Velickovic et al., 2018; Zhou et al., 2020; He et al., 2020).\n\nDespite the progress, session signals remain noisy and vulnerable to popularity bias, and cold-start items rarely benefit from limited context. In this paper, we investigate whether structure-aware augmentation and multi-view contrastive learning can stabilize session representations without incurring heavy inference costs.\n\nOur approach constructs complementary views at the sequence and graph levels and aligns them using an adaptive temperature to balance short-term novelty with long-term preferences. We evaluate on three public session benchmarks and a large-scale production dataset, demonstrating consistent improvements in top-k precision and robustness to sparse regimes.\n\nWe release code and preprocessed splits to facilitate further research on structure-aware self-supervision for session recommendation.",
    "reason": "The span lists prior GNN-based recommendation works and extensions without connecting them to the paper’s problem, approach, or gap, thus failing to synthesize prior literature with the authors’ argument (criterion a).",
    "start": 458,
    "end": 882,
    "label": "Lacks synthesis"
  },
  {
    "span": "The MovieLens dataset has well-documented gender bias.",
    "document": "Related Work\n\nFairness in recommender systems has examined exposure disparities, calibration across user groups, and provider-side equity (Yao and Huang, 2017; Burke, 2017). Auditing methods have been proposed to diagnose demographic skews and to disentangle preference from popularity effects (Ekstrand et al., 2018).\n\nThe MovieLens dataset has well-documented gender bias. Recent mitigation strategies include reweighting, constrained optimization, and post-hoc reranking to improve group fairness while limiting utility loss (Beutel et al., 2019).",
    "reason": "The claim about bias in a specific dataset needs a citation at first mention, per rule (a) and (b).",
    "start": 320,
    "end": 374,
    "label": "Unsupported claim"
  },
  {
    "span": "BLEU correlates poorly with human judgments in open-domain dialogue and should be avoided for system ranking.",
    "document": "Introduction\n\nEvaluation of open-domain conversational agents remains challenging due to the diversity of acceptable responses and contextual dependencies. Automatic reference-based metrics have been widely adopted for convenience, but their validity is hotly debated. BLEU correlates poorly with human judgments in open-domain dialogue and should be avoided for system ranking. Consequently, recent research explores reference-free metrics, learned evaluators, and task-specific criteria such as engagingness and coherence. We contribute an evaluation protocol that combines calibrated pairwise comparisons with uncertainty-aware aggregation to produce more reliable system rankings.\n\nRelated Work\n\nPrior work on dialogue evaluation has examined surface-overlap metrics, distributional similarity, and adversarial detection, alongside human-in-the-loop methods. Despite progress, agreement among human annotators can be low, and variance across topics and prompts complicates interpretation. Our framework integrates multi-facet annotations and bootstrapped significance testing to address these issues.",
    "reason": "Makes a field-specific performance correlation claim about BLEU without citing empirical studies that demonstrate this.",
    "start": 269,
    "end": 378,
    "label": "Unsupported claim"
  },
  {
    "span": "readers fixate on nouns 60% more often than verbs",
    "document": "Introduction\n\nUnderstanding how linguistic categories guide visual attention can inform cognitively grounded NLP models. Eye-tracking research connects syntactic function and information structure to fixation probability and dwell time. Prior work suggests that content words attract more attention than function words, but the degree of category-specific emphasis remains uncertain. In particular, readers fixate on nouns 60% more often than verbs, motivating features that prioritize nominal phrases in predictive attention mechanisms.\n\nOur Study\n\nWe examine fixation distributions across parts of speech and integrate these signals into a token-importance prior for Transformer models.",
    "reason": "Presents a specific quantitative statistic about prior research without any citation or evidence (violates guideline b).",
    "start": 399,
    "end": 448,
    "label": "Unsupported claim"
  },
  {
    "span": "It is well known that demographic parity correlates poorly with individual fairness.",
    "document": "Introduction\n\nAlgorithmic fairness encompasses group-level criteria such as demographic parity and equalized odds, as well as individual-level notions that require similar treatment for similar individuals (Dwork et al., 2012; Hardt et al., 2016). Tensions among these definitions motivate context-dependent selections of metrics and interventions. It is well known that demographic parity correlates poorly with individual fairness. In this work, we study trade-offs introduced by representation learning that explicitly encodes task-relevant but non-sensitive variation while regularizing group disparities.",
    "reason": "The statement asserts a specific relationship between two fairness notions as common knowledge but provides no citation to empirical or theoretical evidence, which is required (rules b and e).",
    "start": 349,
    "end": 433,
    "label": "Unsupported claim"
  },
  {
    "span": "the SemEval-Stance 2020 shared task established stance detection as a mature evaluation benchmark",
    "document": "Related Work\n\nStance detection examines a text’s position toward a target, often under conditions of limited explicit mentions or evolving discourse. Early work focused on supervised classification over curated targets, while recent efforts aim for target-agnostic generalization and domain adaptation across platforms. In this trajectory, the SemEval-Stance 2020 shared task established stance detection as a mature evaluation benchmark, catalyzing research on few-shot transfer and robust negative class modeling.\n\nSubsequent studies investigate debiasing methods, including adversarial training and data augmentation to reduce topic leakage. Others propose calibration-aware metrics to reflect uncertainty in polarized debates. However, cross-lingual stance detection remains understudied, particularly for morphologically rich and low-resource languages.\n\nWe contribute a multilingual stance benchmark with aligned targets across five languages and analyze zero-shot transfer with parameter-efficient adapters.\n",
    "reason": "References a specific shared task and claims its impact without providing a citation at first mention, violating rule a.",
    "start": 340,
    "end": 437,
    "label": "Unsupported claim"
  },
  {
    "span": "Previous audits have found that toxicity filters disproportionately block dialectal English.",
    "document": "Introduction\n\nSafety and Fairness in Language Models. As language models are deployed at scale, content filters are used to prevent harmful outputs and moderate user inputs (Gehman et al., 2020; Welbl et al., 2021). Biases in toxicity detection can arise from spurious lexical cues and imbalanced training distributions (Sap et al., 2019; Davidson et al., 2017). Previous audits have found that toxicity filters disproportionately block dialectal English. Mitigations include counterfactual data augmentation, adversarial training, and calibrated thresholds (Ma et al., 2020; Xia et al., 2020). We study a post-hoc calibration framework with dialect-aware priors that improves parity without sacrificing overall coverage.",
    "reason": "Cites no source for a claim about findings from prior audits, which should be supported by references.",
    "start": 363,
    "end": 455,
    "label": "Unsupported claim"
  },
  {
    "span": "Early multimodal emotion recognition integrated hand-crafted audio and visual descriptors (Busso et al., 2008; Zeng et al., 2009), while recent models fuse deep encoders with attention mechanisms (Poria et al., 2017; Tsai et al., 2019; Zadeh et al., 2018).",
    "document": "Introduction\n\nMultimodal emotion recognition leverages complementary cues from speech, facial expressions, and text. Robust performance requires modeling cross-modal dynamics and coping with missing or noisy streams.\n\nEarly multimodal emotion recognition integrated hand-crafted audio and visual descriptors (Busso et al., 2008; Zeng et al., 2009), while recent models fuse deep encoders with attention mechanisms (Poria et al., 2017; Tsai et al., 2019; Zadeh et al., 2018). Self-supervised pretraining on unlabeled audiovisual data has also been explored (Albanie et al., 2018; Morgado et al., 2021).\n\nWe propose a simple masking strategy with cross-modal distillation. We test under synthetic and natural missing-modality conditions on standard benchmarks.",
    "reason": "The span recounts past methods but does not connect them to the masking strategy or specify what limitation persists, satisfying criteria (a) and (b).",
    "start": 218,
    "end": 474,
    "label": "Lacks synthesis"
  },
  {
    "span": "Early warning systems in learning analytics employ logistic regression, survival analysis, gradient boosting, LSTMs over clickstreams, and multi-view models that integrate LMS logs, demographics, and assessments (Jayaprakash et al., 2014; Dekker et al., 2009; Sweeney et al., 2016; Gardner and Brooks, 2018; Marquez-Velazquez et al., 2020). In this paper, we build a system that predicts risk using institutional data and platform traces.",
    "document": "Introduction\n\nTimely identification of at-risk students enables targeted interventions that can improve retention and success. Yet, institutions face constraints around data availability, interpretability, and equitable outcomes across student populations.\n\nEarly warning systems in learning analytics employ logistic regression, survival analysis, gradient boosting, LSTMs over clickstreams, and multi-view models that integrate LMS logs, demographics, and assessments (Jayaprakash et al., 2014; Dekker et al., 2009; Sweeney et al., 2016; Gardner and Brooks, 2018; Marquez-Velazquez et al., 2020). In this paper, we build a system that predicts risk using institutional data and platform traces.\n\nWe deploy our model in a semester-long study and analyze operational considerations such as lead time, calibration, and practitioner-centered explanations.",
    "reason": "After listing prior work, the span immediately states the authors' contribution without explicitly articulating the gap or motivation that differentiates their system (criterion b/c).",
    "start": 258,
    "end": 696,
    "label": "Lacks synthesis"
  },
  {
    "span": "Curriculum learning in reinforcement learning schedules tasks by increasing difficulty or diversity (Bengio et al., 2009; Narvekar et al., 2020). Goal generation strategies use hindsight and self-play to create progress-aligned experiences (Andrychowicz et al., 2017; OpenAI et al., 2019). Teacher-student frameworks adaptively select environments or goals based on learner competence (Graves et al., 2017; Portelas et al., 2020). Procedural content generation expands training distributions to improve generalization (Tobin et al., 2017; Cobbe et al., 2019).",
    "document": "Related Work\nLearning complex behaviors often benefits from structured exposure to tasks that guide exploration and stabilize optimization. In reinforcement learning, curricula can accelerate convergence, improve sample efficiency, and enhance generalization to unseen environments.\n\nCurriculum learning in reinforcement learning schedules tasks by increasing difficulty or diversity (Bengio et al., 2009; Narvekar et al., 2020). Goal generation strategies use hindsight and self-play to create progress-aligned experiences (Andrychowicz et al., 2017; OpenAI et al., 2019). Teacher-student frameworks adaptively select environments or goals based on learner competence (Graves et al., 2017; Portelas et al., 2020). Procedural content generation expands training distributions to improve generalization (Tobin et al., 2017; Cobbe et al., 2019).\n\nDespite these advances, practical deployment faces instability under sparse rewards and shifting task distributions. We propose a regret-guided scheduler that estimates counterfactual learning gains from off-policy logs, selecting tasks that maximize myopic improvement without hand-tuned difficulty metrics.",
    "reason": "The span enumerates methods and citations but neither identifies a concrete limitation nor relates them to the paper’s approach; it lacks synthesis and author motivation.",
    "start": 284,
    "end": 843,
    "label": "Lacks synthesis"
  },
  {
    "span": "(2019; Brown et al.)",
    "document": "Introduction\n\nUnsupervised domain adaptation aligns feature distributions to reduce shift between source and target domains (Ganin and Lempitsky, 2015; Tzeng et al., 2017). Discrepancy measures based on moments and optimal transport have shown promise (Long et al., 2015; Courty et al., 2017). Recent surveys (2019; Brown et al.) highlight the trend toward self-training with confidence thresholds (Zou et al., 2019; Xu et al., 2020) and the emergence of test-time adaptation (Sun et al., 2020; Wang et al., 2021).\n",
    "reason": "Incorrect order/content in parenthetical citation; the year appears without authors and is separated from authors incorrectly. It should be '(Brown et al., 2019)'.",
    "start": 309,
    "end": 329,
    "label": "Format"
  },
  {
    "span": "End-to-end ASR with attention improves sequence modeling over hybrid HMM systems (Chan et al., 2016). CTC provides monotonic alignment for streaming ASR (Graves et al., 2006). Self-supervised pretraining on unlabeled speech boosts performance in low-resource settings (Baevski et al., 2020). Multilingual training shares acoustic units across languages (Pratap et al., 2020).",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN pipelines to end-to-end architectures that jointly learn acoustic and language modeling components. Recent efforts focus on reducing labeled data needs while maintaining streaming performance.\n\nEnd-to-end ASR with attention improves sequence modeling over hybrid HMM systems (Chan et al., 2016). CTC provides monotonic alignment for streaming ASR (Graves et al., 2006). Self-supervised pretraining on unlabeled speech boosts performance in low-resource settings (Baevski et al., 2020). Multilingual training shares acoustic units across languages (Pratap et al., 2020).\n\nBuilding on these directions, we study semi-autoregressive transducers pretrained with masked acoustic modeling and adapted with language-specific pronunciation lexicons for improved latency-accuracy trade-offs.",
    "reason": "The span shifts topics sentence by sentence (attention-based ASR, CTC alignment, self-supervised pretraining, multilingual training) without transitions or explicit connections, making the relationships among the cited works unclear.",
    "start": 280,
    "end": 655,
    "label": "Coherence"
  },
  {
    "span": "A large body of prior work has settled on unit test pass rate as the only reliable metric.",
    "document": "Related Work\n\nEvaluation of Code Generation. Program synthesis systems are commonly assessed by unit-test pass rate, functional correctness, and static analysis signals (Kulal et al., 2019; Chen et al., 2021). Newer benchmarks introduce code style and security checks (Austin et al., 2021). A large body of prior work has settled on unit test pass rate as the only reliable metric. However, tests can be incomplete and encourage overfitting to benchmark specifics.",
    "reason": "Asserts a consensus in prior work without any citations to support the claim (violates rule b and d).",
    "start": 291,
    "end": 381,
    "label": "Unsupported claim"
  },
  {
    "span": "In task-oriented dialogue, BERT was used in an end-to-end setup trained on dialogues with delexicalized slots.",
    "document": "Related Work\n\nTask-oriented dialogue systems aim to track user goals, manage system actions, and generate appropriate responses grounded in a target knowledge base. Classical pipelines decompose the problem into NLU, dialog state tracking, policy learning, and NLG. Recent neural approaches explore end-to-end training to reduce error propagation across modules.\n\nIn task-oriented dialogue, BERT was used in an end-to-end setup trained on dialogues with delexicalized slots. This line of work suggests that pretraining improves both belief tracking and response selection, especially in low-resource domains.\n\nOur work extends end-to-end modeling with a retrieval-augmented decoder that integrates API results at generation time. We introduce a domain-balanced benchmark split and evaluate robustness under slot rephrasing and unseen entities, demonstrating improvements in both goal completion and response appropriateness.\n\nFurther Discussion\n\nExisting studies compare pretrained encoders across multi-domain datasets and investigate data augmentation via schema paraphrases. However, systematic analysis of delexicalization strategies remains limited, motivating our ablations.",
    "reason": "Describes a specific methodological setup employing BERT without citing the work that introduced or evaluated it.",
    "start": 364,
    "end": 474,
    "label": "Unsupported claim"
  },
  {
    "span": "Neural sequence-to-sequence models generate chit-chat responses using encoder–decoder architectures (Vinyals and Le, 2015). Task-oriented systems perform belief tracking to manage slot–value states (Mrkšić et al., 2017). Large pretrained language models improve open-domain dialogue quality (Zhang et al., 2020). Automatic metrics like BLEU show weak correlation with human judgments (Liu et al., 2016).",
    "document": "Related Work: Dialogue Systems and Evaluation\n\nDialogue research spans open-domain conversation and task-oriented assistants, with recent advances driven by pretraining and better modeling of context and user goals. Evaluation remains challenging due to diversity and ambiguity of valid responses.\n\nNeural sequence-to-sequence models generate chit-chat responses using encoder–decoder architectures (Vinyals and Le, 2015). Task-oriented systems perform belief tracking to manage slot–value states (Mrkšić et al., 2017). Large pretrained language models improve open-domain dialogue quality (Zhang et al., 2020). Automatic metrics like BLEU show weak correlation with human judgments (Liu et al., 2016).\n\nSubsequent work proposes learned reference-free metrics that predict human ratings from context–response pairs (Mehri and Eskenazi, 2020). Others introduce controllable generation for safety and style (See et al., 2019; Xu et al., 2020). We contribute an evaluation protocol that couples uncertainty estimates with human-in-the-loop error analysis.",
    "reason": "The span presents disparate topics—open-domain seq2seq, task-oriented belief tracking, LMs, and evaluation metrics—without transitions or an explicit thread connecting them, resulting in abrupt, incoherent shifts (issues a and b).",
    "start": 299,
    "end": 702,
    "label": "Coherence"
  },
  {
    "span": "BERT was used in an AES task trained on ASAP essays with a pairwise ranking objective",
    "document": "Related Work\n\nAutomatic Essay Scoring (AES) research spans feature-engineered regressors to end-to-end neural models. Early approaches emphasized handcrafted indicators of coherence, syntax, and discourse, while transformer-based encoders now dominate due to superior contextualization. BERT was used in an AES task trained on ASAP essays with a pairwise ranking objective, purportedly improving robustness to prompt variation. Concurrent lines study domain adaptation, fairness, and prompt-agnostic scoring, but often report inconsistent cross-prompt performance.\n\nIntroduction\n\nWe revisit cross-prompt generalization by disentangling lexical memorization from structural proficiency, aiming to assess what AES models truly learn about writing quality across topics and genres.",
    "reason": "This sentence asserts a specific prior setup (BERT used on ASAP with pairwise ranking) without citing the study that did this.",
    "start": 287,
    "end": 372,
    "label": "Unsupported claim"
  },
  {
    "span": "The standard SuperGLUE probing protocol uses 32 demonstration examples per task.",
    "document": "Related Work\n\nFew-shot prompting evaluates pretrained language models on NLP benchmarks with a limited number of exemplars. The standard SuperGLUE probing protocol uses 32 demonstration examples per task. Alternative selection strategies score candidates by similarity or uncertainty, but the effect of demonstration diversity has been underexplored.",
    "reason": "States a specific procedural detail about a widely used benchmark without a citation to the protocol or originating paper.",
    "start": 124,
    "end": 204,
    "label": "Unsupported claim"
  },
  {
    "span": "Ramirez and Lee (2020) presented a transformer backbone for detection. Chandra et al. (2021) proposed deformable attention to reduce quadratic cost. Ito and Park (2022) studied multi-scale feature aggregation with hierarchical tokens. Greene et al. (2023) examined zero-shot transfer using frozen vision-language encoders.",
    "document": "Related Work\n\nObject detection has transitioned from convolutional backbones to transformer-based designs that model long-range dependencies. These advances affect capacity, scalability, and data efficiency. We review representative transformer approaches and training regimes.\n\nRamirez and Lee (2020) presented a transformer backbone for detection. Chandra et al. (2021) proposed deformable attention to reduce quadratic cost. Ito and Park (2022) studied multi-scale feature aggregation with hierarchical tokens. Greene et al. (2023) examined zero-shot transfer using frozen vision-language encoders.\n\nOther works integrate self-training and weak supervision to exploit web-scale images (Ma et al., 2023). Our method aligns detection queries with textual anchors to improve open-vocabulary generalization.",
    "reason": "Multiple sentences list separate papers with no connective phrases or explanation of how each development builds upon or contrasts with the previous one, resulting in abrupt topic shifts and unclear relations.",
    "start": 279,
    "end": 601,
    "label": "Coherence"
  },
  {
    "span": "The first Cross-Lingual Summarization Shared Task standardized evaluation with Pyramid Scores.",
    "document": "Related Work\n\nCross-lingual summarization (CLS) aims to generate summaries in a target language given source-language documents. Early approaches decomposed the problem into translation followed by monolingual summarization, resulting in error propagation across stages. More recent encoder–decoder architectures attempt end-to-end learning from parallel document–summary pairs, though data scarcity remains a central obstacle.\n\nCommunity evaluations have played an important role in shaping the CLS landscape. The first Cross-Lingual Summarization Shared Task standardized evaluation with Pyramid Scores. Subsequent campaigns emphasized human judgments of adequacy and fluency, but often with limited transparency around annotator training and inter-rater reliability.\n\nBeyond shared tasks, benchmark curation has expanded to include low-resource language pairs and domain-specific corpora such as newswire, legislative text, and scientific abstracts. Nevertheless, consistent tokenization, reference quality, and normalization practices vary widely, complicating cross-paper comparisons.\n\nOur work contributes a multilingual, domain-balanced benchmark with script-normalized references and an alignment-aware evaluation suite that harmonizes content coverage metrics with sentence-level faithfulness checks.",
    "reason": "This is the first mention of a specific shared task and its claimed standardization, but no citation is provided to identify the task or evidence for the claim (rule a).",
    "start": 511,
    "end": 605,
    "label": "Unsupported claim"
  },
  {
    "span": "(Basu and Hart)",
    "document": "Introduction\n\nRobotic grasp planning balances geometric reasoning with uncertainty from sensing and actuation. Analytic methods use wrench space and force closure criteria (Navarro and Kim, 2016), while data-driven approaches learn grasp affordances from images (Qiao et al., 2018). Domain randomization improves sim-to-real transfer (Singh and Patel, 2020). Prior benchmarks evaluate on household objects and bin-picking tasks (Rossi and Ahmed, 2019). A line of work argues for task-oriented grasps that consider downstream manipulation (Basu and Hart). We contribute a policy that conditions on task embeddings and demonstrate gains on assembly tasks with tight tolerances.",
    "reason": "Parenthetical citation missing year; should include the publication year, e.g., '(Basu and Hart, YEAR)'.",
    "start": 538,
    "end": 553,
    "label": "Format"
  },
  {
    "span": "Recently, several studies have explored prompt engineering and decoding strategies for code generation (Chen et al., 2021; Austin et al., 2021; Li et al., 2022; Nijkamp et al., 2023). Methods include few-shot prompting with exemplars, chain-of-thought rationales, and self-consistency sampling to improve correctness and style adherence.",
    "document": "Introduction\n\nProgram synthesis from natural language has advanced rapidly with large language models that can map informal intent to executable code. Despite this progress, generated programs often fail on edge cases or violate implicit constraints of target APIs.\n\nRecently, several studies have explored prompt engineering and decoding strategies for code generation (Chen et al., 2021; Austin et al., 2021; Li et al., 2022; Nijkamp et al., 2023). Methods include few-shot prompting with exemplars, chain-of-thought rationales, and self-consistency sampling to improve correctness and style adherence.\n\nConcurrent efforts examine post-generation repair via unit tests or constraint solvers, but these methods require either test availability or heavy search budgets. In this paper, we introduce Static-CoT, a static-analysis-aware prompting framework that weaves lightweight analyses into the reasoning and sampling process to steer models toward specification-compliant outputs. We evaluate Static-CoT on multi-function synthesis tasks and show consistent gains in pass@k with minimal overhead.\n\nOur contributions are: a modular prompting recipe that incorporates pre-execution checks, an uncertainty-triggered refinement loop, and a benchmark of static-violation categories that commonly appear in LLM-generated code.",
    "reason": "The span summarizes prior work on prompting and decoding without explaining how it relates to the paper's approach or what specific gap remains (criteria a and c).",
    "start": 267,
    "end": 604,
    "label": "Lacks synthesis"
  },
  {
    "span": "Most recent VQA models adopt cross-modal transformers",
    "document": "Related Work\n\nVisual question answering (VQA) requires reasoning over both visual content and natural language. Early approaches fused CNN features with RNN question encoders, while later methods incorporated attention to condition visual processing on language. With the advent of large-scale pretraining, multimodal transformers have become increasingly popular.\n\nMost recent VQA models adopt cross-modal transformers, integrating image regions and token embeddings through self- and cross-attention. Despite strong accuracy on standard benchmarks, these models often exhibit shortcut reliance and poor calibration under distribution shift.\n\nWe propose a compositional training curriculum that encourages multi-hop reasoning and reduces sensitivity to spurious correlations.",
    "reason": "The claim references 'most recent VQA models' and describes their architecture without citing representative works, which is an unsupported mention of prior work.",
    "start": 366,
    "end": 419,
    "label": "Unsupported claim"
  },
  {
    "span": "According to industry reports, 73% of production chatbots rely on intent classification pipelines",
    "document": "Introduction\n\nTask-oriented dialogue systems commonly rely on intent classification and slot filling to map user utterances to actionable frames (Henderson et al., 2014; Goo et al., 2018). Joint models have improved performance by sharing features across tasks and leveraging pretrained encoders (Devlin et al., 2019; Radford et al., 2019). According to industry reports, 73% of production chatbots rely on intent classification pipelines. Motivated by this prevalence, we investigate parameter-efficient finetuning strategies that preserve accuracy under domain shift.",
    "reason": "Presents a concrete statistic ('73%') based on 'industry reports' without citing any source, which requires evidence.",
    "start": 341,
    "end": 438,
    "label": "Unsupported claim"
  },
  {
    "span": "the IEMOCAP dataset is the de facto standard benchmark for SER",
    "document": "Introduction\n\nSpeech Emotion Recognition (SER) seeks to infer affective states from acoustic and sometimes lexical signals. Progress is often measured via supervised classification on acted and semi-natural corpora with diverse label taxonomies and recording conditions. Among available corpora, the IEMOCAP dataset is the de facto standard benchmark for SER, with researchers frequently reporting weighted and unweighted accuracy.\n\nRelated Work\n\nApproaches range from spectrogram CNNs and temporal CNN-RNN hybrids to attention-equipped transformers and self-supervised pretraining on large-scale speech corpora. Multimodal fusion incorporating text and facial cues has also shown promise, though label alignment and missing modality issues remain challenges.",
    "reason": "Declares a dataset as the 'de facto standard' without any citation to support this status.",
    "start": 296,
    "end": 358,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT has been widely used in prompt-specific AES setups with cross-prompt transfer showing clear gains.",
    "document": "Related Work\n\nAutomatic Essay Scoring. Neural AES systems have evolved from feature-based regressors (Attali and Burstein, 2006) to deep models with pre-trained encoders (Dong et al., 2017; Zhang and Litman, 2018). With the advent of contextualized models, researchers explored BERT-style encoders for scoring coherence and content relevance. BERT has been widely used in prompt-specific AES setups with cross-prompt transfer showing clear gains. Other work examines calibration and fairness concerns (Yannakoudakis et al., 2018).",
    "reason": "Asserts widespread prior usage and performance gains without citing any specific studies (violates rule a and d).",
    "start": 343,
    "end": 446,
    "label": "Unsupported claim"
  },
  {
    "span": "Fairness in recommender systems has been characterized at user, item, and provider levels with metrics such as disparate impact, exposure parity, and calibration gaps (Burke, 2017; Ekstrand et al., 2022; Singh and Joachims, 2018). Mitigation strategies include reweighting, regularization, and post-hoc re-ranking to trade off utility and equity (Yao and Huang, 2017; Kamishima et al., 2012; Biega et al., 2018).",
    "document": "Related Work\n\nPersonalized recommendation affects how users and providers interact, often amplifying existing inequalities. As a result, the community has explored definitions and interventions for fairness at multiple levels of the recommendation pipeline.\n\nFairness in recommender systems has been characterized at user, item, and provider levels with metrics such as disparate impact, exposure parity, and calibration gaps (Burke, 2017; Ekstrand et al., 2022; Singh and Joachims, 2018). Mitigation strategies include reweighting, regularization, and post-hoc re-ranking to trade off utility and equity (Yao and Huang, 2017; Kamishima et al., 2012; Biega et al., 2018).\n\nIn this work, we introduce Constrained Exposure Calibration, a practical post-processing method that achieves item-group exposure targets with bounded loss in ranking utility, requiring no retraining.",
    "reason": "The span enumerates metrics and mitigation methods without clarifying how they inform or contrast with the proposed approach, leaving the gap and the authors’ perspective implicit (criteria a and c).",
    "start": 259,
    "end": 671,
    "label": "Lacks synthesis"
  },
  {
    "span": "Over 70% of clinical notes contain at least one dosage abbreviation.",
    "document": "Introduction\n\nClinical text mining supports downstream applications such as adverse event detection, cohort selection, and medication reconciliation. However, notes are rife with idiosyncratic abbreviations, misspellings, and shorthand that challenge standard NLP pipelines. Over 70% of clinical notes contain at least one dosage abbreviation. This motivates our normalization module, which jointly infers expanded forms and dosage units using a constrained decoder. We integrate normalization with a medication extraction model and evaluate on multi-institutional data to assess portability across documentation styles.\n\nRelated Work\n\nExisting normalization strategies include dictionary-based expansion and sequence labeling with copy mechanisms. Domain adaptation remains critical due to institution-specific abbreviation inventories.",
    "reason": "The numerical prevalence claim lacks any citation or evidence; statistics should be supported by references.",
    "start": 275,
    "end": 343,
    "label": "Unsupported claim"
  },
  {
    "span": "Most previous works pretrain with 4 to 6 objectives such as MLM, ITM, and MRM.",
    "document": "Related Work\n\nVision-language pretraining (VLP) aligns visual and textual representations to support downstream tasks such as VQA, retrieval, and captioning (Lu et al., 2019; Tan and Bansal, 2019). Objectives span masked language modeling (MLM), image-text matching (ITM), masked region modeling (MRM), and contrastive losses (Radford et al., 2021; Li et al., 2021). Most previous works pretrain with 4 to 6 objectives such as MLM, ITM, and MRM. Despite progress, negative transfer from mismatched objectives remains an open challenge (Gao et al., 2021). We study objective selection via sparse mixture-of-losses.",
    "reason": "Generalizes about 'most previous works' and specific objective counts without any citations (rules b and d).",
    "start": 367,
    "end": 445,
    "label": "Unsupported claim"
  },
  {
    "span": "(Nguyen et al., 2019",
    "document": "Introduction\n\nProgram synthesis from natural language has advanced through neural decoders that map text to executable code (Yin and Neubig, 2017; Rabinovich et al., 2017). Early sequence-to-sequence models struggled with compositionality and long-range dependencies (Lake and Baroni, 2018). Later systems incorporated grammar constraints and type checking to improve correctness (Chen and Zhou, 2018; Brockschmidt et al., 2019).\n\nDespite progress, systematic generalization remains elusive (Keysers et al., 2020). Prior evaluations often conflate lexical memorization with structural reasoning (Johnson et al., 2017). To address this, we design datasets that separate lexical novelty from structural novelty and evaluate models under both conditions, extending the analysis in (Nguyen et al., 2019 and follow-up work (Huang et al., 2021).\n\nWe further propose diagnostics that quantify reliance on spurious correlations, revealing when models shortcut via co-occurrence statistics rather than learning true program semantics.",
    "reason": "Missing closing parenthesis in a parenthetical citation.",
    "start": 778,
    "end": 798,
    "label": "Format"
  },
  {
    "span": "Recent competitions demonstrate that neural solvers now surpass classical enumerative search on standard benchmarks.",
    "document": "Introduction\n\nProgram synthesis seeks to automatically generate programs consistent with a specification, typically given as input–output examples, partial sketches, or natural language descriptions. Classic techniques rely on enumerative or constraint-based search with pruning heuristics, while modern methods incorporate neural components for guidance. Recent competitions demonstrate that neural solvers now surpass classical enumerative search on standard benchmarks. Despite this progress, generalization to out-of-distribution specifications and robustness to spurious correlations remain open problems. We propose a hybrid approach that uses neural models for proposal generation and a symbolic verifier to ensure correctness and compositionality.\n\nRelated Work\n\nNeural guided search, differentiable interpreters, and sketch completion have each shown promise, yet their comparative advantages are context dependent. Our framework unifies these strands via a modular interface between proposal, constraint propagation, and verification.",
    "reason": "Asserts evidence from 'recent competitions' without citing the competitions or results (criterion d).",
    "start": 356,
    "end": 472,
    "label": "Unsupported claim"
  },
  {
    "span": "Transformer-based recommenders dominate industry deployments.",
    "document": "Introduction\n\nSequential Recommendation\n\nRecommender systems increasingly exploit sequence modeling to capture evolving user preferences. Transformer-based recommenders dominate industry deployments. Self-attention allows flexible modeling of long-range dependencies in interaction histories, and side information can be integrated via item embeddings and adapters. However, data sparsity and position bias still limit performance, motivating auxiliary objectives and contrastive pretraining.",
    "reason": "Broad claim about industry adoption without evidence or references (rule b).",
    "start": 138,
    "end": 199,
    "label": "Unsupported claim"
  },
  {
    "span": "Large language models pre-trained on code improve zero-shot synthesis (Chen et al., 2021). Neural execution engines learn to mimic program semantics (Graves et al., 2014; Bunel et al., 2018). Dataset curation impacts the difficulty of natural language-to-code benchmarks (Austin et al., 2021).",
    "document": "Related Work\n\nProgram synthesis from natural language has advanced with sequence-to-sequence models and structured decoders that aim to ensure syntactic validity and semantic fidelity (Yin and Neubig, 2017). Pretraining and execution guidance are two complementary directions explored in recent years.\n\nLarge language models pre-trained on code improve zero-shot synthesis (Chen et al., 2021). Neural execution engines learn to mimic program semantics (Graves et al., 2014; Bunel et al., 2018). Dataset curation impacts the difficulty of natural language-to-code benchmarks (Austin et al., 2021). Our work studies controllable decoding that integrates lightweight interpreters while mitigating data leakage.\n",
    "reason": "The sequence of citations spans pretraining, neural execution, and dataset issues with no connective explanation; the relation between these works is only implied.",
    "start": 303,
    "end": 596,
    "label": "Coherence"
  },
  {
    "span": "recent large-scale user studies show that explanations increase trust regardless of accuracy",
    "document": "Introduction\n\nExplainable AI (XAI) seeks to make model decisions interpretable to end users, auditors, and regulators. A central motivation is the belief that transparency can improve trust, safety, and accountability in deployed systems. However, the relationship between explanation quality, accuracy, and user trust is complex. In practice, recent large-scale user studies show that explanations increase trust regardless of accuracy, raising concerns that persuasive but misleading rationales may backfire in safety-critical contexts.\n\nThis paper proposes a trust-aware evaluation protocol for XAI that disentangles persuasiveness from informativeness. We measure how explanations impact calibrated trust, error detection, and reliance under covariate shift. We also present a set of explanation alignment metrics that correlate with user ability to identify model failures.\n",
    "reason": "Claims findings from 'recent large-scale user studies' without citing any of them, violating rule d and rule a.",
    "start": 344,
    "end": 436,
    "label": "Unsupported claim"
  },
  {
    "span": "In recent years, numerous studies have demonstrated that self-supervised pretraining dramatically reduces the need for annotated radiology reports.",
    "document": "Introduction\n\nAutomated analysis of radiology reports has emerged as a critical application of natural language processing in clinical settings. Labeled data are scarce and expensive to obtain due to the need for domain experts and institutional review. As a result, leveraging unlabeled corpora with self-supervised objectives has become an attractive research direction.\n\nIn recent years, numerous studies have demonstrated that self-supervised pretraining dramatically reduces the need for annotated radiology reports. Building on this intuition, we examine contrastive and masked language modeling objectives tailored to the structure and terminology of radiological narratives. We hypothesize that pretraining on large-scale, heterogeneous hospital archives can yield representations that transfer across reporting styles and institutions.\n\nOur contributions are threefold: (1) we curate a multi-institutional corpus spanning chest X-ray, CT, and MRI reports; (2) we introduce domain-adaptive pretraining tasks that exploit section headers and impression–findings alignments; and (3) we provide a comprehensive evaluation across classification, tagging, and entailment tasks in radiology. We release code and pretrained models to facilitate reproducibility and downstream adoption.",
    "reason": "Claims that many studies showed a concrete effect in a niche domain without providing any citations at first mention.",
    "start": 374,
    "end": 521,
    "label": "Unsupported claim"
  },
  {
    "span": "There have been many recent works demonstrating that cross-silo FL reliably outperforms centralized baselines on EHR tasks.",
    "document": "Introduction\n\nFederated learning (FL) enables multiple institutions to train a shared model without exchanging raw data, thereby addressing privacy and governance constraints that arise in healthcare. Early FL work established the core algorithmic framework for distributed optimization under privacy and communication constraints (McMahan et al., 2017; Kairouz et al., 2021). In the clinical domain, FL has been explored for predictive modeling on electronic health records (EHR) and medical imaging, showing promise for multi-institutional collaboration under regulatory limitations (Rieke et al., 2020; Sheller et al., 2020).\n\nDespite these advances, the community still lacks systematic evidence on when FL provides gains over centralized or siloed learning in real-world EHR settings with heterogeneous coding schemes and missingness patterns. There have been many recent works demonstrating that cross-silo FL reliably outperforms centralized baselines on EHR tasks. However, reported benefits vary with client participation, data imbalance, and local preprocessing pipelines. In this paper, we investigate robustness to these factors by introducing a stratified client sampling protocol and an adaptive aggregation scheme.\n\nRelated Work\n\nPrivacy-preserving analytics in healthcare has considered secure aggregation and differential privacy to mitigate leakage in gradients and updates (Bonawitz et al., 2017; Li et al., 2022). Methods for handling heterogeneity in FL include personalization layers, clustered aggregation, and representation sharing (Smith et al., 2017; Arivazhagan et al., 2019; Mansour et al., 2020). For EHR modeling, temporal architectures and label harmonization techniques have been adapted to federated settings (Lee et al., 2021; Xu et al., 2022). Our work complements these efforts by focusing on cross-silo robustness under realistic participation and shift patterns.",
    "reason": "Mentions 'many recent works' without providing citations to those works (rule d: claims about recent works must be backed by citations).",
    "start": 849,
    "end": 972,
    "label": "Unsupported claim"
  },
  {
    "span": "Counterfactual fairness formalizes invariance to protected attributes (Kusner et al., 2017). Exposure-aware ranking balances opportunities for providers (Singh and Joachims, 2018). Calibration constrains predicted ratings to match observed frequencies (Patro et al., 2020). Differential privacy bounds information leakage in recommendations (McSherry and Mironov, 2009).",
    "document": "Related Work\n\nFairness in recommender systems spans user-facing outcomes, provider exposure, and systemic biases introduced by feedback loops. Recent surveys categorize interventions into pre-processing, in-processing, and post-processing methods that act on data, models, and outputs.\n\nCounterfactual fairness formalizes invariance to protected attributes (Kusner et al., 2017). Exposure-aware ranking balances opportunities for providers (Singh and Joachims, 2018). Calibration constrains predicted ratings to match observed frequencies (Patro et al., 2020). Differential privacy bounds information leakage in recommendations (McSherry and Mironov, 2009).\n\nWe complement these directions by introducing an exposure-robust learning objective that jointly regularizes user utility and provider parity under position bias.",
    "reason": "The span abruptly lists disparate fairness and privacy concepts without transitions or clarifying how they relate, leading to unclear connections between the cited works.",
    "start": 287,
    "end": 657,
    "label": "Coherence"
  },
  {
    "span": "Prior studies have shown that freezing the encoder while tuning only adapters yields at most a 1% drop.",
    "document": "Related Work\n\nParameter-efficient fine-tuning techniques reduce compute and storage costs by updating a small subset of model parameters. Approaches include adapters inserted between transformer layers, low-rank updates to attention projections, and prompt-based tuning. These methods enable rapid task switching and multi-task deployment with shared backbones.\n\nPrior studies have shown that freezing the encoder while tuning only adapters yields at most a 1% drop. Despite this promise, little is known about their behavior under distribution shift or in low-label regimes. We therefore conduct a systematic analysis across domains and sample budgets, focusing on stability and calibration.\n\nOur work complements investigations into transferability and layer-wise adaptation by proposing diagnostics for representation drift and by reporting storage–accuracy trade-offs at scale.",
    "reason": "Provides a quantitative performance claim attributed to prior studies but includes no citations to support the statistic.",
    "start": 363,
    "end": 466,
    "label": "Unsupported claim"
  },
  {
    "span": "Several works have proposed graph-based techniques for stance detection in social media.",
    "document": "Related Work\n\nStance detection aims to infer a user's position toward a target from their posts, leveraging textual signals, user metadata, and conversational context (Mohammad et al., 2017; Sun and Peng, 2020). Pretrained language models with target-aware conditioning have advanced performance across benchmarks (Ghosh et al., 2021; Li and Caragea, 2022).\n\nSeveral works have proposed graph-based techniques for stance detection in social media. In parallel, propagation-based models use conversation trees and user interaction graphs to incorporate relational inductive biases (Baly et al., 2018; Ma et al., 2021). Our approach unifies textual and structural cues via heterogeneous graph transformers with contrastive alignment between post and user subgraphs.",
    "reason": "The span claims prior graph-based approaches exist but does not cite any such works at first mention, violating rule (a) and (d).",
    "start": 359,
    "end": 447,
    "label": "Unsupported claim"
  },
  {
    "span": "Hardt et al. (2016) formalize equalized odds as a group fairness criterion. Kleinberg et al. (2017) prove incompatibility results between calibration and balance. Barocas et al. (2019) discuss legal perspectives on algorithmic fairness. Mitchell et al. (2019) propose model cards for transparent reporting.",
    "document": "Introduction\n\nOperationalizing fairness in machine learning involves specifying measurable criteria, understanding trade-offs, and instituting documentation and governance processes. Different communities emphasize mathematical definitions, institutional constraints, and practical tooling.\n\nHardt et al. (2016) formalize equalized odds as a group fairness criterion. Kleinberg et al. (2017) prove incompatibility results between calibration and balance. Barocas et al. (2019) discuss legal perspectives on algorithmic fairness. Mitchell et al. (2019) propose model cards for transparent reporting.\n\nWe build on these foundations by framing fairness interventions as policy gradients in a constrained optimization view, linking metrics, auditing artifacts, and decision policies within a single objective.",
    "reason": "This span strings together four works from metrics, impossibility results, legal analysis, and documentation without transitions or explanation of how they interrelate. The connection is implicit and abrupt across multiple sentences (criterion a, b, c).",
    "start": 292,
    "end": 598,
    "label": "Coherence"
  },
  {
    "span": "Smith et al.",
    "document": "Related Work\n\nCross-lingual transfer in neural machine translation has been explored from the perspectives of shared subword vocabularies and multilingual encoders (Conneau et al., 2020; Artetxe et al., 2019). As demonstrated by Smith et al., alignment in the latent space can substantially reduce parallel data requirements. Subsequent work introduced iterative back-translation and dual learning to further boost low-resource performance (Sennrich et al., 2016; He et al., 2016). Our approach extends these ideas by incorporating language-adaptive pretraining with constrained decoding for name preservation.",
    "reason": "Narrative citation is missing the publication year. It should appear as a narrative citation with year, e.g., \"Smith et al. (2020)\".",
    "start": 229,
    "end": 241,
    "label": "Format"
  },
  {
    "span": "earlier work reported that test-suite adequacy strongly correlates with repair success",
    "document": "Introduction\n\nAutomated program repair (APR) aims to generate patches that make failing test suites pass while preserving program correctness. Search-based methods and neural program synthesis have both shown promise, but overfitting to weak tests leads to spurious patches. Understanding the interaction between test quality and patch validity is therefore crucial.\n\nIn particular, earlier work reported that test-suite adequacy strongly correlates with repair success, suggesting that augmenting tests can mitigate overfitting. However, few studies quantify how adequacy interacts with the search space induced by different patch operators.\n\nWe introduce an operator-aware adequacy metric and a controlled benchmark to systematically assess their joint effects on APR outcomes.",
    "reason": "This statement attributes a specific finding to prior work without citing it (rule b/e: prior claims and empirical correlations must be referenced).",
    "start": 383,
    "end": 469,
    "label": "Unsupported claim"
  },
  {
    "span": "standard fine-tuning protocols freeze the patch embedding layer",
    "document": "Related Work\n\nVision Transformers (ViTs) adapt self-attention to images by operating over non-overlapping patches. Fine-tuning pre-trained ViTs for downstream tasks typically involves adjusting learning rates, layer-wise decay, and augmentation strategies.\n\nIn transfer from large-scale pretraining, standard fine-tuning protocols freeze the patch embedding layer and update only higher transformer blocks to prevent overfitting on small datasets. Other works instead tune all parameters with strong regularization or use adapters to reduce the number of trainable weights.\n\nOur approach contrasts these strategies by jointly learning a lightweight re-parameterization of the patch projection and a task-specific prompt, improving data efficiency on few-shot classification.",
    "reason": "The claim describes a specific, commonly used training setup as prior practice but does not provide citations to works that employ this protocol.",
    "start": 300,
    "end": 363,
    "label": "Unsupported claim"
  },
  {
    "span": "(Duarte et al, 2017)",
    "document": "Related Work\n\nMedical image segmentation has progressed with encoder–decoder architectures and skip connections enabling fine-grained localization (Ronneberger et al., 2015; Milletari et al., 2016). Attention gates and multi-scale context aggregation improve delineation of small structures (Oktay et al., 2018; Chen et al., 2018). Semi-supervised methods exploit consistency regularization and pseudo-labeling to reduce annotation costs (Bai et al., 2017; Yu et al., 2019).\n\nDomain adaptation addresses scanner variability and protocol shifts via adversarial training and feature normalization (Kamnitsas et al., 2017; Dou et al., 2018). Test-time adaptation further refines predictions under distribution shifts (Wang et al., 2020). For cardiac MRI, classical pipelines integrate shape priors with CNN outputs (Petitjean and Dacher, 2011). A comparative study in (Duarte et al, 2017) evaluated Dice sensitivity to class imbalance across datasets, motivating our reweighting scheme.\n",
    "reason": "Improper abbreviation formatting in a parenthetical citation: missing period after 'al.' and missing comma before the year; should be '(Duarte et al., 2017)'.",
    "start": 865,
    "end": 885,
    "label": "Format"
  },
  {
    "span": "(Smith et al., 2020",
    "document": "Related Work\n\nGraph contrastive learning has gained traction for unsupervised representation learning on graphs, with methods exploring data augmentations (You et al., 2020; Qiu et al., 2020) and topology-aware objectives (Hassani and Khasahmadi, 2020). Early pretraining ideas for graphs include random walk–based embeddings (Perozzi et al., 2014; Grover and Leskovec, 2016). For graphs specifically, (Smith et al., 2020 introduced a noise-contrastive objective tailored to structural roles, while subsequent work refined negative sampling strategies (Chen et al., 2020) and alignment–uniformity trade-offs (Wang and Isola, 2020). More recently, augmentation-free approaches have also been explored (Thakoor et al., 2021).\n",
    "reason": "Missing closing parenthesis in the parenthetical citation; it should be '(Smith et al., 2020)'.",
    "start": 402,
    "end": 421,
    "label": "Format"
  },
  {
    "span": "Neural code retrieval retrieves semantically similar snippets given a query description (Gu et al., 2018). Static analyzers detect potential bugs via abstract interpretation (Cousot and Cousot, 1977). Type inference leverages probabilistic models to recover missing annotations (Hellendoorn et al., 2018). Execution-based feedback evaluates partial programs against unit tests (Zaremba and Sutskever, 2014).",
    "document": "Related Work\n\nProgram synthesis and code understanding\nLarge language models trained on code have advanced tasks such as completion and question answering by learning to synthesize and explain snippets conditioned on natural language (Chen et al., 2021). Retrieval and grounding mechanisms can complement generation by surfacing relevant context from code corpora.\n\nNeural code retrieval retrieves semantically similar snippets given a query description (Gu et al., 2018). Static analyzers detect potential bugs via abstract interpretation (Cousot and Cousot, 1977). Type inference leverages probabilistic models to recover missing annotations (Hellendoorn et al., 2018). Execution-based feedback evaluates partial programs against unit tests (Zaremba and Sutskever, 2014).\n\nOur method integrates retrieval-augmented generation with lightweight static signals to reduce hallucinated APIs while preserving fluency.",
    "reason": "The span lists four areas (retrieval, static analysis, type inference, execution-based feedback) in consecutive sentences without linking statements or transitions, making it unclear how each cited work relates to the others or to the surrounding discussion.",
    "start": 366,
    "end": 773,
    "label": "Coherence"
  },
  {
    "span": "A line of work applies differential privacy mechanisms to protect client updates in federated learning (Abadi et al., 2016; McMahan et al., 2018; Geyer et al., 2017). Another thread focuses on secure aggregation to prevent the server from inspecting individual gradients (Bonawitz et al., 2017; Truex et al., 2019; Bell et al., 2020).",
    "document": "Related Work\n\nFederated learning enables collaborative training without centralizing raw data, but it introduces risks related to gradient leakage and participation inference (Kairouz et al., 2021). As deployment moves from research prototypes to production, the literature has proliferated across privacy guarantees, communication efficiency, and robustness to adversarial behavior.\n\nA line of work applies differential privacy mechanisms to protect client updates in federated learning (Abadi et al., 2016; McMahan et al., 2018; Geyer et al., 2017). Another thread focuses on secure aggregation to prevent the server from inspecting individual gradients (Bonawitz et al., 2017; Truex et al., 2019; Bell et al., 2020). Beyond privacy, compression and sparsification approaches reduce bandwidth consumption (Konečný et al., 2016; Alistarh et al., 2017), and personalized federated learning tailors models to heterogeneous clients (Smith et al., 2017; Arivazhagan et al., 2019).\n\nIn this work, we present a method that integrates privacy and personalization with minimal overhead. We evaluate across vision and language workloads with non-iid data splits.",
    "reason": "The span lists prior differential privacy and secure aggregation works without explaining how they relate to the present study or what specific gap remains, satisfying criterion (a).",
    "start": 385,
    "end": 719,
    "label": "Lacks synthesis"
  },
  {
    "span": "It is well known in program repair that test adequacy metrics severely underestimate patch quality.",
    "document": "Introduction\n\nAutomated program repair (APR) seeks to generate patches that satisfy a given test suite while preserving intended behavior. Traditional pipelines optimize for passing tests, which can lead to overfitting when the suite has low coverage or coarse oracles.\n\nIt is well known in program repair that test adequacy metrics severely underestimate patch quality. This mismatch has motivated semantic equivalence checks, behavioral diversity metrics, and human-in-the-loop validation to better reflect developer expectations.\n\nWe present a patch validation framework that couples specification mining with counterexample-guided refinement. On multiple Java benchmarks, our method identifies overfitting cases missed by baseline adequacy measures and recovers plausible patches with fewer regressions.\n\nWe release tooling to facilitate integration with continuous integration pipelines.",
    "reason": "The sentence invokes a field-wide 'well known' claim about test adequacy without citing foundational studies or surveys that establish this point (rule b).",
    "start": 271,
    "end": 370,
    "label": "Unsupported claim"
  },
  {
    "span": "Safety standards and certification frameworks for collaborative robots address risk assessment and speed-and-separation monitoring (ISO 10218-2, 2011; ISO/TS 15066, 2016; Haddadin et al., 2017). Studies on predictive collision avoidance leverage human motion forecasting and control barriers (Mainprice et al., 2015; Bansal et al., 2021; Singh et al., 2022).",
    "document": "Introduction\n\nHuman-robot collaboration promises productivity gains in flexible manufacturing and logistics. However, ensuring safety without overly conservative behavior remains a key barrier to adoption.\n\nSafety standards and certification frameworks for collaborative robots address risk assessment and speed-and-separation monitoring (ISO 10218-2, 2011; ISO/TS 15066, 2016; Haddadin et al., 2017). Studies on predictive collision avoidance leverage human motion forecasting and control barriers (Mainprice et al., 2015; Bansal et al., 2021; Singh et al., 2022). Learning-based policies have been explored to negotiate shared workspaces (Rana et al., 2020; Dorsa et al., 2018).\n\nWe develop a runtime safety filter that composes with arbitrary planners and demonstrate improved task efficiency in human-in-the-loop trials.",
    "reason": "The span lists standards and techniques without explaining how they relate to or motivate the proposed runtime safety filter, fulfilling criterion (a) and lacking articulated motivation per (c).",
    "start": 207,
    "end": 565,
    "label": "Lacks synthesis"
  },
  {
    "span": "(Lee et. al., 2022)",
    "document": "Introduction\n\nSelf-supervised learning has reduced reliance on labels for medical imaging, achieving strong performance on segmentation and classification (Zhou et al., 2021; Azizi et al., 2021). Contrastive pretraining benefits from domain-specific augmentations that preserve pathology (Chaitanya et al., 2020). Multi-instance learning addresses patient-level labels with patch aggregations (Ilse et al., 2018). Recent studies evaluate robustness to scanner shifts and acquisition protocols (Cohen et al., 2021). We build upon masked image modeling with cross-scale reconstruction to capture fine-grained texture variations (Bao et al., 2022). Prior work reports consistent gains from curriculum schedules (Lee et. al., 2022), but existing curricula often conflict with clinical priors; we resolve this by conditioning difficulty on radiologist-derived attributes.",
    "reason": "Incorrect abbreviation 'et. al.'; correct form is 'et al.' in the citation.",
    "start": 708,
    "end": 727,
    "label": "Format"
  },
  {
    "span": "Session-based recommenders leverage short interaction sequences for next-item prediction (Hidasi et al., 2016). Graph neural networks model item transitions as edges (Wu et al., 2019). Differential privacy adds noise to protect user data (Abadi et al., 2016). Sequential transformers capture long-range dependencies (Kang and McAuley, 2018).",
    "document": "Related Work\n\nRecommendation research spans sequence modeling, graph structure exploitation, and privacy preservation. In session-based scenarios, models must infer intent from short-lived interactions without persistent user identifiers.\n\nSession-based recommenders leverage short interaction sequences for next-item prediction (Hidasi et al., 2016). Graph neural networks model item transitions as edges (Wu et al., 2019). Differential privacy adds noise to protect user data (Abadi et al., 2016). Sequential transformers capture long-range dependencies (Kang and McAuley, 2018).\n\nWe focus on privacy-aware session recommendation by integrating graph-enhanced attention with calibrated noise injection, balancing accuracy and privacy budgets under realistic traffic patterns.",
    "reason": "The span mixes sequence models, graph models, and privacy concepts without transitions or explanations that tie them together, resulting in abrupt topic changes and weak coherence between sentences.",
    "start": 240,
    "end": 581,
    "label": "Coherence"
  },
  {
    "span": "Federated learning aims to train models without centralizing data, with advances in communication efficiency, privacy, and personalization (McMahan et al., 2017; Kairouz et al., 2021). Differential privacy mechanisms and secure aggregation protect user information (Geyer et al., 2017; Bonawitz et al., 2017). Personalization methods adapt global models to client heterogeneity via meta-learning, multi-task objectives, or mixture models (Fallah et al., 2020; Arivazhagan et al., 2019). In this paper, we propose a new personalized aggregation strategy.",
    "document": "Related Work\n\nDistributed training settings have motivated methods that preserve data locality for compliance and bandwidth reasons (Kairouz et al., 2021). Federated optimization algorithms address client drift and partial participation via adaptive server updates and control variates (Reddi et al., 2021; Karimireddy et al., 2020).\n\nFederated learning aims to train models without centralizing data, with advances in communication efficiency, privacy, and personalization (McMahan et al., 2017; Kairouz et al., 2021). Differential privacy mechanisms and secure aggregation protect user information (Geyer et al., 2017; Bonawitz et al., 2017). Personalization methods adapt global models to client heterogeneity via meta-learning, multi-task objectives, or mixture models (Fallah et al., 2020; Arivazhagan et al., 2019). In this paper, we propose a new personalized aggregation strategy.\n\nEvaluation protocols increasingly consider fairness across users and robustness to non-iid distributions, with benchmarks spanning vision, language, and mobile keyboard tasks (Caldas et al., 2018; He et al., 2020).",
    "reason": "The span ends with the authors' contribution immediately after a literature summary without identifying a concrete gap or connecting prior work to the proposed approach, matching definition b.",
    "start": 335,
    "end": 888,
    "label": "Lacks synthesis"
  },
  {
    "span": "Differential privacy mechanisms in federated learning commonly apply noise at the client update level with clipping to bound sensitivity (Abadi et al., 2016; McMahan et al., 2018). Secure aggregation protects individual updates from the server during summation (Bonawitz et al., 2017). Amplification by subsampling improves privacy-utility trade-offs for large client populations (Balle et al., 2018; Mironov, 2017). Personalized federated strategies decouple global and local parameters to mitigate heterogeneity (Arivazhagan et al., 2019; Dinh et al., 2020).",
    "document": "Introduction\nFederated learning enables on-device training without centralizing raw data, but privacy risks persist due to potential gradient leakage and membership inference. Differential privacy and cryptographic protocols are widely adopted to mitigate these threats while preserving utility in heterogeneous client populations.\n\nDifferential privacy mechanisms in federated learning commonly apply noise at the client update level with clipping to bound sensitivity (Abadi et al., 2016; McMahan et al., 2018). Secure aggregation protects individual updates from the server during summation (Bonawitz et al., 2017). Amplification by subsampling improves privacy-utility trade-offs for large client populations (Balle et al., 2018; Mironov, 2017). Personalized federated strategies decouple global and local parameters to mitigate heterogeneity (Arivazhagan et al., 2019; Dinh et al., 2020).\n\nWe study the underexplored interaction between privacy noise and client drift under non-IID data. Our contribution is a drift-aware noise allocation scheme that adapts clipping and noise to local divergence estimates, improving accuracy at fixed privacy budgets on vision and language benchmarks.",
    "reason": "This paragraph describes prior techniques and cites them but does not connect them to a specific research gap or to the proposed method; it offers no author viewpoint.",
    "start": 333,
    "end": 893,
    "label": "Lacks synthesis"
  },
  {
    "span": "Most prior work evaluates on MovieLens-20M with implicit feedback only.",
    "document": "Introduction\n\nRecommender systems commonly learn from implicit feedback (clicks, views) using matrix factorization, factorization machines, and deep sequential models (Rendle, 2010; He et al., 2017; Hidasi et al., 2016). Recent advances incorporate self-attention to capture long-term dependencies and item–item relations (Kang and McAuley, 2018; Sun et al., 2019), and graph-based recommenders model user–item bipartite structure (Wang et al., 2019).\n\nMost prior work evaluates on MovieLens-20M with implicit feedback only. This concentration risks overfitting design choices to a single domain and interaction modality. We expand evaluation to multi-domain datasets with mixed explicit–implicit signals and introduce a calibration objective that aligns predicted scores with elicited preferences.\n\nOur contributions include a unified training framework for heterogeneous feedback and a comprehensive benchmark with standardized preprocessing and negative sampling protocols.",
    "reason": "The sentence generalizes about the majority of prior evaluations without citing any supporting surveys or studies (violates rule b).",
    "start": 453,
    "end": 524,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent benchmarks show that long-context transformers consistently outperform RNNs on question answering tasks.",
    "document": "Related Work\n\nLong-context modeling has become central to open-domain QA, multi-hop reasoning, and document understanding (Roberts et al., 2020; Izacard and Grave, 2021). Efficient attention mechanisms and retrieval-augmented generation reduce computational costs while preserving context utilization (Kitaev et al., 2020; Guo et al., 2022).\n\nRecent benchmarks show that long-context transformers consistently outperform RNNs on question answering tasks. Complementary lines of research explore sparse routing and hierarchical memory to scale beyond hundreds of thousands of tokens (Rae et al., 2021; Dehghani et al., 2023). Our work contributes a segment-aware objective that encourages stable retrieval across context windows.",
    "reason": "The span asserts results from 'recent benchmarks' without citing any specific studies, violating rule (d).",
    "start": 343,
    "end": 454,
    "label": "Unsupported claim"
  },
  {
    "span": "We follow the evaluation setup of the SemEval 2021 Task on Toxic Spans.",
    "document": "Related Work\n\nToxic language detection has evolved from document-level classifications to more fine-grained, token-level analyses that identify the precise spans contributing to toxicity. Span-level supervision enables better interpretability and offers actionable guidance for content moderation workflows.\n\nWe follow the evaluation setup of the SemEval 2021 Task on Toxic Spans. Specifically, we adopt the standard precision/recall metrics over character-level spans and compare systems on a shared subset of social media posts annotated for toxic fragments.\n\nRecent research emphasizes the importance of contextual cues and pragmatic inference in toxicity attribution. Nevertheless, evaluations still vary in span granularity and annotation guidelines, complicating cross-paper comparisons.",
    "reason": "References a shared task/competition without providing a citation at first mention (violates rule a).",
    "start": 309,
    "end": 380,
    "label": "Unsupported claim"
  },
  {
    "span": "Klein and al., 2015",
    "document": "Related Work\n\nText summarization techniques range from extractive pipelines to fully abstractive models (Nallapati et al., 2016; Rush et al., 2015). For abstractive modeling, Klein and al., 2015 demonstrated the benefits of pointer mechanisms for handling rare words and copying salient phrases. Subsequent architectures refine attention and coverage to reduce repetition (See et al., 2017; Paulus et al., 2018).\n\nPretrained encoder-decoder models have since become standard, with fine-tuning strategies that exploit document structure and discourse cues (Liu and Lapata, 2019; Lewis et al., 2020). Evaluation has evolved to include factuality and faithfulness metrics beyond ROUGE (Falke et al., 2019; Kryscinski et al., 2020).",
    "reason": "Incorrect abbreviation in a narrative citation; should be 'Klein et al. (2015)' or parenthetical '(Klein et al., 2015)'.",
    "start": 175,
    "end": 194,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors found that annotators prefer abstractive summaries for news in 78% of cases.",
    "document": "Related Work\n\nHuman preferences in summarization evaluation have been studied to understand what makes a summary useful beyond automatic metric scores (Novikova et al., 2017; Fabbri et al., 2021). While automatic metrics like ROUGE correlate with some aspects of quality, they often fail to capture coherence and faithfulness (Deutsch et al., 2021; Maynez et al., 2020). In a previous study, the authors found that annotators prefer abstractive summaries for news in 78% of cases. These findings motivate the inclusion of judgment protocols that measure readability, informativeness, and factual consistency.\n\nRecent evaluation frameworks incorporate pairwise preference modeling and calibration methods to reduce inter-annotator variance (Amplayo et al., 2022; Koto et al., 2022). We build on this line of work by introducing a multi-criteria rubric and reporting both aggregate and per-criterion preferences.",
    "reason": "This sentence cites a prior study and a specific statistic (78%) but does not provide a citation to support the claim.",
    "start": 371,
    "end": 480,
    "label": "Unsupported claim"
  },
  {
    "span": "In (Doe et al., 2020)",
    "document": "Related Work\n\nFederated learning (FL) enables decentralized optimization under privacy and communication constraints (Kim and Vora, 2019; Santos et al., 2021). Personalization strategies adapt global models to heterogeneous client distributions via meta-learning and mixture-based aggregation (Lu and Chen, 2020).\n\nIn (Doe et al., 2020) the authors propose a proximal objective to stabilize local updates, while later work extends this idea with adaptive regularization (Perez and Wang, 2021). Compression schemes reduce uplink costs by quantizing gradients or sparsifying updates (Ibrahim et al., 2019), and secure aggregation further protects client confidentiality (Zhou and Malik, 2020). We incorporate a variance-corrected optimizer to reduce client drift and complement it with an adaptive client selection strategy.\n\nOur approach contrasts with clustering-based personalization (Feng et al., 2022) and focuses on stateless server-side adaptation to maintain scalability.",
    "reason": "Wrong citation style: 'In (Doe et al., 2020)' should be 'In Doe et al. (2020)' for a narrative citation; the preposition should not enclose the citation in parentheses.",
    "start": 315,
    "end": 336,
    "label": "Format"
  },
  {
    "span": "as per (Ono et al., 2016)",
    "document": "Introduction\n\nWe study robust calibration in probabilistic classifiers. Prior work examined temperature scaling and label smoothing to mitigate overconfidence (Guo et al., 2017; Müller et al., 2019). Consistent with domain shift theory, as per (Ono et al., 2016) we hypothesize that calibration errors compound under covariate shift, motivating distribution-aware post-hoc adjustments (Kumar et al., 2019).",
    "reason": "Wrong citation style: a prepositional phrase precedes a parenthetical citation. It should be as per Ono et al. (2016) in narrative form or the preposition removed with a standalone parenthetical citation.",
    "start": 237,
    "end": 262,
    "label": "Format"
  },
  {
    "span": "((Rahman, 2021))",
    "document": "Related Work\n\nFairness in machine learning covers criteria such as demographic parity, equalized odds, and predictive parity, each suited to different societal objectives (Barocas et al., 2019; Hardt et al., 2016). Post-processing methods adjust decision thresholds to satisfy constraints without retraining (Pleiss et al., 2017), while in-processing approaches incorporate fairness regularizers into the learning objective (Zafar et al., 2017; Agarwal et al., 2018).\n\nCausal perspectives emphasize counterfactual definitions of fairness and path-specific interventions (Kusner et al., 2017; Kilbertus et al., 2017). Recent audits consider subgroup robustness under distributional shift and label noise (Sagawa et al., 2020; Wang et al., 2022). For text classification, debiasing via counterfactual data augmentation reduces spurious correlations (Kaushik et al., 2020). Methods for fair representation learning were surveyed in ((Rahman, 2021)), with emphasis on interpretability and trade-offs in practice.\n",
    "reason": "Double parentheses around the citation; there should be only a single pair: '(Rahman, 2021)'.",
    "start": 929,
    "end": 945,
    "label": "Format"
  },
  {
    "span": "The DSTC9 challenge standardized response selection metrics for open-domain chat.",
    "document": "Related Work\n\nOpen-domain dialogue evaluation has long relied on a mixture of automatic metrics and human judgments, with ongoing debate about their reliability and correlation. Shared tasks and benchmarks play a crucial role in benchmarking progress and aligning evaluation protocols across research groups.\n\nThe DSTC9 challenge standardized response selection metrics for open-domain chat. Nevertheless, many labs also adopt custom negative sampling schemes, making results difficult to compare across papers. Parallel efforts in live user evaluation have introduced interactive protocols, but reproducibility remains a challenge when data and prompts are not fully released.\n\nOur study focuses on unifying offline evaluation by releasing a transparent, reproducible pipeline that includes negative sampling, candidate generation, and human validation procedures.",
    "reason": "Mentions a specific shared task (DSTC9) and makes a claim about its standardization role without citing the task description or supporting references.",
    "start": 310,
    "end": 391,
    "label": "Unsupported claim"
  },
  {
    "span": "Fine-tuning on in-domain parallel data reduces mismatch (Chu et al., 2017). Back-translation synthesizes source-side data from monolingual targets (Sennrich et al., 2016). Curriculum schedules order domains by difficulty (Zhang et al., 2019). Multilingual training shares representations across languages (Aharoni et al., 2019).",
    "document": "Related Work\n\nDomain adaptation in machine translation (MT) addresses distributional shifts between training and deployment data. Classical techniques include instance weighting, data selection, and model interpolation. Modern neural MT leverages synthetic data and parameter-efficient adaptation to improve in-domain performance.\n\nFine-tuning on in-domain parallel data reduces mismatch (Chu et al., 2017). Back-translation synthesizes source-side data from monolingual targets (Sennrich et al., 2016). Curriculum schedules order domains by difficulty (Zhang et al., 2019). Multilingual training shares representations across languages (Aharoni et al., 2019).\n\nOur method combines adaptive data selection with parameter-efficient adapters for rapid specialization. Unlike prior work that treats domains independently, we exploit inter-domain similarity via contrastive regularization to prevent catastrophic forgetting.",
    "reason": "The span enumerates four distinct approaches without articulating how they relate or build on each other. There are no transitions or explicit connections between sentences, so the relevance of each cited work to the previous one remains unclear.",
    "start": 332,
    "end": 660,
    "label": "Coherence"
  },
  {
    "span": "Neighbor sampling alleviates memory constraints by subgraph extraction (Hamilton et al., 2017). Explainers for GNNs attribute decisions to substructures (Ying et al., 2019). Scaling to web-scale graphs uses partitioning (Chiang et al., 2019). Heterogeneous graphs require relation-aware encoders (Hu et al., 2020).",
    "document": "Related Work\n\nGraph neural networks (GNNs) enable learning over relational structures and have been applied to recommendation, chemistry, and knowledge graphs (Kipf and Welling, 2017; Hamilton et al., 2017; Veličković et al., 2018). Handling massive graphs and heterogeneous relations presents unique challenges for scalability and interpretability.\n\nNeighbor sampling alleviates memory constraints by subgraph extraction (Hamilton et al., 2017). Explainers for GNNs attribute decisions to substructures (Ying et al., 2019). Scaling to web-scale graphs uses partitioning (Chiang et al., 2019). Heterogeneous graphs require relation-aware encoders (Hu et al., 2020).\n\nWe build upon these observations to study efficiency–accuracy trade-offs under dynamic graph updates.",
    "reason": "Multiple sentences introduce disparate subtopics (sampling, explainability, partitioning, heterogeneity) without transitions or stating their relationships, resulting in an abrupt, unconnected sequence of citations.",
    "start": 351,
    "end": 665,
    "label": "Coherence"
  },
  {
    "span": "(Perez et al., 2020",
    "document": "Related Work\n\nSelf-supervised objectives have been widely adopted in sequence modeling. Prior work (Perez et al., 2020 explores masked span prediction to regularize long-range dependencies, while others integrate permutation language modeling to capture bidirectional context. These strategies reduce overfitting but may underutilize entity-level signals in low-resource settings.\n\nTo mitigate this, hybrid objectives incorporate entity masking and span corruption, improving robustness across domains such as biomedical and legal text.",
    "reason": "Missing closing parenthesis in the parenthetical citation.",
    "start": 99,
    "end": 118,
    "label": "Format"
  },
  {
    "span": "BERT has been used in AES tasks trained on essays with holistic and analytic scores.",
    "document": "Related Work\n\nAutomated Essay Scoring (AES) aims to predict human-assigned scores from written responses. Traditional approaches relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse markers (Attali and Burstein, 2006; Shermis, 2014). With deep learning, sequence models and pre-trained encoders have become common for text scoring (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong et al., 2017). Pre-trained language models like BERT and RoBERTa offer strong contextual representations that can be fine-tuned for downstream tasks (Devlin et al., 2019; Liu et al., 2019).\n\nBERT has been used in AES tasks trained on essays with holistic and analytic scores. However, concerns remain about domain shift across prompts and the interpretability of predictions for formative feedback. Recent work has explored prompt-aware adapters and multi-task setups to address cross-prompt generalization (Zhang and Litman, 2018; Ke and Ng, 2019). Our approach introduces a calibration module to improve score reliability while maintaining cross-prompt robustness.",
    "reason": "Claims a specific setup of BERT applied to AES (a niche, prior-work claim) without any supporting citation (rule e.iii and rule a).",
    "start": 619,
    "end": 703,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent competitions on HumanEval+ highlight reasoning failures in long-chain program synthesis.",
    "document": "Introduction\n\nCode generation models have achieved impressive results on short, templated problems, yet they often struggle with multi-hop reasoning, tool use, and state management required for real-world tasks. Benchmarks increasingly emphasize compositionality and correctness beyond simple unit tests.\n\nRecent competitions on HumanEval+ highlight reasoning failures in long-chain program synthesis. Participants report that models pass local checks but fail on hidden invariants and interaction protocols that span multiple functions.\n\nApproaches to address these gaps include iterative refinement with execution feedback, retrieval-augmented synthesis from API documentation, and verifier-guided decoding. Our work contributes a planning-aware decoder that conditions on hierarchical sketches and integrates counterexample-guided repair.",
    "reason": "References specific competitions and a named benchmark without providing citations; mentions of datasets/competitions require citation at first mention.",
    "start": 306,
    "end": 401,
    "label": "Unsupported claim"
  },
  {
    "span": "Thomason et al. (2015) studied instruction following with grounded language and vision. Padmakumar et al. (2021) modeled clarifying question generation in task-oriented interaction. Tellex et al. (2020) surveyed natural language for human-robot collaboration.",
    "document": "Related Work\n\nConversational human-robot interaction (HRI) aims to combine dialog capabilities with embodied action to support collaborative tasks. Research spans grounding, repair, learning from feedback, and task allocation.\n\nThomason et al. (2015) studied instruction following with grounded language and vision. Padmakumar et al. (2021) modeled clarifying question generation in task-oriented interaction. Tellex et al. (2020) surveyed natural language for human-robot collaboration.\n\nWe propose a dataset of multimodal repair utterances for tabletop assembly.",
    "reason": "The span contains multiple sentences that cite different strands (instruction following, clarification, surveys) without transitions or explicit relational framing, leaving the connections among them unclear and abrupt.",
    "start": 228,
    "end": 487,
    "label": "Coherence"
  },
  {
    "span": "Automated writing evaluation has evolved from feature-based scoring (Attali and Burstein, 2006) to neural sequence models (Taghipour and Ng, 2016) and pre-trained transformers (Ushio et al., 2022).",
    "document": "Introduction\n\nProviding timely, high-quality feedback is essential for scalable writing instruction. Automated writing evaluation (AWE) systems aim to assess student texts and suggest improvements, but must balance reliability, specificity, and fairness.\n\nAutomated writing evaluation has evolved from feature-based scoring (Attali and Burstein, 2006) to neural sequence models (Taghipour and Ng, 2016) and pre-trained transformers (Ushio et al., 2022). Recent work explores rubric-aligned feedback generation and calibration for fairness across demographics (Jin et al., 2022; Huang et al., 2023).\n\nWe propose a feedback model that grounds suggestions in evidence retrieved from curated writing resources and instructional rubrics.",
    "reason": "The span recites a historical progression without explaining how these stages relate to the authors' approach or highlighting an explicit gap, satisfying (a) and (b).",
    "start": 256,
    "end": 453,
    "label": "Lacks synthesis"
  },
  {
    "span": "Transformers have become the de facto standard for semantic segmentation since 2021.",
    "document": "Related Work\n\nSemantic segmentation has traditionally been dominated by convolutional architectures, with encoder–decoder designs and dilated convolutions serving as the backbone of most high-performing models. The emergence of attention mechanisms prompted a shift toward architectures that model long-range dependencies more explicitly.\n\nTransformers have become the de facto standard for semantic segmentation since 2021. This transition is often attributed to their capacity to capture global context and to scale effectively with data and compute. Hybrid designs that fuse convolutional stems with transformer decoders are widely reported to offer strong performance–efficiency trade-offs.\n\nDespite these advances, most segmentation pipelines still rely on heavy pretraining on large-scale image corpora, which limits applicability in low-resource or specialized domains. Our work explores parameter-efficient alternatives that retain strong performance without such pretraining.",
    "reason": "The claim that transformers are the de facto standard for segmentation and the timing ('since 2021') references specific prior work and field consensus without any supporting citations.",
    "start": 340,
    "end": 424,
    "label": "Unsupported claim"
  },
  {
    "span": "Kim et al. 2017)",
    "document": "Related Work\n\nRepresentation learning for graphs has benefitted from message passing neural networks and attention-based aggregation (Hamilton et al., 2017; Velickovic et al., 2018). As shown by Kim et al. 2017) and later extended by Zhao and Wu (2019), residual connections alleviate oversmoothing in deeper architectures.\n\nBeyond transductive settings, inductive generalization has been addressed with meta-learning over node neighborhoods (Huang and Zitnik, 2020) and self-supervised objectives that promote structure-aware invariances (You et al., 2020; Thakoor et al., 2021). Our work complements these efforts by focusing on calibration under distribution shift, drawing insights from uncertainty estimation in vision (Guo et al., 2017) and NLP (Desai and Durrett, 2020).\n\nWe provide a systematic comparison of post-hoc and training-time calibration for GNNs, revealing trade-offs between sharpness and reliability across datasets.",
    "reason": "Missing opening parenthesis before the year in a narrative citation; should be 'Kim et al. (2017)'.",
    "start": 195,
    "end": 211,
    "label": "Format"
  },
  {
    "span": "Neural program synthesis approaches leverage DSLs, enumerative search, and reinforcement learning to generate programs from specs (Devlin et al., 2017; Ellis et al., 2019; Chen et al., 2021). Our method fits into this landscape.",
    "document": "Related Work\n\nProgram synthesis from incomplete specifications is a long-standing challenge. Recent neural methods combine symbolic structure with learned heuristics to navigate vast search spaces. Neural program synthesis approaches leverage DSLs, enumerative search, and reinforcement learning to generate programs from specs (Devlin et al., 2017; Ellis et al., 2019; Chen et al., 2021). Our method fits into this landscape. Other directions incorporate constraint solving and differentiable interpreters for end-to-end training (Parisotto et al., 2017; Nye et al., 2020).\n\nWe evaluate on string transformation and small algorithmic tasks.",
    "reason": "The span summarizes prior work and then vaguely situates the authors' method without specifying the gap or the distinct contribution, thus lacking synthesis (definition b) and not articulating perspective (definition c).",
    "start": 198,
    "end": 426,
    "label": "Lacks synthesis"
  },
  {
    "span": "Matrix factorization founded many collaborative filtering methods (Koren et al., 2009). Graph convolutional networks propagate interaction signals over user–item graphs (Wang et al., 2019). Session-based models capture short-term intent (Hidasi and Katrin, 2016). Negative sampling strategies improve training stability (Rendle and Freudenthaler, 2012). Social trust information can be leveraged for cold-start users (Jamali and Ester, 2010).",
    "document": "Related Work\n\nRecommendation has evolved from latent factorization toward graph-centric models that exploit relational structure. Beyond accuracy, recent attention targets robustness, efficiency, and cold-start performance. We review representative lines of work.\n\nMatrix factorization founded many collaborative filtering methods (Koren et al., 2009). Graph convolutional networks propagate interaction signals over user–item graphs (Wang et al., 2019). Session-based models capture short-term intent (Hidasi and Katrin, 2016). Negative sampling strategies improve training stability (Rendle and Freudenthaler, 2012). Social trust information can be leveraged for cold-start users (Jamali and Ester, 2010).\n\nAlthough these approaches address complementary aspects, prior studies rarely unify graph propagation with session dynamics and trust signals under a single training objective. Our approach bridges these aspects with a modular, end-to-end framework.",
    "reason": "Multiple sentences enumerate disparate topics (matrix factorization, GCNs, session models, negative sampling, social trust) without transitions or an explicit narrative linking how each relates to the others, creating coherence issues.",
    "start": 265,
    "end": 707,
    "label": "Coherence"
  },
  {
    "span": "In (Garcia et al., 2020)",
    "document": "Related Work\n\nTransfer learning with large-scale pretraining has reshaped domain adaptation in NLP (Howard and Ruder, 2018; Devlin et al., 2019). Methods for aligning source and target distributions include adversarial learning (Ganin and Lempitsky, 2015) and representation matching (Tzeng et al., 2017). Continual adaptation explores streaming domains while mitigating catastrophic forgetting (Riemer et al., 2019; Ke et al., 2020).\n\nIn (Garcia et al., 2020) a controlled study compared multi-source adaptation strategies across sentiment and topic classification, reporting that instance reweighting complements representation alignment. We adopt their evaluation protocol but introduce a calibration-aware loss to improve cross-domain uncertainty estimation. Complementary to these efforts, domain-specific adapters reduce parameter footprint without sacrificing accuracy (Houlsby et al., 2019; Pfeiffer et al., 2020).\n",
    "reason": "Wrong citation style: a leading preposition with a parenthetical citation; it should be 'In Garcia et al. (2020)' (narrative form) rather than 'In (Garcia et al., 2020)'.",
    "start": 436,
    "end": 460,
    "label": "Format"
  },
  {
    "span": "Early molecular property predictors relied on message passing neural networks (Gilmer et al., 2017), graph convolutional networks (Kipf and Welling, 2017), and attention-based GNNs (Veličković et al., 2018) to aggregate local chemical contexts.",
    "document": "Introduction\n\nAccurately predicting molecular properties such as solubility, toxicity, and binding affinity is central to drug discovery and materials science. Computational models promise to reduce experimental cost by screening large libraries in silico, but capturing long-range interactions and subtle stereochemical effects remains challenging.\n\nEarly molecular property predictors relied on message passing neural networks (Gilmer et al., 2017), graph convolutional networks (Kipf and Welling, 2017), and attention-based GNNs (Veličković et al., 2018) to aggregate local chemical contexts. Subsequent advances incorporate 3D geometry (Schütt et al., 2017; Anderson et al., 2019), equivariance (Fuchs et al., 2020; Satorras et al., 2021), and learned molecular fingerprints (Duvenaud et al., 2015; Yang et al., 2019). Pretraining on large unlabeled corpora of SMILES or molecular graphs further improves downstream performance (Hu et al., 2020; Rong et al., 2020).\n\nIn this paper, we introduce a model that augments graph representations with learned long-range interaction tokens and evaluates performance on multiple molecular benchmarks.",
    "reason": "The span lists prior approaches and citations without explaining how they relate to the authors' problem setting or motivating the need for the proposed model, satisfying (a) and (c).",
    "start": 351,
    "end": 595,
    "label": "Lacks synthesis"
  },
  {
    "span": "Prior multimodal summarizers fuse visual and textual cues using hierarchical attention, late fusion, or memory networks (Li et al., 2017; Sanabria et al., 2018; Palaskar et al., 2019). Datasets such as How2, TVSum, and QFVS offer aligned video–text pairs for training and evaluation (Sanabria et al., 2018; Gygli et al., 2014; Rochan et al., 2018).",
    "document": "Introduction\n\nSummarizing multimedia content requires grounding textual abstractions in visual evidence. In instructional videos, long temporal horizons and weak synchronization between narration and frames pose additional challenges for reliable fusion.\n\nPrior multimodal summarizers fuse visual and textual cues using hierarchical attention, late fusion, or memory networks (Li et al., 2017; Sanabria et al., 2018; Palaskar et al., 2019). Datasets such as How2, TVSum, and QFVS offer aligned video–text pairs for training and evaluation (Sanabria et al., 2018; Gygli et al., 2014; Rochan et al., 2018).\n\nExisting approaches seldom address temporal asynchrony explicitly. We introduce alignment-aware contrastive pretraining that learns cross-modal anchors, enabling faithful summaries under variable lag between modalities.",
    "reason": "The span provides a catalogue of methods and datasets but does not explain their limitations relative to temporal asynchrony or how the present work differs, thus lacking synthesis per (a) and (b).",
    "start": 256,
    "end": 604,
    "label": "Lacks synthesis"
  },
  {
    "span": "there has been a surge of recent works on debiasing recommendation algorithms",
    "document": "Related Work\n\nBias in Recommender Systems Exposure and selection biases skew both training data and evaluation metrics in recommenders (Schnabel et al., 2016; Joachims et al., 2017). Off-policy correction with inverse propensity scoring and counterfactual estimators has been proposed to mitigate such biases (Swaminathan and Joachims, 2015; Wang et al., 2019). Recently, there has been a surge of recent works on debiasing recommendation algorithms, exploring interventions from data collection to objective design.\n\nPosition Bias and Feedback Loops Position bias inflates click-through at higher ranks, and feedback loops can entrench popularity, harming long-tail items (Ovaisi et al., 2020; Biega et al., 2018). Our method integrates calibrated propensities with exposure-regularized learning-to-rank to address both issues jointly.",
    "reason": "This mentions a surge of 'recent works' without citing any, which should be supported with references to comply with the requirement.",
    "start": 372,
    "end": 449,
    "label": "Unsupported claim"
  },
  {
    "span": "Several surveys have recently concluded that prompt tuning is more parameter-efficient than adapters across vision-language tasks",
    "document": "Introduction\n\nParameter-efficient adaptation has emerged as a practical alternative to full fine-tuning for large models. Techniques such as adapters, prefix tuning, and prompt tuning aim to minimize trainable parameters while preserving task performance. In vision-language modeling, these methods must bridge modality gaps while maintaining efficiency.\n\nAs model sizes grow, the cost of updating and storing per-task parameters becomes prohibitive, especially in edge or on-device deployments. Consequently, the community prioritizes methods that balance performance with minimal memory and compute overhead.\n\nSeveral surveys have recently concluded that prompt tuning is more parameter-efficient than adapters across vision-language tasks, reinforcing its appeal in resource-constrained settings. Yet, empirical results vary depending on backbone architecture, pretraining corpus, and the degree of cross-modal alignment.\n\nOur work introduces a sparsity-aware prompt composition mechanism that dynamically allocates prompts per modality and layer, reducing redundant capacity while preserving alignment.",
    "reason": "The sentence references 'several surveys' and 'recently concluded' without citing any of them, which violates the requirement to cite prior work at first mention.",
    "start": 612,
    "end": 741,
    "label": "Unsupported claim"
  },
  {
    "span": "Garcia et al. 1",
    "document": "Introduction\n\nInterpretability in neural ranking has gained attention as models become increasingly opaque. While attention-based explanations correlate with relevance under specific constraints (Jain and Wallace, 2019), counterfactual reasoning provides stronger evidence of causal signals as argued by Garcia et al. 1 and subsequent work (Ribeiro et al., 2020). Despite progress, most evaluations rely on narrow benchmarks that may not reflect realistic user needs (Buckley et al., 2007).",
    "reason": "Misuse of footnote/number with an author citation. It should include the year (e.g., Garcia et al. (2019)) or be formatted as a proper footnote marker with a corresponding footnote.",
    "start": 304,
    "end": 319,
    "label": "Format"
  },
  {
    "span": "Early fusion concatenates audio, visual, and text features (Poria et al., 2017). Hierarchical attention weights modality-specific cues (Zadeh et al., 2018). Transformer-based cross-modal alignment improves context modeling (Tsai et al., 2019). CMU-MOSI and CMU-MOSEI provide benchmark annotations (Zadeh et al., 2016; Zadeh et al., 2018b).",
    "document": "Related Work\n\nMultimodal sentiment analysis integrates linguistic, acoustic, and visual signals to understand affect in social video and conversational settings. Work in this area spans fusion strategies, alignment mechanisms, and dataset construction to evaluate fine-grained sentiment cues.\n\nEarly fusion concatenates audio, visual, and text features (Poria et al., 2017). Hierarchical attention weights modality-specific cues (Zadeh et al., 2018). Transformer-based cross-modal alignment improves context modeling (Tsai et al., 2019). CMU-MOSI and CMU-MOSEI provide benchmark annotations (Zadeh et al., 2016; Zadeh et al., 2018b).\n\nWe propose a lightweight cross-modal adapter that conditions text representations on prosodic segments, improving robustness on noisy audio while reducing computation.",
    "reason": "The span abruptly shifts from fusion methods to datasets without transitions or explaining their relation, making the connections among the cited works unclear.",
    "start": 294,
    "end": 633,
    "label": "Coherence"
  },
  {
    "span": "BERT has been used for automated essay scoring with domain-specific fine-tuning.",
    "document": "Related Work\n\nAutomated essay scoring (AES) seeks to predict human-assigned scores for student writing with high reliability and fairness. Early systems relied on handcrafted features capturing syntax, fluency, and discourse organization, whereas modern systems increasingly leverage pretrained language models for end-to-end learning. BERT has been used for automated essay scoring with domain-specific fine-tuning. Other approaches incorporate rubric-aware objectives or multi-task training to better capture coherence and argument quality.\n\nDespite progress, generalization across prompts and genres remains a core challenge. Our work studies cross-prompt adaptation with limited labeled samples and proposes a calibration-aware scoring head to mitigate domain shift.",
    "reason": "Unsupported claim because it mentions a specific prior setup (BERT applied to AES with fine-tuning) without a citation to the work that did it (definition b and iii).",
    "start": 336,
    "end": 416,
    "label": "Unsupported claim"
  },
  {
    "span": "Garcia et al.",
    "document": "Introduction\n\nDomain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled or sparsely labeled target domain (Ben-David et al., 2010; Ganin et al., 2016). Garcia et al. demonstrate that combining moment matching with self-training can substantially reduce the discrepancy between domains while maintaining label consistency. However, adversarial alignment alone may be insufficient for highly imbalanced targets (Zhao et al., 2019), motivating recent work on class-conditional alignment (Long et al., 2018) and optimal transport (Courty et al., 2017).",
    "reason": "Narrative citation is missing the publication year; it should appear as a narrative citation with year, e.g., 'Garcia et al. (2019)'.",
    "start": 184,
    "end": 197,
    "label": "Format"
  },
  {
    "span": "[Klein and Manning 2003]",
    "document": "Related Work\n\nSyntactic Parsing\n\nStatistical parsers leveraged probabilistic context-free grammars with lexicalization to improve accuracy (Collins, 1999; Charniak, 2000). Discriminative reranking further boosted performance by incorporating rich features (Collins and Koo, 2005). While the CKY algorithm remains foundational (Klein and Manning, 2003), neural parsers achieved state-of-the-art performance via bi-affine scoring and pretrained embeddings (Dozat and Manning, 2017; Kitaev and Klein, 2018). Recent studies revisit structured training objectives to reduce label bias in transition systems [Klein and Manning 2003] and explore semi-supervised learning with unlabeled corpora (Yarowsky, 1995; Zoph et al., 2016).",
    "reason": "Wrong bracket style and missing comma in a parenthetical citation; should be (Klein and Manning, 2003) rather than square brackets.",
    "start": 602,
    "end": 626,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on essays from Grade 6 to 12 with prompt-specific heads.",
    "document": "Introduction\n\nAutomated essay scoring (AES) has progressed from handcrafted features (Attali and Burstein, 2006) to neural architectures that capture discourse and prompt relevance (Taghipour and Ng, 2016; Dong and Zhang, 2016). Recent work explores pre-trained language models and domain adaptation across prompts (Mayfield and Black, 2020; Uto and Ueno, 2021). BERT was used in an AES task trained on essays from Grade 6 to 12 with prompt-specific heads. Concurrently, rubric-aware attention and trait-specific scoring have improved interpretability (Cummins and Reynaert, 2018; Mathias and Bhattacharyya, 2020).\n\nWe propose a multi-task calibration layer that aligns holistic and trait scores while preserving prompt sensitivity, enabling robust performance across unseen prompts.",
    "reason": "Mentions a specific model setup for an AES task without citing the corresponding work; this matches example (iii) and violates rule (a).",
    "start": 363,
    "end": 456,
    "label": "Unsupported claim"
  },
  {
    "span": "Recurrent architectures such as GRU-D and LSTM with imputation handle missingness in EHR sequences (Che et al., 2018; Lipton et al., 2016). Temporal convolutional networks achieve competitive long-horizon performance (Bai et al., 2018; Le Guen and Thome, 2019). Transformer-style models introduce self-attention over irregularly sampled events (Li et al., 2020; Zerveas et al., 2021).",
    "document": "Introduction\n\nForecasting clinical trajectories from electronic health records (EHR) requires coping with sparse, irregularly sampled, and multi-scale signals. Building accurate and reliable predictors under these conditions is essential for early warning and resource allocation.\n\nRecurrent architectures such as GRU-D and LSTM with imputation handle missingness in EHR sequences (Che et al., 2018; Lipton et al., 2016). Temporal convolutional networks achieve competitive long-horizon performance (Bai et al., 2018; Le Guen and Thome, 2019). Transformer-style models introduce self-attention over irregularly sampled events (Li et al., 2020; Zerveas et al., 2021).\n\nIn this work, we explore calibration-aware time-series forecasting with uncertainty quantification tailored to irregular sampling, providing risk-sensitive predictions for clinical decision support.",
    "reason": "The span lists categories of models and citations without stating how they inform the authors' approach or what limitation remains unaddressed; it lacks synthesis (criteria a and c).",
    "start": 282,
    "end": 666,
    "label": "Lacks synthesis"
  },
  {
    "span": "Paraphrastic augmentation generates summaries with varied surface forms to improve robustness (Iyyer et al., 2018; Krishna et al., 2020). Back-translation synthesizes source-target pairs by translating monolingual data (Sennrich et al., 2016; Edunov et al., 2018). Reinforcement learning optimizes non-differentiable metrics like ROUGE (Ranzato et al., 2016; Paulus et al., 2018).",
    "document": "Related Work\n\nNeural abstractive summarization has advanced with pre-trained encoder-decoder models, yet still suffers from exposure bias, factual inconsistency, and data scarcity in specialized domains. Data augmentation and training objectives play key roles in addressing these issues.\n\nParaphrastic augmentation generates summaries with varied surface forms to improve robustness (Iyyer et al., 2018; Krishna et al., 2020). Back-translation synthesizes source-target pairs by translating monolingual data (Sennrich et al., 2016; Edunov et al., 2018). Reinforcement learning optimizes non-differentiable metrics like ROUGE (Ranzato et al., 2016; Paulus et al., 2018).\n\nFaithfulness has been tackled by constrained decoding, factuality-aware losses, and knowledge-grounded summarization. Domain adaptation methods leverage unlabeled corpora with self-training and curriculum schedules. Evaluation continues to evolve with factual probes and human-centered criteria beyond n-gram overlap.\n\nOur approach unifies augmentation and training signal shaping by coupling semantic perturbations with reward modeling tuned to factuality and coverage. We demonstrate gains on news, scientific, and clinical summarization benchmarks.",
    "reason": "The span lists three techniques (paraphrasing, back-translation, and reinforcement learning) as separate sentences without transitions or explaining how they relate to each other in the context of summarization, resulting in abrupt, unconnected citations.",
    "start": 290,
    "end": 670,
    "label": "Coherence"
  },
  {
    "span": "RoBERTa has been applied to automated essay scoring with prompt-specific adapters",
    "document": "Introduction\n\nAutomated essay scoring (AES) systems aim to estimate human writing quality using features that capture coherence, syntax, and content relevance (Shermis and Burstein, 2013; Ke and Ng, 2019). The advent of pretrained language models has shifted the focus from handcrafted features to end-to-end fine-tuning.\n\nRoBERTa has been applied to automated essay scoring with prompt-specific adapters, enabling parameter-efficient specialization while sharing a common backbone. However, cross-prompt generalization remains a challenge due to topical drift and rubric variability.\n\nWe present a multi-task training scheme with rubric-aware conditioning and demonstrate improvements on cross-prompt transfer while maintaining within-prompt accuracy.",
    "reason": "This is a claim about a specific model setup in AES without any supporting citation to prior work, matching example iii and rule a for first mentions of methods applied to a task.",
    "start": 323,
    "end": 404,
    "label": "Unsupported claim"
  },
  {
    "span": "Patel et al. 3",
    "document": "Introduction\n\nInteractive retrieval systems increasingly fuse sparse and dense signals to improve ranking quality (Nogueira and Cho, 2019; Khattab and Zaharia, 2020). This observation was first noted by Patel et al. 3 in the context of hybrid indexes, where lexical match provides precision while dense vectors enhance recall. Follow-up studies proposed learned weighting between the two signals (Zamani et al., 2022) and dynamic routing based on query intent (Hofstätter et al., 2021).\n\nHowever, robust evaluation remains challenging due to annotation sparsity and domain drift (Vardasbi et al., 2022). We propose an interventional protocol that perturbs query facets to disentangle semantic matching from term matching, enabling clearer attribution of gains to each component.\n\nOur experiments demonstrate consistent improvements across web and biomedical corpora, with detailed ablations isolating the contributions of re-ranking and hybrid retrieval.",
    "reason": "Incorrect use of a footnote-like number after an author name; should include a year (e.g., 'Patel et al. (YEAR)') or be formatted as a proper footnote.",
    "start": 203,
    "end": 217,
    "label": "Format"
  },
  {
    "span": "Back-translation augments parallel data with synthetic targets (Sennrich et al., 2016). Multilingual training shares parameters across languages (Johnson et al., 2017). Unsupervised NMT relies on monolingual corpora and dual learning (Lample et al., 2018). Byte pair encoding segments rare words into subwords (Sennrich et al., 2016b). Curvature-aware learning rates accelerate convergence (Liu et al., 2020).",
    "document": "Related Work\n\nLow-resource neural machine translation (NMT) has benefited from data augmentation and parameter sharing across related languages. Recent advances also consider optimization and tokenization choices that interact with scarce supervision and domain shift.\n\nBack-translation augments parallel data with synthetic targets (Sennrich et al., 2016). Multilingual training shares parameters across languages (Johnson et al., 2017). Unsupervised NMT relies on monolingual corpora and dual learning (Lample et al., 2018). Byte pair encoding segments rare words into subwords (Sennrich et al., 2016b). Curvature-aware learning rates accelerate convergence (Liu et al., 2020).\n\nOur approach targets vocabulary transfer across related languages by coupling subword sharing with targeted back-translation driven by lexicon priors.",
    "reason": "The span lists heterogeneous topics (data augmentation, multilingual sharing, unsupervised learning, tokenization, optimization) without transitions or articulating relationships, reducing coherence.",
    "start": 270,
    "end": 679,
    "label": "Coherence"
  },
  {
    "span": "It has been conclusively shown that distant supervision harms clinical NER performance.",
    "document": "Related Work\n\nClinical named entity recognition (NER) supports downstream applications such as cohort selection and pharmacovigilance. Supervision scarcity and privacy constraints have spurred interest in weak and distant supervision, as well as transfer from general-domain corpora.\n\nIt has been conclusively shown that distant supervision harms clinical NER performance. Concurrent efforts explore data programming and noise-robust training to mitigate label errors, while prompt-based adaptation of foundation models has emerged as a complementary direction.\n",
    "reason": "Makes a definitive claim about prior empirical findings without citing any supporting studies.",
    "start": 285,
    "end": 372,
    "label": "Unsupported claim"
  },
  {
    "span": "Seasonal-trend decomposition provides additive and multiplicative components (Cleveland et al., 1990). External covariates are incorporated through exogenous regressors (Hyndman et al., 2008). Probabilistic forecasting estimates full predictive distributions (Salinas et al., 2019). Transformers capture long-range temporal dependencies (Zhou et al., 2021).",
    "document": "Introduction\n\nTime-series forecasting methods range from classical statistical models to deep neural architectures, with growing interest in handling long contexts and uncertainty.\n\nSeasonal-trend decomposition provides additive and multiplicative components (Cleveland et al., 1990). External covariates are incorporated through exogenous regressors (Hyndman et al., 2008). Probabilistic forecasting estimates full predictive distributions (Salinas et al., 2019). Transformers capture long-range temporal dependencies (Zhou et al., 2021).\n\nWe propose a hybrid approach that unifies decomposition with sequence modeling and probabilistic outputs.",
    "reason": "The sentences introduce disparate strands (decomposition, exogenous regressors, probabilistic modeling, Transformers) without transitions or explicit links, leaving their relationships and progression unclear.",
    "start": 182,
    "end": 539,
    "label": "Coherence"
  },
  {
    "span": "CLIP aligns image and text embeddings via contrastive learning on web-scale pairs (Radford et al., 2021). Visual question answering benchmarks evaluate vision-language reasoning (Antol et al., 2015). Pretraining on region-level captions improves grounding (Anderson et al., 2018). Prompt tuning adapts frozen encoders to downstream tasks (Lester et al., 2021).",
    "document": "Related Work\n\nVision-language models unify visual and textual representations, enabling flexible zero-shot and few-shot applications. Methods differ in their supervision signals, architectural coupling, and adaptation strategies for downstream tasks.\n\nCLIP aligns image and text embeddings via contrastive learning on web-scale pairs (Radford et al., 2021). Visual question answering benchmarks evaluate vision-language reasoning (Antol et al., 2015). Pretraining on region-level captions improves grounding (Anderson et al., 2018). Prompt tuning adapts frozen encoders to downstream tasks (Lester et al., 2021).\n\nWe investigate parameter-efficient adaptation for retrieval and classification by composing soft prompts with cross-modal adapters, aiming to preserve zero-shot capabilities while specializing to target domains.",
    "reason": "The span presents CLIP, VQA benchmarks, region-caption pretraining, and prompt tuning as disconnected statements with no transitions or explicit relationships, leading to abrupt shifts and unclear coherence.",
    "start": 252,
    "end": 612,
    "label": "Coherence"
  },
  {
    "span": "Over 70% of the world's languages lack transcribed audio resources.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) has achieved near-human performance in high-resource languages with abundant labeled data (Kim and Park, 2020). However, building ASR systems for low-resource languages remains difficult due to scarce annotations and domain mismatch (Singh et al., 2021).\n\nOver 70% of the world's languages lack transcribed audio resources.\n\nThis scarcity motivates self-supervised pretraining and cross-lingual transfer methods that leverage unlabeled audio and related languages (Li et al., 2022). In this work, we present a multilingual framework that adapts pretrained models using phoneme-level constraints and weak supervision from user-generated captions.",
    "reason": "Presents a quantitative statistic about global language resources without any supporting citation.",
    "start": 305,
    "end": 372,
    "label": "Unsupported claim"
  },
  {
    "span": "the COVID-QA shared task",
    "document": "Introduction\n\nQuestion answering in the biomedical domain must cope with rapidly evolving terminology and limited supervision (Tsatsaronis et al., 2015; Jin et al., 2019). During the pandemic, community efforts released datasets and challenges to accelerate research on factual QA over scientific articles (Wang et al., 2020). Following the COVID-QA shared task, we focus on retrieving answerable spans from CORD-19 abstracts and filtering claims with weak evidence. We analyze how domain-adaptive pretraining affects factuality and calibration.",
    "reason": "Mentions a specific shared task without citing its official overview or dataset paper, which is required at first mention.",
    "start": 337,
    "end": 361,
    "label": "Unsupported claim"
  },
  {
    "span": "We evaluate our method on the widely used CleanCode Corpus.",
    "document": "Related Work\n\nAutomatic code summarization connects program structure to natural language descriptions. Early sequence-to-sequence models relied on token streams, while later approaches incorporated abstract syntax trees and graph representations to capture long-range dependencies between identifiers. Benchmarking has typically centered on public corpora and open-source repositories with standardized splits to ensure fair comparison across models. We evaluate our method on the widely used CleanCode Corpus. In addition, we report results on two proprietary datasets from industrial partners to assess generalization beyond open-source code. Our analysis highlights the role of identifier normalization and comment style variability in driving performance differences between transformer encoders and hybrid graph-based decoders.",
    "reason": "First mention of a dataset (CleanCode Corpus) lacks a citation; datasets should be cited when first introduced.",
    "start": 452,
    "end": 511,
    "label": "Unsupported claim"
  },
  {
    "span": "The OLID dataset is widely considered the de facto benchmark for offensive language detection.",
    "document": "Related Work\n\nAutomatic detection of offensive and abusive language has been approached with classical machine learning and deep neural methods (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018). The OLID dataset is widely considered the de facto benchmark for offensive language detection. Beyond English, multilingual corpora have been introduced to capture regional varieties and code-switching (Founta et al., 2018; Pitenis et al., 2020). Models range from n-gram SVMs to pre-trained transformers fine-tuned on social media text.",
    "reason": "Claims community consensus about a specific dataset without citing the dataset paper or supporting sources (rule a/b).",
    "start": 198,
    "end": 292,
    "label": "Unsupported claim"
  },
  {
    "span": "Smith et al., 2018)",
    "document": "Related Work\n\nTime-series imputation methods range from simple interpolation to deep generative models (Yoon et al., 2018; Luo et al., 2018). Variational and diffusion-based approaches capture complex missingness patterns and uncertainty (Li et al., 2019; Tashiro et al., 2021). Several studies also explore state-space models with neural emission functions for long sequences, e.g., Smith et al., 2018) and Gordon et al. (2020). Despite progress, evaluation protocols often mix synthetic and real missingness in ways that obscure failure modes (Bertsimas et al., 2018).",
    "reason": "Missing opening parenthesis in a parenthetical citation: the citation appears as \"Smith et al., 2018)\" without an opening \"(\". It should be \"(Smith et al., 2018)\".",
    "start": 384,
    "end": 403,
    "label": "Format"
  },
  {
    "span": "several datasets exhibit severe exposure bias",
    "document": "Introduction\n\nFairness in recommender systems concerns disparities in exposure and utility across users and items. Ranking policies can amplify historical imbalances, leading to systematic under-exposure of certain providers and reduced diversity for users.\n\nAudits of public benchmarks suggest that several datasets exhibit severe exposure bias, where popularity and position effects dominate relevance signals. This complicates offline evaluation and may mislead model selection toward exploitative policies.\n\nWe propose a counterfactual learning-to-rank objective with item-level propensity correction and group-aware regularization, enabling fairness-aware optimization under logged bandit feedback.",
    "reason": "This sentence asserts prior empirical observations about multiple datasets without citing the studies or audit reports that established the exposure bias.",
    "start": 300,
    "end": 345,
    "label": "Unsupported claim"
  },
  {
    "span": "In (Johnson et al., 2017)",
    "document": "Related Work\n\nGraph representation learning methods map nodes to continuous vectors to preserve network structure and attributes (Perozzi et al., 2014; Kipf and Welling, 2017). In (Johnson et al., 2017), the authors propose a biased random walk strategy to capture higher-order proximity, showing gains on link prediction. Later, attention-based message passing refined the aggregation process and improved scalability (Velickovic et al., 2018). We build on these foundations by integrating temporal dynamics into the encoder, enabling time-aware node embeddings.",
    "reason": "Incorrect citation style: the preposition \"In\" should introduce a narrative citation, not a parenthetical one. It should be \"In Johnson et al. (2017), ...\".",
    "start": 177,
    "end": 202,
    "label": "Format"
  },
  {
    "span": "Exposure bias skews recommendations toward popular items (Abdollahpouri et al., 2019). Calibration aligns the predicted distribution with user profiles (Steck, 2018). Fairness constraints regulate disparate impact across groups (Yao and Huang, 2017). Causal inference disentangles preference from availability (Bonner and Vasile, 2018).",
    "document": "Related Work\n\nFairness in recommender systems concerns both user- and provider-side harms arising from biased data and feedback loops. Mitigation strategies range from reweighting and constrained optimization to causal adjustments and post-processing.\n\nExposure bias skews recommendations toward popular items (Abdollahpouri et al., 2019). Calibration aligns the predicted distribution with user profiles (Steck, 2018). Fairness constraints regulate disparate impact across groups (Yao and Huang, 2017). Causal inference disentangles preference from availability (Bonner and Vasile, 2018).\n\nBuilding on this literature, our approach unifies exposure control and group fairness via counterfactual ranking objectives, optimizing utility under constraints derived from identifiable causal pathways.",
    "reason": "The span covers exposure bias, calibration, fairness constraints, and causal inference as isolated points without clarifying their relationships. The sentences lack transitions and do not make explicit how each cited work connects to the previous one.",
    "start": 253,
    "end": 589,
    "label": "Coherence"
  },
  {
    "span": "It is well known that transformer transducers are more robust to domain shift than CTC models",
    "document": "Introduction\n\nEnd-to-end ASR has evolved from CTC-based encoders to attention and transducer architectures (Graves et al., 2006; Chan et al., 2016; Prabhavalkar et al., 2023). Robustness to domain shift is critical for deployment across accents and environments (Ko et al., 2017; Park et al., 2019). It is well known that transformer transducers are more robust to domain shift than CTC models. Recent studies explore data augmentation and self-training to further enhance generalization (Park et al., 2020; Kahn et al., 2020). We examine architectural and training factors affecting out-of-domain WER.",
    "reason": "Asserts a widely known comparative result without providing citations.",
    "start": 300,
    "end": 393,
    "label": "Unsupported claim"
  },
  {
    "span": "[Kuang, 2021]",
    "document": "Introduction\n\nOffline reinforcement learning (RL) aims to learn policies from static datasets without further environment interaction, mitigating safety and cost concerns (Levine et al., 2020; Fujimoto et al., 2019). Behavior regularization addresses extrapolation error by constraining policies toward the dataset support (Kumar et al., 2020). Recent advances propose value pessimism and uncertainty quantification to avoid overestimation (Agarwal et al., 2020; Kostrikov et al., 2021). In contrast, [Kuang, 2021] explores representation learning that disentangles behavior and reward signals for better policy reuse. We extend this line by introducing a dual-encoder objective that preserves support coverage while enhancing task transfer.\n",
    "reason": "Wrong bracket style for author–year citation; should use parentheses '(Kuang, 2021)' in this style.",
    "start": 501,
    "end": 514,
    "label": "Format"
  },
  {
    "span": "Most prior reinforcement learning work initializes replay buffers with expert trajectories.",
    "document": "Related Work\n\nOff-policy reinforcement learning (RL) leverages experience replay to stabilize learning and improve sample efficiency (Mnih et al., 2015; Lillicrap et al., 2016). Prior studies have explored demonstrations to accelerate exploration and avoid unsafe behavior (Hester et al., 2018; Vecerik et al., 2017). Most prior reinforcement learning work initializes replay buffers with expert trajectories. While effective in certain domains, reliance on high-quality experts can limit applicability and introduce bias.\n\nAlternative approaches synthesize demonstrations via model-based planning or leverage hindsight relabeling to create informative transitions from failed attempts (Andrychowicz et al., 2017; Nair et al., 2018). Our method instead bootstraps from unsupervised skill discovery, populating buffers with diverse behaviors without requiring experts.",
    "reason": "It generalizes about 'most prior work' using expert-initialized replay buffers without citing representative or survey evidence.",
    "start": 318,
    "end": 409,
    "label": "Unsupported claim"
  },
  {
    "span": "Personalization in federated learning has been studied via local fine-tuning (Smith et al., 2017; Dinh et al., 2020), model interpolation and mixture-of-experts (Mansour et al., 2020; Arivazhagan et al., 2019), clustering of clients (Sattler et al., 2020; Ghosh et al., 2020), and meta-learning formulations such as MAML-style updates (Fallah et al., 2020; Jiang et al., 2019). Regularization-based personalization adds proximal or feature alignment terms (Li et al., 2020; Collins et al., 2021). We adopt a meta-learning approach to personalize models across clients.",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative training across distributed clients while keeping data local (McMahan et al., 2017). Heterogeneity in client data distributions poses a central challenge for building performant models that serve all clients effectively.\n\nPersonalization in federated learning has been studied via local fine-tuning (Smith et al., 2017; Dinh et al., 2020), model interpolation and mixture-of-experts (Mansour et al., 2020; Arivazhagan et al., 2019), clustering of clients (Sattler et al., 2020; Ghosh et al., 2020), and meta-learning formulations such as MAML-style updates (Fallah et al., 2020; Jiang et al., 2019). Regularization-based personalization adds proximal or feature alignment terms (Li et al., 2020; Collins et al., 2021). We adopt a meta-learning approach to personalize models across clients.\n\nOur contributions are: (1) a simple bi-level optimizer adapted for on-device constraints, (2) an evaluation protocol spanning cross-device and cross-silo settings, and (3) an analysis of personalization-communication trade-offs.\n",
    "reason": "The span summarizes prior work and immediately states the authors' approach without identifying a specific gap or explaining why existing methods are insufficient, matching case (b).",
    "start": 282,
    "end": 850,
    "label": "Lacks synthesis"
  },
  {
    "span": "Classical time-series anomaly detection includes distance-based methods, density estimators, subspace projections, and rule mining (Keogh et al., 2002; Chandola et al., 2009; Gupta et al., 2014). Deep models range from autoencoders and VAEs to temporal CNNs and sequence models such as LSTMs and Transformers (Malhotra et al., 2016; Tay et al., 2020; Kim et al., 2021).",
    "document": "Related Work\n\nDetecting anomalies in multivariate sensor streams is critical for predictive maintenance and monitoring. Real-world deployments demand methods that are robust to nonstationarity, missing data, and abrupt regime shifts.\n\nClassical time-series anomaly detection includes distance-based methods, density estimators, subspace projections, and rule mining (Keogh et al., 2002; Chandola et al., 2009; Gupta et al., 2014). Deep models range from autoencoders and VAEs to temporal CNNs and sequence models such as LSTMs and Transformers (Malhotra et al., 2016; Tay et al., 2020; Kim et al., 2021).\n\nA growing body of work focuses on forecasting-residual strategies and reconstruction errors, yet both can be miscalibrated under distribution shift. Some recent approaches address covariate shift via domain adaptation or adversarial training, but often require labeled anomalies.\n\nWe propose shift-aware conformal detection that calibrates nonconformity scores online using covariate-aware risk control. Our method adapts thresholds in real time and preserves coverage under nonstationarity, outperforming baselines in synthetic and industrial datasets.",
    "reason": "The span provides a survey-style enumeration of methods without connecting them to the specific limitations (miscalibration, shift) that the paper tackles, thereby lacking synthesis with the authors’ objectives (criterion a).",
    "start": 235,
    "end": 604,
    "label": "Lacks synthesis"
  },
  {
    "span": "Recent work has demonstrated that instruction tuning dramatically improves cross-domain generalization.",
    "document": "Introduction\n\nLarge language models (LLMs) have demonstrated impressive few-shot generalization across a wide array of natural language tasks, especially when adapted with task-specific supervision and preference optimization (Doe et al., 2021; Roe and Smith, 2022). While prompt engineering can elicit strong zero-shot behavior, supervised alignment remains critical for safety and reliability in practical deployments (Lee et al., 2023).\n\nRecent work has demonstrated that instruction tuning dramatically improves cross-domain generalization.\n\nBuilding on this observation, we investigate multilingual instruction tuning for compositional reasoning across domains. We evaluate models on a suite of transfer tasks designed to probe robustness to domain shift and instruction paraphrasing, and we analyze the role of instruction diversity in generalization.",
    "reason": "Mentions 'recent work' without providing citations to the works; first mention of this claim requires references.",
    "start": 441,
    "end": 544,
    "label": "Unsupported claim"
  },
  {
    "span": "Early studies of robustness in convolutional networks examine adversarial examples and input corruptions (Szegedy et al., 2014; Hendrycks and Dietterich, 2019). Vision Transformers scale well with larger datasets and training regimes (Dosovitskiy et al., 2021). Weight decay is a classical regularizer to reduce overfitting (Krogh and Hertz, 1992). DeiT employs knowledge distillation to train ViTs efficiently (Touvron et al., 2021).",
    "document": "Related Work: Robustness of Vision Transformers\n\nUnderstanding model robustness to distribution shift and adversarial perturbations is critical for deployment. Recent work has contrasted convolutional and transformer-based vision models under a range of corruptions and training recipes.\n\nEarly studies of robustness in convolutional networks examine adversarial examples and input corruptions (Szegedy et al., 2014; Hendrycks and Dietterich, 2019). Vision Transformers scale well with larger datasets and training regimes (Dosovitskiy et al., 2021). Weight decay is a classical regularizer to reduce overfitting (Krogh and Hertz, 1992). DeiT employs knowledge distillation to train ViTs efficiently (Touvron et al., 2021).\n\nSubsequent analyses connect data augmentation and token mixing strategies to improved corruption robustness in ViTs (Paul and Chen, 2022). Other studies explore patchifying strategies and frequency biases as mechanisms underlying differences between CNNs and ViTs (Raghu et al., 2021). Our work complements these by isolating the role of training-time perturbations on robustness across architectures.",
    "reason": "The span enumerates four unconnected statements—CNN robustness, ViT scaling, generic regularization, and DeiT distillation—without transitions or an explicit relationship, producing abrupt topic shifts between cited works (issues a and b).",
    "start": 289,
    "end": 723,
    "label": "Coherence"
  },
  {
    "span": "Time-series anomaly detection has leveraged statistical models such as ARIMA and STL decomposition (Box et al., 2015; Cleveland et al., 1990), density and neighbor-based methods (LOF, Isolation Forest) (Breunig et al., 2000; Liu et al., 2008), and deep architectures including LSTM autoencoders, variational models, and Transformers (Malhotra et al., 2016; An and Cho, 2015; Zhou et al., 2021). Reconstruction-based and prediction-based criteria are both common, often augmented with thresholding heuristics (Audibert et al., 2020).",
    "document": "Introduction\n\nDetecting anomalies in time series is critical for monitoring industrial systems, finance, and healthcare, where rare events can be costly and safety-critical.\n\nTime-series anomaly detection has leveraged statistical models such as ARIMA and STL decomposition (Box et al., 2015; Cleveland et al., 1990), density and neighbor-based methods (LOF, Isolation Forest) (Breunig et al., 2000; Liu et al., 2008), and deep architectures including LSTM autoencoders, variational models, and Transformers (Malhotra et al., 2016; An and Cho, 2015; Zhou et al., 2021). Reconstruction-based and prediction-based criteria are both common, often augmented with thresholding heuristics (Audibert et al., 2020).\n\nWe present a unified probabilistic scoring rule that calibrates anomaly scores across nonstationary regimes using seasonal-context conditioning.",
    "reason": "The span enumerates techniques and criteria without articulating how they connect to or motivate the unified probabilistic scoring rule, nor does it state a clear gap (definition a and c).",
    "start": 175,
    "end": 707,
    "label": "Lacks synthesis"
  },
  {
    "span": "In a previous shared task on low-resource NER, most systems relied on CRF taggers",
    "document": "Introduction\n\nNamed entity recognition (NER) in low-resource settings remains challenging due to scarce annotations and domain mismatch. Approaches include cross-lingual transfer, distant supervision, and meta-learning strategies that adapt to new entity types (Pan et al., 2017; Rahimi et al., 2019).\n\nIn a previous shared task on low-resource NER, most systems relied on CRF taggers, often coupled with minimal character-level features. More recent approaches use pre-trained multilingual encoders for improved transfer. Our work contributes a prompt-based adaptation mechanism for handling novel labels with minimal supervision.",
    "reason": "The mention of a specific shared task and the systems used should include a citation to that shared task and accompanying results (rule a).",
    "start": 303,
    "end": 384,
    "label": "Unsupported claim"
  },
  {
    "span": "Performance on SQuAD is largely saturated, limiting further progress.",
    "document": "Introduction\n\nExtractive question answering (QA) benchmarks have catalyzed rapid improvements in reading comprehension models. However, as models approach or surpass human-level scores on popular leaderboards, marginal gains become less informative about true reasoning ability and generalization. Performance on SQuAD is largely saturated, limiting further progress. As a result, attention has shifted to out-of-domain generalization, adversarial contexts, and multi-hop reasoning.\n\nWe present ShiftQA, a distribution-shifted benchmark with controlled lexical and syntactic drift, and evaluate models under a unified protocol that isolates robustness from scale.",
    "reason": "The claim that SQuAD performance is 'largely saturated' is a field-wide assertion that should be supported with citations (e.g., leaderboards or survey papers) but none are provided.",
    "start": 298,
    "end": 367,
    "label": "Unsupported claim"
  },
  {
    "span": "Fairness-aware recommender systems implement pre-, in-, and post-processing interventions, including exposure constraints, re-ranking, adversarial debiasing, calibrated regularization, and counterfactual evaluation (Burke, 2017; Singh and Joachims, 2018; Yao and Huang, 2017; Wang et al., 2020; Bose and Hamilton, 2019).",
    "document": "Related Work\n\nRecommender systems can amplify historical biases, leading to uneven exposure across user groups and item providers. Recent research explores algorithmic interventions and evaluation frameworks to address these disparities.\n\nFairness-aware recommender systems implement pre-, in-, and post-processing interventions, including exposure constraints, re-ranking, adversarial debiasing, calibrated regularization, and counterfactual evaluation (Burke, 2017; Singh and Joachims, 2018; Yao and Huang, 2017; Wang et al., 2020; Bose and Hamilton, 2019).\n\nIn this study we present StreamFair, an online training and serving stack that targets exposure parity under streaming feedback. Our approach integrates bandit learning with constrained optimization to manage non-stationarity.",
    "reason": "The span lists categories of fairness methods without relating them to the online, streaming setting studied or explaining the unresolved gap StreamFair addresses (criterion a/c).",
    "start": 239,
    "end": 559,
    "label": "Lacks synthesis"
  },
  {
    "span": "Human-robot teaming has examined shared autonomy, proactive assistance, and role allocation (Dragan and Srinivasa, 2013; Javdani et al., 2015; Nikolaidis et al., 2017). Communication modalities include gaze, gestures, and natural language (Mutlu et al., 2009; Thomaz and Breazeal, 2008; Tellex et al., 2014). Trust calibration models study transparency and explainability (Hancock et al., 2011; Lee and See, 2004; Kaniarasu et al., 2013).",
    "document": "Introduction\n\nEffective human-robot teaming requires coordination mechanisms that are both efficient and legible to human partners. Teams must align on goals, adapt to changing contexts, and exchange information under uncertainty.\n\nHuman-robot teaming has examined shared autonomy, proactive assistance, and role allocation (Dragan and Srinivasa, 2013; Javdani et al., 2015; Nikolaidis et al., 2017). Communication modalities include gaze, gestures, and natural language (Mutlu et al., 2009; Thomaz and Breazeal, 2008; Tellex et al., 2014). Trust calibration models study transparency and explainability (Hancock et al., 2011; Lee and See, 2004; Kaniarasu et al., 2013).\n\nWe propose a policy that jointly optimizes task reward and a differentiable proxy for team common ground.\n\nOur experiments evaluate collaborative assembly and search-and-rescue tasks.",
    "reason": "Violates (a) and (b): lists prior literature without connecting it to the new policy and moves to the contribution without stating the unresolved problem the work targets.",
    "start": 232,
    "end": 670,
    "label": "Lacks synthesis"
  },
  {
    "span": "Demographic parity requires equal positive rates across groups (Dwork et al., 2012). Equalized odds equalizes error rates conditional on outcomes (Hardt et al., 2016). Calibration aligns predicted probabilities with frequencies (Guo et al., 2017). Counterfactual fairness formalizes fairness via invariance under interventions on protected attributes (Kusner et al., 2017).",
    "document": "Related Work\n\nFairness in machine learning\nClassification fairness has been formalized using multiple, often incompatible criteria, with growing attention to practical trade-offs under distribution shift. Approaches range from pre-processing to post-processing, each with distinct assumptions.\n\nDemographic parity requires equal positive rates across groups (Dwork et al., 2012). Equalized odds equalizes error rates conditional on outcomes (Hardt et al., 2016). Calibration aligns predicted probabilities with frequencies (Guo et al., 2017). Counterfactual fairness formalizes fairness via invariance under interventions on protected attributes (Kusner et al., 2017).\n\nOur study evaluates calibration-preserving post-processing under prior probability shift, highlighting robustness and utility impacts.",
    "reason": "The span lists several fairness concepts in separate sentences without transitions or discussion of how they relate or conflict, leaving the relationships implicit and creating an abrupt sequence of citations.",
    "start": 295,
    "end": 668,
    "label": "Coherence"
  },
  {
    "span": "Early multimodal models align audio and vision with text encoders to predict sentiment labels (Zadeh et al., 2017; Poria et al., 2017). Hazarika et al. (2018) introduce memory networks to capture speaker states in conversations. CLIP pretraining improves cross-modal representations when images accompany text (Radford et al., 2021). Emoji usage correlates with affect on social media and can serve as weak labels (Barbieri et al., 2018). BERT tokenization and contextualization increase robustness to informal language (Devlin et al., 2019).",
    "document": "Related Work\n\nMultimodal Sentiment Analysis\n\nMultimodal sentiment analysis integrates language, vision, and acoustics to infer user affect in conversations, reviews, and social media posts. Early works primarily fused low-level features with shallow classifiers, while more recent approaches employ pre-trained language models and end-to-end architectures to better capture cross-modal dependencies (Tsai et al., 2019; Mai et al., 2020). Datasets such as CMU-MOSI and CMU-MOSEI have catalyzed progress by providing synchronized audiovisual and textual signals at the segment level (Zadeh et al., 2016; Zadeh et al., 2018).\n\nEarly multimodal models align audio and vision with text encoders to predict sentiment labels (Zadeh et al., 2017; Poria et al., 2017). Hazarika et al. (2018) introduce memory networks to capture speaker states in conversations. CLIP pretraining improves cross-modal representations when images accompany text (Radford et al., 2021). Emoji usage correlates with affect on social media and can serve as weak labels (Barbieri et al., 2018). BERT tokenization and contextualization increase robustness to informal language (Devlin et al., 2019).\n\nBeyond modeling and data, challenges persist in domain shift across platforms, missing modalities at inference, and limited interpretability of cross-modal fusion decisions. Recent advances attempt modality dropout during training, uncertainty-aware fusion, and counterfactual explanations to improve robustness and trustworthiness (Mai et al., 2021; Mittal et al., 2022). Our work focuses on robust fusion under partial modality availability and evaluates generalization across datasets.",
    "reason": "The span lists multiple cited works across different subtopics without transitions or explicit relationships. It is unclear how CLIP pretraining, emoji weak labels, and BERT tokenization relate to earlier multimodal models or to each other, violating coherence by lacking sentence-to-sentence connection.",
    "start": 624,
    "end": 1166,
    "label": "Coherence"
  },
  {
    "span": "[23]",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become a standard tool for learning over relational structures, with applications in molecules, recommendation, and knowledge graphs (Hamilton et al., 2017; Kipf and Welling, 2017). While message-passing architectures dominate, spectral methods remain competitive in specific regimes (Defferrard et al., 2016). Several works have explored positional encodings to break the limitations of 1-WL expressivity, improving performance on counting and isomorphism tasks (Dwivedi and Bresson, 2020; Bouritsas et al., 2022). Prior methods often require expensive preprocessing [23], which limits scalability to large graphs. In contrast, we propose a lightweight positional bias derived from random walk hitting times that can be computed on the fly (Singh and Zhao, 2022).",
    "reason": "Inconsistent citation style: numeric bracket \"[23]\" used in a document otherwise following author–year style; should be replaced with an author–year citation or made consistent with the chosen style.",
    "start": 616,
    "end": 620,
    "label": "Format"
  },
  {
    "span": "[12]",
    "document": "Related Work\n\nGraph neural networks (GNNs) generalize convolution to non-Euclidean domains, enabling learning over relational structures (Bronstein et al., 2017; Kipf and Welling, 2017). Variants differ in how they aggregate neighborhood information and address over-smoothing (Xu et al., 2019; Oono and Suzuki, 2020).\n\nRecent studies explore spectral methods with learned filters and positional encodings to capture long-range dependencies (Dwivedi and Bresson, 2021; Ying et al., 2021). Knowledge-augmented GNNs integrate external triples to enrich node representations using attention over edges (Huang and Chen, 2022). As shown in [12], incorporating relation types improves link prediction.\n\nOur approach introduces a calibration layer that normalizes message passing by relation frequency, improving robustness under label sparsity.",
    "reason": "Wrong citation style: numeric bracketed citation '[12]' is inconsistent with the author–year style used throughout the document. It should be replaced with an author–year citation, e.g., (Author et al., Year).",
    "start": 635,
    "end": 639,
    "label": "Format"
  },
  {
    "span": "It is widely accepted that demographic labels are unnecessary for ensuring fairness in face verification.",
    "document": "Related Work\n\nFace verification systems exhibit performance disparities across demographic groups, motivating fairness-aware training and evaluation protocols (Buolamwini and Gebru, 2018; Krishnan et al., 2020). Mitigation strategies include balanced sampling, domain adaptation, and disentangled representations (Gong et al., 2021; Terhörst et al., 2021). It is widely accepted that demographic labels are unnecessary for ensuring fairness in face verification. In contrast, other work argues that explicit group labels enable auditing and targeted remediation (Bellamy et al., 2019; Wang et al., 2022).\n\nOur contribution examines label-free proxies derived from unsupervised clusters and shows how they can approximate group-aware calibration under privacy constraints.",
    "reason": "Makes a broad consensus claim in a niche topic without citing supporting sources; per rule (b), such statements require evidence.",
    "start": 357,
    "end": 462,
    "label": "Unsupported claim"
  },
  {
    "span": "Hernandez et al., 2019)",
    "document": "Related Work\n\nMulti-hop reasoning has benefited from structured retrieval and graph induction (Qi et al., 2019; Asai et al., 2020). Hernandez et al., 2019) introduced a modular architecture for iterative querying, while others proposed differentiable memory controllers to improve evidence aggregation (Graves et al., 2016; Dehghani et al., 2019). Recent research leverages dense retrievers with learned query reformulation to enhance recall (Karpukhin et al., 2020; Xiong et al., 2021). We extend this line by aligning retrieval and reasoning objectives through a shared uncertainty-aware loss (Rao and Kim, 2022).",
    "reason": "Missing opening parenthesis in a parenthetical citation.",
    "start": 132,
    "end": 155,
    "label": "Format"
  },
  {
    "span": "Adaptive user interfaces have been explored in recommender-driven layout optimization, accessibility personalization, and context-aware widgets (Findlater and McGrenere, 2008; Gajos et al., 2010; Yeh et al., 2014; Amershi et al., 2019). Bandit algorithms and Bayesian optimization have been applied to interface configuration and content ranking (Li et al., 2010; Chapelle and Li, 2011; Brochu et al., 2010). Recent work leverages large language models for UI synthesis and modification from natural language (Zhang et al., 2023; Ding et al., 2023).",
    "document": "Introduction\n\nDesigning interfaces that adapt to users and tasks can reduce cognitive load and improve efficiency. However, building adaptation logic that generalizes across applications and user populations remains difficult.\n\nAdaptive user interfaces have been explored in recommender-driven layout optimization, accessibility personalization, and context-aware widgets (Findlater and McGrenere, 2008; Gajos et al., 2010; Yeh et al., 2014; Amershi et al., 2019). Bandit algorithms and Bayesian optimization have been applied to interface configuration and content ranking (Li et al., 2010; Chapelle and Li, 2011; Brochu et al., 2010). Recent work leverages large language models for UI synthesis and modification from natural language (Zhang et al., 2023; Ding et al., 2023).\n\nWe introduce a task-conditioned policy that composes adaptation primitives learned from cross-app interaction logs and evaluate it in a mixed-methods study.",
    "reason": "The paragraph lists prior adaptive UI work across areas without relating it to the specific challenges the authors address or stating the gap that motivates their method.",
    "start": 228,
    "end": 777,
    "label": "Lacks synthesis"
  },
  {
    "span": "there have been many recent works exploring cross-sentence NER with distant supervision",
    "document": "Introduction\n\nNamed entity recognition (NER) in specialized domains often requires reasoning beyond sentence boundaries, especially when coreference and latent discourse cues determine entity spans and types. In the biomedical setting, mentions can be discontinuous or context-dependent, making sentence-local models insufficient for high recall.\n\nTo mitigate annotation costs, there have been many recent works exploring cross-sentence NER with distant supervision, leveraging weak labels from knowledge bases and aligning them to unstructured text. These approaches aim to scale training data while maintaining acceptable precision through noise-aware learning.\n\nHowever, fully exploiting document-level context in low-resource environments remains challenging due to noise accumulation and entity drift across sections. Our work proposes a hierarchical encoder with cross-sentence memory that balances local precision with document-level recall.",
    "reason": "Claims the existence of many recent works without providing citations, which should be included when referencing prior literature.",
    "start": 378,
    "end": 465,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent shared tasks have standardized evaluation for automatic stance detection.",
    "document": "Introduction\n\nStance detection classifies an author's position toward a target and underpins applications in rumor verification, political analysis, and fact-checking (Mohammad et al., 2016; Küçük and Can, 2020). While early datasets focused on tweet-level labels for fixed targets, later corpora expanded to conversational threads and cross-target generalization (Augenstein et al., 2016; Sobhani et al., 2017). Recent shared tasks have standardized evaluation for automatic stance detection. Our work complements these efforts by proposing a target-conditional calibration layer that improves reliability under label shift.",
    "reason": "The claim about 'recent shared tasks' is not supported with citations to the specific tasks, violating rule (d) and (a).",
    "start": 413,
    "end": 493,
    "label": "Unsupported claim"
  },
  {
    "span": "Back-translation has become the de facto standard for exploiting monolingual data in low-resource NMT.",
    "document": "Related Work\n\nNeural machine translation (NMT) in low-resource scenarios benefits considerably from leveraging monolingual corpora. Common strategies include back-translation, self-training, and bilingual lexicon induction. Back-translation has become the de facto standard for exploiting monolingual data in low-resource NMT. Nevertheless, the quality of synthetic sources, domain mismatch, and noise accumulation can limit gains, especially when target-side models are under-regularized. We investigate an iterative schedule that alternates direction, filters synthetic pairs using uncertainty, and rebalances batches to stabilize training.\n\nOur study complements techniques such as data diversification and tagged back-translation by providing a curriculum tied to model confidence.",
    "reason": "Declares a field-standard practice without citing the foundational works that established it (criterion b and d).",
    "start": 224,
    "end": 326,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works have demonstrated remarkable gains on multimodal summarization.",
    "document": "Introduction\n\nAutomatic summarization has traditionally focused on textual inputs, producing concise digests that preserve salient information (Jones and Brown, 2019; Li et al., 2020). With the ubiquity of images and video accompanying news and social media, multimodal summarization seeks to integrate vision and language to generate richer summaries (Chen and Zhao, 2021; Park et al., 2022). Recent works have demonstrated remarkable gains on multimodal summarization. However, it remains unclear which modality contributes most under different content regimes and evaluation settings.\n\nIn this paper, we present a systematic analysis of modality contribution, controlling for model capacity and training data scale. We compare text-only, vision-only, and multimodal fusion architectures under matched conditions (Kumar et al., 2021; Wang et al., 2022). Our contributions include a balanced benchmark with aligned annotations and a set of ablations that quantify the utility of visual features across genres.\n\nRelated Work\n\nMultimodal fusion strategies range from early fusion via shared encoders to late fusion with cross-modal attention (Nguyen et al., 2020; Sun et al., 2021). Datasets for multimodal summarization differ widely in domain and annotation protocol, complicating cross-study comparison (Liu et al., 2022). We follow best practices for evaluation using human judgments and automatic metrics adapted for multimodal content (Bhandari et al., 2020).",
    "reason": "The sentence claims the existence and impact of 'recent works' without providing citations at first mention, which should be referenced.",
    "start": 394,
    "end": 470,
    "label": "Unsupported claim"
  },
  {
    "span": "Nguyen et al.",
    "document": "Related Work\n\nNeural ranking methods have evolved rapidly with the advent of transformer encoders and large-scale pretraining. Early dual-encoder designs prioritized efficient retrieval over precise alignment, while later cross-encoders achieved superior accuracy at higher computational cost. As shown by Nguyen et al., contrastive pretraining with hard negatives significantly boosts retrieval performance in sparse data regimes. Subsequent studies incorporated curriculum sampling and hybrid lexical-dense rerankers to balance latency and accuracy across domains such as QA and web search.\n\nBeyond retrieval, representation learning for domain adaptation has explored instance weighting and pseudo-labeling to stabilize training when label distributions shift. Several surveys summarize these trends and identify open challenges around negative mining, distribution drift, and evaluation robustness in multilingual settings.",
    "reason": "Narrative citation is missing the publication year; should be formatted as Nguyen et al. (YEAR).",
    "start": 306,
    "end": 319,
    "label": "Format"
  },
  {
    "span": "Recent advances incorporate external tools into large language models. Toolformer annotates API calls via self-supervision (Schick et al., 2023). ReAct interleaves reasoning and acting to query tools iteratively (Yao et al., 2023). MRKL routes queries to modular experts using a language router (Press et al., 2022). PAL delegates problem solving to programs synthesized by LLMs (Gao et al., 2023).",
    "document": "Related Work\n\nTool Use and External Reasoning in Large Language Models\n\nAugmenting language models with external tools such as search, calculators, and code execution can improve factuality, reliability, and sample efficiency. A growing body of work explores prompting and training strategies for effective tool invocation.\n\nRecent advances incorporate external tools into large language models. Toolformer annotates API calls via self-supervision (Schick et al., 2023). ReAct interleaves reasoning and acting to query tools iteratively (Yao et al., 2023). MRKL routes queries to modular experts using a language router (Press et al., 2022). PAL delegates problem solving to programs synthesized by LLMs (Gao et al., 2023).\n\nOther methods focus on program-of-thought or scratchpad reasoning to decompose tasks, sometimes combining with retrieval or verification to mitigate hallucinations (Wei et al., 2022; Cobbe et al., 2021).",
    "reason": "The span recites related systems without analyzing their relation to the present work or highlighting the specific problem the authors intend to solve.",
    "start": 325,
    "end": 723,
    "label": "Lacks synthesis"
  },
  {
    "span": "Work on conversational recommendation explores belief tracking, slate generation, and natural language critique (Li et al., 2018; Sun and Zhang, 2018; Chen et al., 2019). Knowledge-grounded recommenders integrate entity graphs and templates (Zhou et al., 2020; Moon et al., 2019). Prompt-based methods adapt pretrained dialogue models with task descriptions (Zhang et al., 2022; Lin et al., 2022).",
    "document": "Related Work\n\nConversational recommendation systems aim to elicit user preferences through dialogue while suggesting relevant items. This paradigm combines elements from dialogue management, natural language generation, and collaborative filtering.\n\nWork on conversational recommendation explores belief tracking, slate generation, and natural language critique (Li et al., 2018; Sun and Zhang, 2018; Chen et al., 2019). Knowledge-grounded recommenders integrate entity graphs and templates (Zhou et al., 2020; Moon et al., 2019). Prompt-based methods adapt pretrained dialogue models with task descriptions (Zhang et al., 2022; Lin et al., 2022).\n\nIn this paper, we introduce a preference-editable state representation and evaluate it on two conversational datasets.",
    "reason": "Violates (b): provides a catalogue of prior work and then states the contribution, but does not highlight the specific unmet need or gap the paper addresses.",
    "start": 250,
    "end": 647,
    "label": "Lacks synthesis"
  },
  {
    "span": "Prior studies report that human evaluators prefer extractive rationales over abstractive ones in clinical QA.",
    "document": "Related Work\n\nExplainability for clinical question answering (QA) is crucial for trust and auditability. Rationales can be presented as extracted evidence spans or as generated summaries that paraphrase the underlying support. Prior studies report that human evaluators prefer extractive rationales over abstractive ones in clinical QA. This preference is often attributed to verifiability and reduced risk of hallucination.\n\nWe introduce a hybrid rationale framework that anchors generated summaries to extracted evidence through alignment constraints, aiming to balance readability and faithfulness.",
    "reason": "Unsupported claim because it attributes a finding to prior studies without citing any specific works (definition b).",
    "start": 227,
    "end": 336,
    "label": "Unsupported claim"
  },
  {
    "span": "the recent shared task on low-resource ASR demonstrated that self-supervision narrows the gap",
    "document": "Related Work\n\nLow-resource automatic speech recognition (ASR) research has increasingly explored unsupervised and semi-supervised learning to reduce dependence on transcribed audio. Approaches range from self-supervised pretraining to pseudo-label refinement and multilingual transfer. Evaluation practices vary widely across corpora and noise conditions, complicating cross-paper comparisons.\n\nIn this context, the recent shared task on low-resource ASR demonstrated that self-supervision narrows the gap between fully supervised and lightly supervised training. However, few studies analyze how label noise interacts with domain shift in genuinely small-data regimes.\n\nWe contribute an analysis-driven framework that jointly models label reliability and domain mismatch using curriculum-aware filtering.",
    "reason": "The text references a specific shared task and its findings without providing a citation (rule a: first mention of a shared task requires citation).",
    "start": 412,
    "end": 505,
    "label": "Unsupported claim"
  },
  {
    "span": "COCO has over 1.5 million labeled instances.",
    "document": "Introduction\n\nObject detection benchmarks have driven significant progress in both architecture design and training strategies (Ren et al., 2015; He et al., 2017; Tan et al., 2020). The COCO benchmark is particularly influential due to its diverse categories and dense annotations, enabling robust evaluation across scales and contexts (Lin et al., 2014). COCO has over 1.5 million labeled instances. Nonetheless, long-tail distributions and small object detection remain open challenges, often requiring specialized augmentations and loss functions (Cui et al., 2019; Wang et al., 2020). We introduce a curriculum-based sampler that adaptively focuses on underrepresented categories without overfitting to rare classes.",
    "reason": "Reports a specific dataset statistic without providing a citation to support the number, which should be referenced.",
    "start": 356,
    "end": 400,
    "label": "Unsupported claim"
  },
  {
    "span": "it is well known that U-Net variants dominate polyp segmentation benchmarks",
    "document": "Related Work\n\nColorectal polyp segmentation supports early detection and removal during colonoscopy, reducing cancer risk. Classical methods relied on handcrafted features sensitive to texture and color, but they struggled with low-contrast lesions and specular highlights. Deep learning has improved performance by leveraging encoder–decoder architectures, attention mechanisms, and multi-scale fusion.\n\nMultiple datasets with pixel-level annotations have enabled systematic evaluation under varying imaging conditions. Nevertheless, generalization across centers remains difficult due to device heterogeneity and differences in bowel preparation quality.\n\nIn the literature, it is well known that U-Net variants dominate polyp segmentation benchmarks, with many works proposing refinements such as improved skip connections, attention gates, and boundary-aware heads. Yet, their robustness to domain shift is inconsistent, and deployment must account for real-time constraints and hardware variability.\n\nWe revisit architectural choices for efficiency, introducing a lightweight decoder with mixed-scale depthwise convolutions and calibration layers designed for endoscopic video streams.",
    "reason": "This field-specific claim ('well known' dominance of U-Net variants on benchmarks) is presented without citations and should be supported by references.",
    "start": 677,
    "end": 752,
    "label": "Unsupported claim"
  },
  {
    "span": "Soft prompt tuning learns continuous embeddings prepended to inputs (Lester et al., 2021). Safety alignment reduces harmful outputs through preference optimization (Ouyang et al., 2022). In-context learning conditions on exemplars without gradient updates (Brown et al., 2020).",
    "document": "Introduction\n\nLarge language models (LLMs) exhibit strong few-shot capabilities but require careful adaptation to new tasks and safe deployment contexts (Brown et al., 2020; Bommasani et al., 2021). Parameter-efficient techniques and instruction tuning have emerged as practical alternatives to full fine-tuning for resource-constrained settings (Houlsby et al., 2019; Sanh et al., 2022).\n\nSoft prompt tuning learns continuous embeddings prepended to inputs (Lester et al., 2021). Safety alignment reduces harmful outputs through preference optimization (Ouyang et al., 2022). In-context learning conditions on exemplars without gradient updates (Brown et al., 2020). Recent efforts investigate how these approaches interplay with scaling laws and data curation to improve generalization and controllability (Wei et al., 2022; Longpre et al., 2023).\n\nWe explore a unified view of controllability that connects prompt optimization with preference-based training, offering a modular recipe for safe, task-adapted LLMs.",
    "reason": "The span juxtaposes soft prompt tuning, safety alignment, and in-context learning without transitions or explanation of their relationships, resulting in abrupt topic shifts and unclear connections.",
    "start": 390,
    "end": 667,
    "label": "Coherence"
  },
  {
    "span": "Prior work models human intent with inverse reinforcement learning, Bayesian filters, or probabilistic movement primitives (Ng and Russell, 2000; Ziebart et al., 2008; Paraschos et al., 2013). Our system learns intents online.",
    "document": "Related Work\n\nEffective human-robot collaboration requires anticipating human goals and adapting robot behavior accordingly. Approaches differ in how they represent uncertainty and how quickly they update beliefs about human intent. Prior work models human intent with inverse reinforcement learning, Bayesian filters, or probabilistic movement primitives (Ng and Russell, 2000; Ziebart et al., 2008; Paraschos et al., 2013). Our system learns intents online. Recent efforts also explore shared autonomy and mixed-initiative control to improve task efficiency (Javdani et al., 2015; Losey et al., 2018).\n\nWe evaluate in a tabletop assembly scenario with partial observability.",
    "reason": "The span names methods and then states the authors' choice without indicating what is missing in prior work or how the proposed approach addresses a gap (definition b) and lacks articulated motivation (definition c).",
    "start": 233,
    "end": 459,
    "label": "Lacks synthesis"
  },
  {
    "span": "Huang et al., 2019 [7]",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative training without centralizing raw data, addressing privacy and regulatory constraints (Konečný et al., 2016; McMahan et al., 2017; Kairouz et al., 2021). Communication efficiency and statistical heterogeneity are key challenges, motivating compression, adaptive aggregation, and personalization techniques (Suresh et al., 2017; Hsu et al., 2019; Li et al., 2021). Privacy amplification via secure aggregation and differential privacy further protects client updates (Bonawitz et al., 2017; Dwork, 2006; Geyer et al., 2017).\n\nRobustness to adversarial clients and data poisoning is an active research area, with Byzantine-resilient aggregators and anomaly detection methods proposed to counter attacks (Blanchard et al., 2017; Yin et al., 2018; Sun et al., 2019). Personalization approaches aim to balance global sharing with local adaptation (Arivazhagan et al., 2019; Fallah et al., 2020). Recent surveys Huang et al., 2019 [7] summarize these threads and outline open problems in real-world deployments.\n\nWe contribute a benchmark and method that jointly evaluate accuracy, fairness, and privacy across heterogeneous clients, offering a holistic view of FL trade-offs.",
    "reason": "Mixed citation styles: combining author–year with a numeric bracketed reference. It should be consistent, e.g., “Huang et al. (2019)” or “(Huang et al., 2019)”, without the “[7]”.",
    "start": 966,
    "end": 988,
    "label": "Format"
  },
  {
    "span": "Chen et al. 3",
    "document": "Related Work\n\nClassical time series forecasting relies on ARIMA and exponential smoothing (Box and Jenkins, 1970; Hyndman et al., 2008). Deep learning approaches exploit sequence models and global datasets for accuracy gains (Salinas et al., 2020; Lim and Zohren, 2021). Chen et al. 3 provide a comprehensive taxonomy of neural forecasting architectures, while concurrent surveys focus on interpretability and uncertainty quantification (Rangapuram et al., 2018; Hewamalage et al., 2021). Recent benchmarks emphasize reproducibility and fair evaluation (Beygelzimer et al., 2021).\n",
    "reason": "Improper footnote-style usage without a proper citation format; it should include the year (e.g., 'Chen et al. (2020)') or be formatted as a proper footnote/endnote.",
    "start": 271,
    "end": 284,
    "label": "Format"
  },
  {
    "span": "For molecular property prediction, message passing neural networks learn atom-centric representations via iterative neighborhood aggregation (Gilmer et al., 2017; Li et al., 2019). Spectral methods operate in the frequency domain of graphs to capture global structure (Bruna et al., 2014; Defferrard et al., 2016). Equivariant architectures encode geometric symmetries present in 3D conformers (Schütt et al., 2018; Fuchs et al., 2020; Satorras et al., 2021).",
    "document": "Related Work\n\nLearning molecular representations that generalize across scaffolds and capture stereochemistry is central to property prediction. Methods vary in how they incorporate topological and geometric information.\n\nFor molecular property prediction, message passing neural networks learn atom-centric representations via iterative neighborhood aggregation (Gilmer et al., 2017; Li et al., 2019). Spectral methods operate in the frequency domain of graphs to capture global structure (Bruna et al., 2014; Defferrard et al., 2016). Equivariant architectures encode geometric symmetries present in 3D conformers (Schütt et al., 2018; Fuchs et al., 2020; Satorras et al., 2021).\n\nThis paper studies low-data adaptation across unseen scaffolds by combining scaffold-aware pretraining with a geometry-regularized contrastive objective. We also introduce a split protocol that isolates scaffold shift from data scarcity to better evaluate generalization.",
    "reason": "The span enumerates categories and citations but provides no synthesis tying them to the low-data adaptation focus or clarifying what remains unresolved, thereby lacking an articulated perspective or gap (criterion a and c).",
    "start": 222,
    "end": 681,
    "label": "Lacks synthesis"
  },
  {
    "span": "ARIMA captures linear dependencies and seasonality with differencing and autoregression (Box and Jenkins, 1970). Prophet models multiple seasonality components with a decomposable trend (Taylor and Letham, 2018). Transformers capture long-range temporal patterns through self-attention (Zerveas et al., 2021). Isolation Forest detects anomalies through random partitioning (Liu et al., 2008).",
    "document": "Related Work: Time Series Forecasting and Representation Learning\n\nForecasting methods range from classical statistical models to modern deep architectures that learn representations from raw sequences. Practical systems must balance accuracy, interpretability, and robustness to shift and anomalies.\n\nARIMA captures linear dependencies and seasonality with differencing and autoregression (Box and Jenkins, 1970). Prophet models multiple seasonality components with a decomposable trend (Taylor and Letham, 2018). Transformers capture long-range temporal patterns through self-attention (Zerveas et al., 2021). Isolation Forest detects anomalies through random partitioning (Liu et al., 2008).\n\nRecent neural models incorporate spectral kernels and multi-scale convolutions to improve long-horizon stability (Wu et al., 2021; Zhou et al., 2021). Hybrid approaches combine statistical priors with learned components to reduce overfitting on short histories (Salinas et al., 2020). We propose a forecasting framework that jointly trains a seasonal-trend decomposition with attention-based residual modeling.",
    "reason": "The span abruptly shifts from forecasting models (ARIMA, Prophet, Transformers) to an anomaly detection method (Isolation Forest) without articulating any connection, lacking transitions and explicit relationships (issues a and b).",
    "start": 302,
    "end": 694,
    "label": "Coherence"
  },
  {
    "span": "Transformer variants have consistently surpassed RNNs on low-resource translation",
    "document": "Introduction\n\nSequence-to-sequence learning for machine translation (MT) has evolved rapidly with attention-based architectures and better training objectives. While high-resource settings benefit from large-scale corpora and transfer, low-resource MT remains challenging due to data scarcity and domain shift. Transformer variants have consistently surpassed RNNs on low-resource translation, particularly in settings with strong regularization and subword modeling. Building on these trends, we explore a parameter-efficient adapter that leverages monolingual denoising signals to improve translation quality in truly low-data regimes.",
    "reason": "Claims a general performance trend about prior work without citing supporting studies, violating (b) and (d).",
    "start": 311,
    "end": 392,
    "label": "Unsupported claim"
  },
  {
    "span": "The SemEval 2018 rumor detection dataset includes over 10,000 threads.",
    "document": "Introduction\n\nRumor detection on social media aims to identify veracity and stance in evolving conversational threads (Zubiaga et al., 2016; Ma et al., 2018). Benchmark datasets and shared tasks have catalyzed progress by providing standardized evaluation across platforms and events (Derczynski et al., 2017). The SemEval 2018 rumor detection dataset includes over 10,000 threads. Building on these resources, we introduce a temporal augmentation protocol that stresses early-detection performance.",
    "reason": "The sentence makes a specific statistical claim about a dataset without providing a citation to the dataset description, violating rule (a) and (b).",
    "start": 311,
    "end": 381,
    "label": "Unsupported claim"
  },
  {
    "span": "A variety of personalization strategies in federated learning have been explored, including fine-tuning local heads (Arivazhagan et al., 2019), meta-learning based personalization (Fallah et al., 2020), clustered aggregation (Sattler et al., 2020), and model interpolation (Dinh et al., 2020). Communication-efficient optimizers such as FedAvg, FedProx, and Scaffold are commonly adopted (McMahan et al., 2017; Li et al., 2020; Karimireddy et al., 2020).",
    "document": "Related Work\n\nFederated learning must contend with non-identically distributed clients and limited communication budgets. Personalization aims to tailor a global model to heterogeneous user data while preserving privacy and efficiency.\n\nA variety of personalization strategies in federated learning have been explored, including fine-tuning local heads (Arivazhagan et al., 2019), meta-learning based personalization (Fallah et al., 2020), clustered aggregation (Sattler et al., 2020), and model interpolation (Dinh et al., 2020). Communication-efficient optimizers such as FedAvg, FedProx, and Scaffold are commonly adopted (McMahan et al., 2017; Li et al., 2020; Karimireddy et al., 2020).\n\nDespite these advances, existing schemes often assume stationary client distributions and struggle under rapid user drift. We propose on-device adapters with implicit regularization that track evolving preferences without inflating communication costs.",
    "reason": "The span lists existing strategies and optimizers without relating them to the stated challenges (user drift, stationarity) or clarifying the gap the paper fills, hence lacks synthesis per (a) and (b).",
    "start": 237,
    "end": 691,
    "label": "Lacks synthesis"
  },
  {
    "span": "The PASCAL-5i dataset is the most widely used benchmark for few-shot semantic segmentation.",
    "document": "Related Work\n\nFew-shot semantic segmentation (FSS) aims to segment novel classes from only a handful of annotated examples by leveraging transferable representations. Early metric-learning techniques adapted prototype-based classifiers to the pixel level, while later works built upon meta-learning and attention mechanisms to better fuse support and query features.\n\nThe PASCAL-5i dataset is the most widely used benchmark for few-shot semantic segmentation. Alternative evaluations include COCO-based partitions and class-incremental protocols that stress continual adaptation. Concurrently, cross-domain FSS studies investigate transfer when the support and query images differ substantially in style or content.\n\nOur approach complements this literature by introducing a geometry-aware support conditioning module that preserves fine-grained boundaries. We report results across 1-shot and 5-shot settings and provide ablations on the effect of support mask quality.",
    "reason": "Asserts a dataset is the most widely used benchmark without any citation; per rule a) dataset mentions should be cited at first mention.",
    "start": 368,
    "end": 459,
    "label": "Unsupported claim"
  },
  {
    "span": "Style transfer maps source images to target appearance with cycle-consistency constraints (Zhu et al., 2017). Self-ensembling averages temporal predictions to regularize targets (French et al., 2018). Test-time adaptation updates batch-norm to handle scanner drift (Schneider et al., 2020). Federated learning trains across hospitals without sharing raw data (Li et al., 2020). Pseudo-label filtering removes uncertain targets before retraining (Bai et al., 2019).",
    "document": "Introduction\n\nDomain Adaptation in Medical Image Segmentation\n\nMedical image segmentation models often degrade when applied to data from unseen scanners, protocols, or institutions due to covariate shift. Unsupervised domain adaptation (UDA) methods aim to transfer models from labeled source domains to unlabeled target domains, balancing performance with clinical reliability (Kamnitsas et al., 2017; Zhang et al., 2022). Practical constraints, including privacy regulations and limited annotation budgets, motivate approaches that avoid sharing raw patient data.\n\nStyle transfer maps source images to target appearance with cycle-consistency constraints (Zhu et al., 2017). Self-ensembling averages temporal predictions to regularize targets (French et al., 2018). Test-time adaptation updates batch-norm to handle scanner drift (Schneider et al., 2020). Federated learning trains across hospitals without sharing raw data (Li et al., 2020). Pseudo-label filtering removes uncertain targets before retraining (Bai et al., 2019).\n\nWe present a distributionally robust UDA framework that combines feature-level alignment with uncertainty-aware pseudo-labeling under communication constraints, targeting reliable deployment across multi-center MRI datasets.",
    "reason": "The span abruptly lists several techniques from different subareas (style transfer, self-ensembling, test-time adaptation, federated learning) without transitions or explanation of their relationships, resulting in unclear coherence.",
    "start": 567,
    "end": 1031,
    "label": "Coherence"
  },
  {
    "span": "In (Doe et al., 2020)",
    "document": "Related Work\n\nGraph pooling methods compress node representations to enable graph-level prediction. In (Doe et al., 2020), the authors introduce differentiable pooling that clusters nodes via learned assignments, improving over global mean pooling (Ramos and Stein, 2018). Hierarchical variants extend this idea with multiscale structure (Kim and Patel, 2021). Beyond pooling, positional encodings enrich message passing with structural biases (Jiang et al., 2022; Ahmed and Torres, 2023). We build on attention-based pooling but focus on stability under node perturbations, which is underexplored in prior studies (Huang and Liu, 2021).",
    "reason": "Wrong citation style: a narrative construction should read 'In Doe et al. (2020)' rather than 'In (Doe et al., 2020)'.",
    "start": 100,
    "end": 121,
    "label": "Format"
  },
  {
    "span": "Lopez et al.",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become the de facto approach for learning on relational data due to their ability to capture local structure via message passing (Kipf and Welling, 2017; Hamilton et al., 2017). Despite their success, scalability and oversmoothing remain central challenges (Li et al., 2018; Oono and Suzuki, 2020). Sampling-based methods (Chen et al., 2018) and subgraph techniques (Zeng et al., 2020) aim to reduce computational cost while preserving predictive performance.\n\nRecent work explores dynamic graphs, temporal reasoning, and inductive generalization (Rossi et al., 2020; Xu et al., 2020). For node classification in heterogeneous networks, type-specific transformations and meta-paths have been shown effective (Dong et al., 2017; Wang et al., 2019). Lopez et al. propose a hybrid message passing scheme that adaptively prunes irrelevant neighbors during training, improving robustness on noisy graphs. We extend this line by introducing a stability regularizer that reduces variance in neighborhood selection across epochs.\n",
    "reason": "Narrative citation is missing the publication year; it should be 'Lopez et al. (Year)'.",
    "start": 796,
    "end": 808,
    "label": "Format"
  },
  {
    "span": "Following standard practice, we adopt the OntoNotes split with 80/10/10 for train/dev/test.",
    "document": "Introduction\n\nNamed entity recognition (NER) benchmarks have catalyzed progress on domain-general extraction as well as specialized settings such as biomedical or financial text. Pre-trained encoders paired with conditional random fields remain competitive, particularly when combined with character-level features or span-based classifiers. Following standard practice, we adopt the OntoNotes split with 80/10/10 for train/dev/test. We also introduce a cross-domain evaluation on conversational transcripts to test robustness to disfluencies and colloquialisms.\n\nRelated Work\n\nPrior work has studied label consistency, span boundary errors, and calibration in NER, but few studies jointly assess domain robustness and calibration under label noise.",
    "reason": "This sentence both references a specific dataset (OntoNotes) and asserts a community-standard split without providing citations; first mention of datasets and claims of standard practice require references.",
    "start": 342,
    "end": 433,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works show that per-example clipping stabilizes training under high noise",
    "document": "Related Work\n\nDifferentially private (DP) learning introduces calibrated noise and gradient clipping to bound information leakage about individual examples. While DP-SGD is widely used, it often incurs significant utility loss, especially at moderate to high privacy budgets. Several techniques aim to mitigate this degradation by adaptive clipping, accounting, and noise scheduling.\n\nRecent works show that per-example clipping stabilizes training under high noise. Nevertheless, selecting clipping thresholds remains challenging due to distributional drift across training and evaluation domains. Moreover, per-layer clipping interacts with optimizer dynamics in poorly understood ways.\n\nWe explore a bilevel optimization scheme that adapts clipping thresholds to validation risk under privacy constraints, improving both stability and accuracy.",
    "reason": "The span refers to 'recent works' and asserts a finding without providing any citations to those works, violating the requirement to cite recent literature when referenced.",
    "start": 385,
    "end": 465,
    "label": "Unsupported claim"
  },
  {
    "span": "Federated learning (FL) research spans communication efficiency through compression and partial participation (Konečný et al., 2017; Sattler et al., 2019), personalization through meta-learning and parameter decoupling (Fallah et al., 2020; Arivazhagan et al., 2019), and privacy via secure aggregation and differential privacy (Bonawitz et al., 2017; Geyer et al., 2018). Systems studies further address stragglers, heterogeneity, and scheduling (Li et al., 2020; Nishio and Yonetani, 2019).",
    "document": "Introduction\n\nDeploying machine learning across decentralized data silos must balance model quality with systems constraints and privacy. Federated learning offers a framework for on-device training with server-side aggregation.\n\nFederated learning (FL) research spans communication efficiency through compression and partial participation (Konečný et al., 2017; Sattler et al., 2019), personalization through meta-learning and parameter decoupling (Fallah et al., 2020; Arivazhagan et al., 2019), and privacy via secure aggregation and differential privacy (Bonawitz et al., 2017; Geyer et al., 2018). Systems studies further address stragglers, heterogeneity, and scheduling (Li et al., 2020; Nishio and Yonetani, 2019).\n\nWe explore training under bursty connectivity and nonstationary client populations. Our method couples an anytime-weighted server optimizer with a client-side elastic head, delivering stable improvements despite churn and distribution drift.",
    "reason": "The span recites broad FL areas and citations without making explicit how existing efforts fail under bursty connectivity or motivate the proposed method; it lacks a clear connection to the authors’ problem framing (criterion a and b).",
    "start": 230,
    "end": 722,
    "label": "Lacks synthesis"
  },
  {
    "span": "A growing line of work investigates prompt-based and in-context strategies with large language models for steering responses and tapping latent knowledge (Chen et al., 2021; Park and Lee, 2022; Zhang et al., 2022; Wu et al., 2023).",
    "document": "Related Work\n\nNeural question answering (QA) has evolved from recurrent architectures with attention to transformer-based models that couple retrieval and reading comprehension components. Early pipelines emphasized efficient document retrieval to bound the search space before extracting spans from candidate passages.\n\nOpen-domain QA expanded this paradigm by scaling retrieval corpora and adopting dense representations. Concurrently, few-shot settings challenged models to generalize with minimal supervision by relying on pre-trained encoders and transfer.\n\nA growing line of work investigates prompt-based and in-context strategies with large language models for steering responses and tapping latent knowledge (Chen et al., 2021; Park and Lee, 2022; Zhang et al., 2022; Wu et al., 2023). Several studies further examine how answer calibration and chain-of-thought cues influence reasoning quality in QA tasks (Huang et al., 2022; Lin et al., 2023).\n\nIn this work, we study QA under limited supervision using lightweight adapters on top of frozen encoders and an efficient retrieval module.",
    "reason": "The span summarizes prior prompting work without explaining how it relates to the paper’s approach, lacks an articulated gap, and does not provide the authors’ perspective (definition a and c).",
    "start": 563,
    "end": 794,
    "label": "Lacks synthesis"
  },
  {
    "span": " (Garcia et al. 2015)",
    "document": "Related Work\n\nTime-series forecasting methods range from statistical baselines to deep sequence models (Box and Jenkins, 1970; Salinas et al., 2020). For electricity load and traffic data, studies (Garcia et al. 2015) demonstrate that hybrid models can capture seasonal regimes, while temporal fusion architectures improve interpretability (Lim et al., 2021). We focus on multivariate settings with exogenous signals (Wu et al., 2021).",
    "reason": "Missing comma before the year; should be '(Garcia et al., 2015)'.",
    "start": 196,
    "end": 217,
    "label": "Format"
  },
  {
    "span": "There are many recent works that explore code-mixed sentiment analysis in low-resource settings.",
    "document": "Introduction\n\nCode-mixed sentiment analysis addresses the challenge of classifying opinions expressed across multiple languages within the same utterance. Social media users frequently blend languages, scripts, and dialects, making conventional monolingual models brittle and data-hungry. There are many recent works that explore code-mixed sentiment analysis in low-resource settings. However, consistent benchmarking remains difficult due to the scarcity of standardized datasets and the variability in annotation schemes across language pairs.\n\nRelated Work\n\nPrior studies have examined token-level normalization, script conversion, and subword modeling to mitigate sparsity in code-mixed text. Others have focused on transfer learning from high-resource languages and multilingual pretraining to bridge gaps in data availability. Despite these advances, reported improvements are often contingent on domain-specific heuristics and fragile preprocessing pipelines. This motivates our exploration of robust architectures that remain competitive without extensive handcrafted rules.\n\nContributions\n\nWe propose a multilingual adapter framework for sentiment classification that learns language-agnostic sentiment cues while preserving code-specific signals. We further release a small but carefully curated evaluation suite spanning three language pairs and two social platforms to facilitate future comparisons.",
    "reason": "The phrase \"recent works\" asserts prior literature but provides no citations to support the claim, violating rule (d) and (a) of the definition.",
    "start": 289,
    "end": 385,
    "label": "Unsupported claim"
  },
  {
    "span": "Low-resource neural machine translation benefits from back-translation (Sennrich et al., 2016), dual learning (He et al., 2016), and multilingual joint training (Johnson et al., 2017; Aharoni et al., 2019). Recent work leverages pre-trained sequence-to-sequence models like mBART (Liu et al., 2020) and mT5 (Xue et al., 2021), data augmentation via noising (Edunov et al., 2018), and vocabulary sharing or subword alignment (Kudo, 2018; Artetxe et al., 2018).",
    "document": "Introduction\n\nImproving translation quality for low-resource language pairs remains challenging due to data scarcity and domain shift between available corpora and downstream use. A variety of data-centric and model-centric strategies have been explored.\n\nLow-resource neural machine translation benefits from back-translation (Sennrich et al., 2016), dual learning (He et al., 2016), and multilingual joint training (Johnson et al., 2017; Aharoni et al., 2019). Recent work leverages pre-trained sequence-to-sequence models like mBART (Liu et al., 2020) and mT5 (Xue et al., 2021), data augmentation via noising (Edunov et al., 2018), and vocabulary sharing or subword alignment (Kudo, 2018; Artetxe et al., 2018).\n\nCorpus filtering and quality estimation further influence performance by removing misaligned or noisy sentence pairs (Koehn et al., 2019; Wang et al., 2018). Domain adaptation techniques, including fine-tuning and instance weighting, aim to reduce mismatch with target distributions (Chu and Wang, 2018).\n\nIn this work, we investigate an iterative pseudo-labeling pipeline that alternates between constrained decoding and lexicon-guided refinement for two under-represented languages.",
    "reason": "The span summarizes prior NMT techniques without stating how they relate to the proposed approach or what gap persists, thus lacking synthesis per (a)/(c).",
    "start": 256,
    "end": 715,
    "label": "Lacks synthesis"
  },
  {
    "span": "To correct for exposure bias and position bias in recommender systems, prior work employs inverse propensity weighting, doubly robust estimation, and randomized logging interventions (Schnabel et al., 2016; Swaminathan and Joachims, 2015; Wang et al., 2020). Causal graph formulations additionally characterize user-item interactions and confounding pathways (Pearl, 2009; Bonner and Vasile, 2018).",
    "document": "Related Work\n\nImplicit feedback datasets in recommendation reflect a mixture of user preferences and platform-induced biases. This has motivated methods that aim to disentangle or correct for systematic confounding.\n\nTo correct for exposure bias and position bias in recommender systems, prior work employs inverse propensity weighting, doubly robust estimation, and randomized logging interventions (Schnabel et al., 2016; Swaminathan and Joachims, 2015; Wang et al., 2020). Causal graph formulations additionally characterize user-item interactions and confounding pathways (Pearl, 2009; Bonner and Vasile, 2018).\n\nRecent studies also explore bandit feedback and off-policy evaluation as practical tools for safer online deployment (Li et al., 2011; Tang et al., 2015).",
    "reason": "The text lists methods and citations but does not explain how they relate to the present work, what limitations remain, or what perspective the authors take, thus lacking synthesis (criteria a and c).",
    "start": 217,
    "end": 615,
    "label": "Lacks synthesis"
  },
  {
    "span": "In a previous study, the authors claim character n-grams outperform word embeddings.",
    "document": "Related Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression to deep neural networks that jointly learn content and coherence representations (Shermis and Burstein, 2013; Taghipour and Ng, 2016; Dong and Zhang, 2016). The ASAP dataset and other large-scale prompts have become standard for benchmarking (Shermis, 2014; Cummins et al., 2016). In a previous study, the authors claim character n-grams outperform word embeddings. Other lines of work incorporate discourse structure via RST parsing and entity-grid models to capture global coherence (Burstein et al., 2013; Zhang and Litman, 2018). More recently, pretrained language models fine-tuned with prompt-level calibration have narrowed the gap to human raters, though robustness to adversarial paraphrases remains a concern (Mayfield and Black, 2020; Uto et al., 2020).",
    "reason": "Mentions a specific prior study and its finding without providing a citation to identify the study.",
    "start": 378,
    "end": 462,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors claim that 5-shot performance already saturates on CIFAR-FS.",
    "document": "Introduction\n\nFew-shot image classification evaluates models that generalize to novel classes from a handful of labeled examples (Vinyals et al., 2016; Snell et al., 2017). Metric-based and optimization-based approaches have shown promise under episodic training (Chen et al., 2019). In a previous study, the authors claim that 5-shot performance already saturates on CIFAR-FS. Nevertheless, recent methods report continued gains from stronger backbones and data augmentation (Tian et al., 2020; Dhillon et al., 2020). We examine the effect of pretraining scale on saturation phenomena.",
    "reason": "References a prior study and its claim without citing the study (rule b and example ii).",
    "start": 284,
    "end": 377,
    "label": "Unsupported claim"
  },
  {
    "span": "Most researchers agree that hierarchical encoders are the de facto standard for meeting transcription.",
    "document": "Related Work\n\nLong-form speech transcription in meetings presents overlapping speech, disfluencies, and long-range dependencies. Early ASR pipelines processed utterances independently, ignoring discourse context, which limited robustness to topic shifts and speaker turns.\n\nMost researchers agree that hierarchical encoders are the de facto standard for meeting transcription. Recent modeling trends incorporate segment-level encoders with conversation-level context modules and diarization-aware attention. However, architecture design and training curricula vary widely, making it difficult to disentangle the benefits of hierarchy from other factors such as data augmentation and language model fusion.\n\nOur study isolates context-window size and hierarchical depth in a controlled setting, providing ablations on overlap handling and speaker-conditioned attention. We show that careful context selection can close much of the gap attributed to deep hierarchy.\n\nWe provide recipes for practical deployment under latency constraints.",
    "reason": "The claim asserts field-wide consensus about a specific architecture without any supporting citations or survey evidence (rule b).",
    "start": 274,
    "end": 376,
    "label": "Unsupported claim"
  },
  {
    "span": "Our approach follows the standard practice of filtering training pairs with ROUGE-L > 0.4 in scientific summarization.",
    "document": "Related Work\n\nTraining data quality strongly affects summarization systems, particularly in scientific domains where abstracts, titles, and sections vary in informativeness. Prior preprocessing pipelines often filter noisy pairs to improve supervision signals. Our approach follows the standard practice of filtering training pairs with ROUGE-L > 0.4 in scientific summarization. We further apply section-aware normalization to account for structural differences across articles.\n\nThis setup provides a cleaner signal for our contrastive objective, enabling the model to focus on salient content while reducing overfitting to boilerplate text.",
    "reason": "Unsupported claim because it asserts a 'standard practice' with a specific threshold but provides no citation to establish this convention (definition b).",
    "start": 261,
    "end": 379,
    "label": "Unsupported claim"
  },
  {
    "span": "(O'Neil et.al., 2016)",
    "document": "Related Work\n\nFairness in machine learning encompasses group fairness metrics, individual fairness, and causal definitions (Barocas et al., 2019; Corbett-Davies and Goel, 2018). Auditing pipelines for bias has gained traction across domains including hiring, lending, and healthcare (Kleinberg et al., 2018; Obermeyer et al., 2019). Several toolkits provide modular components for measurement and mitigation (Bellamy et al., 2019; (O'Neil et.al., 2016)).\n\nRecent approaches incorporate counterfactual reasoning and distributionally robust optimization to improve out-of-domain fairness (Kusner et al., 2017; Hashimoto et al., 2018). Our work complements these efforts with a benchmarking protocol that standardizes evaluation across datasets and metrics.",
    "reason": "Incorrect 'et al.' formatting ('et.al.'); should be '(O'Neil et al., 2016)'.",
    "start": 431,
    "end": 452,
    "label": "Format"
  },
  {
    "span": "Neural program synthesis maps natural language to executable code (Yin and Neubig, 2017). Type systems restrict search spaces via static semantics (Odena and Sutton, 2019). Self-training improves code generation with pseudo-labels (Chen et al., 2021). Retrieval augments decoders with similar programs (Hashimoto et al., 2018).",
    "document": "Related Work\n\nProgram synthesis from natural language has advanced with large language models, retrieval augmentation, and better training signals. Key challenges include handling long-range dependencies, enforcing correctness, and generalizing to unseen libraries or APIs.\n\nNeural program synthesis maps natural language to executable code (Yin and Neubig, 2017). Type systems restrict search spaces via static semantics (Odena and Sutton, 2019). Self-training improves code generation with pseudo-labels (Chen et al., 2021). Retrieval augments decoders with similar programs (Hashimoto et al., 2018).\n\nOur work integrates type-aware constrained decoding with verification-guided learning. We introduce a counterexample buffer that refines the model using failing test cases, improving both correctness and compositional generalization.",
    "reason": "The span lists four areas—basic synthesis, type systems, self-training, and retrieval—without explaining how they relate or transition from one to another. The relationships are only implied, leading to abrupt, incoherent progression across sentences.",
    "start": 275,
    "end": 602,
    "label": "Coherence"
  },
  {
    "span": "Recent surveys estimate that 68% of hospitals prohibit any cross-institutional data transfer.",
    "document": "Introduction\n\nFederated learning (FL) enables training across institutions without centralizing raw data, a property attractive for healthcare applications (McMahan et al., 2017; Li et al., 2020). Hospitals maintain heterogeneous electronic health record (EHR) systems, complicating model interoperability and privacy guarantees (Rajkomar et al., 2018). Recent surveys estimate that 68% of hospitals prohibit any cross-institutional data transfer. This creates mismatched participation and stragglers during training. We develop an adaptive aggregation scheme that tolerates partial participation and non-IID feature spaces.",
    "reason": "Claims a specific statistic from 'surveys' without citing any survey (rule b: unsupported statistic).",
    "start": 354,
    "end": 447,
    "label": "Unsupported claim"
  },
  {
    "span": " (Nguyen et al., 2020",
    "document": "Introduction\n\nFederated learning enables collaborative training across clients without centralized raw data collection, reducing privacy risks (Rossi and Patel, 2019). Yet, non-IID data and straggler effects can degrade convergence and fairness across participants (Zhang and Luo, 2021; Mei et al., 2022).\n\nWe study adaptive aggregation rules that reweight updates based on gradient diversity and client reliability. Prior works propose robust aggregation to address Byzantine failures (Kang and Zhu, 2020) and proximal objectives to reduce client drift (Li et al., 2020). In contrast, we target fairness in personalized federated learning by jointly optimizing local objectives and a global calibration term  (Nguyen et al., 2020 across heterogeneous cohorts.\n\nOur contributions include a convergence analysis under bounded heterogeneity and an empirical study on three cross-silo benchmarks.",
    "reason": "Missing closing parenthesis in a parenthetical citation. It should be (Nguyen et al., 2020), but the closing ')' is absent.",
    "start": 709,
    "end": 730,
    "label": "Format"
  },
  {
    "span": "in (Smith et al., 2019)",
    "document": "Related Work\n\nSelf-training has a long history in semi-supervised learning (Zhu and Goldberg, 2009) and was revitalized by modern consistency objectives (Sohn et al., 2020; Xie et al., 2020). This was demonstrated in (Smith et al., 2019) and later refined by contrastive variants (Tang and Li, 2021; Wu et al., 2022). However, label noise remains a challenge when pseudo-labels are unreliable (Reed et al., 2015; Arazo et al., 2019).\n\nOur method complements these works by estimating uncertainty to filter noisy targets while preserving diverse hypotheses (Mukhoti et al., 2021).",
    "reason": "Wrong citation style after a preposition; should be narrative style 'in Smith et al. (2019)' rather than parenthetical 'in (Smith et al., 2019)'.",
    "start": 214,
    "end": 237,
    "label": "Format"
  },
  {
    "span": "[Johnson et al., 2015]",
    "document": "Introduction\n\nDomain adaptation techniques often rely on minimizing discrepancies between source and target distributions. As noted in [Johnson et al., 2015], aligning marginal feature distributions can fail when class-conditional shifts are present. Subsequent work incorporated conditional invariance and adversarial learning to better address label shift, but practical deployment remains sensitive to target sample size and class imbalance.\n\nWe revisit this problem with a calibration-aware objective that jointly regularizes conditional divergence and class prior estimation.",
    "reason": "Wrong bracket style for author–year citation; square brackets should be parentheses: (Johnson et al., 2015).",
    "start": 135,
    "end": 157,
    "label": "Format"
  },
  {
    "span": "Neural program induction learns to execute algorithms from input–output pairs (Graves et al., 2014). Domain-specific language (DSL) synthesis constrains search with grammar priors (Ellis et al., 2018). Type-driven synthesis uses specifications to prune candidate programs (Polikarpova et al., 2016). Large code models memorize repository patterns (Zhang et al., 2021).",
    "document": "Related Work: Program Synthesis and Code Generation\n\nProgram synthesis methods blend search, learning, and specifications to generate code satisfying input–output or logical constraints. Recent progress leverages structured priors, differentiable components, and large corpora of code.\n\nNeural program induction learns to execute algorithms from input–output pairs (Graves et al., 2014). Domain-specific language (DSL) synthesis constrains search with grammar priors (Ellis et al., 2018). Type-driven synthesis uses specifications to prune candidate programs (Polikarpova et al., 2016). Large code models memorize repository patterns (Zhang et al., 2021).\n\nHybrid neuro-symbolic approaches combine neural proposal mechanisms with symbolic verifiers to ensure correctness (Balog et al., 2017). Prompt-based code LMs condition on natural language to produce API-using snippets (Austin et al., 2021). We introduce a verifier-guided decoding strategy that improves functional correctness without extra training.",
    "reason": "The span strings together four areas—induction, DSL synthesis, type-driven synthesis, and code LMs—without articulating their relationships or providing transitions, making the coherence across cited works unclear (issues a and b).",
    "start": 287,
    "end": 655,
    "label": "Coherence"
  },
  {
    "span": "Domain randomization improves sim-to-real transfer by perturbing dynamics (Tobin et al., 2017). Intrinsic motivation bonuses encourage exploration in sparse reward tasks (Pathak et al., 2017). Differentiable physics enables gradient-based control optimization (Degrave et al., 2019).",
    "document": "Introduction\n\nReinforcement learning (RL) for robotics faces challenges from sample inefficiency, safety constraints, and sim-to-real gaps. Research spans algorithmic advances, environment design, and transfer techniques that aim to reduce reliance on real-world trials.\n\nDomain randomization improves sim-to-real transfer by perturbing dynamics (Tobin et al., 2017). Intrinsic motivation bonuses encourage exploration in sparse reward tasks (Pathak et al., 2017). Differentiable physics enables gradient-based control optimization (Degrave et al., 2019). We investigate structured domain perturbations guided by stability certificates to safely bridge simulation and reality.\n",
    "reason": "The cited areas—sim-to-real randomization, exploration bonuses, and differentiable physics—are presented in sequence without stating their relationships, yielding weak coherence.",
    "start": 272,
    "end": 555,
    "label": "Coherence"
  },
  {
    "span": "Neural code generation has progressed from sequence-to-sequence models with attention to Transformer-based decoders trained on large code corpora (Allamanis et al., 2018; Chen and Zhou, 2019; Patel et al., 2021). Retrieval-augmented generators incorporate nearest neighbor snippets to improve factuality (Khandelwal et al., 2020; Yao and Sun, 2022). Structured decoding techniques constrain outputs to grammar or type semantics (Dong and Lapata, 2018; Yin and Neubig, 2018; Wang et al., 2022).",
    "document": "Introduction\n\nAutomatically generating correct and maintainable code from natural language intent can accelerate development and broaden access to programming. Yet robust generalization across projects, libraries, and evolving APIs remains challenging.\n\nNeural code generation has progressed from sequence-to-sequence models with attention to Transformer-based decoders trained on large code corpora (Allamanis et al., 2018; Chen and Zhou, 2019; Patel et al., 2021). Retrieval-augmented generators incorporate nearest neighbor snippets to improve factuality (Khandelwal et al., 2020; Yao and Sun, 2022). Structured decoding techniques constrain outputs to grammar or type semantics (Dong and Lapata, 2018; Yin and Neubig, 2018; Wang et al., 2022).\n\nWe introduce spec-grounded decoding, which binds generation to executable interface specifications mined from project build graphs. Our approach reduces hallucinated imports and deprecations by aligning code tokens with verified symbols at decode time.",
    "reason": "The span lists prior lines of work and techniques without stating how they relate to or fall short of the proposed spec-grounded decoding; no motivation or gap is articulated (criterion a and c).",
    "start": 254,
    "end": 747,
    "label": "Lacks synthesis"
  },
  {
    "span": "Lee and Chang, 2021)",
    "document": "Introduction\n\nScaling laws suggest predictable improvements in performance with increased data and model capacity. Recent surveys Lee and Chang, 2021) emphasize the need for standardized evaluation across languages, especially for tokenization-sensitive scripts. Complementary work highlights that pretraining corpora composition drives disparities in downstream fairness and calibration.\n\nWe unify these insights by proposing a multilingual benchmark that controls for script diversity, morphology, and domain mismatch.",
    "reason": "Missing opening parenthesis in the parenthetical citation.",
    "start": 130,
    "end": 150,
    "label": "Format"
  },
  {
    "span": "a previous study found that character-aware LSTMs outperform word-level models on noisy user-generated text",
    "document": "Introduction\n\nHandling orthographic variation and misspellings is critical for robust NLP on social media. Prior work explored subword segmentation and byte-pair encoding to reduce OOV errors (Sennrich et al., 2016; Heinzerling and Strube, 2018). However, a previous study found that character-aware LSTMs outperform word-level models on noisy user-generated text. Building on this intuition, we propose a hybrid architecture that composes characters into subwords while preserving contextualized token representations.",
    "reason": "Claims a specific result from 'a previous study' without citing the study, which is a claim about prior work requiring a citation.",
    "start": 256,
    "end": 363,
    "label": "Unsupported claim"
  },
  {
    "span": "Predictive modeling of student performance has relied on logistic regression, gradient boosting, and deep sequence models applied to clickstream logs and assignment histories (Lu et al., 2018; Sweeney et al., 2016; Piech et al., 2015). Early-warning systems integrate risk scores with intervention scheduling (Xing et al., 2016; Jayaprakash et al., 2014). Representation learning approaches use autoencoders and graph embeddings to capture course relationships (Zheng et al., 2020; Ye et al., 2021).",
    "document": "Introduction\n\nLearning analytics seeks to understand and support student learning using data from online platforms, LMS interactions, and assessments (Siemens and Long, 2011). Accurate and timely prediction of at-risk students enables targeted interventions and resource allocation.\n\nPredictive modeling of student performance has relied on logistic regression, gradient boosting, and deep sequence models applied to clickstream logs and assignment histories (Lu et al., 2018; Sweeney et al., 2016; Piech et al., 2015). Early-warning systems integrate risk scores with intervention scheduling (Xing et al., 2016; Jayaprakash et al., 2014). Representation learning approaches use autoencoders and graph embeddings to capture course relationships (Zheng et al., 2020; Ye et al., 2021).\n\nRecent efforts emphasize fairness-aware modeling and interpretability to ensure equitable support and actionable insights for instructors and advisors (Baker and Hawn, 2021; Gardner et al., 2019).",
    "reason": "The span recites prior methods and applications without indicating the authors' perspective, gap, or how their work builds on or differs, per definition a and c.",
    "start": 284,
    "end": 783,
    "label": "Lacks synthesis"
  },
  {
    "span": "Prior work shows that identifier renaming accounts for most of the performance gains in neural code summarizers.",
    "document": "Related Work\n\nNeural models for source code summarization leverage both token sequences and structural representations such as ASTs and data-flow graphs (Allamanis et al., 2018; Alon et al., 2019; Ahmad et al., 2020). Pretrained encoder–decoders adapted to programming languages further boost results through large-scale code corpora (Feng et al., 2020; Guo et al., 2021). Prior work shows that identifier renaming accounts for most of the performance gains in neural code summarizers. More recently, contrastive objectives and copy mechanisms help align code tokens with natural language descriptions (Wei et al., 2020; Haque et al., 2022). We revisit evaluation by controlling for lexical memorization and variable naming artifacts.",
    "reason": "This is a specific claim about findings in prior literature (identifier renaming being the main source of gains) without any supporting citation; per the definition, niche/topic-specific claims require evidence.",
    "start": 373,
    "end": 485,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors show that mixing BM25 and dense retrievers consistently improves factuality.",
    "document": "Related Work\n\nRetrieval-augmented generation (RAG) has emerged as a powerful paradigm for grounding language models in external knowledge bases and document collections (Lewis et al., 2020; Guu et al., 2020; Izacard and Grave, 2021). Hybrid retrievers that combine sparse and dense signals have been explored for open-domain question answering and fact verification (Karpukhin et al., 2020; Xiong et al., 2021). Despite advances in retrieval quality, calibrating the generator to retrieved evidence remains a challenge (Shuster et al., 2021; Menick et al., 2022). In a previous study, the authors show that mixing BM25 and dense retrievers consistently improves factuality. Our work focuses on end-to-end training strategies that align the retriever and generator with factual objectives.",
    "reason": "Claims a result from a prior study without providing a citation to that study (definition ii and a).",
    "start": 564,
    "end": 673,
    "label": "Unsupported claim"
  },
  {
    "span": "According to industry reports, 68% of IoT devices remain unpatched after six months",
    "document": "Introduction\n\nThe proliferation of Internet-of-Things (IoT) devices has expanded the attack surface across consumer, enterprise, and critical infrastructure environments. Many devices operate with limited computational resources, lack secure update mechanisms, and are deployed in unmanaged settings, making them attractive targets for adversaries.\n\nPatch latency is a particularly acute problem. According to industry reports, 68% of IoT devices remain unpatched after six months, leaving known vulnerabilities exploitable long after disclosure. This maintenance gap exacerbates risks associated with default credentials, outdated firmware, and weak cryptographic protocols.\n\nIn response, we propose a lightweight attestation and update orchestration framework that combines periodic integrity checks with opportunistic, bandwidth-aware patch delivery. Our approach is designed for heterogeneous networks and leverages probabilistic scheduling to limit power consumption while maximizing risk reduction.",
    "reason": "This is a numerical statistic attributed to 'industry reports' without specifying or citing any source, which requires evidence.",
    "start": 397,
    "end": 480,
    "label": "Unsupported claim"
  },
  {
    "span": "Diffusion convolution captures directed spatial dynamics on road networks (Li et al., 2018). Temporal attention highlights salient intervals (Zheng et al., 2020). Graph WaveNet employs adaptive adjacency matrices (Wu et al., 2019). Transformers model long-range dependencies in time series (Zerveas et al., 2021).",
    "document": "Related Work\n\nTraffic forecasting models must capture spatial correlations across roads and temporal patterns under dynamic conditions. Graph neural networks and sequence models have become prevalent, with efforts to encode topology, adapt to non-stationarity, and scale to large sensor networks.\n\nDiffusion convolution captures directed spatial dynamics on road networks (Li et al., 2018). Temporal attention highlights salient intervals (Zheng et al., 2020). Graph WaveNet employs adaptive adjacency matrices (Wu et al., 2019). Transformers model long-range dependencies in time series (Zerveas et al., 2021).\n\nWe propose a unified architecture that learns topology-aware temporal kernels, bridging adaptive adjacency with multi-scale attention for improved multi-horizon forecasts.",
    "reason": "The span lists methods across spatial modeling and temporal modeling without transitions or an explicit explanation of their relationships, resulting in abrupt topic shifts.",
    "start": 298,
    "end": 611,
    "label": "Coherence"
  },
  {
    "span": "Koh et al. (2019) developed secure aggregation with modular compression. Patel and Wang (2021) investigated personalization layers for heterogeneous clients. Rivera et al. (2020) analyzed client sampling effects on convergence. Sun and Yu (2022) proposed adaptive clipping for robust training.",
    "document": "Related Work\n\nFederated learning (FL) enables on-device training with privacy preservation, but introduces new challenges in optimization, robustness, and personalization. Here we survey representative advances relevant to our method.\n\nKoh et al. (2019) developed secure aggregation with modular compression. Patel and Wang (2021) investigated personalization layers for heterogeneous clients. Rivera et al. (2020) analyzed client sampling effects on convergence. Sun and Yu (2022) proposed adaptive clipping for robust training.\n\nComplementary directions include communication-efficient variance reduction (Dai et al., 2023) and byzantine resilience (Huang and Rao, 2021). Our approach targets calibration-aware aggregation under heterogeneous label noise.",
    "reason": "The span enumerates unrelated subtopics (security, personalization, sampling, clipping) with no transitions or explicit relationships, leaving the reader unsure how these works connect or motivate the proposed method.",
    "start": 236,
    "end": 529,
    "label": "Coherence"
  },
  {
    "span": "The widely used Electricity and Traffic datasets exhibit strong daily and weekly seasonality and low noise.",
    "document": "Introduction\n\nMultivariate time series forecasting underpins applications in energy, transportation, and retail planning. Classical statistical models such as ARIMA and exponential smoothing remain competitive on short horizons (Hyndman and Athanasopoulos, 2018), while deep models capture complex nonlinear dependencies and cross-series correlations (Wen et al., 2017; Lim et al., 2021). Recent architectures leverage attention and graph structure to model long-range temporal patterns and spatial relations (Wu et al., 2019; Li et al., 2018).\n\nBenchmark selection and data properties strongly influence reported improvements. The widely used Electricity and Traffic datasets exhibit strong daily and weekly seasonality and low noise. In this work, we quantify inductive bias–dataset alignment and propose a simple seasonal decomposition preprocessor that improves generalization on datasets with weaker periodicity.\n\nWe provide extensive evaluations on Electricity, Traffic, Exchange, ETTh/ETTm, and Weather, with ablations on seasonality strength and input window lengths.",
    "reason": "This claim makes specific statements about properties of particular datasets without providing citations; niche, dataset-specific characteristics should be supported by references or evidence.",
    "start": 628,
    "end": 735,
    "label": "Unsupported claim"
  },
  {
    "span": "According to industry reports, 73% of retailers now deploy chatbots for customer support.",
    "document": "Introduction\n\nConversational agents are increasingly embedded in e-commerce workflows to handle pre-sale inquiries, product discovery, and post-purchase support. According to industry reports, 73% of retailers now deploy chatbots for customer support. Despite widespread adoption, effectiveness remains uneven due to domain drift and the long-tail nature of user queries. We present a retrieval-augmented dialogue system that grounds responses in up-to-date catalog and policy documents, reducing hallucinations and improving factuality.\n",
    "reason": "A specific statistic is asserted without citing a source, making the claim unsupported.",
    "start": 162,
    "end": 251,
    "label": "Unsupported claim"
  },
  {
    "span": "DPR remains the de facto baseline on NQ and TriviaQA.",
    "document": "Related Work\n\nOpen-domain question answering (ODQA) systems are commonly decomposed into a retriever and a reader (Chen et al., 2017). Dense retrieval techniques learn bi-encoder representations to overcome lexical mismatch problems typical of sparse methods like BM25 (Karpukhin et al., 2020; Xiong et al., 2021). Hard-negative mining, multi-vector encoders, and improved pretraining have further improved retrieval quality (Qu et al., 2021; Izacard and Grave, 2021). DPR remains the de facto baseline on NQ and TriviaQA. However, dense models are sensitive to domain shift and often require extensive negative sampling to remain competitive. Our approach addresses this by aligning dense representations with entity-centric signals using weak supervision from hyperlink structures.",
    "reason": "Claims a baseline status of a prior method across specific datasets without providing citations or evidence supporting the assertion.",
    "start": 469,
    "end": 522,
    "label": "Unsupported claim"
  },
  {
    "span": "Prior work in household manipulation mostly relies on privileged depth sensors",
    "document": "Introduction\n\nRobotic manipulation in unstructured homes requires robust perception and closed-loop control under occlusions and clutter. End-to-end visuomotor methods have shown promise by jointly optimizing feature extraction and control policies. However, brittle generalization and the cost of data collection remain significant barriers to practical deployment.\n\nPrior work in household manipulation mostly relies on privileged depth sensors, which may be unavailable or unreliable in reflective or sunlit environments. Consequently, policies trained with depth often degrade when transferred to RGB-only settings. To mitigate this gap, we explore representation learning that aligns depth-induced structure with RGB features during training while requiring only RGB at test time.\n\nOur contributions include a cross-modal consistency objective and a large-scale evaluation across diverse homes and tasks.",
    "reason": "This span makes a comparative claim about prior work's reliance on depth sensors without providing any citations to support the characterization of the literature.",
    "start": 368,
    "end": 446,
    "label": "Unsupported claim"
  },
  {
    "span": "ResNet-50 was used in a global chest X-ray screening competition to detect pneumonia",
    "document": "Related Work\n\nDeep convolutional architectures have become the dominant paradigm for thoracic disease detection from chest radiographs. Advances include attention to local lesions, multi-task learning across labels, and uncertainty-aware calibration. Transfer learning from large-scale natural image pretraining remains common when labeled radiographs are limited.\n\nResNet-50 was used in a global chest X-ray screening competition to detect pneumonia, and subsequent variants introduced squeeze-and-excitation and hybrid transformer backbones. Beyond architecture, training protocols such as heavy augmentation and label smoothing have shown improvements in sensitivity while maintaining specificity.\n\nOur approach focuses on test-time adaptation for domain shifts across hospitals without requiring access to source data.",
    "reason": "The mention of a specific competition setup and model usage lacks any citation to the competition or source (rule a: first mention of a competition/task requires citation).",
    "start": 366,
    "end": 450,
    "label": "Unsupported claim"
  },
  {
    "span": "Miller et al. (20)",
    "document": "Related Work\n\nLearning with partial labels has been studied through consistency regularization and pseudo-label refinement (Lee, 2013; Arazo et al., 2020). Following Miller et al. (20), we consider confidence thresholds adaptive to instance difficulty. Other approaches construct label graphs to propagate constraints (Veit et al., 2017; Rizve et al., 2021). We integrate adaptive thresholds with calibration-aware temperature scaling to improve reliability under long-tailed distributions (Kumar et al., 2019; Menon et al., 2021).",
    "reason": "Incomplete year in a narrative citation; should provide a full year like “(2020)”.",
    "start": 166,
    "end": 184,
    "label": "Format"
  },
  {
    "span": "A previous shared task reported that macro-F1 is a more reliable metric for this problem.",
    "document": "Related Work\n\nEvaluation metrics for imbalanced classification are a recurring challenge in toxicity and harmful content detection. Accuracy obscures minority-class performance, and ROC-AUC can be misleading under extreme skew. Researchers have proposed alternative summary metrics and calibration-aware assessments to better reflect utility in moderation pipelines.\n\nA previous shared task reported that macro-F1 is a more reliable metric for this problem. Despite its popularity, macro-F1 may still conflate thresholding with ranking quality and can be sensitive to label noise. Recent studies argue for cost-sensitive evaluation and per-subgroup breakdowns, highlighting disparities across demographic categories.\n\nOur study complements this line of work by introducing a decision-focused evaluation that incorporates asymmetric costs and abstention. We compare threshold-free ranking measures with operating-point-aware metrics to provide a comprehensive picture of performance in realistic deployment scenarios.",
    "reason": "Mentions a shared task and a specific conclusion without citing the task or providing a reference.",
    "start": 368,
    "end": 457,
    "label": "Unsupported claim"
  },
  {
    "span": "It is well known that persona consistency correlates with human judgments of coherence.",
    "document": "Related Work\n\nOpen-domain dialogue systems increasingly incorporate user or agent personas to promote consistent behavior and reduce contradictory responses. Approaches range from retrieval over persona-conditioned indexes to generative models that encode profile attributes in the context representation.\n\nEvaluation is challenging because automatic metrics often fail to capture pragmatic qualities. It is well known that persona consistency correlates with human judgments of coherence. Recent advances include contrastive objectives that penalize contradictions, but the effect sizes across domains and languages are not yet settled.",
    "reason": "Asserts a widely known empirical correlation without citing studies that establish it (rule b, e).",
    "start": 402,
    "end": 489,
    "label": "Unsupported claim"
  },
  {
    "span": "In (Kumar et al., 2019)",
    "document": "Related Work\n\nSemi-supervised learning has progressed along two main threads: graph-based label propagation and consistency regularization. Early graph methods exploit manifold structures to spread labels from a small set of seeds to unlabeled nodes (Zhu and Ghahramani, 2002; Chapelle et al., 2006). Consistency-based methods encourage invariance under perturbations using techniques such as Π-models and temporal ensembling (Laine and Aila, 2017) and later Mean Teacher (Tarvainen and Valpola, 2017). For text classification, variational objectives have been explored to leverage unlabeled data (Kingma et al., 2014), while pseudo-labeling remains a strong baseline (Lee, 2013).\n\nIn (Kumar et al., 2019), the authors propose a hybrid approach that combines graph smoothness with strong data augmentations for robust decision boundaries. More recent work considers curriculum schedules for unlabeled ratios (Sohn et al., 2020) and sharpness-aware training to stabilize consistency losses (Foret et al., 2021). Our method differs in that we adaptively allocate augmentations based on confidence calibration while maintaining a graph-aware regularizer for boundary refinement.",
    "reason": "Wrong citation style: a preposition is placed before a parenthetical citation. It should be narrative (e.g., \"In Kumar et al. (2019)\") or the preposition should be removed from before the parentheses.",
    "start": 682,
    "end": 705,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claim that attention-based GNNs outperform temporal CNNs under extreme sparsity.",
    "document": "Related Work\n\nTraffic forecasting models increasingly leverage spatiotemporal graphs to encode road network structure and temporal dynamics. Early approaches relied on temporal convolutions applied to grid-based representations, while later methods use graph neural networks (GNNs) to model non-Euclidean connectivity among sensors. In a previous study, the authors claim that attention-based GNNs outperform temporal CNNs under extreme sparsity. Despite promising reports, reproducing such results is complicated by inconsistent preprocessing and missing sensor metadata across datasets. Our work isolates the impact of sparsity by introducing a controlled masking protocol and evaluating multiple architectures under identical conditions.\n\nWe also review imputation-aware training strategies, where the model jointly predicts missing values and future traffic speeds. Although this joint objective can improve robustness, it risks overfitting to imputation artifacts without proper regularization. Our proposed benchmark includes standardized imputations and public code to facilitate consistent comparisons.",
    "reason": "References a 'previous study' and its claim without providing a citation (criterion b/ii).",
    "start": 333,
    "end": 446,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES task trained on TOEFL essays.",
    "document": "Related Work\n\nAutomated essay scoring has evolved from handcrafted features to neural encoders and pretraining. BERT was used in an AES task trained on TOEFL essays. Subsequent studies examined domain transfer and robustness under adversarial paraphrasing, but cross-prompt generalization remains challenging.",
    "reason": "Claims a specific model-dataset setup in prior work without providing a citation to that work.",
    "start": 112,
    "end": 165,
    "label": "Unsupported claim"
  },
  {
    "span": "Message passing neural networks aggregate neighbor features to form node representations (Gilmer et al., 2017). Quantum-chemical methods approximate electronic structure to compute energies (Perdew et al., 1996). Pretraining on large molecular corpora improves downstream property prediction (Hu et al., 2020).",
    "document": "Related Work\n\nMolecular representation learning has advanced through the use of graph neural networks (GNNs) that encode atoms and bonds in a relational structure (Duvenaud et al., 2015; Kearnes et al., 2016). Modern models incorporate geometric information to better capture stereochemistry and non-bonded interactions (Schütt et al., 2017; Satorras et al., 2021). Data scarcity remains a key limitation for many property prediction tasks, motivating transfer and self-supervised learning strategies (Rong et al., 2020; Wang et al., 2022).\n\nMessage passing neural networks aggregate neighbor features to form node representations (Gilmer et al., 2017). Quantum-chemical methods approximate electronic structure to compute energies (Perdew et al., 1996). Pretraining on large molecular corpora improves downstream property prediction (Hu et al., 2020). Recent benchmarks highlight the need for models that are both data-efficient and physically faithful (Wu et al., 2018; Morris et al., 2020).\n\nWe situate our approach at the intersection of structure-aware GNNs and physics-inspired constraints, aiming to leverage unlabeled corpora while maintaining consistency with fundamental chemical principles.",
    "reason": "The span abruptly jumps from neural GNN methods to ab initio quantum chemistry and then to pretraining without articulating how these areas relate, causing unclear connections across sentences.",
    "start": 542,
    "end": 852,
    "label": "Coherence"
  },
  {
    "span": "more recent surveys argue that BLEU is unsuitable for dialog evaluation",
    "document": "Related Work\n\nEvaluating open-domain dialog remains an open challenge. N-gram overlap metrics such as BLEU and ROUGE correlate weakly with human judgments, especially when multiple valid responses exist. More recent surveys argue that BLEU is unsuitable for dialog evaluation, motivating learned metrics that incorporate semantics, persona consistency, and safety considerations.\n\nParallel to metric development, response generation has progressed from retrieval-based systems to large pretrained decoders with controllable attributes. Our work complements these efforts by proposing a reference-free metric that aligns with human preferences through pairwise ranking signals.",
    "reason": "Refers to 'recent surveys' making a specific evaluative claim but provides no citations to those surveys.",
    "start": -1,
    "end": -1,
    "label": "Unsupported claim"
  },
  {
    "span": "Back-translation leverages monolingual corpora to augment training (Sennrich et al., 2016). Multilingual pretraining transfers knowledge across related languages (Conneau and Lample, 2019). Unsupervised BPE segmentation mitigates OOV issues (Sennrich et al., 2015).",
    "document": "Introduction\n\nLow-resource machine translation (MT) strives to deliver acceptable quality under scarce parallel data. Strategies typically exploit monolingual text, cross-lingual transfer, and subword modeling to alleviate data sparsity.\n\nBack-translation leverages monolingual corpora to augment training (Sennrich et al., 2016). Multilingual pretraining transfers knowledge across related languages (Conneau and Lample, 2019). Unsupervised BPE segmentation mitigates OOV issues (Sennrich et al., 2015). We propose a curriculum that schedules synthetic data and multilingual signals under domain shift constraints.\n",
    "reason": "The cited works are listed without transitions or explanation of interdependencies, causing an abrupt shift from back-translation to multilingual pretraining to BPE.",
    "start": 239,
    "end": 504,
    "label": "Coherence"
  },
  {
    "span": "Kim and Park (2019",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative training without centralizing raw data, reducing privacy risks while maintaining model utility (McMahan et al., 2017). Subsequent surveys have systematized challenges in communication efficiency, personalization, and robustness (Kairouz et al., 2021; Li et al., 2020). Privacy-preserving techniques such as secure aggregation and differential privacy are often combined with FL to mitigate information leakage (Bonawitz et al., 2017; Abadi et al., 2016).\n\nOptimization under client heterogeneity has been widely studied. FedProx introduces a proximal term to stabilize updates across non-IID clients (Li et al., 2020), while adaptive methods focus on variance reduction and client sampling strategies (Karimireddy et al., 2020). Personalization approaches leverage meta-learning or mixture-of-experts to accommodate diverse user distributions (Smith et al., 2017; Arivazhagan et al., 2019).\n\nIn mobile text prediction, compression and partial participation are crucial to meet device constraints (Chen et al., 2019; Reddi et al., 2020). Kim and Park (2019 propose a hierarchical aggregator for straggler mitigation, reporting improved convergence on skewed workloads. Our work builds on these insights by introducing a communication-aware objective that adapts to client availability patterns.\n",
    "reason": "Missing closing parenthesis in the narrative citation; it should be 'Kim and Park (2019)' with a closing bracket.",
    "start": 1097,
    "end": 1115,
    "label": "Format"
  },
  {
    "span": "(Johnson 2020)",
    "document": "Introduction\n\nText classification has benefited from pretrained encoders that capture contextual semantics (Devlin et al., 2019; Liu et al., 2019). However, domain-specific nuances often require adaptation strategies (Gururangan et al., 2020). A comprehensive survey (Johnson 2020) argues that task-adaptive pretraining is particularly effective when label spaces are fine-grained.",
    "reason": "Missing comma between author and year in the parenthetical citation; it should be '(Johnson, 2020)'.",
    "start": 267,
    "end": 281,
    "label": "Format"
  },
  {
    "span": "The Kinetics-700 dataset contains more than 700,000 labeled clips sampled uniformly across classes.",
    "document": "Introduction\n\nVideo action recognition has progressed from 2D convolutional baselines to 3D CNNs and transformer-based architectures that capture long-range temporal dependencies. Benchmarks such as UCF101 and HMDB51 catalyzed early gains and later gave way to large-scale datasets that support training deeper models. Self-supervised pretraining on uncurated videos has further improved backbone representations for downstream tasks including temporal localization and action detection.\n\nThe Kinetics-700 dataset contains more than 700,000 labeled clips sampled uniformly across classes. Models trained on large-scale curated datasets are often transferred to smaller benchmarks via fine-tuning, though domain shifts in camera motion, scene composition, and label granularity can degrade performance. Recent work explores multi-modal cues such as audio and optical flow to complement RGB inputs and improve robustness.",
    "reason": "This sentence states specific dataset statistics without citing the dataset; first mentions of datasets and their characteristics require citations (rule a).",
    "start": 489,
    "end": 588,
    "label": "Unsupported claim"
  },
  {
    "span": "Most clinical NER datasets underrepresent social determinants of health.",
    "document": "Related Work\n\nClinical named entity recognition (NER) enables downstream phenotyping and cohort discovery by extracting medications, problems, and procedures from narratives (Uzuner et al., 2011; Wang et al., 2018). Transformer-based models pretrained on biomedical and clinical corpora currently achieve state-of-the-art results across multiple benchmarks (Lee et al., 2020; Alsentzer et al., 2019). Dataset composition, label schema, and annotation guidelines critically influence model generalization (Pradhan et al., 2014).\n\nMost clinical NER datasets underrepresent social determinants of health. This limitation complicates equitable evaluation and may bias deployed systems against underserved populations. We curate a new benchmark emphasizing socioeconomic context and evaluate mitigation strategies via domain-adaptive pretraining.",
    "reason": "The dataset coverage claim should be supported by citations to specific datasets or analyses; no references are given.",
    "start": 529,
    "end": 601,
    "label": "Unsupported claim"
  },
  {
    "span": "The LibriSpeech test-other set remains the most commonly reported benchmark for English ASR under noisy conditions.",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) has progressed through stronger encoder architectures, data augmentation, and self-supervised pretraining. Evaluation typically considers both clean and challenging acoustic environments to assess robustness.\n\nThe LibriSpeech test-other set remains the most commonly reported benchmark for English ASR under noisy conditions. Beyond this, researchers often include domain-shifted corpora and spontaneous speech to stress-test generalization.\n\nRecent advances leverage conformer encoders, multi-condition training, and pseudo-labeling from large unlabeled audio. We investigate the interaction between noise-robust features and shallow fusion decoding with domain-adaptive language models.",
    "reason": "States a field-wide usage claim about a specific benchmark without providing citations; dataset mentions at first occurrence also require citation.",
    "start": 270,
    "end": 385,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an automated essay scoring task trained on thousands of middle school essays.",
    "document": "Introduction\n\nAutomated essay scoring (AES) seeks to predict holistic or trait-specific writing quality from student compositions. Neural approaches have reduced feature engineering by learning rich contextual representations directly from text. BERT was used in an automated essay scoring task trained on thousands of middle school essays. More recent methods augment text encoders with discourse and coherence signals to better capture global structure, but they often require rubric-specific annotations.\n\nWe propose a rubric-agnostic scoring framework that aligns latent discourse cues with holistic quality through contrastive pretraining on weakly labeled revisions. Our experiments on multiple educational datasets demonstrate consistent improvements over strong encoder-only baselines while maintaining interpretability through alignment probes.",
    "reason": "Describes a specific prior setup (BERT applied to AES on a particular dataset scale) without citing the work.",
    "start": 246,
    "end": 340,
    "label": "Unsupported claim"
  },
  {
    "span": "To the best of our knowledge, this is the first end-to-end ASR model evaluated on the SEAME 2.0 test set using byte-level BPE.",
    "document": "Related Work\n\nCode-switching automatic speech recognition (ASR) poses challenges due to intertwined phonotactics, pronunciation variants, and language identification at the frame level. Hybrid HMM-DNN systems have been complemented by end-to-end approaches that jointly learn acoustic and language modeling with subword units. Multilingual pretraining and lexicon-free decoding have reduced reliance on hand-crafted resources and improved performance in mixed-language conditions.\n\nTo the best of our knowledge, this is the first end-to-end ASR model evaluated on the SEAME 2.0 test set using byte-level BPE. Prior efforts incorporated auxiliary language identification losses and shared subword vocabularies to encourage cross-lingual transfer, while data augmentation with speed perturbation and noise injection has proven beneficial for robustness. Nevertheless, strong baseline comparisons on standard code-switching benchmarks remain limited.",
    "reason": "This sentence claims novelty ('first') and mentions a specific dataset at first use without any citation; both the novelty claim and the dataset mention require supporting references (rule a and e).",
    "start": 482,
    "end": 608,
    "label": "Unsupported claim"
  },
  {
    "span": "The COCO-Lite dataset has become the de facto benchmark for few-shot detection.",
    "document": "Related Work\n\nFew-Shot Object Detection\n\nFew-shot detection aims to recognize novel categories with limited annotations, often by transferring knowledge from base classes. Meta-learning approaches learn class-agnostic detectors that can adapt quickly to new categories (Kang et al., 2019; Yan et al., 2019), while metric-learning methods exploit embedding spaces to support prototype-based classification at the region level (Sung et al., 2018). The COCO-Lite dataset has become the de facto benchmark for few-shot detection. Augmentation strategies and pseudo-labeling have improved low-shot performance, but their impact depends heavily on base-class diversity (Wang et al., 2020). More recently, transformer-based detectors have been adapted to the few-shot regime using prompt-like conditioning on support examples (Zhu et al., 2021).",
    "reason": "Introduces a specific dataset and a benchmarking claim without any citation at its first mention, violating rule a and requiring evidence.",
    "start": 446,
    "end": 525,
    "label": "Unsupported claim"
  },
  {
    "span": "To the best of our knowledge, no prior work has addressed incremental normalization in streaming news.",
    "document": "Related Work\n\nEntity normalization maps textual mentions to canonical identifiers in a knowledge base. Traditional approaches rely on candidate generation followed by ranking with contextual features or neural encoders (Leaman et al., 2013; Gillick et al., 2019). Recent methods integrate mention detection, linking, and normalization in end-to-end frameworks with pre-trained language models (Wu et al., 2020; Onoe et al., 2021).\n\nStreaming settings introduce latency constraints and evolving knowledge bases, which render batch-oriented inference impractical. To the best of our knowledge, no prior work has addressed incremental normalization in streaming news. Closest lines of research consider online entity linking with periodic index refreshes or approximate nearest neighbor search for dynamic catalogs (Raiman and Raiman, 2018; Logeswaran et al., 2019). We formalize incremental normalization and present a low-latency update mechanism that amortizes index maintenance over the stream.\n\nOur evaluation uses timestamped newswire corpora with controlled KB updates, measuring both accuracy and end-to-end latency under budget constraints.",
    "reason": "Asserts novelty by claiming the absence of prior work without providing citations to substantiate a comprehensive survey or boundary of related efforts (rule b/a: claim about prior work requires evidence).",
    "start": 562,
    "end": 664,
    "label": "Unsupported claim"
  },
  {
    "span": "the MATH-QA-Plus dataset is the largest public resource for multi-step math word problems",
    "document": "Introduction\n\nReasoning over math word problems requires models to parse narrative text and generate multi-step symbolic solutions. Existing benchmarks differ in problem difficulty, annotation quality, and solution form. In our study, we evaluate models across diverse arithmetic operations and compositional templates. To standardize evaluation, the MATH-QA-Plus dataset is the largest public resource for multi-step math word problems and offers fine-grained rationales for each step.\n\nWe propose a chain-of-thought constrained decoder that integrates solution sketches with symbolic execution. Our contributions include a new inference-time verifier and an error taxonomy covering unit mismatch and operation ordering. Empirical results show consistent gains across varying reasoning depths.",
    "reason": "First mention of a specific dataset and a quantitative claim ('largest public resource') are made without any citation.",
    "start": 347,
    "end": 436,
    "label": "Unsupported claim"
  },
  {
    "span": "There are many recent works that explore multilingual intent detection.",
    "document": "Related Work\n\nIntent detection has evolved from keyword-based rules to deep neural architectures that model semantic representations of user utterances. Early approaches emphasized monolingual settings and relied on feature engineering. Subsequent neural models leveraged distributed representations and sequence encoders to improve robustness across domains. There are many recent works that explore multilingual intent detection. However, most existing methods assume access to balanced and parallel data across languages, which rarely holds in practical deployments. In contrast, our approach targets imbalanced corpora and leverages cross-lingual supervision transfer without relying on exact parallelism.\n\nWithin transfer learning, prior studies have examined zero-shot cross-lingual transfer and language-adaptive fine-tuning, yet these strategies often fail when label distributions drift across locales. Our framework focuses on label-shift resilience and introduces a calibration module that corrects for cross-market differences in intent frequency.",
    "reason": "The sentence claims the existence of 'many recent works' but provides no citations to support the claim, violating rule (d) that mentions of recent works should be backed up with citations.",
    "start": 360,
    "end": 431,
    "label": "Unsupported claim"
  },
  {
    "span": "[Singh, 2020]",
    "document": "Related Work\n\nRecommendation systems have evolved from memory-based approaches to latent factor models and, more recently, deep neural architectures (Koren et al., 2009; Rendle, 2010; He et al., 2017). Context-aware recommenders incorporate temporal and situational signals to better capture user intent (Adomavicius and Tuzhilin, 2015). Session-based models leverage sequential dependencies via RNNs and attention mechanisms (Hidasi et al., 2016; Li et al., 2017).\n\nGraph-based collaborative filtering exploits user–item bipartite structures to propagate preferences (Ying et al., 2018; Wang et al., 2019). Cold-start solutions integrate side information such as text and images, aligning representations across modalities (Zhang et al., 2016; Chen et al., 2019). Recent surveys [Singh, 2020] summarize these trends and highlight open challenges in fairness and robustness.\n\nIn this work, we propose a debiased graph contrastive framework that explicitly models exposure bias while maintaining recommendation accuracy under sparse feedback.",
    "reason": "Wrong bracket style for an author–year citation. The surrounding style uses parentheses; this should be “(Singh, 2020)” instead of square brackets.",
    "start": 780,
    "end": 793,
    "label": "Format"
  },
  {
    "span": "The MIMIC-CXR dataset has over 500,000 labeled reports in its latest release.",
    "document": "Introduction\n\nAutomating radiology report generation aims to translate chest X-ray images into clinically meaningful text, reducing reporting burden and improving consistency (Liu et al., 2019; Jing et al., 2018). Several large-scale datasets have enabled progress, including corpora of de-identified hospital records and public benchmarks for captioning medical images (Shin et al., 2016; Demner-Fushman et al., 2016). The MIMIC-CXR dataset has over 500,000 labeled reports in its latest release. Despite dataset scale, capturing rare findings and long-tailed terminology remains challenging (Zhang et al., 2020), motivating retrieval-augmented and report-grammar-aware models.\n",
    "reason": "Provides a specific dataset statistic without any citation; per (a) and (b) such claims about datasets and their size require a supporting reference.",
    "start": 420,
    "end": 497,
    "label": "Unsupported claim"
  },
  {
    "span": "Smith et al.",
    "document": "Related Work\n\nSelf-supervised learning has emerged as a powerful paradigm for representation learning across modalities (Jing and Tian, 2020; Grill et al., 2020). Following Smith et al., we adopt masked token prediction as a pretraining objective to encourage contextual understanding of inputs. Several improvements build on this by adding span corruption and permutation-based masking (Jones and Patel, 2020; Wang et al., 2021).\n\nBeyond objectives, architectural choices such as deeper normalization and better initialization have been shown to stabilize training (Ba et al., 2016; Xiong et al., 2020). Data diversity and curriculum strategies further improve robustness and transferability (Kumar et al., 2019; Sukhbaatar et al., 2018).",
    "reason": "Narrative citation is missing the publication year; should be 'Smith et al. (YEAR)', e.g., 'Smith et al. (2019)'.",
    "start": 173,
    "end": 185,
    "label": "Format"
  },
  {
    "span": "The winning solution of the 2019 Kidney Tumor Segmentation Challenge used a cascaded 3D U-Net with test-time augmentation.",
    "document": "Related Work\n\nMedical image segmentation has been dominated by U-Net style architectures and their 3D variants, with attention and residual connections improving boundary delineation (Ronneberger et al., 2015; Çiçek et al., 2016; Isensee et al., 2018). Self-ensembling and pseudo-labeling have further advanced semi-supervised setups, particularly for organ and lesion delineation (Bortsova et al., 2019). The winning solution of the 2019 Kidney Tumor Segmentation Challenge used a cascaded 3D U-Net with test-time augmentation. Recent studies leverage nnU-Net style automated configuration to standardize pre- and post-processing across datasets (Isensee et al., 2021). We adopt a curriculum-based consistency regularization scheme tailored to class imbalance typical in renal tumor datasets.\n",
    "reason": "Mentions a shared task winner and specific methodological details without citation, violating rule (a).",
    "start": 406,
    "end": 528,
    "label": "Unsupported claim"
  },
  {
    "span": "It is widely known that dynamic analysis is prohibitively expensive for large-scale Android malware screening.",
    "document": "Introduction\n\nAndroid malware detection commonly employs static analysis of APKs to extract permissions, API calls, and control-flow features (Arp et al., 2014; Li et al., 2018). Dynamic analysis captures runtime behaviors but requires sandboxing and instrumentation that limit throughput and scalability. Hybrid methods attempt to combine the strengths of both paradigms (Wei et al., 2017). It is widely known that dynamic analysis is prohibitively expensive for large-scale Android malware screening. This motivates learning static proxies that approximate behavioral signals without incurring runtime overhead.\n\nWe propose a contrastive pretraining framework on API sequence graphs to distill behaviorally salient patterns from a small set of instrumented apps into a scalable static detector.",
    "reason": "Makes a field-specific general claim about costs without supporting evidence or citations, which should be provided.",
    "start": 392,
    "end": 502,
    "label": "Unsupported claim"
  },
  {
    "span": "There are many recent works that explore this topic.",
    "document": "Related Work: Evaluation of Open-Domain Dialogue\n\nEvaluating open-domain dialogue remains challenging because surface metrics often fail to capture coherence, engagement, and user satisfaction. A number of approaches attempt to approximate human judgment using reference-free embeddings, next-utterance prediction, or quality estimation signals (See et al., 2019; Mehri and Eskenazi, 2020).\n\nThere are many recent works that explore this topic. Nevertheless, methodological inconsistencies across datasets, scales, and annotator expertise complicate head-to-head comparison. We build on this literature by standardizing prompts and reporting agreement-adjusted correlations with human ratings across multiple domains.",
    "reason": "Generic reference to 'many recent works' must be supported by citations according to the definition; none are given.",
    "start": 392,
    "end": 444,
    "label": "Unsupported claim"
  },
  {
    "span": "(Nguyen et al., 2018",
    "document": "Related Work\n\nNeural machine translation (NMT) shifted the paradigm from phrase-based models to end-to-end learning with sequence-to-sequence architectures (Sutskever et al., 2014; Bahdanau et al., 2015). The Transformer further improved translation quality by replacing recurrence with multi-head attention (Vaswani et al., 2017). Data augmentation via back-translation continues to be a simple yet powerful approach for leveraging monolingual corpora (Sennrich et al., 2016).\n\nLow-resource NMT has been tackled through transfer learning, multilingual training, and vocabulary sharing (Johnson et al., 2017; Zoph et al., 2016). More recent work explores unsupervised objectives and cross-lingual pretraining to reduce reliance on parallel data (Lample et al., 2018; Conneau and Lample, 2019). Semi-supervised consistency training has also been proposed to stabilize training under limited supervision (Nguyen et al., 2018 across diverse language pairs). Our method integrates consistency regularization with a curriculum on synthetic data quality to further improve robustness.",
    "reason": "Missing closing parenthesis in a parenthetical citation. It should be \"(Nguyen et al., 2018)\".",
    "start": 902,
    "end": 922,
    "label": "Format"
  },
  {
    "span": "Zhang et al. 1",
    "document": "Related Work\n\nKnowledge graph completion methods range from translational distance models (Bordes et al., 2013) to tensor factorization and neural link predictors (Trouillon et al., 2016; Sun et al., 2019). Building on path-based reasoning, Zhang et al. 1 proposed a reinforcement learning approach to discover multi-hop explanations. Recent work integrates textual evidence to enrich relation representations (Xiong et al., 2018), yet explanation faithfulness and scalability remain open problems.",
    "reason": "Improper use of a footnote marker ('1') directly after an author–year reference; it should include a year or be formatted as a proper footnote, e.g., 'Zhang et al. (2018)' or a superscript footnote with a corresponding note.",
    "start": 241,
    "end": 255,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on essays from three public corpora to great effect.",
    "document": "Related Work\n\nAutomated Essay Scoring (AES) aims to predict holistic and trait scores of student writing, supporting large-scale assessment and formative feedback (Shermis and Burstein, 2013; Taghipour and Ng, 2016). Neural models have shifted from feature-engineered pipelines to pre-trained language models fine-tuned on essay corpora (Devlin et al., 2019; Rodriguez et al., 2021). BERT was used in an AES task trained on essays from three public corpora to great effect. However, cross-prompt generalization and robustness to adversarial edits remain open challenges (Mayfield and Black, 2020; Ke and Ng, 2019).\n\nRecent studies highlight the importance of prompt-aware adapters and calibration for score distributions (Uto et al., 2020; Huang et al., 2022). We extend this direction by introducing prompt-conditioned normalization and uncertainty-aware loss functions to better capture rater variability.",
    "reason": "It mentions a specific prior setup (BERT on three public AES corpora) without citing the study that used this configuration.",
    "start": 384,
    "end": 473,
    "label": "Unsupported claim"
  },
  {
    "span": "Multimodal emotion recognition has leveraged early fusion of handcrafted features (Zeng et al., 2009), intermediate fusion with attention across modalities (Zadeh et al., 2018; Tsai et al., 2019), and late fusion ensembles that combine modality-specific predictors (Poria et al., 2017). Alignment techniques such as cross-modal transformers and CTC-based synchronization have also been explored (Pham et al., 2020; Mai et al., 2021).",
    "document": "Related Work\n\nAutomatically inferring affect from audio, video, and text requires integrating heterogeneous signals that differ in sampling rate, noise profile, and semantic granularity.\n\nMultimodal emotion recognition has leveraged early fusion of handcrafted features (Zeng et al., 2009), intermediate fusion with attention across modalities (Zadeh et al., 2018; Tsai et al., 2019), and late fusion ensembles that combine modality-specific predictors (Poria et al., 2017). Alignment techniques such as cross-modal transformers and CTC-based synchronization have also been explored (Pham et al., 2020; Mai et al., 2021).\n\nIn contrast to these approaches, our study emphasizes test-time adaptation with modality dropout to handle sensor failures.\n",
    "reason": "This span summarizes related techniques without connecting them to the authors' goals, perspective, or highlighting limitations to motivate their method, meeting (a) and (c).",
    "start": 188,
    "end": 621,
    "label": "Lacks synthesis"
  },
  {
    "span": "Shermis and Burstein (2013) survey AES systems in standardized testing. Taghipour and Ng (2016) model essays with CNN-LSTM hybrids. Farag et al. (2018) explore prompt-specific scoring with domain adaptation. Beigman Klebanov et al. (2014) examine topical relevance features.",
    "document": "Related Work\n\nAutomated essay scoring (AES) research covers holistic scoring, trait-specific rubrics, and fairness considerations across prompts and demographics. Neural models have improved predictive accuracy but often struggle with generalization across prompts and spurious correlations.\n\nShermis and Burstein (2013) survey AES systems in standardized testing. Taghipour and Ng (2016) model essays with CNN-LSTM hybrids. Farag et al. (2018) explore prompt-specific scoring with domain adaptation. Beigman Klebanov et al. (2014) examine topical relevance features.\n\nWe address cross-prompt robustness by combining content-preserving data augmentation with causal regularization that penalizes topic leakage from surface cues.",
    "reason": "The four citations are presented as isolated sentences without transitions or explicit connections, leaving unclear how survey work, neural architectures, domain adaptation, and feature analyses relate. This reduces coherence across multiple sentences (criterion a, b, c).",
    "start": 293,
    "end": 567,
    "label": "Coherence"
  },
  {
    "span": "Prior benchmarks report that adding a pronunciation lexicon reduces WER by 15% relative on average.",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) has achieved strong results with CTC and attention-based models (Graves et al., 2006; Chan et al., 2016). For low-resource languages, transfer learning and multilingual training are common strategies (Kunze et al., 2017; He et al., 2020). Prior benchmarks report that adding a pronunciation lexicon reduces WER by 15% relative on average. However, lexicon construction is costly, prompting interest in grapheme-based models and subword units (Schwarz et al., 2019; Pratap et al., 2020). We study when lexica help under varying amounts of paired data.",
    "reason": "Presents a quantitative statistic attributed to benchmarks without citing any source (rule b: statistics require evidence).",
    "start": 299,
    "end": 398,
    "label": "Unsupported claim"
  },
  {
    "span": "Heuristic schedulers minimize peak power via DVFS and consolidation (Beloglazov and Buyya, 2012; Mishra et al., 2010). Control-theoretic methods regulate cooling and IT loads (Wang et al., 2011; Gandhi et al., 2010). Reinforcement learning has been applied to thermal-aware and energy-proportional scheduling (Zhang et al., 2020; Mao et al., 2016).",
    "document": "Related Work\n\nData center energy optimization spans compute, cooling, and network subsystems. Practical schedulers must jointly manage performance, thermal constraints, and service-level objectives under nonstationary workloads.\n\nHeuristic schedulers minimize peak power via DVFS and consolidation (Beloglazov and Buyya, 2012; Mishra et al., 2010). Control-theoretic methods regulate cooling and IT loads (Wang et al., 2011; Gandhi et al., 2010). Reinforcement learning has been applied to thermal-aware and energy-proportional scheduling (Zhang et al., 2020; Mao et al., 2016).\n\nOur work examines generalization across workload regimes by learning proxy objectives aligned with energy-delay product, enabling transfer without per-cluster retuning.",
    "reason": "The span merely enumerates categories of approaches and citations without articulating how they inform or contrast with the present work; no gap or perspective is provided (criteria a and c).",
    "start": 230,
    "end": 578,
    "label": "Lacks synthesis"
  },
  {
    "span": "Duarte and Kim, (2020)",
    "document": "Related Work\n\nEvaluation of natural language generation has moved from lexical overlap metrics to embedding-based and learned evaluators (Papineni et al., 2002; Zhang et al., 2020). However, learned scorers can be biased toward surface patterns (Kryscinski et al., 2020). Reference-free metrics leverage pretrained LMs to assess consistency and faithfulness (Yuan et al., 2021). As noted by Duarte and Kim, (2020) current datasets underrepresent long-form reasoning, leading to inflated metric correlations. We expand prior benchmarks with discourse-annotated summaries and measure robustness to paraphrase and negation (Maynez et al., 2020; Durmus et al., 2020). Our method calibrates metric scores via isotonic regression to improve agreement with human judgments.",
    "reason": "Wrong punctuation in a narrative citation: comma before a parenthetical year. It should be Duarte and Kim (2020) without the comma.",
    "start": 391,
    "end": 413,
    "label": "Format"
  },
  {
    "span": "There are many recent works that explore structured pruning for Transformers.",
    "document": "Introduction\n\nTransformers have become the backbone of modern NLP and vision systems, delivering strong results across translation, language modeling, and image recognition (Vaswani et al., 2017; Dosovitskiy et al., 2021). The push to deploy such models on resource-constrained devices has spurred interest in compression via pruning and quantization (Han et al., 2016; Jacob et al., 2018).\n\nDespite extensive progress in unstructured sparsification, the multi-head attention and deep feed-forward stacks in Transformers make structured compression particularly challenging. There are many recent works that explore structured pruning for Transformers. These methods target entire heads, MLP channels, or attention blocks, often using magnitude heuristics or sparsity-inducing regularizers.\n\nHowever, structured pruning can interact with pretraining dynamics in subtle ways, potentially harming transfer. In this paper, we propose a criterion that jointly scores head utility and inter-layer redundancy, enabling safe removal without aggressive retraining. We validate our approach on GLUE and ImageNet transfer settings and analyze robustness to domain shift.",
    "reason": "Mentions 'many recent works' without citing any of them; violates rule d) requiring citations for claims about recent works.",
    "start": 575,
    "end": 652,
    "label": "Unsupported claim"
  },
  {
    "span": "Nguyen 2019",
    "document": "Related Work\n\nData valuation methods estimate the contribution of each training point to model performance using influence functions, Shapley values, or gradient-based approximations (Koh and Liang, 2017; Ghorbani and Zou, 2019). Prior studies compared these paradigms across tasks and datasets (Nguyen 2019; Kumar et al., 2020; Lee and Park, 2021), showing that approximation quality depends on curvature and data heterogeneity. Our work complements these analyses by introducing a curvature-aware surrogate that scales to large models without prohibitive Hessian computations (Wang and Chen, 2022).",
    "reason": "Incorrect parenthetical citation formatting within a list: missing comma between author and year; should be “Nguyen, 2019”.",
    "start": 296,
    "end": 307,
    "label": "Format"
  },
  {
    "span": "Transformer-based architectures have been applied to radiology classification and segmentation with promising results (Dosovitskiy et al., 2021; Chen et al., 2021). Hybrid CNN-Transformer models aim to balance local inductive biases with global context (Graham et al., 2021; Liu et al., 2021). Self-supervised pretraining on unlabeled scans improves downstream efficiency (Zhou et al., 2021; He et al., 2020). Multi-scale tokens and windowed attention reduce computational cost while preserving resolution detail (Srinivas et al., 2021; Wang et al., 2021).",
    "document": "Introduction\nMedical image analysis benefits from both fine-grained local features and long-range contextual reasoning. Transformers, with their global receptive fields, have recently been explored as alternatives or complements to convolutional networks for classification, segmentation, and detection in radiology and pathology.\n\nTransformer-based architectures have been applied to radiology classification and segmentation with promising results (Dosovitskiy et al., 2021; Chen et al., 2021). Hybrid CNN-Transformer models aim to balance local inductive biases with global context (Graham et al., 2021; Liu et al., 2021). Self-supervised pretraining on unlabeled scans improves downstream efficiency (Zhou et al., 2021; He et al., 2020). Multi-scale tokens and windowed attention reduce computational cost while preserving resolution detail (Srinivas et al., 2021; Wang et al., 2021).\n\nHowever, domain shifts across scanners, institutions, and protocols can degrade performance. We introduce a scanner-conditional vision transformer that modulates attention using acquisition metadata during both pretraining and finetuning, improving robustness under cross-site generalization on chest CT and brain MRI.",
    "reason": "This paragraph summarizes prior work without linking it to a specific gap or clarifying how the proposed approach differs; it provides no explicit author perspective.",
    "start": 332,
    "end": 888,
    "label": "Lacks synthesis"
  },
  {
    "span": "Cooperative multi-agent reinforcement learning has progressed through value factorization (VDN, QMIX), mean-field approximations, and centralized training with decentralized execution (Sunehag et al., 2018; Rashid et al., 2018; Yang et al., 2018; Lowe et al., 2017). We propose a lightweight message-passing protocol for coordination under bandwidth limits.",
    "document": "Introduction\n\nLearning to Coordinate in Multi-Agent Systems\n\nEffective coordination among agents with partial observability and limited communication remains a core challenge in multi-agent reinforcement learning. Practical applications include traffic control, resource allocation, and multi-robot systems where scalability and robustness are critical.\n\nCooperative multi-agent reinforcement learning has progressed through value factorization (VDN, QMIX), mean-field approximations, and centralized training with decentralized execution (Sunehag et al., 2018; Rashid et al., 2018; Yang et al., 2018; Lowe et al., 2017). We propose a lightweight message-passing protocol for coordination under bandwidth limits.\n\nWe evaluate across matrix games, particle environments, and grid worlds, emphasizing sample efficiency and compatibility with off-the-shelf learners.",
    "reason": "After citing prior methods, the authors introduce their protocol without explaining what limitation in existing approaches motivates it, thus lacking a clear synthesis or gap statement.",
    "start": 355,
    "end": 712,
    "label": "Lacks synthesis"
  },
  {
    "span": "Causal discovery in time series has drawn on Granger causality and VAR modeling (Granger, 1969; Hamilton, 1994), constraint-based methods with conditional independence tests (Spirtes et al., 2000; Runge et al., 2019), and continuous optimization of acyclicity constraints (Zheng et al., 2018; Pamfil et al., 2020). Recent extensions address nonlinearity and latent confounders (Peters et al., 2017; Löwe et al., 2022). We propose a differentiable score for discovering causal structure with latent confounding in nonstationary multivariate series.",
    "document": "Introduction\n\nUnderstanding causal structure in multivariate time series is essential for reliable forecasting, policy design, and scientific inference. Observational data often involve feedback loops, delays, and potential confounding, complicating discovery.\n\nCausal discovery in time series has drawn on Granger causality and VAR modeling (Granger, 1969; Hamilton, 1994), constraint-based methods with conditional independence tests (Spirtes et al., 2000; Runge et al., 2019), and continuous optimization of acyclicity constraints (Zheng et al., 2018; Pamfil et al., 2020). Recent extensions address nonlinearity and latent confounders (Peters et al., 2017; Löwe et al., 2022). We propose a differentiable score for discovering causal structure with latent confounding in nonstationary multivariate series.\n\nWe validate on synthetic benchmarks and real-world climate datasets, showing improvements in F1 and orientation accuracy.\n",
    "reason": "The authors present their method immediately after listing prior work but do not identify a concrete shortcoming in existing approaches or a gap their method fills, aligning with (b).",
    "start": 262,
    "end": 809,
    "label": "Lacks synthesis"
  },
  {
    "span": "The LibriSpeech-Adapt shared task introduced noisy-reverberant test sets in 2023.",
    "document": "Introduction\n\nDomain adaptation in automatic speech recognition (ASR) aims to maintain accuracy as recording conditions, speakers, and content shift. LibriSpeech and related corpora have served as standard benchmarks for acoustic and language modeling (Panayotov et al., 2015; Kahn et al., 2020). Robust ASR requires handling far-field speech, background noise, and channel mismatch through augmentation, multi-condition training, and adaptation techniques (Ko et al., 2015; Karita et al., 2019).\n\nThe LibriSpeech-Adapt shared task introduced noisy-reverberant test sets in 2023. While synthetic augmentations like SpecAugment can partially bridge the gap, real recording artifacts remain challenging. We propose a teacher-student framework that distills robustness from a multi-condition teacher into a lightweight student using target-domain pseudo-labels.\n\nRelated Work\n\nUnsupervised domain adaptation approaches include self-training, adversarial learning, and feature normalization (Sun et al., 2017; Weninger et al., 2019). Recent transformer-based acoustic models demonstrate strong performance but can overfit to clean conditions without targeted augmentation (Gulati et al., 2020). Our method aligns with self-training paradigms while focusing on stability under severe reverberation.",
    "reason": "Mentions a specific shared task and its contribution without citing any task report or organizing publication (rule a).",
    "start": 498,
    "end": 579,
    "label": "Unsupported claim"
  },
  {
    "span": "Early shared tasks focused exclusively on Mandarin-English code-switching.",
    "document": "Introduction\n\nCode-switched automatic speech recognition (ASR) presents unique challenges owing to phonotactic interference, variable switching granularity, and scarce labeled data (Li and Fung, 2012; Yilmaz et al., 2018). Approaches include multilingual acoustic modeling, lexicon unification, and language identification–aware decoding (Khassanov et al., 2019; Toshniwal et al., 2018). Recent advances in self-supervised pretraining on multilingual audio have improved robustness under low-resource conditions (Baevski et al., 2020; Conneau et al., 2021).\n\nEarly shared tasks focused exclusively on Mandarin-English code-switching. As the field matured, attention expanded to other language pairs and spontaneous conversational speech. However, standardized evaluation protocols and open corpora remain limited, hindering reproducibility and fair comparison across systems.\n\nWe release a curated benchmark spanning three Asian language pairs with matched annotation schemas and report end-to-end ASR results under consistent decoding settings.",
    "reason": "Historical claim about shared tasks is made without any citations supporting it.",
    "start": 559,
    "end": 633,
    "label": "Unsupported claim"
  },
  {
    "span": "The LibriSpeech corpus contains roughly 1,000 hours of read English speech and serves as the de facto benchmark for open-vocabulary ASR.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) has seen substantial improvements due to self-supervised pretraining and end-to-end architectures. Benchmark datasets and standardized evaluation protocols have played a critical role in measuring progress and diagnosing failure modes across domains.\n\nThe LibriSpeech corpus contains roughly 1,000 hours of read English speech and serves as the de facto benchmark for open-vocabulary ASR. Despite the popularity of this dataset, domain shifts to conversational or noisy settings continue to expose robustness gaps, underscoring the need for methods that generalize beyond clean read speech.\n",
    "reason": "Introduces a dataset at first mention and states a specific statistic and status without any citation, which should be supported by references.",
    "start": 301,
    "end": 437,
    "label": "Unsupported claim"
  },
  {
    "span": "Miller et al., (2016)",
    "document": "Introduction\n\nInteractive systems benefit from mixed-initiative design where both the human and the agent can steer task flow (Horvitz, 1999; Amershi et al., 2014). Following Miller et al., (2016), we structure explanations to answer why, why-not, and what-if queries. However, existing toolkits emphasize static explanations that fail to adapt as user goals evolve (Patel and Zhou, 2020; Kim and Cramer, 2021). We present a dialogue-centered framework that updates explanation granularity based on inferred user expertise, improving task success in simulation and user studies.\n\nRelated Work\n\nRecent advances in counterfactual generation (Wachter et al., 2017; Mothilal et al., 2020) and causal abstractions (Beckers and Halpern, 2019) inform interactive explanation design. Our work complements these by focusing on real-time adaptation and user modeling.",
    "reason": "Extraneous comma between author name and year in a narrative citation; should be 'Miller et al. (2016)'.",
    "start": 175,
    "end": 196,
    "label": "Format"
  },
  {
    "span": "In the robotics literature, several competitions have already standardized the sim-to-real transfer protocol.",
    "document": "Related Work\n\nSim-to-real transfer seeks to bridge the gap between idealized simulation environments and noisy real-world deployments. Techniques such as domain randomization and adaptation have shown promise for robust policy transfer.\n\nIn the robotics literature, several competitions have already standardized the sim-to-real transfer protocol. Nevertheless, reported evaluation criteria differ in sensor setups and tolerance thresholds, complicating apples-to-apples comparisons across platforms.\n\nWe propose a unified benchmark that specifies calibration procedures, perturbation budgets, and safety checks, enabling reproducible comparisons for manipulation tasks.",
    "reason": "Refers to competitions and standardized protocols in prior literature without citing any competitions or sources, violating rule (a) and (b).",
    "start": 238,
    "end": 347,
    "label": "Unsupported claim"
  },
  {
    "span": "Prior work has shown that disentangled representations enable controllable text generation.",
    "document": "Related Work\n\nControllability in text generation is often sought by learning latent factors that separate style from content. Prior work has shown that disentangled representations enable controllable text generation. Methods include variational objectives, adversarial training, and attribute classifiers to steer decoding.",
    "reason": "References prior work and a key result without providing citations to the studies claiming this finding.",
    "start": 126,
    "end": 217,
    "label": "Unsupported claim"
  },
  {
    "span": "According to industry reports, 80% of enterprise data is unstructured.",
    "document": "Introduction\n\nEnterprises increasingly rely on unstructured data such as text, images, and audio to drive analytics and decision-making. Traditional data warehouses were designed for structured tables, leaving gaps in governance, quality, and access when integrating unstructured modalities (Stonebraker and Çetintemel, 2005). According to industry reports, 80% of enterprise data is unstructured. We present a unified indexing and retrieval layer that surfaces unstructured signals alongside structured attributes with consistent security and lineage controls.",
    "reason": "This sentence cites a specific statistic attributed vaguely to 'industry reports' without an actual reference, which requires a concrete citation (rules b and e).",
    "start": 327,
    "end": 397,
    "label": "Unsupported claim"
  },
  {
    "span": "(Chen et al 2020)",
    "document": "Introduction\n\nData augmentation helps reduce overfitting in low-resource settings. Earlier work (Chen et al 2020) explored back-translation for robust paraphrasing, while contemporaneous efforts applied mixup at the token and embedding levels. However, these methods may degrade calibration and introduce distributional artifacts that confound evaluation.\n\nWe propose uncertainty-aware augmentation that conditions transformations on model confidence, preserving label semantics while improving generalization.",
    "reason": "Missing period after \"al\" in an author–year citation; should be (Chen et al., 2020).",
    "start": 96,
    "end": 113,
    "label": "Format"
  },
  {
    "span": "The MovieLens-1M dataset is known to contain gender biases in occupation labels.",
    "document": "Introduction\n\nFairness in recommender systems has gained attention as deployments affect diverse user populations. Standard collaborative filtering objectives can inadvertently encode historical and systemic biases present in interaction logs and auxiliary metadata.\n\nThe MovieLens-1M dataset is known to contain gender biases in occupation labels. These imbalances can propagate through user and item embeddings, leading to disparate exposure and ranking outcomes across demographic groups.\n\nMitigation strategies span reweighting, regularization toward demographic parity, and post-hoc calibration of exposure. We propose a counterfactual evaluation framework that estimates recommendation disparity under balanced occupation distributions while preserving utility.",
    "reason": "Claims a specific bias property of a named dataset without citing any empirical analysis or source; datasets should be cited at first mention.",
    "start": 268,
    "end": 348,
    "label": "Unsupported claim"
  },
  {
    "span": "Bias mitigation techniques for toxicity detection have predominantly relied on counterfactual data augmentation.",
    "document": "Related Work\n\nAutomated toxicity detection has been widely studied for content moderation, yet models often exhibit disparate error rates across demographic groups (Prasad et al., 2020; Wang and Chang, 2021). Approaches to fairness include debiasing pretraining corpora, adversarial learning, and calibrated thresholding.\n\nBias mitigation techniques for toxicity detection have predominantly relied on counterfactual data augmentation.\n\nAlternative strategies include lexicon-level constraints and representation orthogonalization (Lee et al., 2022). Our approach differs by explicitly modeling group-conditional uncertainty during training.",
    "reason": "Claims a field-wide trend ('predominantly relied on') without providing citations to support the assertion.",
    "start": 323,
    "end": 435,
    "label": "Unsupported claim"
  },
  {
    "span": "(Legrand and Patel, 2015",
    "document": "Introduction\n\nRobust optimization for neural networks has seen rapid growth, with methods ranging from adversarial training to certified defenses (Madry et al., 2018; Wong and Kolter, 2018). Early work on robust representations emphasized margin maximization and Lipschitz continuity (Legrand and Patel, 2015, Hein and Andriushchenko, 2017). More recent techniques use randomized smoothing to provide distributional guarantees under noise (Cohen et al., 2019; Salman et al., 2020). We target the complementary problem of robustness under distribution shift by regularizing feature geometry and imposing spectrum constraints (Zhang and Wang, 2021; Liu et al., 2022).",
    "reason": "Missing closing parenthesis in a parenthetical citation.",
    "start": 284,
    "end": 308,
    "label": "Format"
  },
  {
    "span": "Score-based and continuous optimization methods for causal discovery over continuous variables include NOTEARS (Zheng et al., 2018), DAG-GNN (Yu et al., 2019), GraN-DAG (Lachapelle et al., 2020), and RL-based variants (Zhu et al., 2020). Constraint-based approaches like PC and FCI (Spirtes et al., 2000) and score-based GES (Chickering, 2002) remain strong baselines.",
    "document": "Related Work\n\nCausal structure learning aims to recover directed acyclic graphs (DAGs) from observational or interventional data. Modern approaches seek to scale to high dimensions while providing statistical guarantees or practical robustness.\n\nScore-based and continuous optimization methods for causal discovery over continuous variables include NOTEARS (Zheng et al., 2018), DAG-GNN (Yu et al., 2019), GraN-DAG (Lachapelle et al., 2020), and RL-based variants (Zhu et al., 2020). Constraint-based approaches like PC and FCI (Spirtes et al., 2000) and score-based GES (Chickering, 2002) remain strong baselines.\n\nRecent work extends to nonlinearities and non-Gaussian noise (Shimizu et al., 2006; Peters et al., 2014). Interventional data and invariance constraints further improve identifiability (Eaton and Murphy, 2007; Heinze-Deml et al., 2018).\n\nIntroduction\n\nWe propose a scalable acyclicity-penalized estimator with sparse interventions.",
    "reason": "The span lists families of causal discovery algorithms and baselines but does not explain how they inform or contrast with the proposed estimator, indicating lack of synthesis per (a)/(c).",
    "start": 246,
    "end": 614,
    "label": "Lacks synthesis"
  },
  {
    "span": "Smith et al.",
    "document": "Introduction\n\nEdge intelligence promises low-latency, privacy-preserving learning by training models directly on devices while aggregating updates centrally (Kumar et al., 2021; Liang and Zhao, 2022). Following Smith et al., we assume a partially synchronous setting where clients participate opportunistically but aggregation occurs on fixed intervals. However, real-world networks exhibit bursty connectivity and non-i.i.d. data distributions (Chen et al., 2020), which degrade convergence and fairness (Roth and Bauer, 2021). We address these challenges by proposing an adaptive participation scheduler that couples client selection with drift-aware aggregation (Tan and Gupta, 2023).\n\nRelated Work\n\nClient selection strategies have evolved from random sampling (McRae et al., 2019) to utility-based approaches that prioritize gradient diversity (Park et al., 2021) and fairness (Li and Ahmed, 2020). Communication reduction techniques include quantization (Vega et al., 2019) and periodic aggregation (Omar and Lee, 2020). Our work complements drift-robust objectives (Zhou et al., 2022) by acting at the scheduling layer rather than modifying the loss.",
    "reason": "Narrative citation missing year; should be formatted as Smith et al. (YEAR) in narrative style.",
    "start": 211,
    "end": 223,
    "label": "Format"
  },
  {
    "span": "Transformer-based architectures have been adapted to medical image segmentation through hybrid CNN-Transformer encoders (Chen et al., 2021), shifted window attention (Hatamizadeh et al., 2022), axial attention (Wang et al., 2020), and pyramid features (Xie et al., 2021). Other studies focus on multi-scale context (Zhou et al., 2019) and boundary-aware decoders (Kervadec et al., 2018). In this work, we present MedFormer, a transformer-based network for 2D and 3D segmentation tasks.",
    "document": "Related Work\n\nMedical image segmentation has transitioned from pure convolutional encoders to hybrid or attention-based architectures that can model long-range dependencies. This trend parallels developments in natural image understanding, with adaptations to account for limited labeled data and domain-specific priors.\n\nTransformer-based architectures have been adapted to medical image segmentation through hybrid CNN-Transformer encoders (Chen et al., 2021), shifted window attention (Hatamizadeh et al., 2022), axial attention (Wang et al., 2020), and pyramid features (Xie et al., 2021). Other studies focus on multi-scale context (Zhou et al., 2019) and boundary-aware decoders (Kervadec et al., 2018). In this work, we present MedFormer, a transformer-based network for 2D and 3D segmentation tasks.\n\nSelf-supervised pretraining and data-efficient learning have also been explored to reduce annotation requirements (Tajbakhsh et al., 2020; Zhuang et al., 2019), while uncertainty estimation aids clinical decision support (Kohl et al., 2018).\n\nIntroduction\n\nWe evaluate MedFormer on multi-organ CT and cardiac MRI datasets and report improvements on Dice and HD95.",
    "reason": "The span jumps from listing prior work to announcing the authors’ model without articulating the specific gap or how their approach addresses limitations, satisfying criterion (b)/(c).",
    "start": 322,
    "end": 807,
    "label": "Lacks synthesis"
  },
  {
    "span": "BERT was used in an AES task trained on essays from non-native English learners",
    "document": "Introduction\n\nAutomatic Essay Scoring (AES) assesses writing quality using computational models and has been studied for decades in educational measurement and NLP (Shermis and Burstein, 2013). Early systems relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse cues, while recent neural approaches learn holistic representations from raw text.\n\nBERT was used in an AES task trained on essays from non-native English learners, and attention patterns were reported to correlate with rubric dimensions such as coherence and grammar. While neural encoders improve accuracy, they can be sensitive to prompt leakage and length biases, motivating research on fairness and generalization (Feinstein et al., 2020). We position our work in this space by introducing prompt-agnostic calibration techniques.",
    "reason": "This is a specific claim about a prior setup and population in AES that should be accompanied by a citation to the study (rule b and example iii).",
    "start": 386,
    "end": 465,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent benchmarks have shown that ViT dominates CNNs on small datasets.",
    "document": "Related Work\n\nVision Transformers (ViTs) have achieved competitive performance on large-scale image classification by modeling long-range dependencies with self-attention (Dosovitskiy et al., 2020; Touvron et al., 2021). Hybrid architectures that combine convolutional stems with transformer blocks improve data efficiency and robustness (Graham et al., 2021; Liu et al., 2021). Recent benchmarks have shown that ViT dominates CNNs on small datasets. In this paper, we focus on low-resource regimes and propose strong regularization and token-sparsity to mitigate overfitting.",
    "reason": "Asserts a comparative result from benchmarks without citing any benchmark studies (definition d and ii).",
    "start": 379,
    "end": 450,
    "label": "Unsupported claim"
  },
  {
    "span": "Probabilistic forecasting with deep autoregressive flows models multimodal futures (Salinas et al., 2019). Matrix profile techniques detect discords in long sequences (Yeh et al., 2016). Seasonal-trend decomposition helps isolate periodic patterns (Cleveland et al., 1990). GAN-based reconstruction flags deviations from learned manifolds (Schlegl et al., 2017).",
    "document": "Introduction\n\nAnomaly detection in time series is central to monitoring industrial systems, finance, and healthcare. Methods differ in assumptions about seasonality, noise, and the availability of labels, leading to varied approaches grounded in forecasting, similarity search, and representation learning.\n\nProbabilistic forecasting with deep autoregressive flows models multimodal futures (Salinas et al., 2019). Matrix profile techniques detect discords in long sequences (Yeh et al., 2016). Seasonal-trend decomposition helps isolate periodic patterns (Cleveland et al., 1990). GAN-based reconstruction flags deviations from learned manifolds (Schlegl et al., 2017).\n\nOur method unifies predictive uncertainty with shape-aware reconstruction to reduce false positives in seasonal data, but prior work treats these ingredients in isolation.",
    "reason": "The span juxtaposes forecasting, matrix profile search, decomposition, and GAN reconstruction without articulating their relationships or transitions, leaving the connections between cited works implicit.",
    "start": 308,
    "end": 670,
    "label": "Coherence"
  },
  {
    "span": "The SemLex 2022 shared task established that contextualized embeddings are the dominant approach.",
    "document": "Introduction\n\nLexical semantic change detection seeks to identify shifts in word meaning across corpora separated in time or domain. Methods range from alignment of static embedding spaces to contextualized representations combined with clustering over usages. Community evaluation has accelerated progress through shared tasks that define common protocols for data preparation and scoring. The SemLex 2022 shared task established that contextualized embeddings are the dominant approach. Building on this trend, we propose a contrastive objective that disentangles frequency effects from sense induction, and we provide an error analysis focusing on polysemous nouns with domain-specific senses.\n\nRelated Work\n\nPrior efforts have examined temporal drift in both news and scientific corpora, with particular attention to frequency confounds and corpus comparability. While contextualized embeddings offer fine-grained usage modeling, recent analyses caution that sense induction remains sensitive to sampling and windowing choices.",
    "reason": "The statement about a specific shared task and its conclusion lacks a citation to the shared task or its report; first mentions of shared tasks and their findings require references.",
    "start": 391,
    "end": 488,
    "label": "Unsupported claim"
  },
  {
    "span": "see (Park et al., 2022)",
    "document": "Introduction\n\nRecent advances in causal representation learning seek to disentangle generative factors from observations (Locatello et al., 2019; Schölkopf et al., 2021). For downstream prediction under interventions, invariance penalties encourage stable predictors across environments (Peters et al., 2016; Arjovsky et al., 2020). For a survey, see (Park et al., 2022) and references therein. Our work examines finite-sample selection effects that cause failure of invariance tests under covariate shift.",
    "reason": "Wrong citation style: the preposition 'see' should introduce a narrative citation 'see Park et al. (2022)' instead of a parenthetical one.",
    "start": 347,
    "end": 370,
    "label": "Format"
  },
  {
    "span": "BERT has been applied to automatic essay scoring with fine-tuning on holistic score labels.",
    "document": "Introduction\n\nAutomatic essay scoring (AES) aims to predict human-assigned quality scores for essays to assist formative assessment. Traditional AES approaches rely on hand-crafted features targeting grammar, coherence, and lexical sophistication (Attali and Burstein, 2006; Shermis and Burstein, 2013). Neural methods learn representations directly from raw text and have improved robustness across prompts under sufficient data (Dong and Zhang, 2016; Taghipour and Ng, 2016).\n\nBERT has been applied to automatic essay scoring with fine-tuning on holistic score labels. Despite strong performance, domain adaptation across prompts remains challenging due to prompt-specific content leakage and topical artifacts. We propose prompt-invariant adapters combined with contrastive alignment across prompts to mitigate overfitting while retaining linguistic proficiency cues.\n\nWe evaluate on widely used AES corpora with cross-prompt splits and analyze fairness across demographic subgroups following recent auditing frameworks (Blodgett et al., 2020).",
    "reason": "The sentence asserts prior application of BERT to AES without citing any supporting studies at the first mention of that approach (violates rule a).",
    "start": 479,
    "end": 570,
    "label": "Unsupported claim"
  },
  {
    "span": "Fairness in conversational agents has been studied through dataset audits, debiasing objectives, and evaluation metrics such as demographic parity, equalized odds, and calibration (Zhao et al., 2018; Huang et al., 2020; Blodgett et al., 2021; Dixon et al., 2018; Wang and Rovatsos, 2021).",
    "document": "Related Work\n\nConversational AI systems can exhibit biased behavior that affects user trust and safety. Bias may arise from imbalanced data, spurious correlations, or reinforcement from user interactions at deployment.\n\nFairness in conversational agents has been studied through dataset audits, debiasing objectives, and evaluation metrics such as demographic parity, equalized odds, and calibration (Zhao et al., 2018; Huang et al., 2020; Blodgett et al., 2021; Dixon et al., 2018; Wang and Rovatsos, 2021).\n\nBeyond static metrics, several works explore counterfactual data augmentation and adversarial training to reduce representational harms. However, online adaptation can reintroduce disparities as dialogue policies learn from skewed feedback.\n\nOur work examines fairness under continual learning, proposing a constrained policy update that preserves group-level fairness certificates while improving task success. We validate on multi-domain dialogue benchmarks with simulated and human-in-the-loop evaluations.",
    "reason": "The span catalogs fairness methods and metrics but does not relate them to conversational agents under continual learning or state a gap the current paper addresses, thus lacking synthesis (criterion a).",
    "start": 220,
    "end": 508,
    "label": "Lacks synthesis"
  },
  {
    "span": "The widely used XSum dataset contains more abstractive summaries than CNN/DailyMail.",
    "document": "Related Work\n\nNeural abstractive summarization has benefited from large-scale news datasets and pretrained sequence-to-sequence models (See et al., 2017; Lewis et al., 2020). Research has examined the trade-off between faithfulness and abstraction across domains and training regimes (Falke et al., 2019; Durmus et al., 2020). The widely used XSum dataset contains more abstractive summaries than CNN/DailyMail. Our method targets factuality by integrating entity-level constraints during decoding.",
    "reason": "This sentence introduces specific datasets and makes a comparative claim without citing the dataset papers, violating rule (a) and (b).",
    "start": 327,
    "end": 411,
    "label": "Unsupported claim"
  },
  {
    "span": "(2015) Johnson et al.",
    "document": "Introduction\n\nRecent advances in multilingual modeling leverage shared subword vocabularies and joint training objectives (Conneau and Lample, 2019; Artetxe et al., 2020). According to (2015) Johnson et al., zero-shot translation emerges from parameter sharing across languages, a finding corroborated by later scaling studies (Aharoni et al., 2019). However, domain mismatch remains a challenge (Koehn and Knowles, 2017).\n\nWe propose a curriculum that balances language similarity and corpus size to improve cross-lingual transfer in low-resource settings (Ruder et al., 2019).",
    "reason": "Year placed before authors inside parentheses in a narrative context; should be formatted as a narrative citation like \"Johnson et al. (2015)\".",
    "start": 185,
    "end": 206,
    "label": "Format"
  },
  {
    "span": "Recent works have achieved near-human performance on paraphrase detection across multiple benchmarks.",
    "document": "Introduction\n\nParaphrase identification seeks to determine whether two sentences convey the same meaning. It has broad applications in question answering, duplicate detection in community forums, and textual entailment. Datasets such as the Microsoft Research Paraphrase Corpus (MRPC) and Quora Question Pairs (QQP) have served as standard benchmarks for this task, with evaluation typically reported in terms of accuracy and F1.\n\nNeural architectures have progressively improved performance, starting from sentence encoders based on recurrent networks and attention to pre-trained transformer models fine-tuned on paraphrase objectives. Transfer learning and data augmentation have further enhanced generalization across domains.\n\nRecent works have achieved near-human performance on paraphrase detection across multiple benchmarks. Nonetheless, despite strong aggregate scores, models may still overfit lexical overlap and fail on adversarial or low-resource variants, motivating more robust evaluation protocols.",
    "reason": "Claims that 'recent works' achieved near-human performance without providing any citations to the specific studies, violating the requirement to cite prior work at first mention (rule d and a).",
    "start": 732,
    "end": 833,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works challenge the assumption that BLEU correlates with human judgments.",
    "document": "Introduction\n\nAutomatic evaluation remains a bottleneck in machine translation (MT) research. While n-gram overlap metrics are inexpensive and widely reported, they can fail to capture adequacy and fluency in light of paraphrastic variation.\n\nRecent works challenge the assumption that BLEU correlates with human judgments. In particular, evidence from multiple language pairs suggests that system rankings by BLEU can diverge from expert assessments when models employ aggressive paraphrasing.\n\nWe contribute a cross-lingual study comparing reference-based and reference-free metrics under controlled paraphrase generation.",
    "reason": "Uses the phrase “recent works” to make a prior-work claim without citing any of those works (violates rule d).",
    "start": 243,
    "end": 323,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT has been successfully used for holistic AES on multi-prompt essay sets.",
    "document": "Introduction\n\nAutomatic Essay Scoring (AES) seeks to predict human-assigned holistic scores for student essays. Traditional approaches combine engineered features with linear or tree-based models, while neural encoders have more recently improved robustness and generalization.\n\nBERT has been successfully used for holistic AES on multi-prompt essay sets. However, adapting pretrained language models to AES raises concerns about prompt leakage, distribution shift across prompts, and sensitivity to adversarial paraphrases.\n\nWe propose a prompt-anchored calibration procedure that disentangles prompt-invariant linguistic competence from prompt-specific genre expectations. Our experiments evaluate transfer across prompts and institutions, and we analyze calibration error under prompt reassignment.",
    "reason": "Claims prior success of BERT on a specific AES setup without citing the corresponding studies; violates rule a) about citing prior work at first mention.",
    "start": 279,
    "end": 355,
    "label": "Unsupported claim"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nContrastive learning aligns representations by bringing positive pairs closer while pushing negatives apart (Oord et al., 2018; Chen et al., 2020). While several works analyze the role of augmentations and batch size (Tian et al., 2020; Zhao and Akhtar, 2021), the effect of temperature scheduling remains unclear, as shown in [12]. We investigate temperature curricula and propose an adaptive scheme that stabilizes training under limited negatives (Khosla et al., 2020; Robinson et al., 2021).",
    "reason": "Wrong citation style: numeric bracketed reference used in an author–year context; should cite by author and year.",
    "start": 341,
    "end": 345,
    "label": "Format"
  },
  {
    "span": "Privacy-preserving federated learning employs secure aggregation to protect individual updates (Bonawitz et al., 2017), homomorphic encryption for outsourced computation (Aono et al., 2017; Hardy et al., 2017), and differential privacy to bound information leakage from gradients (Abadi et al., 2016; Geyer et al., 2017; McMahan et al., 2018). Techniques such as PATE and noisy client sampling further reduce privacy risks (Papernot et al., 2017; Truex et al., 2019). Compression and quantization approaches also help reduce communication overhead while obfuscating signals (Alistarh et al., 2017; Konečný et al., 2016). We propose FedGuard, a practical method for client-level privacy in cross-device settings.",
    "document": "Introduction\n\nFederated learning enables training across many devices without centralizing raw data. However, model updates can still leak sensitive information about individual participants, prompting developments in cryptography and privacy-preserving learning.\n\nPrivacy-preserving federated learning employs secure aggregation to protect individual updates (Bonawitz et al., 2017), homomorphic encryption for outsourced computation (Aono et al., 2017; Hardy et al., 2017), and differential privacy to bound information leakage from gradients (Abadi et al., 2016; Geyer et al., 2017; McMahan et al., 2018). Techniques such as PATE and noisy client sampling further reduce privacy risks (Papernot et al., 2017; Truex et al., 2019). Compression and quantization approaches also help reduce communication overhead while obfuscating signals (Alistarh et al., 2017; Konečný et al., 2016). We propose FedGuard, a practical method for client-level privacy in cross-device settings.\n\nWe evaluate FedGuard on text and image benchmarks under strict participation constraints and provide a cost-accuracy-privacy analysis relevant to mobile deployment.",
    "reason": "The paragraph lists techniques and then states the contribution, but it does not specify the gap or limitation in existing methods that necessitates FedGuard, nor does it articulate the authors' perspective.",
    "start": 265,
    "end": 976,
    "label": "Lacks synthesis"
  },
  {
    "span": "Ortega et al. 1",
    "document": "Introduction\n\nProgram synthesis from natural language has benefited from neural semantic parsing and constrained decoding (Dong and Lapata, 2018; Yin and Neubig, 2017). Hybrid neuro-symbolic systems improve consistency by enforcing grammar constraints during beam search (Bastani and Solar-Lezama, 2020). Ortega et al. 1 introduce an interactive correction loop that solicits clarifying feedback from users to resolve ambiguities, but the approach assumes access to gold execution traces. We instead propose a verifier-guided refinement strategy that learns to predict executable patches without gold traces (Huang and Meyer, 2022), improving sample efficiency.\n\nWe benchmark on diverse domains, including string transformations and table operations, to demonstrate the generality of our approach.",
    "reason": "Wrong use of footnotes: the citation appears as a footnote-style number \"1\" after the authors without a proper year or reference; should be formatted as an author–year citation (e.g., \"Ortega et al. (2019)\") or as a proper footnote.",
    "start": 305,
    "end": 320,
    "label": "Format"
  },
  {
    "span": "Back-translation has become the standard augmentation technique for QA.",
    "document": "Related Work\n\nData augmentation has been investigated to improve generalization of question answering systems, including paraphrasing and synthetic generation (Yu et al., 2018; Lewis et al., 2021). Back-translation has become the standard augmentation technique for QA. Other approaches use knowledge-guided perturbations and counterfactual editing to diversify contexts (Kobayashi, 2018; Kaushik et al., 2020). Our method complements these by targeting reasoning patterns rather than surface forms.",
    "reason": "Asserts field-wide standard practice without citing supporting studies demonstrating its prevalence (rule b).",
    "start": 198,
    "end": 269,
    "label": "Unsupported claim"
  },
  {
    "span": "Classical motion planning methods include grid-based search like A star and sampling-based planners such as PRM and RRT, which trade optimality for computational efficiency (Hart et al., 1968; Kavraki et al., 1996; LaValle, 1998). Learning-based planners leverage value functions, imitation, and differentiable optimization to accelerate planning (Tamar et al., 2016; Okada et al., 2017; Srinivas et al., 2018). Recent work explores diffusion and energy-based models to generate collision-free trajectories (Janner et al., 2022; Chi et al., 2023). We design a planner that leverages diffusion priors for real-time navigation.",
    "document": "Introduction\n\nSafe and efficient motion planning remains a core challenge in robotics, particularly in cluttered and partially observable environments. Planners must balance optimality, robustness, and runtime constraints to operate on real hardware.\n\nClassical motion planning methods include grid-based search like A star and sampling-based planners such as PRM and RRT, which trade optimality for computational efficiency (Hart et al., 1968; Kavraki et al., 1996; LaValle, 1998). Learning-based planners leverage value functions, imitation, and differentiable optimization to accelerate planning (Tamar et al., 2016; Okada et al., 2017; Srinivas et al., 2018). Recent work explores diffusion and energy-based models to generate collision-free trajectories (Janner et al., 2022; Chi et al., 2023). We design a planner that leverages diffusion priors for real-time navigation.\n\nWe demonstrate closed-loop performance on aerial and ground robots and analyze robustness to sensor noise and dynamic obstacles.",
    "reason": "The paragraph lists prior planning approaches and announces the proposed planner but does not state what specific limitation remains in existing planners or why diffusion priors are needed, thus lacking synthesis.",
    "start": 252,
    "end": 877,
    "label": "Lacks synthesis"
  },
  {
    "span": "Most previous studies focus on English and Chinese, leaving typologically diverse languages underexplored.",
    "document": "Related Work\n\nCross-lingual sentiment analysis seeks models that generalize across languages with minimal labeled data. Approaches include multilingual pretraining, pivot-based translation, and alignment via parallel or comparable corpora. Evaluation typically spans multiple domains, such as product reviews and social media.\n\nMost previous studies focus on English and Chinese, leaving typologically diverse languages underexplored. Recent benchmarks have started to include morphologically rich and low-resource languages, but coverage and domain diversity remain limited.\n\nOur work contributes a balanced evaluation suite with strict train–test language disjointness and examines robustness under translationese and code-switching.",
    "reason": "Makes a broad claim about the distribution of prior studies across languages without citing surveys, benchmarks, or representative prior work to support it.",
    "start": 328,
    "end": 434,
    "label": "Unsupported claim"
  },
  {
    "span": "End-to-end speech-to-speech translation benefits from pretraining ASR and TTS components on large corpora (Jia et al., 2019; Wang et al., 2021). Prosody transfer methods aim to model pitch and rhythm across speakers (Skerry-Ryan et al., 2018; Kenter et al., 2019). Vowel space normalization has been studied in phonetics to compare speakers (Lobanov, 1971).",
    "document": "Introduction\n\nSpeech-to-speech translation seeks to map source language speech directly to target language speech, minimizing latency and compounding errors across cascaded components. By integrating acoustic modeling, linguistic content, and prosody, end-to-end approaches promise more natural and efficient cross-lingual communication.\n\nEnd-to-end speech-to-speech translation benefits from pretraining ASR and TTS components on large corpora (Jia et al., 2019; Wang et al., 2021). Prosody transfer methods aim to model pitch and rhythm across speakers (Skerry-Ryan et al., 2018; Kenter et al., 2019). Vowel space normalization has been studied in phonetics to compare speakers (Lobanov, 1971).\n\nRecent work aligns latent acoustic spaces across languages and leverages self-supervised speech representations. Data sparsity remains a major obstacle, pushing researchers toward synthetic targets, multilingual pretraining, and voice conversion techniques. Evaluation increasingly includes intelligibility, naturalness, and speaker similarity metrics.\n\nWe introduce a cross-lingual prosody aware model that disentangles content and style with lightweight constraints. Our training regimen combines multilingual self-supervision with cycle consistency to improve robustness and reduce reliance on parallel data.",
    "reason": "The span mentions pretraining for S2ST, then jumps to prosody transfer, and finally to a classic phonetics method without transitions or explanation of their relationships, making the connection between the cited works unclear.",
    "start": 339,
    "end": 696,
    "label": "Coherence"
  },
  {
    "span": "BERT has been used in document-level NMT as a reranker for beam candidates",
    "document": "Related Work\n\nDocument-level machine translation (DocMT) seeks to model inter-sentential dependencies to improve cohesion and coherence (Voita et al., 2019; Liu et al., 2020). Popular strategies include context-augmented encoders, cache mechanisms, and hierarchical attention over preceding sentences (Jean et al., 2017; Zhang et al., 2020).\n\nBeyond architectural integration, external language models and pretrained encoders have been explored for rescoring. BERT has been used in document-level NMT as a reranker for beam candidates, with the aim of capturing discourse-level signals missed by left-to-right decoders. However, reranking pipelines often require heavy engineering and can be brittle to domain shifts.\n\nWe propose a lightweight contrastive objective that directly optimizes discourse-aware preferences within the translation model, avoiding post-hoc reranking while retaining coherence gains.",
    "reason": "This is a specific setup claim about using BERT for DocMT reranking without a citation, matching example iii and rule a for first mentions of a method applied in a task.",
    "start": 460,
    "end": 534,
    "label": "Unsupported claim"
  },
  {
    "span": "Fairness in recommender systems has been investigated from provider fairness, consumer fairness, and item popularity perspectives (Wu and Zhang, 2019; Diaz et al., 2020; Ahmed and Roy, 2021). Mitigation techniques include re-ranking, adversarial regularization, and post-hoc calibration (Singh et al., 2019; Choi and Lee, 2020; Morton et al., 2022). Measurement frameworks propose exposure metrics, calibration gaps, and counterfactual evaluation (Patil et al., 2020; Chen and Du, 2021; Ivanov et al., 2022).",
    "document": "Related Work\n\nRecommendation quality and fairness are increasingly viewed as intertwined objectives. Systems must balance engagement goals with equitable treatment of users and content providers.\n\nFairness in recommender systems has been investigated from provider fairness, consumer fairness, and item popularity perspectives (Wu and Zhang, 2019; Diaz et al., 2020; Ahmed and Roy, 2021). Mitigation techniques include re-ranking, adversarial regularization, and post-hoc calibration (Singh et al., 2019; Choi and Lee, 2020; Morton et al., 2022). Measurement frameworks propose exposure metrics, calibration gaps, and counterfactual evaluation (Patil et al., 2020; Chen and Du, 2021; Ivanov et al., 2022).\n\nIn contrast, we focus on dynamic allocation under shifting traffic and content supply. We develop an online exposure controller with stability guarantees that modulates slot-level delivery using constrained optimization, and we introduce a drift-robust counterfactual auditor for continual monitoring.",
    "reason": "The span lists areas and techniques with citations but does not connect them to the paper’s dynamic allocation setting or articulate what limitation they leave unresolved, thus lacking synthesis (criterion a and c).",
    "start": 197,
    "end": 705,
    "label": "Lacks synthesis"
  },
  {
    "span": "BERT was used in an AES task trained on essays from multiple prompts.",
    "document": "Related Work\n\nAutomated Essay Scoring (AES) traditionally relied on handcrafted features capturing discourse structure, grammar, and vocabulary richness (Shermis and Burstein, 2013). Neural approaches introduced convolutional and recurrent architectures to learn semantic and syntactic cues end-to-end (Taghipour and Ng, 2016). More recently, pretrained language models have been explored for AES to improve cross-prompt generalization and calibration. BERT was used in an AES task trained on essays from multiple prompts. Complementary lines of work address fairness and bias in scoring, especially across demographic subgroups and topical variance (Wang et al., 2020).",
    "reason": "This is a claim about a specific prior setup (BERT used for AES across prompts) without citing the work that introduced or evaluated it, which requires citation (rules a and e.iii).",
    "start": 453,
    "end": 522,
    "label": "Unsupported claim"
  },
  {
    "span": "Graph-based recommenders leverage message passing to capture high-order user–item relations (Zhang and Chen, 2019; Wang et al., 2020). Incorporating side information such as reviews or knowledge graphs further enriches user and item embeddings (Li et al., 2020; Sun et al., 2021). Sequential recommenders integrate temporal signals with self-attention over interaction histories (Kang and McAuley, 2018; Zhou et al., 2020). In this paper, we propose a hybrid model that combines graph propagation with sequence modeling.",
    "document": "Introduction\n\nPersonalized recommendation often requires balancing structural signals from interaction graphs with temporal dynamics in user behavior. While deep learning approaches have improved top-N recommendation, the optimal fusion of graph context and sequence dependencies remains an active research topic.\n\nGraph-based recommenders leverage message passing to capture high-order user–item relations (Zhang and Chen, 2019; Wang et al., 2020). Incorporating side information such as reviews or knowledge graphs further enriches user and item embeddings (Li et al., 2020; Sun et al., 2021). Sequential recommenders integrate temporal signals with self-attention over interaction histories (Kang and McAuley, 2018; Zhou et al., 2020). In this paper, we propose a hybrid model that combines graph propagation with sequence modeling.\n\nWe evaluate our approach across three public datasets using standard ranking metrics and conduct ablations on propagation depth and sequence length.",
    "reason": "The span ends with the authors’ contribution immediately after listing prior work but does not identify any specific gap or explain how the contribution addresses shortcomings (criterion b).",
    "start": 315,
    "end": 835,
    "label": "Lacks synthesis"
  },
  {
    "span": "It is well known that audio features are less reliable than visual ones when sarcasm is present.",
    "document": "Introduction\n\nMultimodal sentiment analysis integrates visual, acoustic, and textual cues to infer affective states. Prior studies have demonstrated that cross-modal interactions can capture subtle cues missed by unimodal systems (Poria et al., 2017; Zadeh et al., 2018). Robust fusion remains challenging due to modality asynchrony, missing signals, and domain shifts across recording settings (Tsai et al., 2019; Hazarika et al., 2020).\n\nIt is well known that audio features are less reliable than visual ones when sarcasm is present. However, the relative reliability of modalities likely depends on cultural norms, recording quality, and individual speaking styles. We therefore propose an adaptive gating mechanism that weights modalities per instance using uncertainty estimates derived from calibration losses.\n\nRelated Work\n\nFusion strategies include early concatenation, cross-modal attention, and tensor fusion (Chen et al., 2017; Tsai et al., 2019). Uncertainty-aware fusion has been explored to mitigate noisy modalities but often assumes stationary distributions (Kumar et al., 2020). Our method complements these approaches by dynamically adjusting modality contribution via learned confidence measures.",
    "reason": "Makes a specific, domain-dependent claim presented as common knowledge without any supporting evidence or citation (rule b).",
    "start": 440,
    "end": 536,
    "label": "Unsupported claim"
  },
  {
    "span": "Autoencoder-based detectors reconstruct normal patterns to flag deviations (Zong et al., 2018). Change-point detection methods localize distribution shifts (Truong et al., 2020). Probabilistic forecasting with deep state space models yields calibrated uncertainty (Salinas et al., 2020).",
    "document": "Related Work\n\nTime series anomaly detection encompasses reconstruction-based methods, predictive approaches, and statistical tests that target rare or unexpected behaviors. Recent deep models leverage temporal context and uncertainty to improve detection fidelity under noise.\n\nAutoencoder-based detectors reconstruct normal patterns to flag deviations (Zong et al., 2018). Change-point detection methods localize distribution shifts (Truong et al., 2020). Probabilistic forecasting with deep state space models yields calibrated uncertainty (Salinas et al., 2020). We combine predictive uncertainty with adaptive thresholds to better handle nonstationary regimes.\n",
    "reason": "The three sentences cover disparate techniques without articulating how they connect; there are no transitions that relate reconstruction, change-point, and forecasting approaches.",
    "start": 278,
    "end": 565,
    "label": "Coherence"
  },
  {
    "span": "Most prior work evaluates on LibriSpeech and VoxCeleb.",
    "document": "Related Work\n\nSelf-supervised learning for speech representation has improved downstream performance on automatic speech recognition and speaker verification (Baevski et al., 2020; Hsu et al., 2021). Pretraining objectives such as masked prediction and contrastive learning enable models to leverage large-scale unlabeled audio. Most prior work evaluates on LibriSpeech and VoxCeleb. While these datasets offer standardized splits and protocols, domain gaps arise when transferring to conversational and far-field audio, motivating evaluation on more diverse corpora (Zhu et al., 2021).",
    "reason": "The sentence generalizes about common evaluation datasets without citing supporting surveys or studies, which should be referenced (rules a and d).",
    "start": 329,
    "end": 383,
    "label": "Unsupported claim"
  },
  {
    "span": "We observed that 65% of prior arts report results on pre-release test sets.",
    "document": "Related Work\n\nBenchmarking practices in machine learning have drawn scrutiny due to issues of data leakage, selective reporting, and unstable baselines (Sculley et al., 2018; Lipton and Steinhardt, 2019). The reproducibility movement emphasizes transparent protocols, fixed evaluation splits, and comprehensive ablations (Pineau et al., 2020; Gundersen and Kjensmo, 2018). We observed that 65% of prior arts report results on pre-release test sets. Such practices hinder fair comparison and can inflate perceived progress.\n\nTo address this, recent initiatives advocate leaderboards with statistical significance testing and submission caps (Bouthillier et al., 2019; Dodge et al., 2019). Our work contributes a standardized evaluation harness with versioned datasets and retrainable baselines to reduce variance across runs and hardware.",
    "reason": "The sentence presents a specific quantitative claim about prior work without citation or evidence for the 65% figure.",
    "start": 373,
    "end": 448,
    "label": "Unsupported claim"
  },
  {
    "span": "Acoustic prosody features capture affective cues in speech (Eyben et al., 2010). Sarcasm detection in text uses pragmatic signals and context modeling (Joshi et al., 2017). Temporal modeling in video with 3D CNNs improves emotion recognition (Tran et al., 2015).",
    "document": "Related Work\n\nMultimodal sentiment analysis integrates language, audio, and vision to infer user attitudes (Poria et al., 2017). Recent approaches fuse representations at different granularities, using transformers to capture cross-modal dependencies (Tsai et al., 2019).\n\nAcoustic prosody features capture affective cues in speech (Eyben et al., 2010). Sarcasm detection in text uses pragmatic signals and context modeling (Joshi et al., 2017). Temporal modeling in video with 3D CNNs improves emotion recognition (Tran et al., 2015). We focus on learning alignment between modalities under label sparsity, proposing a contrastive objective that operates across asynchronous streams.\n",
    "reason": "The sentences jump across audio, text sarcasm, and video with no explanation of how they relate, lacking transitions that would connect these cited strands.",
    "start": 273,
    "end": 535,
    "label": "Coherence"
  },
  {
    "span": " (Li et al.,, 2022)",
    "document": "Related Work\n\nLarge language models benefit from scale and diverse pretraining corpora (Brown et al., 2020; Rae et al., 2021). Scaling laws suggest predictable returns to data and compute (Kaplan et al., 2020; Hoffmann et al., 2022). For instruction following, mixtures of supervised and preference data are effective (Ouyang et al., 2022; Bai et al., 2022). We follow retrieval-augmented pretraining (Li et al.,, 2022) and extend it with on-the-fly indexing for long-horizon tasks (Izacard et al., 2022; Borgeaud et al., 2022).",
    "reason": "Punctuation error in citation (double comma); should be '(Li et al., 2022)'.",
    "start": 400,
    "end": 419,
    "label": "Format"
  },
  {
    "span": "previous studies have consistently found that early forum activity is the strongest predictor of dropout",
    "document": "Related Work\n\nMOOC Dropout Prediction Predicting attrition in large-scale online courses enables timely interventions to improve learner outcomes (Kloft et al., 2014; Gardner and Brooks, 2018). Behavioral features such as video consumption patterns and assignment submissions have been widely leveraged in survival and classification models (Whitehill et al., 2017; Fei and Yeung, 2015). In this line of work, previous studies have consistently found that early forum activity is the strongest predictor of dropout, motivating our inclusion of discourse signals in the feature set.\n\nInterventions and Fairness Recent research has examined personalized nudges and the fairness of predictive models across demographic groups (Liu et al., 2020; Doroudi et al., 2019). We extend this by incorporating bias-aware calibration to mitigate disparate false positive rates for intervention targeting.",
    "reason": "This sentence attributes a consistent finding to 'previous studies' without citing them, which should be supported by references.",
    "start": 410,
    "end": 514,
    "label": "Unsupported claim"
  },
  {
    "span": "to the best of our knowledge, our approach is the first to combine graph transformers with diffusion models for traffic forecasting",
    "document": "Introduction\n\nAccurate traffic forecasting underpins intelligent transportation systems, enabling proactive congestion management and safer routing. Spatial–temporal models have progressed from graph convolutional networks to attention-based architectures that better capture long-range dependencies. Diffusion generative modeling, meanwhile, offers a principled way to represent multi-modal future trajectories.\n\nBridging these ideas, to the best of our knowledge, our approach is the first to combine graph transformers with diffusion models for traffic forecasting. We design a noise-conditioned graph-attention backbone that learns denoising trajectories while preserving road network topology.\n\nExperiments on three metropolitan sensor networks show consistent gains in heavy-congestion regimes and improved calibration of predictive intervals.",
    "reason": "A novelty/first-claim about prior work is made without any comparative citations to substantiate it (rule b/e: claims about being the first should be supported with references).",
    "start": 436,
    "end": 567,
    "label": "Unsupported claim"
  },
  {
    "span": "(Kim and Lee, 2019 and Patel et al., 2020)",
    "document": "Related Work\n\nDebiasing methods in recommendation systems include adversarial training (Zhang et al., 2018) and constraint-based optimization (Zafar et al., 2017). Exposure bias was studied in learning-to-rank settings (Joachims et al., 2017). Recent studies propose counterfactual estimators to correct logging bias (Kim and Lee, 2019 and Patel et al., 2020), while others advocate for calibration-aware metrics (Gupta et al., 2021).\n\nWe unify these perspectives with a propensity-aware training objective that jointly optimizes utility and fairness (Wang et al., 2021).",
    "reason": "Incorrect separator in a multi-citation; should use a semicolon between citations, e.g., \"(Kim and Lee, 2019; Patel et al., 2020)\".",
    "start": 317,
    "end": 359,
    "label": "Format"
  },
  {
    "span": "The SemEval 2020 figurative language shared task standardized the evaluation metrics for sarcasm detection",
    "document": "Related Work\n\nSarcasm detection has evolved from rule-based heuristics and lexical cues to context-aware neural architectures that model user history, conversation threads, and multimodal signals. Early datasets emphasized sentence-level labels, while newer corpora capture conversational context and author metadata.\n\nBenchmarking remains challenging due to annotation subjectivity and cultural variation. The SemEval 2020 figurative language shared task standardized the evaluation metrics for sarcasm detection, encouraging consistent reporting and enabling comparisons across models. Building on this, researchers have explored pretraining objectives that better capture pragmatic cues and discourse-level incongruity.\n\nOur contribution lies in a contrastive, conversation-grounded objective that refines representations for ironic intent. We also introduce a cross-domain evaluation suite spanning multiple platforms and genres to assess robustness beyond in-domain performance.",
    "reason": "The sentence mentions a specific shared task and its impact without citing the task or providing evidence, which should be referenced at first mention.",
    "start": 407,
    "end": 513,
    "label": "Unsupported claim"
  },
  {
    "span": "Early metric-based approaches learn embedding spaces where class prototypes support rapid adaptation (Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018). Optimization-based methods adapt model parameters to new tasks with a few gradient steps (Finn et al., 2017; Nichol et al., 2018). Recent works leverage large pre-trained language models and prompt-based tuning for few-shot classification (Brown et al., 2020; Schick and Schütze, 2021; Gao et al., 2021). Data augmentation and meta-regularization techniques further improve stability under label sparsity (Mintun et al., 2021; Goldblum et al., 2020; Tian et al., 2020).",
    "document": "Introduction\nFew-shot text classification aims to induce robust classifiers from only a handful of labeled examples per class. This paradigm is attractive for domains where annotation is costly or expertise-laden, such as biomedical curation and legal analysis. Despite progress, challenges remain in generalization under distribution shift, calibration under extreme label scarcity, and stability across tasks that vary in granularity and semantics.\n\nEarly metric-based approaches learn embedding spaces where class prototypes support rapid adaptation (Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018). Optimization-based methods adapt model parameters to new tasks with a few gradient steps (Finn et al., 2017; Nichol et al., 2018). Recent works leverage large pre-trained language models and prompt-based tuning for few-shot classification (Brown et al., 2020; Schick and Schütze, 2021; Gao et al., 2021). Data augmentation and meta-regularization techniques further improve stability under label sparsity (Mintun et al., 2021; Goldblum et al., 2020; Tian et al., 2020).\n\nIn this paper, we examine the role of task descriptors as an external inductive bias for few-shot classification. We propose a dual-view meta-learner that jointly encodes class names and unlabeled support features to seed an adaptive prototype prior, improving sample efficiency and calibration under shift. We evaluate across five benchmarks with controlled domain shift and provide ablations on descriptor granularity, stability, and calibration.",
    "reason": "The span catalogs prior approaches and citations without relating them to the paper's goals, identifying a specific gap, or articulating the author's perspective; it is descriptive only.",
    "start": 452,
    "end": 1085,
    "label": "Lacks synthesis"
  },
  {
    "span": "Exploration in reinforcement learning has been approached via intrinsic rewards based on prediction error or information gain (Pathak et al., 2017; Houthooft et al., 2016), count-based or pseudo-count bonuses (Bellemare et al., 2016; Tang et al., 2017), and ensembles or bootstrapped value functions (Osband et al., 2016; Agarwal et al., 2020). In this paper, we propose a simple policy-gradient-compatible bonus that scales with uncertainty in state visitation density estimated by a learned energy model.",
    "document": "Introduction\n\nSparse rewards and deceptive local optima make exploration a core challenge in reinforcement learning (RL). Effective exploration balances novelty seeking with exploitation of discovered high-value behaviors.\n\nExploration in reinforcement learning has been approached via intrinsic rewards based on prediction error or information gain (Pathak et al., 2017; Houthooft et al., 2016), count-based or pseudo-count bonuses (Bellemare et al., 2016; Tang et al., 2017), and ensembles or bootstrapped value functions (Osband et al., 2016; Agarwal et al., 2020). In this paper, we propose a simple policy-gradient-compatible bonus that scales with uncertainty in state visitation density estimated by a learned energy model.\n\nWe evaluate across Atari and continuous control benchmarks, showing improved sample efficiency and robustness to reward sparsity.",
    "reason": "After listing prior work, the span immediately states the contribution without articulating what gap or specific limitation in existing methods the new bonus addresses (definition b).",
    "start": 224,
    "end": 730,
    "label": "Lacks synthesis"
  },
  {
    "span": "Instance weighting adjusts the training objective according to domain relevance (Wang et al., 2017). The WMT14 En-De dataset remains a standard benchmark. Bilingual lexicon induction can bootstrap terminology coverage (Artetxe et al., 2018).",
    "document": "Related Work\n\nDomain adaptation in neural machine translation (NMT) targets performance degradation when source and target domains diverge (Chu and Wang, 2018). Popular strategies include fine-tuning on in-domain data, multi-domain training with domain tags, and data selection techniques to emphasize relevant examples (Luong and Manning, 2015; Britz et al., 2017; Aharoni and Goldberg, 2020).\n\nInstance weighting adjusts the training objective according to domain relevance (Wang et al., 2017). The WMT14 En-De dataset remains a standard benchmark. Bilingual lexicon induction can bootstrap terminology coverage (Artetxe et al., 2018). Other works incorporate lexical constraints during decoding to ensure key terms are preserved in specialized domains (Post and Vilar, 2018).\n\nOur work proposes an uncertainty-aware data selection method for low-resource domains, integrating lexical priors into a multi-stage fine-tuning pipeline.",
    "reason": "The middle sentence about WMT14 as a benchmark interrupts two method-focused citations without explaining its relevance or linking it to the surrounding works, resulting in a lack of coherent transitions.",
    "start": 396,
    "end": 637,
    "label": "Coherence"
  },
  {
    "span": "Lee & Kim (2021)",
    "document": "Introduction\n\nMobile sensing systems infer user context by fusing signals from motion, location, and device usage (Lane et al., 2010; Wang et al., 2019). Recent frameworks emphasize on-device learning to reduce latency and preserve privacy (Bonawitz et al., 2019). Lee & Kim (2021) introduce a hierarchical encoder that adapts to sensor availability, reporting improvements on activity recognition. Building on this idea, we propose a multi-resolution transformer that aligns sampling rates across sensors during training.",
    "reason": "Ampersand used in a narrative citation. In APA style, narrative citations should use \"and\" (\"Lee and Kim (2021)\") while ampersands are reserved for parenthetical citations.",
    "start": 265,
    "end": 281,
    "label": "Format"
  },
  {
    "span": "In-situ AR guidance has been studied for assembly, maintenance, and training scenarios, using overlays, ghosting, and attention cues (Henderson and Feiner, 2011; Funk et al., 2017; Nee et al., 2012; Rios et al., 2018).",
    "document": "Related Work\n\nAugmented reality (AR) has been explored as a medium for procedural guidance, promising to reduce cognitive load and hands-off time by aligning instructions with the physical workspace. Prior systems vary in display modality, content design, and task complexity.\n\nIn-situ AR guidance has been studied for assembly, maintenance, and training scenarios, using overlays, ghosting, and attention cues (Henderson and Feiner, 2011; Funk et al., 2017; Nee et al., 2012; Rios et al., 2018). Recent work investigates adaptive sequencing and user modeling for just-in-time support (Hou and Wang, 2013; Bogdanovych et al., 2020). Head-mounted displays raise ergonomic and field-of-view trade-offs (Morrison et al., 2009; Kruijff et al., 2010).\n\nWe focus on non-expert task execution in constrained workcells, but we do not position our design choices relative to these prior content strategies.",
    "reason": "The span summarizes prior AR guidance techniques without explaining their relation to the authors’ constrained workcell setting or motivating a gap, thus lacking synthesis (definition a, c).",
    "start": 278,
    "end": 496,
    "label": "Lacks synthesis"
  },
  {
    "span": "Recent competitions have benchmarked graph transformers on the GraphWorld-LSC leaderboard.",
    "document": "Related Work\n\nAdvances in graph representation learning have spurred interest in architectures that better capture long-range dependencies. Graph transformers combine attention with structural priors to address limitations of message passing in deep regimes. Recent competitions have benchmarked graph transformers on the GraphWorld-LSC leaderboard. These events highlight scalability and inductive biases as key determinants of performance on large-scale node and graph classification.\n\nOur contribution examines structure-aware pretraining objectives that improve data efficiency and transfer across graph domains with different sparsity profiles.",
    "reason": "Unsupported claim because it mentions competitions and a leaderboard without providing citations to the events or reports (definition a and d).",
    "start": 259,
    "end": 349,
    "label": "Unsupported claim"
  },
  {
    "span": "Garcia and Patel, 2017)",
    "document": "Related Work\n\nDetecting adversarial text has drawn on statistical cues, uncertainty estimates, and representation geometry. Statistical detectors model shifts in token distributions (Mendes and Roy, 2018) and syntax patterns (Olson et al., 2020). Confidence-based screening leverages calibration and predictive entropy (Ferrer and Zhu, 2019). Geometry-driven approaches estimate local density and curvature in embedding space, revealing off-manifold perturbations, as shown by Garcia and Patel, 2017) in recurrent networks and extended by Wu et al. (2021) to transformers. Recent methods incorporate paraphrase consistency checks (Ilyas and Chen, 2022) and masked-token repair (Rahman et al., 2022). Our work combines neighborhood consistency with gradient-based salience to flag brittle predictions without retraining the base model.",
    "reason": "Missing opening parenthesis in a parenthetical citation; should be '(Garcia and Patel, 2017)'.",
    "start": 477,
    "end": 500,
    "label": "Format"
  },
  {
    "span": "Early graph transformers consistently outperform message-passing networks on all large-scale benchmarks.",
    "document": "Related Work\n\nGraph Representation Learning\n\nMessage-passing neural networks have established strong performance on molecular property prediction, node classification, and link prediction by iteratively aggregating neighborhood information (Gilmer et al., 2017; Hamilton et al., 2017). However, oversmoothing and limited receptive fields have motivated architectures that incorporate global attention and positional encodings (Ying et al., 2021). Early graph transformers consistently outperform message-passing networks on all large-scale benchmarks. Hybrid models combine attention with local aggregation to balance expressivity and scalability (Dwivedi et al., 2021). Nevertheless, training stability and inductive bias selection for sparse graphs remain active research questions.",
    "reason": "Makes a broad comparative performance claim about prior work without citations or evidence; such a niche claim requires references per rule b.",
    "start": 447,
    "end": 551,
    "label": "Unsupported claim"
  },
  {
    "span": "The MIMIC-CXR dataset contains 377k radiology reports and is the de facto benchmark for report generation.",
    "document": "Introduction\n\nAutomating radiology report generation requires models to capture fine-grained clinical findings, temporal comparisons, and uncertainty expressions. Vision-language methods have improved caption quality, yet clinical correctness often lags behind fluency.\n\nThe MIMIC-CXR dataset contains 377k radiology reports and is the de facto benchmark for report generation. Despite its scale, label noise and reporting conventions introduce challenges for factuality, motivating clinically aware decoding constraints.\n\nWe propose a structured generation approach that first predicts a set of clinical observations and then composes a report under radiology-specific style guidelines.",
    "reason": "States a specific dataset, exact size, and its benchmark status without any citation, violating rule (a) and (b).",
    "start": 271,
    "end": 377,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works demonstrate that audio transformers surpass CNNs on non-speech acoustic event detection",
    "document": "Introduction\n\nAcoustic event detection (AED) aims to identify and localize events in complex auditory scenes, often under weak labeling and noisy conditions. Traditional CNN-based systems rely on log-mel spectrograms and temporal pooling, while newer approaches adapt transformer architectures to capture long-range dependencies. Recent works demonstrate that audio transformers surpass CNNs on non-speech acoustic event detection, suggesting that global context modeling is critical for rare, overlapping events.\n\nDespite these gains, transformers are compute-intensive and require careful regularization to avoid overfitting when labels are weak. Moreover, robust handling of domain shift across recording devices and environments remains a challenge.\n\nWe introduce a hybrid conformer-style model with structured sparsity and multi-scale tokenization, achieving improved efficiency without sacrificing accuracy on weakly labeled AED benchmarks.\n",
    "reason": "Mentions 'recent works' making a comparative performance claim without citing any papers, violating rule d.",
    "start": 330,
    "end": 430,
    "label": "Unsupported claim"
  },
  {
    "span": "Hu et al. (2008) formulate implicit feedback matrix factorization. He et al. (2017) propose Neural CF with MLP interactions. Rendle (2010) introduces BPR for pairwise ranking with implicit data. Covington et al. (2016) describe YouTube's deep candidate generation.",
    "document": "Related Work\n\nRecommendation models for implicit feedback have evolved from linear factorization to non-linear deep architectures. Despite progress, challenges persist in long-tail recommendation and cold-start scenarios, motivating models that can leverage side information and self-supervision.\n\nHu et al. (2008) formulate implicit feedback matrix factorization. He et al. (2017) propose Neural CF with MLP interactions. Rendle (2010) introduces BPR for pairwise ranking with implicit data. Covington et al. (2016) describe YouTube's deep candidate generation.\n\nIn contrast, our approach integrates graph-based contrastive learning to propagate sparse behavior signals and regularize item embeddings, improving ranking under data sparsity.",
    "reason": "Papers are listed sequentially without transitions or explicit statements connecting their contributions or situating them along a shared dimension (e.g., objective functions or architectures). The lack of connective tissue causes coherence issues across multiple sentences (criterion a, b, c).",
    "start": 298,
    "end": 562,
    "label": "Coherence"
  },
  {
    "span": "BERT was used in an AES task trained on essays from the ASAP dataset with pairwise ranking objectives.",
    "document": "Related Work\n\nAutomated essay scoring (AES) aims to predict holistic or trait scores assigned by human raters. Traditional approaches rely on engineered features capturing grammar, coherence, and lexical sophistication, while neural methods learn representations directly from text.\n\nPublicly available corpora, including the ASAP dataset, enable comparative evaluation and have spurred interest in transfer learning and domain robustness. Recent neural AES work explores pre-trained language models and learning-to-rank formulations to better capture relative quality differences.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with pairwise ranking objectives. While such approaches show promise, they can be sensitive to prompt leakage and score scale shifts, highlighting the importance of careful cross-prompt validation.",
    "reason": "Describes a specific prior setup (model, dataset, objective) without citing the corresponding study that introduced or evaluated it (rule a and b; matches example iii).",
    "start": 583,
    "end": 685,
    "label": "Unsupported claim"
  },
  {
    "span": "Beyond classical Granger causality tests, recent methods leverage sparse VAR, nonparametric kernels, and neural Granger models (Arnold et al., 2007; Tank et al., 2018; Xu et al., 2018). Temporal causal discovery with attention and continuous-time flows has also been proposed (Zheng et al., 2020; Löwe et al., 2022).",
    "document": "Related Work\n\nCausal discovery in multivariate time series aims to recover directed dependencies that drive system dynamics. Real-world applications such as energy grids and healthcare introduce irregular sampling, interventions, and latent confounding that violate standard assumptions.\n\nBeyond classical Granger causality tests, recent methods leverage sparse VAR, nonparametric kernels, and neural Granger models (Arnold et al., 2007; Tank et al., 2018; Xu et al., 2018). Temporal causal discovery with attention and continuous-time flows has also been proposed (Zheng et al., 2020; Löwe et al., 2022).\n\nHowever, most approaches presume fully observed systems and uniform sampling. Our method combines invariant risk minimization with interval-censored likelihoods to remain robust under missingness and irregular grids.",
    "reason": "The span summarizes prior techniques but does not connect them to the paper’s concerns (latent confounding, irregular sampling) or identify a gap, thus lacking synthesis per (a) and (c).",
    "start": 289,
    "end": 605,
    "label": "Lacks synthesis"
  },
  {
    "span": "(Lee et. al., 2017)",
    "document": "Related Work\n\nSession-based recommendation leverages sequential models to capture evolving user intent (Hidasi et al., 2016; Quadrana et al., 2017). Graph-based encoders propagate signals across item co-occurrence graphs to address sparsity (Wu et al., 2019; Wang et al., 2020). Hybrid models that combine content and sequence features have shown promise (Kang and McAuley, 2018; Sun et al., 2019). A line of work explores self-attention for session modeling (Lee et. al., 2017), but evaluation often overlooks cold-start scenarios. We present a split protocol that stresses novelty.",
    "reason": "Incorrect punctuation in 'et al.': written as 'et. al.'; should be 'et al.' in the citation.",
    "start": 459,
    "end": 478,
    "label": "Format"
  },
  {
    "span": "Recent competitions have shown steady progress on code generation from natural language.",
    "document": "Introduction\n\nTranslating natural language specifications into executable code has attracted substantial interest due to applications in developer assistance and education. Progress is often tracked through public leaderboards and community challenges that encourage standardized tasks and evaluation.\n\nRecent competitions have shown steady progress on code generation from natural language. Yet, improvements are not uniform across programming languages and problem categories, and evaluation often overlooks runtime correctness or security aspects beyond unit tests.\n\nWe propose a multilingual benchmark with diverse specifications and rigorous execution-time checks, aiming to provide a more comprehensive picture of model capabilities.",
    "reason": "Mentions 'recent competitions' and claims progress without citing any specific competitions, reports, or leaderboards to substantiate the statement.",
    "start": 303,
    "end": 391,
    "label": "Unsupported claim"
  },
  {
    "span": "Prior work generally agrees that character-level models are superior for agglutinative languages.",
    "document": "Related Work\n\nSubword and character modeling. Byte-pair encoding and unigram LM segmentations improve robustness to rare words in morphologically rich languages (Sennrich et al., 2016; Kudo, 2018). Character-level encoders capture productive morphology but can suffer from longer sequences and optimization challenges (Kim et al., 2016; Cherry et al., 2018). Prior work generally agrees that character-level models are superior for agglutinative languages. Hybrid schemes combining characters with morpheme-aware units attempt to balance capacity and efficiency (Bojanowski et al., 2017; Heinzerling and Strube, 2018).\n",
    "reason": "Makes a broad consensus claim about prior literature without citing specific supporting studies; per (b) niche-topic claims should be backed by citations.",
    "start": 359,
    "end": 456,
    "label": "Unsupported claim"
  },
  {
    "span": "(Kim, 2019 and Patel, 2020)",
    "document": "Related Work\n\nMultisource pretraining seeks to leverage heterogeneous corpora by balancing domain influence and mitigating negative transfer (Gururangan et al., 2020; Pfeiffer et al., 2021). Prior work (Kim, 2019 and Patel, 2020) treats domain mixtures as a curriculum, while adapter-based methods offer modularity for continual updates (Houlsby et al., 2019). Yet, selecting sources dynamically at scale remains underexplored.",
    "reason": "Improper separator within multiple parenthetical citations; 'and' should be replaced by a semicolon, e.g., '(Kim, 2019; Patel, 2020)'.",
    "start": 202,
    "end": 229,
    "label": "Format"
  },
  {
    "span": "Data augmentation for ASR includes speed perturbation, SpecAugment masking, noise and reverberation simulation, and codec or bandwidth variations (Ko et al., 2015; Park et al., 2019; Kim et al., 2018; Cui et al., 2015). Recent work explores text-only augmentation via TTS and pseudo-labeling (Kahn et al., 2020; Park et al., 2020).",
    "document": "Related Work\n\nAcoustic Modeling\nModern ASR systems rely on large-scale acoustic and linguistic data. Generalization across speakers and environments is often improved through augmentation and semi-supervised learning.\n\nData Augmentation Techniques\nData augmentation for ASR includes speed perturbation, SpecAugment masking, noise and reverberation simulation, and codec or bandwidth variations (Ko et al., 2015; Park et al., 2019; Kim et al., 2018; Cui et al., 2015). Recent work explores text-only augmentation via TTS and pseudo-labeling (Kahn et al., 2020; Park et al., 2020).\n\nSemi-Supervised Learning\nNoisy student training and self-training extend labeled datasets with model-generated hypotheses. Confidence filtering and consistency regularization are common to reduce error propagation.\n\nOur Perspective\nWe consider augmentation-selection under compute budgets and focus on sample-efficient policies that adapt to domain shifts.",
    "reason": "The span lists augmentation methods without relating them to the paper's objectives or explaining comparative trade-offs, thus lacking synthesis per (a) and (c).",
    "start": 248,
    "end": 579,
    "label": "Lacks synthesis"
  },
  {
    "span": "The WMT18 shared task introduced significant expansions to low-resource language tracks.",
    "document": "Related Work in Neural Machine Translation\n\nNeural machine translation (NMT) has advanced rapidly with encoder-decoder architectures and attention mechanisms, but persistent gaps remain for low-resource language pairs. Shared tasks have played an important role in standardizing evaluation and fostering community progress. The WMT18 shared task introduced significant expansions to low-resource language tracks. These tracks spurred interest in transfer learning and multilingual pretraining strategies that leverage related languages to bootstrap performance.\n\nSubsequent efforts have focused on domain adaptation, synthetic data generation through back-translation, and active learning to maximize the utility of scarce parallel corpora. While multilingual models can transfer representations across typologically related languages, data imbalance and script variation still present challenges for consistent gains.\n\nIn this paper, we study a curriculum-based sampling approach that emphasizes high-quality sentence pairs early in training and gradually introduces noisier synthetic data. We evaluate across several low-resource pairs with varying degrees of relatedness to high-resource anchors and report improvements in BLEU and chrF with stable training dynamics.",
    "reason": "This sentence mentions a specific shared task (WMT18) and a concrete change (expansions to low-resource tracks) without a citation, violating rule (a) for first mention of a shared task.",
    "start": 324,
    "end": 412,
    "label": "Unsupported claim"
  },
  {
    "span": "There are many recent works that explore prompt-based learning for ABSA.",
    "document": "Introduction\n\nAspect-based sentiment analysis (ABSA) aims to identify fine-grained sentiments expressed towards specific aspects within a text. Early neural approaches modeled ABSA as sequence labeling or span extraction (Wang et al., 2016; Ma et al., 2017), while transformer-based methods achieved strong gains by leveraging contextualized representations (Devlin et al., 2019; Liu et al., 2019). Datasets from SemEval tasks popularized standardized evaluation, fostering research into cross-domain robustness (Pontiki et al., 2014, 2015, 2016).\n\nRecently, prompt-based and instruction-following paradigms have emerged as alternatives to conventional supervised fine-tuning. There are many recent works that explore prompt-based learning for ABSA. However, it remains unclear how prompt design, verbalizers, and label space granularity interact under domain shift.\n\nIn this paper, we revisit ABSA through a prompt calibration lens. We analyze how template choice, verbalizer selection, and calibration strategies affect performance across domains and aspect taxonomies. We also provide a unified evaluation across SemEval benchmarks and an out-of-domain restaurant review corpus to assess generalization.",
    "reason": "Mentions unspecified 'recent works' without citing them (rule d). A first mention of specific prior studies requires citations.",
    "start": 677,
    "end": 749,
    "label": "Unsupported claim"
  },
  {
    "span": "In the EmotiW 2019 challenge, organizers introduced a cross-lingual split and most top systems adopted temporal attention to fuse frames.",
    "document": "Introduction\n\nMultimodal emotion recognition in the wild focuses on extracting affective signals from unconstrained audio-visual data. Despite rapid gains from deep architectures, progress remains uneven across languages, domains, and capture conditions. Community benchmarks and challenges have played an important role in consolidating evaluation practices and encouraging reproducibility. In the EmotiW 2019 challenge, organizers introduced a cross-lingual split and most top systems adopted temporal attention to fuse frames. This observation motivates our study of cross-domain and cross-lingual generalization strategies that do not rely on large-scale labeled data.\n\nRelated Work\n\nEarly approaches emphasized hand-crafted facial descriptors and prosodic cues, which were gradually replaced by convolutional and recurrent encoders that jointly exploit spatial and temporal structure. Recent attention-based fusion schemes aim to weight salient segments adaptively across modalities. However, it remains unclear which attention mechanisms are robust under language shifts and variable speaking styles. Our work examines modality dropout and alignment-free fusion to mitigate domain shift, while comparing late- and intermediate-fusion strategies under limited supervision.",
    "reason": "Mentions a specific challenge, a novel split, and what top systems did without any citation to the challenge overview or winning system reports.",
    "start": 392,
    "end": 529,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent competitions have shown that retrieval-augmented generation dominates closed-book QA.",
    "document": "Introduction\n\nOpen-domain question answering systems can answer by recalling from parametric memory or by consulting external corpora at inference time. Retrieval-augmented generation integrates evidence selection with answer synthesis, promising improved accuracy and verifiability. Recent competitions have shown that retrieval-augmented generation dominates closed-book QA. However, performance depends strongly on retrieval coverage, latency, and the calibration of abstentions in the face of uncertainty.\n\nRelated Work\n\nHybrid pipelines align dense and sparse retrieval signals, while iterative reranking aims to refine evidence selection. Work on verifiability introduces citation requirements and faithfulness checks to mitigate unsupported assertions in generated answers.\n\nOur Contributions\n\nWe propose a budget-aware retrieval controller that optimizes the cost–accuracy trade-off. We also introduce a grounded evaluation protocol that penalizes unsupported answers and rewards concise, cited responses.",
    "reason": "References \"recent competitions\" and draws a comparative conclusion without citing the competitions or reports, violating rule (d).",
    "start": 284,
    "end": 376,
    "label": "Unsupported claim"
  },
  {
    "span": "(Chen and Li, 2018",
    "document": "Related Work\n\nGraph neural networks for recommendation leverage message passing across user–item graphs to encode collaborative signals (Hamilton et al., 2017; Ying et al., 2018; Wu et al., 2020). Early hybrid models (Chen and Li, 2018 integrated content features into neighborhood aggregation but suffered from oversmoothing on dense subgraphs. Subsequent work proposed contrastive objectives to regularize embeddings (He et al., 2020; Wang et al., 2021), while others explored disentangling user intents (Ma et al., 2019). Despite these advances, cold-start robustness and cross-domain transfer remain open challenges.",
    "reason": "The parenthetical citation is missing the closing parenthesis, resulting in malformed citation formatting.",
    "start": 217,
    "end": 235,
    "label": "Format"
  },
  {
    "span": "Work on multilingual and cross-lingual summarization spans supervised training on mBART-style encoders, translationese pipelines that translate then summarize or vice versa, and multilingual knowledge distillation from high-resource languages (Liu et al., 2020; Tang et al., 2021; Fernandes et al., 2022; Khan and Aziz, 2023). Additional efforts explore cross-lingual content selection and coverage constraints to reduce hallucination (Goyal et al., 2021; Ortega et al., 2022).",
    "document": "Related Work\n\nAbstractive summarization has progressed rapidly with pre-trained sequence-to-sequence models; nevertheless, low-resource languages remain underserved due to scarce parallel data and inconsistent evaluation practices.\n\nWork on multilingual and cross-lingual summarization spans supervised training on mBART-style encoders, translationese pipelines that translate then summarize or vice versa, and multilingual knowledge distillation from high-resource languages (Liu et al., 2020; Tang et al., 2021; Fernandes et al., 2022; Khan and Aziz, 2023). Additional efforts explore cross-lingual content selection and coverage constraints to reduce hallucination (Goyal et al., 2021; Ortega et al., 2022).\n\nOur method introduces a consistency regularization framework that aligns cross-lingual predictions under back-translation perturbations. We provide experiments across five language pairs and conduct analyses on factuality and robustness.",
    "reason": "The span summarizes existing methods without clarifying how they relate to the proposed consistency regularization approach or what specific shortcomings remain (criterion a/c).",
    "start": 233,
    "end": 710,
    "label": "Lacks synthesis"
  },
  {
    "span": "Singh and Rao (2018) introduced matrix factorization with demographic constraints. Howard et al. (2020) analyzed exposure bias in top-k ranking. Li et al. (2022) studied calibration of recommendation probabilities. Ortiz and Chen (2021) examined long-tail coverage optimization.",
    "document": "Related Work\n\nFairness and calibration in recommender systems have attracted growing attention due to concerns about systemic bias and exposure disparities. Research spans modeling choices, ranking objectives, and evaluation protocols.\n\nSingh and Rao (2018) introduced matrix factorization with demographic constraints. Howard et al. (2020) analyzed exposure bias in top-k ranking. Li et al. (2022) studied calibration of recommendation probabilities. Ortiz and Chen (2021) examined long-tail coverage optimization.\n\nSeveral benchmark suites and simulation environments have been proposed to study user feedback loops (Bretz and Xu, 2023). Our work focuses on exposure-aware calibration that couples ranking with uncertainty estimates.",
    "reason": "The span presents four separate works as standalone sentences without transitions or explicit links, leaving the reader to infer connections between constraints, exposure, calibration, and coverage; this weakens coherence.",
    "start": 237,
    "end": 515,
    "label": "Coherence"
  },
  {
    "span": "There are many recent works that demonstrate that audio features are no longer necessary for robust multimodal sentiment models.",
    "document": "Introduction\n\nMultimodal sentiment analysis (MSA) aims to leverage text, audio, and vision to infer opinions. Prior work has proposed fusion mechanisms and modality-specific encoders (Lee and Park, 2019; Alvarez et al., 2020). However, the contribution of each modality remains debated. There are many recent works that demonstrate that audio features are no longer necessary for robust multimodal sentiment models. At the same time, some studies argue that prosody is essential when textual signals are ambiguous (Chen et al., 2021). In this paper we revisit modality utility under stronger text encoders and balanced evaluation.",
    "reason": "Mentions 'recent works' to support a claim without providing any citations (rule d).",
    "start": 287,
    "end": 415,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works have shown that end-to-end models surpass hybrid HMM/DNN systems on far-field speech.",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) has shifted from hybrid HMM/DNN pipelines to end-to-end models such as CTC, RNN-Transducer, and attention-based encoder–decoders (Graves et al., 2006; Chan et al., 2016; Graves, 2012). Far-field ASR introduces reverberation and noise challenges; front-end enhancement and beamforming have been integrated with neural back-ends to improve robustness (Heymann et al., 2016; Erdogan et al., 2015).\n\nRecent works have shown that end-to-end models surpass hybrid HMM/DNN systems on far-field speech. However, training stability and data requirements remain obstacles, particularly under limited multi-microphone data. We propose a self-supervised pretraining scheme with synthetic reverberation augmentation to bridge the gap between near-field corpora and far-field target conditions.\n\nWe report word error rates across simulated and real far-field benchmarks and analyze robustness to room impulse response mismatch.",
    "reason": "The sentence references 'recent works' claiming superiority without providing citations, which should be included at first mention (violates rule d).",
    "start": 445,
    "end": 543,
    "label": "Unsupported claim"
  },
  {
    "span": "(Ortiz, 2018, Park, 2019)",
    "document": "Related Work\n\nTime-series anomaly detection includes reconstruction-based autoencoders (Lee and Gupta, 2017), probabilistic forecasting residuals (Nguyen and Silva, 2019), and change-point detection via online segmentation (Rao and Mendes, 2020). Recent transformer models capture long-range dependencies but risk overfitting under scarce anomalies (Iyer and Chen, 2021). Prior work compared windowed and streaming inference (Ortiz, 2018), and explored seasonal-trend decomposition to reduce false positives (Park, 2019). Several studies jointly model trends and events (Ferrer and Liu, 2020; (Ortiz, 2018, Park, 2019)). We build on these insights with a hybrid residual detector that combines robust forecasting with event-conditioned likelihood ratios.",
    "reason": "Incorrect separator between multiple citations inside parentheses; separate distinct works with a semicolon: '(Ortiz, 2018; Park, 2019)'.",
    "start": 593,
    "end": 618,
    "label": "Format"
  },
  {
    "span": "Recent competitions demonstrate that model-based agents dominate Atari benchmarks.",
    "document": "Related Work\n\nDeep reinforcement learning on the Atari 2600 suite has long been a testbed for algorithmic progress (Mnih et al., 2015; Hessel et al., 2018). Model-free methods dominate earlier leaderboards, while model-based approaches have recently shown promise via learned dynamics models and planning (Schrittwieser et al., 2020; Hafner et al., 2021). Community benchmarks and competitions have standardized evaluation under varying data budgets and human-normalized scores (Machado et al., 2018). Recent competitions demonstrate that model-based agents dominate Atari benchmarks. Nevertheless, reproducibility challenges and differences in compute budgets complicate head-to-head comparison.\n\nWe present a sample-efficient hybrid agent with uncertainty-aware planning and offline pretraining, evaluated under fixed interaction budgets to ensure fair comparisons.",
    "reason": "Asserts outcomes of 'recent competitions' without citing the competitions, which should be referenced.",
    "start": 502,
    "end": 584,
    "label": "Unsupported claim"
  },
  {
    "span": "Eye-tracking offers insights into users' attention and cognitive load (Duchowski, 2017). Gesture-based interfaces reduce reliance on keyboards in constrained spaces (Wigdor and Wixon, 2011). Wearable AR headsets present situated instructions to technicians (Azuma, 1997). Crowd-powered systems scale moderation in online communities (Kittur et al., 2013).",
    "document": "Related Work\n\nHuman–computer interaction for productivity support\nA broad range of interaction modalities and systems have been explored to improve users' efficiency and reduce cognitive overhead during complex tasks. While our work focuses on lightweight guidance for documentation-heavy workflows, prior art spans sensing, novel input channels, and collaborative systems.\n\nEye-tracking offers insights into users' attention and cognitive load (Duchowski, 2017). Gesture-based interfaces reduce reliance on keyboards in constrained spaces (Wigdor and Wixon, 2011). Wearable AR headsets present situated instructions to technicians (Azuma, 1997). Crowd-powered systems scale moderation in online communities (Kittur et al., 2013).\n\nWe instead investigate inline micro-tutorials triggered by intent signals learned from context, requiring no specialized hardware.",
    "reason": "The span enumerates four disparate HCI areas without transitions or an explicit explanation of how they relate to each other or to the paper’s focus, causing abrupt topic shifts and weak coherence.",
    "start": 375,
    "end": 730,
    "label": "Coherence"
  },
  {
    "span": "there are many recent works that explore this topic",
    "document": "Related Work\n\nAbstractive Summarization of Scientific Articles Research on neural abstractive summarization has progressed rapidly with sequence-to-sequence models and pretrained transformers (See et al., 2017; Gehrmann et al., 2018; Lewis et al., 2020). For scientific articles, domain-specific encoders and citation-aware decoders have been proposed to better capture long-range structure (Cohan et al., 2018; Yasunaga et al., 2019). In addition to classic extractive approaches, there are many recent works that explore this topic. However, existing methods often struggle with section-level coherence and factual consistency across lengthy inputs. Our work builds on hierarchical attention and long-context transformers to address these challenges.\n\nDatasets for Scientific Summarization Prior datasets such as PubMed and arXiv have enabled progress on long-form summarization (Cohan et al., 2018), while newer collections emphasize domain diversity and metadata such as citation graphs (Wang et al., 2021). Despite improvements in scale, benchmarks still underrepresent methodological sections and negative results, which we aim to capture with our new dataset.",
    "reason": "The claim mentions 'many recent works' without providing citations, violating the requirement that mentions of recent works be supported by references.",
    "start": 482,
    "end": 533,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works have proposed detoxification methods based on contrastive decoding",
    "document": "Related Work\n\nLarge language models can inadvertently generate toxic or unsafe content (Gehman et al., 2020; Xu et al., 2021). Safety interventions include data filtering, adversarial training, and decoding-time controls (Dathathri et al., 2020; Krause et al., 2021). Recent works have proposed detoxification methods based on contrastive decoding. Other approaches leverage attribute models or reinforcement learning with human feedback to reduce harmful outputs (Ziegler et al., 2019; Bai et al., 2022). Our method complements decoding controls by adding a content-aware rejection sampler.",
    "reason": "Uses 'recent works' phrasing without citing any of those works.",
    "start": 268,
    "end": 347,
    "label": "Unsupported claim"
  },
  {
    "span": "((Kaur and Lin, 2020))",
    "document": "Related Work\n\nMultimodal fusion techniques span early concatenation (Chandra and Bose, 2018), bilinear pooling (Ibrahim and Noor, 2019), and transformer-based cross-attention (Vega and Holland, 2021). Robustness to missing modalities has been addressed with gating and modality dropout (Shah and Arora, 2020). Recent surveys summarize advances in vision–language pretraining (Mehta et al., 2022) and audio–visual speech (Quinn and Diaz, 2021). A line of work claims late fusion is competitive when alignment is weak ((Kaur and Lin, 2020)), whereas tightly coupled objectives excel under synchronized cues (Ortiz and Wang, 2021). Our approach bridges these views by adaptively weighting fusion depth based on estimated alignment confidence.",
    "reason": "Extraneous double parentheses around a citation; should be a single pair '(Kaur and Lin, 2020)'.",
    "start": 516,
    "end": 538,
    "label": "Format"
  },
  {
    "span": "to the best of our knowledge, the first to jointly learn topology and attributes for dynamic heterogeneous graphs",
    "document": "Related Work\n\nGraph representation learning has progressed from static homogeneous settings to dynamic, multi-relation graphs. Existing methods either assume fixed topology while updating node states or learn evolving structure with hand-crafted temporal kernels. Our approach is, to the best of our knowledge, the first to jointly learn topology and attributes for dynamic heterogeneous graphs, enabling end-to-end training with intermittent supervision. We compare against strong baselines in temporal link prediction and time-stamped node classification.",
    "reason": "Makes a novelty/firstness claim about prior work without citing evidence or surveying alternatives, thus unsupported under (b).",
    "start": 281,
    "end": 394,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors showed that chrF++ correlates better with human judgments than BLEU on morphologically rich languages.",
    "document": "Related Work\n\nAutomatic evaluation metrics for machine translation have traditionally relied on surface-form overlap, with BLEU remaining widely reported due to its simplicity and historical adoption. Nevertheless, character-based and embedding-based metrics have gained traction for better sensitivity to morphology and paraphrasing.\n\nIn a previous study, the authors showed that chrF++ correlates better with human judgments than BLEU on morphologically rich languages. Other strands of work propose contextualized metrics that compare semantic similarity in representation space and attempt to approximate direct assessment.\n\nDespite varied options, agreement with human evaluation is context-dependent, and metric reliability can drift under domain shift, low-resource conditions, and noisy references. We investigate how correlation profiles change under different tokenization strategies and translationese effects.",
    "reason": "References a specific prior study and its findings without citing the paper, which is required for claims about prior work.",
    "start": 336,
    "end": 471,
    "label": "Unsupported claim"
  },
  {
    "span": "UNet variants use dense skip connections, attention gates, or multi-scale decoders to improve segmentation quality (Ronneberger et al., 2015; Oktay et al., 2018; Zhou et al., 2019). We build upon this line of work.",
    "document": "Introduction\n\nSemantic segmentation in medical imaging is critical for diagnosis and treatment planning. Despite substantial gains from convolutional architectures, challenges persist in delineating small structures and coping with scanner variability. UNet variants use dense skip connections, attention gates, or multi-scale decoders to improve segmentation quality (Ronneberger et al., 2015; Oktay et al., 2018; Zhou et al., 2019). We build upon this line of work. Prior benchmarks have highlighted the role of data augmentation and normalization in cross-site generalization (Isensee et al., 2021; Zhou et al., 2021).\n\nOur method targets organ boundary refinement in multi-center CT data.",
    "reason": "The span summarizes prior work and then states \"We build upon this line of work\" without explicitly articulating the specific gap or how the contribution addresses it (definition b) and lacks explicit author perspective (definition c).",
    "start": 253,
    "end": 467,
    "label": "Lacks synthesis"
  },
  {
    "span": "Several recent works release de-identified Spanish clinical corpora for NER.",
    "document": "Introduction\n\nClinical named entity recognition (NER) has benefited from domain-specific pretraining and weak supervision strategies to cope with limited annotated data (Habibi et al., 2017; Beltagy et al., 2019). While English resources are comparatively abundant, multilingual clinical NLP remains underresourced, hindering cross-lingual transfer and evaluation.\n\nSeveral recent works release de-identified Spanish clinical corpora for NER. Despite this progress, annotation guidelines vary widely, complicating direct model comparisons across datasets. We introduce a harmonized schema and report standardized benchmarks across institutions.",
    "reason": "Asserts the existence of multiple recent Spanish clinical NER corpora without citing any of them; mentions of datasets and recent works require citations.",
    "start": 366,
    "end": 442,
    "label": "Unsupported claim"
  },
  {
    "span": "Graph-based fraud detection has adopted message-passing neural networks to propagate relational signals across users, devices, and merchants (Kipf and Welling, 2017; Hamilton et al., 2017). Heterogeneous graph learning captures multi-typed nodes and edges for richer semantics (Wang et al., 2019; Hu et al., 2020). Temporal graph architectures model evolving interactions to track dynamic fraud patterns (Rossi et al., 2020; Xu et al., 2020). Contrastive pretraining augments representation robustness under label sparsity (You et al., 2020; Qiu et al., 2020).",
    "document": "Related Work\nDetecting fraud in large-scale financial platforms requires leveraging relational structures spanning accounts, devices, IPs, and transactions. Graph learning offers a natural substrate for modeling such dependencies, particularly under partial labels and rapidly shifting adversarial behavior. Key desiderata include long-range relational reasoning, temporal awareness, and robustness to distributional shift.\n\nGraph-based fraud detection has adopted message-passing neural networks to propagate relational signals across users, devices, and merchants (Kipf and Welling, 2017; Hamilton et al., 2017). Heterogeneous graph learning captures multi-typed nodes and edges for richer semantics (Wang et al., 2019; Hu et al., 2020). Temporal graph architectures model evolving interactions to track dynamic fraud patterns (Rossi et al., 2020; Xu et al., 2020). Contrastive pretraining augments representation robustness under label sparsity (You et al., 2020; Qiu et al., 2020).\n\nIndustry practice further integrates rule systems and manual investigation, but reconciling symbolic and learned signals remains challenging. Our work focuses on calibrating graph anomaly scores across time using a shift-aware teacher-student training scheme, yielding improved precision at low false-positive rates.",
    "reason": "The span lists categories of existing methods and citations but does not explain their limitations or how they connect to the present study; it lacks synthesis and author stance.",
    "start": 425,
    "end": 985,
    "label": "Lacks synthesis"
  },
  {
    "span": "O'Neil et.al. (2016)",
    "document": "Introduction\n\nFairness in machine learning addresses disparities that arise when models systematically disadvantage protected groups. Definitions of fairness vary—ranging from demographic parity to equalized odds—reflecting different normative commitments (Narayanan and Rao, 2019; Verma and Rubin, 2018).\n\nAuditing practices combine dataset diagnostics with counterfactual evaluation to identify sources of bias in features and labels. Prior critiques argue that focusing solely on predictive parity can obscure harms that manifest in allocation contexts, and propose multi-metric reporting to mitigate cherry-picking. Building on O'Neil et.al. (2016), we emphasize the broader social impacts of automated decision-making and the need for stakeholder involvement throughout the ML lifecycle.\n\nWe contribute a measurement framework that jointly assesses allocation, quality of service, and error distribution across subpopulations.",
    "reason": "Punctuation error in citation: 'et.al.' should be 'et al.' with a space and without the extra period after 'al'. Correct narrative form is O'Neil et al. (2016).",
    "start": 632,
    "end": 652,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claim that domain-adaptive pretraining yields a 5–8% accuracy gain on BioASQ.",
    "document": "Related Work\n\nPretrained language models have been adapted to biomedical question answering via continued pretraining on in-domain corpora and specialized vocabularies. Several strategies have been proposed, including masked language modeling on PubMed abstracts and entity-aware objectives tuned for biomedical nomenclature.\n\nIn a previous study, the authors claim that domain-adaptive pretraining yields a 5–8% accuracy gain on BioASQ. Other works emphasize the importance of specialized tokenization to capture rare biomedical terms and abbreviations. However, the degree to which these gains transfer across QA formats (factoid vs. list vs. yes/no) is less clear, and systematic comparisons remain limited.\n\nOur approach revisits domain-adaptive pretraining with a focus on sample efficiency, aiming to reduce the volume of in-domain text required to realize competitive gains.",
    "reason": "Mentions a specific prior study and quantitative results on a named dataset without any citation, violating rule (a) and (b).",
    "start": 327,
    "end": 437,
    "label": "Unsupported claim"
  },
  {
    "span": "A prior study showed that adjusting for proxies eliminates confounding in offline policy evaluation.",
    "document": "Related Work\n\nOffline reinforcement learning seeks to learn policies from logged data without further environment interaction, with techniques addressing extrapolation error and distributional shift (Levine et al., 2020; Kumar et al., 2020). Causal perspectives emphasize confounding, selection bias, and identifiability conditions for valid off-policy evaluation (OPE) (Bareinboim and Pearl, 2016; Uehara et al., 2020).\n\nMethods incorporating propensity correction, marginalized importance sampling, and learned models aim to reduce bias in OPE under various assumptions (Thomas and Brunskill, 2016; Jin et al., 2021). A prior study showed that adjusting for proxies eliminates confounding in offline policy evaluation. We revisit this claim by characterizing identifiability with imperfect proxies and proposing a two-stage estimator with sensitivity analysis.",
    "reason": "It references a specific 'prior study' and attributes a concrete finding without any citation to that study.",
    "start": 620,
    "end": 720,
    "label": "Unsupported claim"
  },
  {
    "span": "Spatial–temporal forecasting has been addressed with diffusion convolution, Chebyshev filters, attention-based GNNs, and dynamic graph learning (Li et al., 2018; Wu et al., 2019; Guo et al., 2019; Bai et al., 2020; Yu et al., 2018).",
    "document": "Related Work\n\nTraffic forecasting models increasingly rely on graph neural networks (GNNs) to capture spatial correlations and temporal dynamics across sensor networks. Research has focused on architectural expressiveness, dynamic connectivity, and long-horizon stability.\n\nSpatial–temporal forecasting has been addressed with diffusion convolution, Chebyshev filters, attention-based GNNs, and dynamic graph learning (Li et al., 2018; Wu et al., 2019; Guo et al., 2019; Bai et al., 2020; Yu et al., 2018). Parallel work explores sequence models such as temporal convolutions and transformers (Bai et al., 2018; Zhou et al., 2021). Other efforts incorporate exogenous signals like weather and events (Hoang et al., 2016; Xu et al., 2020).\n\nOur study considers multi-resolution inputs but does not attempt to unify these design axes.",
    "reason": "The span lists categories and citations without clarifying how they connect to the authors’ multi-resolution focus or identifying a specific gap, thus lacking synthesis (definition a, b, c).",
    "start": 274,
    "end": 506,
    "label": "Lacks synthesis"
  },
  {
    "span": "It is widely accepted that sim-to-real transfer is primarily hindered by contact modeling errors.",
    "document": "Introduction\n\nBridging the sim-to-real gap is essential for deploying reinforcement learning policies on physical robots. Common strategies include domain randomization, system identification, and residual learning to mitigate discrepancies between simulated and real dynamics (Tobin et al., 2017; Peng et al., 2018; Hwangbo et al., 2019). Accurate modeling of contacts, friction, and compliance is particularly challenging for high-DOF manipulators and legged systems.\n\nIt is widely accepted that sim-to-real transfer is primarily hindered by contact modeling errors. To directly address this bottleneck, we introduce a contact-adaptive policy that conditions on uncertainty-aware estimates of local interaction parameters inferred online from proprioceptive signals.",
    "reason": "Consensus claim about the primary cause of sim-to-real failures requires citation to prior literature; none is provided.",
    "start": 471,
    "end": 568,
    "label": "Unsupported claim"
  },
  {
    "span": "It is widely known that CTC-based ASR underperforms attention-based sequence-to-sequence models on long utterances.",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) systems commonly adopt either Connectionist Temporal Classification (CTC), attention-based encoder–decoders (AED), or hybrids (Graves et al., 2006; Chan et al., 2016; Watanabe et al., 2017). CTC offers monotonic alignment and efficient training, while AED provides flexible attention mechanisms for modeling long-range dependencies. Recent work explores transducer architectures that combine alignment modeling with strong language modeling (Graves, 2012; He et al., 2019).\n\nDespite these advances, robustness to domain shift and long-form audio remains challenging. It is widely known that CTC-based ASR underperforms attention-based sequence-to-sequence models on long utterances. We address this by introducing a hierarchical chunked attention layer with consistency regularization that improves stability while retaining CTC’s monotonicity benefits.\n\nExperiments on LibriSpeech, TED-LIUM, and long-form audiobooks demonstrate improved word error rates and reduced deletion errors.",
    "reason": "The statement asserts a comparative performance relationship as accepted knowledge without any citation; such claims about prior findings must be supported by references.",
    "start": 627,
    "end": 742,
    "label": "Unsupported claim"
  },
  {
    "span": "Vatswani et al.",
    "document": "Related Work\n\nTransfer learning in NLP has been driven by large pretrained language models (Smith et al., 2021; Lee et al., 2020). Earlier encoder-decoder architectures laid the groundwork for sequence transduction with attention (Garcia and Patel, 2019). Vatswani et al. demonstrated that scaling attention depth improves long-range reasoning, but subsequent work pointed out diminishing returns at extreme depths (Hernandez et al., 2022). More recent studies explore parameter-efficient tuning (Houlsby et al., 2019) and modular adapters (Pfeiffer et al., 2020) to reduce deployment costs.\n\nOur work complements this line by analyzing robustness under distribution shift (Ouyang et al., 2021) and by benchmarking sample efficiency across domains (Koh et al., 2020).",
    "reason": "Narrative citation missing year; should be formatted as a narrative citation with year, e.g., \"Vatswani et al. (2020)\".",
    "start": 256,
    "end": 271,
    "label": "Format"
  },
  {
    "span": "BERT has been used in AES to model discourse-level coherence.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) has a long history of leveraging handcrafted features and classical machine learning (Attali and Burstein, 2006; Shermis and Burstein, 2013). With the advent of deep learning, neural architectures such as CNNs and BiLSTMs have shown improvements by capturing lexical and syntactic patterns (Taghipour and Ng, 2016; Dong and Zhang, 2016). Pretrained language models like ELMo and BERT further improved performance by providing contextualized representations (Peters et al., 2018; Devlin et al., 2019). BERT has been used in AES to model discourse-level coherence. Recent work also explores incorporating prompt-aware conditioning and domain adaptation to handle distribution shifts across prompts (Li et al., 2020; Mayfield and Black, 2020). Our approach differs by explicitly modeling feedback signals aligned with rubric criteria.",
    "reason": "Mentions a specific prior use of BERT in AES with a particular purpose (discourse-level coherence) but provides no citation to the study that did so (rule a and example iii).",
    "start": 545,
    "end": 606,
    "label": "Unsupported claim"
  },
  {
    "span": "Several recent works have demonstrated that cross-modal transformers surpass prior architectures on multimodal sentiment benchmarks.",
    "document": "Introduction\n\nMultimodal sentiment analysis (MSA) seeks to infer user sentiment by integrating language, vision, and acoustic cues. Despite clear gains from end-to-end deep models, robust fusion across modalities remains challenging due to modality asynchrony and noise. Several recent works have demonstrated that cross-modal transformers surpass prior architectures on multimodal sentiment benchmarks. Motivated by these trends, we propose a lightweight interaction module that aligns unimodal signals through shared attention while maintaining efficiency for real-time applications.\n\nOur study focuses on generalization across domains and robustness to missing modalities, two settings where existing models tend to degrade. We evaluate on standard datasets and introduce a controlled modality-drop protocol to assess reliability under partial observations.\n",
    "reason": "Claims about 'recent works' showing superiority require citations to those works.",
    "start": 271,
    "end": 403,
    "label": "Unsupported claim"
  },
  {
    "span": "68% of users abandon onboarding flows after the second step",
    "document": "Introduction\n\nDesigning effective onboarding is crucial for retaining users in productivity applications. Completion rates are influenced by cognitive load, perceived value, and the clarity of progressive disclosure. According to industry reports, 68% of users abandon onboarding flows after the second step, yet teams rarely instrument granular drop-off analytics to diagnose friction points. We present a lightweight framework that recommends adaptive micro-tasks during onboarding to align perceived value with user goals and reduce early exits.",
    "reason": "Presents a precise statistic without any source or evidence, constituting an unsupported claim per definition.",
    "start": 248,
    "end": 307,
    "label": "Unsupported claim"
  },
  {
    "span": "There is a consensus that graph transformers outperform GCNs on long-range reasoning.",
    "document": "Related Work\n\nGraph representation learning has evolved from message passing neural networks (MPNNs) and graph convolutional networks (GCNs) to architectures that incorporate global attention (Gilmer et al., 2017; Kipf and Welling, 2017; Dwivedi and Bresson, 2021). Attention-based models can in principle aggregate information across distant nodes more effectively than fixed-radius neighborhood schemes. There is a consensus that graph transformers outperform GCNs on long-range reasoning. Nevertheless, the benefits may depend on positional encoding, graph sparsity, and training data scale (Rampasek et al., 2022; Ying et al., 2021).\n\nWe study this question by constructing synthetic and real benchmarks with controllable path lengths and by evaluating robustness to structural perturbations. Our results suggest that hybrid models with restricted attention offer a better efficiency–accuracy trade-off.",
    "reason": "It asserts a field-wide consensus about comparative performance without citing evidence supporting that consensus.",
    "start": 406,
    "end": 491,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was first applied to legal entailment in early 2019",
    "document": "Related Work\n\nLegal NLP research has examined document classification, statute retrieval, and entailment between legal arguments (Aletras et al., 2016; Chalkidis et al., 2019). Early methods relied on BM25 and SVMs with handcrafted features, while recent systems adopt pretrained language models for improved semantics (Devlin et al., 2019; Liu et al., 2019). BERT was first applied to legal entailment in early 2019. Subsequent work adapted domain-specific pretraining schemes and case-law corpora to capture legal terminology more effectively.",
    "reason": "Asserts a historical claim about first application of BERT to a task without providing a supporting citation.",
    "start": 360,
    "end": 416,
    "label": "Unsupported claim"
  },
  {
    "span": "For privacy-preserving aggregation, secure aggregation protocols, homomorphic encryption, and differential privacy have all been investigated (Bonawitz et al., 2017; Acar et al., 2018; Geyer et al., 2017; McMahan et al., 2018).",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative training across distributed clients without centralizing raw data. Despite its promise, FL faces challenges including statistical heterogeneity, communication constraints, and privacy guarantees under adversarial threat models.\n\nFor privacy-preserving aggregation, secure aggregation protocols, homomorphic encryption, and differential privacy have all been investigated (Bonawitz et al., 2017; Acar et al., 2018; Geyer et al., 2017; McMahan et al., 2018). Beyond aggregation, personalization strategies such as meta-learning and mixture-of-experts have been proposed to address client drift (Fallah et al., 2020; Arivazhagan et al., 2019), while compression and quantization reduce uplink costs (Sattler et al., 2019; Alistarh et al., 2017).\n\nOur work studies robustness to targeted poisoning under partial participation schedules. We evaluate aggregation rules and selective sampling but leave a full integration with formal privacy frameworks to future work.",
    "reason": "The span enumerates techniques and citations without linking them to the paper’s focus on poisoning robustness or articulating why existing methods are insufficient, lacking synthesis and motivation (definition a, c).",
    "start": 289,
    "end": 516,
    "label": "Lacks synthesis"
  },
  {
    "span": "Gradient inversion attacks reconstruct client data from shared updates (Zhu et al., 2019; Geiping et al., 2020). Membership inference in FL has been studied under both centralized and decentralized settings (Nasr et al., 2019; Truex et al., 2019). Property inference and attribute inference further expose latent features (Melis et al., 2019; Wang et al., 2021).",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative model training across organizations without sharing raw data. However, the exchange of gradients and model parameters has been shown to leak sensitive information, raising concerns for deployments in healthcare and finance.\n\nGradient inversion attacks reconstruct client data from shared updates (Zhu et al., 2019; Geiping et al., 2020). Membership inference in FL has been studied under both centralized and decentralized settings (Nasr et al., 2019; Truex et al., 2019). Property inference and attribute inference further expose latent features (Melis et al., 2019; Wang et al., 2021).\n\nExisting defenses range from secure aggregation to differential privacy and gradient compression, but their costs and guarantees vary sharply with task and topology. In this work, we introduce FedGuard, a protocol that composes update obfuscation with selective aggregation to maintain accuracy under strict leakage budgets.",
    "reason": "The span enumerates attacks and citations without connecting them to the paper's aims or stating what remains unresolved; it lacks synthesis and author perspective (criteria a and c).",
    "start": 285,
    "end": 647,
    "label": "Lacks synthesis"
  },
  {
    "span": "nearly all prior domain adaptation methods rely on adversarial alignment",
    "document": "Related Work\n\nDomain adaptation for text classification addresses distribution shifts between labeled source data and unlabeled target data. Approaches include representation matching, instance reweighting, self-training, and pseudo-labeling. Despite this diversity, nearly all prior domain adaptation methods rely on adversarial alignment to minimize source–target discrepancies in feature space.\n\nWhile adversarial techniques can reduce divergence, they may also obscure task-relevant distinctions. Recent research explores class-conditional alignment and target-aware normalization. Our method departs from adversarial training by using risk-aware invariance constraints informed by uncertainty estimates.",
    "reason": "Makes a sweeping claim about 'nearly all prior' methods using a particular technique without citing supporting literature.",
    "start": 267,
    "end": 339,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent surveys estimate that 70% of production ML pipelines fail to reach deployment.",
    "document": "Introduction\n\nAs machine learning applications proliferate, the gap between model prototyping and production deployment remains a major bottleneck. Challenges include data drift, dependency management, reproducibility, and monitoring at scale. Recent surveys estimate that 70% of production ML pipelines fail to reach deployment. This motivates research on MLOps practices and tooling that improve end-to-end reliability and observability.\n\nIn this work, we present a declarative configuration system for model training and serving that unifies lineage tracking with automated rollback. We evaluate the system across multiple organizations through case studies and controlled experiments on data drift resilience.",
    "reason": "Presents a precise statistic from 'surveys' without any citation or supporting evidence.",
    "start": 244,
    "end": 329,
    "label": "Unsupported claim"
  },
  {
    "span": "Compositional generalization benchmarks test systematicity (Keysers et al., 2020). Grammar-based decoders enforce well-formed outputs (Yin and Neubig, 2018). Prompt-based methods steer pretrained decoders with task instructions (Wei et al., 2022).",
    "document": "Related Work\n\nSemantic parsing maps natural language to structured meaning representations such as SQL, logical forms, or code (Zelle and Mooney, 1996; Dong and Lapata, 2016). Generalizing to novel compositions of seen primitives remains challenging, leading to benchmarks that diagnose systematic generalization gaps (Lake and Baroni, 2018; Keysers et al., 2020).\n\nCompositional generalization benchmarks test systematicity (Keysers et al., 2020). Grammar-based decoders enforce well-formed outputs (Yin and Neubig, 2018). Prompt-based methods steer pretrained decoders with task instructions (Wei et al., 2022). Concurrently, structured training signals and latent variable models have been explored to improve compositionality (Gupta et al., 2020; Herzig and Berant, 2021).\n\nOur approach integrates grammar constraints with instruction tuning to better handle out-of-distribution compositions while maintaining syntactic validity.",
    "reason": "The span moves from a benchmark description to two different modeling strategies without transitions or explaining how the benchmark motivates the methods, leading to an abrupt, implied relationship.",
    "start": 366,
    "end": 613,
    "label": "Coherence"
  },
  {
    "span": "A recent shared task on temporal knowledge graph completion highlighted the need for inductive benchmarks.",
    "document": "Related Work\n\nTemporal knowledge graph completion (TKGC) extends static KGC by modeling time-stamped facts and evolving entity relations (Garcia et al., 2020). Methods vary in how they encode time, from discrete embeddings to continuous functions and neural ordinary differential equations (Sun and Zhao, 2021; Ahmed et al., 2022).\n\nA recent shared task on temporal knowledge graph completion highlighted the need for inductive benchmarks.\n\nIn response, several datasets introduce held-out entities and future timestamps to evaluate generalization beyond transductive settings (Lin et al., 2022). Our work complements these efforts by proposing a standardized evaluation protocol and temporal negative sampling strategy.",
    "reason": "Mentions a shared task but does not provide a citation to the task at first mention.",
    "start": 333,
    "end": 439,
    "label": "Unsupported claim"
  },
  {
    "span": "(Smith et al., 2018)",
    "document": "Related Work\n\nGraph neural networks (GNNs) have advanced semi-supervised node classification by propagating features across local neighborhoods (Kipf and Welling, 2017; Hamilton et al., 2017). According to (Smith et al., 2018), attention mechanisms can mitigate over-smoothing by weighting neighbor contributions, a claim later explored by Velickovic et al. (2018) through graph attention layers. Beyond architectures, sampling strategies improve scalability, with layer-wise sampling reducing variance in large graphs (Chen et al., 2018). Recent work also studies heterophily and the limits of message passing (Ma et al., 2021), proposing decoupled propagation and feature transformations. Our study builds on these strands by combining adaptive aggregation with topology-aware regularization to balance expressivity and stability.\n",
    "reason": "Wrong citation style: narrative form uses a parenthetical after 'According to'; should be 'Smith et al. (2018)'.",
    "start": 206,
    "end": 226,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on essays from grades 7–10 with holistic scores.",
    "document": "Introduction\n\nAutomatic essay scoring (AES) aims to predict human-assigned grades from student writing, providing timely feedback at scale. Neural text encoders have improved scoring consistency, yet concerns remain about fairness across demographics and robustness to adversarial edits. BERT was used in an AES task trained on essays from grades 7–10 with holistic scores. Motivated by these developments, we investigate calibration techniques to ensure that predicted scores align with human rubrics across prompts and proficiency levels.\n\nOur contributions include a cross-prompt evaluation framework and a stress test for surface-level perturbations.\n",
    "reason": "Mentions a specific prior setup (model, task, grade range, label type) without a supporting citation.",
    "start": 288,
    "end": 373,
    "label": "Unsupported claim"
  },
  {
    "span": "Existing methods address personalization via fine-tuning, meta-learning, or clustered aggregation (Smith et al., 2017; Fallah et al., 2020; Sattler et al., 2020). In this paper, we propose a new personalized federated optimizer for heterogeneous clients.",
    "document": "Introduction\n\nFederated learning enables collaborative model training across distributed clients without centralizing raw data, mitigating privacy risk and regulatory constraints (McMahan et al., 2017). However, client data are often non-IID and imbalanced, creating significant challenges for convergence and generalization (Zhao et al., 2018). Existing methods address personalization via fine-tuning, meta-learning, or clustered aggregation (Smith et al., 2017; Fallah et al., 2020; Sattler et al., 2020). In this paper, we propose a new personalized federated optimizer for heterogeneous clients.\n\nWe evaluate across vision and language benchmarks and show gains under severe heterogeneity. Our study further analyzes communication efficiency and compatibility with common compression schemes.",
    "reason": "The span summarizes prior approaches and immediately states the contribution without identifying a concrete gap or articulating why a new optimizer is needed, thus lacking synthesis (criterion b).",
    "start": 346,
    "end": 600,
    "label": "Lacks synthesis"
  },
  {
    "span": "Sequence-level knowledge distillation trains students on teacher outputs to simplify targets (Kim and Rush, 2016). Soft-label distillation transfers probability distributions at the token level (Hinton et al., 2015). Curriculum schedules gradually increase source complexity (Zhang et al., 2019). Back-translation augments data with synthetic sources (Sennrich et al., 2016). Data filtering removes noisy web text before distillation (Khayrallah and Koehn, 2018).",
    "document": "Related Work\n\nKnowledge Distillation and Data Augmentation for NMT\n\nCompressing neural machine translation (NMT) models while preserving translation quality is critical for deployment on edge devices and latency-sensitive applications. Knowledge distillation (KD) reduces model size by transferring knowledge from larger teachers to compact students, often leading to improved decoding speed and memory footprint (Kim and Rush, 2016; Tang et al., 2019). Data augmentation techniques such as back-translation have also been widely adopted to exploit target-side monolingual corpora for improved generalization (Sennrich et al., 2016).\n\nSequence-level knowledge distillation trains students on teacher outputs to simplify targets (Kim and Rush, 2016). Soft-label distillation transfers probability distributions at the token level (Hinton et al., 2015). Curriculum schedules gradually increase source complexity (Zhang et al., 2019). Back-translation augments data with synthetic sources (Sennrich et al., 2016). Data filtering removes noisy web text before distillation (Khayrallah and Koehn, 2018).\n\nOur approach integrates KD with selective back-translation guided by uncertainty estimates from the teacher, aiming to prioritize beneficial synthetic pairs while avoiding domain drift. We further analyze how label smoothing and temperature scaling interact with student capacity.",
    "reason": "The span lists several techniques without transitions or explanation of how they relate within the KD pipeline. The abrupt shifts from KD to curriculum to back-translation lack explicit connections, causing coherence issues.",
    "start": 635,
    "end": 1098,
    "label": "Coherence"
  },
  {
    "span": "Datasets in this area are notoriously small and noisy, with most containing fewer than 5,000 labeled examples.",
    "document": "Related Work\n\nHate speech and abusive language detection has been studied across platforms, languages, and annotation schemes (Davidson et al., 2017; Founta et al., 2018; Fortuna and Nunes, 2018). Recent work explores context modeling and user-level priors to reduce false positives and capture implicit abuse (Waseem et al., 2018; Qian et al., 2018). Multilingual and cross-domain transfer learning have also been investigated to mitigate domain shift (Mendonça et al., 2019; Pamungkas and Patti, 2019).\n\nDatasets in this area are notoriously small and noisy, with most containing fewer than 5,000 labeled examples. Label uncertainty, annotator disagreement, and shifting platform policies further complicate learning robust classifiers. Semi-supervised and weakly supervised approaches attempt to leverage large unlabeled corpora, but remain sensitive to spurious correlations (Liu et al., 2019; Karimi et al., 2020).\n\nOur work addresses label noise via confidence-aware training and evaluates robustness under topic and platform shifts.",
    "reason": "States specific statistics about dataset sizes without providing citations or evidence.",
    "start": 506,
    "end": 616,
    "label": "Unsupported claim"
  },
  {
    "span": "The CodeSum-1M dataset includes 1 million functions from 50 languages.",
    "document": "Related Work\n\nCode summarization seeks to generate concise natural-language descriptions of source code. Early approaches used information retrieval and API pattern mining (Rastkar et al., 2014; Movshovitz-Attias and Cohen, 2013). Neural encoder-decoder models with copy mechanisms and structural encoders have since achieved strong results (Iyer et al., 2016; LeClair and McMillan, 2019; Ahmad et al., 2020).\n\nLarge-scale datasets have enabled pre-training and robust evaluation across languages and repositories. The CodeSum-1M dataset includes 1 million functions from 50 languages. Despite the scale, cross-project generalization and license filtering remain open issues. Our work introduces a contamination-checked benchmark with repository-level splits and standardized license auditing.\n\nPre-training on code and natural language, including models like CodeBERT and GraphCodeBERT, has improved summarization quality (Feng et al., 2020; Guo et al., 2021). We build on these encoders with a structure-aware decoder that attends to abstract syntax trees and control-flow graphs.",
    "reason": "Introduces a specific dataset and its statistics without citing the dataset source at first mention (rule a).",
    "start": 515,
    "end": 585,
    "label": "Unsupported claim"
  },
  {
    "span": "Previous competitions standardized the use of MAPE for load forecasting evaluation.",
    "document": "Introduction\n\nShort-term load forecasting (STLF) underpins grid operations, demand response, and market bidding (Hong and Fan, 2016). Classical models such as ARIMA and exponential smoothing remain strong baselines, while machine learning approaches leverage weather, calendar, and exogenous signals for improved accuracy (Hyndman and Athanasopoulos, 2018; Lago et al., 2021). Probabilistic forecasting has gained prominence to quantify uncertainty in operational decisions (Gneiting and Katzfuss, 2014; Ziel and Weron, 2018).\n\nPrevious competitions standardized the use of MAPE for load forecasting evaluation. Despite its popularity, MAPE can be unstable at low loads and may misalign with operational cost metrics. Recent studies advocate scale-free or pinball-based metrics for probabilistic settings (Makridakis et al., 2020; Hong et al., 2019).\n\nWe compare scale-dependent and quantile-based losses on three utility datasets, relating metric choice to dispatch costs under realistic constraints.",
    "reason": "Claims a historical practice in competitions without providing any citation or documentary evidence.",
    "start": 528,
    "end": 611,
    "label": "Unsupported claim"
  },
  {
    "span": "Chen et al. (2021) proposed knowledge-augmented prompting for question answering. Li and Kumar (2022) scale instruction tuning with mixtures of datasets. Park et al. (2020) examined calibration of confidence under distribution shift. Nguyen et al. (2023) fine-tuned models with preference signals for factual consistency.",
    "document": "Related Work\n\nPrompting large language models has rapidly evolved, with diverse strategies to elicit knowledge, control style, and improve factuality. Early approaches treated prompts as fixed templates, while recent work learns soft prompts or leverages instruction-tuned checkpoints. This section reviews prior art relevant to factual and controllable generation.\n\nChen et al. (2021) proposed knowledge-augmented prompting for question answering. Li and Kumar (2022) scale instruction tuning with mixtures of datasets. Park et al. (2020) examined calibration of confidence under distribution shift. Nguyen et al. (2023) fine-tuned models with preference signals for factual consistency.\n\nOther threads consider retrieval-augmented prompting to ground responses in external corpora (Duarte and Singh, 2022) and self-consistency decoding to mitigate brittleness (Huang et al., 2023). While our method also targets factuality, we focus on adaptive grounding signals that respond to prompt-level uncertainty.",
    "reason": "The span lists several works in consecutive sentences without transitions or explicit explanations of how they relate to each other or to the stated goal of factual and controllable generation, creating abrupt shifts and unclear connections.",
    "start": 367,
    "end": 688,
    "label": "Coherence"
  },
  {
    "span": "Work on private deep learning has examined gradient clipping and noise addition, privacy amplification by subsampling, and accounting frameworks (Abadi et al., 2016; Mironov, 2017; Wang et al., 2019).",
    "document": "Introduction\n\nTraining models with rigorous privacy guarantees is increasingly important as datasets contain sensitive information. Differential privacy (DP) offers a formal framework, but practical adoption hinges on accuracy, robustness, and scalable accounting. Work on private deep learning has examined gradient clipping and noise addition, privacy amplification by subsampling, and accounting frameworks (Abadi et al., 2016; Mironov, 2017; Wang et al., 2019). Complementary strands study privacy-utility trade-offs, hyperparameter selection, and adaptive clipping (Bu et al., 2020; Andrew et al., 2021).\n\nWe focus on image classification under tight privacy budgets.",
    "reason": "The span enumerates prior techniques without relating them to the current study's aims, shortcomings to be addressed, or the authors' stance (definition a) and lacks explicit motivation (definition c).",
    "start": 265,
    "end": 465,
    "label": "Lacks synthesis"
  },
  {
    "span": "Twitter15 and Twitter16 are widely used datasets for rumor detection.",
    "document": "Related Work\n\nRumor detection on social media has been approached via tree-structured propagation models, stance-aware classifiers, and pretrained language models (Ma et al., 2017; Kochkina et al., 2018; Liu et al., 2020). Cross-event generalization and early detection remain open challenges, particularly when propagation signals are sparse.\n\nTwitter15 and Twitter16 are widely used datasets for rumor detection. However, event sparsity, annotation inconsistencies, and platform evolution question their suitability for contemporary evaluations. Recent studies explore multi-platform corpora and temporal adaptation to address distribution shift, but consensus on standardized splits is lacking.\n\nWe introduce a temporally stratified benchmark spanning multiple events and platforms, accompanied by a protocol for early detection and cross-event testing.",
    "reason": "First mention of specific datasets without citations to their original releases or papers (rule a).",
    "start": 345,
    "end": 414,
    "label": "Unsupported claim"
  },
  {
    "span": "We evaluate on the ACE 2005 dataset and the new X-Events corpus introduced by prior work.",
    "document": "Introduction\n\nEvent extraction (EE) aims to detect event triggers and arguments from unstructured text and has evolved from pipeline feature-based systems to end-to-end neural models (Nguyen et al., 2016; Yang and Mitchell, 2017; Lin et al., 2020). Joint modeling approaches reduce error propagation and model interdependencies among triggers, arguments, and roles (Zhao et al., 2020; He and Sun, 2021). Pretrained language models have further improved robustness to sparse triggers and cross-domain generalization (Qian et al., 2021).\n\nWe evaluate on the ACE 2005 dataset and the new X-Events corpus introduced by prior work. To test cross-domain transfer, we include newswire and social media domains and measure trigger classification and argument role labeling under both fully supervised and few-shot settings. Our contributions include a span-centric decoding strategy and domain-adaptive prompt tuning.",
    "reason": "First mentions of datasets ('ACE 2005' and 'X-Events corpus') lack citations to their sources or creators, violating rule (a).",
    "start": 537,
    "end": 626,
    "label": "Unsupported claim"
  },
  {
    "span": "Previous studies prove that adding differential privacy is unnecessary in cross-silo federated learning.",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative training across multiple clients without centralizing raw data, with cross-device and cross-silo settings differing in scale and trust assumptions (Kairouz et al., 2021). Privacy-preserving mechanisms include secure aggregation, differential privacy (DP), and hybrid protocols to mitigate inference attacks (Bonawitz et al., 2017; Geyer et al., 2017; Truex et al., 2019).\n\nPrevious studies prove that adding differential privacy is unnecessary in cross-silo federated learning. Nevertheless, recent attacks suggest that membership inference and property inference can still succeed under naive aggregation, motivating calibrated noise and clipping strategies even in semi-trusted environments. We position our work within this debate by proposing utility-aware DP that adapts per-silo noise levels based on validation sensitivity.\n\nWe evaluate on healthcare and finance datasets with heterogeneous label distributions and report both privacy and utility metrics under realistic participation patterns.",
    "reason": "The sentence makes a strong claim about conclusions from prior studies without any citations to those studies (violates rule b; claims about prior work require evidence).",
    "start": 433,
    "end": 537,
    "label": "Unsupported claim"
  },
  {
    "span": "Standard practice is to pretrain on CommonCrawl before fine-tuning for code summarization.",
    "document": "Related Work\n\nNeural code summarization has benefited from advances in sequence-to-sequence modeling and large-scale pretraining. Effective systems must capture both syntactic structure and domain-specific idioms while avoiding leakage from docstrings to summary targets.\n\nStandard practice is to pretrain on CommonCrawl before fine-tuning for code summarization. Alternative strategies leverage code-only corpora, bimodal pretraining on code–text pairs, or contrastive objectives aligning code snippets with natural language descriptions.\n\nWe explore a syntax-aware pretraining regime that aligns abstract syntax trees with token sequences, followed by task-adaptive fine-tuning on diverse repositories. Experiments demonstrate gains in faithfulness and robustness to noisy docstrings.\n\nTo encourage reproducibility, we provide data processing scripts and evaluation checklists.",
    "reason": "The sentence claims a field-wide 'standard practice' and names a specific corpus without any citation to empirical studies or widely adopted frameworks supporting this assertion (rule b).",
    "start": 273,
    "end": 363,
    "label": "Unsupported claim"
  },
  {
    "span": "Several recent works explore debiasing exposure using inverse propensity scoring.",
    "document": "Related Work\n\nRecommendation models trained on observational logs inherit exposure biases, which can lead to misleading offline metrics and suboptimal user experiences. Correcting for selection bias is critical for fair and reliable evaluation and learning.\n\nSeveral recent works explore debiasing exposure using inverse propensity scoring. While such techniques can reduce bias in counterfactual estimation, they are sensitive to propensity misspecification and high-variance estimators in sparse regimes.\n\nOur approach combines calibrated propensities with representation regularization to mitigate both exposure and user-feature confounding, providing more stable estimates under practical logging policies.",
    "reason": "References 'recent works' and a specific methodological direction without citing any supporting literature, failing the requirement to cite on first mention.",
    "start": 259,
    "end": 340,
    "label": "Unsupported claim"
  },
  {
    "span": "(Hale, 2011 and Perez, 2014)",
    "document": "Introduction\n\nEnd-to-end speech understanding systems integrate ASR and NLU to reduce latency and error propagation (Serdyuk et al., 2018; Haghani et al., 2018). For robustness, researchers have investigated multi-style training and augmentation to handle acoustic variability (Ko et al., 2015). Cognitive load metrics based on surprisal have been used to analyze processing difficulty in speech (Levy, 2008). Prior psycholinguistic work (Hale, 2011 and Perez, 2014) informs our approach to modeling incremental comprehension signals. We propose a joint training objective that aligns surprisal with intent predictions to improve downstream understanding.\n",
    "reason": "Wrong coordination inside parentheses; multiple citations in APA-style should be separated by a semicolon, e.g., '(Hale, 2011; Perez, 2014)'.",
    "start": 438,
    "end": 466,
    "label": "Format"
  },
  {
    "span": "Classical forecasting models include ARIMA and exponential smoothing for univariate series (Box and Jenkins, 1976; Hyndman et al., 2008), while deep architectures such as sequence-to-sequence RNNs, temporal convolutions, and attention-based models learn nonlinear dependencies (Lai et al., 2018; Lim et al., 2019; Wu et al., 2020).",
    "document": "Related Work\n\nTime-series forecasting supports decision-making across energy, retail, and transportation. Methods differ in their inductive biases, handling of seasonality, and capacity to leverage cross-series information.\n\nClassical forecasting models include ARIMA and exponential smoothing for univariate series (Box and Jenkins, 1976; Hyndman et al., 2008), while deep architectures such as sequence-to-sequence RNNs, temporal convolutions, and attention-based models learn nonlinear dependencies (Lai et al., 2018; Lim et al., 2019; Wu et al., 2020). Multivariate extensions incorporate covariates and probabilistic objectives to better capture uncertainty (Salinas et al., 2019; Rangapuram et al., 2021).\n\nOur approach provides a simple baseline for medium-horizon forecasting on retail datasets.",
    "reason": "The span summarizes prior forecasting models but does not connect them to the proposed approach or specify the unresolved problem, exhibiting lack of synthesis (definition a and b).",
    "start": 225,
    "end": 556,
    "label": "Lacks synthesis"
  },
  {
    "span": "The Codex family reset the state of the art on program synthesis benchmarks such as HumanEval and MBPP.",
    "document": "Program Synthesis from Natural Language\n\nMapping natural language to executable code has advanced rapidly with large language models trained on paired text-code corpora. Benchmark suites measure functional correctness using unit tests that capture input-output behavior for short programming tasks. The Codex family reset the state of the art on program synthesis benchmarks such as HumanEval and MBPP. Despite strong average pass rates, performance remains uneven across tasks requiring complex control flow, data structure manipulation, or multi-step reasoning.\n\nWe explore a verifier-guided decoding framework that integrates lightweight static analysis with test-time search. Our method prunes inconsistent partial programs and prioritizes candidates with higher likelihood of satisfying specification constraints.\n\nEmpirical results show gains in pass@k and reduced brittleness to prompt variations. We also analyze failure cases involving ambiguous specifications and propose guidelines for benchmark design to improve coverage and clarity.",
    "reason": "The sentence names specific prior models and benchmarks and claims state-of-the-art performance without any citation to the underlying works or evaluations, violating rule (a) and (b).",
    "start": 299,
    "end": 402,
    "label": "Unsupported claim"
  },
  {
    "span": "In low-resource ASR, vowel harmony errors are the most common failure mode.",
    "document": "Introduction\n\nLow-resource automatic speech recognition (ASR) faces challenges stemming from data scarcity, domain mismatch, and phonological variability. Cross-lingual transfer and self-supervised pre-training have improved label efficiency, yet performance remains uneven across languages with complex morphophonology. In low-resource ASR, vowel harmony errors are the most common failure mode. This observation motivates our approach: a phonology-aware adapter that conditions acoustic representations on learned constraints over vowel inventories. We evaluate our model on three agglutinative languages under strict few-shot protocols and analyze substitution patterns and error locality.\n\nRelated Work\n\nPrior studies have incorporated pronunciation lexicons and articulatory features to improve recognition in low-resource settings. Other works leverage multilingual pre-training and language identification to stabilize decoding when training data is limited.",
    "reason": "This is a specific, niche empirical claim about error prevalence in a specialized subfield without any supporting citation or evidence.",
    "start": 321,
    "end": 396,
    "label": "Unsupported claim"
  },
  {
    "span": "A previous study reported that self-training reduces word error rate by 15% on far-field speech.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) systems benefit from large-scale labeled audio, yet annotation remains costly, especially for far-field and noisy conditions. Semi-supervised techniques leverage abundant unlabeled audio to improve robustness without proportional labeling effort.\n\nA previous study reported that self-training reduces word error rate by 15% on far-field speech. However, the effectiveness of pseudo-labeling depends heavily on confidence calibration, domain match, and augmentation strategies during student training.\n\nWe present a calibrated self-training pipeline for far-field ASR that combines noise-aware filtering with iterative re-labeling, and we evaluate gains under varying unlabeled-to-labeled ratios.",
    "reason": "Cites a numerical result from a 'previous study' without providing a reference to that study, making the claim unsupported.",
    "start": 297,
    "end": 393,
    "label": "Unsupported claim"
  },
  {
    "span": "There is a consensus that back-translation saturates around 50 million synthetic pairs.",
    "document": "Introduction\n\nNeural machine translation (NMT) for low-resource settings often relies on monolingual data through back-translation and self-training. Advances in multilingual pretraining and data filtering have reduced the gap to high-resource languages, but data scaling laws and saturation points remain debated.\n\nThere is a consensus that back-translation saturates around 50 million synthetic pairs. Motivated by this hypothesis, we investigate scaling strategies that prioritize quality over quantity by filtering synthetic data using uncertainty estimates and cycle-consistency checks.\n\nWe report results across three language pairs and analyze the trade-off between noise and diversity during iterative back-translation.",
    "reason": "States a community consensus and a specific numerical threshold without citing empirical studies or surveys supporting the claim.",
    "start": 316,
    "end": 403,
    "label": "Unsupported claim"
  },
  {
    "span": "The IEMOCAP corpus contains 12 hours of speech annotated for six emotions.",
    "document": "Introduction\n\nSpeech emotion recognition (SER) aims to map acoustic and lexical cues to affective states in realistic conditions (Schuller et al., 2018; Cowen et al., 2020). Advances in self-supervised audio pretraining have improved performance under limited labels, while multimodal fusion leverages prosody and text jointly (Hsu et al., 2021; Tripathi and Chen, 2022).\n\nBenchmark datasets play a central role in SER progress, enabling consistent evaluation across models and features. The IEMOCAP corpus contains 12 hours of speech annotated for six emotions. Despite its popularity, domain shifts between scripted and spontaneous speech and class imbalance complicate generalization. We propose a partition-aware training strategy with calibrated decision thresholds to mitigate these issues.",
    "reason": "It provides specific statistics about a well-known dataset and mentions it for the first time without any citation to support the numeric claim.",
    "start": 488,
    "end": 562,
    "label": "Unsupported claim"
  },
  {
    "span": "There have been many recent works exploring prompt-based few-shot NER in low-resource settings.",
    "document": "Introduction\n\nNamed entity recognition (NER) is a core task in information extraction, requiring models to identify and categorize mentions of entities in text. Despite substantial progress with pre-trained language models, performance often degrades in domains and languages with limited annotations. There have been many recent works exploring prompt-based few-shot NER in low-resource settings. However, these methods typically assume access to task-specific verbalizers or rely on heuristics for label mapping that do not transfer across domains.\n\nIn this paper, we study a template-agnostic approach that reduces the need for handcrafted prompts while preserving label semantics. We evaluate our method across multiple domains and show gains in both span-level and entity-level metrics under constrained annotation budgets. We also analyze sensitivity to label imbalance and template variation to understand robustness in practical deployments.",
    "reason": "Mentions 'recent works' without providing any citations to support the claim.",
    "start": 302,
    "end": 397,
    "label": "Unsupported claim"
  },
  {
    "span": "Researchers have proposed re-ranking, exposure constraints, and adversarial debiasing to address unfairness in recommendations (Singh and Joachims, 2018; Yao and Huang, 2017; Beutel et al., 2019). We study fairness in exposure for multi-stakeholder platforms.",
    "document": "Related Work\n\nFairness in recommender systems spans individual user treatment, provider fairness, and overall marketplace equity. Various optimization formulations and learning strategies have been applied to alter rankings and allocations with fairness objectives. Researchers have proposed re-ranking, exposure constraints, and adversarial debiasing to address unfairness in recommendations (Singh and Joachims, 2018; Yao and Huang, 2017; Beutel et al., 2019). We study fairness in exposure for multi-stakeholder platforms. Recent surveys categorize these methods by fairness notion and intervention stage (Ekstrand et al., 2022).\n\nOur experiments consider music and job recommendation datasets.",
    "reason": "The span lists prior work and then states the study focus without clarifying how this focus differs from or improves upon existing methods; no explicit gap is articulated (definition b), and there is no synthesis linking the literature to the authors' argument (definition a).",
    "start": 266,
    "end": 525,
    "label": "Lacks synthesis"
  },
  {
    "span": "BERT was used in an AES task trained on essays from multiple grade levels.",
    "document": "Related Work\n\nAutomated essay scoring (AES) has evolved from hand-crafted features and linear models (Page, 1968; Attali and Burstein, 2006) to neural architectures leveraging pretraining (Taghipour and Ng, 2016; Dong et al., 2017). Recent advances rely on transformer-based encoders to capture discourse and syntax signals across long inputs. BERT was used in an AES task trained on essays from multiple grade levels. Our approach extends prior neural AES models by explicitly modeling rubric criteria through multi-task learning.",
    "reason": "This statement describes a specific setup of a task (BERT used in AES on multi-grade essays) without citing the work, matching example (iii) and rule (a).",
    "start": 344,
    "end": 418,
    "label": "Unsupported claim"
  },
  {
    "span": "Spectral methods identify anomalies by examining eigenvectors of graph Laplacians (Akoglu et al., 2015; Ma et al., 2021). Generative adversarial networks model normality on graphs and flag deviations (Ding et al., 2019; Wang et al., 2020). Time-series outlier detection surveys discuss seasonality and trend adjustments (Blázquez-García et al., 2021).",
    "document": "Related Work\n\nGraph anomaly detection addresses the identification of unusual nodes, edges, or subgraphs that deviate from normative patterns. Applications include fraud detection, intrusion detection, and sensor network monitoring. Methods vary by whether they assume static or dynamic graphs and by what statistical or learned models define normal behavior.\n\nSpectral methods identify anomalies by examining eigenvectors of graph Laplacians (Akoglu et al., 2015; Ma et al., 2021). Generative adversarial networks model normality on graphs and flag deviations (Ding et al., 2019; Wang et al., 2020). Time-series outlier detection surveys discuss seasonality and trend adjustments (Blázquez-García et al., 2021).\n\nRepresentation learning on graphs via graph neural networks enables end-to-end anomaly scoring using embeddings. In dynamic settings, recurrent or attention-based mechanisms track evolution over time. Evaluation remains challenging due to label sparsity and drift, motivating semi-synthetic benchmarks and proxy tasks.\n\nOur approach couples temporal attention with structure-aware contrastive learning to estimate uncertainty-adjusted anomaly scores. We test on transaction and communication graphs with annotated anomalies and analyze robustness under distribution shift.",
    "reason": "The span abruptly moves from spectral methods to GAN-based graph modeling and then to general time-series outlier surveys without explaining their connection or providing transitions, making the relations between cited works unclear.",
    "start": 361,
    "end": 712,
    "label": "Coherence"
  },
  {
    "span": "Prior studies show that exposure bias is negligible in music recommendation.",
    "document": "Related Work\n\nRecommendation systems trained from logged implicit feedback suffer from exposure and selection biases that confound counterfactual evaluation (Schnabel et al., 2016; Joachims et al., 2017). In music recommendation, position bias and feedback loops have been documented in large-scale platforms (Jannach and Adomavicius, 2017; Mansoury et al., 2020). Prior studies show that exposure bias is negligible in music recommendation. In response, debiasing approaches such as inverse propensity scoring (Swaminathan and Joachims, 2015) and self-normalized estimators (Swaminathan and Joachims, 2015) have been adapted to top-k ranking (Wang et al., 2019; Agarwal et al., 2019).\n\nWe extend these methods with a calibration-aware objective that jointly estimates propensities and user affinities, enabling improved off-policy evaluation under dynamic playlists.",
    "reason": "Makes a general claim about prior studies and a niche conclusion without any supporting citations, violating rules (b) and (d).",
    "start": 365,
    "end": 441,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors showed that teacher–student temperature scheduling stabilizes training.",
    "document": "Related Work\nKnowledge distillation (KD) transfers information from a high-capacity teacher to a compact student by matching predictive distributions or intermediate representations (Hinton et al., 2015; Park and Kim, 2019). In automatic speech recognition (ASR), KD has been used to compress streaming models and to bridge acoustic modeling mismatches between offline and online decoders (Chen et al., 2020; Wu et al., 2021).\nIn a previous study, the authors showed that teacher–student temperature scheduling stabilizes training. Other works propose to distill alignment information via CTC posteriors or to regularize students with teacher-guided augmentations (Li and Rao, 2020; Nguyen et al., 2022). Unlike these approaches, we introduce layer-wise distillation with curriculum masking that progressively increases difficulty for the student.",
    "reason": "This sentence references a specific prior study and its finding but does not cite it. Per rule (a) and example (ii), first mentions of a study must include a citation.",
    "start": 427,
    "end": 531,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works report that batch correction can erase biological signal in scRNA-seq datasets.",
    "document": "Related Work\n\nSingle-cell RNA-seq (scRNA-seq) integration methods aim to remove technical batch effects while preserving true biological variability, employing mutual nearest neighbors, variational autoencoders, and graph harmonization (Haghverdi et al., 2018; Stuart et al., 2019; Lopez et al., 2018). Metrics for evaluating integration balance separation and mixing across cell types and batches.\n\nRecent works report that batch correction can erase biological signal in scRNA-seq datasets. This motivates approaches with explicit regularization to protect rare cell states. We contribute a contrastive alignment method with cell-type-aware invariances and extensive sensitivity analyses.",
    "reason": "Uses a vague 'recent works report' claim without citing any studies; mentions of recent works must be supported by citations.",
    "start": 400,
    "end": 492,
    "label": "Unsupported claim"
  },
  {
    "span": "(Brown et. al., 2019)",
    "document": "Introduction\n\nRecent progress in large-scale language modeling shows strong gains from scaling parameters and data (Radford et al., 2019). Instruction tuning further narrows the gap between pretraining and downstream tasks (Sanh et al., 2022), while model compression targets practical deployment constraints (Sanh et al., 2020). Notably, few-shot generalization has been reported in (Brown et. al., 2019), yet the role of data curation remains debated.",
    "reason": "Incorrect punctuation in the citation: 'et. al.' should be 'et al.' without a period after 'et'.",
    "start": 384,
    "end": 405,
    "label": "Format"
  },
  {
    "span": "Early neural conversational models framed response generation as sequence-to-sequence learning (Takahashi and Lee, 2015). Safety-oriented filtering has been proposed to reduce toxic outputs (Dion and Patel, 2019). Persona-based conditioning improves consistency (Liu et al., 2016). Knowledge-grounded dialogue integrates external evidence (Gao and Chen, 2018). Counterfactual data augmentation can debias toxic classifiers (Kaur et al., 2020).",
    "document": "Related Work\n\nConversational agents have rapidly advanced with the advent of large neural models, yet safety and grounding remain open challenges. Prior literature spans modeling paradigms, data curation, and evaluation schemes, creating a fragmented landscape that complicates the design of safety-aware, knowledge-grounded systems. We summarize related directions below.\n\nEarly neural conversational models framed response generation as sequence-to-sequence learning (Takahashi and Lee, 2015). Safety-oriented filtering has been proposed to reduce toxic outputs (Dion and Patel, 2019). Persona-based conditioning improves consistency (Liu et al., 2016). Knowledge-grounded dialogue integrates external evidence (Gao and Chen, 2018). Counterfactual data augmentation can debias toxic classifiers (Kaur et al., 2020).\n\nOur work targets safe knowledge-grounded generation under sparse supervision, but existing strands of research provide only partial ingredients for that goal. We therefore combine explicit retrieval, lightweight safety constraints, and pragmatic decoding to balance informativeness and safety.",
    "reason": "The span lists several distinct strands—seq2seq dialogue, safety filters, persona conditioning, knowledge grounding, counterfactual augmentation—without transitions or explicit relationships among them, making the connection between cited works abrupt and unclear.",
    "start": 374,
    "end": 817,
    "label": "Coherence"
  },
  {
    "span": "Most commonsense knowledge bases are organized as typed triples.",
    "document": "Related Work\n\nAugmenting language models with external knowledge has been explored for question answering, dialogue, and reasoning tasks (Dhingra et al., 2020; He et al., 2021). Integration techniques include retrieval-augmented generation, graph neural encoders, and prompt-based adapters (Lewis et al., 2020; Yasunaga et al., 2021; Kassner et al., 2021). Most commonsense knowledge bases are organized as typed triples. Prominent resources include ConceptNet, ATOMIC, and Wikidata-based commonsense subsets, which provide relational signals that complement distributional text statistics. Our method retrieves relation paths conditioned on the input and injects them into the encoder.\n",
    "reason": "Makes a domain-specific structural claim about how commonsense KBs are organized without citing any supporting references (rule b).",
    "start": 357,
    "end": 421,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors claimed that schema-guided approaches eliminate the need for domain ontologies.",
    "document": "Related Work\n\nDialogue State Tracking. Classical task-oriented dialogue systems decomposed the problem into NLU, dialogue state tracking (DST), policy learning, and NLG (Young et al., 2013; Williams and Young, 2007). Neural approaches have gradually unified these components, with pretraining significantly boosting DST robustness (Henderson et al., 2014; Hosseini-Asl et al., 2020). Schema-guided dialogue frameworks further generalize to unseen services by conditioning on parameterized slots and intents (Rastogi et al., 2020; Lee et al., 2021). In a previous study, the authors claimed that schema-guided approaches eliminate the need for domain ontologies. Recent works also incorporate retrieval to ground slot values in external APIs (Peng et al., 2021; Gao et al., 2022). We build on the schema-guided setting but propose a contrastive objective to better align slot textual descriptions with latent belief representations.",
    "reason": "Refers to an unspecified prior study and its claim without providing a citation, violating the requirement to cite when mentioning previous work.",
    "start": 549,
    "end": 661,
    "label": "Unsupported claim"
  },
  {
    "span": "Curiosity-driven agents use prediction error as intrinsic motivation (Pathak et al., 2017). Offline RL algorithms learn from fixed datasets without further interaction (Levine et al., 2020). Count-based exploration regulates novelty bonuses via state visitation (Bellemare et al., 2016).",
    "document": "Related Work\n\nExploration remains a central challenge in reinforcement learning (RL), especially in sparse-reward environments where extrinsic feedback is rare (Ecoffet et al., 2021). Intrinsic motivation and uncertainty estimation have emerged as key strategies to guide agents toward informative experiences (Ostrovski et al., 2017; Burda et al., 2019).\n\nCuriosity-driven agents use prediction error as intrinsic motivation (Pathak et al., 2017). Offline RL algorithms learn from fixed datasets without further interaction (Levine et al., 2020). Count-based exploration regulates novelty bonuses via state visitation (Bellemare et al., 2016). Recent work integrates model-based planning with intrinsic rewards to improve sample efficiency (Sekar et al., 2020).\n\nWe build a unified framework that quantifies epistemic uncertainty in both online and offline settings, connecting exploration bonuses with conservative policy improvement.",
    "reason": "The span mixes curiosity-driven exploration, offline RL, and count-based methods without transitions or explanation of their relationships, creating abrupt topic shifts across sentences.",
    "start": 357,
    "end": 644,
    "label": "Coherence"
  },
  {
    "span": "Mitigating bias in recommender systems has been studied through debiasing click data (Joachims et al., 2017; Wang et al., 2018), fairness-aware matrix factorization (Kamishima et al., 2012; Beutel et al., 2019), exposure regularization (Biega et al., 2018; Diaz et al., 2020), and post-hoc re-ranking to satisfy fairness constraints (Zehlike et al., 2017; Singh and Joachims, 2018). In our work we design a re-ranking method to improve exposure fairness for underrepresented providers.",
    "document": "Introduction\n\nRecommendation platforms can amplify historical inequities, leading to systematic under-exposure of certain user groups or item providers. Addressing these issues requires interventions spanning data collection, modeling, and ranking.\n\nMitigating bias in recommender systems has been studied through debiasing click data (Joachims et al., 2017; Wang et al., 2018), fairness-aware matrix factorization (Kamishima et al., 2012; Beutel et al., 2019), exposure regularization (Biega et al., 2018; Diaz et al., 2020), and post-hoc re-ranking to satisfy fairness constraints (Zehlike et al., 2017; Singh and Joachims, 2018). In our work we design a re-ranking method to improve exposure fairness for underrepresented providers.\n\nWe evaluate on two large-scale marketplaces and report gains in exposure parity with minimal utility loss.\n",
    "reason": "The authors state their contribution immediately after listing prior work but do not specify what gap remains or why existing re-ranking methods are insufficient, matching condition (b).",
    "start": 250,
    "end": 735,
    "label": "Lacks synthesis"
  },
  {
    "span": "Prototype networks compute class centroids in embedding space (Snell et al., 2017). Prompt tuning conditions language models with task cues (Lester et al., 2021). Meta-learning adapts parameters across tasks (Finn et al., 2017). Calibration mitigates overconfidence under distribution shift (Guo et al., 2017).",
    "document": "Related Work\n\nFew-shot text classification seeks robust generalization from minimal labeled examples. Approaches include metric learning, adaptation via meta-learning, and leveraging pretrained language models with task-specific prompts.\n\nPrototype networks compute class centroids in embedding space (Snell et al., 2017). Prompt tuning conditions language models with task cues (Lester et al., 2021). Meta-learning adapts parameters across tasks (Finn et al., 2017). Calibration mitigates overconfidence under distribution shift (Guo et al., 2017).\n\nOur framework unifies prototype learning with prompt-conditioned encoders and introduces episodic calibration losses that stabilize confidence estimates in extreme low-shot regimes.",
    "reason": "The span lists methods from different lines of work without describing how they connect or contribute to a common narrative. Each sentence stands alone, lacking transitions, which creates an incoherent flow.",
    "start": 239,
    "end": 549,
    "label": "Coherence"
  },
  {
    "span": "Recent works demonstrate that self-supervised pretraining closes the gap for many low-resource languages.",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) systems have benefited greatly from large-scale self-supervised pretraining. By learning from raw audio at scale, these models reduce label dependence and transfer effectively to downstream speech tasks.\n\nRecent works demonstrate that self-supervised pretraining closes the gap for many low-resource languages. Nevertheless, challenges remain around domain mismatch, code-switching, and phonotactic diversity that can degrade recognition in truly low-resource settings.\n\nWe study cross-lingual transfer with a pronunciation-informed adapter that incorporates grapheme-to-phoneme constraints. Evaluations target languages with fewer than 20 hours of labeled data, and we analyze performance under noisy channel conditions.",
    "reason": "Uses 'Recent works demonstrate' without providing any citations; per rule d) such claims about recent works must be backed by references.",
    "start": 265,
    "end": 370,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works have demonstrated that whisper-to-normal conversion dramatically improves ASR accuracy.",
    "document": "Introduction\n\nWhispered speech lacks periodic excitation and exhibits altered spectral characteristics, posing challenges for automatic speech recognition (ASR) systems trained on modal voice (Junqua, 1993; Hansen and Patil, 2007). Classic approaches adapt acoustic models via vocal-tract length normalization and feature-space transforms (Gales and Woodland, 1996), while neural front-ends aim to recover prosodic cues or denoise spectra (Seltzer et al., 2013; Watanabe et al., 2017).\n\nRecent works have demonstrated that whisper-to-normal conversion dramatically improves ASR accuracy. Motivated by this observation, we propose a non-parallel, cycle-consistent speech conversion framework that leverages self-supervised representations to reconstruct pseudo-modal characteristics from whispered inputs, enabling off-the-shelf ASR models to achieve robust performance.",
    "reason": "The phrase 'Recent works have demonstrated' refers to prior studies but does not cite any; mentions of recent works require supporting citations.",
    "start": 487,
    "end": 587,
    "label": "Unsupported claim"
  },
  {
    "span": "a previous study showed that adding character n-grams improves low-resource tagging by 7.3%",
    "document": "Introduction\n\nPart-of-speech tagging in low-resource languages is hindered by data scarcity and morphological complexity. Subword modeling is a common strategy to mitigate sparsity, complementing limited token-level supervision with character-level signals. While multilingual transfer and self-training provide additional leverage, careful design of subword features remains crucial for robustness.\n\nMotivated by this, a previous study showed that adding character n-grams improves low-resource tagging by 7.3%, suggesting that rich morphological priors can substitute for large labeled corpora. Building on this insight, we propose a lightweight adapter that injects character-factorized representations into a frozen encoder, improving sample efficiency during fine-tuning.\n\nWe validate our approach across four typologically diverse languages and analyze its behavior under extreme data reduction.",
    "reason": "This sentence reports a quantitative result attributed to a previous study but does not cite the study (rule b/e: specific prior finding and statistic require a citation).",
    "start": 420,
    "end": 511,
    "label": "Unsupported claim"
  },
  {
    "span": "Classical time-series anomaly detection approaches include control charts, ARIMA modeling, and change-point tests (Page, 1954; Box and Jenkins, 1970; Basseville and Nikiforov, 1993). Deep learning methods leverage autoencoders, recurrent networks, and graph-based temporal models (Zhou et al., 2019; Kwon et al., 2020; Hsu and Lin, 2021). Hybrid detectors combine statistical filtering with neural scoring to improve robustness (Park et al., 2022; Yuan et al., 2022).",
    "document": "Background and Related Work\n\nDetecting anomalies in multivariate time-series is a core requirement in operations, finance, and healthcare. The challenge lies in capturing nonstationarity, inter-variable dependencies, and scarce labeled anomalies.\n\nClassical time-series anomaly detection approaches include control charts, ARIMA modeling, and change-point tests (Page, 1954; Box and Jenkins, 1970; Basseville and Nikiforov, 1993). Deep learning methods leverage autoencoders, recurrent networks, and graph-based temporal models (Zhou et al., 2019; Kwon et al., 2020; Hsu and Lin, 2021). Hybrid detectors combine statistical filtering with neural scoring to improve robustness (Park et al., 2022; Yuan et al., 2022).\n\nOur work targets low-latency streaming with covariate shift. We propose a calibration-aware detector that couples conformal prediction with adaptive graph filters to maintain coverage under nonstationary regimes.",
    "reason": "The span summarizes prior methods but does not explain how they compare to or motivate the proposed calibration-aware detector, failing to identify a gap or relation to the present work (criterion a and b).",
    "start": 248,
    "end": 715,
    "label": "Lacks synthesis"
  },
  {
    "span": "Sequence-to-sequence ASR models benefit from teacher–student training (Hinton et al., 2015). CTC alignment smooths the supervision signal (Graves et al., 2006). SpecAugment improves robustness with time–frequency masking (Park et al., 2019). Streaming architectures trade off latency and accuracy (Zhang et al., 2020).",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) integrates acoustic, pronunciation, and language modeling, but remains sensitive to domain shift and latency constraints. Research has focused on training signals, data augmentation, and architectural adaptations.\n\nSequence-to-sequence ASR models benefit from teacher–student training (Hinton et al., 2015). CTC alignment smooths the supervision signal (Graves et al., 2006). SpecAugment improves robustness with time–frequency masking (Park et al., 2019). Streaming architectures trade off latency and accuracy (Zhang et al., 2020).\n\nWe explore a unified distillation framework that couples CTC-guided alignment with augmentations tailored for streaming constraints, which prior work has addressed piecemeal.",
    "reason": "The span lists several techniques in separate sentences without transitions or an explicit narrative of how they relate, producing an abrupt, incoherent flow between cited works.",
    "start": 275,
    "end": 593,
    "label": "Coherence"
  },
  {
    "span": "Commonly used knowledge bases include ConceptNet and ATOMIC for everyday reasoning (Speer et al., 2017; Hwang et al., 2021). YAGO and Freebase provide encyclopedic facts (Suchanek et al., 2007; Bollacker et al., 2008). GNNs aggregate neighbor messages to encode node representations (Hamilton et al., 2017). KG-BERT formulates link prediction as sentence classification (Yao et al., 2019).",
    "document": "Related Work\n\nInfusing external knowledge into NLP systems can improve robustness and interpretability, especially for reasoning beyond surface text. Prior work spans knowledge resource construction and model architectures that exploit these resources for downstream tasks.\n\nCommonly used knowledge bases include ConceptNet and ATOMIC for everyday reasoning (Speer et al., 2017; Hwang et al., 2021). YAGO and Freebase provide encyclopedic facts (Suchanek et al., 2007; Bollacker et al., 2008). GNNs aggregate neighbor messages to encode node representations (Hamilton et al., 2017). KG-BERT formulates link prediction as sentence classification (Yao et al., 2019).\n\nDespite progress, a gap remains in aligning unstructured text with structured triples at inference time. We propose a retrieval-and-rerank pipeline that grounds LMs in mixed commonsense and encyclopedic graphs while preserving textual fluency.",
    "reason": "The span enumerates commonsense KGs, encyclopedic KGs, GNNs, and KG-BERT sequentially without clarifying how the resources relate to the methods or how one motivates the next, creating abrupt shifts and implied relationships only.",
    "start": 275,
    "end": 664,
    "label": "Coherence"
  },
  {
    "span": "Early neural approaches to math word problem solving used sequence-to-sequence models with attention.",
    "document": "Introduction\n\nMath word problem (MWP) solving requires mapping natural language to structured reasoning steps or executable programs (Wang et al., 2018; Xie and Sun, 2019). Recent work has explored program induction, symbolic execution, and chain-of-thought prompting with large language models to improve arithmetic and algebraic generalization (Cobbe et al., 2021; Wei et al., 2022).\n\nEarly neural approaches to math word problem solving used sequence-to-sequence models with attention. Subsequent research introduced tree-structured decoders and equation templates to better capture operator precedence and commutativity (Zhang et al., 2020; Li et al., 2021). Our method unifies symbolic constraints with neural decoding via a guided search over derivations.",
    "reason": "The span references a specific line of prior work ('seq2seq with attention for MWPs') without citing any papers at first mention, violating rule (a).",
    "start": 387,
    "end": 488,
    "label": "Unsupported claim"
  },
  {
    "span": "Smith, 2019",
    "document": "Related Work\n\nTransfer learning has improved performance in low-resource regimes by leveraging representations learned on large corpora (Howard and Ruder, 2018; Peters et al., 2018). As shown by Smith, 2019, cross-domain pretraining facilitates rapid adaptation with minimal labeled data. Subsequent studies incorporate adapters and prompts to specialize large models efficiently (Houlsby et al., 2019; Lester et al., 2021).\n\nRobust fine-tuning techniques address distribution shifts and catastrophic forgetting (Kirkpatrick et al., 2017; Chen et al., 2020). Data selection strategies further enhance transfer by prioritizing relevant sources (Vu et al., 2020; Gururangan et al., 2020).",
    "reason": "Improper in-text citation style: author and year separated by a comma outside parentheses. Should be narrative 'Smith (2019)' or parenthetical '(Smith, 2019)'.",
    "start": 195,
    "end": 206,
    "label": "Format"
  },
  {
    "span": "BERT was used in automated essay scoring trained on TOEFL essays to achieve state-of-the-art performance",
    "document": "Related Work\n\nAutomated essay scoring (AES) traditionally relied on handcrafted features capturing mechanics, coherence, and argumentation. Neural approaches have shifted toward end-to-end text encoders with holistic representations of writing quality. BERT was used in automated essay scoring trained on TOEFL essays to achieve state-of-the-art performance, inspiring follow-up work on domain adaptation and prompt robustness. Parallel research explores pairwise ranking losses and calibration to better reflect grader uncertainty.\n\nGap\n\nDespite these advances, model explanations often fail to align with rubric criteria. We address this with rubric-guided supervision.",
    "reason": "Describes a specific setup and performance claim about prior work (BERT on TOEFL AES) without citing the corresponding study (violates guideline e/iii and a).",
    "start": 253,
    "end": 357,
    "label": "Unsupported claim"
  },
  {
    "span": "distilling from an ensemble consistently improves BLEU by 2–3 points",
    "document": "Related Work\n\nKnowledge distillation (KD) transfers behavior from a teacher model to a student, often improving accuracy and efficiency. In neural machine translation (NMT), KD can be applied sequence-level, word-level, or via intermediate representations. While single strong teachers are common, ensembles can provide richer targets by smoothing uncertain predictions.\n\nIn broad evaluations on WMT benchmarks, distilling from an ensemble consistently improves BLEU by 2–3 points compared to single-teacher KD. Nevertheless, naïve distillation can reduce diversity and harm domain robustness. Recent advances mitigate these issues through temperature tuning, data selection, and curriculum learning, but gaps remain for low-resource and noisy-domain scenarios.\n\nOur work revisits ensemble KD with a focus on low-resource NMT, introducing a selective distillation pipeline that filters high-entropy segments to preserve lexical diversity while maintaining gains in sentence-level adequacy.",
    "reason": "The claim provides a quantitative summary of prior results (BLEU improvements) attributed to earlier studies but includes no citations to support the statistic.",
    "start": 412,
    "end": 480,
    "label": "Unsupported claim"
  },
  {
    "span": "SemEval-2016 Task 4 defined the sentiment evaluation setting we adopt in this paper",
    "document": "Introduction\n\nSentence-level sentiment analysis benchmarks provide a common basis for comparing classifiers across domains and languages. Variants of polarity and intensity prediction emerged from shared tasks that standardized input formats and metrics. SemEval-2016 Task 4 defined the sentiment evaluation setting we adopt in this paper, enabling direct comparison with systems trained under similar constraints.\n\nOur Contribution\n\nWe propose a contrastive objective that aligns representations across domains while preserving label topology, improving cross-domain generalization.",
    "reason": "First mention of a specific shared task lacks a citation to the task overview or dataset (violates guideline a).",
    "start": 255,
    "end": 338,
    "label": "Unsupported claim"
  },
  {
    "span": "De-identification systems tag HIPAA-protected entities in clinical narratives (Dernoncourt et al., 2017). Entity linking maps mentions to UMLS concepts using dictionary and neural methods (Wu et al., 2019). Phenotyping infers patient conditions from longitudinal EHRs (Pivovarov and Elhadad, 2015). Prompt-based extraction conditions language models on label-verbalizers (Schick and Schütze, 2021).",
    "document": "Related Work\n\nBiomedical NLP\nClinical text processing supports downstream applications by extracting structured information from notes, lab reports, and discharge summaries. Pretrained biomedical language models have improved accuracy on tasks ranging from NER to relation extraction by adapting to domain-specific vocabulary.\n\nDe-identification systems tag HIPAA-protected entities in clinical narratives (Dernoncourt et al., 2017). Entity linking maps mentions to UMLS concepts using dictionary and neural methods (Wu et al., 2019). Phenotyping infers patient conditions from longitudinal EHRs (Pivovarov and Elhadad, 2015). Prompt-based extraction conditions language models on label-verbalizers (Schick and Schütze, 2021).\n\nWe target few-shot extraction of temporally anchored events, combining time-aware prompts with weak supervision from billing codes.",
    "reason": "The span abruptly juxtaposes de-identification, entity linking, phenotyping, and prompt-based extraction without transitions or explicit connections between these tasks, resulting in poor coherence.",
    "start": 328,
    "end": 726,
    "label": "Coherence"
  },
  {
    "span": "Privacy in federated learning has been studied through secure aggregation, differential privacy, and homomorphic encryption, with methods that mask client updates, clip and perturb gradients, or compute encrypted model averages (Bonawitz et al., 2017; Geyer et al., 2017; Truex et al., 2019; Acar et al., 2021; Li et al., 2020).",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative model training without centralizing raw data, but the exchange of model updates can leak sensitive information. Prior research addresses threats ranging from gradient inversion to membership inference under various adversarial models.\n\nPrivacy in federated learning has been studied through secure aggregation, differential privacy, and homomorphic encryption, with methods that mask client updates, clip and perturb gradients, or compute encrypted model averages (Bonawitz et al., 2017; Geyer et al., 2017; Truex et al., 2019; Acar et al., 2021; Li et al., 2020).\n\nComplementary to privacy, communication efficiency and personalization have been explored via compression, client subsampling, and meta-learning, though tensions arise between utility and privacy budgets. Auditing mechanisms and privacy accounting aim to provide end-to-end guarantees but often assume synchronized participation and honest-but-curious servers.\n\nIn this work, we focus on asynchronous FL with intermittent clients and propose privacy amplification by partial aggregation that reduces exposure while preserving accuracy. We empirically analyze the utility–privacy trade-offs under realistic participation patterns and report improvements over strong baselines.",
    "reason": "The span summarizes categories of privacy methods with citations but does not connect them to the specific focus (asynchronous FL) or identify limitations these methods have in that setting, thus lacking synthesis with the paper’s argument (criterion a).",
    "start": 296,
    "end": 624,
    "label": "Lacks synthesis"
  },
  {
    "span": "To mitigate privacy leakage, differential privacy (Abadi et al., 2016), secure aggregation (Bonawitz et al., 2017), and homomorphic encryption (Gentry, 2009; Acar et al., 2018) are commonly employed in federated training.",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative model training without centralizing raw data, offering a promising paradigm for privacy-sensitive domains such as healthcare and finance. Despite its appeal, FL faces challenges including statistical heterogeneity, communication efficiency, and privacy leakage through gradients and model updates.\n\nTo mitigate privacy leakage, differential privacy (Abadi et al., 2016), secure aggregation (Bonawitz et al., 2017), and homomorphic encryption (Gentry, 2009; Acar et al., 2018) are commonly employed in federated training. Complementary work addresses client drift with optimization variants (Karimireddy et al., 2020; Reddi et al., 2021) and reduces bandwidth with compression and sparsification (Suresh et al., 2017; Alistarh et al., 2018).\n\nIn this work, we present a communication-adaptive FL method that balances update frequency with privacy-preserving noise injection to maintain accuracy under constrained network conditions.",
    "reason": "The span introduces a list of techniques with citations but provides no explanation of how these methods relate to the authors' approach or what gap remains, aligning with (a) and (b).",
    "start": 359,
    "end": 580,
    "label": "Lacks synthesis"
  },
  {
    "span": "Recent WMT shared tasks have standardized evaluation protocols for document-level machine translation.",
    "document": "Related Work\n\nDocument-level machine translation (MT) aims to leverage cross-sentence context for improved coherence and consistency. Approaches range from context-augmented encoders to cache-based decoders and multi-sentence transformers that encode surrounding sentences jointly with the current segment.\n\nRecent WMT shared tasks have standardized evaluation protocols for document-level machine translation. Despite this progress, contextual MT systems still struggle with long-distance discourse phenomena and domain shift, motivating architectures that scale to longer contexts with robust generalization.\n",
    "reason": "Mentions specific shared tasks and claims about standardization without citing the relevant WMT task descriptions or papers.",
    "start": 308,
    "end": 410,
    "label": "Unsupported claim"
  },
  {
    "span": "There are many recent works exploring non-autoregressive translation for low-resource languages.",
    "document": "Introduction\n\nNon-autoregressive machine translation (NAT) reduces latency by predicting tokens in parallel, but it often suffers from multimodality and under-translation. While progress has been made through iterative refinement and length prediction, challenges persist in low-resource scenarios due to limited supervision. There are many recent works exploring non-autoregressive translation for low-resource languages. In this paper, we address data scarcity by coupling NAT with lexical constraints derived from bilingual lexicons, aiming to stabilize decoding without sacrificing speed.\n\nWe present a thorough evaluation across diverse typological families, focusing on robustness under domain shift and noisy inputs.\n",
    "reason": "Generic reference to 'many recent works' needs concrete citations.",
    "start": 326,
    "end": 422,
    "label": "Unsupported claim"
  },
  {
    "span": "mBERT and XLM-R",
    "document": "Introduction\n\nCross-lingual named entity recognition (NER) seeks to transfer supervision from high-resource to low-resource languages without relying on manual annotation. Pre-trained multilingual encoders such as mBERT and XLM-R have become the de facto backbone for building cross-lingual NER systems due to their ability to share subword vocabularies across scripts. Despite their success, zero-shot transfer remains brittle when source and target languages are typologically distant. In this paper, we propose a lexicon-anchored adapter that selectively aligns entity-bearing spans across languages while preserving language-specific morphology. We evaluate our method under zero-shot and few-shot conditions and analyze robustness to script variation.\n",
    "reason": "First mention of specific models requires citations to the original papers introducing them, but none are provided.",
    "start": 214,
    "end": 229,
    "label": "Unsupported claim"
  },
  {
    "span": "The MovieLens-25M dataset has become the de facto benchmark for session-based recommendation.",
    "document": "Related Work\n\nSession-based recommendation (SBR) models user short-term intent using sequential architectures such as GRU4Rec and self-attention networks (Hidasi et al., 2016; Kang and McAuley, 2018). Beyond accuracy, recent studies emphasize robustness to noise and counterfactual evaluation for bias mitigation (Saito, 2020; Schnabel et al., 2016).\n\nThe MovieLens-25M dataset has become the de facto benchmark for session-based recommendation. However, its long-tailed distribution and sparse session boundaries pose challenges for fair comparison. We therefore curate three complementary benchmarks with explicit session demarcations and release standardized splits.",
    "reason": "Introduces a specific dataset as a standard benchmark without providing a citation at first mention, which is required for datasets.",
    "start": 352,
    "end": 445,
    "label": "Unsupported claim"
  },
  {
    "span": "Vision Transformers have been adapted to medical imaging through domain-specific pretraining, hybrid CNN-Transformer backbones, and token sparsification to handle high-resolution scans (Dosovitskiy et al., 2021; Chen et al., 2021; Hatamizadeh et al., 2022). Self-supervised objectives such as masked image modeling and contrastive learning have shown benefits under limited annotations (He et al., 2022; Zhou et al., 2022).",
    "document": "Related Work\n\nMedical imaging tasks pose unique challenges due to high resolution, limited annotations, and domain shifts across scanners and populations. Architectures and training strategies have evolved to address these constraints.\n\nVision Transformers have been adapted to medical imaging through domain-specific pretraining, hybrid CNN-Transformer backbones, and token sparsification to handle high-resolution scans (Dosovitskiy et al., 2021; Chen et al., 2021; Hatamizadeh et al., 2022). Self-supervised objectives such as masked image modeling and contrastive learning have shown benefits under limited annotations (He et al., 2022; Zhou et al., 2022).\n\nMulti-task learning and multi-modal fusion further leverage complementary signals across tasks and imaging modalities (Baltrusaitis et al., 2019; Gao et al., 2021).",
    "reason": "This span inventories approaches and findings without connecting them to the paper’s aims, shortcomings to be addressed, or a clear viewpoint, satisfying the lack of synthesis conditions a and c.",
    "start": 237,
    "end": 660,
    "label": "Lacks synthesis"
  },
  {
    "span": "Fairness metrics in supervised learning cover demographic parity, equality of opportunity, equalized odds, and calibration across groups (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017). Mitigation strategies range from pre-processing reweighting to in-processing regularizers and post-processing threshold adjustments (Kamiran and Calders, 2012; Zafar et al., 2017; Pleiss et al., 2017).",
    "document": "Related Work\n\nAlgorithmic fairness has been formalized through a variety of statistical criteria and optimization-based interventions. Practical deployments must contend with trade-offs among accuracy, fairness, and interpretability.\n\nFairness metrics in supervised learning cover demographic parity, equality of opportunity, equalized odds, and calibration across groups (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017). Mitigation strategies range from pre-processing reweighting to in-processing regularizers and post-processing threshold adjustments (Kamiran and Calders, 2012; Zafar et al., 2017; Pleiss et al., 2017).\n\nContextualized evaluations increasingly emphasize subgroup robustness and intersectional analyses across realistic distribution shifts (Kearns et al., 2018; Sagawa et al., 2020).",
    "reason": "The span catalogs metrics and mitigation strategies without indicating which limitations motivate the present work or how these methods relate to the authors’ approach, failing to synthesize prior literature with their argument (criteria a and c).",
    "start": 235,
    "end": 639,
    "label": "Lacks synthesis"
  },
  {
    "span": "Multimodal grounding for robotics has leveraged joint vision–language pretraining, contrastive alignment between images and text, and affordance learning from demonstrations (Lu et al., 2019; Radford et al., 2021; Sharma et al., 2022; Nair et al., 2022; Zeng et al., 2020).",
    "document": "Related Work\n\nGrounding natural language instructions in robotic manipulation requires connecting linguistic references to actionable percepts and motor skills. Data scarcity and domain shift from web imagery to robot viewpoints complicate the problem.\n\nMultimodal grounding for robotics has leveraged joint vision–language pretraining, contrastive alignment between images and text, and affordance learning from demonstrations (Lu et al., 2019; Radford et al., 2021; Sharma et al., 2022; Nair et al., 2022; Zeng et al., 2020).\n\nSeveral works adapt web-pretrained models to table-top settings, but transferring to cluttered, dynamic scenes remains difficult. We explore a 3D-aware grounding pipeline that fuses language-conditioned radiance fields with contact-centric policies for precise manipulation.\n\nWe benchmark on multi-object manipulation tasks with partial observability and show improved success rates and generalization to novel object layouts.",
    "reason": "The span lists existing multimodal grounding approaches without tying them to the specific 3D-aware pipeline or identifying the transfer gap to cluttered dynamic scenes, thus lacking synthesis (criterion a).",
    "start": 254,
    "end": 527,
    "label": "Lacks synthesis"
  },
  {
    "span": "In the M3C shared task, most participating systems used ensemble methods.",
    "document": "Related Work\n\nBenchmark competitions catalyze progress by publishing standardized data and blind test servers. In the M3C shared task, most participating systems used ensemble methods. Post-hoc analyses further indicate that data augmentation and reranking contributed more gains than architectural novelty.",
    "reason": "Refers to a specific shared task and summarizes participant behavior without any citation to the task report or overview paper.",
    "start": 111,
    "end": 184,
    "label": "Unsupported claim"
  },
  {
    "span": " (Nguyen et al., 2018",
    "document": "Related Work\n\nMedical image segmentation has progressed from atlas-based techniques to fully convolutional networks (Long et al., 2015; Ronneberger et al., 2015). Multi-scale feature aggregation and attention improve delineation of small structures (Oktay et al., 2018; Schlemper et al., 2019). As reported in (Nguyen et al., 2018 we observe that semi-supervised consistency losses can close much of the gap to fully supervised training, especially with strong augmentations (Zhou et al., 2019; Bai et al., 2017).",
    "reason": "Missing closing parenthesis in the citation.",
    "start": 309,
    "end": 330,
    "label": "Format"
  },
  {
    "span": "We adopt the preprocessing pipeline of the WMT20 News Translation task.",
    "document": "Experimental Setup\n\nWe evaluate English–German translation under controlled tokenization and truecasing to isolate the impact of pretraining objectives. Consistency in preprocessing is critical for fair comparison across models and training regimens.\n\nWe adopt the preprocessing pipeline of the WMT20 News Translation task. Our training data include standard parallel corpora filtered by sentence length and language identification, with development sets drawn from held-out news articles.\n\nAll systems are trained with identical batch sizes and learning-rate schedules to avoid confounding effects.",
    "reason": "References a specific shared task setup without citing the task description or official resources (violates rule a).",
    "start": 252,
    "end": 323,
    "label": "Unsupported claim"
  },
  {
    "span": "There have been numerous competitions on open-domain dialogue that standardized evaluation protocols.",
    "document": "Related Work\n\nOpen-domain dialogue systems aim to produce engaging, safe, and informative responses in unconstrained conversations (Gupta et al., 2020). Despite progress, evaluation remains challenging due to weak correlation of automatic metrics with human judgments (Huang and Li, 2021).\n\nThere have been numerous competitions on open-domain dialogue that standardized evaluation protocols.\n\nConcurrently, benchmarks have diversified to include safety, grounding, and long-horizon coherence. Our work focuses on human-in-the-loop evaluation with calibrated annotator incentives.",
    "reason": "References 'numerous competitions' without providing citations to any of the competitions or shared tasks.",
    "start": 291,
    "end": 392,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works demonstrate that retrieval-augmented decoders consistently outperform purely parametric models on competitive coding benchmarks.",
    "document": "Introduction\n\nProgram synthesis and code generation have advanced rapidly with the emergence of large language models tailored to source code, enabling competitive performance on tasks such as code completion, repair, and documentation generation (Smith and Rao, 2021; Li et al., 2022). Despite impressive results, purely parametric models struggle with long-tail library usage and project-specific conventions that are not well represented in pretraining corpora (Zhang et al., 2023).\n\nA prominent strategy to mitigate these limitations augments generation with retrieval from external repositories, bringing in relevant snippets, API examples, or project histories at inference time (Kumar et al., 2022; Perez et al., 2023). Retrieval-augmented generation aligns code synthesis with concrete, verifiable contexts and allows models to ground predictions in real code artifacts.\n\nRecent works demonstrate that retrieval-augmented decoders consistently outperform purely parametric models on competitive coding benchmarks. However, open questions remain about how to select, filter, and structure retrieved content for long-context decoders, and how retrieval interacts with model scale and domain specialization. We investigate these questions with a systematic study of retrieval sources and prompt structuring strategies.",
    "reason": "The sentence claims performance advantages by 'recent works' without providing any citations to those works, violating the requirement to cite prior studies when first mentioned.",
    "start": 880,
    "end": 1021,
    "label": "Unsupported claim"
  },
  {
    "span": "Acoustic cues have been modeled with temporal CNNs and LSTMs to capture prosody (Zhou et al., 2018; Rana and Gupta, 2019). The CMU-MOSI and MOSEI datasets are commonly used benchmarks (Zadeh et al., 2016; Zadeh et al., 2018). Transformers fine-tuned on text have improved sentiment detection (Gao and Lin, 2020).",
    "document": "Related Work\n\nMultimodal sentiment analysis integrates signals from language, vision, and audio to assess opinions. Early works fused hand-crafted features across modalities (Poria et al., 2015), while recent methods learn end-to-end representations (Mai et al., 2020). Acoustic cues have been modeled with temporal CNNs and LSTMs to capture prosody (Zhou et al., 2018; Rana and Gupta, 2019). The CMU-MOSI and MOSEI datasets are commonly used benchmarks (Zadeh et al., 2016; Zadeh et al., 2018). Transformers fine-tuned on text have improved sentiment detection (Gao and Lin, 2020). Visual attention mechanisms further localize informative frames (Sun et al., 2021). Our approach revisits cross-modal alignment with a contrastive objective tailored to scarce labeled data.",
    "reason": "The span abruptly jumps from modeling acoustic cues to naming datasets and then to transformer-based text modeling without articulating how these elements are connected; there are no transitions or explicit relationships between the cited works.",
    "start": 270,
    "end": 582,
    "label": "Coherence"
  },
  {
    "span": "Approaches to robotic grasp planning include analytical methods based on wrench space analysis (Prattichizzo and Trinkle, 2016), learning-based grasp affordance maps (Lopez-Guevara et al., 2019), and depth-driven sampling strategies with CNN scoring (Hernandez et al., 2020; Yu and Chen, 2021).",
    "document": "Related Work\n\nRobotic grasping has transitioned from model-based planning under rigid assumptions to data-driven policies that generalize to novel objects and cluttered scenes. The literature spans physics-based reasoning and large-scale supervised training.\n\nApproaches to robotic grasp planning include analytical methods based on wrench space analysis (Prattichizzo and Trinkle, 2016), learning-based grasp affordance maps (Lopez-Guevara et al., 2019), and depth-driven sampling strategies with CNN scoring (Hernandez et al., 2020; Yu and Chen, 2021). Extensions address uncertainty via active perception and domain randomization (Kwan et al., 2022; Dai et al., 2023).\n\nThis paper proposes a coarse-to-fine policy for bin-picking with minimal calibration.",
    "reason": "The span lists categories and citations without explaining how they inform the proposed coarse-to-fine policy or what shortcomings remain, reflecting a lack of synthesis (definition a and c).",
    "start": 260,
    "end": 554,
    "label": "Lacks synthesis"
  },
  {
    "span": "Large-scale pretraining has transformed summarization, question answering, and information extraction by enabling models to adapt with minimal supervision (Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020). Prompting, instruction tuning, and in-context learning further improve zero-shot performance across tasks (Brown et al., 2020; Sanh et al., 2022; Ouyang et al., 2022). Several studies investigate controllability via constraints and policy shaping (Keskar et al., 2019; Dathathri et al., 2020; Lu et al., 2022).",
    "document": "Related Work\n\nControllable summarization aims to generate targeted, faithful summaries under user-specified attributes such as length, focus, or style. Ensuring control without sacrificing factuality is an open challenge.\n\nLarge-scale pretraining has transformed summarization, question answering, and information extraction by enabling models to adapt with minimal supervision (Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020). Prompting, instruction tuning, and in-context learning further improve zero-shot performance across tasks (Brown et al., 2020; Sanh et al., 2022; Ouyang et al., 2022). Several studies investigate controllability via constraints and policy shaping (Keskar et al., 2019; Dathathri et al., 2020; Lu et al., 2022).\n\nOur work studies control tokens that modulate content selection and coverage while maintaining factuality under domain shift.",
    "reason": "The span summarizes general advances with citations but does not connect them to controllable summarization specifically or explain how they motivate or contrast with the authors' approach.",
    "start": 223,
    "end": 752,
    "label": "Lacks synthesis"
  },
  {
    "span": "Zhang et.al., 2020",
    "document": "Related Work\n\nVision Transformers adapted self-attention to image recognition, achieving competitive results with sufficient data and regularization (Dosovitskiy et al., 2021; Steiner et al., 2021). Subsequent works improved data efficiency through distillation and hybrids with convolutions (Touvron et al., 2021; Graham et al., 2021). Zhang et.al., 2020 introduced hierarchical token aggregation for dense prediction, while later models explored windowed attention and locality biases (Liu et al., 2021; Chu et al., 2021).\n",
    "reason": "Incorrect 'et al.' formatting; it should be 'Zhang et al., 2020' (no period after 'et' and space before 'al.').",
    "start": 337,
    "end": 355,
    "label": "Format"
  },
  {
    "span": "There have been many recent works that integrate graph encoders into multimodal transformers",
    "document": "Related Work\n\nVision–language pretraining has advanced rapidly with contrastive objectives aligning images and text (Radford et al., 2021; Jia et al., 2021). Structured knowledge has also been explored to enrich representations for reasoning over objects and relations (Yao et al., 2018; Marino et al., 2017). There have been many recent works that integrate graph encoders into multimodal transformers. In parallel, retrieval-augmented models leverage external corpora to ground generation (Gao et al., 2022; Izacard and Grave, 2021). Our method combines region-level graphs with cross-modal attention to improve compositional understanding.",
    "reason": "Mentions 'recent works' without providing citations.",
    "start": 310,
    "end": 402,
    "label": "Unsupported claim"
  },
  {
    "span": "Prior work shows that adding sentence-level supervision consistently improves open-domain QA.",
    "document": "Related Work\n\nOpen-Domain Question Answering. Retrieval-augmented readers achieved large gains by jointly learning dense retrieval and extraction (Chen et al., 2017; Karpukhin et al., 2020). Improvements stem from better negative sampling, multi-vector representations, and reranking (Xiong et al., 2021; Ma et al., 2021; Formal et al., 2021). Prior work shows that adding sentence-level supervision consistently improves open-domain QA. Recent approaches also exploit iterative retrieve-and-read loops and generative readers for long-form answers (Izacard and Grave, 2021; Lewis et al., 2020). Our method introduces lightweight sentence rationales that regularize both retriever and reader with minimal annotation cost.",
    "reason": "Claims a consensus from prior work but provides no citations to support the statement.",
    "start": 344,
    "end": 437,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works have explored prompt-based methods for stance detection across social media platforms.",
    "document": "Introduction\n\nStance detection aims to infer an author's position toward a target (e.g., policy, person, or product) from text. While early work relied on lexical features and distant supervision, modern approaches increasingly leverage pretrained language models to handle domain shift and sparse supervision. Zero- and few-shot settings are especially appealing because annotated data for new targets is often limited.\n\nOne promising direction reformulates stance detection as textual entailment by pairing a post with a hypothesis about support or opposition. Another line of work uses label descriptions or templates to better align input and output spaces for transfer. Recent works have explored prompt-based methods for stance detection across social media platforms. However, the extent to which prompt design and calibration transfer across targets and domains remains underexplored, motivating our study.",
    "reason": "Mentions 'recent works' without providing citations to specific studies (rule d).",
    "start": 675,
    "end": 774,
    "label": "Unsupported claim"
  },
  {
    "span": "There are many recent works that leverage retrieval-augmented generation for multilingual QA",
    "document": "Introduction\n\nOpen-domain question answering (QA) has increasingly shifted toward models that combine parametric knowledge with non-parametric retrieval. While monolingual QA has received substantial attention, multilingual QA remains underexplored due to data scarcity, uneven resource distribution across languages, and cross-lingual knowledge gaps. There are many recent works that leverage retrieval-augmented generation for multilingual QA, arguing that retrieval mitigates hallucination and improves cross-lingual grounding. However, the precise role of retrieval quality versus generation capacity across languages is not well understood.\n\nIn this paper, we study retrieval-augmented generation for multilingual QA under varying retrieval resources. We propose a controlled evaluation suite that pairs balanced multilingual corpora with standardized retrieval pipelines, enabling apples-to-apples comparisons across languages. We further examine the impact of contrastive reranking and cross-lingual query reformulation on answer accuracy and calibration.\n\nOur contributions are threefold: (1) a framework for controlled multilingual retrieval evaluation, (2) an analysis of generation robustness under retrieval perturbations, and (3) a set of practical recommendations for deploying RAG systems in multilingual settings.\n",
    "reason": "Mentions 'recent works' without providing any citations, violating rule d and rule a for first mentions of prior work.",
    "start": 352,
    "end": 444,
    "label": "Unsupported claim"
  },
  {
    "span": "Prompting has been used to elicit latent knowledge from large language models (Brown et al., 2020; Liu et al., 2023). Instruction tuning aligns models with human-preferred behaviors (Ouyang et al., 2022). Few-shot chain-of-thought prompting improves multi-step reasoning on math benchmarks (Wei et al., 2022). Retrieval augmentation attaches an external memory to pretrained decoders (Lewis et al., 2020).",
    "document": "Related Work\n\nLarge language models (LLMs) have rapidly advanced the state of the art across many NLP tasks, but their behavior remains sensitive to how inputs are framed. Prior work has examined both intrinsic prompting strategies and extrinsic knowledge integration to steer model outputs.\n\nPrompting has been used to elicit latent knowledge from large language models (Brown et al., 2020; Liu et al., 2023). Instruction tuning aligns models with human-preferred behaviors (Ouyang et al., 2022). Few-shot chain-of-thought prompting improves multi-step reasoning on math benchmarks (Wei et al., 2022). Retrieval augmentation attaches an external memory to pretrained decoders (Lewis et al., 2020).\n\nOther strands consider controllability through constrained decoding (Hokamp and Liu, 2017) and safety filters for refusal and hallucination reduction (Solaiman et al., 2019). Our work studies how lightweight prompt edits interact with retrieval augmentation under distribution shift, focusing on sample-efficient adaptation without additional fine-tuning.",
    "reason": "The span lists several lines of research (prompting, instruction tuning, chain-of-thought, retrieval augmentation) in consecutive sentences without transitions or an explicit explanation of how each relates to the previous one, creating abrupt topic shifts and unclear connections.",
    "start": 293,
    "end": 698,
    "label": "Coherence"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nSubword segmentation is critical for neural machine translation because it mediates the trade-off between vocabulary coverage and sequence length (Sennrich et al., 2016; Kudo, 2018). As argued by [12], overly aggressive segmentation harms morphology-rich languages by fragmenting affixes. However, choosing segmentation granularity a priori ignores domain-specific token statistics (Alvarez and Noor, 2020). We propose a data-driven controller that jointly tunes segmentation and model capacity on a validation proxy of translation adequacy (Bae and Collins, 2022). Our experiments on four language pairs show consistent BLEU and COMET gains over fixed BPE settings.",
    "reason": "Wrong citation style: numeric bracketed citation used in an author–year context; should be an author–year citation.",
    "start": 210,
    "end": 214,
    "label": "Format"
  },
  {
    "span": "Matrix factorization captures user–item interactions via low-rank embeddings (Koren et al., 2009). Graph-based recommenders propagate preferences over user–item bipartite graphs (Wang et al., 2019). Session-based recommenders use recurrent networks to model short-term intent (Hidasi et al., 2016). Counterfactual explanations justify recommendations by minimal changes (Mothilal et al., 2020).",
    "document": "Related Work: Recommendation Algorithms and Explanations\n\nRecommender systems have evolved from rating prediction to context-aware, sequential, and explainable recommendation. Methods aim to leverage structure, capture dynamics, and provide transparent rationales to end-users.\n\nMatrix factorization captures user–item interactions via low-rank embeddings (Koren et al., 2009). Graph-based recommenders propagate preferences over user–item bipartite graphs (Wang et al., 2019). Session-based recommenders use recurrent networks to model short-term intent (Hidasi et al., 2016). Counterfactual explanations justify recommendations by minimal changes (Mothilal et al., 2020).\n\nRecent work unifies graph and sequence modeling through graph transformers for dynamic recommendation (Xu et al., 2020). Other approaches provide model-agnostic post-hoc rationales using knowledge graphs and templates (Wang et al., 2018). Our method integrates counterfactual reasoning into training, aligning ranking objectives with explanation faithfulness.",
    "reason": "The span lists four areas—matrix factorization, graph methods, session-based models, and counterfactual explanations—without explaining how they relate or progress; the introduction of explanations is abrupt and unconnected to prior sentences (issues a and b).",
    "start": 279,
    "end": 673,
    "label": "Coherence"
  },
  {
    "span": "Alvarez et al.",
    "document": "Related Work\n\nNeural sequence labeling has evolved from feature-driven models to neural architectures with contextualized representations. Early approaches relied on conditional random fields and handcrafted features (Smith and Eisner, 2008; Finkel et al., 2005), while recent methods leverage transformers with task-specific heads (Devlin et al., 2019; Liu et al., 2019). Following Alvarez et al., we adopt a linear-chain decoding layer to enforce label consistency, but we replace sparse features with self-attentive token encoders (Chen et al., 2019; Liu and Wu, 2020). Data augmentation for low-resource tagging has explored paraphrasing and back-translation (Xie et al., 2020; Ding et al., 2021), as well as consistency regularization (Clark et al., 2020). In contrast, our work focuses on structure-aware perturbations that preserve entity boundaries while improving robustness to domain shift (Gururangan et al., 2020; Ruder et al., 2019).",
    "reason": "Narrative citation missing year; should be formatted as Alvarez et al. (YEAR) in APA-style prose.",
    "start": 383,
    "end": 397,
    "label": "Format"
  },
  {
    "span": "the widely used StreetScenes-500 dataset",
    "document": "Related Work\n\nUrban scene understanding has advanced rapidly with the advent of high-capacity segmentation backbones and refined decoding heads. Benchmarking in this space typically involves diverse conditions, such as day/night cycles and adverse weather, to evaluate robustness. We evaluate on the widely used StreetScenes-500 dataset and complement our analysis with results on UrbanDrive, focusing on cross-weather generalization. Our contributions lie in a simple yet strong data-centric approach that augments rare classes without introducing label noise.",
    "reason": "Introduces a specific dataset at first mention without a citation, which should be provided per (a).",
    "start": 296,
    "end": 336,
    "label": "Unsupported claim"
  },
  {
    "span": "Klein and Novak",
    "document": "Introduction\n\nCurriculum learning schedules examples from easy to hard to accelerate convergence and improve generalization in non-convex training (Martins et al., 2017). While pacing functions can be hand-crafted (Rao et al., 2018) or learned via bandit objectives (Lee et al., 2020), recent works integrate curricula with dynamic data curation pipelines.\n\nFollowing the original formulation by Klein and Novak, we consider a difficulty-aware sampler that ranks instances by a surrogate loss and gradually increases the sampling temperature. Later refinements leverage uncertainty calibration to counter curriculum-induced bias (Patel and Xu, 2019) and use diversity constraints to avoid mode collapse (Ng and Silva, 2021). We adopt these principles and show that our pacing policy is robust across domains.\n\nIn addition, we connect curricula with test-time adaptation, showing that paced fine-tuning can mitigate distribution shifts (Hernandez et al., 2022).",
    "reason": "Narrative citation missing the year; it should be 'Klein and Novak (2019)' (or the appropriate year) to conform to author–year style.",
    "start": 396,
    "end": 411,
    "label": "Format"
  },
  {
    "span": "For object detection, common augmentation techniques include geometric transforms, photometric jittering, and composition-based mixing (Shorten and Khoshgoftaar, 2019; Yun et al., 2019; Ghiasi et al., 2021; Zoph et al., 2020). MixUp and CutMix blend images and labels to regularize training, while mosaic and copy-paste augment small object frequency and context diversity (Zhang et al., 2018; Yun et al., 2019; Bochkovskiy et al., 2020).",
    "document": "Related Work\n\nData Augmentation for Object Detection\nFor object detection, common augmentation techniques include geometric transforms, photometric jittering, and composition-based mixing (Shorten and Khoshgoftaar, 2019; Yun et al., 2019; Ghiasi et al., 2021; Zoph et al., 2020). MixUp and CutMix blend images and labels to regularize training, while mosaic and copy-paste augment small object frequency and context diversity (Zhang et al., 2018; Yun et al., 2019; Bochkovskiy et al., 2020).\n\nPolicy Search for Augmentation\nAutomated policies such as AutoAugment and RandAugment search for augmentation schedules, while TrivialAugment and AugMix emphasize simplicity and robustness (Cubuk et al., 2019; Cubuk et al., 2020; Müller and Hutter, 2021; Hendrycks et al., 2020).\n\nLong-Tail Recognition\nAugmentation has also been adapted to mitigate long-tail class imbalance using re-sampling, re-weighting, and synthetic minority over-sampling (Kang et al., 2020; Buda et al., 2018; Chawla et al., 2002).",
    "reason": "Pure enumeration of techniques and citations without connecting them to the authors' problem setting or indicating how the present work builds on or differs from them (definition a).",
    "start": 53,
    "end": 491,
    "label": "Lacks synthesis"
  },
  {
    "span": "Transformer transducers consistently outperform hybrid HMM-DNN systems on low-resource ASR.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) in low-resource languages remains challenging due to limited transcribed audio and domain shifts between training and deployment. Transformer transducers consistently outperform hybrid HMM-DNN systems on low-resource ASR. However, these sequence-to-sequence models are sensitive to pronunciation variation and require careful data augmentation. We propose a pronunciation-informed pretraining scheme that injects phoneme-level constraints into the encoder, improving recognition for underrepresented phonotactics.\n",
    "reason": "A comparative performance claim about model families is made without any citation or empirical evidence in the text.",
    "start": 179,
    "end": 270,
    "label": "Unsupported claim"
  },
  {
    "span": "Smith (2019; 2020)",
    "document": "Related Work\n\nCausal Inference and Representation Learning\n\nBalancing methods aim to approximate randomized experiments in observational data (Rosenbaum and Rubin, 1983). Smith (2019; 2020) explored learned representations that satisfy overlap while minimizing confounding, complementary to propensity-based approaches (Austin, 2011). Deep causal inference has proposed invariant representation learning across environments (Arjovsky et al., 2019) and counterfactual regression with integral probability metrics (Shalit et al., 2017). Recent works integrate textual covariates using pretrained encoders to reduce unobserved confounding (Veitch et al., 2020).",
    "reason": "Wrong delimiter between multiple years in a narrative citation; should be Smith (2019, 2020).",
    "start": 171,
    "end": 189,
    "label": "Format"
  },
  {
    "span": "Multilingual PLMs enable zero-shot transfer across languages (Conneau et al., 2020; Xue et al., 2021). Active learning selects informative samples to reduce labeling costs (Settles, 2009). Cross-lingual NER benefits from shared subword vocabularies (Pires et al., 2019). BatchBALD improves batch selection by modeling mutual information (Kirsch et al., 2019).",
    "document": "Related Work\n\nLow-resource named entity recognition (NER) can benefit from both cross-lingual transfer and efficient data acquisition strategies. We discuss advances in multilingual representation learning and active learning that motivate our pool-based selection method for NER.\n\nMultilingual PLMs enable zero-shot transfer across languages (Conneau et al., 2020; Xue et al., 2021). Active learning selects informative samples to reduce labeling costs (Settles, 2009). Cross-lingual NER benefits from shared subword vocabularies (Pires et al., 2019). BatchBALD improves batch selection by modeling mutual information (Kirsch et al., 2019).\n\nOur approach unifies cross-lingual priors with uncertainty-aware selection by estimating token-level informativeness under a shared multilingual encoder, enabling label-efficient adaptation in target languages.",
    "reason": "The span juxtaposes multilingual transfer, active learning, subword sharing, and BatchBALD without connecting statements or transitions that clarify how they relate, resulting in an abrupt and incoherent flow.",
    "start": 282,
    "end": 641,
    "label": "Coherence"
  },
  {
    "span": "Garcia et al. 2",
    "document": "Related Work\n\nGraph-based encoders have improved relation extraction by modeling structural dependencies (Zhang and Chen, 2020; Yao et al., 2019). A preliminary version appeared in Garcia et al. 2 and was expanded in subsequent conference proceedings (Hsu et al., 2021; Li et al., 2022). Most recent work integrates span-level cues with global constraints (Paolini et al., 2021; Wang and Lu, 2020).\n\nWe differ by introducing a constraint-aware decoder that enforces type and role compatibility during inference.",
    "reason": "Improper footnote-like notation '2' in place of a proper citation; it should include the year or be formatted as a proper footnote/endnote reference.",
    "start": 181,
    "end": 196,
    "label": "Format"
  },
  {
    "span": "Garcia et al.",
    "document": "Introduction\n\nNeural sequence tagging has advanced rapidly with the advent of contextual encoders and large-scale pretraining (Peters et al., 2018; Devlin et al., 2019). Early pool-based active learning for NER emphasized uncertainty sampling (Lewis and Gale, 1994) and density weighting (Settles, 2009). Building on this line, Garcia et al. introduce a margin-based query strategy for multilingual settings, showing gains on low-resource corpora. However, domain shift remains challenging even with modern transformers (Wolf et al., 2020). In this paper, we revisit sampling under distribution shift and propose a distributionally robust criterion that balances informativeness and coverage.\n\nRelated Work\n\nActive learning for sequence labeling spans uncertainty-, diversity-, and representativeness-based approaches (Sener and Savarese, 2018; Ash et al., 2020). The interplay between pretraining and acquisition strategies has been explored in QA and NLI (Yuan et al., 2020; Ein-Dor et al., 2020), yet few works assess multilingual robustness (Conneau et al., 2020). We position our method as a lightweight alternative to Bayesian ensembling (Gal and Ghahramani, 2016) that scales to millions of instances.",
    "reason": "Narrative citation missing year. The narrative mention 'Garcia et al.' should include a year in parentheses, e.g., 'Garcia et al. (2019)'.",
    "start": 328,
    "end": 341,
    "label": "Format"
  },
  {
    "span": "Huang et al. (2020",
    "document": "Related Work\n\nGraph contrastive learning has advanced through objectives that maximize agreement between augmented views of a node or subgraph (Jiang et al., 2019; Lopez and Kim, 2021). Early methods considered context prediction on random walks and substructures to shape node embeddings (Park et al., 2018). Variational perspectives further connected mutual information bounds with practical estimators for graphs (Sun and Duarte, 2020).\n\nFor semi-supervised classification on sparse graphs, Huang et al. (2020 propose an adaptive neighborhood sampler that strengthens local signals while preserving global structure. Concurrently, meta-learning approaches attempt to transfer acquisition strategies across domains (Qi and Ramos, 2021), while self-training pipelines refine pseudo-labels using confidence thresholds (Singh et al., 2022). Our method builds on these insights by aligning local and global objectives with an uncertainty-aware selection step.\n\nWe differ from augmentation-heavy pipelines by constraining transformations to degree-preserving rewires (Cho and Feldman, 2020), which we find to be more stable for small homophily regimes (Diaz et al., 2022).",
    "reason": "Missing closing parenthesis in the narrative citation; it should be 'Huang et al. (2020)' with a closing ')'.",
    "start": 494,
    "end": 512,
    "label": "Format"
  },
  {
    "span": "[27]",
    "document": "Related Work\n\nRecommender systems began with neighborhood-based collaborative filtering (Resnick et al., 1994) and matrix factorization (Koren et al., 2009). Neural recommenders leverage implicit feedback with pairwise ranking objectives (Rendle et al., 2009; He et al., 2017) and graph structures for higher-order connectivity (Wang et al., 2019). Several deep architectures report state-of-the-art results [27], while others emphasize explainability and counterfactual evaluation (Zhang and Chen, 2020; Wang et al., 2020).\n",
    "reason": "Numeric bracketed citation '[27]' is inconsistent with the surrounding author–year style; it should be replaced with an author–year citation (e.g., '(Author, Year)').",
    "start": 408,
    "end": 412,
    "label": "Format"
  },
  {
    "span": "State-of-the-art end-to-end models consistently outperform hybrid systems by 15% relative WER on conversational benchmarks.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN pipelines to fully neural, end-to-end (E2E) architectures such as CTC, RNN-T, and attention-based transducers. These models jointly learn acoustic and language modeling components and simplify training and deployment.\n\nWhile hybrid systems remain competitive in some domains, E2E approaches have gained traction due to modeling flexibility and scalability. State-of-the-art end-to-end models consistently outperform hybrid systems by 15% relative WER on conversational benchmarks. However, domain shift, accent variability, and limited on-device compute still pose significant challenges.",
    "reason": "Claims a quantified, consistent performance gap across benchmarks without citing comparative studies (rule b, e).",
    "start": 443,
    "end": 566,
    "label": "Unsupported claim"
  },
  {
    "span": "The i2b2 2014 track established a standard corpus for PHI tagging in clinical notes.",
    "document": "Introduction\n\nClinical named entity recognition (NER) and de-identification are foundational for secondary use of electronic health records, enabling cohort discovery and clinical research while preserving privacy (Meystre et al., 2014; Johnson et al., 2016). Earlier systems relied on rules and dictionaries, but neural sequence models have become prevalent with the availability of annotated corpora (Lample et al., 2016; Dernoncourt et al., 2017). The i2b2 2014 track established a standard corpus for PHI tagging in clinical notes. Despite this progress, portability across institutions remains limited due to domain shift and annotation guideline drift. We propose a calibration-based uncertainty framework to flag low-confidence spans for human review, improving safety without retraining.",
    "reason": "Introduces a specific shared task/dataset at first mention without providing a citation, which is required.",
    "start": 451,
    "end": 535,
    "label": "Unsupported claim"
  },
  {
    "span": "Lee et al. 3",
    "document": "Related Work\n\nAbstractive summarization methods have progressed from sequence-to-sequence baselines to pretrained encoder–decoder models with copy and coverage mechanisms (Rossi and Nguyen, 2019; Banerjee et al., 2020). Recent advances focus on factuality and controllability via constrained decoding or planning modules (Kumar and Wang, 2021; Ortega et al., 2022).\n\nHybrid extractive–abstractive pipelines first select salient sentences and then paraphrase them to improve fluency. Early evidence suggests that learning selection and rewriting jointly mitigates exposure bias (Fang and Silva, 2020). For a thorough survey of planning-based approaches, see Lee et al. 3 and references therein.\n\nOur work extends planning with entity-aware templates that preserve coreference chains while allowing stylistic variation.",
    "reason": "Wrong use of footnote-style numbering with author–year references. 'Lee et al. 3' should include a year (e.g., Lee et al., 2021) or be reformatted as a proper footnote/endnote; numeric superscripts are inconsistent with the author–year style used elsewhere.",
    "start": 657,
    "end": 669,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claim that over-smoothing is inevitable as depth grows.",
    "document": "Related Work\n\nGraph neural networks (GNNs) propagate and transform node features across edges to learn powerful representations for classification and link prediction. Architectures such as GCNs, GraphSAGE, and GAT differ primarily in their aggregation and attention mechanisms, with regularization strategies to mitigate overfitting on sparse labels.\n\nIn a previous study, the authors claim that over-smoothing is inevitable as depth grows. Follow-up works propose residual connections, normalization, and decoupled propagation to retain discriminability over multiple layers. Other lines of research examine over-squashing and curvature-aware message passing to address information bottlenecks.\n\nOur method revisits propagation with a frequency-aware filter that selectively amplifies class-relevant components, enabling deeper stacks without collapsing node embeddings.",
    "reason": "References 'a previous study' and its claim without citing which study; violates rule b) and e) requiring a citation when mentioning prior work.",
    "start": 353,
    "end": 441,
    "label": "Unsupported claim"
  },
  {
    "span": "Back-translation leverages monolingual target data to generate synthetic sources and has become a standard augmentation technique (Sennrich et al., 2016; Edunov et al., 2018). Tagged back-translation and noise-aware objectives further improve robustness (Caswell et al., 2019; Marie et al., 2020). For extremely low-resource settings, multilingual transfer and pivoting have been explored (Johnson et al., 2017; Aji et al., 2020). We follow this line by synthesizing pseudo-parallel corpora for our language pair.",
    "document": "Related Work\n\nData Augmentation in Low-Resource Neural Machine Translation\n\nNeural machine translation quality degrades in low-resource regimes due to overfitting and exposure bias. Augmentation strategies aim to enrich the training signal using monolingual data, multilingual supervision, or synthetic examples to improve generalization.\n\nBack-translation leverages monolingual target data to generate synthetic sources and has become a standard augmentation technique (Sennrich et al., 2016; Edunov et al., 2018). Tagged back-translation and noise-aware objectives further improve robustness (Caswell et al., 2019; Marie et al., 2020). For extremely low-resource settings, multilingual transfer and pivoting have been explored (Johnson et al., 2017; Aji et al., 2020). We follow this line by synthesizing pseudo-parallel corpora for our language pair.\n\nRecent work also investigates denoising autoencoding, dual learning, and consistency regularization to stabilize training when parallel data are scarce (He et al., 2016; Xia et al., 2017; Xie et al., 2020).",
    "reason": "The text lists prior augmentation techniques and states that the authors follow this line, but it does not specify what shortcoming remains or how their approach addresses a particular gap.",
    "start": 340,
    "end": 853,
    "label": "Lacks synthesis"
  },
  {
    "span": "Recently, several studies have investigated persona-grounded dialogue with memory-augmented transformers.",
    "document": "Related Work\n\nGrounded conversation models aim to produce responses that are faithful to external sources such as profiles, documents, or knowledge graphs (Zhang et al., 2018; Dinan et al., 2019). Retrieval-augmented generation has emerged as an effective paradigm to inject evidence into neural dialogue systems (Lewis et al., 2020; Guu et al., 2020). Recently, several studies have investigated persona-grounded dialogue with memory-augmented transformers. In parallel, controllable generation methods seek to steer dialogue style and content using conditioning variables or learned control codes (Keskar et al., 2019; Dathathri et al., 2020). Our work combines retrieval with lightweight control signals to improve persona consistency without degrading factuality.",
    "reason": "The sentence claims the existence of multiple recent studies but provides no citations; per the definition, mentions of 'recent works' must be backed by references.",
    "start": 353,
    "end": 458,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors claim that user-side debiasing hurts long-tail exposure in movie recommenders.",
    "document": "Related Work\n\nBias mitigation in recommender systems spans data-level reweighting, model-side constraints, and post-hoc reranking. Metrics for fairness trade-offs often balance provider exposure, user satisfaction, and catalog coverage. In a previous study, the authors claim that user-side debiasing hurts long-tail exposure in movie recommenders. This assertion has influenced the design of many reranking pipelines that prioritize provider-side interventions. Yet, the empirical landscape is fragmented: datasets differ in sparsity, exposure policies, and implicit feedback noise. Our work systematically revisits these trade-offs under matched evaluation settings to clarify when user-side adjustments exacerbate or alleviate long-tail underexposure.",
    "reason": "References an unspecified prior study and attributes a concrete claim to it without providing a citation.",
    "start": 237,
    "end": 348,
    "label": "Unsupported claim"
  },
  {
    "span": "Classical cost models approximate join cardinalities using independence assumptions (Selinger et al., 1979), whereas learned approaches train regressors on past queries (Kraska et al., 2018; Ortiz et al., 2019).",
    "document": "Introduction\n\nAccurate cardinality estimation underpins cost-based query optimization. Misestimates propagate to poor join orders and suboptimal access paths, causing significant performance regressions in analytical workloads.\n\nClassical cost models approximate join cardinalities using independence assumptions (Selinger et al., 1979), whereas learned approaches train regressors on past queries (Kraska et al., 2018; Ortiz et al., 2019). Sampling-based estimators and synopsis structures such as histograms and sketches also remain widely used (Ioannidis and Poosala, 1995; Chaudhuri et al., 1998; Acharya et al., 1999).\n\nWe propose an adaptive estimator that integrates runtime feedback from executed plans to adjust selectivity predictions without retraining.",
    "reason": "The span contrasts categories of prior work but does not explain their relation to the authors' setting or articulate what gap the new estimator addresses, meeting (a) and (b).",
    "start": 229,
    "end": 440,
    "label": "Lacks synthesis"
  },
  {
    "span": "Several recent works demonstrate that self-supervised speech encoders outperform supervised baselines on emotion recognition.",
    "document": "Introduction\n\nSpeech emotion recognition (SER) seeks to infer affective states from acoustic and prosodic cues. Traditional approaches rely on engineered features such as MFCCs, pitch contours, and formant statistics, typically coupled with SVM or HMM classifiers. With the advent of deep learning, end-to-end models using spectrogram inputs have become standard.\n\nSeveral recent works demonstrate that self-supervised speech encoders outperform supervised baselines on emotion recognition. However, it remains unclear whether such gains stem from better phonetic representations, robustness to channel variability, or implicit speaker normalization.\n\nWe present a systematic study of self-supervised pretraining for SER under domain shift. We compare representations learned from large-scale unlabeled audio to supervised encoders trained on curated emotional speech corpora. Our experiments decouple speaker, content, and channel factors through controlled data splits, and we propose a calibration strategy to counteract class imbalance prevalent in SER datasets.\n\nRelated Work\n\nSER literature encompasses feature-based pipelines, CNN-RNN hybrids, and attention-based temporal models. Transfer learning has been explored from ASR and speaker verification domains, but its efficacy on cross-corpus SER remains debated. Our work complements this line by analyzing representation fragility under heterogeneous acoustic conditions.",
    "reason": "Mentions 'recent works' and a comparative performance claim without any citations to substantiate it.",
    "start": 365,
    "end": 490,
    "label": "Unsupported claim"
  },
  {
    "span": "there are many recent works that explore this topic",
    "document": "Introduction\n\nAbstractive text summarization has benefited substantially from large language models and instruction tuning. Early neural methods relied on sequence-to-sequence architectures with attention (See et al., 2017; Paulus et al., 2018), while later approaches leveraged pretraining on massive corpora before fine-tuning on news datasets such as CNN/DailyMail (Hermann et al., 2015; Nallapati et al., 2016). More recently, constraints like factual consistency and faithfulness have become central evaluation criteria (Maynez et al., 2020; Kryscinski et al., 2020). Despite these advances, there remain open questions around domain transfer to long, technical inputs such as scientific papers, patents, and legal decisions. In particular, there are many recent works that explore this topic, yet consensus on best practices is lacking. Our work examines controllable summarization for scientific discourse with explicit section-aware conditioning and evaluates cross-domain generalization.",
    "reason": "The phrase claims the existence of 'many recent works' without providing citations to those works, violating the requirement to cite recent related studies (rule d).",
    "start": 746,
    "end": 797,
    "label": "Unsupported claim"
  },
  {
    "span": "(Smith 2018)",
    "document": "Related Work\n\nPolicy gradient methods optimize expected return directly and have inspired a family of variance-reduced estimators (Williams, 1992; Schulman et al., 2017). Actor–critic architectures stabilize training by learning value baselines and advantage functions (Konda and Tsitsiklis, 2000).\n\nEntropy regularization and trust-region constraints improve exploration and prevent destructive policy updates (Agarwal et al., 2021). Prior analyses link step-size schedules to monotonic improvement guarantees (Kakade and Langford, 2002). While off-policy corrections enable replay, they can introduce bias if importance weights are truncated improperly (Smith 2018).\n\nWe examine a clipped-penalty objective that balances bias and variance in truncated importance sampling.",
    "reason": "Missing comma between author and year in a parenthetical citation. APA-style parenthetical citation should be (Smith, 2018), not (Smith 2018).",
    "start": 655,
    "end": 667,
    "label": "Format"
  },
  {
    "span": "Data augmentation for low-resource speech recognition includes speed perturbation, tempo/pitch shifts, and additive noise (Ko et al., 2015; Cui et al., 2015), SpecAugment-style time/frequency masking (Park et al., 2019), room impulse response simulation for reverberation (Ravanelli et al., 2018), and TTS-based synthetic data generation (Rosenberg et al., 2019; Laptev et al., 2020).",
    "document": "Related Work\n\nEnd-to-end ASR systems often degrade sharply in low-resource regimes due to overfitting and poor acoustic coverage. Augmentation techniques partially mitigate this by expanding the effective training distribution.\n\nData augmentation for low-resource speech recognition includes speed perturbation, tempo/pitch shifts, and additive noise (Ko et al., 2015; Cui et al., 2015), SpecAugment-style time/frequency masking (Park et al., 2019), room impulse response simulation for reverberation (Ravanelli et al., 2018), and TTS-based synthetic data generation (Rosenberg et al., 2019; Laptev et al., 2020).\n\nWe explore corpus-matched augmentation policies discovered via Bayesian optimization under a fixed compute budget.\n",
    "reason": "The sentence lists prior augmentation methods without explaining their efficacy, limitations, or how they inform the proposed approach, reflecting (a) and (c).",
    "start": 229,
    "end": 613,
    "label": "Lacks synthesis"
  },
  {
    "span": "BERT-large was first used for automated essay scoring on the ASAP dataset with a pairwise ranking objective.",
    "document": "Related Work\n\nAutomated essay scoring (AES) has progressed from handcrafted features and regression to neural architectures that model discourse and coherence. Pretrained language models have further improved performance by capturing syntax and semantics with minimal feature engineering.\n\nBERT-large was first used for automated essay scoring on the ASAP dataset with a pairwise ranking objective. Subsequent systems explored domain adaptation, prompt-specific calibration, and fairness-aware training to mitigate bias across demographic groups.\n\nWe extend this line by introducing contrastive calibration between score bands and analyzing robustness to adversarial perturbations that preserve content while altering surface form.",
    "reason": "Makes a claim about the first use of a specific model and training objective for a named dataset without providing any citation.",
    "start": 290,
    "end": 398,
    "label": "Unsupported claim"
  },
  {
    "span": "The majority of biomedical QA datasets annotate answer rationales at the sentence level",
    "document": "Introduction\n\nExplainability in biomedical question answering (QA) is often operationalized via rationales that justify predicted answers, facilitating expert verification and trust (DeYoung et al., 2020). Annotation schemes vary in granularity, from token highlights to sentence spans and paragraph-level evidence (Nye et al., 2020; Jin et al., 2019). The majority of biomedical QA datasets annotate answer rationales at the sentence level. This heterogeneity complicates cross-dataset transfer, as models must adapt to differing supervision signals and evaluation criteria.\n\nRelated Work\n\nDistant supervision strategies align answers with abstracts to cheaply approximate rationales but introduce noise (Verga et al., 2018). Joint modeling of retrieval and reasoning has improved evidence attribution and end-to-end accuracy (Karpukhin et al., 2020). We compare rationale granularity across benchmarks and introduce a unifying evaluation protocol that normalizes evidence spans to a common unit.",
    "reason": "The sentence asserts a quantitative field-wide characterization without any citation or evidence, violating rule (b).",
    "start": 353,
    "end": 440,
    "label": "Unsupported claim"
  },
  {
    "span": "Knowledge distillation for speech recognition typically transfers soft posterior distributions from a larger teacher to a compact student (Hinton et al., 2015; Watanabe et al., 2017). Sequence-level and CTC-to-attention distillation have also been investigated to stabilize training and improve word error rate (Kurata and Audhkhasi, 2018; Kim et al., 2019). Data augmentation such as SpecAugment is commonly applied to reduce overfitting (Park et al., 2019).",
    "document": "Related Work\n\nModel Compression and Distillation in End-to-End ASR\n\nDeploying automatic speech recognition on edge devices requires compact models with low latency while maintaining accuracy. Distillation and pruning are widely used to transfer knowledge from large teachers to smaller students for efficient inference.\n\nKnowledge distillation for speech recognition typically transfers soft posterior distributions from a larger teacher to a compact student (Hinton et al., 2015; Watanabe et al., 2017). Sequence-level and CTC-to-attention distillation have also been investigated to stabilize training and improve word error rate (Kurata and Audhkhasi, 2018; Kim et al., 2019). Data augmentation such as SpecAugment is commonly applied to reduce overfitting (Park et al., 2019).\n\nOther compression avenues include low-rank factorization, quantization, and neural architecture search tailored to streaming constraints (Prabhavalkar et al., 2016; Wu et al., 2016; Han et al., 2016).",
    "reason": "This paragraph only summarizes previous ASR distillation and augmentation methods without clarifying how they inform or contrast with the authors' approach.",
    "start": 321,
    "end": 780,
    "label": "Lacks synthesis"
  },
  {
    "span": "Graph-based decoders consistently outperform sequence decoders for AMR-to-text.",
    "document": "Related Work\n\nAbstract Meaning Representation (AMR) generation converts semantic graphs into fluent text, with early neural approaches linearizing graphs and applying sequence-to-sequence models (Konstas et al., 2017). Subsequent methods introduced graph encoders and structural constraints to better preserve semantics (Damonte and Cohen, 2019; Ribeiro et al., 2021). Graph-based decoders consistently outperform sequence decoders for AMR-to-text. In this study, we revisit decoder design and propose a hybrid planner that alternates between graph traversal and surface realization.",
    "reason": "This is a comparative performance claim about prior work without providing supporting citations, violating rule (b).",
    "start": 369,
    "end": 448,
    "label": "Unsupported claim"
  },
  {
    "span": "The LibriPhone dataset contains 10,000 hours of noisy telephone speech.",
    "document": "Introduction\n\nRobust automatic speech recognition (ASR) in noisy and channel-mismatched conditions remains challenging. Approaches based on data augmentation (Ko et al., 2015) and domain-adversarial training (Shinohara, 2016) have improved robustness, but performance degrades under severe telephony artifacts. Public corpora like LibriSpeech (Panayotov et al., 2015) and Common Voice (Ardila et al., 2020) provide large-scale read speech, whereas telephone speech resources are comparatively scarce and fragmented. The LibriPhone dataset contains 10,000 hours of noisy telephone speech. This scale enables training of robust acoustic models that generalize across codecs and sampling rates.\n\nWe investigate multi-condition training with codec simulation and contrastive representation learning to bridge the gap between wideband and narrowband conditions. Experiments on mixed-domain test sets demonstrate that our method reduces word error rate relative to strong augmentation baselines.",
    "reason": "States a specific dataset name and statistic without any citation or evidence, which should be supported by a reference.",
    "start": 516,
    "end": 587,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent self-supervised methods learn invariances by contrasting positive pairs against negatives or implicit negatives. SimCLR scales contrastive learning with large batches and strong augmentations (Chen et al., 2020). MoCo maintains a dynamic memory to decouple batch size from dictionary size (He et al., 2020). BYOL removes explicit negatives through online and target networks (Grill et al., 2020). SwAV clusters features with online assignment to form self-labels (Caron et al., 2020).",
    "document": "Related Work\n\nSelf-Supervised Representation Learning for Vision\n\nSelf-supervised learning has emerged as a scalable alternative to supervised pretraining, reducing reliance on labeled data while retaining transfer performance across tasks. A central theme is constructing pretext tasks that induce semantic structure in learned embeddings.\n\nRecent self-supervised methods learn invariances by contrasting positive pairs against negatives or implicit negatives. SimCLR scales contrastive learning with large batches and strong augmentations (Chen et al., 2020). MoCo maintains a dynamic memory to decouple batch size from dictionary size (He et al., 2020). BYOL removes explicit negatives through online and target networks (Grill et al., 2020). SwAV clusters features with online assignment to form self-labels (Caron et al., 2020).\n\nOther approaches move beyond instance discrimination to incorporate clustering, bootstrapping, and masked modeling on images and patches (Caron et al., 2021; Xie et al., 2022). Transfer to detection and segmentation typically requires careful finetuning strategies and augmentations (Kolesnikov et al., 2019; Grill et al., 2020).",
    "reason": "The span summarizes prior self-supervised methods without explaining their relation to the present work, the authors' perspective, or the gap motivating the new method.",
    "start": 342,
    "end": 833,
    "label": "Lacks synthesis"
  },
  {
    "span": "There are many recent works that explore this topic.",
    "document": "Introduction\n\nOpen-domain question answering (ODQA) aims to answer factoid questions using large text collections without closed-world assumptions. Pipeline architectures typically combine dense retrieval with neural readers, building upon seminal systems like DrQA and DPR (Chen et al., 2017; Karpukhin et al., 2020). There are many recent works that explore this topic. Despite substantial progress, retrieval latency and domain shift remain persistent challenges, spurring research on better pretraining objectives and hard-negative mining (Xiong et al., 2020; Qu et al., 2021).",
    "reason": "The claim about 'many recent works' requires citations to support it, per guideline (d) for mentions of recent work.",
    "start": 319,
    "end": 371,
    "label": "Unsupported claim"
  },
  {
    "span": "Gonzalez and Hart (2018) proposed sequence-to-sequence baselines for hydrologic forecasting. Wei et al. (2020) introduced physics-guided loss terms. Novak and Blake (2021) studied regime shifts under nonstationarity. Chen et al. (2022) evaluated transfer across basins using meta-learning.",
    "document": "Related Work\n\nData-driven hydrologic forecasting seeks to combine machine learning with physical insights to predict streamflow and water quality under changing climates. Research themes include architecture design, physics integration, and generalization across sites.\n\nGonzalez and Hart (2018) proposed sequence-to-sequence baselines for hydrologic forecasting. Wei et al. (2020) introduced physics-guided loss terms. Novak and Blake (2021) studied regime shifts under nonstationarity. Chen et al. (2022) evaluated transfer across basins using meta-learning.\n\nOther efforts use probabilistic models for uncertainty quantification (Ibrahim et al., 2023). Our work focuses on physics-constrained adapters that improve cross-basin robustness while preserving calibration.",
    "reason": "The span lists four works back-to-back without transitions or an explicit explanation of how baseline models, physics guidance, regime analysis, and transfer learning relate, causing coherence issues.",
    "start": 271,
    "end": 560,
    "label": "Coherence"
  },
  {
    "span": "The BioMedQA shared task emphasized multi-hop evidence synthesis",
    "document": "Related Work\n\nBiomedical question answering (QA) poses unique challenges due to specialized terminology, long-range dependencies, and the need for verifiable evidence. The BioMedQA shared task emphasized multi-hop evidence synthesis and highlighted the importance of retrieval pipelines that capture complementary abstracts. Subsequent systems have focused on tighter integration between retrieval and reasoning, yet evidence attribution remains limited. Our approach introduces a contrastive retriever trained jointly with a rationale generator to improve verifiability.",
    "reason": "Mentions a specific shared task without providing a citation at first mention, violating (a).",
    "start": 168,
    "end": 232,
    "label": "Unsupported claim"
  },
  {
    "span": "The widely noted inverse relation leak in WN18 is known to inflate results.",
    "document": "Related Work\n\nKnowledge graph embedding models are commonly evaluated on link prediction benchmarks derived from lexical and encyclopedic resources. The widely noted inverse relation leak in WN18 is known to inflate results. As a consequence, subsequent datasets introduced filtered splits and relation pruning to reduce leakage, yet test-time biases persist.",
    "reason": "Makes a specific claim about a dataset flaw in prior benchmarks without citing supporting studies.",
    "start": 149,
    "end": 224,
    "label": "Unsupported claim"
  },
  {
    "span": "The SemEval-2020 Task on irony detection established the prevailing preprocessing pipeline.",
    "document": "Related Work\n\nIrony and sarcasm detection in social media texts has been studied using lexical cues, context modeling, and user-level features (Davidov et al., 2010; Ghosh and Veale, 2017; Ghosh et al., 2018). Shared tasks have played a central role in standardizing datasets and evaluation protocols (Rosenthal et al., 2014; Van Hee et al., 2018). The SemEval-2020 Task on irony detection established the prevailing preprocessing pipeline. Building on these efforts, recent neural systems leverage contextual embeddings, emoji semantics, and conversation history to improve robustness across domains (Barbieri et al., 2020; Potamias et al., 2020).",
    "reason": "Mentions a specific shared task and attributes a contribution without citing the task or evidence (definition a and ii).",
    "start": 349,
    "end": 440,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from grades 6–8 with quadratic weighted kappa as the objective.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) aims to predict holistic or trait scores consistent with human raters. Traditional approaches relied on handcrafted features capturing lexical richness, syntactic variety, and discourse coherence. Neural models have increasingly replaced feature engineering with learned representations, employing CNNs, RNNs, and transformers to encode essay content and structure. Prompt-specific modeling and domain adaptation are common strategies to handle variation across prompts and grade levels.\n\nBERT was used in an AES task trained on essays from grades 6–8 with quadratic weighted kappa as the objective. Beyond holistic scoring, recent work explores multi-task learning for trait-level predictions and adversarial training to reduce bias with respect to prompt, length, and demographic proxies. Cross-prompt transfer remains difficult due to shifts in topical content and expected writing proficiency.",
    "reason": "This sentence presents a specific setup of an AES task (model, data, objective) without citing the corresponding work; first mention of such a study must include a citation (rules a and e).",
    "start": 533,
    "end": 643,
    "label": "Unsupported claim"
  },
  {
    "span": "The CoQA dataset has been widely criticized for annotation artifacts.",
    "document": "Introduction\n\nConversational question answering aims to model multi-turn information-seeking dialogues grounded in text. While early benchmarks focused on single-turn reading comprehension, recent resources capture dialog state and coreference phenomena. The CoQA dataset has been widely criticized for annotation artifacts. This work proposes a new collection emphasizing answerability and follow-up intent prediction.",
    "reason": "Asserts widespread criticism of a specific dataset without citing sources that document those criticisms.",
    "start": 255,
    "end": 324,
    "label": "Unsupported claim"
  },
  {
    "span": "There are many recent works exploring contrastive pretraining for code summarization",
    "document": "Introduction\n\nCode summarization seeks to generate natural-language descriptions of code snippets to aid software maintenance and comprehension. Early approaches relied on template-based or statistical models (Haiduc et al., 2010; McBurney and McMillan, 2014), while neural sequence-to-sequence models improved fluency and coverage (Allamanis et al., 2016; Iyer et al., 2016). With the advent of large-scale pretraining on code corpora, models such as CodeBERT and GraphCodeBERT have further advanced the state of the art by leveraging bimodal objectives and structure-aware encoders (Feng et al., 2020; Guo et al., 2021).\n\nThere are many recent works exploring contrastive pretraining for code summarization, aiming to align code representations with their corresponding natural-language descriptions. These approaches typically mine positive and hard negative pairs from repositories and optimize instance-level or graph-level contrastive losses to improve retrieval and generation. Despite progress, few studies have examined how contrastive objectives interact with domain shifts across repositories or programming languages. We revisit this question by introducing a domain-adaptive contrastive stage prior to fine-tuning and by measuring cross-language generalization on Java, Python, and Go datasets.\n\nOur contributions are threefold: (1) a modular contrastive pretraining scheme that can be layered on top of existing code encoders, (2) a new evaluation protocol for cross-repository generalization, and (3) an analysis of code–text alignment under varying negative sampling strategies.",
    "reason": "The sentence claims 'many recent works' on contrastive pretraining for code summarization but provides no citations to support this claim (violates rule d about 'recent works').",
    "start": 624,
    "end": 708,
    "label": "Unsupported claim"
  },
  {
    "span": "GraphSAGE variants have consistently dominated the Open Graph Benchmark leaderboard",
    "document": "Related Work\n\nGraph neural networks (GNNs) generalize message passing to learn node, edge, and graph-level representations. Architectures such as GCN, GraphSAGE, and GAT differ in aggregation mechanisms and inductive biases, leading to varied performance across homophilous and heterophilous settings. GraphSAGE variants have consistently dominated the Open Graph Benchmark leaderboard, motivating extensive study of neighbor sampling strategies, normalization layers, and training pipelines.\n\nBeyond architecture, recent work investigates scaling laws for GNN depth and width, curriculum sampling, and subgraph-level pretraining. Nevertheless, GNNs remain sensitive to distribution shift and suffer from oversmoothing in deeper stacks.\n\nOur study revisits these trade-offs with a controlled evaluation that equalizes negative sampling, sampler seeds, and normalization schedules across models.\n",
    "reason": "Claims a leaderboard dominance by a specific method without citing the source or providing evidence, violating rule a and rule d.",
    "start": 302,
    "end": 385,
    "label": "Unsupported claim"
  },
  {
    "span": "Nguyen et al. 2",
    "document": "Related Work\n\nCross-lingual summarization methods leverage multilingual encoders and constrained decoding to preserve semantic fidelity across languages (Ladhak et al., 2020; Bai et al., 2021). Aligning sentence-level representations via parallel corpora remains effective but resource-intensive (Artetxe et al., 2018). Unsupervised approaches rely on back-translation and pseudo-parallel generation (Lample et al., 2018).\n\nQuality control in low-resource settings often uses knowledge distillation from high-resource teachers (Hinton et al., 2015; Sun et al., 2019). Nguyen et al. 2 propose bilingual content filtering that prioritizes entailment-consistent pairs, achieving better factuality on noisy web text. We combine this filtering with contrastive objectives tailored for cross-lingual entailment signals.\n",
    "reason": "Wrong use of footnote-like numbering after an author name; it should include a year (e.g., 'Nguyen et al. (2022)') or be formatted as a proper footnote.",
    "start": 568,
    "end": 583,
    "label": "Format"
  },
  {
    "span": "Contrastive learning frameworks such as SimCLR and MoCo pretrain encoders using instance discrimination (Chen et al., 2020; He et al., 2020). RandAugment explores simple policy searches for data augmentation (Cubuk et al., 2020). Self-distillation without labels stabilizes training (Caron et al., 2021). Vision Transformers scale to large datasets with patch tokens (Dosovitskiy et al., 2021).",
    "document": "Related Work\n\nSelf-supervised learning (SSL) has become a dominant paradigm for visual representation learning, reducing dependence on labeled data by exploiting structure in images and videos. Approaches vary in their pretext tasks, training objectives, and architectural choices.\n\nContrastive learning frameworks such as SimCLR and MoCo pretrain encoders using instance discrimination (Chen et al., 2020; He et al., 2020). RandAugment explores simple policy searches for data augmentation (Cubuk et al., 2020). Self-distillation without labels stabilizes training (Caron et al., 2021). Vision Transformers scale to large datasets with patch tokens (Dosovitskiy et al., 2021).\n\nWhile many works address pretraining, transfer efficiency in low-label regimes remains challenging (Tian et al., 2020). We examine augmentation-aware finetuning strategies tailored to ViT backbones, aiming to bridge pretrain–finetune mismatches.",
    "reason": "The span jumps from contrastive methods to augmentation, to self-distillation, to architecture scaling, without explicit links or transitions clarifying their relationships, causing weak coherence between consecutive sentences.",
    "start": 283,
    "end": 677,
    "label": "Coherence"
  },
  {
    "span": "Neural-guided search prunes program spaces using learned policies (Neelakantan et al., 2016). Domain-specific languages simplify parsing from utterances (Raza et al., 2015). Pretrained code models provide strong priors for generation (Feng et al., 2020). User studies examine interpretability and debugging effort (Head et al., 2021).",
    "document": "Related Work\n\nProgram synthesis from natural language lies at the intersection of semantic parsing, code generation, and interactive systems. Prior work explores learning to search, grammar design, and pretraining on large code corpora to improve generalization and sample efficiency.\n\nNeural-guided search prunes program spaces using learned policies (Neelakantan et al., 2016). Domain-specific languages simplify parsing from utterances (Raza et al., 2015). Pretrained code models provide strong priors for generation (Feng et al., 2020). User studies examine interpretability and debugging effort (Head et al., 2021).\n\nWe introduce a constrained decoding framework that incorporates executable feedback signals online, aligning natural language intents with verifiable program behaviors.",
    "reason": "The span jumps between search, DSL design, pretraining, and user studies with no transitions or stated relationships, making the connection among cited works abrupt and unclear.",
    "start": 286,
    "end": 620,
    "label": "Coherence"
  },
  {
    "span": "Our system exceeds all reported baselines on the SemEval 2021 Task 7 benchmark for humor detection.",
    "document": "Introduction\n\nUnderstanding figurative language is critical for robust natural language understanding, with humor detection posing unique challenges due to cultural and contextual subtleties. Benchmarks from shared evaluations have catalyzed progress by standardizing data and metrics. Our system exceeds all reported baselines on the SemEval 2021 Task 7 benchmark for humor detection. We attribute these gains to multi-view representations that combine semantic incongruity modeling with pragmatic cues derived from user reactions.\n\nBeyond leaderboard performance, we analyze cross-domain generalization to unseen humor genres and show that our modeling of incongruity significantly contributes to transferability.",
    "reason": "First mention of a specific shared task/benchmark lacks a citation and asserts comparative performance without evidence.",
    "start": 286,
    "end": 385,
    "label": "Unsupported claim"
  },
  {
    "span": "Matrix factorization remains the de facto standard in top e-commerce platforms.",
    "document": "Introduction\n\nIndustrial recommender systems rely on scalable models that balance accuracy and latency. Classical collaborative filtering methods such as matrix factorization (Koren et al., 2009) and item-based nearest neighbors (Sarwar et al., 2001) have seen widespread adoption. Deep learning approaches incorporating sequence models and graph representations have reported improvements on public benchmarks (Kang and McAuley, 2018; He et al., 2017). However, deployment constraints, data sparsity, and cold-start issues shape model choice in practice. Matrix factorization remains the de facto standard in top e-commerce platforms. Understanding when and why practitioners choose simpler models is crucial for designing deployable recommender architectures.\n\nWe analyze production traffic logs and conduct controlled A/B tests to compare sequence-aware models against tuned MF baselines under tight latency budgets.",
    "reason": "Asserts industry-wide practice without citations to case studies, surveys, or reports that substantiate the claim.",
    "start": 556,
    "end": 635,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent methods learn disentangled or invariant representations to address confounding and distribution shift (Peters et al., 2016; Arjovsky et al., 2020; Schölkopf et al., 2021). Our method builds on invariant risk minimization and causal feature learning.",
    "document": "Related Work\n\nCausal Representation Learning\nRecent methods learn disentangled or invariant representations to address confounding and distribution shift (Peters et al., 2016; Arjovsky et al., 2020; Schölkopf et al., 2021). Our method builds on invariant risk minimization and causal feature learning.\n\nDomain Generalization\nApproaches for domain generalization exploit stable predictors and data augmentation strategies to improve out-of-distribution performance (Gulrajani and Lopez-Paz, 2021; Krueger et al., 2021).",
    "reason": "States prior work and then claims to build on it without identifying a specific gap or articulating how the proposed method differs or why it is needed (definition b/c).",
    "start": 45,
    "end": 301,
    "label": "Lacks synthesis"
  },
  {
    "span": "There have been many recent shared tasks on domain adaptation for MT.",
    "document": "Introduction\n\nNeural machine translation (NMT) systems suffer performance drops when test data diverge from training domains (Chu and Wang, 2018). Classical adaptation techniques include fine-tuning, instance weighting, and multi-domain training (Britz et al., 2017; Freitag and Al-Onaizan, 2016). There have been many recent shared tasks on domain adaptation for MT. While findings vary by language pair, constrained settings reveal the importance of in-domain terminology and robust decoding (Bawden et al., 2020; Kocabiyikoglu et al., 2018). We target low-resource domain shifts where in-domain data are scarce.",
    "reason": "Mentions 'recent shared tasks' without citing any tasks or proceedings (rules a and d).",
    "start": 298,
    "end": 367,
    "label": "Unsupported claim"
  },
  {
    "span": "(Chen and Rao, 2017",
    "document": "Related Work\n\nGraph neural networks have become the de facto tool for representation learning on relational data (Kipf and Welling, 2017; Veličković et al., 2018). Scalability is often addressed via sampling and partitioning, reducing memory and compute overhead on large graphs (Hamilton et al., 2017). Several neighbor-sampling methods (Chen and Rao, 2017 propose mini-batch training with stochastic neighborhoods, while layer-wise sampling further improves throughput (Chen et al., 2018).\n\nBeyond sampling, topology-aware augmentations and positional encodings enhance expressivity (Dwivedi et al., 2021; Kreuzer et al., 2021). We combine scalable sampling with structure-preserving regularization to improve generalization across sparse regimes.",
    "reason": "Missing closing parenthesis in a parenthetical citation. The citation '(Chen and Rao, 2017' should be closed as '(Chen and Rao, 2017)'.",
    "start": 338,
    "end": 357,
    "label": "Format"
  },
  {
    "span": "Inverse propensity weighting rebalances logged exposure to correct click bias (Schnabel et al., 2016; Joachims et al., 2017). Instrumental variable techniques seek exogenous variation to identify causal effects (Angrist and Pischke, 2009; Hartford et al., 2017). Graph embedding methods learn vector representations of users and items (Perozzi et al., 2014; Grover and Leskovec, 2016).",
    "document": "Introduction\n\nRecommendation systems optimized via observational logs are susceptible to bias and confounding, leading to miscalibrated relevance estimates. Causal inference provides tools to address these issues, from counterfactual estimators to robust identification strategies. Practical deployment requires balancing assumptions, sample efficiency, and computational constraints.\n\nInverse propensity weighting rebalances logged exposure to correct click bias (Schnabel et al., 2016; Joachims et al., 2017). Instrumental variable techniques seek exogenous variation to identify causal effects (Angrist and Pischke, 2009; Hartford et al., 2017). Graph embedding methods learn vector representations of users and items (Perozzi et al., 2014; Grover and Leskovec, 2016).\n\nRecent advances integrate causal objectives directly into recommenders, including deconfounded training and counterfactual evaluation protocols. Interventions such as randomized explorations or natural experiments help probe causal structure. Scalability concerns motivate approximate estimators and self-normalized importance sampling.\n\nWe propose a debiased training framework that couples exposure modeling with representation regularization. Our approach yields stable gains in both offline counterfactual metrics and online A/B testing on a large-scale platform.",
    "reason": "The span combines IPW, instrumental variables, and graph embeddings without transitions or explicit connections, leaving readers unclear how the cited areas relate to each other within causal recommendation.",
    "start": 386,
    "end": 771,
    "label": "Coherence"
  },
  {
    "span": "Audio-visual emotion recognition has seen fusion strategies such as early concatenation, bilinear pooling, and cross-modal attention (Zadeh et al., 2017; Tsai et al., 2019; Sun et al., 2020).",
    "document": "Introduction\n\nUnderstanding human emotion from multimodal signals is essential for affective computing applications. Combining audio and visual cues can improve robustness, but fusion design and temporal alignment remain open questions. Audio-visual emotion recognition has seen fusion strategies such as early concatenation, bilinear pooling, and cross-modal attention (Zadeh et al., 2017; Tsai et al., 2019; Sun et al., 2020). Datasets vary widely in recording conditions and label granularity, complicating fair comparisons (Dhall et al., 2018; Kossaifi et al., 2019).\n\nWe present results on in-the-wild corpora with spontaneous expressions.",
    "reason": "The span lists techniques and citations without linking them to the authors' objectives, highlighting limitations, or forming an argument, thereby lacking synthesis (definition a) and not conveying motivation (definition c).",
    "start": 237,
    "end": 428,
    "label": "Lacks synthesis"
  },
  {
    "span": "Text as treatment, text as mediator, and text as outcome have been separately studied using embeddings, topic models, and supervised labels (Roberts et al., 2020; Egami et al., 2018; Keith et al., 2020). Methods for estimating heterogeneous effects from text rely on propensity models and representation learning (Johansson et al., 2016; Zhang et al., 2021; Deaner et al., 2022).",
    "document": "Introduction\n\nCausal inference with high-dimensional text poses identification and estimation challenges. Text can play multiple roles in a causal graph, and naive adjustments risk collider bias or post-treatment conditioning.\n\nPrior Work\nText as treatment, text as mediator, and text as outcome have been separately studied using embeddings, topic models, and supervised labels (Roberts et al., 2020; Egami et al., 2018; Keith et al., 2020). Methods for estimating heterogeneous effects from text rely on propensity models and representation learning (Johansson et al., 2016; Zhang et al., 2021; Deaner et al., 2022).\n\nOpen Problems\nStability to annotation noise, domain shift, and model misspecification remains underexplored, particularly when text proxies unobserved confounders.\n\nThis Paper\nWe propose a sensitivity-aware estimator that integrates text-derived proxies with uncertainty bounds to improve interpretability under partial identification.",
    "reason": "The span summarizes strands of literature without connecting them to the authors' framing or specifying what remains unresolved, lacking synthesis per (a) and (c).",
    "start": 239,
    "end": 618,
    "label": "Lacks synthesis"
  },
  {
    "span": "In (Lopez et al., 2018)",
    "document": "Related Work\n\nGraph-based traffic forecasting has evolved from handcrafted spatial features to end-to-end neural architectures. Early approaches relied on convolutional filters over fixed road grids (Khan and Duarte, 2017) and learned embeddings for sensor stations (Mora et al., 2019). More recent models use message passing to capture dynamic flow patterns, often combined with temporal modules (Rao et al., 2020; Silva et al., 2021). In (Lopez et al., 2018), the authors propose a spectral formulation that scales to large sensor graphs, but requires expensive preprocessing to compute eigen-bases. Building on this, Nguyen and Costa (2020) introduce attention over edges to adapt to changing traffic regimes. Complementary studies explore exogenous signals such as weather and events (Ibrahim et al., 2020), showing consistent gains across metropolitan datasets. Despite these advances, robustness to missing sensors and topology shifts remains an open challenge, as highlighted by Pereira et al. (2022).",
    "reason": "Wrong citation style: a narrative use with a leading preposition includes a parenthetical citation; should be 'In Lopez et al. (2018)'.",
    "start": 437,
    "end": 460,
    "label": "Format"
  },
  {
    "span": "Deep representation learning for causal effect estimation learns balanced latent spaces where treatment and control distributions are similar (Johansson et al., 2016; Shalit et al., 2017). Domain adaptation and adversarial training further reduce confounding by aligning covariates across groups (Ganin et al., 2016; Hassanpour and Greiner, 2019). Recent works incorporate temporal structure of EHR via recurrent and attention models (Bica et al., 2020; Li et al., 2021).",
    "document": "Introduction\n\nEstimating individualized treatment effects from observational data is central to decision support in healthcare, where randomized trials are often infeasible (Rubin, 2005; Hernán and Robins, 2020). Electronic health records introduce challenges such as time-varying confounding, censoring, and missingness.\n\nDeep representation learning for causal effect estimation learns balanced latent spaces where treatment and control distributions are similar (Johansson et al., 2016; Shalit et al., 2017). Domain adaptation and adversarial training further reduce confounding by aligning covariates across groups (Ganin et al., 2016; Hassanpour and Greiner, 2019). Recent works incorporate temporal structure of EHR via recurrent and attention models (Bica et al., 2020; Li et al., 2021).\n\nEvaluation commonly relies on semi-synthetic benchmarks and proxy outcomes, with sensitivity analyses to assess robustness to unobserved confounding (Yoon et al., 2018; Schwab et al., 2019).",
    "reason": "The span reports prior methods and extensions but does not connect them to the authors' motivation or specify a gap, consistent with definition a and c. Since this is not the very first paragraph, the lack of argument is not justified by clause d.",
    "start": 323,
    "end": 794,
    "label": "Lacks synthesis"
  },
  {
    "span": "Few-shot prompting elicits latent program synthesis capabilities in large language models (Brown et al., 2020; Chen et al., 2021). AST-level repair systems modify code structures to enforce syntactic correctness (Bader et al., 2019; Gupta et al., 2017). Static analyzers detect potential bugs using dataflow summaries (Sui and Xue, 2016; Livshits and Lam, 2005).",
    "document": "Related Work\n\nProgram synthesis with large language models leverages natural language specifications and code corpora to generate executable programs. Reliability and correctness remain key challenges, prompting work on constrained decoding, verification, and repair. Datasets span competitive programming, library usage tasks, and API-heavy development scenarios.\n\nFew-shot prompting elicits latent program synthesis capabilities in large language models (Brown et al., 2020; Chen et al., 2021). AST-level repair systems modify code structures to enforce syntactic correctness (Bader et al., 2019; Gupta et al., 2017). Static analyzers detect potential bugs using dataflow summaries (Sui and Xue, 2016; Livshits and Lam, 2005).\n\nHybrid methods integrate symbolic constraints or test-driven feedback into generation loops. Semantic repair has incorporated unit test outcomes and fuzzing to guide edits toward functional correctness. Execution-time feedback further enables iterative refinement and self-correction.\n\nOur method introduces verifier-guided decoding with incremental type-state checks, improving pass rates under strict unit test regimes and reducing post-hoc repair overhead.",
    "reason": "The span abruptly shifts from prompting for synthesis to AST repair and to static analysis, without transitions or explanation of their relationships, making the linkage between the cited works implicit and unclear.",
    "start": 366,
    "end": 728,
    "label": "Coherence"
  },
  {
    "span": "Contrastive pretraining aligns instance-level features with strong data augmentation (He et al., 2020). Masked image modeling reconstructs tokenized patches as a pretext task (Bao et al., 2021). Self-distillation removes the need for negative pairs (Grill et al., 2020). Data2vec learns a unified representation across modalities (Baevski et al., 2022).",
    "document": "Related Work\n\nSelf-supervised vision pretraining\nSelf-supervised learning has emerged as a dominant paradigm for visual representation learning, reducing reliance on human labels while improving transfer to detection and segmentation benchmarks. Early contrastive methods encourage agreement between differently augmented views of the same image while repelling other instances (Chen et al., 2020). Contrastive pretraining aligns instance-level features with strong data augmentation (He et al., 2020). Masked image modeling reconstructs tokenized patches as a pretext task (Bao et al., 2021). Self-distillation removes the need for negative pairs (Grill et al., 2020). Data2vec learns a unified representation across modalities (Baevski et al., 2022).\n\nDownstream adaptation\nAdapting pretrained backbones to dense prediction tasks often requires careful choice of fine-tuning schedules and feature pyramid integration (Kaiming et al., 2017; Xie et al., 2021). Recent works explore freezing early blocks to preserve invariances learned during pretraining (Kornblith et al., 2019).\n\nWe build upon masked image modeling but target label-scarce dense tasks where preserving spatial fidelity is crucial.",
    "reason": "The span enumerates several self-supervised approaches in separate sentences without clarifying their relationships or contrasts, and lacks transitions connecting these cited works, making the connection between them abrupt and unclear.",
    "start": 399,
    "end": 752,
    "label": "Coherence"
  },
  {
    "span": "Automated program repair leverages search-based mutation (Le Goues et al., 2012), template-based synthesis (Long and Rinard, 2016), and constraint solving (Martinez and Monperrus, 2015). With large language models, studies have explored prompt-based repair (Chen et al., 2021), retrieval-augmented code completion (Yao et al., 2022), and fine-tuning on bug-fix corpora (Liu et al., 2021).",
    "document": "Introduction\n\nSoftware defects remain costly, motivating automated program repair (APR) techniques that propose plausible patches with minimal developer intervention. Recent advances in learned code representations and large language models (LLMs) have renewed interest in data-driven APR.\n\nAutomated program repair leverages search-based mutation (Le Goues et al., 2012), template-based synthesis (Long and Rinard, 2016), and constraint solving (Martinez and Monperrus, 2015). With large language models, studies have explored prompt-based repair (Chen et al., 2021), retrieval-augmented code completion (Yao et al., 2022), and fine-tuning on bug-fix corpora (Liu et al., 2021).\n\nBenchmarking efforts such as Defects4J and QuixBugs facilitate standardized evaluation (Just et al., 2014; Lin et al., 2017). Nevertheless, challenges persist in overfitting and patch correctness.\n\nWe present PatchLM, an LLM-based system guided by static analysis signals to reduce spurious fixes.",
    "reason": "The span lists APR methods and LLM-based approaches without connecting them to the authors’ objectives or specifying what remains unresolved, meeting criterion (a)/(c).",
    "start": 291,
    "end": 679,
    "label": "Lacks synthesis"
  },
  {
    "span": "BERT was used in an AES task trained on essays from grades 7–9.",
    "document": "Introduction\n\nAutomated Essay Scoring (AES) systems seek to provide reliable and fair assessments of student writing. Early AES models emphasized handcrafted features such as grammar error counts and discourse markers. Neural encoders have since become standard, capturing surface fluency and higher-level coherence within a unified architecture. BERT was used in an AES task trained on essays from grades 7–9. Despite performance gains, concerns persist about domain shift across prompts and the interpretability of scores. We present a calibration-aware AES framework that disentangles content and form through dual encoders, and we evaluate robustness to adversarial surface perturbations.\n\nRelated Work\n\nResearch on fairness in AES highlights disparities across demographic groups and prompt topics, motivating calibration and uncertainty estimates alongside point predictions.",
    "reason": "This mentions a specific prior setup (BERT applied to a particular AES dataset and grade range) without a supporting citation; task configurations and datasets require references.",
    "start": 347,
    "end": 410,
    "label": "Unsupported claim"
  },
  {
    "span": "Clinical text de-identification has been modeled with CRFs and hand-engineered features (Stubbs et al., 2015), recurrent neural networks with character-level signals (Dernoncourt et al., 2017; Liu and Xu, 2018), and transformer-based taggers that leverage contextual embeddings (Johnson and Wang, 2019; Park et al., 2020). We build a transformer-based tagger for multi-institutional notes.",
    "document": "Introduction\n\nRemoving protected health information (PHI) from clinical narratives is essential for data sharing and secondary analysis. Automated de-identification aims to tag PHI spans reliably across diverse note types.\n\nClinical text de-identification has been modeled with CRFs and hand-engineered features (Stubbs et al., 2015), recurrent neural networks with character-level signals (Dernoncourt et al., 2017; Liu and Xu, 2018), and transformer-based taggers that leverage contextual embeddings (Johnson and Wang, 2019; Park et al., 2020). We build a transformer-based tagger for multi-institutional notes.\n\nWe assess performance across heterogeneous corpora and analyze errors by PHI category.",
    "reason": "The span lists prior approaches and immediately states the authors’ method without identifying any unresolved limitation or explaining how their method is novel, thus lacking explicit gap articulation (criterion b).",
    "start": 224,
    "end": 613,
    "label": "Lacks synthesis"
  },
  {
    "span": "R-GCN (Schlichtkrull et al., 2018), GIN (Xu et al., 2019), and MPNN variants (Gilmer et al., 2017; Yang et al., 2019) have been widely adopted for molecular property prediction. Directional message passing approaches such as DimeNet and DimeNet++ (Klicpera et al., 2020a; Klicpera et al., 2020b) incorporate angular information, while 3D-aware models like SchNet (Schütt et al., 2018) and PaiNN (Schütt et al., 2021) leverage continuous-filter convolutions. Self-supervised pretraining frameworks further improve data efficiency (You et al., 2020; Hu et al., 2020).",
    "document": "Related Work\n\nMolecular property prediction has seen rapid progress with graph neural networks that operate on molecular graphs and, increasingly, on 3D structures. Prior work varies in how edges, geometry, and electronic features are modeled, with notable trade-offs in accuracy and computational cost.\n\nR-GCN (Schlichtkrull et al., 2018), GIN (Xu et al., 2019), and MPNN variants (Gilmer et al., 2017; Yang et al., 2019) have been widely adopted for molecular property prediction. Directional message passing approaches such as DimeNet and DimeNet++ (Klicpera et al., 2020a; Klicpera et al., 2020b) incorporate angular information, while 3D-aware models like SchNet (Schütt et al., 2018) and PaiNN (Schütt et al., 2021) leverage continuous-filter convolutions. Self-supervised pretraining frameworks further improve data efficiency (You et al., 2020; Hu et al., 2020).\n\nWhile model classes and design choices continue to proliferate, a key consideration is balancing geometric fidelity with inference latency on large compound libraries. In this paper, we study this balance in high-throughput virtual screening and evaluate a lightweight geometric encoder tailored to conformer ensembles.",
    "reason": "The span lists prior methods and citations without explaining their relation to the authors' problem or approach and does not articulate a gap or perspective (criteria a and c).",
    "start": 305,
    "end": 870,
    "label": "Lacks synthesis"
  },
  {
    "span": "Prior work on automated feedback for student writing includes classifier-based rubric scoring (Taghipour and Ng, 2016; Ridley et al., 2021), retrieval of exemplar comments (Zhang and Litman, 2014; Nguyen et al., 2019), and neural generation of suggestions (Uthus et al., 2020; Yuan et al., 2021).",
    "document": "Related Work\n\nProviding timely formative feedback helps students improve writing skills at scale, but manual grading is resource-intensive. NLP systems have been developed to offer rubric-aligned assessments and concrete revision suggestions.\n\nPrior work on automated feedback for student writing includes classifier-based rubric scoring (Taghipour and Ng, 2016; Ridley et al., 2021), retrieval of exemplar comments (Zhang and Litman, 2014; Nguyen et al., 2019), and neural generation of suggestions (Uthus et al., 2020; Yuan et al., 2021). Some studies explore controlling feedback tone and specificity (Huang et al., 2022) and addressing fairness concerns (Floridi et al., 2020).\n\nWe introduce a controllable generator that conditions on learning objectives and student proficiency, with evaluations across two academic corpora.",
    "reason": "The span aggregates prior approaches without explaining how they inform or contrast with the proposed controllable generator, reflecting criterion (a).",
    "start": 244,
    "end": 540,
    "label": "Lacks synthesis"
  },
  {
    "span": "BioBERT was used in an ADE extraction task to jointly label drugs and events",
    "document": "Related Work\n\nBiomedical Relation and Event Extraction Neural architectures have substantially advanced biomedical relation extraction by leveraging domain-specific embeddings and ontologies (Wang et al., 2018; Beltagy et al., 2019). Joint models that couple entity recognition with relation prediction reduce error propagation compared to pipeline designs (Li et al., 2016; Giorgi et al., 2019). BioBERT was used in an ADE extraction task to jointly label drugs and events, demonstrating the promise of pretrained transformers for clinical text.\n\nAdverse Drug Event Corpora Several annotated corpora capture pharmacovigilance information from clinical narratives and social media (Sarker and Gonzalez, 2015; Henry et al., 2020). However, label imbalance and annotation inconsistencies across sources hamper cross-domain transfer. We address these issues with a curriculum that reweights minority ADE types and aligns schema variants.",
    "reason": "This is a specific claim about the use of a model in a particular task without providing a citation to the study, which should be cited at first mention.",
    "start": 397,
    "end": 473,
    "label": "Unsupported claim"
  },
  {
    "span": "The LibriSpeech corpus remains the de facto benchmark in English ASR.",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) has progressed rapidly with attention-based encoder–decoder models and self-supervised pretraining (Graves et al., 2014; Chan et al., 2016; Baevski et al., 2020). Performance gains have been observed across both supervised and semi-supervised settings as unlabeled audio has become easier to leverage.\n\nThe LibriSpeech corpus remains the de facto benchmark in English ASR. Nevertheless, domain mismatch between read speech and conversational settings motivates evaluation on more diverse corpora and out-of-domain robustness.",
    "reason": "First mention of a specific dataset as a community benchmark lacks a supporting citation, violating rule (a).",
    "start": 363,
    "end": 432,
    "label": "Unsupported claim"
  },
  {
    "span": "MS MARCO passage ranking is widely considered solved at large scale",
    "document": "Related Work\n\nNeural information retrieval has transitioned from sparse lexical matching to dense and late-interaction architectures that better capture semantic similarity. Benchmarks with human judgments and clicks have facilitated rapid iteration on retrieval objectives and negative sampling strategies. MS MARCO passage ranking is widely considered solved at large scale, shifting attention toward out-of-domain generalization, low-resource domains, and efficiency constraints.\n\nPosition\n\nWe argue that benchmark saturation masks brittle behavior under distribution shift and propose evaluation on temporally updated corpora.",
    "reason": "Makes a broad field-level claim about the status of a specific benchmark without citing supporting studies or surveys (violates guideline b/d).",
    "start": 308,
    "end": 375,
    "label": "Unsupported claim"
  },
  {
    "span": "(O'Connor et. al., 2017)",
    "document": "Introduction\n\nCross-lingual transfer has expanded with multilingual encoders that share subword vocabularies (Conneau et al., 2020; Hu et al., 2020). Prior work (O'Connor et. al., 2017) explored bilingual projection, while later studies refined alignment with parallel corpora (Artetxe et al., 2018; Lample et al., 2018).\n\nWe instead propose a language-agnostic output space that reduces reliance on translation resources.",
    "reason": "Incorrect abbreviation 'et. al.'; the correct form is 'et al.' hence it should be '(O'Connor et al., 2017)'.",
    "start": 161,
    "end": 185,
    "label": "Format"
  },
  {
    "span": "DTW-aug was shown to hurt performance on anomaly detection",
    "document": "Introduction\n\nTime series augmentation has emerged as a critical ingredient for improving generalization in data-scarce domains. Augmentations such as scaling, jittering, and time warping can regularize models but may also distort semantics relevant to downstream tasks. The impact of augmentation is therefore task-dependent and requires careful evaluation.\n\nDTW-aug was shown to hurt performance on anomaly detection, suggesting that warping may obscure transient spikes that signal rare events. By contrast, context-preserving augmentations that operate in frequency space appear less harmful. Motivated by these observations, we design a selective augmentation policy that conditions transforms on instance-level saliency.\n\nWe validate our approach across industrial sensor datasets and release code to facilitate reproducibility.",
    "reason": "This span cites a specific conclusion about a competing method ('DTW-aug') without referencing the study or evidence supporting it, making it an unsupported claim about prior work.",
    "start": 360,
    "end": 418,
    "label": "Unsupported claim"
  },
  {
    "span": "Accordingly, directly applying univariate forecasting models to multivariate settings leads to significantly worse accuracy.",
    "document": "Introduction\n\nForecasting multivariate time series under limited supervision is critical for applications in energy, healthcare, and logistics. While classical approaches often model each variable independently, recent neural methods attempt to capture cross-series dependencies and shared structure.\n\nAccordingly, directly applying univariate forecasting models to multivariate settings leads to significantly worse accuracy. This motivates architectures that explicitly parameterize inter-series correlations and leverage shared representations for improved generalization.\n",
    "reason": "Asserts a performance degradation as a general rule without providing evidence or citations to empirical studies.",
    "start": 302,
    "end": 426,
    "label": "Unsupported claim"
  },
  {
    "span": "UCF-101 has been the de facto standard for evaluating action recognition models.",
    "document": "Introduction\n\nVideo action recognition has advanced quickly with the advent of 3D convolutional networks and transformer-based architectures. Benchmarks play a crucial role in measuring progress and identifying remaining challenges such as long-range temporal reasoning and motion bias. UCF-101 has been the de facto standard for evaluating action recognition models. However, its limited diversity and relatively short clips can inflate apparent performance through context cues rather than genuine motion understanding.\n\nWe propose LongAct, a dataset emphasizing extended temporal dependencies and cross-scene variability, and provide a standardized protocol for fair comparison.",
    "reason": "The statement asserts a field-wide status for a specific dataset but omits a citation to the dataset's original paper or a survey supporting the claim.",
    "start": 287,
    "end": 367,
    "label": "Unsupported claim"
  },
  {
    "span": "and there are many recent works that explore this topic",
    "document": "Introduction\n\nEvaluating open-domain dialogue systems remains a central challenge because surface-form overlap with references often correlates poorly with human judgments (Liu et al., 2016). To address this, researchers have proposed learned metrics that leverage contextual embeddings to better capture semantic adequacy and coherence (Zhang et al., 2020). Reference-free evaluation is particularly attractive for interactive systems where a single gold response is infeasible (Deriu et al., 2020). Robust, reference-free dialogue evaluation has become a key goal, and there are many recent works that explore this topic. Despite promising progress, agreement with human raters still varies across domains and conversational styles, highlighting the need for metrics calibrated to pragmatic factors such as user intent and persona (Mehri and Eskénazi, 2020).\n\nRelated Work\n\nClassical metrics such as BLEU and ROUGE were designed for machine translation and summarization, respectively, and are known to underperform in conversational settings (Papineni et al., 2002; Lin, 2004). Embedding-based metrics such as BERTScore and MoverScore improved semantic sensitivity by comparing token-level or distributional alignments (Zhang et al., 2020; Zhao et al., 2019). Learned dialogue-specific metrics trained on human ratings, including USR and HolisticEval, reported higher correlations but may overfit to annotation artifacts (Mehri and Eskénazi, 2020; Hashimoto et al., 2019). Our work investigates calibration strategies that explicitly model conversation acts and topicality alongside semantic similarity to improve cross-domain generalization.",
    "reason": "The phrase claims the existence of many recent works without providing any citations at the first mention, violating rule (d) about citing recent works.",
    "start": 567,
    "end": 622,
    "label": "Unsupported claim"
  },
  {
    "span": "Patel et al (2022)",
    "document": "Related Work\n\nMulti-omics integration for cancer subtyping combines gene expression, methylation, and proteomics with probabilistic models (Mo et al., 2013; Lock et al., 2013). Patel et al (2022) argue that contrastive pretraining across modalities improves downstream clustering stability. Meanwhile, graph-based fusion exploits patient similarity networks for robust subtypes (Wang et al., 2014).\n\nOur contribution unifies contrastive fusion with uncertainty quantification to provide calibrated subtype assignments.",
    "reason": "Punctuation error in 'et al.' within a citation. It is missing the period after 'al'; the correct form is 'Patel et al. (2022)'.",
    "start": 177,
    "end": 195,
    "label": "Format"
  },
  {
    "span": "Prompting strategies for code generation have proliferated, including zero/few-shot prompting (Brown et al., 2020), chain-of-thought style exemplars (Wei et al., 2022), self-consistency sampling (Wang et al., 2023), automatic prompt construction (Gao et al., 2021; Liu et al., 2023), and tool-augmented prompting that invokes external APIs (Schick et al., 2023).",
    "document": "Related Work\n\nProgram synthesis with large language models (LLMs) increasingly relies on careful prompt design and inference-time strategies rather than architecture changes. Such practices aim to elicit latent knowledge and robust reasoning from pretrained models.\n\nPrompting strategies for code generation have proliferated, including zero/few-shot prompting (Brown et al., 2020), chain-of-thought style exemplars (Wei et al., 2022), self-consistency sampling (Wang et al., 2023), automatic prompt construction (Gao et al., 2021; Liu et al., 2023), and tool-augmented prompting that invokes external APIs (Schick et al., 2023).\n\nWe focus on functionally constrained synthesis where specifications include type signatures, tests, and soft resource budgets.\n",
    "reason": "This sentence lists related prompting work without clarifying how it informs or contrasts with the authors' method, aligning with (a) and the example (ii).",
    "start": 267,
    "end": 629,
    "label": "Lacks synthesis"
  },
  {
    "span": "It is well known that PGD training closes most of the robustness gap on CIFAR-10.",
    "document": "Related Work\n\nAdversarial robustness research has explored certified defenses, robust training, and detection. Projected gradient descent (PGD) adversarial training remains a strong empirical baseline for l∞-bounded attacks. It is well known that PGD training closes most of the robustness gap on CIFAR-10. However, robustness often comes at the cost of standard accuracy and increased computational overhead.\n\nOur approach aims to improve the robustness–accuracy trade-off by decoupling representation learning from robust fine-tuning.\n",
    "reason": "Asserting a widely accepted empirical result for a specific dataset without citation is an unsupported claim.",
    "start": 225,
    "end": 306,
    "label": "Unsupported claim"
  },
  {
    "span": "Univariate detectors rely on statistical thresholds or forecasting residuals, while multivariate methods leverage covariance structure, autoencoders, or graph-based dependencies (Basu et al., 2015; Audibert et al., 2020; Xu et al., 2018; Deldari et al., 2021).",
    "document": "Introduction\n\nAnomaly Detection in Time Series\nDetecting anomalies in temporal data underpins monitoring in manufacturing, finance, and healthcare. Challenges include non-stationarity, sparse labels, and high-dimensional dependencies.\n\nMethodological Landscape\nUnivariate detectors rely on statistical thresholds or forecasting residuals, while multivariate methods leverage covariance structure, autoencoders, or graph-based dependencies (Basu et al., 2015; Audibert et al., 2020; Xu et al., 2018; Deldari et al., 2021).\n\nOur Contributions\nWe present a detection framework that integrates seasonal-trend decomposition with adaptive neighborhood modeling to capture context-aware deviations in multivariate streams.",
    "reason": "Summarizes prior categories without articulating how they relate to or motivate the proposed approach; no explicit stance or gap is provided (definition a/c).",
    "start": 261,
    "end": 521,
    "label": "Lacks synthesis"
  },
  {
    "span": "winners reported Dice scores above 0.90",
    "document": "Related Work\n\nMedical image segmentation has been transformed by encoder–decoder architectures and attention mechanisms. Public challenges catalyze progress by standardizing tasks and metrics across institutions.\n\nOn several organ-specific MICCAI challenges, winners reported Dice scores above 0.90, often achieved with heavy test-time augmentation and model ensembling. While these pipelines maximize leaderboard performance, they can be impractical in clinical workflows due to computational constraints and latency requirements.\n\nWe target single-pass, real-time segmentation with calibration-aware losses, narrowing the accuracy gap without reliance on ensembles.",
    "reason": "The statement references specific prior challenge results and quantitative outcomes without providing citations to the challenge reports or papers.",
    "start": 259,
    "end": 298,
    "label": "Unsupported claim"
  },
  {
    "span": "Previous studies have shown that character-level models outperform subword models on noisy OCR text.",
    "document": "Introduction\n\nDocument understanding systems deployed on historical archives and scanned records must contend with pervasive OCR noise. Mis-segmentation, diacritic loss, and typeface variability induce character substitutions and deletions that degrade lexical cues crucial for downstream tasks such as NER and entity linking.\n\nPrevious studies have shown that character-level models outperform subword models on noisy OCR text. Despite this intuition, subword tokenizers remain the default in many pipelines due to efficiency and pretraining availability. A systematic comparison across noise regimes and document genres is lacking, and the interaction between normalization, language modeling, and downstream supervision remains underexplored.\n\nWe present a controlled evaluation across synthetic and real OCR noise levels using matched architectures at character, subword, and hybrid granularities. Our analysis isolates the contributions of tokenization, data augmentation, and noise-aware pretraining. Results reveal conditions under which subword models recover robustness and when hybrid designs dominate.\n\nWe provide guidelines for selecting granularity based on noise profiles and resource constraints.",
    "reason": "The sentence attributes findings to 'previous studies' without citing any specific papers to substantiate the claim about model performance (rule b/a).",
    "start": 328,
    "end": 428,
    "label": "Unsupported claim"
  },
  {
    "span": "The SQuAD leaderboard was dominated by span-extraction models until very recently.",
    "document": "Related Work\n\nExtractive question answering (QA) has seen rapid progress due to large-scale pretraining and efficient fine-tuning strategies. Benchmarks such as SQuAD and Natural Questions catalyzed the development of models that align contextual embeddings with answer spans. The SQuAD leaderboard was dominated by span-extraction models until very recently. As generative architectures became more prevalent, the community explored end-to-end decoding that integrates retrieval and reasoning.\n\nOur work revisits extractive QA under resource constraints, aiming to retain competitive accuracy with fewer parameters through improved span selection and calibration.\n",
    "reason": "Historical claim about a public leaderboard should be supported with citations or evidence.",
    "start": 277,
    "end": 359,
    "label": "Unsupported claim"
  },
  {
    "span": "Causal discovery methods include constraint-based algorithms that test conditional independencies (Spirtes et al., 2000; Colombo and Maathuis, 2014), score-based search over graph structures (Chickering, 2002; Hauser and Bühlmann, 2012), functional causal models exploiting asymmetries (Hoyer et al., 2009; Peters et al., 2014), and continuous optimization over DAGs (Zheng et al., 2018; Yu et al., 2019). Recent work considers nonlinearities and latent confounding with varying assumptions (Kocaoglu et al., 2019; Brehmer et al., 2022).",
    "document": "Related Work\n\nInferring causal structure from observational data requires assumptions and algorithms that can trade off identifiability with robustness. Practical pipelines must also handle finite samples, noise, and potential hidden variables.\n\nCausal discovery methods include constraint-based algorithms that test conditional independencies (Spirtes et al., 2000; Colombo and Maathuis, 2014), score-based search over graph structures (Chickering, 2002; Hauser and Bühlmann, 2012), functional causal models exploiting asymmetries (Hoyer et al., 2009; Peters et al., 2014), and continuous optimization over DAGs (Zheng et al., 2018; Yu et al., 2019). Recent work considers nonlinearities and latent confounding with varying assumptions (Kocaoglu et al., 2019; Brehmer et al., 2022).\n\nThis paper focuses on heterogeneous datasets with site-specific shifts. We propose an invariance-regularized learner that aggregates local conditional distributions to recover stable edges while tolerating mild confounding.",
    "reason": "The span compiles categories and citations without articulating how they relate to heterogeneous, site-shifted data or what gap motivates the proposed invariance-regularized learner, hence lacking synthesis (criterion a and c).",
    "start": 246,
    "end": 783,
    "label": "Lacks synthesis"
  },
  {
    "span": "Hernandez and Cole (2019) collected a grasp dataset with multi-view RGB. Li et al. (2020) optimized grasp quality metrics with analytic models. Park and Nair (2021) trained policies from demonstrations. Zhao et al. (2022) incorporated tactile signals for slip detection.",
    "document": "Related Work\n\nRobotic grasping has progressed through data-driven perception, analytic metrics, and tactile feedback integration. Learning-based approaches leverage large datasets and demonstrations, while hybrid methods use physics for stability assessment.\n\nHernandez and Cole (2019) collected a grasp dataset with multi-view RGB. Li et al. (2020) optimized grasp quality metrics with analytic models. Park and Nair (2021) trained policies from demonstrations. Zhao et al. (2022) incorporated tactile signals for slip detection.\n\nRecent efforts consider domain randomization for sim-to-real transfer (Singh et al., 2023). We build on this by fusing visual and tactile embeddings with uncertainty-aware selection to improve grasp success under distribution shift.",
    "reason": "The span presents four studies in isolation without transitions or explanation of how datasets, analytic metrics, demonstrations, and tactile sensing interrelate, producing abrupt shifts and unclear linkage.",
    "start": 260,
    "end": 530,
    "label": "Coherence"
  },
  {
    "span": "Transformer-based recommenders have recently dominated top positions on the RecSys challenge leaderboard",
    "document": "Introduction\n\nSession-based recommendation benefits from modeling sequential user behavior, with recurrent and attention-based architectures yielding strong performance (Hidasi et al., 2016; Kang and McAuley, 2018). Pretrained sequence models adapted from language modeling have further improved long-range dependency capture and cold-start robustness (Sun et al., 2019). Transformer-based recommenders have recently dominated top positions on the RecSys challenge leaderboard. Yet, questions remain about their data efficiency and stability under distribution shift in production environments.\n\nRelated Work\n\nGraph neural networks leverage item co-occurrence and heterogeneous relations to refine user and item representations (Wu et al., 2019; Wang et al., 2019). Contrastive objectives and augmentation strategies improved robustness to sparse interactions (Xie et al., 2020). Our approach integrates sequential transformers with graph propagation and introduces calibration losses to temper popularity bias.",
    "reason": "This claim refers to competition results and 'leaderboard' dominance without citing the shared task or leaderboard source, violating rule (a) and (d).",
    "start": 372,
    "end": 476,
    "label": "Unsupported claim"
  },
  {
    "span": "Johnson et al., 2018",
    "document": "Related Work\n\nContrastive representation learning has been widely adopted in vision and language. Following Johnson et al., 2018, we consider margin-based objectives alongside temperature-scaled softmax losses (Oord et al., 2018). Subsequent work introduced momentum encoders and hard negative mining (He et al., 2020; Xiong et al., 2021), while in NLP, semantic retrieval benefits from careful in-batch negatives (Karpukhin et al., 2020).",
    "reason": "Narrative citation is missing the correct narrative format. It should be Johnson et al. (2018) rather than including a comma and year without parentheses.",
    "start": 108,
    "end": 128,
    "label": "Format"
  },
  {
    "span": "Multiple lines of work have proposed fairness metrics for recommenders, such as exposure parity, calibration, demographic parity, and equal opportunity (Singh and Joachims, 2018; Biega et al., 2018; Ekstrand et al., 2021; Burke, 2017). Algorithmic interventions include re-ranking (Zehlike et al., 2017; Singh and Joachims, 2019), constrained optimization (Yao and Huang, 2017; Narasimhan, 2018), and adversarial representation learning (Beutel et al., 2019; Li et al., 2021).",
    "document": "Introduction\n\nRecommender systems are increasingly deployed in high-stakes domains, raising concerns about the distribution of exposure and benefits across users and items. Although accuracy remains a key objective, stakeholders expect guarantees that recommendations do not systematically disadvantage certain groups.\n\nMultiple lines of work have proposed fairness metrics for recommenders, such as exposure parity, calibration, demographic parity, and equal opportunity (Singh and Joachims, 2018; Biega et al., 2018; Ekstrand et al., 2021; Burke, 2017). Algorithmic interventions include re-ranking (Zehlike et al., 2017; Singh and Joachims, 2019), constrained optimization (Yao and Huang, 2017; Narasimhan, 2018), and adversarial representation learning (Beutel et al., 2019; Li et al., 2021).\n\nHowever, existing approaches typically assume a static catalog and overlook short-horizon supply shifts induced by the recommender itself. We hypothesize that such feedback loops create fairness drift and propose an exposure budgeting mechanism that anticipates catalog dynamics via counterfactual simulation.",
    "reason": "The span enumerates metrics and methods without tying them to the paper’s focus on dynamic exposure drift or stating how prior work falls short, thus lacking synthesis per (a) and (b).",
    "start": 320,
    "end": 796,
    "label": "Lacks synthesis"
  },
  {
    "span": "The widely used XYZ-10K dataset contains exactly 10,742 annotated tweets.",
    "document": "Related Work\n\nDatasets for abusive language and hate speech detection vary widely in scale, annotation protocol, and domain coverage. Prior benchmarks often mix public figures' posts with crowd-sourced data, introducing shifts in style and topic that complicate generalization. Furthermore, label taxonomies differ, ranging from binary toxicity to fine-grained intent and target identification.\n\nThe widely used XYZ-10K dataset contains exactly 10,742 annotated tweets. In addition to its size, it includes demographic attributes for a subset of authors and document-level metadata such as timestamps and retweet counts. Several follow-up works have reported robustness gaps when models trained on one dataset are evaluated on another, pointing to spurious lexical cues.\n\nWe focus on robustness under lexical perturbation and topical shift by constructing controlled splits that isolate proxies (e.g., slurs and group identifiers) from true causal indicators of hatefulness. Our evaluation protocol emphasizes cross-corpus transfer to quantify overfitting to dataset-specific artifacts.",
    "reason": "Reports a precise dataset statistic and asserts its widespread use without citing the dataset or source.",
    "start": 396,
    "end": 469,
    "label": "Unsupported claim"
  },
  {
    "span": "[Lee, 2015]",
    "document": "Related Work\n\nDomain adaptation for sentiment analysis typically relies on pivot features and distribution alignment (Blitzer et al., 2007; Chen and Cardie, 2018). While early approaches emphasized feature augmentation, later work introduced adversarial objectives to learn domain-invariant representations (Ganin et al., 2016; Tzeng et al., 2017). More recently, self-training with confidence filtering has shown competitive results across multiple shifts (Zou and Zhu, 2019). Prior surveys [Lee, 2015] provide a taxonomy of adaptation strategies, but do not cover the rapid developments in transformer-based methods (Gururangan et al., 2020). Our contribution synthesizes these lines by combining adversarial alignment with pseudo-label calibration for improved target-domain robustness (Hsu and Ko, 2021).",
    "reason": "Wrong bracket style for author–year citation: used square brackets \"[Lee, 2015]\" instead of parentheses; should be \"(Lee, 2015)\" to match the author–year style in the document.",
    "start": 492,
    "end": 503,
    "label": "Format"
  },
  {
    "span": "In federated learning, baseline aggregation methods like FedAvg and FedProx address heterogeneous data and system constraints (McMahan et al., 2017; Li et al., 2020). Personalization is further explored via cluster-based aggregation, mixture models, and meta-learning (Mansour et al., 2020; Arivazhagan et al., 2019; Fallah et al., 2020). Our method follows this line of work.",
    "document": "Introduction\n\nFederated learning enables training on decentralized data while keeping raw data on devices. A persistent challenge is client heterogeneity, which manifests in both statistical and system dimensions.\n\nIn federated learning, baseline aggregation methods like FedAvg and FedProx address heterogeneous data and system constraints (McMahan et al., 2017; Li et al., 2020). Personalization is further explored via cluster-based aggregation, mixture models, and meta-learning (Mansour et al., 2020; Arivazhagan et al., 2019; Fallah et al., 2020). Our method follows this line of work.\n\nWe study how per-client adaptations can improve utility without incurring substantial communication or memory overhead on resource-constrained devices.",
    "reason": "The paragraph summarizes prior work and then states that the authors follow this line without articulating what is missing in prior methods or what specific gap is addressed, thus lacking synthesis per criterion b (and also lacking the author’s perspective per c).",
    "start": 215,
    "end": 591,
    "label": "Lacks synthesis"
  },
  {
    "span": "user simulators reduce annotation cost by over 60%",
    "document": "Related Work\n\nTask-oriented dialog systems require substantial annotated interactions to learn effective policies. To reduce data collection burdens, user simulators approximate human behavior, enabling scalable training and evaluation. Prior studies have consistently shown that user simulators reduce annotation cost by over 60% while maintaining comparable task success in offline evaluations.\n\nNevertheless, simulators often exhibit limited linguistic diversity and brittle behavior outside scripted domains. Recent methods aim to improve realism via latent goal modeling and adversarial training, but transferring simulator-trained policies to real users remains challenging. Our work addresses this gap with a hybrid simulator that injects controlled variability at both discourse and semantic levels.",
    "reason": "Presents a specific statistic ('over 60%') about prior work without providing a citation to support the numerical claim.",
    "start": 280,
    "end": 330,
    "label": "Unsupported claim"
  },
  {
    "span": "Federated averaging aggregates on-device gradients without centralizing data (McMahan et al., 2017). Differential privacy limits information leakage via noise (Abadi et al., 2016). Secure aggregation prevents the server from inspecting individual updates (Bonawitz et al., 2017). Compression of updates reduces communication (Alistarh et al., 2017).",
    "document": "Introduction\n\nFederated learning enables collaborative training across edge devices while keeping raw data local. Practical deployments must balance privacy, security, and communication efficiency, motivating techniques that address these concerns at different layers of the stack.\n\nFederated averaging aggregates on-device gradients without centralizing data (McMahan et al., 2017). Differential privacy limits information leakage via noise (Abadi et al., 2016). Secure aggregation prevents the server from inspecting individual updates (Bonawitz et al., 2017). Compression of updates reduces communication (Alistarh et al., 2017).\n\nOur contribution composes secure aggregation with record-level differential privacy and sparsified updates, providing end-to-end guarantees under adversarial participation.",
    "reason": "The span enumerates distinct mechanisms (optimization, privacy, cryptography, compression) with no transitions or explanation of how they interact, making the connections between citations abrupt and implicit.",
    "start": 283,
    "end": 632,
    "label": "Coherence"
  },
  {
    "span": "(Johnson et al. 2019)",
    "document": "Related Work\n\nDomain adaptation for robotics perception tackles distribution shifts between simulation and reality through feature alignment and data augmentation (Tzeng et al., 2017; James et al., 2019). Texture randomization reduces overfitting to visual cues (Tobin et al., 2017). Cycle-consistent translation refines synthetic images to appear realistic (Zhu et al., 2017). While behavior cloning offers strong baselines, covariate shift leads to compounding errors (Ross et al., 2011). Recent advances in self-supervised representation learning have improved sample efficiency (Caron et al., 2020; He et al., 2020). For grasp planning, ensemble uncertainty has been shown to correlate with success rates (Johnson et al. 2019), but prior methods lack principled calibration under distribution shift.",
    "reason": "Missing comma between author and year in a parenthetical citation; should be (Johnson et al., 2019).",
    "start": 709,
    "end": 730,
    "label": "Format"
  },
  {
    "span": "Exposure-aware recommenders aim to balance item visibility across providers (Singh and Joachims, 2018; Diaz et al., 2020). Calibration aligns recommended distributions with user profiles (Steck, 2018). Explanations for recommendations improve user trust (Tintarev and Masthoff, 2015).",
    "document": "Related Work\n\nFairness in recommender systems has been studied at the levels of users, items, and providers. Exposure-aware recommenders aim to balance item visibility across providers (Singh and Joachims, 2018; Diaz et al., 2020). Calibration aligns recommended distributions with user profiles (Steck, 2018). Explanations for recommendations improve user trust (Tintarev and Masthoff, 2015). Beyond ranking, auditing methods diagnose disparate impacts in logs (Sapiezynski et al., 2019). We focus on provider fairness under constrained catalog coverage with minimal quality loss.",
    "reason": "The span strings together exposure fairness, calibration, and explanations without transitions or an explicit rationale linking them, making the connections between the cited works unclear and abrupt.",
    "start": 109,
    "end": 393,
    "label": "Coherence"
  },
  {
    "span": "Prior studies on fairness in recommender systems show that exposure parity can be achieved with negligible utility loss.",
    "document": "Related Work\n\nFairness in recommendation has focused on balancing exposure across items, creators, and user groups while maintaining relevance. Approaches include re-ranking, constrained optimization, and counterfactual inference to address selection bias.\n\nPrior studies on fairness in recommender systems show that exposure parity can be achieved with negligible utility loss. However, most methods assume static user preferences and do not model feedback loops that may amplify disparities over time.\n\nWe introduce a dynamic re-ranking algorithm that jointly optimizes exposure and long-term engagement under explicit utility–fairness trade-offs.",
    "reason": "Claims findings from prior studies without citing any of those studies, violating rule (a) and (d).",
    "start": 258,
    "end": 378,
    "label": "Unsupported claim"
  },
  {
    "span": "Hybrid HMM-DNN systems dominated early LVCSR benchmarks (Hinton et al., 2012). End-to-end CTC and attention models simplify training pipelines (Graves et al., 2006; Chorowski et al., 2015). Streaming transducers enable low-latency decoding (Graves, 2012). Multilingual sharing reduces data demands for low-resource languages (Pratap et al., 2020).",
    "document": "Related Work\n\nAutomatic speech recognition\nSpeech recognition systems have evolved from hybrid pipelines to end-to-end neural architectures, substantially simplifying engineering while improving performance. Advances in self-supervised pretraining and data augmentation have further closed the gap on challenging domains.\n\nHybrid HMM-DNN systems dominated early LVCSR benchmarks (Hinton et al., 2012). End-to-end CTC and attention models simplify training pipelines (Graves et al., 2006; Chorowski et al., 2015). Streaming transducers enable low-latency decoding (Graves, 2012). Multilingual sharing reduces data demands for low-resource languages (Pratap et al., 2020).\n\nOur work focuses on confidence estimation for streaming ASR, aligning token-level uncertainty with real-time decoding constraints.",
    "reason": "The span strings together four sentences about different ASR paradigms without transitions or explanation of relationships, making the connection between the cited works abrupt and leaving their relevance to one another implicit.",
    "start": 323,
    "end": 670,
    "label": "Coherence"
  },
  {
    "span": "In (Lopez et al., 2020)",
    "document": "Introduction\n\nOpen-domain question answering requires robust retrieval and reasoning. In (Lopez et al., 2020) the authors argue that dense retrievers outperform sparse methods when training data is abundant, while hybrid indices remain competitive in low-resource settings (Chen et al., 2017; Karpukhin et al., 2020). Subsequent analyses show that retriever-reader co-training further narrows the gap to human performance (Izacard and Grave, 2021).\n\nWe revisit this debate by incorporating domain-specific pretraining (Gururangan et al., 2020) and contrastive objectives (Gao et al., 2021), evaluating on multilingual benchmarks (Conneau et al., 2020).",
    "reason": "Wrong citation style for narrative context; should be \"In Lopez et al. (2020)\" rather than a parenthetical citation after the preposition.",
    "start": 86,
    "end": 109,
    "label": "Format"
  },
  {
    "span": "There are many recent works that explore cross-language code summarization.",
    "document": "Related Work\n\nNeural code summarization seeks to generate natural language descriptions of source code, aiding software maintenance and comprehension (Allamanis et al., 2018; Ahmad et al., 2020). Transformer-based models leverage structural cues from ASTs and data flow to improve faithfulness (LeClair et al., 2020; Wang et al., 2021). There are many recent works that explore cross-language code summarization. Parallel corpora are scarce and noisy, making it challenging to transfer knowledge across programming languages with divergent idioms (Feng et al., 2020). We study multilingual pretraining with modality-aligned adapters to bridge syntax and semantics across languages.\n\nIntroduction\n\nWe evaluate on Java, Python, and Go datasets and introduce a new benchmark for low-resource languages.",
    "reason": "Uses a vague 'recent works' claim about a specific topic without providing citations to those works.",
    "start": 337,
    "end": 412,
    "label": "Unsupported claim"
  },
  {
    "span": "The VQA v3.0 dataset introduced a fairness-aware split specifically to reduce language priors.",
    "document": "Related Work on Bias and Robustness in VQA\n\nVisual Question Answering (VQA) models often exploit language priors rather than genuine visual grounding, leading to inflated in-distribution accuracy and poor robustness (Goyal et al., 2017; Agrawal et al., 2018). Dataset interventions and counterfactual data augmentation have been proposed to address these issues (Niu et al., 2021).\n\nThe VQA v3.0 dataset introduced a fairness-aware split specifically to reduce language priors. Beyond partition design, several methods propose debiasing objectives and modular attention mechanisms to encourage reliance on image evidence. Our work complements these approaches by quantifying question-type-specific shifts and evaluating calibration under distribution change.",
    "reason": "Claims a specific dataset change ('VQA v3.0' fairness-aware split) without citing the dataset release or paper introducing that split.",
    "start": 383,
    "end": 477,
    "label": "Unsupported claim"
  },
  {
    "span": "(Brown et al. 2020)",
    "document": "Introduction\n\nLarge language models have demonstrated emergent capabilities when scaled in parameters and data (Kaplan et al., 2020; Hoffmann et al., 2022). The GPT-3 model (Brown et al. 2020) showcased strong few-shot performance across diverse NLP tasks without task-specific fine-tuning. Instruction tuning and alignment further improve usability and safety (Sanh et al., 2022; Ouyang et al., 2022).\n\nSubsequent research explores efficient adaptation via parameter-efficient methods and retrieval augmentation (Lester et al., 2021; Borgeaud et al., 2022). Data curation and contamination analysis have also proven crucial for reliable evaluation (Dodge et al., 2021; Magar and Schwartz, 2022).",
    "reason": "Missing comma before the year in a parenthetical citation; should be '(Brown et al., 2020)'.",
    "start": 173,
    "end": 192,
    "label": "Format"
  },
  {
    "span": "Existing studies have repeatedly documented gender bias in contextual embeddings across multiple languages.",
    "document": "Related Work\n\nFairness in language technologies requires understanding how demographic biases propagate through representations into downstream tasks. Contextual embeddings derived from large corpora can encode social stereotypes that manifest in classification, generation, and retrieval systems.\n\nExisting studies have repeatedly documented gender bias in contextual embeddings across multiple languages. Despite growing awareness, there is limited consensus on measurement protocols and on the relationship between intrinsic bias scores and extrinsic task harms.\n\nWe contribute a cross-lingual evaluation suite that harmonizes bias probes and task-level audits, and we analyze the trade-offs between debiasing strength and task performance.",
    "reason": "Claims a body of prior work documenting bias without providing citations, which is required when referencing existing studies.",
    "start": 299,
    "end": 406,
    "label": "Unsupported claim"
  },
  {
    "span": "Early spatiotemporal models for traffic prediction use graph convolutional networks to encode road topology, including STGCN (Yu et al., 2018), DCRNN (Li et al., 2018), and Graph WaveNet (Wu et al., 2019). Subsequent work extends temporal modeling with dilated causal convolutions (Bai et al., 2018), attention over time (Guo et al., 2019), and adaptive adjacency matrices (Pan et al., 2021). More recent methods incorporate dynamic graphs (Han et al., 2021) and external factors such as weather and events (Zheng et al., 2020; Xu et al., 2021).",
    "document": "Related Work\n\nSpatiotemporal forecasting on road networks has become a central topic in intelligent transportation systems due to the growing availability of sensor data and the operational need for reliable short-term forecasts. Methods vary widely in how they capture spatial dependencies, temporal patterns, and exogenous signals.\n\nEarly spatiotemporal models for traffic prediction use graph convolutional networks to encode road topology, including STGCN (Yu et al., 2018), DCRNN (Li et al., 2018), and Graph WaveNet (Wu et al., 2019). Subsequent work extends temporal modeling with dilated causal convolutions (Bai et al., 2018), attention over time (Guo et al., 2019), and adaptive adjacency matrices (Pan et al., 2021). More recent methods incorporate dynamic graphs (Han et al., 2021) and external factors such as weather and events (Zheng et al., 2020; Xu et al., 2021).\n\nSome approaches emphasize robustness to sensor failures and missing data by imputing time series prior to prediction (Chen et al., 2020) or jointly learning imputation and forecasting (Cini et al., 2020). Others consider multi-step forecasting stability and error propagation, introducing curriculum learning or direct multi-horizon objectives (Lim and Zohren, 2021).\n\nIntroduction\n\nUrban congestion and incident response demand accurate traffic predictions across horizons and network scales. While recent GNN-based models have made progress, their deployment constraints and maintenance cost remain significant in practice. In this paper, we introduce T-MoST, a modular spatiotemporal forecaster designed for real-time operation on large urban networks.",
    "reason": "The span catalogs prior traffic forecasting models and extensions without relating them to the paper’s aims or stating what is missing in existing approaches, satisfying criterion (a)/(c) for lack of synthesis.",
    "start": 335,
    "end": 880,
    "label": "Lacks synthesis"
  },
  {
    "span": "We evaluate our models on the widely adopted MIMIC-III dataset.",
    "document": "Related Work\n\nClinical text classification leverages electronic health records to predict outcomes such as mortality, readmission, and adverse events (Gehrmann et al., 2018; Si et al., 2019). Prior work has explored hierarchical attention networks and ontology-informed representations to encode long documents and domain terminology (Zhang et al., 2018; Huang et al., 2019).\n\nWe evaluate our models on the widely adopted MIMIC-III dataset. In addition, we consider de-identification artifacts and note that domain-specific tokenization can materially affect performance on clinical abbreviations (Neumann et al., 2019).",
    "reason": "The first mention of the dataset 'MIMIC-III' lacks a citation to its source, violating rule (a) about citing datasets on first mention.",
    "start": 377,
    "end": 440,
    "label": "Unsupported claim"
  },
  {
    "span": "Prior studies have shown that random walk-based graph augmentation always improves recommendation accuracy.",
    "document": "Introduction\n\nGraph-based recommender systems exploit user–item interaction structures to learn representations that capture collaborative signals. Methods based on graph neural networks (GNNs) propagate embeddings along edges, synthesizing neighborhood information to refine user and item features. Data augmentation has emerged as a technique to improve robustness, with strategies including edge dropout, feature masking, and random walk sampling.\n\nPrior studies have shown that random walk-based graph augmentation always improves recommendation accuracy. Nevertheless, the mechanisms by which augmentation impacts long-tail items and cold-start users are not well understood, and the benefits may vary with graph sparsity and degree distribution.\n\nWe conduct a comprehensive evaluation of augmentation strategies across datasets with diverse sparsity profiles. We also propose a degree-aware augmentation policy that balances exploration and preservation of informative edges. Empirically, our approach leads to consistent gains on top-n recommendation metrics while preserving interpretability of learned embeddings.\n\nRelated Work\n\nRecommendation augmentation has been explored primarily in the context of contrastive learning, where positive pairs are constructed via stochastic perturbations of the interaction graph. Our analysis extends these findings by examining the interaction between augmentation intensity and user/item degree.",
    "reason": "Asserts a universal effect ('always improves') based on prior studies without providing citations.",
    "start": 452,
    "end": 559,
    "label": "Unsupported claim"
  },
  {
    "span": "Zhou et al.",
    "document": "Introduction\n\nReasoning over temporal knowledge graphs requires modeling both structural connectivity and time-evolving relations. Earlier static methods struggled to capture temporal ordering and recurrence patterns (Bordes et al., 2013; Trouillon et al., 2016). Zhou et al. introduce a recurrent encoder that conditions link prediction on localized temporal windows, improving extrapolation. More recent approaches combine contrastive time-aware objectives with graph attention (Trivedi et al., 2017; Goel et al., 2020; Xu and Li, 2021). However, evaluation protocols are often inconsistent across datasets, hindering fair comparison (Sadeghian et al., 2019). To address this gap, we propose a unified benchmark with standardized train/validation/test splits and time-sliced negatives.",
    "reason": "Narrative citation missing year: the narrative reference \"Zhou et al.\" omits the publication year. It should be formatted as \"Zhou et al. (YEAR)\" per author–year style.",
    "start": 264,
    "end": 275,
    "label": "Format"
  },
  {
    "span": "Personalized federated learning has explored regularization-based methods such as pFedMe (Dinh et al., 2020), model interpolation as in Per-FedAvg (Fallah et al., 2020), representation sharing in FedRep (Collins et al., 2021), and multi-task formulations like MOCHA (Smith et al., 2017). Other studies consider meta-learning (Ji et al., 2019), mixture models (Arivazhagan et al., 2019), and proximal objectives (Li et al., 2020) to stabilize personalization across clients.",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative model training without centralizing raw data, but client heterogeneity makes a single global model suboptimal. This has motivated a large body of work on personalization.\n\nPersonalized federated learning has explored regularization-based methods such as pFedMe (Dinh et al., 2020), model interpolation as in Per-FedAvg (Fallah et al., 2020), representation sharing in FedRep (Collins et al., 2021), and multi-task formulations like MOCHA (Smith et al., 2017). Other studies consider meta-learning (Ji et al., 2019), mixture models (Arivazhagan et al., 2019), and proximal objectives (Li et al., 2020) to stabilize personalization across clients.\n\nCommunication efficiency has been addressed via partial participation, client sampling, and compressed updates (Konečný et al., 2016; Sahu et al., 2018), while privacy is often incorporated through secure aggregation and differential privacy (Bonawitz et al., 2017; Truex et al., 2019).\n\nIntroduction\n\nWe study personalization when clients face scarce labeled data but abundant unlabeled data. We propose a semi-supervised FL method with consistency regularization and client-specific adapters for improved local generalization.",
    "reason": "The span lists methods and citations but does not connect them to the authors’ problem setting or clarify limitations they intend to address, matching criterion (a)/(c).",
    "start": 232,
    "end": 705,
    "label": "Lacks synthesis"
  },
  {
    "span": "In a previous study, the authors claim that multi-task learning mitigates spurious correlations in sarcasm detection.",
    "document": "Related Work\n\nSarcasm detection often exploits lexical cues that correlate spuriously with labels, leading to fragile models. Multi-task learning (MTL) has been proposed to induce more robust representations by sharing structure with related tasks such as sentiment or emotion classification.\n\nIn a previous study, the authors claim that multi-task learning mitigates spurious correlations in sarcasm detection. Follow-up analyses suggest that the effect may depend on the auxiliary task’s label topology and the alignment between datasets.\n\nOur work revisits these claims under controlled splits and measures robustness using contrastive test sets.",
    "reason": "Mentions a specific prior study and its claim but provides no citation (violates rule a).",
    "start": 294,
    "end": 411,
    "label": "Unsupported claim"
  },
  {
    "span": "Large language models have been applied to code completion, summarization, and bug fixing (Svyatkovskiy et al., 2020; Ahmad et al., 2021; Chen et al., 2021). Several datasets and benchmarks target automated program repair with neural methods (Tufano et al., 2019; Jiang et al., 2021; Le et al., 2022).",
    "document": "Related Work\n\nAutomated program repair (APR) aims to generate patches that resolve defects without introducing regressions. Recent advances in pre-trained code models have created new opportunities for learning-based APR under limited supervision.\n\nLarge language models have been applied to code completion, summarization, and bug fixing (Svyatkovskiy et al., 2020; Ahmad et al., 2021; Chen et al., 2021). Several datasets and benchmarks target automated program repair with neural methods (Tufano et al., 2019; Jiang et al., 2021; Le et al., 2022). Other works study search-based repair with hand-crafted templates (Kim et al., 2013; Martinez and Monperrus, 2014).\n\nWe fine-tune a decoder-only model with contrastive objectives for patch plausibility and evaluate on Java and Python defects.",
    "reason": "The span enumerates prior applications and datasets without clarifying how the proposed fine-tuning differs or which unmet need it addresses, matching criteria (a) and (b).",
    "start": 249,
    "end": 550,
    "label": "Lacks synthesis"
  },
  {
    "span": "It is well known in the QG literature that answer-aware models outperform answer-agnostic ones.",
    "document": "Related Work\n\nQuestion generation (QG) has evolved from rule-based templates to neural models conditioned on passages and auxiliary signals. Conditioning on the answer span has been proposed to guide content selection and improve answerability, while passage-only models rely on implicit identification of salient regions.\n\nIt is well known in the QG literature that answer-aware models outperform answer-agnostic ones. Subsequent methods have explored richer encodings of answer context, including syntactic cues and discourse relations, as well as copy mechanisms and constrained decoding for better faithfulness.\n\nOur approach builds on this intuition by augmenting answer-aware encoders with graph representations over entity mentions and coreference chains. We show that structural signals further improve content selection and reduce hallucination, especially on out-of-domain evaluation sets.",
    "reason": "Asserts a generally accepted finding in a specific niche area without providing citations to support it.",
    "start": 324,
    "end": 419,
    "label": "Unsupported claim"
  },
  {
    "span": "The METR-LA dataset has over 340 sensors and is the de facto standard for urban traffic forecasting.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become central to spatiotemporal traffic forecasting because they capture both spatial topology and temporal dynamics. Early approaches relied on fixed graph convolutions, while more recent methods learn adaptive adjacency or utilize attention to model long-range interactions. The METR-LA dataset has over 340 sensors and is the de facto standard for urban traffic forecasting. In addition to sensor coverage, evaluation protocols typically consider multi-horizon metrics such as 15-, 30-, and 60-minute MAE and RMSE. Despite widespread use, inconsistencies in preprocessing (e.g., normalization and missing value imputation) hinder direct comparison across studies.\n",
    "reason": "First mention of a specific dataset and a strong normative claim ('de facto standard') require citations.",
    "start": 326,
    "end": 426,
    "label": "Unsupported claim"
  },
  {
    "span": "(Lee and Kim 2021)",
    "document": "Related Work\n\nInteractive image retrieval benefits from relevance feedback loops and attribute-based explanations (Parikh and Grauman, 2011; Kovashka and Grauman, 2015). We adopt a human-in-the-loop pipeline similar to (Lee and Kim 2021) to refine candidate sets using sparse feedback. Complementary work explores counterfactual visual explanations for retrieval (Goyal et al., 2019).\n\nOur method departs from prior art by jointly optimizing retrieval quality and feedback efficiency under a fixed interaction budget.",
    "reason": "Missing comma between author and year in a parenthetical citation. In APA style it should be '(Lee and Kim, 2021)'.",
    "start": 219,
    "end": 237,
    "label": "Format"
  },
  {
    "span": "The NELL knowledge base is widely used for lifelong learning experiments.",
    "document": "Related Work\n\nLifelong and continual learning. Methods mitigate catastrophic forgetting via parameter isolation, regularization, and rehearsal buffers (Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017; Aljundi et al., 2019). In knowledge-rich settings, continual extraction incrementally expands relation schemas and entity inventories (Carlson et al., 2010; Mitchell et al., 2018). The NELL knowledge base is widely used for lifelong learning experiments. Recent advances integrate structured memory with gradient-based adaptation for scalable task growth (de Masson d'Autume et al., 2019; AlShedivat et al., 2018).\n",
    "reason": "Asserts widespread usage of a specific resource without any supporting citation; per (a) first mentions of datasets/KBs should be cited.",
    "start": 388,
    "end": 461,
    "label": "Unsupported claim"
  },
  {
    "span": "In the 2020 Math Word Problem Challenge, ensemble models dominated the leaderboard.",
    "document": "Introduction\n\nSolving math word problems (MWP) combines semantic parsing with numerical reasoning, and recent neural solvers leverage pretraining and program induction to improve accuracy (Xie and Sun, 2019; Liu et al., 2020). Public challenges offer shared evaluation and spur progress through standardized test sets.\n\nIn the 2020 Math Word Problem Challenge, ensemble models dominated the leaderboard. However, the gains often came with significant computational overheads, motivating our study on compact solvers that maintain accuracy while reducing inference cost.",
    "reason": "Refers to a specific shared task outcome without citing the challenge report or leaderboard source; first mentions of a shared task should be cited.",
    "start": 320,
    "end": 403,
    "label": "Unsupported claim"
  },
  {
    "span": "The MIMIC-III dataset contains over 60% noisy labels.",
    "document": "Introduction\n\nClinical NLP for ICU Notes\n\nAutomated coding and phenotyping from intensive care unit notes requires handling misspellings, abbreviations, and domain-specific terminology. The MIMIC-III dataset contains over 60% noisy labels. This motivates methods that are resilient to label noise and capable of leveraging unlabeled text. We propose a semi-supervised framework that combines consistency regularization with uncertainty-aware loss reweighting, targeting improved generalization across hospital cohorts.",
    "reason": "Makes a specific statistical claim about a named dataset without evidence or citation; also first mention of the dataset lacks citation (rules a and b).",
    "start": 186,
    "end": 239,
    "label": "Unsupported claim"
  },
  {
    "span": "Distillation for vision transformers transfers knowledge from CNN or transformer teachers using soft labels, intermediate feature matching, or token-level supervision (Hinton et al., 2015; Touvron et al., 2021; Yuan et al., 2021; Xu et al., 2022). Some methods regularize attention maps or align patch embeddings (Park et al., 2019; Zagoruyko and Komodakis, 2017; Wang et al., 2022). Our method builds on teacher–student distillation.",
    "document": "Related Work\n\nCompact Vision Models\nEdge deployment benefits from compact models that maintain accuracy under strict latency and memory budgets. Knowledge distillation is a dominant strategy to transfer capacity from a large teacher to a smaller student.\n\nDistillation for Vision Transformers\nDistillation for vision transformers transfers knowledge from CNN or transformer teachers using soft labels, intermediate feature matching, or token-level supervision (Hinton et al., 2015; Touvron et al., 2021; Yuan et al., 2021; Xu et al., 2022). Some methods regularize attention maps or align patch embeddings (Park et al., 2019; Zagoruyko and Komodakis, 2017; Wang et al., 2022). Our method builds on teacher–student distillation.\n\nOptimization and Data Regimes\nRecent work studies hard-to-learn tokens, augmentation interplay, and semi-supervised data to improve student robustness.\n\nContribution Overview\nWe introduce a token-aware temperature schedule and an alignment loss that adapts to spatial saliency without auxiliary labels.",
    "reason": "The span summarizes prior techniques and then states the authors' reliance on distillation without identifying a specific shortcoming or motivation, lacking synthesis per (b) and (c).",
    "start": 293,
    "end": 727,
    "label": "Lacks synthesis"
  },
  {
    "span": "Previous studies consistently report that message-passing depth beyond three harms generalization in molecular property prediction",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become the standard for molecular property prediction, capturing local substructure through message passing and pooling (Gilmer et al., 2017; Yang et al., 2019). Regularization techniques and expressivity analyses seek to mitigate over-smoothing and oversquashing in deep GNNs.\n\nPrevious studies consistently report that message-passing depth beyond three harms generalization in molecular property prediction. Various remedies, including residual connections, virtual nodes, and hierarchical pooling, have been explored to improve depth robustness. We complement these efforts by introducing a spectrally regularized layer that preserves discriminative signals at greater depths.",
    "reason": "This generalization about prior empirical findings lacks citations to the studies that report it (rule a and b).",
    "start": 327,
    "end": 457,
    "label": "Unsupported claim"
  },
  {
    "span": "Model-based planning leverages learned dynamics for sample efficiency (Janner et al., 2019). Offline RL addresses distributional shift from fixed datasets (Fujimoto et al., 2019). Intrinsic motivation provides exploration by rewarding novelty (Burda et al., 2019). Recurrent policies handle partial observability in POMDPs (Hausknecht and Stone, 2015).",
    "document": "Related Work\n\nReinforcement learning foundations\nModern RL research spans data collection, learning objectives, and planning modules, with differing assumptions on interaction budgets and observability. Our setting targets limited interaction with stochastic dynamics and partial observations.\n\nModel-based planning leverages learned dynamics for sample efficiency (Janner et al., 2019). Offline RL addresses distributional shift from fixed datasets (Fujimoto et al., 2019). Intrinsic motivation provides exploration by rewarding novelty (Burda et al., 2019). Recurrent policies handle partial observability in POMDPs (Hausknecht and Stone, 2015).\n\nWe propose a unified algorithm that reuses offline data, learns latent dynamics, and schedules exploration bonuses under a fixed interaction budget.",
    "reason": "The span lists distinct RL subtopics in separate sentences without transitions or explicit ties between them, resulting in abrupt shifts and unclear relationships among the cited works.",
    "start": 295,
    "end": 647,
    "label": "Coherence"
  },
  {
    "span": "(Chen and Li, 2018)",
    "document": "Introduction\n\nVision Transformers (ViTs) have shown competitive performance on image classification by modeling long-range dependencies with self-attention (Dosovitskiy et al., 2021). Data-efficient extensions use distillation and strong augmentations to close the gap with CNNs at smaller scales (Touvron et al., 2021; Steiner et al., 2021). Hybrid architectures retain convolutional stems for improved inductive bias (Graham et al., 2021).\n\nScaling laws suggest predictable gains with model size and data quantity, motivating large curated datasets (Zhai et al., 2022). Regularization via stochastic depth and token dropping improves training efficiency (Huang et al., 2016; Rao et al., 2021). Recent studies examine robustness to corruptions and distribution shifts (Taori et al., 2020; Paul and Chen, 2022), while fine-tuning protocols standardize evaluation across benchmarks (Wightman et al., 2021). Empirical comparisons often report complementary strengths of CNNs and ViTs (Chen and Li, 2018) in transfer and sample efficiency.\n",
    "reason": "Wrong conjunction inside a parenthetical citation for APA style; use '&' instead of 'and': '(Chen & Li, 2018)'.",
    "start": 982,
    "end": 1001,
    "label": "Format"
  },
  {
    "span": "Recent approaches fuse modalities via early fusion (Zadeh et al., 2017), tensor fusion (Zadeh et al., 2017), and transformer-based co-attention mechanisms (Tsai et al., 2019; Hazarika et al., 2020).",
    "document": "Related Work\n\nMultimodal sentiment analysis aims to infer users' affective states by integrating signals from language, vision, and acoustics. The primary difficulty lies in modeling cross-modal interactions and handling temporal asynchrony and noise across modalities.\n\nRecent approaches fuse modalities via early fusion (Zadeh et al., 2017), tensor fusion (Zadeh et al., 2017), and transformer-based co-attention mechanisms (Tsai et al., 2019; Hazarika et al., 2020). Others incorporate graph structures to capture inter-utterance dependencies (Hazarika et al., 2018; Hu et al., 2022) and contrastive objectives to align modalities (Han et al., 2021; Sung et al., 2021).\n\nWe introduce a lightweight fusion module that conditions each modality on a shared latent bottleneck, and we evaluate its robustness to missing modalities on benchmark datasets.",
    "reason": "The span merely catalogs existing fusion strategies without clarifying their limitations or how they motivate the proposed method, satisfying (a) and (c).",
    "start": 271,
    "end": 469,
    "label": "Lacks synthesis"
  },
  {
    "span": "We follow the standard train/dev/test split of the FLORES benchmark.",
    "document": "Related Work\n\nLow-resource machine translation benefits from transfer learning, multilingual pretraining, and lexicon induction to compensate for scarce parallel corpora (Zoph et al., 2016; Conneau and Lample, 2019). Evaluations increasingly include typologically diverse languages and domain-shift scenarios to reflect real-world deployment (Haddow et al., 2021).\n\nBenchmarking protocols matter for comparability across systems. We follow the standard train/dev/test split of the FLORES benchmark. While widely used, differences in preprocessing and tokenization can still yield nontrivial variance. Our experiments standardize normalization, script handling, and subword training to isolate model contributions.",
    "reason": "The benchmark is mentioned for the first time and a specific standard split is invoked without any citation to the benchmark or documentation that defines the split.",
    "start": 430,
    "end": 498,
    "label": "Unsupported claim"
  },
  {
    "span": "To the best of our knowledge, no prior work studies temporal covariate shift in clinical time series at patient-level granularity.",
    "document": "Related Work\n\nDistribution shift is pervasive in healthcare data due to evolving practices, sensor upgrades, and population changes. Prior literature has addressed domain adaptation at the cohort level and label shift under fixed feature distributions. Temporal drift has been explored for aggregate predictive maintenance in non-clinical settings.\n\nTo the best of our knowledge, no prior work studies temporal covariate shift in clinical time series at patient-level granularity. Our work decomposes drift into feature and conditional components and proposes a patient-conditioned reweighting scheme to stabilize predictions over time.\n\nWe situate our approach within covariate-shift correction and provide an empirical study on two multi-year hospital datasets with varying measurement protocols.",
    "reason": "This is a claim about the absence of prior work on a specific topic without citing surveyed literature or contrasting studies, thus unsupported per rule (b) and (e).",
    "start": 350,
    "end": 480,
    "label": "Unsupported claim"
  },
  {
    "span": "State-of-the-art visual navigation agents consistently surpass human teleoperation on unseen houses.",
    "document": "Introduction\n\nEmbodied visual navigation requires agents to perceive their surroundings, localize, and plan goal-directed actions from egocentric observations (Perez et al., 2020). Progress has been driven by photorealistic simulators and large-scale datasets that enable training at scale (Ibrahim and Chen, 2021).\n\nState-of-the-art visual navigation agents consistently surpass human teleoperation on unseen houses.\n\nYet, generalization to novel layouts and sensor conditions remains limited. We propose a map-free exploration policy that leverages uncertainty-aware semantic priors to improve success on zero-shot environments.",
    "reason": "Makes a strong 'state-of-the-art surpasses humans' performance claim without any supporting citations.",
    "start": 317,
    "end": 417,
    "label": "Unsupported claim"
  },
  {
    "span": "There are many recent works that explore contrastive decoding for controllable text generation.",
    "document": "Related Work\n\nControllable text generation. Prior approaches condition generation on explicit attributes or control codes to steer style and content (Keskar et al., 2019; Dathathri et al., 2020; Krause et al., 2021). Methods based on constrained decoding further guide outputs using lexical or syntactic restrictions (Post and Vilar, 2018; Hokamp and Liu, 2017). There are many recent works that explore contrastive decoding for controllable text generation. In parallel, calibration techniques adjust token probabilities to align with desired control targets (Holtzman et al., 2019; Liu et al., 2021b). Our work differs by combining soft control signals with light-weight guidance during inference.\n",
    "reason": "Claims the existence of 'many recent works' without citing any of them; per rule (d) mentions of recent works must be supported by citations.",
    "start": 363,
    "end": 458,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from three grade levels",
    "document": "Introduction\n\nAutomated essay scoring (AES) aims to predict human-assigned scores for student writing and has a long history in educational assessment (Attali and Burstein, 2006). Modern neural approaches model both local coherence and global discourse structure to capture rubric-aligned qualities such as organization and style (Taghipour and Ng, 2016; Dong and Zhang, 2016). The ASAP dataset popularized benchmark comparisons and error analyses for prompt-specific scoring (Shermis, 2014). BERT was used in an AES task trained on essays from three grade levels, yet questions remain about whether pretrained encoders capture rubric constructs or merely spurious correlations.\n\nRelated Work\n\nPairwise ranking losses and holistic scoring have been contrasted with trait-level scoring to provide formative feedback (Cummins et al., 2016; Zhang and Litman, 2018). Recent work investigates fairness by auditing performance across demographic and prompt subgroups, emphasizing the need for transparent, interpretable features (Mayfield and Black, 2019). We extend this line by probing trait sensitivity and calibration across prompts with controlled lexical perturbations.",
    "reason": "The sentence asserts a specific prior setup (use of BERT in AES on three grade levels) without any citation, matching example (iii) and rule (a).",
    "start": 493,
    "end": 563,
    "label": "Unsupported claim"
  },
  {
    "span": "The SemEval-2021 Task 7 defined this setup as an official shared task",
    "document": "Introduction\n\nSarcasm detection requires modeling pragmatic cues, context, and sometimes external knowledge to disambiguate literal from intended meaning (Joshi et al., 2017). Recent datasets incorporate conversational context and multimodal signals to better approximate real-world usage (Hazarika et al., 2018; Cai et al., 2019). The SemEval-2021 Task 7 defined this setup as an official shared task, drawing attention to contextual benchmarks and standardized evaluation protocols.\n\nRelated Work\n\nApproaches range from feature-rich SVMs with sentiment and incongruity indicators to transformer-based models with context encoders and contrastive learning (Davidov et al., 2010; Zhang et al., 2021). Multimodal variants integrate image-text alignment to capture visual sarcasm, though alignment noise remains a challenge (Cai et al., 2019). We build on context-aware transformers and propose a discourse-level incongruity signal to improve generalization across domains.",
    "reason": "The text mentions a specific shared task without providing a citation at first mention, contravening rule (a).",
    "start": 332,
    "end": 401,
    "label": "Unsupported claim"
  },
  {
    "span": "It is well known that back-translation consistently yields over 5 BLEU improvements in all low-resource pairs.",
    "document": "Introduction\n\nLow-Resource Machine Translation. Transfer learning and monolingual data utilization are central to progress in low-resource MT (Sennrich et al., 2016; Neubig and Hu, 2018). Back-translation and self-training provide pseudo-parallel data to augment scarce bitext. It is well known that back-translation consistently yields over 5 BLEU improvements in all low-resource pairs. Nevertheless, improvements depend on monolingual quality and domain match.",
    "reason": "Presents a quantitative performance claim across all language pairs without citations (violates rule b and d).",
    "start": 278,
    "end": 388,
    "label": "Unsupported claim"
  },
  {
    "span": "The LibriPhone corpus includes 20,000 hours of telephone speech.",
    "document": "Related Work\n\nCorpora for Robust ASR\n\nAutomatic speech recognition in the wild demands robustness to channel variability, background noise, and speaking style. Public corpora such as LibriSpeech and Switchboard have enabled progress on read and conversational speech (Panayotov et al., 2015; Godfrey and Holliman, 1993). The LibriPhone corpus includes 20,000 hours of telephone speech. Domain-mismatched augmentation techniques, including codec simulation and room impulse responses, have been used to bridge performance gaps between close-talk and far-field conditions (Ko et al., 2017; Park et al., 2019). Our work focuses on multi-domain pretraining with explicit channel-aware objectives.",
    "reason": "Introduces a specific dataset and quantitative detail at first mention without providing a citation, violating rule a.",
    "start": 321,
    "end": 385,
    "label": "Unsupported claim"
  },
  {
    "span": "the widely used CodeSearchNet dataset",
    "document": "Related Work\n\nNeural code search aims to retrieve relevant code snippets given natural language queries. Progress has been driven by large-scale datasets and pre-trained models adapted to source code. Dual-encoder architectures align natural language descriptions with code representations via contrastive or cross-entropy objectives.\n\nA major source of empirical comparison has been the widely used CodeSearchNet dataset, alongside smaller language-specific corpora. However, differences in negative sampling and deduplication strategies lead to inconsistent estimates of retrieval difficulty. Furthermore, leakage across train and test splits can inflate apparent generalization.\n\nWe address these issues by curating a family of leakage-checked benchmarks and by introducing a hard-negative mining protocol aligned with real-world developer workflows.",
    "reason": "This is a first mention of a specific dataset and should be accompanied by a citation, but none is provided.",
    "start": 384,
    "end": 421,
    "label": "Unsupported claim"
  },
  {
    "span": "Graph contrastive learning methods maximize agreement between augmented views of graphs or nodes, including DGI (Velickovic et al., 2019), InfoGraph (Sun et al., 2020), GRACE (Zhu et al., 2020), and GraphCL (You et al., 2020). Negative sampling strategies and debiasing have also been investigated (Robinson et al., 2021; Chandra et al., 2022).",
    "document": "Related Work\n\nSelf-supervised learning on graphs seeks to leverage structural signals without labels, often through contrastive objectives or generative pretext tasks. These approaches aim to learn transferable representations for downstream tasks such as node classification and link prediction.\n\nGraph contrastive learning methods maximize agreement between augmented views of graphs or nodes, including DGI (Velickovic et al., 2019), InfoGraph (Sun et al., 2020), GRACE (Zhu et al., 2020), and GraphCL (You et al., 2020). Negative sampling strategies and debiasing have also been investigated (Robinson et al., 2021; Chandra et al., 2022).\n\nGenerative pretraining alternatives reconstruct node attributes or edges (Hu et al., 2020; Qiu et al., 2020). Augmentation design, view alignment, and objective calibration remain active areas of study.\n\nOur approach builds on contrastive pretraining for heterophilous graphs.",
    "reason": "The span enumerates prior graph contrastive methods without explaining their relation to the proposed approach or articulating a motivating gap, hence lacking synthesis under (a)/(c).",
    "start": 298,
    "end": 642,
    "label": "Lacks synthesis"
  },
  {
    "span": "Prior competitions like the DARPA Subterranean Challenge demonstrated that multi-robot coordination is essential for reliable exploration.",
    "document": "Related Work\n\nAutonomous exploration in unstructured environments requires robust perception, communication, and planning under uncertainty. Multi-robot teams promise improved coverage and resilience through redundancy and specialization.\n\nPrior competitions like the DARPA Subterranean Challenge demonstrated that multi-robot coordination is essential for reliable exploration. Insights from these events motivate research on decentralized mapping, task allocation, and resilient communication in challenging conditions.\n\nBuilding on these ideas, we propose a coordination framework that adaptively balances local autonomy with shared objectives under bandwidth constraints.",
    "reason": "The sentence references a specific competition and infers a general conclusion about its findings without providing any citation to support the claim.",
    "start": 240,
    "end": 378,
    "label": "Unsupported claim"
  },
  {
    "span": "Explanation techniques for recommender systems range from attention-weighted textual rationales (Santos et al., 2019), to feature attribution over user–item interactions (Wang and Koren, 2020), to post-hoc surrogate models that approximate black-box recommenders (Qi et al., 2021; Duarte and Silva, 2022).",
    "document": "Related Work\n\nExplainable recommendation has been studied to improve user trust, satisfaction, and decision-making. Methods vary by how they extract, post-process, or generate interpretable signals associated with predicted items.\n\nExplanation techniques for recommender systems range from attention-weighted textual rationales (Santos et al., 2019), to feature attribution over user–item interactions (Wang and Koren, 2020), to post-hoc surrogate models that approximate black-box recommenders (Qi et al., 2021; Duarte and Silva, 2022). Some approaches incorporate natural language generation to produce free-form justifications conditioned on user profiles (Zhang et al., 2021).\n\nIn this paper, we focus on explanation faithfulness and provide a new evaluation dataset.",
    "reason": "The span inventories prior approaches but does not connect them to the authors’ aims or specify why existing explanations are insufficient, thus lacking synthesis (definition a and c).",
    "start": 232,
    "end": 537,
    "label": "Lacks synthesis"
  },
  {
    "span": "There are many recent works that explore self-supervised pretraining for anomaly detection.",
    "document": "Introduction\nTime-series anomaly detection underpins critical monitoring in finance, manufacturing, and healthcare. Traditional methods rely on forecasting residuals or density estimation under stationary assumptions, which often fail under distribution shift (Gao and Sun, 2019; Patel et al., 2021). More recent paradigms leverage representation learning to capture complex temporal dependencies and rare events.\nThere are many recent works that explore self-supervised pretraining for anomaly detection. While these methods show promise, they are frequently tailored to specific domains or require extensive negative sampling strategies. Our work proposes a general pretext task based on masked temporal reconstruction that transfers across domains without task-specific heuristics.\nWe benchmark on five public datasets spanning industrial sensors and ECG signals, demonstrating improved detection at low false-positive rates.",
    "reason": "The phrase \"many recent works\" makes a claim about prior literature without listing or citing any works. According to rule (d) and example (i), mentions of recent works must be backed by citations.",
    "start": 414,
    "end": 505,
    "label": "Unsupported claim"
  },
  {
    "span": "Park et al., 2017)",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) has benefited from attention-based encoder–decoder models and CTC objectives, enabling simplified pipelines with competitive accuracy (Graves et al., 2006; Chan et al., 2016). Advances in self-supervised pretraining have further improved recognition in low-resource regimes (Schneider et al., 2019; Baevski et al., 2020). Data augmentation techniques such as SpecAugment are now standard for improving robustness against acoustic variability (Ko et al., 2015; Park et al., 2019).\n\nDespite these achievements, domain mismatch and speaker variability still degrade performance in out-of-distribution settings (Ghorbani et al., 2021). Prior work on test-time adaptation and confidence-based pseudo-labeling shows promise but often introduces latency overhead (Sun et al., 2020). We explore lightweight adaptation strategies using uncertainty-aware beam rescoring to close this gap, building on ideas introduced by Park et al., 2017) and subsequent augmentation-based training protocols.",
    "reason": "Unmatched closing parenthesis: the citation ends with “)” but lacks the opening “(”. It should be parenthetical \"(Park et al., 2017)\" or narrative \"Park et al. (2017)\".",
    "start": 971,
    "end": 989,
    "label": "Format"
  },
  {
    "span": "In (Perez et al., 2021)",
    "document": "Introduction\n\nTransfer learning has become a standard strategy for adapting pretrained representations to specialized tasks. Domain-adaptive pretraining narrows the gap between source and target distributions (Gomes and Shah, 2020), while task-adaptive objectives can encode label-specific features (Reeves et al., 2022).\n\nIn (Perez et al., 2021), we propose a contrastive finetuning scheme that aligns hard negatives with semantically similar instances. Building on this idea, we incorporate curriculum negatives selected by density-aware sampling (Costa and Liu, 2023) and evaluate under label shift.\n\nOur experiments show consistent improvements across three datasets with minimal additional computation, suggesting that negative selection is a critical factor in transfer efficacy.",
    "reason": "Wrong citation style for narrative usage. The preposition 'In' should be followed by the narrative form Perez et al. (2021), not the parenthetical form (Perez et al., 2021).",
    "start": 323,
    "end": 346,
    "label": "Format"
  },
  {
    "span": "Back-translation augments labeled data with paraphrases generated via machine translation (Sennrich et al., 2016). EDA applies simple operations like synonym replacement and random deletion (Wei and Zou, 2019). Unlabeled examples can be pseudo-labeled to expand the training set (Xie et al., 2020). Curriculum learning orders training instances by difficulty (Bengio et al., 2009).",
    "document": "Related Work: Data Augmentation for Text Classification\n\nData augmentation mitigates overfitting and label sparsity by exposing models to diverse, label-preserving variations. In NLP, augmentation spans from rule-based edits to model-based generation, each trading off controllability and fidelity.\n\nBack-translation augments labeled data with paraphrases generated via machine translation (Sennrich et al., 2016). EDA applies simple operations like synonym replacement and random deletion (Wei and Zou, 2019). Unlabeled examples can be pseudo-labeled to expand the training set (Xie et al., 2020). Curriculum learning orders training instances by difficulty (Bengio et al., 2009).\n\nRecent works leverage pretrained language models to produce semantically constrained paraphrases and label-aware counterfactuals (Kumar et al., 2020; Wu et al., 2021). Others introduce edit-based policies guided by semantic similarity and toxicity filters to preserve label semantics (Chen et al., 2021). We focus on controllable, constraint-driven augmentation that aligns edits with class-discriminative rationales.",
    "reason": "The sentences list four techniques without linking them or explaining their interrelations; the shift to curriculum learning is especially abrupt and unrelated to augmentation, creating poor coherence between cited works (issues a and b).",
    "start": 300,
    "end": 681,
    "label": "Coherence"
  },
  {
    "span": "The SemEval-2020 sarcasm detection shared task highlighted the importance of context windows.",
    "document": "Introduction\n\nSarcasm detection seeks to identify utterances where literal meanings diverge from intended meanings, often requiring pragmatic and contextual cues (Campbell and Katz, 2012; Joshi et al., 2017). With the rise of social media, datasets now include conversational context and user-level features to improve performance (Ghosh and Veale, 2016; Hazarika et al., 2018). The SemEval-2020 sarcasm detection shared task highlighted the importance of context windows. Building on this insight, models have incorporated hierarchical encoders and cross-utterance attention to leverage surrounding dialogue turns.\n\nRelated Work\n\nContext modeling includes leveraging thread structure, speaker history, and sentiment shifts (Matero et al., 2019; Oprea and Magdy, 2020). Pre-trained transformers fine-tuned with conversation-aware objectives further improve disambiguation of figurative language (Zhang et al., 2021; Potamias et al., 2022). Our work introduces a contrastive objective that aligns sarcastic cues across turns while penalizing spurious correlations with user metadata.",
    "reason": "The sentence references a specific shared task and claims a key finding without providing a citation to the task or its report.",
    "start": 379,
    "end": 472,
    "label": "Unsupported claim"
  },
  {
    "span": "Miller et al. 2017)",
    "document": "Related Work\n\nKnowledge graph completion methods include translational distance models and bilinear forms (Zhang and Qian, 2016; Patel et al., 2018). Regularization via relation-specific constraints and adversarial negative sampling improves generalization (Kim and Duarte, 2019).\n\nMiller et al. 2017) demonstrated that shallow multiplicative interactions can match deeper architectures on link prediction benchmarks. Subsequent works add path-based reasoning and rule induction to capture longer dependencies (Lopez and Wang, 2020). Our approach integrates lightweight path encoders with contrastive calibration to stabilize training.\n\nWe also consider inductive settings where new entities appear at test time (Hernandez and Silva, 2021).",
    "reason": "Unbalanced parenthetical citation: missing opening parenthesis before the year; should be 'Miller et al. (2017)'.",
    "start": 282,
    "end": 301,
    "label": "Format"
  },
  {
    "span": "Lopez and Chen",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become the de facto approach for learning over relational data, enabling advances in semi-supervised node classification and link prediction (Kipf and Welling, 2017; Hamilton et al., 2017; Veličković et al., 2018). Despite their success, oversmoothing and oversquashing remain core challenges, limiting the depth and expressivity of message passing architectures (Li et al., 2018; Alon and Yahav, 2021). Regularization and architectural innovations such as residual connections and jumping-knowledge aggregation have partially mitigated these issues (Xu et al., 2018; Chen et al., 2020).\n\nBuilding on these trends, Lopez and Chen argue that constraining the spectral radius of propagation operators yields more stable training under distribution shift, while maintaining competitive accuracy on benchmark datasets such as Cora and OGBN-Arxiv. Our work complements this line by proposing an adaptive propagation depth that is selected per-node during inference, reducing over-smoothing effects without incurring heavy training costs.",
    "reason": "Narrative citation missing year. It should appear as a narrative citation with year, e.g., \"Lopez and Chen (2021)\" (or the appropriate year).",
    "start": 663,
    "end": 677,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on essays from the ASAP dataset with rubric-specific heads.",
    "document": "Related Work in Automatic Essay Scoring\n\nAutomatic essay scoring (AES) has evolved from hand-engineered features and regression models to neural architectures that learn holistic representations of writing quality (Shermis and Burstein, 2013; Taghipour and Ng, 2016). More recently, pretrained language models have delivered strong gains by capturing syntactic and discourse cues implicitly (Ushio et al., 2022).\n\nBERT was used in an AES task trained on essays from the ASAP dataset with rubric-specific heads. Other studies explore pairwise ranking and ordinal regression to better model adjacent score levels, and domain adaptation methods to mitigate prompt-specific overfitting. However, the literature remains split on how much prompt leakage influences performance, motivating our cross-prompt evaluation design.",
    "reason": "Mentions a specific model–dataset setup ('BERT' on 'ASAP' with a particular head design) without citing the prior work where this was introduced.",
    "start": 414,
    "end": 510,
    "label": "Unsupported claim"
  },
  {
    "span": "(Miller et al., 2020,; Chen, 2019)",
    "document": "Introduction\n\nGraph neural networks (GNNs) have achieved state-of-the-art results on node and graph classification tasks (Kipf and Welling, 2017; Hamilton et al., 2017). Multiple studies (Miller et al., 2020,; Chen, 2019) report that naive splitting protocols can inadvertently leak label information across partitions. Consequently, standardized benchmarks and splits have been proposed to ensure fair comparison (Shchur et al., 2018; Hu et al., 2020).\n\nFurther advances include scalable sampling methods for large graphs and improved message passing schemes (Chiang et al., 2019; Xu et al., 2019). Regularization and calibration strategies have also been explored to enhance generalization under distribution shift (Zhou et al., 2020; Sun et al., 2020).",
    "reason": "Malformed multi-citation punctuation: extra comma before the semicolon. Should be '(Miller et al., 2020; Chen, 2019)'.",
    "start": 187,
    "end": 221,
    "label": "Format"
  },
  {
    "span": "Classical multi-robot path planning methods use sampling-based planners and prioritized decoupling (LaValle, 2006; Erdmann and Lozano-Pérez, 1987; Silver, 2005). Learning-based approaches train policies via reinforcement learning or imitation to resolve conflicts (Chen et al., 2017; Everett et al., 2018; Li et al., 2020). We present a hybrid planner that blends search with neural predictions for large teams.",
    "document": "Related Work\n\nMulti-Robot Path Planning\nClassical multi-robot path planning methods use sampling-based planners and prioritized decoupling (LaValle, 2006; Erdmann and Lozano-Pérez, 1987; Silver, 2005). Learning-based approaches train policies via reinforcement learning or imitation to resolve conflicts (Chen et al., 2017; Everett et al., 2018; Li et al., 2020). We present a hybrid planner that blends search with neural predictions for large teams.\n\nCoordination Under Constraints\nCoordination under communication and kinematic constraints has also been studied using graph-based and market-based formulations (Bullo et al., 2009; Gerkey and Mataric, 2004).",
    "reason": "Presents prior work and immediately states the contribution without clarifying what is missing in the literature or why a hybrid planner is needed (definition b).",
    "start": 40,
    "end": 451,
    "label": "Lacks synthesis"
  },
  {
    "span": "Prior studies report that U-Net variants account for over 80% of submissions to ischemic stroke segmentation challenges.",
    "document": "Introduction\n\nAutomated segmentation of ischemic stroke lesions in CT perfusion is a critical step toward timely clinical decision-making. Convolutional encoder–decoder architectures, especially the U-Net family, remain the dominant backbone for medical image segmentation due to their strong inductive biases and skip connections (Ronneberger et al., 2015; Çiçek et al., 2016). Domain adaptation and uncertainty estimation have been studied to improve cross-scanner generalization (Kamnitsas et al., 2017; Wang et al., 2019).\n\nDespite rapid progress, algorithmic performance can degrade substantially across institutions and acquisition protocols, motivating standardized evaluations in community challenges such as ISLES. Prior studies report that U-Net variants account for over 80% of submissions to ischemic stroke segmentation challenges. However, few works systematically evaluate how architectural depth and multi-scale context interact with perfusion-derived features under strict external validation.\n\nWe present a study quantifying the trade-offs between model capacity and generalization for stroke lesion segmentation across three multi-center datasets.",
    "reason": "Presents a quantitative claim about prior work participation rates in challenges without any supporting citation.",
    "start": 724,
    "end": 844,
    "label": "Unsupported claim"
  },
  {
    "span": "Transformers have been adopted for time-series anomaly detection with self-attention over temporal windows or patches (Zhou et al., 2021; Liu et al., 2022; Xu et al., 2022).",
    "document": "Related Work\n\nTime-series anomaly detection. Traditional detectors rely on forecasting residuals or density estimation under parametric assumptions (Fox, 1972; Chandola et al., 2009). Deep models leverage autoencoders and recurrent networks to capture nonlinear dynamics (Malhotra et al., 2015; Audibert et al., 2020). Transformers have been adopted for time-series anomaly detection with self-attention over temporal windows or patches (Zhou et al., 2021; Liu et al., 2022; Xu et al., 2022). Recent efforts address scalability via sparse attention and memory compression (Kitaev et al., 2020; Tay et al., 2020).\n\nWe target non-stationary streams with regime shifts and propose a detector that conditions attention on change-point-aware embeddings learned in a self-supervised manner.",
    "reason": "The span mentions the adoption of transformers without connecting those methods to the specific non-stationarity challenge or to the proposed conditioning mechanism, hence lacking synthesis (criteria a and c).",
    "start": 319,
    "end": 492,
    "label": "Lacks synthesis"
  },
  {
    "span": "Most smartphones participate in fewer than three rounds per day.",
    "document": "Introduction\n\nFederated learning (FL) enables on-device training without centralized raw data collection, offering privacy and scalability benefits for mobile applications (McMahan et al., 2017; Kairouz et al., 2021). Practical deployments must handle partial participation, non-IID data, and intermittent connectivity (Bonawitz et al., 2019; Pillutla et al., 2022). Most smartphones participate in fewer than three rounds per day. This low participation rate motivates algorithms that are robust to stale updates and device heterogeneity. We propose StrataFed, which adaptively schedules clients using stratified sampling over participation histories, improving convergence under strict availability constraints.\n\nRelated Work\n\nServer-side optimizers and client adaptation have improved stability under heterogeneity (Karimireddy et al., 2020; Reddi et al., 2021). Participation-aware scheduling has been explored via importance sampling and fairness constraints (Nishio and Yonetani, 2019; Cho et al., 2020). Our work complements these approaches with a history-aware, variance-reducing selector.",
    "reason": "Presents a specific statistic about real-world device participation without a citation or empirical evidence.",
    "start": 367,
    "end": 431,
    "label": "Unsupported claim"
  },
  {
    "span": "It has been repeatedly shown that WER fails to capture semantic fidelity for conversational ASR.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) quality is traditionally reported using word error rate (WER), which aggregates substitutions, insertions, and deletions. However, conversational settings involve paraphrase, hesitations, and discourse phenomena that challenge purely string-based metrics.\n\nIt has been repeatedly shown that WER fails to capture semantic fidelity for conversational ASR. Alternatives that rely on meaning representation or downstream task performance have been proposed to bridge this gap but remain underutilized in practice.\n\nWe propose a semantic robustness evaluation suite for ASR that aligns recognition outputs with intent and slot recovery, providing a more actionable signal for conversational systems.",
    "reason": "The claim appeals to unspecified repeated findings about WER’s inadequacy without citing any studies, making it an unsupported claim about prior work.",
    "start": 306,
    "end": 402,
    "label": "Unsupported claim"
  },
  {
    "span": "Many architectures have been proposed, including DCRNN, STGCN, Graph WaveNet, GMAN, STGODE, MTGNN, and AGCRN (Li et al., 2018; Yu et al., 2018; Wu et al., 2019; Zhang et al., 2020; Fang et al., 2021; Wu et al., 2020; Bai et al., 2020).",
    "document": "Related Work\n\nTraffic forecasting. Classical approaches rely on statistical models such as ARIMA and VAR that assume linear dependencies and stationarity (Williams and Hoel, 2003; Min and Wynter, 2011). With the advent of deep learning, recurrent and convolutional architectures have been applied to capture temporal patterns and local spatial correlations (Ma et al., 2015; Yu et al., 2017). Many architectures have been proposed, including DCRNN, STGCN, Graph WaveNet, GMAN, STGODE, MTGNN, and AGCRN (Li et al., 2018; Yu et al., 2018; Wu et al., 2019; Zhang et al., 2020; Fang et al., 2021; Wu et al., 2020; Bai et al., 2020). These methods differ in how they encode graph structure, handle multi-step prediction, and incorporate exogenous signals such as weather or events.\n\nSpatio-temporal graphs. Learning over dynamic graphs has been explored with attention mechanisms, diffusion processes, and continuous-time operators (Velickovic et al., 2018; Atwood and Towsley, 2016; Chang et al., 2020). Representation learning on sensor networks often leverages predefined road topology, while some works attempt to learn latent connectivity from data (Pan et al., 2019; Jiang et al., 2021). \n\nOur work studies how to robustly forecast under abrupt distribution shifts without access to labels during deployment. We focus on test-time adaptation strategies that adjust model components on-the-fly using self-supervised objectives computed from incoming streams.",
    "reason": "The sentence lists prior GNN architectures with citations but does not explain their relevance to the authors' problem or how the proposed approach relates, thereby lacking synthesis (criteria a and c).",
    "start": 393,
    "end": 628,
    "label": "Lacks synthesis"
  },
  {
    "span": "Most face recognition datasets are biased toward lighter skin tones, which skews evaluation results.",
    "document": "Introduction\n\nFace recognition systems are widely deployed in consumer devices and public safety applications. Alongside accuracy improvements, there is a growing emphasis on fairness and robustness across demographic groups to prevent disparate impacts.\n\nMost face recognition datasets are biased toward lighter skin tones, which skews evaluation results. This observation motivates dataset curation and model calibration strategies designed to reduce demographic performance gaps in real-world scenarios.\n",
    "reason": "Makes a demographic bias claim about datasets without providing citations or empirical evidence.",
    "start": 256,
    "end": 356,
    "label": "Unsupported claim"
  },
  {
    "span": "(Rao, 2018 and Lin et al., 2020)",
    "document": "Introduction\n\nEnd-to-end parsing frameworks increasingly rely on pretraining and structured decoding (Dozat and Manning, 2017; Zhou and Zhao, 2019). Several systems (Rao, 2018 and Lin et al., 2020) report competitive results on multilingual benchmarks, but they diverge in training curricula and data curation (Bouma et al., 2020; de Lhoneux et al., 2018).\n\nOur approach standardizes curriculum and introduces cross-tree constraints to stabilize training.",
    "reason": "Improper conjunction 'and' inside a parenthetical multi-citation; standard style uses a semicolon to separate citations, e.g., '(Rao, 2018; Lin et al., 2020)'.",
    "start": 165,
    "end": 197,
    "label": "Format"
  },
  {
    "span": "BERT-style masked prediction has been used in 3D U-Net pretraining for tumor segmentation.",
    "document": "Introduction\n\nSelf-supervised learning has become a powerful paradigm for medical image analysis, reducing annotation demands in segmentation and detection (Tajbakhsh et al., 2020; Azizi et al., 2021). Masked image modeling extends language-modeling principles to vision by predicting missing patches or tokens (He et al., 2022; Bao et al., 2022). BERT-style masked prediction has been used in 3D U-Net pretraining for tumor segmentation. However, volumetric medical scans pose unique challenges due to anisotropy, low contrast, and domain-specific anatomical priors. We introduce VolMAE-UNet, a tokenized 3D masked autoencoder with anatomy-aware masking and show improved transfer on brain and abdominal segmentation benchmarks.\n\nRelated Work\n\nPretext tasks for 3D data include context restoration, rotation prediction, and contrastive instance discrimination (Zhuang et al., 2019; Taleb et al., 2020). Recent works adapt masked autoencoding to volumetric CT and MRI, but design choices for tokenization and decoders remain open (Tang et al., 2022).",
    "reason": "Claims a specific prior application (masked prediction used for 3D U-Net pretraining) without citing any study demonstrating it.",
    "start": 348,
    "end": 438,
    "label": "Unsupported claim"
  },
  {
    "span": "Rehearsal-based continual learning stores a subset of past samples and interleaves them during training (Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2019). Knowledge distillation transfers information from old models to new ones (Hinton et al., 2015; Li and Hoiem, 2017). Reinforcement learning has been used to adapt agents to non-stationary reward landscapes (Kirkpatrick et al., 2017; Rolnick et al., 2019).",
    "document": "Related Work\n\nContinual learning aims to train models on a sequence of tasks without catastrophic forgetting. Vision transformers have renewed interest in this setting due to their modularity and capacity, yet they remain vulnerable to representational drift. Strategies to preserve important parameters or recapitalize past data are extensively studied across supervised and reinforcement learning.\n\nRehearsal-based continual learning stores a subset of past samples and interleaves them during training (Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2019). Knowledge distillation transfers information from old models to new ones (Hinton et al., 2015; Li and Hoiem, 2017). Reinforcement learning has been used to adapt agents to non-stationary reward landscapes (Kirkpatrick et al., 2017; Rolnick et al., 2019).\n\nParameter isolation and regularization methods constrain updates to limit interference. Elastic penalties, orthogonal gradients, and sparsity-inducing masks are common mechanisms. In transformers, adapter modules provide a way to attach task-specific capacities while reusing a shared backbone.\n\nWe focus on token-level rehearsal and attention map anchoring to stabilize representations across tasks. Our approach complements memory-based rehearsal with structure-aware constraints in self-attention, achieving improved accuracy under tight memory budgets.",
    "reason": "The span jumps among rehearsal, distillation, and reinforcement learning without clarifying their relationships or providing transitions, leaving the connection between the cited works implicit and unclear.",
    "start": 401,
    "end": 813,
    "label": "Coherence"
  },
  {
    "span": "Garcia et al. 1",
    "document": "Related Work\n\nConversational interfaces for behavior change have adopted modular pipelines with NLU, dialog management, and NLG components (Gao et al., 2019; Laranjo et al., 2018). Prior work Garcia et al. 1 explored tailoring motivational messages using user-stage classifications, but offered limited evaluation on long-term adherence. More recent systems incorporate reinforcement learning to personalize action suggestions over time (Takanashi et al., 2021). Our framework unifies user modeling and planning in a single policy network trained from interaction logs.",
    "reason": "Improper use of a footnote-like marker without a proper citation format. It should include the year as a standard citation (e.g., \"Garcia et al. (2019)\") or be formatted as a proper footnote.",
    "start": 192,
    "end": 207,
    "label": "Format"
  },
  {
    "span": "Message passing neural networks aggregate neighbor features to predict molecular properties (Gilmer et al., 2017). Vision transformers learn global interactions through self-attention (Dosovitskiy et al., 2021). Contrastive pretraining creates invariant graph embeddings (You et al., 2020).",
    "document": "Related Work\n\nPredicting molecular properties from graphs has been extensively studied with graph neural networks. Message passing neural networks aggregate neighbor features to predict molecular properties (Gilmer et al., 2017). Vision transformers learn global interactions through self-attention (Dosovitskiy et al., 2021). Contrastive pretraining creates invariant graph embeddings (You et al., 2020). Multi-task learning and transfer across assays have also been explored (Yang et al., 2019). Our method combines long-range attention with scaffold-aware pretraining for better out-of-distribution generalization.",
    "reason": "The span abruptly shifts from molecular GNNs to vision transformers and then to contrastive pretraining without stating how these works relate or connect; the relationship among the cited works is not made explicit.",
    "start": 115,
    "end": 405,
    "label": "Coherence"
  },
  {
    "span": "U-Net introduced an encoder–decoder with skip connections for biomedical segmentation (Ronneberger et al., 2015). 3D variants extend convolutional kernels to volumetric scans (Çiçek et al., 2016). Self-supervised pretext tasks improve representations from unlabeled images (Misra and Maaten, 2020). Radiomics extracts handcrafted features for prognosis (Aerts et al., 2014).",
    "document": "Related Work: Medical Image Segmentation and Representation Learning\n\nAccurate medical image segmentation underpins diagnosis and treatment planning. Methods range from fully supervised architectures to semi/self-supervised techniques that leverage limited labels and domain priors.\n\nU-Net introduced an encoder–decoder with skip connections for biomedical segmentation (Ronneberger et al., 2015). 3D variants extend convolutional kernels to volumetric scans (Çiçek et al., 2016). Self-supervised pretext tasks improve representations from unlabeled images (Misra and Maaten, 2020). Radiomics extracts handcrafted features for prognosis (Aerts et al., 2014).\n\nRecent work explores weak supervision from scribbles and points to reduce annotation burden (Lin et al., 2016). Others integrate shape priors and uncertainty estimation to improve calibration in clinical settings (Kohl et al., 2018). Our approach unifies self-supervised pretraining with label-efficient fine-tuning via shape-aware consistency.",
    "reason": "The span mixes segmentation architectures, self-supervised learning, and radiomics without explaining how these areas relate; sentences lack transitions and the connection between cited works is abrupt (issues a and b).",
    "start": 284,
    "end": 658,
    "label": "Coherence"
  },
  {
    "span": "There have been many recent works demonstrating that large language models can reliably evaluate summaries without human oversight.",
    "document": "Introduction\n\nAutomatic evaluation of summarization systems has traditionally relied on lexical overlap metrics such as ROUGE (Lin, 2004), with later methods introducing semantic matching via embeddings (Zhang et al., 2020) and question-answering based approaches (Eyal et al., 2019). More recently, researchers have explored prompting large language models (LLMs) to act as judges of summary quality, reporting promising correlations with human ratings in limited settings (Khandelwal and Jurafsky, 2023; Wang et al., 2023). There have been many recent works demonstrating that large language models can reliably evaluate summaries without human oversight. However, the validity of LLM-as-judge protocols depends on careful prompt design, calibration, and domain coverage, which remain open issues (Bowman, 2023).\n\nRelated Work\n\nEfforts to reduce evaluator bias include rubric-based prompts and pairwise preference protocols (Liu et al., 2022), while others probe LLM sensitivity to factuality and hallucinations (Maynez et al., 2020; Goyal and Durrett, 2021). Beyond correlations, some studies investigate agreement calibration and measurement error decomposition (Deutsch et al., 2022). Our work contributes a standardized protocol that couples rubric-grounded prompts with uncertainty estimation, enabling reproducible comparisons across models and datasets.",
    "reason": "This sentence claims the existence of many recent works and a substantive result without providing any citations, violating rule (d) about 'recent works' needing references.",
    "start": 526,
    "end": 657,
    "label": "Unsupported claim"
  },
  {
    "span": "Several recent works have addressed unsupervised domain adaptation for low-resource ASR by leveraging pseudo-labels and consistency training.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) systems degrade substantially when deployed in domains that differ from their training distributions. This performance drop is exacerbated in low-resource settings where labeled audio is scarce and collecting transcriptions is costly. Domain adaptation aims to bridge this gap by transferring knowledge from a source domain with labels to a target domain with limited or no labels. Several recent works have addressed unsupervised domain adaptation for low-resource ASR by leveraging pseudo-labels and consistency training. However, the effectiveness of these approaches depends on the reliability of pseudo-labels generated by the source model, which can be brittle under accent and noise variations. In this paper, we propose a target-aware confidence calibration strategy that improves the quality of pseudo-labels and stabilizes training across shifts in acoustic conditions.\n\nRelated Work\n\nPrior adaptation strategies for ASR include feature space normalization, speaker adaptation, and semi-supervised training. While self-training has emerged as a practical avenue, it often suffers from confirmation bias. Consistency regularization mitigates this issue by enforcing stability under augmentations, but selecting augmentations that reflect target-domain variability remains an open challenge. Our method complements these approaches by explicitly calibrating confidence on target-domain audio before pseudo-labeling.",
    "reason": "Mentions 'recent works' and summarizes their approach without any citations to support the claim (criterion d).",
    "start": 431,
    "end": 572,
    "label": "Unsupported claim"
  },
  {
    "span": "GAT quickly became the state-of-the-art on citation network benchmarks such as Cora and Citeseer.",
    "document": "Graph Neural Networks: Prior Work\n\nGraph convolutional methods have emerged as a dominant paradigm for learning on relational data. Early models adapted spectral graph convolutions to semi-supervised node classification, while neighborhood sampling methods improved scalability on large graphs. GAT quickly became the state-of-the-art on citation network benchmarks such as Cora and Citeseer. Attention-based neighborhood aggregation subsequently inspired extensions to heterogeneous graphs, dynamic graphs, and link prediction settings.\n\nDespite these advances, robustness to structural perturbations and label noise remains an open problem. Recent architectures incorporate uncertainty estimates and adversarial training to mitigate oversmoothing and oversquashing effects encountered in deep stacks.\n\nOur contribution builds on these insights with a residual-attention framework that preserves high-frequency components while stabilizing training through layer-wise normalization. We evaluate on transductive and inductive splits, measuring accuracy, calibration, and robustness under random edge deletions.",
    "reason": "The sentence makes a state-of-the-art claim about a prior model (GAT) and specific datasets (Cora, Citeseer) without providing citations or evidence, violating rule (a) and (b).",
    "start": 295,
    "end": 392,
    "label": "Unsupported claim"
  },
  {
    "span": "It is widely known that transformer models underperform on long-horizon forecasts compared to simple linear baselines.",
    "document": "Related Work\n\nTime Series Forecasting. Classical statistical models such as ARIMA and ETS remain competitive for short horizons and stationary series (Box and Jenkins, 1970; Hyndman and Athanasopoulos, 2018). Neural approaches including RNNs, Temporal Convolutional Networks, and attention-based models improve performance by capturing nonlinear dependencies (Bai et al., 2018; Salinas et al., 2020). It is widely known that transformer models underperform on long-horizon forecasts compared to simple linear baselines. Recent architectures address efficiency and inductive bias through sparse attention, frequency filtering, and decomposition (Zhou et al., 2021; Wu et al., 2021). Our contribution is a decomposition-aware transformer with slope-regularized heads tailored for multi-scale seasonality.",
    "reason": "Asserts a community-wide belief about comparative performance without providing citations to studies or benchmarks.",
    "start": 401,
    "end": 519,
    "label": "Unsupported claim"
  },
  {
    "span": "Classical IoT routing protocols emphasize energy-aware paths using residual energy or duty cycles (Heinzelman et al., 2000; Ye et al., 2002; Al-Karaki and Kamal, 2004). Other approaches apply geographic or clustering strategies to prolong network lifetime (Karp and Kung, 2000; Lindsey and Raghavendra, 2002; Younis and Fahmy, 2004).",
    "document": "Introduction\n\nLow-power Internet of Things deployments demand routing strategies that extend lifetime while maintaining reliable delivery over lossy links. Battery constraints and topology changes complicate traditional shortest-path formulations and encourage cross-layer designs.\n\nClassical IoT routing protocols emphasize energy-aware paths using residual energy or duty cycles (Heinzelman et al., 2000; Ye et al., 2002; Al-Karaki and Kamal, 2004). Other approaches apply geographic or clustering strategies to prolong network lifetime (Karp and Kung, 2000; Lindsey and Raghavendra, 2002; Younis and Fahmy, 2004). Reinforcement learning methods have also been introduced to adaptively select routes (Zhang et al., 2019; Anisi et al., 2020).\n\nWe propose a lightweight routing heuristic with probabilistic link probing and queue-aware forwarding. We evaluate it on testbeds and simulations under variable duty-cycling.",
    "reason": "The span lists categories of prior protocols but does not explain how they differ from or inform the proposed heuristic, nor does it highlight a specific gap, satisfying criteria (a) and (b).",
    "start": 283,
    "end": 616,
    "label": "Lacks synthesis"
  },
  {
    "span": "Huang et al.",
    "document": "Introduction\n\nDomain adaptation for text classification has progressed from instance reweighting to representation learning (Ben-David et al., 2010; Ganin et al., 2016). Recent adversarial methods learn domain-invariant features but often suppress task-relevant cues (Zhao et al., 2019; Li and Pan, 2021). Huang et al. demonstrate that aligning marginal distributions can harm conditional alignment; to address this, they propose class-conditional discriminators and contrastive objectives. Complementary approaches incorporate pseudo-label refinement (Saito et al., 2018) and self-training with confidence calibration (Chen and He, 2020). We build on these insights by decoupling domain and label information through causal interventions.",
    "reason": "Narrative citation is missing the publication year; it should appear as 'Huang et al. (YYYY)'.",
    "start": 306,
    "end": 318,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claimed that curriculum learning always improves few-shot performance.",
    "document": "Related Work\n\nFew-shot learning. Meta-learning algorithms learn to adapt rapidly from limited examples, using episodic training and task-conditioned initialization (Finn et al., 2017; Snell et al., 2017; Nichol et al., 2018). Curriculum learning organizes tasks from easy to hard to stabilize training and improve generalization (Bengio et al., 2009; Platanios et al., 2019). In a previous study, the authors claimed that curriculum learning always improves few-shot performance. However, the effectiveness of curricula often depends on task difficulty modeling and the alignment between training and evaluation distributions (Hacohen and Weinshall, 2019; Soviany et al., 2022).\n",
    "reason": "References a 'previous study' and attributes a claim to it without citing the study; per (ii) such mentions require a citation.",
    "start": 376,
    "end": 479,
    "label": "Unsupported claim"
  },
  {
    "span": "To our knowledge, we are the first to use proxy labels for deconfounding in this setting.",
    "document": "Related Work\n\nCausal Inference with Text Features. Text as confounders or mediators has been studied via propensity models and representation learning (Roberts et al., 2020; Keith et al., 2020). Recent approaches use deep proxies to adjust for selection bias (Veitch et al., 2020). To our knowledge, we are the first to use proxy labels for deconfounding in this setting. Our method learns a surrogate objective that aligns textual signals with unobserved confounders.",
    "reason": "Claims novelty relative to prior work without providing supporting citations or a survey to justify the claim (violates rule b).",
    "start": 282,
    "end": 371,
    "label": "Unsupported claim"
  },
  {
    "span": "Most prior approaches rely on attention mechanisms to propagate user-item signals.",
    "document": "Related Work\n\nGraph-based recommender systems exploit user-item interaction structures to capture collaborative signals beyond traditional matrix factorization. Message passing architectures have proven effective for modeling high-order connectivity. Most prior approaches rely on attention mechanisms to propagate user-item signals. Despite their flexibility, attention layers can be computationally expensive and introduce instability under sparse feedback.\n\nOur method replaces attention with spectral filters that are calibrated to user homophily levels, enabling scalable propagation with theoretical guarantees on smoothing and over-squashing.",
    "reason": "General claim about what 'most prior approaches' do is not supported with citations to representative works.",
    "start": 251,
    "end": 333,
    "label": "Unsupported claim"
  },
  {
    "span": "Content-based detectors use linguistic features and neural encoders (Rashkin et al., 2017; Zhou and Zafarani, 2019), while propagation-based methods exploit social graphs and user behavior (Shu et al., 2019; Bian et al., 2020).",
    "document": "Related Work\n\nDetecting misinformation on social media involves modeling textual signals and the dynamics of diffusion across user networks. A key challenge is generalizing to evolving narratives and adversarial manipulation.\n\nContent-based detectors use linguistic features and neural encoders (Rashkin et al., 2017; Zhou and Zafarani, 2019), while propagation-based methods exploit social graphs and user behavior (Shu et al., 2019; Bian et al., 2020). Hybrid approaches combine both types of evidence with multi-view learning (Jin et al., 2020; Wang et al., 2021).\n\nWe develop a detector that leverages time-aware user embeddings and contrastive pretraining on retrieval-augmented evidence.",
    "reason": "The span summarizes two lines of work without connecting them to the paper's aims or specifying limitations motivating the proposed detector, satisfying (a) and (c).",
    "start": 227,
    "end": 454,
    "label": "Lacks synthesis"
  },
  {
    "span": "BERT-style encoders were first applied to code search using dual encoders trained with in-batch negatives.",
    "document": "Introduction\n\nCode search aims to retrieve relevant code snippets given natural language queries. Early systems relied on lexical matching or API tagging, but neural approaches now dominate, learning joint embeddings of queries and code. BERT-style encoders were first applied to code search using dual encoders trained with in-batch negatives. Despite their scalability, such models may underutilize structure present in code, such as syntax trees and control flow. We present a structure-aware retriever that fuses token-level representations with lightweight graph features, improving retrieval without sacrificing latency.\n\nRelated Work\n\nSubsequent work explored cross-encoders for reranking and contrastive pretraining on large code–text corpora. Our method complements these by integrating structural biases in the first-stage retriever.",
    "reason": "Attributes a 'first applied' prior contribution without any citation to the original work (criterion b/ii).",
    "start": 238,
    "end": 344,
    "label": "Unsupported claim"
  },
  {
    "span": "the METR-LA dataset is the de facto benchmark for traffic forecasting",
    "document": "Related Work\n\nSpatiotemporal forecasting models traffic dynamics by capturing spatial dependencies among sensors and temporal patterns in flow and speed. Graph neural networks (GNNs) and attention mechanisms have become standard for learning from sensor networks, outperforming traditional autoregressive baselines in multi-step prediction.\n\nIn empirical evaluations, the METR-LA dataset is the de facto benchmark for traffic forecasting, alongside regional datasets with varying sensor coverage. However, these benchmarks often underrepresent distribution shifts caused by road work, events, or sensor outages, limiting their reflection of real-world deployment.\n\nWe advocate for robustness-oriented benchmarks and introduce perturbation suites that simulate realistic disruptions during evaluation.",
    "reason": "The statement names a specific dataset and claims it is the 'de facto benchmark' without citing any surveys or benchmark papers to support this assertion.",
    "start": 368,
    "end": 437,
    "label": "Unsupported claim"
  },
  {
    "span": "SemEval 2020 Task on Offensive Language",
    "document": "Related Work\n\nOffensive language detection has progressed rapidly with the introduction of multilingual corpora and shared benchmarks. The SemEval 2020 Task on Offensive Language provided a shared benchmark that catalyzed cross-lingual evaluations and error taxonomies. Later datasets expanded coverage to code-switched text and context-dependent labeling. We target a pragmatic setting where limited in-domain labels are available and propose a weak-to-strong supervision pipeline that transfers knowledge from noisy web labels to curated platform-specific guidelines.\n",
    "reason": "A specific shared task is mentioned at first occurrence without citing the corresponding task description paper.",
    "start": 139,
    "end": 178,
    "label": "Unsupported claim"
  },
  {
    "span": "On Atari, state-of-the-art methods require billions of frames.",
    "document": "Introduction\n\nDeep reinforcement learning has achieved impressive results on visually rich control tasks, yet sample efficiency remains a central challenge (Mnih et al., 2015; Hessel et al., 2018). Off-policy algorithms and model-based approaches have been proposed to reduce interaction cost while maintaining strong performance (Sutton and Barto, 2018; Kaiser et al., 2019).\n\nOn Atari, state-of-the-art methods require billions of frames. This motivates exploration of representation learning and prioritized exploration strategies that extract more value from each transition.",
    "reason": "This is a quantitative claim about prior results without evidence or citation, making it unsupported per the definition.",
    "start": 378,
    "end": 440,
    "label": "Unsupported claim"
  },
  {
    "span": "Katz and Stentz (2016) employ D*-Lite variants for dynamic replanning. Kaelbling and Lozano-Pérez (2011) formalize TAMP for long-horizon tasks. Levine et al. (2016) use guided policy search for manipulation. Kalashnikov et al. (2018) train QT-Opt for grasping at scale.",
    "document": "Related Work\n\nTask and motion planning (TAMP) and learning-based control each address complementary aspects of long-horizon robotic autonomy: symbolic task abstraction and continuous control under uncertainty. Bridging them requires representations that can support both discrete and continuous reasoning.\n\nKatz and Stentz (2016) employ D*-Lite variants for dynamic replanning. Kaelbling and Lozano-Pérez (2011) formalize TAMP for long-horizon tasks. Levine et al. (2016) use guided policy search for manipulation. Kalashnikov et al. (2018) train QT-Opt for grasping at scale.\n\nWe propose a hybrid planner that uses learned affordances to prune the TAMP search space, connecting symbolic operators with data-driven feasibility predictors.",
    "reason": "This span lists four works from different subareas in rapid succession without clarifying their relationships or transitions (e.g., how planning and learning-based methods complement or contrast). The linkage is not explicit, and the coherence issue spans multiple sentences (criterion a, b, c).",
    "start": 307,
    "end": 576,
    "label": "Coherence"
  },
  {
    "span": "The COVID-QA dataset contains 10k question–answer pairs derived from scientific abstracts.",
    "document": "Introduction\n\nRapidly evolving scientific domains require QA systems that adapt to new terminology and shifting evidence. COVID-19 posed a unique challenge by producing large volumes of heterogeneous publications within a short timeframe.\n\nThe COVID-QA dataset contains 10k question–answer pairs derived from scientific abstracts. In this work, we investigate whether contrastive pretraining on citation graphs improves retrieval sensitivity to temporally adjacent publications.\n\nWe release code and evaluation scripts to facilitate reproducible experiments on time-aware scientific QA.",
    "reason": "First mention of a dataset with specific statistics and provenance lacks a citation (violates rule a and b).",
    "start": 240,
    "end": 330,
    "label": "Unsupported claim"
  },
  {
    "span": "earlier shared tasks on event coreference established the mention-pair paradigm",
    "document": "Related Work\n\nEvent coreference resolution links textual mentions that refer to the same event instance. Various model families exist, including mention-pair classifiers, entity-centric clustering, and neural span-ranking. Earlier shared tasks on event coreference established the mention-pair paradigm, shaping subsequent benchmark design and evaluation metrics.\n\nRecent methods incorporate document-level context and external knowledge, while others leverage contrastive objectives to improve cluster separability. Despite improvements, cross-domain generalization remains limited. We build upon span-level encoders and introduce a cluster-aware loss that balances precision and recall in low-resource settings.",
    "reason": "Refers to 'earlier shared tasks' and credits them with establishing a paradigm without citing the relevant shared tasks or proceedings.",
    "start": -1,
    "end": -1,
    "label": "Unsupported claim"
  },
  {
    "span": "(Hamilton et al., 2017",
    "document": "Introduction\n\nGraph neural networks generalize convolution to non-Euclidean domains by aggregating neighborhood features (Bruna et al., 2014; Kipf and Welling, 2017). Sampling-based methods scale to large graphs by restricting receptive fields (Chen et al., 2018) and improve inductive generalization (Hamilton et al., 2017). Despite their success, most architectures assume homophily, which limits performance on heterophilous graphs (Zhu et al., 2020). We propose an adaptive message-passing scheme that modulates aggregation paths conditioned on edge semantics and task objectives.",
    "reason": "Missing closing parenthesis in a parenthetical citation: \"(Hamilton et al., 2017\" lacks the closing \")\". It should be \"(Hamilton et al., 2017)\".",
    "start": 301,
    "end": 323,
    "label": "Format"
  },
  {
    "span": "Most recent works adopt self-supervised contrastive learning for recommendation under sparse feedback.",
    "document": "Related Work\n\nNeural Recommenders with Sparse Feedback. Matrix factorization and item2vec laid the foundation for collaborative filtering (Koren et al., 2009; Barkan and Koenigstein, 2016). Neural architectures exploit sequence signals via attention and transformers to model user intent drift (Kang and McAuley, 2018; Sun et al., 2019). Most recent works adopt self-supervised contrastive learning for recommendation under sparse feedback. Side-information and graph structures further enhance representations when interactions are scarce (Wang et al., 2019; He et al., 2020). We contribute a debiased contrastive objective that reduces popularity shortcuts while preserving the benefits of sequence augmentation.",
    "reason": "Asserts a trend in 'recent works' without providing citations to support the claim.",
    "start": 338,
    "end": 440,
    "label": "Unsupported claim"
  },
  {
    "span": "Most recent works focus on post-processing debiasing methods for top-N recommenders",
    "document": "Related Work\n\nFairness in recommender systems has been studied across user, item, and provider perspectives, with interventions at pre-, in-, and post-processing stages (Ekstrand et al., 2022). Objectives range from exposure parity to calibrated relevance, often trading off accuracy and fairness.\n\nMost recent works focus on post-processing debiasing methods for top-N recommenders, adjusting exposure or re-ranking lists to meet fairness constraints. In contrast, our approach integrates fairness regularization directly into the candidate generation stage, enabling upstream control over exposure and relevance.",
    "reason": "The phrase 'recent works' requires citations to representative papers supporting the claim (rule d).",
    "start": 299,
    "end": 382,
    "label": "Unsupported claim"
  },
  {
    "span": "[Nguyen, 2019)",
    "document": "Related Work\n\nExploration strategies for robotic control leverage intrinsic motivation (Pathak et al., 2017), parameter space noise (Plappert et al., 2018), and uncertainty-driven objectives (Osband et al., 2016). For a comprehensive survey, see [Nguyen, 2019) and recent benchmarks on sparse-reward tasks (Andrychowicz et al., 2017). Despite progress, sim-to-real gaps persist due to dynamics mismatch and sensor noise (Chebotar et al., 2019).\n\nWe address these issues by combining real-to-sim adaptation with policy distillation, thereby reducing variance during finetuning on hardware.",
    "reason": "Mismatched brackets in a citation. The citation starts with '[' and ends with ')' as '[Nguyen, 2019)'; it should consistently use either parentheses '(Nguyen, 2019)' or brackets '[Nguyen, 2019]'.",
    "start": 246,
    "end": 260,
    "label": "Format"
  },
  {
    "span": "Transformer-based models have achieved strong results in long-horizon forecasting (Zhou et al., 2021). Probabilistic forecasting captures predictive uncertainty through quantiles or distributions (Salinas et al., 2020). Change-point detection identifies regime shifts affecting model performance (Truong et al., 2020). Calendar effects are incorporated with learned embeddings (Smyl, 2020).",
    "document": "Related Work\n\nTime series forecasting methods increasingly adopt deep sequence models that combine flexible nonlinear dynamics with probabilistic outputs. Practical deployments must also handle distribution shifts and exogenous drivers.\n\nTransformer-based models have achieved strong results in long-horizon forecasting (Zhou et al., 2021). Probabilistic forecasting captures predictive uncertainty through quantiles or distributions (Salinas et al., 2020). Change-point detection identifies regime shifts affecting model performance (Truong et al., 2020). Calendar effects are incorporated with learned embeddings (Smyl, 2020).\n\nOur approach unifies uncertainty-aware transformers with online shift detection and covariate embeddings, yielding robust multi-horizon forecasts under nonstationarity.",
    "reason": "The span strings together different topics—architectures, uncertainty modeling, change-point detection, and feature engineering—without transitions or explicit explanations of how they connect, resulting in poor coherence.",
    "start": 238,
    "end": 628,
    "label": "Coherence"
  },
  {
    "span": "DETR formulates detection as set prediction with bipartite matching (Carion et al., 2020). Swin Transformer introduces shifted windows for hierarchical features (Liu et al., 2021). SparseRCNN iteratively refines proposals without anchors (Sun et al., 2021). Multi-scale deformable attention accelerates convergence (Zhu et al., 2021).",
    "document": "Related Work\n\nTransformer-based object detectors replace hand-crafted components with attention-driven architectures that directly model long-range dependencies. This shift has prompted research on training dynamics, optimization speed, and multi-scale reasoning for robust detection.\n\nDETR formulates detection as set prediction with bipartite matching (Carion et al., 2020). Swin Transformer introduces shifted windows for hierarchical features (Liu et al., 2021). SparseRCNN iteratively refines proposals without anchors (Sun et al., 2021). Multi-scale deformable attention accelerates convergence (Zhu et al., 2021).\n\nOur method targets convergence speed in dense scenes by coupling deformable attention with curriculum-based query initialization.",
    "reason": "The span presents four separate detectors and mechanisms in isolation with no transitions or explicit links, leaving the relationships among the cited works unexplained.",
    "start": 286,
    "end": 620,
    "label": "Coherence"
  },
  {
    "span": "The PC algorithm remains the de facto standard for constraint-based causal discovery.",
    "document": "Related Work\n\nCausal discovery from observational data often relies on either constraint-based methods, which exploit conditional independence tests, or score-based approaches that search over DAGs with a regularized objective. Hybrid strategies combine both to improve robustness under finite samples.\n\nThe PC algorithm remains the de facto standard for constraint-based causal discovery. Variants extend PC to handle latent confounders, nonlinearity, and mixed data types, while recent work explores differentiable relaxations for gradient-based learning.\n\nOur contribution introduces a stability-regularized tester that adaptively allocates samples across conditioning sets, improving recall under limited data budgets.",
    "reason": "Asserts a 'de facto standard' about a specific algorithm without citation; per rule b) niche field assertions require evidence or citations at first mention.",
    "start": 304,
    "end": 389,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors claim that hierarchical attention mirrors human fixation patterns.",
    "document": "Related Work\n\nAttention mechanisms enable models to dynamically weight inputs, but their interpretability remains debated. Some research correlates attention distributions with gradient-based importance, while others question causal links between attention and prediction. Eye-tracking corpora introduce a complementary lens for evaluating alignment between model saliency and human reading behavior. In a previous study, the authors claim that hierarchical attention mirrors human fixation patterns. Our work revisits this connection by comparing hierarchical and flat attention under controlled architectures and by introducing a perturbation test that measures the stability of attention-human alignment under paraphrase.\n\nWe further contrast attention-based explanations with counterfactual reasoning techniques, examining fidelity and sensitivity across text classification and natural language inference.",
    "reason": "The sentence references a specific prior study and its claim but provides no citation; prior studies must be cited at first mention.",
    "start": 401,
    "end": 500,
    "label": "Unsupported claim"
  },
  {
    "span": "The BC5CDR corpus includes normalization annotations that yield a 3-point F1 improvement when leveraged during training.",
    "document": "Related Work\n\nBiomedical named entity recognition (BioNER) focuses on tagging mentions of diseases, chemicals, and genes in scholarly text. Progress has accelerated with domain-specific pretraining and the use of curated corpora that include entity spans and, in some cases, normalized identifiers.\n\nNormalization facilitates linking mentions to controlled vocabularies, enabling downstream applications such as adverse event mining and knowledge base construction. Models that jointly learn recognition and linking can potentially exploit shared structure.\n\nThe BC5CDR corpus includes normalization annotations that yield a 3-point F1 improvement when leveraged during training. Nonetheless, cross-corpus generalization remains challenging due to annotation discrepancies and domain shift across journals and time.",
    "reason": "Reports a specific quantitative improvement attributed to a dataset feature without any supporting citation or experimental evidence (rule b).",
    "start": 559,
    "end": 679,
    "label": "Unsupported claim"
  },
  {
    "span": "CoNLL-2014 remains the de facto standard benchmark for GEC.",
    "document": "Introduction\n\nGrammatical error correction (GEC) has evolved from rule-based and statistical MT approaches to neural encoder–decoder systems with pretraining and edit-based modeling (Ng et al., 2014; Chollampatt and Ng, 2018; Bryant et al., 2019). Data augmentation and synthetic error generation have been used to expand limited annotated corpora (Lichtarge et al., 2019; Grundkiewicz et al., 2019). CoNLL-2014 remains the de facto standard benchmark for GEC. Nevertheless, distributional shifts between learner corpora and real-world inputs challenge deployment (Napoles et al., 2017; Flachs et al., 2021). We propose confidence-aware decoding to balance precision and recall under shift.",
    "reason": "The sentence names a specific benchmark as the standard without citing the benchmark dataset or shared task; per the definition, datasets/shared tasks must be cited at first mention.",
    "start": 401,
    "end": 460,
    "label": "Unsupported claim"
  },
  {
    "span": "COCO 2017 test-dev contains 20,000 images and 80 object categories",
    "document": "Introduction\n\nObject detection research commonly evaluates on MS COCO due to its diverse scenes and challenging small-object instances. Standard practice compares mean Average Precision across IoU thresholds using single- and multi-scale inference. COCO 2017 test-dev contains 20,000 images and 80 object categories, serving as a widely accepted testbed for detector generalization.\n\nRelated Work\n\nTwo-stage detectors (e.g., region proposal-based) emphasize accuracy, while one-stage designs target efficiency. Recent advances include transformer-based architectures, dynamic heads, and label assignment strategies geared toward aligning classification and localization quality.",
    "reason": "Presents a specific dataset statistic without citing the dataset paper or official documentation.",
    "start": 249,
    "end": 315,
    "label": "Unsupported claim"
  },
  {
    "span": "Knowledge distillation for object detection transfers representations from high-capacity teachers to lightweight students using feature imitation (Romero et al., 2015; Chen et al., 2017). Response-based distillation aligns logits or localization outputs (Hinton et al., 2015; Li et al., 2017). Relation-based objectives preserve inter-instance or inter-feature dependencies (Zhang et al., 2019; Guo et al., 2021). Task-aware schemes incorporate detection-specific priors such as IoU-aware losses and focal weighting (Deng et al., 2019; Wang et al., 2019).",
    "document": "Related Work\nDeploying object detectors on resource-constrained devices requires compressing models while maintaining accuracy. Knowledge distillation (KD) is a prevalent approach to transfer information from large teachers to compact students via feature, response, or relation matching.\n\nKnowledge distillation for object detection transfers representations from high-capacity teachers to lightweight students using feature imitation (Romero et al., 2015; Chen et al., 2017). Response-based distillation aligns logits or localization outputs (Hinton et al., 2015; Li et al., 2017). Relation-based objectives preserve inter-instance or inter-feature dependencies (Zhang et al., 2019; Guo et al., 2021). Task-aware schemes incorporate detection-specific priors such as IoU-aware losses and focal weighting (Deng et al., 2019; Wang et al., 2019).\n\nWe investigate the overlooked role of anchor-free spatial cues in KD and introduce a center-aware distillation that transfers uncertainty-weighted heatmaps and offsets, improving AP on COCO with mobile backbones.",
    "reason": "The span recites categories of KD methods and references without drawing connections to the paper’s focus or highlighting what remains unsolved; it lacks synthesis and author viewpoint.",
    "start": 290,
    "end": 845,
    "label": "Lacks synthesis"
  },
  {
    "span": "Unsupervised domain adaptation aligns feature distributions between source and target corpora through adversarial objectives (Ganin et al., 2016; Tzeng et al., 2017). Multi-source adaptation aggregates heterogeneous sources to improve stability (Peng et al., 2019). Pretrained language models exhibit strong zero-shot transfer to unseen domains (Brown et al., 2020). Instance reweighting adjusts example importance using importance sampling (Huang et al., 2007).",
    "document": "Related Work: Domain Adaptation for NLP\n\nDomain adaptation seeks to transfer knowledge from a labeled source domain to a different, often unlabeled, target domain. In NLP, distributional shift across genres, topics, and registers degrades model performance, spurring methods that learn domain-invariant features or explicitly model shift.\n\nUnsupervised domain adaptation aligns feature distributions between source and target corpora through adversarial objectives (Ganin et al., 2016; Tzeng et al., 2017). Multi-source adaptation aggregates heterogeneous sources to improve stability (Peng et al., 2019). Pretrained language models exhibit strong zero-shot transfer to unseen domains (Brown et al., 2020). Instance reweighting adjusts example importance using importance sampling (Huang et al., 2007).\n\nBeyond feature alignment, contrastive objectives have been introduced to encourage class-consistent neighborhoods across domains (Kim et al., 2020). Adapter-based transfer reduces catastrophic forgetting while enabling efficient per-domain customization (Houlsby et al., 2019). Pseudo-labeling and self-training further exploit unlabeled target data to refine decision boundaries (Lee, 2013; He et al., 2020).\n\nIn this work, we study a simple, data-centric approach that augments target-domain examples with structured perturbations, complementing representation-level alignment and requiring no task-specific architectural changes.",
    "reason": "Multiple sentences list disparate approaches (adversarial alignment, multi-source aggregation, zero-shot transfer, instance reweighting) without explaining how they relate or transition, making the connection between cited works abrupt and unclear (issues a and b).",
    "start": 340,
    "end": 802,
    "label": "Coherence"
  },
  {
    "span": "Wu et al. (2021) presented Autoformer for series decomposition with autocorrelation. Zhou et al. (2021) introduced Informer with ProbSparse attention. Sen et al. (2019) evaluated transformer variants for multivariate forecasting. Salinas et al. (2020) proposed DeepAR for probabilistic univariate forecasting.",
    "document": "Introduction\n\nLong-horizon time series forecasting requires capturing seasonality, trend, and long-range dependencies while maintaining uncertainty calibration.\n\nWu et al. (2021) presented Autoformer for series decomposition with autocorrelation. Zhou et al. (2021) introduced Informer with ProbSparse attention. Sen et al. (2019) evaluated transformer variants for multivariate forecasting. Salinas et al. (2020) proposed DeepAR for probabilistic univariate forecasting.\n\nWe study representation leakage across horizons using contrastive losses.",
    "reason": "The span lists methods without describing how they relate (e.g., decomposition vs. sparsity vs. probabilistic modeling). There are no transitions or explicit comparisons, so the connection among sentences is only implied by shared topic, causing coherence issues.",
    "start": 162,
    "end": 471,
    "label": "Coherence"
  },
  {
    "span": "Federated averaging aggregates model updates across clients without centralizing data (McMahan et al., 2017). Secure aggregation prevents the server from reading individual updates (Bonawitz et al., 2017). Differential privacy is important in clinical prediction models (Dwork et al., 2014; Xu et al., 2020).",
    "document": "Related Work\n\nFederated learning enables decentralized training over distributed data silos, often under strict privacy and governance constraints. Personalization aims to adapt global models to local idiosyncrasies without sacrificing collective generalization. Communication efficiency and robustness to heterogeneous client participation are central challenges.\n\nFederated averaging aggregates model updates across clients without centralizing data (McMahan et al., 2017). Secure aggregation prevents the server from reading individual updates (Bonawitz et al., 2017). Differential privacy is important in clinical prediction models (Dwork et al., 2014; Xu et al., 2020).\n\nBeyond aggregation, meta-learning and multi-task formulations tailor models to client-specific distributions. Clustered federated learning groups clients with similar data to improve accuracy. Personalization layers and adapters provide lightweight local tuning under limited compute.\n\nWe propose a regularized local adaptation scheme that aligns client-specific heads via shared representation constraints. Our method achieves favorable trade-offs among personalization accuracy, privacy accounting, and communication overhead on diverse healthcare and finance datasets.",
    "reason": "The span lists three points (FedAvg, secure aggregation, differential privacy in clinical models) without transitions or an explicit explanation of how they relate to each other in the context of federated learning, resulting in poor coherence.",
    "start": 366,
    "end": 674,
    "label": "Coherence"
  },
  {
    "span": "The 2018 Clinical Concept Extraction Shared Task standardized the evaluation protocol for negation and uncertainty.",
    "document": "Related Work\n\nClinical named entity recognition (NER) focuses on identifying and normalizing spans such as problems, tests, and treatments from clinical narratives. Foundational toolkits introduced rule-based modules and lexicon-driven pipelines, while subsequent neural models have improved recall through contextual embeddings.\n\nThe 2018 Clinical Concept Extraction Shared Task standardized the evaluation protocol for negation and uncertainty. Building upon this, subsequent studies integrated assertion status prediction with entity recognition, proposing multi-task architectures that jointly learn spans and assertions.\n\nDespite these advances, consistent benchmarking across institutions remains challenging due to variability in annotation guidelines and PHI redaction practices. This motivates our release of a harmonized schema and cross-institutional evaluation split designed to measure robustness of clinical NER systems.\n\nOur work differs from prior frameworks by incorporating section-aware encoders and radiology-specific assertion cues, improving transferability across note types and departments.",
    "reason": "Refers to a specific shared task and its purported impact without citing the task or providing evidence.",
    "start": 331,
    "end": 446,
    "label": "Unsupported claim"
  },
  {
    "span": "Forecasting-based detectors flag large residuals from predictive models (Schölkopf et al., 2012; Xu et al., 2018). Reconstruction-based methods use autoencoders to capture normal patterns (Zong et al., 2018). Contrastive learning builds invariant representations across augmentations (Tonekaboni et al., 2021). Graph neural approaches model inter-sensor constraints for detection (Chen et al., 2021). Weak supervision leverages rule-based heuristics to generate labels (Ratner et al., 2017).",
    "document": "Related Work\n\nTime Series Anomaly Detection\n\nDetecting anomalies in multivariate time series is critical for industrial monitoring, IT operations, and healthcare. Approaches include statistical tests, predictive modeling, and representation learning, with evaluation complicating factors such as delayed labels and event windows (Blázquez-García et al., 2021; Ren et al., 2019). Robustness to distribution shift and scalability to high-dimensional streams remain open challenges.\n\nForecasting-based detectors flag large residuals from predictive models (Schölkopf et al., 2012; Xu et al., 2018). Reconstruction-based methods use autoencoders to capture normal patterns (Zong et al., 2018). Contrastive learning builds invariant representations across augmentations (Tonekaboni et al., 2021). Graph neural approaches model inter-sensor constraints for detection (Chen et al., 2021). Weak supervision leverages rule-based heuristics to generate labels (Ratner et al., 2017).\n\nOur method unifies forecasting and reconstruction via a shared latent space with temporal contrastive constraints, and we introduce window-aware evaluation that accounts for event duration and latency.",
    "reason": "The span lists several categories of methods with no transitions and no explanation of how they relate or differ, leaving relationships implicit and creating coherence issues.",
    "start": 481,
    "end": 972,
    "label": "Coherence"
  },
  {
    "span": "Brown & White (2015)",
    "document": "Introduction\n\nAdaptive text entry systems personalize language models and key layouts to user behavior (Kim and Alvarez, 2016). Gesture typing reduces input latency by inferring word paths from continuous traces (Olsen et al., 2017). Personalization typically blends global priors with user-specific statistics to handle idiosyncratic vocabularies (Chen and Park, 2018).\n\nBrown & White (2015) introduced an adaptive keyboard that updates touch models online, while follow-up work integrated context signals such as app and location (Jain and Wu, 2019). We revisit adaptive decoding with on-device federated updates and privacy-preserving telemetry.\n\nOur evaluation complements prior user studies (Lopez et al., 2020) with large-scale field telemetry to quantify real-world gains.",
    "reason": "Wrong conjunction style in a narrative citation; in author–year styles, narrative form should use 'and' instead of '&': 'Brown and White (2015)'.",
    "start": 372,
    "end": 392,
    "label": "Format"
  },
  {
    "span": "BERT has been used in AES to evaluate argumentative structure with token-level supervision.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features to neural encoders (Shermis and Burstein, 2013; Dong and Zhang, 2016). Pretrained Transformers have recently become popular for holistic score prediction (Ke and Ng, 2019; Mayfield and Black, 2020). BERT has been used in AES to evaluate argumentative structure with token-level supervision. Beyond content quality, researchers also study prompt-specific calibration and domain adaptation (Ridley et al., 2020; Liu and Huang, 2021). We build on this literature by exploring multi-task objectives that jointly model rubric criteria.",
    "reason": "Claims a specific setup of a task ('BERT' in AES with token-level supervision) without a supporting citation (rule e/iii).",
    "start": 281,
    "end": 372,
    "label": "Unsupported claim"
  },
  {
    "span": "On COCO, anchor-free detectors consistently outperform anchor-based ones when trained with identical schedules.",
    "document": "Introduction\n\nModern object detection has evolved from anchor-based designs requiring extensive hyperparameter tuning to anchor-free formulations that predict keypoints or centers directly. While both paradigms have achieved strong results, understanding the conditions under which one outperforms the other remains an active area of study. On COCO, anchor-free detectors consistently outperform anchor-based ones when trained with identical schedules. This observation motivates our analysis of optimization dynamics and label assignment strategies, decoupling architectural choices from training heuristics.\n\nRelated Work\n\nPrior comparisons were often confounded by differing backbones, input resolutions, and augmentations. Our benchmark standardizes these factors to enable fairer evaluation across detector families.",
    "reason": "Makes a dataset-specific performance claim without any citation or presented evidence (criterion b).",
    "start": 341,
    "end": 452,
    "label": "Unsupported claim"
  },
  {
    "span": "Speech separation has progressed from NMF-based techniques to deep clustering, mask inference, and time-domain separation (Smaragdis and Brown, 2003; Hershey et al., 2016; Wang and Chen, 2018; Luo and Mesgarani, 2019). Streaming and low-latency variants apply causal encoders, chunk-wise processing, and memory mechanisms (Chang et al., 2020; Zhang et al., 2021).",
    "document": "Related Work\n\nSingle-channel speech separation aims to disentangle overlapping speakers in realistic conditions while meeting application constraints such as latency and compute. Recent advances have emphasized end-to-end architectures and training objectives aligned with perceptual quality.\n\nSpeech separation has progressed from NMF-based techniques to deep clustering, mask inference, and time-domain separation (Smaragdis and Brown, 2003; Hershey et al., 2016; Wang and Chen, 2018; Luo and Mesgarani, 2019). Streaming and low-latency variants apply causal encoders, chunk-wise processing, and memory mechanisms (Chang et al., 2020; Zhang et al., 2021).\n\nWe propose StreamAlign, a latency-aware alignment loss that stabilizes causal mask estimation without additional model parameters, improving SDR at strict lookahead budgets.",
    "reason": "The span catalogs prior approaches and their streaming adaptations but does not explain how these relate to the proposed method or what gap remains unaddressed (criteria a and c).",
    "start": 294,
    "end": 657,
    "label": "Lacks synthesis"
  },
  {
    "span": "In (Smith et al., 2018)",
    "document": "Related Work\n\nObject Detection and Anchors\n\nModern object detectors evolved from sliding-window pipelines to end-to-end trainable systems (Girshick et al., 2014; Ren et al., 2015). In (Smith et al., 2018), anchors were redesigned to better match object scales, while subsequent approaches learned anchor shapes directly (Zhang et al., 2019). One-stage detectors (Redmon et al., 2016; Liu et al., 2016) traded some accuracy for speed, but later variants narrowed the gap via focal loss and improved label assignment (Lin et al., 2017; Tian et al., 2019). Transformer-based detectors (Carion et al., 2020) removed anchors altogether, relying on set prediction and global attention.",
    "reason": "Wrong citation style for a narrative reference; should be used as narrative 'In Smith et al. (2018)' rather than 'In (Smith et al., 2018)'.",
    "start": 181,
    "end": 204,
    "label": "Format"
  },
  {
    "span": "Prior work formulates recommendation as sequential decision making, using contextual bandits for short-term click optimization and deep reinforcement learning for long-horizon value (Li et al., 2010; Zou et al., 2019; Chen et al., 2021). Building on these, we propose a policy-gradient framework that optimizes long-term user engagement via reward shaping.",
    "document": "Introduction\n\nReinforcement Learning for Recommender Systems\n\nPersonalized recommendation platforms must optimize long-term utility across sessions while respecting user experience constraints. Modeling the interaction as a sequential decision process provides a principled way to reason about exploration, non-myopic rewards, and stateful preferences.\n\nPrior work formulates recommendation as sequential decision making, using contextual bandits for short-term click optimization and deep reinforcement learning for long-horizon value (Li et al., 2010; Zou et al., 2019; Chen et al., 2021). Building on these, we propose a policy-gradient framework that optimizes long-term user engagement via reward shaping.\n\nWe study a large-scale setting with delayed conversions, where reward signals are sparse and confounded. Our empirical evaluation spans multiple domains with offline counterfactual estimators and online A/B tests under strict traffic constraints.",
    "reason": "After listing related methods, the authors immediately introduce their approach without identifying a concrete limitation in prior work or articulating a motivating gap.",
    "start": 354,
    "end": 710,
    "label": "Lacks synthesis"
  },
  {
    "span": "Johnson et al. 1",
    "document": "Introduction\n\nOnline communities increasingly rely on automated moderation to manage harmful content (Reeves and Malik, 2020). Prior survey by Johnson et al. 1 summarizes classifiers for toxicity detection but overlooks cross-community domain shift. This gap matters because models trained on one forum often fail on another due to vocabulary and norm differences (Lin and Forbes, 2021; Qureshi et al., 2022). We present a benchmark spanning eight communities with aligned annotation protocols and propose an adaptive calibration layer that preserves precision under shift (Nguyen and Hart, 2023).\n\nRelated Work\n\nToxicity detection has progressed from lexicon-based filtering (Sato and Hill, 2017) to contextual encoders (Vargas et al., 2019) and multi-task setups incorporating discourse cues (Ortega and Kim, 2021). Domain adaptation approaches include instance reweighting (Li and Rossi, 2020) and adversarial invariance (Basu et al., 2021). Our contribution complements these by focusing on post-hoc calibration across domains.",
    "reason": "Wrong use of footnotes within a citation; 'Johnson et al. 1' should include a year (e.g., Johnson et al., 2019) or be formatted as a proper footnote.",
    "start": 143,
    "end": 159,
    "label": "Format"
  },
  {
    "span": "Prior datasets for multi-document summarization include Multi-News, WikiSum, WCEP, and XL-Sum (Fabbri et al., 2019; Liu et al., 2018; Ghalandari et al., 2020; Narayan et al., 2022). Approaches range from extractive graph-based ranking to abstractive encoder-decoder models with copy and coverage mechanisms (Erkan and Radev, 2004; See et al., 2017; Lebanoff et al., 2018).",
    "document": "Introduction\n\nMulti-document summarization seeks to produce concise, factual summaries from sets of related articles. The setting introduces challenges of content selection, redundancy reduction, and contradiction handling across sources.\n\nPrior datasets for multi-document summarization include Multi-News, WikiSum, WCEP, and XL-Sum (Fabbri et al., 2019; Liu et al., 2018; Ghalandari et al., 2020; Narayan et al., 2022). Approaches range from extractive graph-based ranking to abstractive encoder-decoder models with copy and coverage mechanisms (Erkan and Radev, 2004; See et al., 2017; Lebanoff et al., 2018).\n\nWe propose FactWeave, a constrained decoding framework that uses cross-document entailment signals to guide generation toward statements supported by multiple sources. Our experiments demonstrate improvements in factual consistency without sacrificing ROUGE.",
    "reason": "The span summarizes datasets and methods but does not connect them to the paper’s aims or identify a specific shortcoming the paper addresses (criteria a and c).",
    "start": 240,
    "end": 612,
    "label": "Lacks synthesis"
  },
  {
    "span": "Counterfactual data augmentation perturbs inputs to break spurious cues (Kaushik et al., 2020). Invariant risk minimization seeks predictors stable across environments (Arjovsky et al., 2019). Causal discovery infers graph structure from observational text features (Glymour et al., 2019). Front-door adjustments use mediators to control confounding (Pearl, 2009).",
    "document": "Related Work\n\nCausality in NLP\nSpurious correlations in textual datasets can cause models to overfit artifacts rather than task-relevant signals. Causal perspectives motivate interventions that tease apart stable causal relations from shortcuts.\n\nCounterfactual data augmentation perturbs inputs to break spurious cues (Kaushik et al., 2020). Invariant risk minimization seeks predictors stable across environments (Arjovsky et al., 2019). Causal discovery infers graph structure from observational text features (Glymour et al., 2019). Front-door adjustments use mediators to control confounding (Pearl, 2009).\n\nWe propose a simple environment construction scheme for text classification that operationalizes causal invariance using weak supervision from metadata splits.",
    "reason": "The span presents multiple causal techniques in isolation with no transitional phrasing or explicit relational mapping, resulting in abrupt shifts between topics and unclear connections among the cited works.",
    "start": 247,
    "end": 611,
    "label": "Coherence"
  },
  {
    "span": "a previous study reported human-level accuracy on emoji prediction",
    "document": "Introduction\n\nEmoji prediction maps textual messages to likely emoji used by authors, serving both as a proxy for affect and as a practical input method. The task is challenging due to polysemy, platform differences, and user-specific preferences. Motivating the task's difficulty, a previous study reported human-level accuracy on emoji prediction for a limited subset of frequent classes, suggesting that ceiling effects may be within reach on narrow label spaces.\n\nIn this work, we analyze generalization across platforms and long-tail emojis. We propose a temperature-scaled focal loss and user-conditioned adapters, and present a comprehensive error breakdown across sentiment, sarcasm, and code-switching scenarios.",
    "reason": "References 'a previous study' with a strong performance claim but does not provide a citation to identify the study.",
    "start": 282,
    "end": 348,
    "label": "Unsupported claim"
  },
  {
    "span": "Kumar et al., 2020)",
    "document": "Introduction\n\nPretrained sequence-to-sequence models have reshaped text generation by enabling strong transfer from large unlabeled corpora to task-specific fine-tuning. Early encoder–decoder baselines struggled with long-range dependencies (Bahdanau et al., 2015), but subsequent architectures leveraged deeper attention and larger training sets. In summarization and question answering, models like T5 Kumar et al., 2020) and BART (Lewis et al., 2020) have become de facto backbones due to their robust performance across domains. Despite these advances, adapting such models efficiently to new domains remains challenging (Devlin et al., 2019; Xiong et al., 2021).",
    "reason": "The citation has a closing parenthesis without an opening one; it should be either (Kumar et al., 2020) as a parenthetical citation or Kumar et al. (2020) as a narrative citation.",
    "start": 404,
    "end": 423,
    "label": "Format"
  },
  {
    "span": "Count-based schemes approximate pseudo-counts in high-dimensional spaces (Bellemare et al., 2016). Intrinsic curiosity modules reward prediction errors (Pathak et al., 2017). Meta-learning tunes exploration strategies across tasks (Rakelly et al., 2019). Safety constraints limit exploration in risk-sensitive domains (García and Fernández, 2015).",
    "document": "Introduction\n\nExploration remains a central challenge in reinforcement learning (RL), especially in sparse-reward and long-horizon environments. Numerous approaches incentivize visiting novel states or reducing uncertainty while controlling risk.\n\nCount-based schemes approximate pseudo-counts in high-dimensional spaces (Bellemare et al., 2016). Intrinsic curiosity modules reward prediction errors (Pathak et al., 2017). Meta-learning tunes exploration strategies across tasks (Rakelly et al., 2019). Safety constraints limit exploration in risk-sensitive domains (García and Fernández, 2015).\n\nIn this paper, we unify exploration and constraint satisfaction through a single uncertainty-aware objective.",
    "reason": "The four sentences enumerate different exploration paradigms without any connective phrases or explanation of how each relates to the others, leaving the relationship between the cited works implicit and the flow abrupt.",
    "start": 248,
    "end": 595,
    "label": "Coherence"
  },
  {
    "span": "There are many recent works tackling zero-shot transfer in QA.",
    "document": "Introduction\n\nMultilingual question answering (QA) evaluates a system's ability to understand queries and retrieve or generate answers across languages. Zero-shot transfer, where models trained in a high-resource language are applied to low-resource languages without target-language supervision, has gained traction due to the cost of annotation.\n\nThere are many recent works tackling zero-shot transfer in QA. However, the reported gains vary widely across datasets, and the role of pretraining corpora versus alignment losses is not fully disentangled.\n\nWe present a contrastive alignment framework that distills cross-lingual representations from parallel question–answer pairs while preserving monolingual performance. Our experiments cover extractive and generative QA, evaluating both passage selection and answer accuracy across typologically diverse languages.\n\nRelated Work\n\nCross-lingual transfer has leveraged multilingual encoders, parallel corpora, and alignment constraints. Prior evaluations often conflate retrieval and reading components; our modular analysis isolates each stage to better understand transfer bottlenecks.",
    "reason": "Uses a vague 'recent works' claim without citing any representative papers as required at first mention.",
    "start": 349,
    "end": 411,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works demonstrate that prompt-based tuning consistently outperforms fine-tuning on few-shot intent classification.",
    "document": "Introduction\n\nFew-shot text classification aims to learn reliable decision boundaries from only a handful of labeled examples per class. In conversational agents, intent classification often suffers from label scarcity due to the cost of annotating domain-specific utterances. To address this, researchers have proposed leveraging prior knowledge from large language models via prompts, verbalizers, and calibration strategies.\n\nRecent works demonstrate that prompt-based tuning consistently outperforms fine-tuning on few-shot intent classification. However, the extent to which these gains transfer across domains and label spaces remains unclear, especially when label semantics are ambiguous or overlapping. Moreover, most evaluations focus on English-only benchmarks, leaving the cross-lingual generality of prompt-based methods largely unexplored.\n\nIn this paper, we present a systematic comparison of prompt design choices under limited supervision. We analyze robustness to label imbalance, distribution shift, and noisy annotations, and we propose a simple calibration strategy that improves stability without adding parameters.",
    "reason": "Claims performance trends of 'recent works' without providing any citations or evidence, violating the requirement to cite when referring to prior studies.",
    "start": 429,
    "end": 550,
    "label": "Unsupported claim"
  },
  {
    "span": "Single-cell RNA-seq clustering methods range from graph-based Louvain/Leiden to variational autoencoders and contrastive embeddings (Blondel et al., 2008; Traag et al., 2019; Eraslan et al., 2019; Xie et al., 2020; Dong et al., 2022).",
    "document": "Related Work\n\nUnsupervised analysis of single-cell RNA sequencing (scRNA-seq) aims to uncover cell populations and trajectories from high-dimensional, sparse count data. A central step is clustering cells after normalization, dimensionality reduction, and neighborhood graph construction.\n\nSingle-cell RNA-seq clustering methods range from graph-based Louvain/Leiden to variational autoencoders and contrastive embeddings (Blondel et al., 2008; Traag et al., 2019; Eraslan et al., 2019; Xie et al., 2020; Dong et al., 2022). Batch correction and integration techniques address cross-dataset alignment (Haghverdi et al., 2018; Stuart et al., 2019). Downstream analyses include marker discovery and differential expression (Satija et al., 2015; Trapnell et al., 2014).\n\nWe analyze rare cell detection under limited labels but do not relate our approach to the above clustering families.",
    "reason": "The span lists method categories without articulating how they bear on rare cell detection or what specific limitations motivate the new approach, hence lacking synthesis (definition a, c).",
    "start": 290,
    "end": 524,
    "label": "Lacks synthesis"
  },
  {
    "span": " [Miller et al., 2018]",
    "document": "Related Work\n\nFairness in machine learning formalizes criteria such as demographic parity and equalized odds (Dwork et al., 2012; Hardt et al., 2016). Pre-processing approaches modify datasets, in-processing adds constraints, and post-processing calibrates outputs (Kamiran and Calders, 2012; Zafar et al., 2017; Pleiss et al., 2017). Post-processing methods include [Miller et al., 2018] and calibrated equalized odds, which trade slight accuracy for improved group metrics. We study subgroup robustness under distribution shift (Kearns et al., 2018; Sagawa et al., 2020).",
    "reason": "Wrong bracket style for author–year; should use parentheses: '(Miller et al., 2018)'.",
    "start": 366,
    "end": 388,
    "label": "Format"
  },
  {
    "span": "There are many recent works exploring unsupervised symbolic music segmentation.",
    "document": "Related Work\n\nSegmenting symbolic music into coherent sections supports analysis and generation. There are many recent works exploring unsupervised symbolic music segmentation. Approaches range from self-similarity matrices to contrastive learning over note sequences, yet a consensus evaluation protocol is still emerging.",
    "reason": "Uses the phrase 'recent works' without citing any of those works, which should be referenced.",
    "start": 97,
    "end": 176,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT has been used in AES for cross-prompt and cross-lingual scoring.",
    "document": "Related Work\n\nAutomated essay scoring (AES) has evolved from handcrafted features and linear models to neural approaches that learn holistic representations of writing quality. Early systems emphasized lexical diversity, grammar, and cohesion signals, while neural encoders capture broader semantic and discourse patterns. BERT has been used in AES for cross-prompt and cross-lingual scoring. However, model stability and fairness remain open issues, particularly under prompt drift and domain shift.\n\nWe extend prior AES research by analyzing robustness to topical shift and demographic confounds, introducing a new evaluation protocol with stratified resampling across prompts and institutions.",
    "reason": "This statement references prior work using BERT in AES and specific setups (cross-prompt, cross-lingual) but lacks citations for the claimed applications, which should be cited at first mention.",
    "start": 323,
    "end": 392,
    "label": "Unsupported claim"
  },
  {
    "span": "There are many recent works that explore multimodal rationale generation.",
    "document": "Introduction\n\nExplainability in multimodal learning has received growing attention as models are increasingly deployed in high-stakes domains (Doshi-Velez and Kim, 2017; Gilpin et al., 2018). Early efforts focused on visual saliency maps and attention mechanisms to provide post-hoc explanations for decisions (Selvaraju et al., 2017; Chattopadhyay et al., 2018). More recently, researchers have begun to study rationales that align language justifications with visual evidence to improve faithfulness and human interpretability. There are many recent works that explore multimodal rationale generation. In this paper, we propose a dataset and evaluation protocol that emphasize faithfulness through causal interventions, complementing prior saliency-based approaches.",
    "reason": "The sentence claims the existence of 'many recent works' without citing any of them, violating rule (d) about mentioning recent works without citations.",
    "start": 530,
    "end": 603,
    "label": "Unsupported claim"
  },
  {
    "span": "Subsequent works extend DETR with improved bipartite matching, deformable attention, and auxiliary queries (Zhu et al., 2021; Sun et al., 2021; Meng et al., 2021; Wang et al., 2022).",
    "document": "Related Work\n\nObject detection with transformers. DETR reframed detection as set prediction with a transformer encoder-decoder and bipartite matching loss (Carion et al., 2020). While elegant, its training is data- and compute-intensive. Subsequent works extend DETR with improved bipartite matching, deformable attention, and auxiliary queries (Zhu et al., 2021; Sun et al., 2021; Meng et al., 2021; Wang et al., 2022). Parallel efforts integrate multi-scale features and specialized positional encodings to stabilize optimization (Liu et al., 2021; Gao et al., 2021).\n\nVision backbones. Hierarchical transformers deliver stronger multi-scale representations than plain ViT, improving detection and segmentation performance (Dosovitskiy et al., 2021; Liu et al., 2021). Convolutional and hybrid backbones remain competitive, especially in resource-constrained settings (He et al., 2016; Yuan et al., 2021).\n\nIn this work, we explore conditioning mechanisms for dense queries to improve data efficiency without architectural overhauls.",
    "reason": "The sentence lists multiple DETR variants without clarifying how they relate to the challenges the paper addresses or to the proposed conditioning approach, failing to synthesize prior work with the paper’s argument (criterion a).",
    "start": 238,
    "end": 420,
    "label": "Lacks synthesis"
  },
  {
    "span": "Garcia et. al. (2017)",
    "document": "Related Work\n\nVision-and-language models align visual features with linguistic structure to generate descriptive captions (Xu et al., 2015; Anderson et al., 2018). Building on Garcia et. al. (2017), we incorporate attribute supervision to encourage object-centric phrasing. Transformer decoders have improved fluency but can overfit to frequent n-grams (Huang and Ritter, 2020). Recent work introduces contrastive objectives that penalize mismatched image–text pairs (Radford et al., 2021; Chen and Li, 2022). Our approach combines attribute prompts with a contrastive pretraining stage to mitigate degeneracy in rare object mentions.",
    "reason": "Incorrect abbreviation 'et. al.'; should be 'et al.' without the period after 'et'.",
    "start": 176,
    "end": 197,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on essays from three grade levels",
    "document": "Related Work\n\nAutomated Essay Scoring (AES) systems have progressed from handcrafted features and linear models to deep contextual encoders that capture discourse and semantics. Modern AES approaches leverage pretrained language models, task-specific adapters, and multi-aspect scoring objectives to improve both accuracy and fairness. BERT was used in an AES task trained on essays from three grade levels, demonstrating the utility of contextual embeddings for holistic and trait-specific scoring.\n\nSubsequent research explored rubric-aware encoders, counterfactual data augmentation to mitigate prompt leakage, and uncertainty modeling to better align with human rater variability. Nevertheless, issues of bias across demographics and prompts persist, and model calibration remains underexplored in high-stakes educational settings.\n\nOur work contributes a prompt-agnostic AES framework that disentangles surface fluency from content relevance using dual encoders regularized by contrastive objectives, alongside a new protocol for evaluating cross-prompt generalization.\n",
    "reason": "Describes a specific experimental setup in prior work (BERT in AES with three grade levels) without any citation, violating rule a and rule e(iii).",
    "start": 336,
    "end": 406,
    "label": "Unsupported claim"
  },
  {
    "span": "Intrusion detection has evolved from signature-based systems to anomaly-based detection using classical machine learning, deep autoencoders, graph neural networks, and flow-based detectors (Roesch, 1999; Sommer and Paxson, 2010; Kim et al., 2016; Luo et al., 2018; Yousefi et al., 2021).",
    "document": "Related Work\n\nNetwork intrusion detection systems (IDS) must adapt to rapidly changing traffic patterns and novel attack vectors. Research has increasingly focused on learning-based methods to reduce manual signature engineering.\n\nIntrusion detection has evolved from signature-based systems to anomaly-based detection using classical machine learning, deep autoencoders, graph neural networks, and flow-based detectors (Roesch, 1999; Sommer and Paxson, 2010; Kim et al., 2016; Luo et al., 2018; Yousefi et al., 2021).\n\nIn this paper we develop ShiftGuard, a distributionally robust IDS that combines risk-sensitive training with online calibration to maintain recall under covariate and prior shifts.",
    "reason": "The span describes the evolution of IDS methods without connecting these trends to the proposed distributionally robust approach or stating the open problem it targets (criterion a/c).",
    "start": 231,
    "end": 518,
    "label": "Lacks synthesis"
  },
  {
    "span": "Early multimodal QA works emphasized attention over visual regions (Anderson et al., 2018). Transformer-based fusion improves cross-modal alignment (Tan and Bansal, 2019). Caption pretraining enhances downstream reasoning (Zhou et al., 2020). Video QA leverages temporal coherence for answer selection (Lei et al., 2018).",
    "document": "Related Work\n\nMultimodal question answering (MQA) integrates signals from text and images or video to produce grounded answers. Prior research explores visual attention, representation learning, and cross-modal fusion to bridge modality gaps and support reasoning. Despite progress, existing systems still struggle with compositional queries and out-of-domain shifts.\n\nEarly multimodal QA works emphasized attention over visual regions (Anderson et al., 2018). Transformer-based fusion improves cross-modal alignment (Tan and Bansal, 2019). Caption pretraining enhances downstream reasoning (Zhou et al., 2020). Video QA leverages temporal coherence for answer selection (Lei et al., 2018).\n\nIn contrast to these lines, our work focuses on robust cross-modal retrieval-augmented reasoning, coupling retrieval of visual evidence with query-conditioned fusion layers. We additionally contribute a challenging evaluation suite that disentangles object recognition from relational reasoning under distribution shift.",
    "reason": "The span lists multiple cited works from different sub-areas (attention, fusion, caption pretraining, video QA) without transitions or explaining how each relates to the previous one. The relationship between the works is implied but not explicitly stated, making the connections abrupt and incoherent.",
    "start": 369,
    "end": 690,
    "label": "Coherence"
  },
  {
    "span": "Differential privacy adds calibrated noise to gradients to bound disclosure (Abadi et al., 2016). Secure aggregation prevents the server from seeing individual updates (Bonawitz et al., 2017). Personalization layers adapt global models to client heterogeneity (Arivazhagan et al., 2019). Gradient sparsification reduces communication costs with top-k selection (Stich et al., 2018). Robust aggregation mitigates poisoning by Byzantine clients (Blanchard et al., 2017).",
    "document": "Related Work\n\nPrivacy and Robustness in Federated Learning\n\nFederated learning (FL) enables training across distributed clients without centralizing raw data, raising challenges in privacy, robustness, and communication efficiency (Kairouz et al., 2021). Techniques for privacy preservation, system efficiency, and statistical heterogeneity have been extensively studied, yet their combined effects on utility remain insufficiently understood in practice.\n\nDifferential privacy adds calibrated noise to gradients to bound disclosure (Abadi et al., 2016). Secure aggregation prevents the server from seeing individual updates (Bonawitz et al., 2017). Personalization layers adapt global models to client heterogeneity (Arivazhagan et al., 2019). Gradient sparsification reduces communication costs with top-k selection (Stich et al., 2018). Robust aggregation mitigates poisoning by Byzantine clients (Blanchard et al., 2017).\n\nOur work investigates the joint design of secure aggregation and personalized head adapters under user-level differential privacy, analyzing the privacy-utility-communication trade-offs on mobile vision and speech tasks.",
    "reason": "The span presents a series of works on disparate concerns (privacy, aggregation, personalization, compression, robustness) with no transitions or explicit links between them, making the relationship between sentences unclear.",
    "start": 457,
    "end": 925,
    "label": "Coherence"
  },
  {
    "span": "BERT was used in an AES task trained on essays written by non-native speakers.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) combines linguistic features with machine learning to predict human-assigned proficiency scores (Attali and Burstein, 2006; Ke and Ng, 2019). Neural approaches have shifted from feature-engineered pipelines to pretrained language models fine-tuned on essay corpora, improving robustness to topic variation (Zong and Wang, 2020; Liu et al., 2021).\n\nRecent studies emphasize prompt-sensitivity, length bias, and adversarial robustness, proposing calibration and content-based constraints for fairer assessments (Saha et al., 2021). BERT was used in an AES task trained on essays written by non-native speakers. Yet, cross-prompt generalization and resistance to superficial cues remain open issues. We address these by disentangling content and structure through contrastive objectives.",
    "reason": "This sentence references a specific prior setup ('BERT was used in an AES task') without citing the study, despite mentioning a particular model and dataset characteristics, which requires a citation at first mention.",
    "start": 574,
    "end": 652,
    "label": "Unsupported claim"
  },
  {
    "span": "Benchmark studies consistently find that PPO outperforms TRPO on locomotion tasks.",
    "document": "Related Work\n\nPolicy gradient methods form a core family of deep reinforcement learning algorithms for continuous control (Schulman et al., 2015; Lillicrap et al., 2016). Trust region–based updates improve stability but can be computationally expensive (Schulman et al., 2015). Proximal policy optimization (PPO) simplifies constraints via clipped objectives and has become a popular choice in practice (Schulman et al., 2017). Sample efficiency and robustness under sparse rewards remain open challenges (Andrychowicz et al., 2020; Laskin et al., 2020).\n\nBenchmark studies consistently find that PPO outperforms TRPO on locomotion tasks. Nevertheless, sensitivity to reward scaling, normalization, and action bounds complicates head-to-head comparisons across implementations.\n\nWe provide a controlled study with matched hyperparameters, unified codebase, and diverse MuJoCo tasks to isolate algorithmic differences from implementation artifacts.",
    "reason": "Makes a comparative claim about benchmark results without citing any of the benchmark studies.",
    "start": 556,
    "end": 638,
    "label": "Unsupported claim"
  },
  {
    "span": "Dosovitskiy et al. (2021) introduced the Vision Transformer architecture. Touvron et al. (2021) proposed distillation strategies for training data-efficient vision transformers. Liu et al. (2021) hybridized hierarchical windows in Swin Transformer. Bello et al. (2019) incorporated attention into convolutional networks.",
    "document": "Related Work\n\nSelf-attention has shifted the landscape of visual recognition, challenging convolutional inductive biases and enabling long-range dependency modeling.\n\nDosovitskiy et al. (2021) introduced the Vision Transformer architecture. Touvron et al. (2021) proposed distillation strategies for training data-efficient vision transformers. Liu et al. (2021) hybridized hierarchical windows in Swin Transformer. Bello et al. (2019) incorporated attention into convolutional networks.\n\nOur work investigates lightweight token mixing for edge devices.",
    "reason": "The span enumerates several vision transformer and attention variants but does not articulate how they relate or differ. There are no transitions or explanations linking the contributions, producing abrupt shifts across multiple sentences.",
    "start": 167,
    "end": 487,
    "label": "Coherence"
  },
  {
    "span": "Our method outperforms state-of-the-art baselines on two benchmarks",
    "document": "Introduction\n\nQuery-focused summarization (QFS) targets content most relevant to an information need and has applications in search and multi-document exploration (Dang, 2008). Graph-based encoders capture cross-sentence relations and can help model salience and redundancy (Yasunaga et al., 2017; Li et al., 2020). Nevertheless, aligning the query with discourse structure remains challenging, particularly when evidence is dispersed across documents. Our method outperforms state-of-the-art baselines on two benchmarks, highlighting the benefits of entity-level reasoning for QFS.\n\nRelated Work\n\nNeural QFS has leveraged query conditioning via attention, span selection, and contrastive objectives (Nema et al., 2017; Xu and Lapata, 2020). Entity-centric summarization builds graphs over coreference chains and discourse relations to better control redundancy (Christensen et al., 2013; Liu and Lapata, 2019). We integrate query signals into graph construction and utilize a planning module to sequence salient units before surface realization.",
    "reason": "The claim references 'state-of-the-art baselines' without identifying or citing them, invoking prior work without citation per rule (a).",
    "start": 453,
    "end": 520,
    "label": "Unsupported claim"
  },
  {
    "span": "there are many recent works that explore cross-lingual code summarization",
    "document": "Introduction\n\nSource code summarization aims to generate natural-language descriptions of program functions and classes. Pretrained encoder-decoder models trained on large corpora of code and text have recently reshaped this area by enabling stronger generalization across repositories and programming paradigms. Despite these advances, documentation scarcity and stylistic inconsistency still limit performance in low-resource languages.\n\nTo address data imbalance, there are many recent works that explore cross-lingual code summarization. However, existing approaches often assume parallel code-comment pairs across languages or rely on brittle token-level alignment, which may not hold in practice. In contrast, we propose a language-agnostic intermediate representation to bridge structural gaps while maintaining semantic fidelity.\n\nOur contributions are threefold: (1) we introduce a training objective that jointly aligns code structure and intent signals, (2) we present a scalable curriculum that adapts to repository-level shift, and (3) we provide a comprehensive evaluation across diverse repositories and programming languages.",
    "reason": "The span makes a claim about 'many recent works' without providing any citations to support the statement, which is required for mentions of recent work.",
    "start": 467,
    "end": 540,
    "label": "Unsupported claim"
  },
  {
    "span": "Prior competitions have standardized toxicity metrics used across safety benchmarks.",
    "document": "Introduction\n\nSafety in Open-Domain Dialogue. Toxicity, harassment, and unsafe advice are key risks in neural conversational agents (Dinan et al., 2019; Xu et al., 2021). RLHF and red-teaming have been used to reduce harmful outputs (Bai et al., 2022; Ganguli et al., 2022). Prior competitions have standardized toxicity metrics used across safety benchmarks. Yet definitions of 'harm' vary across datasets and cultures, complicating evaluation.",
    "reason": "References prior competitions and a field-wide outcome without citing any specific competition or evidence (violates rule a and b).",
    "start": 275,
    "end": 359,
    "label": "Unsupported claim"
  },
  {
    "span": "(Lee and Park, 2018",
    "document": "Related Work\n\nExploration in reinforcement learning balances reward maximization and information gain (Auer et al., 2002; Osband et al., 2016). Early work (Lee and Park, 2018 proposed optimism under uncertainty for continuous control, but required strong smoothness assumptions. Posterior sampling approaches yield strong empirical results (Russo and Van Roy, 2014; O’Connor and Singh, 2020), yet can be brittle under model misspecification (Zhang and Wei, 2022). In contrast, entropy-regularized objectives provide robust improvements with modest tuning (Khan et al., 2021). Our method integrates risk-aware bonuses with model-based rollouts to stabilize long-horizon planning.",
    "reason": "Missing closing parenthesis in a parenthetical citation.",
    "start": 155,
    "end": 174,
    "label": "Format"
  },
  {
    "span": "Recent works show that preference modeling scales linearly with the number of annotators.",
    "document": "Introduction\n\nReinforcement learning from human feedback (RLHF) aligns language models with user preferences by training a reward model on pairwise comparisons, followed by policy optimization (Christiano et al., 2017; Ouyang et al., 2022). Sample efficiency and label quality are critical for stable reward learning and safe deployment.\n\nRecent works show that preference modeling scales linearly with the number of annotators. However, the interplay between annotator diversity and label noise remains under-explored. We present a stratified labeling protocol and a variance-aware objective to disentangle scaling effects from rater heterogeneity.",
    "reason": "Mentions 'recent works' and a quantitative scaling claim without providing supporting citations.",
    "start": 339,
    "end": 428,
    "label": "Unsupported claim"
  },
  {
    "span": "Most recent work on Python-to-Java translation relies on AST-guided Transformers.",
    "document": "Introduction\n\nTranscompilation between high-level programming languages demands precise handling of syntax, typing conventions, and library idioms. Neural code translation has progressed from token-level sequence models to structure-aware encoders that incorporate abstract syntax trees (ASTs). Most recent work on Python-to-Java translation relies on AST-guided Transformers. Despite improved syntactic fidelity, semantic preservation and idiomatic target code remain challenging, especially for dynamic language features and exception handling. We present a dual-channel model that aligns AST and bytecode traces to capture both structural and operational semantics.\n\nRelated Work\n\nEarlier approaches leveraged rule-based transpilers augmented with statistical models for disambiguation. Subsequent neural systems integrated code graphs, data-flow, and type hints. Parallel code corpora scarcity has prompted back-translation and noisy mining strategies. Our contribution focuses on bridging structural and behavioral signals to reduce compilation errors and runtime discrepancies.",
    "reason": "Uses the phrase \"Most recent work\" to generalize about the state of the art without providing citations to representative studies.",
    "start": 295,
    "end": 376,
    "label": "Unsupported claim"
  },
  {
    "span": "The COCO dataset contains over 2 million labeled images.",
    "document": "Introduction\nObject detection benchmarks have catalyzed progress in recognition models by standardizing evaluation and enabling large-scale comparisons. Early datasets emphasized limited categories and bounding boxes, while newer corpora include richer annotations and long-tail distributions (Everett et al., 2017; Patel and Singh, 2018).\nThe COCO dataset contains over 2 million labeled images. Recent detectors employ multi-scale features, dynamic heads, and improved label assignment strategies to better exploit dense annotations (Zhao et al., 2020; Romero et al., 2021). In this work, we focus on training-time optimizations that reduce compute while preserving accuracy on common benchmarks.",
    "reason": "This is a quantitative claim about a dataset without a supporting citation. Per rule (b), statistics specific to a dataset should be cited.",
    "start": 340,
    "end": 396,
    "label": "Unsupported claim"
  },
  {
    "span": "In (Gonzalez et al., 2020)",
    "document": "Introduction\n\nPretrained language models can be adapted to handle domain-specific terminology through vocabulary augmentation and continued pretraining (Rao and Jensen, 2019). In (Gonzalez et al., 2020), the authors introduce a domain-adaptive masking scheme that selectively prioritizes rare terms. However, subsequent work shows that dynamic masking during fine-tuning can close much of the gap without changes to pretraining (Hart and Lim, 2021). We revisit these findings in the context of biomedical entity normalization, where controlled vocabularies and long-tail entities complicate transfer (Park and Silva, 2022). Our results suggest that balancing token-level rarity with span-level uncertainty yields more robust performance than uniform masking (Liu and Weber, 2021).\n\nWe further compare against curriculum-based token selection, which emphasizes increasingly complex terminology over training epochs (Ahmed and Roy, 2020), and show that selective span corruption improves generalization to unseen concepts.",
    "reason": "Wrong citation style: narrative context incorrectly uses a parenthetical lead-in \"In (Gonzalez et al., 2020)\"; it should be narrative style, e.g., \"Gonzalez et al. (2020)\".",
    "start": 176,
    "end": 202,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claim that entropy regularization eliminates mode collapse.",
    "document": "Related Work\n\nRegularization in Policy Optimization\n\nEntropy regularization and KL constraints are standard techniques to encourage exploration and stabilize updates in policy gradient methods (Williams, 1992; Schulman et al., 2017). Auxiliary objectives, such as behavioral cloning from replay buffers or demonstrations, further guide learning under sparse rewards (Hester et al., 2018). In a previous study, the authors claim that entropy regularization eliminates mode collapse. Nevertheless, excessive entropy can hinder convergence, motivating adaptive schedules that balance exploration and exploitation (Agarwal et al., 2020). We investigate how uncertainty-aware penalties interact with off-policy corrections in continuous control.",
    "reason": "Refers to a specific prior study and its claim without providing a citation at first mention, violating rule a.",
    "start": 389,
    "end": 481,
    "label": "Unsupported claim"
  },
  {
    "span": "Representation learning for causal effect estimation includes balancing methods like CFR (Shalit et al., 2017) and TARNet (Johansson et al., 2016). Latent variable models such as CEVAE account for unobserved confounding (Louizos et al., 2017). Recent architectures like Dragonnet and SITE integrate propensity and outcome modeling (Shi et al., 2019; Yao et al., 2018).",
    "document": "Related Work\n\nEstimating treatment effects from observational data requires addressing selection bias and confounding. Representation learning approaches have shown promise by mapping covariates into spaces where treated and control groups are comparable.\n\nRepresentation learning for causal effect estimation includes balancing methods like CFR (Shalit et al., 2017) and TARNet (Johansson et al., 2016). Latent variable models such as CEVAE account for unobserved confounding (Louizos et al., 2017). Recent architectures like Dragonnet and SITE integrate propensity and outcome modeling (Shi et al., 2019; Yao et al., 2018).\n\nIn contrast, we study calibration-constrained objectives that explicitly trade off balance and predictive fidelity, with a focus on transport across shifts in covariate distributions.",
    "reason": "The span lists prior methods without explaining how they relate to the current study or identifying a concrete shortcoming to be addressed; it lacks synthesis (criteria a and c).",
    "start": 257,
    "end": 625,
    "label": "Lacks synthesis"
  },
  {
    "span": "Algorithmic fairness has been studied through group metrics such as demographic parity, equalized odds, and predictive parity (Dwork et al., 2012; Hardt et al., 2016; Chouldechova, 2017), as well as individual fairness and counterfactual notions (Dwork et al., 2012; Kusner et al., 2017). Mitigation techniques include pre-processing reweighting (Kamiran and Calders, 2012), in-processing constrained optimization (Zafar et al., 2017), and post-processing calibration (Pleiss et al., 2017). Causality-aware approaches model structural biases to guide interventions (Kilbertus et al., 2017; Nabi and Shpitser, 2018).",
    "document": "Related Work\n\nFairness in machine learning encompasses definitions and interventions that address disparities in predictions across groups and individuals. The literature spans statistical criteria, causal models, and practical mitigation algorithms.\n\nAlgorithmic fairness has been studied through group metrics such as demographic parity, equalized odds, and predictive parity (Dwork et al., 2012; Hardt et al., 2016; Chouldechova, 2017), as well as individual fairness and counterfactual notions (Dwork et al., 2012; Kusner et al., 2017). Mitigation techniques include pre-processing reweighting (Kamiran and Calders, 2012), in-processing constrained optimization (Zafar et al., 2017), and post-processing calibration (Pleiss et al., 2017). Causality-aware approaches model structural biases to guide interventions (Kilbertus et al., 2017; Nabi and Shpitser, 2018).\n\nWe develop a distributionally robust objective that aligns with equalized odds while accounting for label noise in minority groups.",
    "reason": "The span catalogs metrics and methods without synthesizing how they motivate the proposed robust objective or what problem remains open, leaving the authors' perspective unstated (definition a and c).",
    "start": 252,
    "end": 867,
    "label": "Lacks synthesis"
  },
  {
    "span": "In a previous clinical trial, the authors reported a 40% reduction in readmission using our triage model.",
    "document": "Introduction\n\nHospital readmission prediction leverages electronic health records (EHRs) to identify at-risk patients and guide post-discharge interventions (Shickel et al., 2018; Razavian et al., 2016). Recent transformer architectures adapted to irregular time series have improved predictive performance over traditional gradient boosting baselines (Song et al., 2020; Li et al., 2022). In a previous clinical trial, the authors reported a 40% reduction in readmission using our triage model. While promising, translating predictive gains into clinical impact requires careful evaluation of deployment settings, alert fatigue, and resource constraints (Sendak et al., 2020; Wiens et al., 2019).\n\nRelated Work\n\nProspective validations for EHR models remain scarce, and outcome drift under policy changes can erode performance (Nestor et al., 2019; Ghassemi et al., 2018). We build on cost-sensitive decision policies (Obermeyer and Emanuel, 2016) and recent work on uncertainty-aware triage (Leibig et al., 2017), proposing a budget-constrained policy learning framework.",
    "reason": "Claims a specific prior clinical trial result (40% reduction) without citing the study; per rule (a), first mention of a study must include a citation.",
    "start": 390,
    "end": 495,
    "label": "Unsupported claim"
  },
  {
    "span": "Vision Transformers (ViTs) have been adapted for medical image analysis in a variety of ways, including pure transformer backbones (Dosovitskiy et al., 2021; Chen et al., 2021), hybrid CNN-Transformer architectures (Graham et al., 2021; Xu et al., 2022), and hierarchical variants that introduce local attention mechanisms (Wang et al., 2021; Liu et al., 2021). Pretraining strategies range from supervised transfer on natural images (Kolesnikov et al., 2020) to self-supervised objectives such as masked image modeling (He et al., 2022; Xie et al., 2022). Data-efficient training has also been explored through distillation (Touvron et al., 2021) and token pruning (Tang et al., 2022).",
    "document": "Related Work\n\nConvolutional neural networks (CNNs) have long been the default backbone for medical image analysis, achieving strong performance on segmentation, detection, and classification tasks across modalities (Ronneberger et al., 2015; Litjens et al., 2017). However, their limited receptive fields have motivated interest in architectures with more global context aggregation.\n\nVision Transformers (ViTs) have been adapted for medical image analysis in a variety of ways, including pure transformer backbones (Dosovitskiy et al., 2021; Chen et al., 2021), hybrid CNN-Transformer architectures (Graham et al., 2021; Xu et al., 2022), and hierarchical variants that introduce local attention mechanisms (Wang et al., 2021; Liu et al., 2021). Pretraining strategies range from supervised transfer on natural images (Kolesnikov et al., 2020) to self-supervised objectives such as masked image modeling (He et al., 2022; Xie et al., 2022). Data-efficient training has also been explored through distillation (Touvron et al., 2021) and token pruning (Tang et al., 2022).\n\nAt the same time, domain shift and limited annotations remain key challenges in clinical deployment. Prior works explore test-time adaptation (Sun et al., 2022), domain generalization (Kamnitsas et al., 2017), and weak supervision (Zhou et al., 2018), but these strategies often assume access to auxiliary unlabeled data or multiple source domains.\n\nIn this paper, we revisit token-level sparsity for ViTs in medical imaging and show that targeted pruning guided by structure-aware uncertainty can reduce compute while maintaining accuracy in out-of-distribution scenarios.\n",
    "reason": "This paragraph lists prior approaches and citations without explaining their relation to the present study or articulating the authors' perspective, fulfilling (a) and (c) of the definition.",
    "start": 385,
    "end": 1071,
    "label": "Lacks synthesis"
  },
  {
    "span": "Unsupervised domain adaptation for semantic segmentation typically minimizes distribution shift using adversarial learning, self-training with pseudo-labels, or image translation (Hoffman et al., 2018; Tsai et al., 2018; Zou et al., 2019; Sankaranarayanan et al., 2018; Yang and Soatto, 2020). Some studies introduce curriculum learning or feature-level alignment to stabilize training (Zhang et al., 2019; Wang et al., 2020). In this work, we adopt a pseudo-label refinement strategy combined with strong data augmentation.",
    "document": "Introduction\n\nSemantic segmentation models degrade when deployed across domains with distinct lighting, texture, or sensor characteristics. Unsupervised domain adaptation (UDA) aims to leverage labeled source and unlabeled target data to bridge this gap.\n\nRelated Work on UDA for Segmentation\nUnsupervised domain adaptation for semantic segmentation typically minimizes distribution shift using adversarial learning, self-training with pseudo-labels, or image translation (Hoffman et al., 2018; Tsai et al., 2018; Zou et al., 2019; Sankaranarayanan et al., 2018; Yang and Soatto, 2020). Some studies introduce curriculum learning or feature-level alignment to stabilize training (Zhang et al., 2019; Wang et al., 2020). In this work, we adopt a pseudo-label refinement strategy combined with strong data augmentation.\n\nChallenges\nPseudo-label quality varies spatially and temporally, leading to confirmation bias when errors are reinforced. Furthermore, class imbalance and boundary artifacts can exacerbate drift.\n\nOverview of Our Approach\nWe develop a confidence-aware refinement module that modulates labels by region difficulty and a texture-consistent augmentation pipeline that preserves structure while perturbing style.",
    "reason": "The span summarizes prior methods and immediately states the authors' choice without articulating the specific gap or motivation, satisfying lack of synthesis per (b) and (c).",
    "start": 293,
    "end": 817,
    "label": "Lacks synthesis"
  },
  {
    "span": "Clinical NER typically relies on de-identified notes from the 2012 i2b2 challenge.",
    "document": "Introduction\n\nClinical named entity recognition (NER) is foundational for structuring information in electronic health records, enabling downstream tasks in cohort discovery and pharmacovigilance (Uzuner et al., 2011; Wang et al., 2018). Domain adaptation and privacy constraints have motivated the development of models and datasets specific to clinical text (Johnson et al., 2016; Peng et al., 2019). Clinical NER typically relies on de-identified notes from the 2012 i2b2 challenge. However, differences in annotation guidelines and de-identification practices complicate transfer to contemporary hospital systems.\n\nWe propose a cross-institutional evaluation protocol with harmonized entity schemas and release a set of weakly supervised annotations derived from publicly available case reports. Our experiments assess robustness across institutions and note types.",
    "reason": "It references a specific dataset/source as the typical basis for clinical NER without providing a citation to support the claim.",
    "start": 403,
    "end": 485,
    "label": "Unsupported claim"
  },
  {
    "span": "Previous competitions reported that scaffold split is substantially harder than random split.",
    "document": "Related Work\n\nMolecular property prediction benchmarks commonly use graph neural networks to model atom-bond interactions, with message passing and attention mechanisms yielding strong baselines (Gilmer et al., 2017; Rong et al., 2020). Distribution shifts between training and test chemistries require careful evaluation protocols to avoid overly optimistic assessments (Yang et al., 2019).\n\nData splitting strategies such as random, scaffold, and temporal splits aim to probe generalization under varying chemical novelty. Previous competitions reported that scaffold split is substantially harder than random split. While widely assumed, the magnitude of this gap depends on label noise, molecular diversity, and task difficulty. We revisit splitting protocols and propose stratified scaffold partitioning to reduce variance across runs.",
    "reason": "It cites 'Previous competitions' and claims a comparative difficulty without providing any citation to those competitions or studies.",
    "start": 525,
    "end": 618,
    "label": "Unsupported claim"
  },
  {
    "span": "In previous studies, graph neural networks consistently outperformed HMMs on protein family classification.",
    "document": "Related Work\n\nProtein family classification has long relied on profile hidden Markov models (HMMs) and sequence alignment frameworks, which provide interpretable motif discovery and strong baselines (Eddy, 1998; Finn et al., 2015). Neural models increasingly leverage evolutionary profiles and pretraining on massive sequence corpora to capture remote homology signals (Rao et al., 2019; Elnaggar et al., 2021). Recent efforts incorporate structural cues through contact maps and residue graphs to better reflect biophysical constraints (Ingraham et al., 2019).\n\nIn previous studies, graph neural networks consistently outperformed HMMs on protein family classification. Building on this trend, we introduce a hybrid architecture that fuses profile-based emissions with graph-attentional message passing over predicted distance maps, aiming to combine HMM interpretability with GNN expressivity.",
    "reason": "Comparative performance claims about prior studies should cite the specific works and results; no citations are provided.",
    "start": 563,
    "end": 670,
    "label": "Unsupported claim"
  },
  {
    "span": "Utilizing side information in recommendation has been studied via factorization machines (Rendle, 2010), neural FM and deep interaction layers (Guo et al., 2017), sequence-aware models (Hidasi et al., 2016), and graph-based recommenders that propagate signals over user–item graphs (Wang et al., 2019; He et al., 2020). Context modeling includes time, device, and location features (Adomavicius and Tuzhilin, 2005; Quadrana et al., 2018). We propose a model that integrates side information and graph propagation.",
    "document": "Related Work\n\nRecommender systems commonly combine collaborative signals with auxiliary information to mitigate cold-start issues and improve personalization. Deep models have expanded the capacity to learn higher-order interactions and sequential patterns.\n\nUtilizing side information in recommendation has been studied via factorization machines (Rendle, 2010), neural FM and deep interaction layers (Guo et al., 2017), sequence-aware models (Hidasi et al., 2016), and graph-based recommenders that propagate signals over user–item graphs (Wang et al., 2019; He et al., 2020). Context modeling includes time, device, and location features (Adomavicius and Tuzhilin, 2005; Quadrana et al., 2018). We propose a model that integrates side information and graph propagation.\n\nCold-start and sparsity remain key challenges, and recent work explores pre-training on large-scale browsing logs (Sun et al., 2019) and knowledge-graph augmentation (Wang et al., 2018).\n\nIntroduction\n\nOur experiments span three public datasets and a large-scale e-commerce platform.",
    "reason": "The span moves directly from listing prior methods to stating the authors’ model without articulating the specific gap or novelty relative to these works, matching criterion (b)/(c).",
    "start": 259,
    "end": 772,
    "label": "Lacks synthesis"
  },
  {
    "span": "most existing datasets focus on English newswire",
    "document": "Introduction\n\nEvent extraction aims to detect structured event mentions and arguments from unstructured text. The task is challenging due to lexical variety, argument optionality, and domain shift. While recent models leverage contextual encoders and span-based decoding, performance often degrades outside the training domain.\n\nFrom a data perspective, most existing datasets focus on English newswire, limiting generalization to informal or multilingual settings. This motivates constructing resources that diversify domains and languages while maintaining consistent annotation schemes.\n\nWe propose a cross-domain benchmark spanning conversational forums, scientific abstracts, and policy documents, paired with a unified evaluation protocol.",
    "reason": "This broad claim about the dataset landscape is made without any supporting citations (rule b/e: field-wide assertions about datasets should be referenced).",
    "start": 354,
    "end": 402,
    "label": "Unsupported claim"
  },
  {
    "span": "More than 70% of mobile apps now deploy some form of on-device personalization.",
    "document": "Introduction\n\nFederated learning enables privacy-preserving model training by keeping user data local and aggregating updates on a central server. This paradigm has been adopted in predictive typing, recommendation, and healthcare scenarios where centralized data collection is infeasible or sensitive.\n\nMore than 70% of mobile apps now deploy some form of on-device personalization. Despite this rapid adoption, practical challenges remain: non-IID data, partial client participation, and personalization under strict communication budgets. Recent optimization methods address client drift and server-side adaptivity (e.g., proximal regularization and server momentum), while personalization strategies explore fine-tuning, multi-task heads, and meta-learning.\n\nWe introduce a bi-level objective that simultaneously learns a global representation and client-adaptive heads under per-round communication constraints, achieving competitive accuracy with fewer upload steps.",
    "reason": "Reports a specific statistic without any source or evidence; violates the definition (claim about statistics without citation).",
    "start": 304,
    "end": 383,
    "label": "Unsupported claim"
  },
  {
    "span": "DSTC8 introduced a new schema-guided paradigm for task-oriented dialogue",
    "document": "Related Work\n\nTask-oriented dialogue systems require modeling user intents, slots, and API interactions across domains. Earlier work focused on domain-specific ontologies, while recent efforts target scalable schema representations and zero-shot transfer. DSTC8 introduced a new schema-guided paradigm for task-oriented dialogue, prompting research on schema generalization and service description understanding.\n\nIntroduction\n\nWe build on schema-guided dialogue modeling by proposing a cross-domain encoder that jointly aligns service descriptions with latent dialogue states using contrastive objectives.",
    "reason": "Refers to a specific shared task (DSTC8) and its contribution without providing a citation.",
    "start": 256,
    "end": 328,
    "label": "Unsupported claim"
  },
  {
    "span": "Robotic manipulation has benefited from imitation learning, reinforcement learning, and hybrid methods that use demonstrations to initialize policies (Argall et al., 2009; Levine et al., 2016; Nair et al., 2018). Sim-to-real transfer techniques mitigate the reality gap through domain randomization, dynamics randomization, and adaptation modules (Tobin et al., 2017; Peng et al., 2018; James et al., 2019). Benchmark suites provide standardized evaluation across grasping and assembly tasks (Yu et al., 2020; Brockman et al., 2016).",
    "document": "Related Work\n\nGeneral-purpose manipulation policies aim to handle diverse objects and tasks with minimal retuning, a long-standing challenge due to contact-rich dynamics and partial observability (Levine et al., 2016; Kalashnikov et al., 2018).\n\nRobotic manipulation has benefited from imitation learning, reinforcement learning, and hybrid methods that use demonstrations to initialize policies (Argall et al., 2009; Levine et al., 2016; Nair et al., 2018). Sim-to-real transfer techniques mitigate the reality gap through domain randomization, dynamics randomization, and adaptation modules (Tobin et al., 2017; Peng et al., 2018; James et al., 2019). Benchmark suites provide standardized evaluation across grasping and assembly tasks (Yu et al., 2020; Brockman et al., 2016).\n\nRecent trends include modular architectures that decouple perception from control and language-conditioned policies that leverage pretrained vision-language models (Shridhar et al., 2022; Ahn et al., 2022).",
    "reason": "The span catalogs prior categories and benchmarks without integrating them into the authors' argument or stating why existing work is insufficient, violating definition a and c.",
    "start": 246,
    "end": 779,
    "label": "Lacks synthesis"
  },
  {
    "span": "Public datasets for industrial anomaly detection contain on average less than 1% anomalies.",
    "document": "Introduction\n\nIndustrial time-series anomaly detection is critical for predictive maintenance and process monitoring. Models must handle nonstationary signals, sensor noise, and severe class imbalance. While reconstruction-based methods and probabilistic forecasting are prevalent, their behavior under rare-event regimes requires careful assessment.\n\nPublic datasets for industrial anomaly detection contain on average less than 1% anomalies. This extreme imbalance challenges both training and evaluation, often inflating precision at the expense of recall when thresholds are tuned globally.\n\nWe introduce a calibration framework that adapts thresholds per context window using uncertainty estimates from a probabilistic encoder. Our benchmark aggregates multiple datasets under a common scoring protocol and reports operating points across a range of application-relevant false alarm rates.\n\nRelated Work\n\nPrior research has proposed variational autoencoders, normalizing flows, and deep state-space models for anomaly detection. Efforts to address imbalance include synthetic oversampling and cost-sensitive learning, though their impact on temporal false positive bursts remains underexplored.",
    "reason": "Presents a quantitative statistic about datasets without citing any source or evidence.",
    "start": 352,
    "end": 443,
    "label": "Unsupported claim"
  },
  {
    "span": "Brown et al., (2016)",
    "document": "Introduction\n\nContextual word representations have transformed downstream NLP performance by capturing polysemy and syntax in a unified embedding space (Peters et al., 2018; Devlin et al., 2019). Brown et al., (2016) explored early context fusion mechanisms that predate large-scale masked language modeling. Subsequent models introduced bidirectional conditioning and segment-level objectives to better exploit context (Lample and Conneau, 2019). Our approach augments token embeddings with discourse cues to capture cross-sentence coherence, which standard pretraining objectives underutilize (Martinez and Wu, 2021).\n\nWe evaluate our model on tasks requiring long-range reasoning, including coreference resolution and discourse relation classification, and show consistent gains over strong baselines.",
    "reason": "Incorrect punctuation/style in narrative citation: extra comma before the parenthetical year in \"Brown et al., (2016)\"; should be \"Brown et al. (2016)\".",
    "start": 196,
    "end": 216,
    "label": "Format"
  },
  {
    "span": "Recent competitions consistently report that byte-level models outperform phoneme-based systems by over 10% WER.",
    "document": "Introduction\n\nLow-resource automatic speech recognition (ASR) faces challenges from limited supervision and domain shift (Besacier et al., 2014; Stoian et al., 2020). Self-supervised pretraining with wav2vec-style objectives has narrowed the gap to supervised systems (Baevski et al., 2020; Hsu et al., 2021). Recent competitions consistently report that byte-level models outperform phoneme-based systems by over 10% WER. However, the interaction between tokenization schemes and language morphology remains underexplored in truly low-resource regimes.\n\nWe analyze tokenization granularity under fixed pretraining budgets across six typologically diverse languages, providing controlled comparisons absent from prior leaderboards.",
    "reason": "Claims aggregate competition outcomes and a specific statistic (>10% WER) without any citations to the competitions or reports, violating rule (a) and (d).",
    "start": 310,
    "end": 422,
    "label": "Unsupported claim"
  },
  {
    "span": "Prior medical NLP studies report F1 above 0.9 on de-identification using only rule-based methods.",
    "document": "Introduction\n\nClinical text de-identification aims to remove protected health information while preserving clinical utility. High precision is critical to minimize leakage risks, but excessive redaction can harm downstream analytics. Prior medical NLP studies report F1 above 0.9 on de-identification using only rule-based methods. Such results, however, may reflect dataset-specific artifacts and annotation conventions rather than generalizable performance.\n\nRelated Work\n\nApproaches range from hand-crafted regular expressions to sequence labeling with contextual encoders and hybrid systems that combine dictionaries with neural models. Evaluation typically occurs on institution-specific corpora, which limits cross-site generalization.\n\nOur Approach\n\nWe conduct a cross-institutional study with harmonized PHI schemas and propose a confidence-aware hybrid model that defers to rules in high-precision regimes while leveraging neural context for ambiguous cases.",
    "reason": "Cites performance statistics attributed to prior studies without providing references, violating rule (a) and (b).",
    "start": 234,
    "end": 331,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent work shows that multilingual pretraining reduces WER by up to 35% in zero-shot settings.",
    "document": "Introduction\n\nEnd-to-end ASR for low-resource languages benefits from cross-lingual transfer, pronunciation modeling, and self-supervised pretraining on untranscribed audio. Contrastive objectives on multilingual speech have narrowed the gap to supervised systems in zero-resource regimes, while shallow fusion with external language models improves decoding for morphologically rich languages (Kahn et al., 2020; Pratap et al., 2020). Recent work shows that multilingual pretraining reduces WER by up to 35% in zero-shot settings. Nevertheless, adaptation often suffers from negative transfer when source and target phonotactics diverge substantially. We address this by using phone-aware adapters and target-specific lexicon constraints during fine-tuning.\n",
    "reason": "Claims specific results from 'recent work' (a 35% reduction) without any supporting citations, violating rule (d) and (b).",
    "start": 436,
    "end": 531,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works propose contrastive learning objectives tailored to relation extraction.",
    "document": "Related Work\n\nRelation Extraction. Supervised RE has benefited from pretrained language models (Baldini Soares et al., 2019; Zhou and Chen, 2021) and distant supervision (Mintz et al., 2009). Data augmentation aims to improve robustness through adversarial or semantic transformations (Wei and Zou, 2019; Alt et al., 2020). Recent works propose contrastive learning objectives tailored to relation extraction. Despite promising results, uniform negative sampling may not reflect relation taxonomy structures.",
    "reason": "Mentions 'recent works' without providing citations to those works (violates rule d).",
    "start": 324,
    "end": 409,
    "label": "Unsupported claim"
  },
  {
    "span": "McMahan et al. (2017) introduced federated averaging for on-device training. Kairouz et al. (2021) provided a comprehensive survey of federated learning. Truex et al. (2019) analyzed privacy leakage under membership inference. Bonawitz et al. (2017) presented secure aggregation for distributed learning.",
    "document": "Introduction\n\nFederated learning enables collaborative model training without centralized data collection. Challenges include communication efficiency, privacy, and system heterogeneity.\n\nMcMahan et al. (2017) introduced federated averaging for on-device training. Kairouz et al. (2021) provided a comprehensive survey of federated learning. Truex et al. (2019) analyzed privacy leakage under membership inference. Bonawitz et al. (2017) presented secure aggregation for distributed learning.\n\nWe study personalization under partial participation and system heterogeneity.",
    "reason": "This span lists works from algorithms to surveys to privacy and cryptography without explaining their interrelations. The sentences lack transitions and do not make explicit how each cited work connects to the others, resulting in coherence issues across multiple sentences.",
    "start": 188,
    "end": 492,
    "label": "Coherence"
  },
  {
    "span": "Recent competitions have demonstrated that few-shot prompting solves most introductory programming tasks",
    "document": "Related Work\n\nProgram synthesis with large language models has progressed rapidly due to improved pretraining corpora and decoding strategies. Benchmarks range from introductory exercises to competitive programming, with evaluation sensitive to test coverage and execution-based metrics. Recent competitions have demonstrated that few-shot prompting solves most introductory programming tasks, spurring interest in curriculum design and test-time augmentation.\n\nIntroduction\n\nWe investigate robustness under distribution shifts in problem templates and hidden unit tests, analyzing failure modes that persist despite strong headline accuracies.",
    "reason": "Asserts evidence from 'recent competitions' without citing any competition reports or papers.",
    "start": 288,
    "end": 392,
    "label": "Unsupported claim"
  },
  {
    "span": "Early warning systems for student risk have applied logistic regression on demographics and grades (Smith and Patel, 2016), matrix factorization over course enrollments (Zhang and Luo, 2017), and sequence models that ingest LMS clickstreams (Thomas et al., 2019; Rivera and Lee, 2020). More recent work uses attention mechanisms for interpretability (Ahmad et al., 2021; Yoon and Choi, 2022).",
    "document": "Related Work\n\nPredicting which students are at risk of not completing a course enables timely, targeted interventions. The literature varies in signal sources, modeling capacity, and attention to interpretability for stakeholders.\n\nEarly warning systems for student risk have applied logistic regression on demographics and grades (Smith and Patel, 2016), matrix factorization over course enrollments (Zhang and Luo, 2017), and sequence models that ingest LMS clickstreams (Thomas et al., 2019; Rivera and Lee, 2020). More recent work uses attention mechanisms for interpretability (Ahmad et al., 2021; Yoon and Choi, 2022).\n\nDatasets differ widely in institutional context, privacy policies, and the granularity of engagement logs, which complicates cross-study comparison (Miller et al., 2021).",
    "reason": "The span summarizes prior methods and cites studies without linking them to the authors’ argument, perspective, or the specific research gap they will address (criteria a and c).",
    "start": 232,
    "end": 624,
    "label": "Lacks synthesis"
  },
  {
    "span": "Sequential recommenders capture evolving user interests with autoregressive transformers (Kang and McAuley, 2018). Knowledge graph embeddings inject side information through relation-aware propagation (Wang et al., 2019). Contextual bandits address exploration in interactive settings (Li et al., 2010). Fairness-aware objectives re-balance exposure across providers (Singh and Joachims, 2018).",
    "document": "Related Work\n\nRecommender systems\nCollaborative filtering traditionally learns latent factors from user–item interactions (Koren et al., 2009). Deep architectures combine content features and implicit feedback to overcome sparsity (He et al., 2017). Sequential recommenders capture evolving user interests with autoregressive transformers (Kang and McAuley, 2018). Knowledge graph embeddings inject side information through relation-aware propagation (Wang et al., 2019). Contextual bandits address exploration in interactive settings (Li et al., 2010). Fairness-aware objectives re-balance exposure across providers (Singh and Joachims, 2018).\n\nEvaluation protocols\nOffline metrics such as NDCG and Recall@K remain standard but can overestimate performance due to selection bias (Yang et al., 2018). Counterfactual estimators aim to correct for logging policies using inverse propensity scoring (Schnabel et al., 2016).\n\nOur approach studies representation learning that supports both accuracy and calibrated exposure, with a focus on robust performance under long-tail distributions.",
    "reason": "The span abruptly lists four different lines of recommender research (sequential modeling, knowledge graphs, bandits, fairness) without transitions or explanation of how they relate to each other, leaving the connections implicit and unclear.",
    "start": 250,
    "end": 644,
    "label": "Coherence"
  },
  {
    "span": "Prior bandit approaches are known to suffer from severe exposure bias in real platforms.",
    "document": "Related Work\n\nReinforcement learning has been proposed to optimize long-term user engagement in recommender systems beyond myopic click-through objectives (Zou et al., 2019; Chen et al., 2021). Counterfactual learning and off-policy evaluation aim to leverage logged bandit data under selection bias (Swaminathan and Joachims, 2015; Schnabel et al., 2016). Prior bandit approaches are known to suffer from severe exposure bias in real platforms. Recent work proposes slate-level models and variance reduction via doubly robust estimators to mitigate these issues (Ie et al., 2019; Jagerman et al., 2020). Our contribution is a deconfounded policy gradient method that jointly models user intent and platform curation signals.\n\nIntroduction\n\nWe validate on a public news recommendation dataset and a large-scale industrial A/B test, showing improvements in dwell time and satisfaction proxies.",
    "reason": "Claims a known limitation of prior approaches in real platforms without citing empirical or theoretical evidence supporting the claim.",
    "start": 357,
    "end": 445,
    "label": "Unsupported claim"
  },
  {
    "span": "Graph-based spatiotemporal models, including DCRNN (Li et al., 2018), STGCN (Yu et al., 2018), and Graph WaveNet (Wu et al., 2019), capture non-Euclidean road network dependencies.",
    "document": "Related Work\n\nTraffic forecasting systems must capture spatial correlations across road networks and temporal dynamics under variable conditions. Accurate short-term predictions support congestion mitigation and routing.\n\nGraph-based spatiotemporal models, including DCRNN (Li et al., 2018), STGCN (Yu et al., 2018), and Graph WaveNet (Wu et al., 2019), capture non-Euclidean road network dependencies. Temporal attention and dilated convolutions further improve long-horizon forecasts (Zhang et al., 2020; Pan et al., 2021). External factors such as weather and events are incorporated via auxiliary features (Lv et al., 2018; Jiang et al., 2021).\n\nWe present a dynamic graph learner that updates edge weights from streaming sensor data to better handle distribution shifts during incidents.",
    "reason": "The span lists representative models without clarifying their strengths/limitations relative to the authors' problem or motivating the need for a dynamic graph learner, meeting (a) and (c).",
    "start": 222,
    "end": 402,
    "label": "Lacks synthesis"
  },
  {
    "span": "Users prefer explanations that are contrastive rather than exhaustive.",
    "document": "Related Work\n\nExplainable AI (XAI) research seeks to make model behavior understandable to diverse stakeholders. Methods range from feature attribution and counterfactuals to example-based and rule-based explanations, each with different trade-offs in fidelity and usability.\n\nUsers prefer explanations that are contrastive rather than exhaustive. Motivated by this perspective, recent system designs prioritize highlighting minimal changes or key differences that would alter model predictions, aiming to better align with human reasoning.\n",
    "reason": "Claims a user preference about explanation style without citing user studies or empirical evidence.",
    "start": 277,
    "end": 347,
    "label": "Unsupported claim"
  },
  {
    "span": "Online evaluations in e-commerce recommenders typically show 3–5% CTR lifts when switching to deep models.",
    "document": "Introduction\n\nRecommender systems are core to e-commerce platforms, where even marginal improvements in ranking can translate into significant revenue. Traditional methods based on factorization and gradient-boosted trees have strong baselines, but recent advances in deep architectures promise richer modeling of sequence dynamics and context.\n\nOnline evaluations in e-commerce recommenders typically show 3–5% CTR lifts when switching to deep models. While such gains are compelling, deployment challenges include feature freshness, serving latency, and robust training under delayed feedback. Moreover, offline metrics such as NDCG do not always predict online performance, motivating careful experimentation.\n\nIn this work, we present a production-ready sequential recommender that combines self-attention with calibrated exploration. We detail system design choices, discuss trade-offs in candidate generation versus re-ranking, and provide a comprehensive offline–online correlation study across multiple traffic segments.",
    "reason": "Reports specific typical effect sizes without citing any studies or industry reports that substantiate the claim.",
    "start": 346,
    "end": 452,
    "label": "Unsupported claim"
  },
  {
    "span": "Most existing corpora annotate only binary interactions without directionality.",
    "document": "Related Work\n\nInformation extraction in the biomedical domain targets relations such as protein–protein interactions (PPI), disease–gene associations, and drug–drug interactions (Krallinger et al., 2017; Li et al., 2016). Neural models with domain-specific pretraining have improved over feature-based baselines (Beltagy et al., 2019; Lee et al., 2020). Corpus design strongly influences what models can learn: annotation granularity, negation, and directionality all impact downstream utility. Most existing corpora annotate only binary interactions without directionality. As a result, models trained on these resources struggle to capture causal mechanisms or regulatory roles.\n\nWe introduce a directionally annotated PPI corpus and a span-level causal labeling scheme, and present experiments showing gains on mechanism-aware extraction benchmarks.",
    "reason": "Claims a broad characteristic of existing datasets without citing any specific corpora or surveys to support it.",
    "start": 495,
    "end": 574,
    "label": "Unsupported claim"
  },
  {
    "span": "To the best of our knowledge, this is the first triply-anchored entailment dataset.",
    "document": "Introduction\n\nNatural language inference (NLI) evaluates whether a hypothesis can be inferred from a premise and has become a central benchmark for language understanding (Bowman et al., 2015; Williams et al., 2018). Many datasets annotate instance-level phenomena such as lexical overlap, negation, or world knowledge, but few provide explicit grounding to external evidence sources. To the best of our knowledge, this is the first triply-anchored entailment dataset. We pair each premise-hypothesis example with citations to three independent evidence snippets, enabling robust evaluation of multi-source justification.",
    "reason": "Asserts novelty relative to prior work without providing citations to substantiate the 'first' claim (definition b/d).",
    "start": 385,
    "end": 468,
    "label": "Unsupported claim"
  },
  {
    "span": "in (Kumar et al., 2020)",
    "document": "Related Work\n\nGraph neural networks (GNNs) aggregate neighborhood information via message passing (Kipf and Welling, 2017; Hamilton et al., 2017). However, oversmoothing limits depth and expressivity, as shown in (Kumar et al., 2020) and subsequent analyses (Oono and Suzuki, 2020; Chen et al., 2020). Regularization techniques including DropEdge and residual connections mitigate these issues (Rong et al., 2020; Li et al., 2019). Our approach revisits normalization and positional encodings for scalable training.",
    "reason": "Wrong citation style: the preposition 'in' should introduce a narrative citation 'in Kumar et al. (2020)' rather than a parenthetical one.",
    "start": 210,
    "end": 233,
    "label": "Format"
  },
  {
    "span": "Fairness in recommender systems has been studied from user-side parity (Yao and Huang, 2017; Li et al., 2021), item/provider exposure (Singh and Joachims, 2018; Biega et al., 2018), and multi-stakeholder trade-offs (Burke, 2017; Mehrotra et al., 2020).",
    "document": "Related Work\n\nAs recommenders mediate access to information and opportunity, ensuring equitable outcomes across users and providers has become a central concern. Research spans metrics, algorithms, and evaluations under realistic constraints.\n\nFairness in recommender systems has been studied from user-side parity (Yao and Huang, 2017; Li et al., 2021), item/provider exposure (Singh and Joachims, 2018; Biega et al., 2018), and multi-stakeholder trade-offs (Burke, 2017; Mehrotra et al., 2020). Auditing frameworks and simulators have been proposed to evaluate long-term impacts (Chaney et al., 2018; Mansoury et al., 2020).\n\nWe present an online re-ranking method that balances exposure with minimal regret. Experiments include counterfactual evaluation and live A/B tests.",
    "reason": "The span enumerates areas of fairness research but does not articulate the specific gap the proposed re-ranking method addresses, meeting criterion (b).",
    "start": 244,
    "end": 496,
    "label": "Lacks synthesis"
  },
  {
    "span": "There are many recent works that use transformers to fuse audio and text for multimodal sentiment.",
    "document": "Related Work\n\nMultimodal sentiment analysis integrates heterogeneous signals—text, audio, and vision—to infer user affect. Early fusion strategies concatenate features, whereas late fusion aggregates unimodal predictions; both struggle with cross-modal alignment and missing data.\n\nSelf-attention architectures enable flexible cross-modal interactions and have rapidly gained popularity. There are many recent works that use transformers to fuse audio and text for multimodal sentiment. Nevertheless, the field lacks standardized protocols for handling desynchronization and modality dropout in real-world conditions.",
    "reason": "Uses the phrase 'many recent works' without citing any of them (rule d).",
    "start": 388,
    "end": 486,
    "label": "Unsupported claim"
  },
  {
    "span": "The SyGuS 2017 competition introduced real-world benchmarks that remain widely used today.",
    "document": "Related Work\n\nProgram synthesis has benefited from standardized evaluation through community benchmarks and competitions. Constraints-based formulations and neural-guided search have both contributed to improved scalability on diverse synthesis tasks. The SyGuS 2017 competition introduced real-world benchmarks that remain widely used today. Subsequent efforts explored richer grammars and domain-specific languages to capture practical programming patterns.\n\nWe position our approach as complementary, offering a search strategy that adapts to grammar structure while leveraging learned priors.\n",
    "reason": "First mention of a specific competition and its impact requires citation to the competition reports or proceedings.",
    "start": 252,
    "end": 342,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from multiple prompts",
    "document": "Introduction\n\nAutomated Essay Scoring (AES) aims to predict holistic or trait-specific scores for student essays. Early approaches relied on handcrafted features such as lexical diversity and syntactic complexity, while more recent neural models learn representations directly from raw text. Despite strong progress, robust cross-prompt generalization remains challenging.\n\nTo contextualize our contributions, we consider both prompt-specific and cross-prompt AES. Prompt-specific models can overfit to topical cues, whereas cross-prompt models must capture writing quality independent of content. BERT was used in an AES task trained on essays from multiple prompts, yielding strong baselines, but cross-prompt performance remains limited. However, these models often lack mechanisms to calibrate uncertainty and to disentangle content from style.\n\nRelated Work\n\nNeural AES methods have explored recurrent, convolutional, and transformer-based encoders. Work on domain adaptation and adversarial invariance attempts to mitigate topic leakage. Complementary research introduces calibration techniques for regression tasks and explores ordinal regression losses to better align with the graded nature of scores. In this paper we focus on uncertainty-aware calibration for cross-prompt AES without access to prompt labels at test time.",
    "reason": "This sentence references a specific prior setup and empirical result involving BERT in AES across multiple prompts but does not provide a citation at first mention of the study.",
    "start": 598,
    "end": 666,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent work on prompt-based learning for code generation has explored discrete prompt search, soft prompt tuning, and instruction engineering for large language models (Lu et al., 2022; Lester et al., 2021; Chen et al., 2021). Discrete methods enumerate templates or use gradient-free search to find effective tokens, while soft prompts optimize continuous embeddings prepended to the input. Instruction engineering studies show that minor wording changes can substantially alter model outputs (Zhou et al., 2023; Kojima et al., 2022).",
    "document": "Related Work\n\nProgram synthesis from natural language has advanced with transformer-based decoders trained on large code corpora and aligned with textual intent (Chen et al., 2021; Austin et al., 2021). Data augmentation and execution-guided decoding further improve correctness by filtering non-compiling or failing candidates (Le et al., 2022; Zheng et al., 2023).\n\nRecent work on prompt-based learning for code generation has explored discrete prompt search, soft prompt tuning, and instruction engineering for large language models (Lu et al., 2022; Lester et al., 2021; Chen et al., 2021). Discrete methods enumerate templates or use gradient-free search to find effective tokens, while soft prompts optimize continuous embeddings prepended to the input. Instruction engineering studies show that minor wording changes can substantially alter model outputs (Zhou et al., 2023; Kojima et al., 2022).\n\nEvaluation has also broadened beyond pass-k to include robustness under paraphrase, unit-test generalization, and human preference judgments (Chowdhery et al., 2022; Li et al., 2023). Benchmarks such as HumanEval, MBPP, and CodeContests provide diverse difficulty ranges and test-time constraints (Chen et al., 2021; Austin et al., 2021; Li et al., 2022).",
    "reason": "The span summarizes multiple prior approaches and findings but does not relate them to the authors' goals or articulate any gap or perspective, matching definition a and c.",
    "start": 368,
    "end": 903,
    "label": "Lacks synthesis"
  },
  {
    "span": "Reconstruction-based detectors model normality via autoencoders (Sakurada and Yairi, 2014). Probabilistic changepoint models segment regimes over time (Adams and MacKay, 2007). Forecasting errors reveal deviations from expected trends (Hundman et al., 2018). Graph-based methods capture inter-series relations (Deng and Hooi, 2021).",
    "document": "Related Work\n\nTime series anomaly detection encompasses reconstruction, density estimation, and predictive modeling approaches. Practical systems must handle seasonality, multivariate dependencies, and non-stationarity while providing interpretable alerts.\n\nReconstruction-based detectors model normality via autoencoders (Sakurada and Yairi, 2014). Probabilistic changepoint models segment regimes over time (Adams and MacKay, 2007). Forecasting errors reveal deviations from expected trends (Hundman et al., 2018). Graph-based methods capture inter-series relations (Deng and Hooi, 2021).\n\nOur method combines graph-temporal forecasting with uncertainty-aware thresholds, improving early detection across diverse operating conditions with limited tuning.",
    "reason": "The span abruptly lists four disparate approaches with citations and offers no transitions or rationale linking them. The relationships among the methods are not made explicit, reducing coherence.",
    "start": 258,
    "end": 590,
    "label": "Coherence"
  },
  {
    "span": "Neural code summarization approaches include sequence-to-sequence encoders over ASTs, graph neural models, and pretrained code transformers (Hu et al., 2018; LeClair et al., 2019; Ahmad et al., 2020; Feng et al., 2020; Guo et al., 2020).",
    "document": "Related Work\n\nAutomatically generating natural-language summaries of source code helps developers navigate unfamiliar repositories and understand APIs. Research spans learning from paired code–comment corpora and adapting pretrained language models to software domains.\n\nNeural code summarization approaches include sequence-to-sequence encoders over ASTs, graph neural models, and pretrained code transformers (Hu et al., 2018; LeClair et al., 2019; Ahmad et al., 2020; Feng et al., 2020; Guo et al., 2020). Evaluation typically relies on n-gram overlap and retrieval-based metrics, with growing interest in human judgments (Hu et al., 2020; Lahiri et al., 2021).\n\nWe target cross-repository generalization but do not position our method relative to these architectural families in terms of inductive bias or pretraining objectives.",
    "reason": "The span lists method families with citations but does not explain their relation to the authors’ cross-repository goal or clarify what gap remains, thus lacking synthesis (definition a, c).",
    "start": 271,
    "end": 508,
    "label": "Lacks synthesis"
  },
  {
    "span": "Recent studies investigate prompting strategies to elicit reasoning from large language models (Brown et al., 2020; Liu et al., 2021; Wei et al., 2022). Chain-of-thought and self-consistency improve multi-step problem solving (Nye et al., 2021; Kojima et al., 2022; Wang et al., 2022). Mechanistic interpretability analyzes circuits and attention heads to explain behaviors (Olah et al., 2020; Nanda et al., 2023). Faithfulness metrics compare explanations to causal influence or counterfactual effects (Jacovi and Goldberg, 2020; Pruthi et al., 2020).",
    "document": "Related Work\nExplaining the decisions of large language models is central to safe deployment in high-stakes settings. Prior work spans prompt engineering for elicitation, mechanistic analysis of internals, and evaluation of explanation faithfulness and plausibility.\n\nRecent studies investigate prompting strategies to elicit reasoning from large language models (Brown et al., 2020; Liu et al., 2021; Wei et al., 2022). Chain-of-thought and self-consistency improve multi-step problem solving (Nye et al., 2021; Kojima et al., 2022; Wang et al., 2022). Mechanistic interpretability analyzes circuits and attention heads to explain behaviors (Olah et al., 2020; Nanda et al., 2023). Faithfulness metrics compare explanations to causal influence or counterfactual effects (Jacovi and Goldberg, 2020; Pruthi et al., 2020).\n\nHowever, it remains unclear how to calibrate explanation confidence while preserving task performance. We introduce a selective rationalization objective that jointly optimizes correctness and calibrated abstention on explanations, improving faithfulness-accuracy trade-offs.",
    "reason": "The span lists lines of work and citations with no integration or explicit link to a gap, and it does not articulate the author’s argument; it is descriptive rather than synthetic.",
    "start": 268,
    "end": 820,
    "label": "Lacks synthesis"
  },
  {
    "span": "most recent works have adopted hybrid sparse–dense retrieval strategies",
    "document": "Introduction\n\nRetrieval-augmented generation (RAG) in biomedical question answering aims to combine external knowledge with powerful sequence models to produce faithful, evidence-grounded answers. While early systems relied purely on sparse lexical matchers, recent neural retrievers enable semantic access to domain-specific corpora such as PubMed and clinical notes. Following this trend, most recent works have adopted hybrid sparse–dense retrieval strategies to balance precision and recall. However, the biomedical domain presents unique challenges including terminology variation, long documents, and shifting evidence bases.\n\nRelated Work\n\nClassical QA pipelines paired TF-IDF or BM25 with extractive readers (e.g., BiLSTM-based readers), later improved by pre-trained transformers for span extraction and generation. Biomedical adaptations commonly inject domain-specific vocabularies and pre-train on curated corpora, with generative models increasingly preferred for abstractive synthesis. Despite progress, evaluation remains complicated by paraphrastic references and incomplete gold annotations in biomedical datasets.",
    "reason": "This is a claim about trends in prior work ('most recent works') without providing citations to support it.",
    "start": 391,
    "end": 462,
    "label": "Unsupported claim"
  },
  {
    "span": "It is well known that the FB15k-237 dataset contains label leakage issues addressed by later work.",
    "document": "Related Work\n\nKnowledge graph completion (KGC) methods include translational models, bilinear factorization, and neural message passing approaches (Bordes et al., 2013; Trouillon et al., 2016; Schlichtkrull et al., 2018). Recent advances incorporate textual descriptions and pretrained encoders to enrich entity and relation representations (Yao et al., 2019; Wang et al., 2021).\n\nIt is well known that the FB15k-237 dataset contains label leakage issues addressed by later work. Benchmarking protocols have also evolved to discourage test contamination and encourage inductive evaluation under realistic sparsity (Ruffinelli et al., 2020; Ali et al., 2022). We follow these recommendations and additionally propose a calibration layer that aligns scores across relations with varying arity.",
    "reason": "The span asserts a commonly held belief about a specific dataset ('FB15k-237') without providing a citation at first mention, violating rule (a) and (b).",
    "start": 381,
    "end": 479,
    "label": "Unsupported claim"
  },
  {
    "span": "As reported by the competition organizers, the top systems relied on rule mining.",
    "document": "Related Work\n\nKnowledge graph completion (KGC) methods estimate missing links by leveraging observed structure and auxiliary signals. Embedding-based models project entities and relations into vector spaces, while rule-based and path-based methods capture logical regularities.\n\nCommunity benchmarks and challenges have stimulated progress by providing standardized splits and blind test sets. As reported by the competition organizers, the top systems relied on rule mining. Beyond leaderboard performance, recent research explores calibration, uncertainty estimation, and reasoning under distribution shift.",
    "reason": "References a competition and specific organizer report without citing the event or report (rule a).",
    "start": 394,
    "end": 475,
    "label": "Unsupported claim"
  },
  {
    "span": "Simultaneous localization and mapping (SLAM) methods include feature-based pipelines using keypoints and descriptors (Murphy and Hall, 2015; Campos et al., 2019), direct methods optimizing photometric error (Suter and Meyer, 2016; Engel and Wenzel, 2017), and semantic SLAM that incorporates object landmarks (Yang et al., 2018; Zhou and Li, 2020). We focus on indoor dynamic environments with moving objects.",
    "document": "Related Work\n\nRobust SLAM in real-world scenes must contend with illumination changes, texture scarcity, and dynamic elements. Numerous algorithmic families have emerged to address different aspects of the problem.\n\nSimultaneous localization and mapping (SLAM) methods include feature-based pipelines using keypoints and descriptors (Murphy and Hall, 2015; Campos et al., 2019), direct methods optimizing photometric error (Suter and Meyer, 2016; Engel and Wenzel, 2017), and semantic SLAM that incorporates object landmarks (Yang et al., 2018; Zhou and Li, 2020). We focus on indoor dynamic environments with moving objects.\n\nDatasets for evaluating dynamic SLAM vary in motion patterns, sensor modalities, and annotation detail (Park and Lee, 2020; Howard et al., 2021).",
    "reason": "The span lists prior SLAM categories and then states the authors’ focus without clarifying why existing methods are insufficient or what specific gap is addressed (criterion b) and lacks author motivation (criterion c).",
    "start": 216,
    "end": 625,
    "label": "Lacks synthesis"
  },
  {
    "span": "Recent works have shown that shared subword vocabularies consistently improve transfer for distant language pairs.",
    "document": "Related Work\n\nTransfer learning for low-resource machine translation commonly relies on parameter sharing and multilingual pretraining. A central design choice is the construction of the input vocabulary and the degree of sharing across languages. Recent works have shown that shared subword vocabularies consistently improve transfer for distant language pairs. Nevertheless, the benefits depend on segmentation granularity, tokenization heuristics, and domain overlap. Alternatives such as language-specific adapters and vocabulary partitioning have been explored to reduce negative interference, but consensus on best practices remains elusive. Our study revisits vocabulary sharing under controlled data scales and typological distances, evaluating its interaction with adapter-based specialization.",
    "reason": "Uses the phrase \"Recent works have shown\" to make a field-wide claim without providing any supporting citations.",
    "start": 248,
    "end": 362,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors claim that silver annotations are sufficient for cross-domain transfer.",
    "document": "Related Work\n\nNamed entity recognition (NER) under domain shift has been approached via feature augmentation, adversarial training, and multi-task learning (Ruder and Plank, 2018; Gui et al., 2020). Distant supervision and silver labels are often proposed to reduce annotation costs in specialized domains such as biomedicine and law (Ratner et al., 2017; Lee et al., 2019). In a previous study, the authors claim that silver annotations are sufficient for cross-domain transfer. However, concerns remain regarding noise accumulation and error propagation when labels are inferred from weak heuristics (Bekker and Goldberger, 2016).",
    "reason": "The sentence references a 'previous study' and a specific claim but does not cite the study, violating the requirement to cite prior work at first mention (rules a and e.ii).",
    "start": 375,
    "end": 479,
    "label": "Unsupported claim"
  },
  {
    "span": "BC5CDR contains 1,500 abstracts and is considered the de facto benchmark for chemical–disease NER.",
    "document": "Introduction\n\nBiomedical named entity recognition (BioNER) identifies domain-specific concepts such as diseases, chemicals, and genes, enabling downstream relation extraction and knowledge base construction. Recent transformer-based models pre-trained on biomedical corpora have advanced the field (Beltagy et al., 2019; Lee et al., 2020), while span-level classifiers and dictionary-informed decoding further improve boundary detection (Ju et al., 2018; Luo et al., 2018).\n\nBC5CDR contains 1,500 abstracts and is considered the de facto benchmark for chemical–disease NER. To assess generalization, we also evaluate on JNLPBA and NCBI-Disease using the standard train/dev/test splits. Our approach introduces an ontology-aware adapter that conditions token representations on UMLS semantic types, aiming to reduce cross-corpus drift.\n\nWe report micro-F1, entity-level precision/recall, and boundary error decompositions, and we analyze error types associated with abbreviation and coordination structures.",
    "reason": "The sentence states dataset statistics and a status claim ('de facto benchmark') without providing citations at first mention of the dataset (violates rule a and b).",
    "start": 475,
    "end": 573,
    "label": "Unsupported claim"
  },
  {
    "span": "The HSD-4M dataset has become the de facto standard for multimodal hate speech detection.",
    "document": "Related Work\n\nMultimodal hate speech detection integrates textual and visual signals to capture coded expressions and memes (Kiela et al., 2020; Gomez et al., 2020). Joint encoders and late-fusion architectures have been explored with contrastive pretraining (Zhou et al., 2021; Das et al., 2022). The HSD-4M dataset has become the de facto standard for multimodal hate speech detection. While recent benchmarks emphasize cross-domain generalization and robustness (Prabhumoye et al., 2021), questions remain about annotation consistency and cultural context.\n\nWe introduce a semi-supervised framework that leverages weak labels from moderation logs and self-training to reduce reliance on costly expert annotations.",
    "reason": "Asserts dataset primacy ('de facto standard') without any citation; per rule (a) datasets should be cited at first mention, and per (b) popularity claims need evidence.",
    "start": 298,
    "end": 387,
    "label": "Unsupported claim"
  },
  {
    "span": "Our pilot survey of 120 clinicians indicates that 78% prefer extractive notes over abstractive ones.",
    "document": "Introduction\n\nClinical note generation seeks to alleviate documentation burden while preserving accuracy and clinical utility (Shickel et al., 2018; Miotto et al., 2016). Summarization methods in the electronic health record setting must balance faithfulness to evidence with readability and time savings (Morley et al., 2018; Krishna et al., 2021). Our pilot survey of 120 clinicians indicates that 78% prefer extractive notes over abstractive ones. Motivated by this preference, we design a constrained generation framework that guarantees traceability to source spans while allowing limited paraphrasing for clarity.",
    "reason": "Presents a specific statistic without any citation or methodological reference, offering no evidence (definition: claim about statistics without citation or evidence).",
    "start": 350,
    "end": 450,
    "label": "Unsupported claim"
  },
  {
    "span": "Jones and Patel",
    "document": "Introduction\n\nVision Transformers (ViTs) have reshaped image classification by replacing convolutional inductive biases with self-attention over patch tokens (Dosovitskiy et al., 2021). Jones and Patel show that hybrid tokenization bridges data efficiency gaps between CNNs and ViTs, especially under low-data regimes, a result complementary to DeiT's distillation approach (Touvron et al., 2021). Subsequent work adapts ViTs to dense tasks via hierarchical tokens and windowed attention (Liu et al., 2021), while others explore token pruning for efficiency (Bolya et al., 2022). We investigate pretraining curricula that modulate token granularity across phases to improve transfer and robustness.\n",
    "reason": "Narrative citation missing year; should include the year as 'Jones and Patel (YEAR)'.",
    "start": 186,
    "end": 201,
    "label": "Format"
  },
  {
    "span": "GPT-style decoders have been used in program repair to predict patches from buggy code alone.",
    "document": "Related Work\n\nNeural code models increasingly leverage large-scale pretraining on source repositories to learn syntax and semantics useful for downstream tasks such as code summarization, translation, and repair (Allamanis et al., 2018; Feng et al., 2020). Edit-based approaches explicitly model changes to abstract syntax trees, while sequence models generate fixed patches conditioned on buggy contexts (Chen et al., 2019). GPT-style decoders have been used in program repair to predict patches from buggy code alone. More recent methods incorporate both static analysis and execution feedback, aligning generated edits with failing test cases (Jiang et al., 2021). Our work unifies instruction-tuned decoders with verifier-guided reranking to reduce overfitting to frequent edit patterns.\n",
    "reason": "Introduces a specific prior setup and application without citing the relevant papers, violating rule (a) and (b).",
    "start": 426,
    "end": 519,
    "label": "Unsupported claim"
  },
  {
    "span": "Kobayashi et al.",
    "document": "Introduction\n\nTransfer learning has become a cornerstone of modern NLP and CV systems, enabling models to leverage knowledge from large corpora and adapt to downstream tasks with minimal supervision (Lin and Zhao, 2016; Ortiz et al., 2018; Chen et al., 2020). According to Kobayashi et al., aligning intermediate representations across tasks can reduce overfitting in low-resource regimes while preserving generalization. Subsequent work refined this idea with task-specific adapters and improved regularization (Huang and Li, 2019; Sato and Mori, 2021). Despite these advances, the effect of domain shift on calibration remains underexplored, especially when multimodal inputs are partially missing (Gao et al., 2022). We address this gap by proposing a calibration-aware adaptation framework and benchmarking it on multilingual, multimodal datasets (Singh and Rao, 2021; Patel and Ahmed, 2023).",
    "reason": "Narrative citation missing year; should include the year as in “Kobayashi et al. (2017)” or similar.",
    "start": 273,
    "end": 289,
    "label": "Format"
  },
  {
    "span": "Prior studies analyze privacy in federated learning by demonstrating gradient inversion attacks, property inference, and membership inference against shared updates (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019). Defensive lines include differential privacy mechanisms, secure aggregation, and gradient compression (Geyer et al., 2017; Bonawitz et al., 2019; Sattler et al., 2020).",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative training over decentralized data, but gradients and model updates can leak sensitive information. This raises regulatory and practical concerns for deployments in health, finance, and mobile contexts.\n\nPrior studies analyze privacy in federated learning by demonstrating gradient inversion attacks, property inference, and membership inference against shared updates (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019). Defensive lines include differential privacy mechanisms, secure aggregation, and gradient compression (Geyer et al., 2017; Bonawitz et al., 2019; Sattler et al., 2020).\n\nWe introduce Auditable Gradient Masking (AGM), a method that selectively obscures sensitive directions while preserving task-relevant signal, accompanied by a verification protocol to assess effective privacy budgets during training.",
    "reason": "The span catalogs attacks and defenses but does not state how these relate to the proposed AGM method or which limitations motivate it (criterion a/c).",
    "start": 262,
    "end": 654,
    "label": "Lacks synthesis"
  },
  {
    "span": "Corbett and Anderson (1995) introduced Bayesian Knowledge Tracing for modeling latent mastery. Piech et al. (2015) proposed Deep Knowledge Tracing with recurrent networks. Gervet et al. (2020) compared DKT variants and regularization strategies.",
    "document": "Related Work\n\nStudent modeling aims to predict learner performance and mastery across skills, informing personalization and curriculum design. Models range from cognitive diagnostics to sequence-based approaches.\n\nCorbett and Anderson (1995) introduced Bayesian Knowledge Tracing for modeling latent mastery. Piech et al. (2015) proposed Deep Knowledge Tracing with recurrent networks. Gervet et al. (2020) compared DKT variants and regularization strategies.\n\nOur framework blends cognitive diagnosis with sequence models via structured priors.",
    "reason": "This span presents three works sequentially without transitions, failing to state how BKT relates to DKT or how comparisons inform the field. The relationships are left implicit, making the multi-sentence passage incoherent.",
    "start": 214,
    "end": 459,
    "label": "Coherence"
  },
  {
    "span": "We follow the widely adopted practice of pre-training on CC-12M before fine-tuning on downstream tasks.",
    "document": "Introduction\n\nVision–language pretraining leverages large-scale image–text pairs to learn aligned representations that transfer to retrieval, captioning, and VQA. Contrastive objectives and dual-encoder architectures have enabled efficient scaling and zero-shot classification capabilities.\n\nWe follow the widely adopted practice of pre-training on CC-12M before fine-tuning on downstream tasks. To better handle long-tail concepts, we incorporate an adaptive temperature schedule and curriculum sampling by caption perplexity.\n\nOur experiments evaluate data efficiency, domain shift to scientific figures, and robustness to spurious text in images, with ablations on tokenizer granularity and image resolution.",
    "reason": "Asserts a common community practice involving a specific dataset without citing any prior works that establish or exemplify the practice.",
    "start": 292,
    "end": 395,
    "label": "Unsupported claim"
  },
  {
    "span": "there are many recent works that explore multimodal sarcasm detection in social media",
    "document": "Introduction\n\nSarcasm detection has evolved from purely textual cues to models that leverage images, emojis, and user context. While early approaches relied on lexical incongruity and sentiment flips, newer methods integrate visual and textual signals to better capture pragmatic cues unique to online platforms. Despite this progress, detecting sarcasm across domains and languages remains challenging due to sparse explicit markers and domain-specific humor norms.\n\nIn this paper, we focus on multimodal sarcasm detection where both text and associated images are present. Specifically, we analyze attention mechanisms that align visual regions with ironic textual spans. Motivated by the observation that there are many recent works that explore multimodal sarcasm detection in social media, we revisit alignment objectives and propose a contrastive approach that emphasizes cross-modal inconsistencies.\n\nWe evaluate our method on multiple social media datasets and examine generalization to memes and short-form videos, highlighting failure modes in low-resource settings.",
    "reason": "This is a claim about recent works without providing citations to those works, violating rule (d).",
    "start": 708,
    "end": 793,
    "label": "Unsupported claim"
  },
  {
    "span": "Fully convolutional networks pioneered medical image segmentation by enabling dense pixel-wise predictions (Long et al., 2015). U-Net and its variants introduced skip connections and symmetric encoder-decoder designs that remain the default backbone for organ and lesion delineation (Ronneberger et al., 2015; Çiçek et al., 2016; Milletari et al., 2016). Subsequent work explores multi-scale feature fusion, attention gates, and residual pathways to refine boundaries and capture context (Oktay et al., 2018; Chen et al., 2018; Zhou et al., 2018).",
    "document": "Related Work\n\nMedical Image Segmentation\n\nAutomated delineation of anatomical structures is central to clinical workflows, enabling quantitative analysis and decision support. The field has evolved rapidly with deep learning, particularly encoder-decoder architectures and attention mechanisms for robust pixel-level prediction across modalities.\n\nFully convolutional networks pioneered medical image segmentation by enabling dense pixel-wise predictions (Long et al., 2015). U-Net and its variants introduced skip connections and symmetric encoder-decoder designs that remain the default backbone for organ and lesion delineation (Ronneberger et al., 2015; Çiçek et al., 2016; Milletari et al., 2016). Subsequent work explores multi-scale feature fusion, attention gates, and residual pathways to refine boundaries and capture context (Oktay et al., 2018; Chen et al., 2018; Zhou et al., 2018).\n\nBeyond architectural innovations, researchers have examined domain shift, limited annotations, and class imbalance through self-supervision, semi-supervised learning, and curriculum strategies (Bortsova et al., 2019; Bai et al., 2017; Zhou et al., 2020). Recent benchmarks focus on cross-site generalization and calibration under distributional changes (Maier-Hein et al., 2018; Karimi et al., 2020).",
    "reason": "The paragraph lists prior segmentation methods and improvements without articulating how they relate to the authors' goals, contributions, or the specific gap their work addresses.",
    "start": 348,
    "end": 895,
    "label": "Lacks synthesis"
  },
  {
    "span": "Wang and Sato (2020) introduced surrogate gradients for deep spiking networks. Kumar et al. (2021) explored temporal coding schemes for latency reduction. Ortega and Zhu (2022) proposed event-driven normalization layers. Li and Peng (2023) benchmarked energy efficiency on neuromorphic hardware.",
    "document": "Related Work\n\nSpiking neural networks (SNNs) promise low-latency, energy-efficient inference by exploiting event-driven computation. Advances span training methods, coding strategies, and hardware-software co-design.\n\nWang and Sato (2020) introduced surrogate gradients for deep spiking networks. Kumar et al. (2021) explored temporal coding schemes for latency reduction. Ortega and Zhu (2022) proposed event-driven normalization layers. Li and Peng (2023) benchmarked energy efficiency on neuromorphic hardware.\n\nConcurrent work studies conversion from analog networks to SNNs with accuracy guarantees (Miller and Xu, 2023). We contribute a calibration-aware surrogate loss that stabilizes deep SNN training across coding regimes.",
    "reason": "The span presents four distinct contributions with no connective tissue or explanation of their relationships, making the progression from training to coding to normalization to hardware abrupt and incoherent.",
    "start": 218,
    "end": 513,
    "label": "Coherence"
  },
  {
    "span": "Self-supervised pretraining has been widely explored for medical image segmentation, with contrastive objectives (Chen et al., 2020; Chaitanya et al., 2020), context restoration (Zhou et al., 2019; He et al., 2020), and masked autoencoding (Tang et al., 2022; Zhou et al., 2022). Encoder–decoder backbones such as U-Net variants remain prevalent (Ronneberger et al., 2015; Isensee et al., 2021), and transformers have recently been adapted to volumetric scans (Dosovitskiy et al., 2020; Hatamizadeh et al., 2022).",
    "document": "Related Work\n\nMedical image segmentation has benefited from advances in both convolutional and transformer-based architectures. While early approaches primarily relied on fully convolutional networks, recent work has explored hybrid designs and pretraining strategies tailored to limited annotation budgets. In this section, we summarize representative lines of work relevant to our study of data-efficient segmentation in 3D imaging.\n\nSelf-supervised pretraining has been widely explored for medical image segmentation, with contrastive objectives (Chen et al., 2020; Chaitanya et al., 2020), context restoration (Zhou et al., 2019; He et al., 2020), and masked autoencoding (Tang et al., 2022; Zhou et al., 2022). Encoder–decoder backbones such as U-Net variants remain prevalent (Ronneberger et al., 2015; Isensee et al., 2021), and transformers have recently been adapted to volumetric scans (Dosovitskiy et al., 2020; Hatamizadeh et al., 2022).\n\nAnother branch studies domain adaptation and semi-supervised learning, focusing on consistency regularization, pseudo-label refinement, and adversarial alignment (Bortsova et al., 2019; Yu et al., 2019; Chen et al., 2021). Separately, architectural choices such as attention gates and multi-scale decoders aim to capture long-range dependencies and fine structures (Oktay et al., 2018; Chen et al., 2020b).",
    "reason": "The span lists categories of prior work and citations without explaining how these approaches relate to the authors' problem setting or how their method differs, thus failing to connect literature to their argument (definition a, c).",
    "start": 436,
    "end": 949,
    "label": "Lacks synthesis"
  },
  {
    "span": "Transformer-based forecasters have been proposed with seasonal-trend decomposition, sparse attention, and frequency-domain modules (Zhou et al., 2021; Wu et al., 2021; Zeng et al., 2022).",
    "document": "Introduction\n\nAccurate time series forecasting under long horizons remains difficult due to nonstationarity, noise, and varying seasonalities. Neural architectures have evolved from RNNs to convolutional encoders and attention-based models to better capture long-range dependencies. Transformer-based forecasters have been proposed with seasonal-trend decomposition, sparse attention, and frequency-domain modules (Zhou et al., 2021; Wu et al., 2021; Zeng et al., 2022). Classical methods such as SARIMA and ETS still provide competitive baselines under certain regimes (Hyndman and Athanasopoulos, 2018).\n\nWe investigate performance on datasets exhibiting regime shifts and intermittent demand.",
    "reason": "The span presents a list of prior approaches without integrating them into the paper's argument, explaining limitations, or linking to the authors' goals (definition a) and lacks explicit perspective (definition c).",
    "start": 283,
    "end": 470,
    "label": "Lacks synthesis"
  },
  {
    "span": "MovieLens 1M is the de facto standard for measuring demographic bias in recommenders.",
    "document": "Introduction\n\nBias and fairness in recommender systems have attracted growing attention as platforms seek equitable exposure and outcomes across user groups (Burke, 2017; Ekstrand et al., 2018). Approaches include reweighting, constrained optimization, and post-hoc calibration to trade off utility and disparity (Yao and Huang, 2017; Mansoury et al., 2020).\n\nChoice of evaluation data critically shapes conclusions about fairness interventions. MovieLens 1M is the de facto standard for measuring demographic bias in recommenders. However, its coarse user attributes and entertainment-domain focus limit generalizability to other verticals with different feedback dynamics. We therefore introduce a multi-domain benchmark with richer protected attributes and standardized fairness metrics to stress-test algorithms across settings.",
    "reason": "The dataset is introduced and labeled as 'de facto standard' without providing a citation at first mention or evidence supporting that status.",
    "start": 446,
    "end": 531,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from multiple prompts.",
    "document": "Related Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Shermis and Hamner, 2013) to deep neural architectures that learn representations of content and writing quality (Taghipour and Ng, 2016; Dong et al., 2017). Pretrained language models have further improved performance on holistic scoring and trait-level evaluation (Uto et al., 2020; Ridley et al., 2021). BERT was used in an AES task trained on essays from multiple prompts. While these methods achieve competitive metrics, concerns remain regarding construct validity, fairness across demographics, and robustness to adversarial inputs.",
    "reason": "Claims a specific prior setup using BERT in AES without citing the work that introduced it (definition iii and a).",
    "start": 412,
    "end": 481,
    "label": "Unsupported claim"
  },
  {
    "span": "Meta-learning for few-shot classification has been studied through optimization-based methods such as MAML and Reptile (Finn et al., 2017; Nichol et al., 2018), metric-based methods like Matching Networks and Prototypical Networks (Vinyals et al., 2016; Snell et al., 2017), and relation-based classifiers (Sung et al., 2018). Task distribution regularization has been investigated via episodic training schemes (Ravi and Larochelle, 2017) and hierarchical priors (Grant et al., 2018). Recent methods consider transductive inference and label propagation in the embedding space (Liu et al., 2019; Dhillon et al., 2019).",
    "document": "Related Work\n\nFew-shot learning aims to generalize from a handful of labeled examples by leveraging prior experience across tasks. Approaches differ in how they parameterize inductive biases and how they adapt to novel classes at test time.\n\nMeta-learning for few-shot classification has been studied through optimization-based methods such as MAML and Reptile (Finn et al., 2017; Nichol et al., 2018), metric-based methods like Matching Networks and Prototypical Networks (Vinyals et al., 2016; Snell et al., 2017), and relation-based classifiers (Sung et al., 2018). Task distribution regularization has been investigated via episodic training schemes (Ravi and Larochelle, 2017) and hierarchical priors (Grant et al., 2018). Recent methods consider transductive inference and label propagation in the embedding space (Liu et al., 2019; Dhillon et al., 2019).\n\nWe focus on robustness to domain shift by calibrating prototypes with uncertainty-aware adaptation, leveraging a simple Bayesian linear head that preserves fast test-time updates.",
    "reason": "The span enumerates categories and representative works but does not articulate how they connect to, differ from, or motivate the authors' proposed method (definition a and c).",
    "start": 242,
    "end": 861,
    "label": "Lacks synthesis"
  },
  {
    "span": "According to (Zhang et al., 2020)",
    "document": "Introduction\n\nAlgorithmic fairness has been studied across classification and ranking with group- and individual-level criteria (Hardt et al., 2016; Dwork et al., 2012). According to (Zhang et al., 2020), post-processing can mitigate disparities without retraining, but may hurt calibration. In contrast, in-processing methods optimize fairness-aware losses (Zafar et al., 2017; Cotter et al., 2019) yet can be sensitive to hyperparameters.\n\nWe propose a plug-in approach that preserves calibration while improving both demographic parity and equalized odds on multiple datasets.",
    "reason": "Wrong narrative style using a parenthetical citation after 'According to'. It should read 'According to Zhang et al. (2020)' rather than 'According to (Zhang et al., 2020)'.",
    "start": 170,
    "end": 203,
    "label": "Format"
  },
  {
    "span": "The FLORES devtest has 1k sentences per language.",
    "document": "Introduction\n\nEvaluation in low-resource machine translation increasingly relies on multilingual test suites that balance domain coverage and typological diversity (Koehn and Knowles, 2017; Guzmán et al., 2019). Robustness across languages requires careful selection of prompts and references to avoid lexical bias (Smith et al., 2020). The FLORES devtest has 1k sentences per language. While widely used, these sets may not reflect inflectional complexity or pragmatic phenomena prominent in underrepresented families. We introduce FLEURS-Lite, a complementary suite focusing on morphologically rich and agglutinative languages with calibrated difficulty.\n\nRelated Work\n\nData augmentation via back-translation and multilingual finetuning have reduced gaps for low-resource pairs (Sennrich et al., 2016; Aharoni et al., 2019). New evaluation sets target dialectal variation and domain shift (Khabsa et al., 2021).",
    "reason": "States a specific statistic about a benchmark split without citing the source that defines the dataset.",
    "start": 337,
    "end": 386,
    "label": "Unsupported claim"
  },
  {
    "span": "Miller et al. 1",
    "document": "Introduction\n\nPersonalized recommendation models have evolved from matrix factorization (Koren et al., 2009) to neural ranking architectures (He et al., 2017; Covington et al., 2016). While implicit feedback is abundant, unbiased learning remains difficult due to exposure confounding (Schnabel et al., 2016). Miller et al. 1 demonstrate the importance of counterfactual evaluation for top-k ranking, motivating propensity-based corrections.\n\nRelated Work\n\nRecent advances include inverse propensity scoring (Swaminathan and Joachims, 2015), self-normalized objectives (Guo et al., 2021), and slate bandits for page-level optimization (Ie et al., 2019). Our framework integrates deconfounding with sequential user modeling to handle evolving preferences.",
    "reason": "Wrong use of footnote formatting in a citation. 'Miller et al. 1' uses a superscript-like number instead of a year or proper citation; it should be a standard citation such as 'Miller et al. (2019)' or a properly formatted footnote.",
    "start": 310,
    "end": 325,
    "label": "Format"
  },
  {
    "span": "Classical domain adaptation for sentiment leverages pivot features to bridge domains (Blitzer et al., 2007; Pan et al., 2010). Adversarial approaches learn domain-invariant representations to reduce distribution shift (Ganin et al., 2016; Bousmalis et al., 2017; Chen et al., 2018). Meta-learning and metric learning enable rapid adaptation from few labeled examples (Finn et al., 2017; Snell et al., 2017; Bao et al., 2020). Prompt-based methods with pretrained language models further reduce label requirements by reformulating classification as cloze-style prediction (Gao et al., 2021; Schick and Schutze, 2021). In this paper, we introduce SentAdapt, a simple meta-learning framework for few-shot cross-domain sentiment.",
    "document": "Introduction\n\nTransferring sentiment classifiers across domains remains challenging due to shifts in lexical cues and label distributions. Although large pretrained language models have improved zero-shot performance, few-shot adaptation is often required to meet practical accuracy targets in new domains.\n\nClassical domain adaptation for sentiment leverages pivot features to bridge domains (Blitzer et al., 2007; Pan et al., 2010). Adversarial approaches learn domain-invariant representations to reduce distribution shift (Ganin et al., 2016; Bousmalis et al., 2017; Chen et al., 2018). Meta-learning and metric learning enable rapid adaptation from few labeled examples (Finn et al., 2017; Snell et al., 2017; Bao et al., 2020). Prompt-based methods with pretrained language models further reduce label requirements by reformulating classification as cloze-style prediction (Gao et al., 2021; Schick and Schutze, 2021). In this paper, we introduce SentAdapt, a simple meta-learning framework for few-shot cross-domain sentiment.\n\nWe evaluate SentAdapt on six domain transfers spanning product reviews, social media, and support tickets, and we provide an empirical study of initialization strategies that impact few-shot generalization.",
    "reason": "The paragraph lists prior approaches and then immediately states the authors' contribution without explicitly identifying the gap or how the proposed framework addresses limitations in the cited work.",
    "start": 308,
    "end": 1033,
    "label": "Lacks synthesis"
  },
  {
    "span": "Transformer-based recommenders are now the de facto standard in session-based recommendation.",
    "document": "Introduction\n\nSession-based recommendation predicts the next item based on short user interaction sequences, where inductive bias for order and recency is crucial (Hidasi et al., 2016; Quadrana et al., 2018). Methods have evolved from factorization and RNNs to attention architectures that better capture long-range dependencies (Kang and McAuley, 2018; Sun et al., 2019). Transformer-based recommenders are now the de facto standard in session-based recommendation. We revisit this assumption by benchmarking lightweight convolutional mixers that offer competitive accuracy with reduced latency.",
    "reason": "A broad field-level claim about current 'de facto standard' lacks citations to representative works supporting the statement, violating rule (b) and (d).",
    "start": 373,
    "end": 466,
    "label": "Unsupported claim"
  },
  {
    "span": "There are many recent works that explore hierarchical topic modeling with transformers.",
    "document": "Introduction\n\nTopic modeling has traditionally relied on probabilistic generative models such as Latent Dirichlet Allocation (LDA) to discover latent themes in document collections (Blei et al., 2003). More recently, neural topic models leverage variational autoencoders and contextual embeddings to improve coherence and interpretability (Miao et al., 2016; Dieng et al., 2020). Pretrained language models like BERT have also been adapted to topic discovery via document-level representations and attention-based clustering (Zhao et al., 2021).\n\nThere are many recent works that explore hierarchical topic modeling with transformers. However, despite these advances, systematic evaluations across domains remain limited, and most studies report results on a narrow suite of news or academic corpora. Our work complements prior neural approaches by proposing a simple and scalable hierarchy induction method with strong zero-shot generalization.",
    "reason": "Vague reference to 'many recent works' without any citations violates the requirement that mentions of recent works be backed up by references.",
    "start": 547,
    "end": 634,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors claim that code-mixed NER benefits from phonetic transliteration",
    "document": "Related Work\n\nNamed entity recognition (NER) in code-mixed social media text is complicated by non-standard spellings, transliteration, and frequent language switches (Aguilar et al., 2018). Approaches commonly incorporate subword modeling, bilingual lexicons, and context normalization to mitigate sparsity (Pires et al., 2019; Jain et al., 2020). In a previous study, the authors claim that code-mixed NER benefits from phonetic transliteration. However, the evidence for how such transformations interact with neural subword tokenizers remains mixed, especially in noisy, low-resource settings. Concurrent efforts explore multilingual pretraining and adversarial language identification as auxiliary tasks to improve robustness at code-switch points (Wu and Dredze, 2019; Raganato et al., 2020).\n\nOur work situates within this landscape by examining tokenization strategies and phonetic normalizers under varying degrees of lexical noise. We also probe error types to assess whether transliteration primarily aids boundary detection or label semantics.",
    "reason": "This sentence references a specific prior study and its claim without citing the study at first mention, violating rule (a).",
    "start": 349,
    "end": 446,
    "label": "Unsupported claim"
  },
  {
    "span": "The MedNotes dataset contains over 3 million de-identified clinical notes",
    "document": "Related Work\n\nClinical NLP research leverages a variety of corpora to extract entities, relations, and phenotypes from electronic health records (Uzuner et al., 2011; Wang et al., 2018). Domain adaptation and privacy-preserving learning are crucial in this space due to institutional variability and sensitive content.\n\nThe MedNotes dataset contains over 3 million de-identified clinical notes and supports both entity recognition and temporal relation extraction. Prior work has highlighted its diversity across departments and note types, making it a challenging benchmark for generalization. Our approach focuses on scalable pre-training with constrained vocabularies to mitigate PHI leakage.",
    "reason": "This sentence introduces a specific dataset with statistics but provides no citation to the dataset or paper describing it (rule a and example i).",
    "start": 320,
    "end": 393,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors claim that dictionary augmentation improves recall by 12 points.",
    "document": "Related Work\n\nBiomedical named entity recognition (BioNER) has progressed from CRF-based models with hand-engineered features (Settles, 2004) to neural architectures that combine character and word representations (Lample et al., 2016) and, more recently, domain-pretrained transformers (Lee et al., 2020; Beltagy et al., 2019). Benchmarks such as JNLPBA (Kim et al., 2004) and BC5CDR (Li et al., 2016) highlight the challenges of boundary detection and cross-corpus generalization.\n\nExternal knowledge, such as ontologies and dictionaries, has been integrated through gazetteer features, attention over knowledge graphs, and post-hoc entity linking (Peters et al., 2019; Liu et al., 2020). In a previous study, the authors claim that dictionary augmentation improves recall by 12 points. Despite reported benefits, these approaches can introduce precision errors when lexicons are incomplete or ambiguous.\n\nOur work revisits lexicon integration for BioNER by decoupling boundary detection and normalization. We propose a constrained decoding strategy that softly conditions on lexicon matches, aiming to retain recall gains while mitigating false positives.",
    "reason": "Claims a result from a prior study with a specific statistic without providing a citation (rule b and example ii).",
    "start": 691,
    "end": 788,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an automated essay scoring task trained on holistic scores from human raters.",
    "document": "Related Work\n\nAutomated essay scoring (AES) seeks to predict human writing scores using linguistic and semantic features. Traditional AES models rely on handcrafted features and regression, while recent approaches leverage pretrained language models to encode essays.\n\nBERT was used in an automated essay scoring task trained on holistic scores from human raters. Subsequent work explores domain adaptation across prompts and the integration of discourse-aware representations to improve reliability and fairness of scores.\n",
    "reason": "Describes a specific application of a model to a task without citing the corresponding study that introduced this setup.",
    "start": 269,
    "end": 363,
    "label": "Unsupported claim"
  },
  {
    "span": "We build upon the MultiDocQuest 2025 shared task introduced last year.",
    "document": "Related Work\n\nMulti-document question answering (MDQA) requires reasoning over evidence scattered across multiple sources. Prior efforts study retrieval-augmented generation, citation grounding, and verification to ensure factuality. We build upon the MultiDocQuest 2025 shared task introduced last year. That benchmark emphasizes faithful aggregation, requiring systems to both locate diverse sources and justify their answers with traceable evidence.\n\nOur method extends retrieval with cross-document entailment constraints and confidence-weighted aggregation. We compare against strong generative baselines and ablate each component for its contribution to faithfulness.",
    "reason": "Unsupported claim because it references a specific shared task at first mention without any citation or link to the task (definition a).",
    "start": 234,
    "end": 304,
    "label": "Unsupported claim"
  },
  {
    "span": "Identifiability of latent factors is established under group transformations (Khemakhem et al., 2020). Beta-VAE encourages disentanglement via capacity control (Higgins et al., 2017). Independent mechanisms suggest modular changes across environments (Scholkopf et al., 2021). Domain adaptation mitigates distribution shift with invariance (Arjovsky et al., 2019).",
    "document": "Related Work\n\nCausal representation learning seeks encodings whose dimensions correspond to stable, intervenable factors of variation. Progress spans theoretical identifiability, inductive biases for disentanglement, and learning invariances across environments to improve generalization.\n\nIdentifiability of latent factors is established under group transformations (Khemakhem et al., 2020). Beta-VAE encourages disentanglement via capacity control (Higgins et al., 2017). Independent mechanisms suggest modular changes across environments (Scholkopf et al., 2021). Domain adaptation mitigates distribution shift with invariance (Arjovsky et al., 2019).\n\nWe focus on environment-labeled datasets and propose a contrastive objective that aligns interventions with latent axes while preserving downstream predictive performance.",
    "reason": "The span lists four separate lines of work with no transitions or explicit links, leaving the relationships among identifiability, disentanglement, independent mechanisms, and domain adaptation implicit.",
    "start": 290,
    "end": 654,
    "label": "Coherence"
  },
  {
    "span": "Unsupervised domain adaptation for medical segmentation has leveraged adversarial learning, self-ensembling, and style transfer (Kamnitsas et al., 2017; French et al., 2018; Yoo et al., 2019; Chen et al., 2020).",
    "document": "Related Work\n\nDomain shift in medical imaging. Differences in scanners, protocols, and patient populations degrade segmentation performance when models are deployed out of distribution (Wen et al., 2020; Karani et al., 2018). Unsupervised domain adaptation for medical segmentation has leveraged adversarial learning, self-ensembling, and style transfer (Kamnitsas et al., 2017; French et al., 2018; Yoo et al., 2019; Chen et al., 2020). Semi-supervised approaches further exploit limited target labels or pseudo-labels to improve boundary consistency (Bortsova et al., 2019; Perone and Cohen-Adad, 2019).\n\nWe consider test-time adaptation where access to target labels is unavailable during deployment. Our method updates normalization statistics using self-supervised equivariance constraints derived from anatomy-preserving augmentations.",
    "reason": "The sentence lists categories of prior UDA methods without articulating how they fall short for test-time adaptation or how the new method addresses those shortcomings, thus lacking synthesis (criteria a and b).",
    "start": 226,
    "end": 437,
    "label": "Lacks synthesis"
  },
  {
    "span": "Following the GRU4Rec line, later papers typically adopt cross-entropy loss with time-based negatives.",
    "document": "Related Work\n\nSession-based recommendation models user actions within short anonymous sessions, emphasizing order and short-term intent (Hidasi et al., 2016; Quadrana et al., 2018). Recurrent networks, self-attention, and graph-based encoders have all been applied, with improvements in capturing item transitions and context (Li et al., 2017; Wu et al., 2019; Wang et al., 2020). Negative sampling and loss design are known to affect both ranking quality and training stability (Rendle et al., 2009; BPR; He et al., 2017).\n\nFollowing the GRU4Rec line, later papers typically adopt cross-entropy loss with time-based negatives. However, recent studies suggest alignment between the sampling distribution and deployment exposure is crucial for unbiased evaluation. Contrastive learning objectives have also been explored to improve representation quality under sparse feedback (Sohn, 2016; Xie et al., 2020).\n\nWe unify these perspectives by proposing a calibrated negative sampler aligned with exposure while maintaining efficient training.",
    "reason": "Asserts a common practice in subsequent literature without citing any supporting works.",
    "start": 525,
    "end": 627,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES task trained on TOEFL essays.",
    "document": "Related Work\n\nAutomated Essay Scoring\n\nNeural approaches to automated essay scoring (AES) have progressed from surface features to deep contextual representations. Early work combined lexical richness and syntactic complexity features in regression models (Attali and Burstein, 2006), while recent neural models utilize pre-trained language encoders to capture coherence and argumentation quality (Ke et al., 2019). BERT was used in an AES task trained on TOEFL essays. Other lines of work explore prompt-specific calibration and domain adaptation across essay sets with differing rubrics (Uto et al., 2020; Ridley et al., 2021). Our study examines cross-prompt generalization under limited labeled data.",
    "reason": "Mentions a specific model and dataset setup from prior work without citing the source at first mention, violating rule a and example iii.",
    "start": 416,
    "end": 469,
    "label": "Unsupported claim"
  },
  {
    "span": "Large language models have been adapted for code generation using in-context learning, instruction tuning, and reinforcement learning with human feedback (Brown et al., 2020; Ouyang et al., 2022; Chen et al., 2021). Benchmarks such as HumanEval, MBPP, and MultiPL-E are widely used to assess functional correctness (Chen et al., 2021; Austin et al., 2021; Cassano et al., 2022).",
    "document": "Introduction\n\nNeural code generation has advanced rapidly with large language models capable of synthesizing functions from natural language. Yet, models often exploit spurious cues, struggle with compositional generalization, and overfit to benchmark artifacts.\n\nLarge language models have been adapted for code generation using in-context learning, instruction tuning, and reinforcement learning with human feedback (Brown et al., 2020; Ouyang et al., 2022; Chen et al., 2021). Benchmarks such as HumanEval, MBPP, and MultiPL-E are widely used to assess functional correctness (Chen et al., 2021; Austin et al., 2021; Cassano et al., 2022).\n\nWe focus on robustness to prompt perturbations and semantic variations. Our contribution is a counterfactual augmentation framework that enforces specification adherence via unit-test contrast pairs, improving out-of-domain generalization.",
    "reason": "The span catalogs adaptation strategies and benchmarks without relating them to robustness or explaining shortcomings the paper addresses, so it lacks synthesis per (a) and (b).",
    "start": 264,
    "end": 642,
    "label": "Lacks synthesis"
  },
  {
    "span": "Brown et al. 1",
    "document": "Introduction\n\nPretraining with massive corpora has enabled models to learn general-purpose linguistic knowledge that can be adapted to diverse tasks (Radford et al., 2019; Brown et al. 1). However, naive fine-tuning can lead to catastrophic forgetting and calibration drift (Kirkpatrick et al., 2017; Desai and Durrett, 2020). To mitigate these issues, parameter-efficient tuning and selective layer freezing have been extensively studied (Houlsby et al., 2019; Lester et al., 2021). In this work, we analyze the interplay between selective adaptation and post-hoc calibration under distribution shifts encountered in real-world deployments (Ovadia et al., 2019; Zhang et al., 2022).",
    "reason": "Wrong use of footnotes/footnote marker in place of a proper citation; the year is missing and the superscript-like “1” should not appear.",
    "start": 172,
    "end": 186,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claimed that hierarchical attention always improves performance on long documents.",
    "document": "Introduction\n\nDocument-level classification requires modeling discourse structure and the interactions of distant sentences. Hierarchical encoders offer a natural inductive bias by separating sentence-level and document-level representations. In a previous study, the authors claimed that hierarchical attention always improves performance on long documents. Yet, empirical outcomes vary widely with dataset characteristics such as label granularity, document length distribution, and domain shift.\n\nRelated Work\n\nResearch has explored memory-augmented transformers, sparse attention, and segment-wise recurrence to scale to long inputs. At the same time, efficient token selection and routing mechanisms aim to preserve critical evidence while reducing computation. Few works, however, rigorously analyze the failure modes of hierarchical architectures under distribution shift and adversarial truncation.\n\nContributions\n\nWe present a controlled benchmark with matched label entropy across length buckets and propose a diagnostics suite probing sensitivity to evidence localization. Our results suggest that design choices in pooling, positional encoding, and cross-sentence gating are pivotal in practice.",
    "reason": "References a specific \"previous study\" and its claim without citing or identifying the work, violating rule (a) and example (ii).",
    "start": 243,
    "end": 358,
    "label": "Unsupported claim"
  },
  {
    "span": "Prior work demonstrated that fuzzing discovers 60% more bugs than symbolic execution on real-world binaries.",
    "document": "Introduction\n\nAutomated vulnerability discovery techniques span fuzzing, concolic testing, and static analysis, each with distinct coverage and scalability trade-offs (Cadar and Sen, 2013; Manès et al., 2019). Hybrid approaches aim to combine the breadth of fuzzing with the path reasoning strengths of symbolic execution.\n\nPrior work demonstrated that fuzzing discovers 60% more bugs than symbolic execution on real-world binaries. Yet such comparisons often differ in time budgets, sanitizers, and ground-truth deduplication criteria. We present a fair benchmarking suite that standardizes these factors and reports statistically robust outcomes.",
    "reason": "Makes a quantitative comparative claim about prior work ('60% more bugs') but provides no citation to the study supporting it.",
    "start": 324,
    "end": 432,
    "label": "Unsupported claim"
  },
  {
    "span": "(Singh et al., 2016",
    "document": "Introduction\n\nContext-aware recommendation integrates signals such as time, location, and social ties into preference modeling. Matrix factorization with side information (Torres and Mei, 2015) provides a strong baseline, while sequence models capture evolving user intents (Ahmed and Krishnan, 2018). Recent surveys (Chen, 2015; Kim and Ojo, 2018) discuss scalability and cold-start challenges, and (Singh et al., 2016 review neural architectures that fuse context via attention and gating. Complementary efforts personalize exploration strategies under uncertainty (Bello and Tran, 2019). We propose a lightweight context encoder that interfaces with any backbone and improves top-k precision under strict latency constraints.",
    "reason": "Missing closing parenthesis in a parenthetical citation; should be '(Singh et al., 2016)'.",
    "start": 400,
    "end": 419,
    "label": "Format"
  },
  {
    "span": "Contrastive objectives learn instance discrimination without labels (Chen et al., 2020). Predictor-target asymmetry avoids collapse (Grill et al., 2020). Self-distillation with attention emerges in ViT backbones (Caron et al., 2021). Masked image modeling reconstructs missing patches (He et al., 2022).",
    "document": "Related Work\n\nSelf-supervised learning (SSL) for visual representations has advanced rapidly, enabling pretraining without annotations and benefiting many downstream tasks.\n\nContrastive objectives learn instance discrimination without labels (Chen et al., 2020). Predictor-target asymmetry avoids collapse (Grill et al., 2020). Self-distillation with attention emerges in ViT backbones (Caron et al., 2021). Masked image modeling reconstructs missing patches (He et al., 2022).\n\nWe analyze when generative masking complements discriminative invariances under data scarcity.",
    "reason": "The four sentences enumerate different SSL families without connective tissue or explanation of how they relate to each other, producing abrupt shifts and implicit relationships only.",
    "start": 174,
    "end": 477,
    "label": "Coherence"
  },
  {
    "span": "The CoNLL 2003 shared task established standard evaluation protocols for sequence labeling.",
    "document": "Related Work\n\nSequence labeling underpins core NLP tasks such as part-of-speech tagging and named entity recognition. Standardized datasets and evaluation protocols have enabled consistent comparison across architectures. The CoNLL 2003 shared task established standard evaluation protocols for sequence labeling. Subsequent efforts extended these benchmarks to multilingual and domain-specific settings, spurring advances in transfer learning and domain adaptation.\n\nWe build upon this line by examining label noise and annotation drift over time, proposing calibration-aware training that remains robust under varying noise levels.",
    "reason": "Mentions a specific shared task and its role without citing the task or dataset publication.",
    "start": 222,
    "end": 313,
    "label": "Unsupported claim"
  },
  {
    "span": "(Barton et al., 1",
    "document": "Related Work\n\nResearch on vision–language compositionality has explored structured datasets and probing methods. Early VQA systems relied on attention over CNN features (Patel and Roy, 2019; Chen et al., 2020) but struggled with compositional splits (Hudson and Manning, 2019). To diagnose shortcut learning, several studies proposed counterfactual evaluation and falsification tests (Barton et al., 1; Moreno, 2018), as well as controlled grammar-based generation (Santos and Li, 2021). More recent work leverages program-guided reasoning to disentangle skills (Okeke et al., 2022; Ruan and Kafle, 2023). Our study differs by introducing a lightweight suite of perturbations targeted at relational predicates while keeping the visual evidence constant.",
    "reason": "Stray footnote marker '1' appears inside the parenthetical citation, which is not a valid author–year format.",
    "start": 384,
    "end": 401,
    "label": "Format"
  },
  {
    "span": "we follow the evaluation protocol of the GLUE leaderboard",
    "document": "Related Work\n\nGeneral-purpose language understanding benchmarks catalyzed rapid progress by standardizing tasks and metrics (Bowman et al., 2015; Williams et al., 2018). Pretrained transformers improved transfer across tasks via finetuning and prompt-based adaptation (Peters et al., 2018; Liu et al., 2020). For fair comparison, we follow the evaluation protocol of the GLUE leaderboard. Prior work also examines few-shot and multitask extensions that stress calibration and robustness (Wang et al., 2019; Hendrycks et al., 2020).",
    "reason": "References a specific benchmark and its protocol without citing the GLUE paper or leaderboard documentation at first mention.",
    "start": 330,
    "end": 387,
    "label": "Unsupported claim"
  },
  {
    "span": "A previous study showed a 12% mAP gain when replacing anchor-based heads with anchor-free ones on COCO.",
    "document": "Related Work\n\nModern object detectors can be grouped into anchor-based and anchor-free paradigms. Anchor-based methods such as Faster R-CNN and RetinaNet rely on predefined priors and classification-regression heads (Ren et al., 2015; Lin et al., 2017). Anchor-free approaches instead predict object centers or keypoints, simplifying design and potentially improving localization (Tian et al., 2019; Zhou et al., 2019).\n\nA previous study showed a 12% mAP gain when replacing anchor-based heads with anchor-free ones on COCO. However, these gains may conflate improvements from scale-agnostic feature pyramids and stronger training schedules. We revisit this comparison under controlled data and compute budgets.",
    "reason": "Claims a specific quantitative improvement ('12% mAP gain on COCO') attributed to a prior study but provides no citation to that study.",
    "start": 421,
    "end": 524,
    "label": "Unsupported claim"
  },
  {
    "span": "Hospitals typically deploy FedAvg due to its simplicity.",
    "document": "Introduction\n\nFederated learning enables collaborative training without sharing raw data, which is crucial for medical imaging across institutions (McMahan et al., 2017; Rieke et al., 2020). Real-world deployments must handle heterogeneity in scanners, protocols, and labelers (Li et al., 2020; Yeganeh et al., 2020). Hospitals typically deploy FedAvg due to its simplicity. However, naive averaging suffers when client data distributions are highly skewed, motivating personalization and robust aggregation (Li et al., 2021; Karimireddy et al., 2020). We propose a curriculum-based aggregation to address clinical heterogeneity.",
    "reason": "Claims widespread real-world use of a specific algorithm without evidence or citation.",
    "start": 318,
    "end": 374,
    "label": "Unsupported claim"
  },
  {
    "span": "recent works show that sequence-level knowledge distillation improves student fluency",
    "document": "Related Work\n\nKnowledge distillation for neural machine translation (NMT) transfers knowledge from a strong teacher to a compact student through softened targets or privileged data. Distillation can reduce latency and memory while retaining accuracy, making it attractive for on-device translation.\n\nIn practice, recent works show that sequence-level knowledge distillation improves student fluency, often by training the student on the teacher’s argmax translations rather than the original references. Complementary techniques include curriculum schedules, temperature tuning, and intermediate representation matching.\n\nOur study revisits sequence-level distillation under domain shift, proposing a teacher-ensemble curriculum that better preserves adequacy while retaining fluency gains.",
    "reason": "The phrase \"recent works show\" introduces a claim about prior literature without any citations to those works (rule d: mentions of recent works must be accompanied by citations).",
    "start": 313,
    "end": 398,
    "label": "Unsupported claim"
  },
  {
    "span": "Previous studies have shown that adapter layers outperform full fine-tuning on NER in low-resource settings.",
    "document": "Related Work\n\nParameter-efficient transfer learning aims to reduce the cost of adapting large language models while maintaining competitive downstream performance. Among these techniques, adapter tuning, prefix tuning, and LoRA have received considerable attention for their favorable trade-offs.\n\nPrevious studies have shown that adapter layers outperform full fine-tuning on NER in low-resource settings. This suggests that isolating task-specific parameters can lead to better generalization when labeled data are scarce, possibly by regularizing representation drift in the backbone.\n\nHowever, the evidence across tasks is mixed, and methodological differences (e.g., tokenization, entity boundary conventions) confound comparisons.",
    "reason": "Claims prior results (“previous studies have shown…”) without citing the studies (violates rule b).",
    "start": 298,
    "end": 406,
    "label": "Unsupported claim"
  },
  {
    "span": "Aggregation rules mitigate client drift (Karimireddy et al., 2020). Personalization adapts global models to local data (Arivazhagan et al., 2019). Communication compression reduces uplink overhead (Konečný et al., 2016). Secure aggregation protects updates from the server (Bonawitz et al., 2017).",
    "document": "Related Work\n\nFederated learning studies training across decentralized data while preserving privacy and minimizing communication (McMahan et al., 2017). Practical deployments face heterogeneity, stragglers, and privacy threats.\n\nAggregation rules mitigate client drift (Karimireddy et al., 2020). Personalization adapts global models to local data (Arivazhagan et al., 2019). Communication compression reduces uplink overhead (Konečný et al., 2016). Secure aggregation protects updates from the server (Bonawitz et al., 2017).\n\nWe consider the joint effect of heterogeneity and compression on convergence under partial participation.",
    "reason": "The list covers four distinct aspects of federated learning without transitions or articulation of their interdependencies, resulting in abrupt topic shifts and unclear relationships between citations.",
    "start": 230,
    "end": 527,
    "label": "Coherence"
  },
  {
    "span": "Most prior work trains on MIMIC-III and MIMIC-IV, often without external validation.",
    "document": "Related Work\n\nPredictive modeling on electronic health records (EHR) commonly targets in-hospital mortality, length of stay, and early warning of clinical deterioration. Deep sequence models and gradient boosting methods have both been widely explored for these tasks.\n\nMost prior work trains on MIMIC-III and MIMIC-IV, often without external validation. As a result, reported gains may not transfer to different institutions, coding practices, or patient populations.\n\nWe address this gap by evaluating models across three independent hospital systems and analyzing covariate shift at the feature and outcome levels.",
    "reason": "The statement references specific datasets (MIMIC-III/MIMIC-IV) and characterizes prior literature practices but provides no citations, which are required at first mention of datasets and claims about prior work.",
    "start": 270,
    "end": 354,
    "label": "Unsupported claim"
  },
  {
    "span": " (RoBERTa Liu et al. (2019))",
    "document": "Introduction\n\nPretrained transformer models have drastically improved performance across many NLP benchmarks by leveraging large-scale unsupervised corpora (Devlin et al., 2019; Radford et al., 2019; Vaswani et al., 2017). Recent studies explore domain-adaptive pretraining and task-specific fine-tuning to close the gap between generic representations and downstream needs (Gururangan et al., 2020). We fine-tune transformer encoders (RoBERTa Liu et al. (2019)) alongside task adapters (Houlsby et al., 2019) to examine sample efficiency under low-resource conditions. In addition, we compare vanilla fine-tuning to parameter-efficient strategies to understand stability and generalization (Raffel et al., 2020).",
    "reason": "The citation is incorrectly formatted with nested parentheses and model name inside the parentheses. It should be a narrative citation like \"RoBERTa (Liu et al., 2019)\" or a purely parenthetical one like \"(Liu et al., 2019)\" without the extra parentheses.",
    "start": 434,
    "end": 462,
    "label": "Format"
  },
  {
    "span": "We evaluate on the SemEval Aspect-Based Sentiment Analysis shared task.",
    "document": "Introduction\n\nAspect-based sentiment analysis (ABSA) decomposes sentiment prediction into aspect extraction and sentiment classification, enabling fine-grained opinion mining in product and service reviews (Pontiki et al., 2014). Pre-trained language models with task-specific prompts and constrained decoding have achieved strong performance by leveraging implicit knowledge while preserving aspect faithfulness (Devlin et al., 2019; Li et al., 2021).\n\nOur approach introduces a structure-aware adapter that conditions on aspect spans and syntactic dependencies, improving robustness to aspect paraphrasing and negation. We evaluate on the SemEval Aspect-Based Sentiment Analysis shared task. Additionally, we test on cross-domain ABSA benchmarks to assess generalization beyond in-domain restaurant and laptop reviews.\n\nWe analyze error types related to aspect boundary detection and sentiment polarity under compositional negation.",
    "reason": "The shared task is mentioned for the first time without any citation; per the guideline, first mentions of shared tasks should include a reference.",
    "start": 622,
    "end": 693,
    "label": "Unsupported claim"
  },
  {
    "span": "The 2021 CLOTH shared task established that distractor quality is the primary determinant of system ranking.",
    "document": "Background and Related Work\n\nCloze-style reading comprehension for language learners evaluates both linguistic knowledge and pragmatic inference. The CLOTH dataset offers challenging, teacher-authored blanks with plausible distractors that target specific error types (Xia et al., 2018). Subsequent work explored pretraining objectives and distractor generation strategies to improve robustness.\n\nThe 2021 CLOTH shared task established that distractor quality is the primary determinant of system ranking. Building on this observation, we propose a controllable distractor generator conditioned on learner profile and target skill, and we measure its impact on both absolute accuracy and error-type calibration.",
    "reason": "Mentions a specific 'shared task' outcome without citing the shared task report or proceedings.",
    "start": 397,
    "end": 505,
    "label": "Unsupported claim"
  },
  {
    "span": "Knowledge distillation transfers dark knowledge from a large teacher to a smaller student via soft targets (Hinton et al., 2015). Weight pruning removes redundant parameters to compress models (Leung and Xie, 2017). Quantization reduces precision to shrink model size (Park et al., 2018).",
    "document": "Related Work\n\nDeploying neural networks under resource constraints has motivated model compression and acceleration techniques. Knowledge distillation transfers dark knowledge from a large teacher to a smaller student via soft targets (Hinton et al., 2015). Weight pruning removes redundant parameters to compress models (Leung and Xie, 2017). Quantization reduces precision to shrink model size (Park et al., 2018). Neural architecture search has also been explored for compact designs (Cai et al., 2019). While these strands are often combined in practice, few works study their joint effects under strict latency budgets; we address this gap.",
    "reason": "The span enumerates distillation, pruning, and quantization as separate sentences without explaining their relationships or transitions, leaving unclear how each cited work relates to the previous one.",
    "start": 128,
    "end": 416,
    "label": "Coherence"
  },
  {
    "span": "Prompt engineering with few-shot exemplars improves code generation accuracy on HumanEval (Brown et al., 2020; Chen et al., 2021). Chain-of-thought prompts elicit intermediate reasoning steps (Wei et al., 2022). Self-consistency samples multiple candidates and selects majority programs (Wang et al., 2022). CodeT5 pretraining on code-search pairs yields better representations (Wang et al., 2021). Program repair models learn edit sequences from bug-fix commits (Tufano et al., 2019).",
    "document": "Introduction\n\nLarge Language Models for Program Synthesis\n\nProgram synthesis with large language models (LLMs) has rapidly advanced through scaling, instruction tuning, and specialized pretraining on code. Despite strong zero-shot capabilities, models still struggle with specification adherence, multi-step reasoning, and execution-time correctness (Austin et al., 2021; Le et al., 2022). Benchmarks like HumanEval and MBPP evaluate functional correctness using unit tests, but coverage and robustness remain open concerns (Chen et al., 2021; Austin et al., 2021).\n\nPrompt engineering with few-shot exemplars improves code generation accuracy on HumanEval (Brown et al., 2020; Chen et al., 2021). Chain-of-thought prompts elicit intermediate reasoning steps (Wei et al., 2022). Self-consistency samples multiple candidates and selects majority programs (Wang et al., 2022). CodeT5 pretraining on code-search pairs yields better representations (Wang et al., 2021). Program repair models learn edit sequences from bug-fix commits (Tufano et al., 2019).\n\nIn contrast to prior work focusing on prompting and pretraining, we propose an execution-guided refinement loop that couples test generation with iterative synthesis, prioritizing failing traces and type constraints. This design aims to reduce hallucinated APIs and improve generalization across tasks with sparse specifications.",
    "reason": "The span enumerates disparate areas (prompting, decoding strategies, pretraining, program repair) without transitions or an explicit linking rationale, making the relationship between cited works unclear and disrupting coherence.",
    "start": 567,
    "end": 1052,
    "label": "Coherence"
  },
  {
    "span": "Diarization pipelines typically adopt x-vector or ECAPA embeddings with PLDA or spectral clustering back-ends (Snyder et al., 2018; Desplanques et al., 2020; Park et al., 2019; Wang et al., 2020).",
    "document": "Introduction\n\nSpeaker diarization aims to determine “who spoke when” in multi-speaker audio. Robust diarization remains challenging in far-field, noisy, and overlapping speech conditions.\n\nDiarization pipelines typically adopt x-vector or ECAPA embeddings with PLDA or spectral clustering back-ends (Snyder et al., 2018; Desplanques et al., 2020; Park et al., 2019; Wang et al., 2020). Overlap handling is often addressed with separate detection modules or EEND-style models (Bullock et al., 2020; Fujita et al., 2019). Recently, constrained agglomerative clustering with resegmentation has improved performance in broadcast domains (Zhang et al., 2019; Garcia-Romero et al., 2017).\n\nWe present experiments on meeting corpora with varied microphone arrays and examine runtime-memory trade-offs.",
    "reason": "The span merely catalogs standard embeddings and clustering choices without stating how they inform the authors’ approach or what limitation motivates their study, lacking synthesis and perspective (definition a, c).",
    "start": 189,
    "end": 385,
    "label": "Lacks synthesis"
  },
  {
    "span": "Univariate anomaly detection techniques include seasonal decomposition, robust statistics, and spectral methods (Candes et al., 2011; Hyndman and Athanasopoulos, 2018; Hochenbaum et al., 2017). Deep methods for multivariate series use autoencoders, VAEs, and graph models (Malhotra et al., 2015; Xu et al., 2018; Deng and Hooi, 2021). Forecasting-based detectors compare predicted and observed residuals (Salinas et al., 2020; Rangapuram et al., 2018).",
    "document": "Related Work\n\nDetecting anomalies in time series is critical for monitoring industrial systems and online services. Approaches span statistics, signal processing, and machine learning, and are differentiated by assumptions on stationarity, seasonality, and dimensionality.\n\nUnivariate anomaly detection techniques include seasonal decomposition, robust statistics, and spectral methods (Candes et al., 2011; Hyndman and Athanasopoulos, 2018; Hochenbaum et al., 2017). Deep methods for multivariate series use autoencoders, VAEs, and graph models (Malhotra et al., 2015; Xu et al., 2018; Deng and Hooi, 2021). Forecasting-based detectors compare predicted and observed residuals (Salinas et al., 2020; Rangapuram et al., 2018).\n\nWe develop a detector that conditions on topology-aware context and offers calibrated scores across entities.\n\nResults on AIOps benchmarks indicate improved early detection under concept drift.",
    "reason": "Violates (a): provides a catalogue of approaches without synthesizing their limitations or linking them to the paper’s proposed detector.",
    "start": 274,
    "end": 726,
    "label": "Lacks synthesis"
  },
  {
    "span": "and there are many recent works that explore controllable toxicity mitigation in open-domain chatbots.",
    "document": "Related Work\n\nOpen-domain dialogue models have benefited from large-scale pretraining but remain susceptible to unsafe and toxic outputs (Xu et al., 2020; Dinan et al., 2021). Safety interventions include data filtering, post-hoc classifiers, and reinforcement learning with human feedback (Kim et al., 2022; Bai et al., 2022). and there are many recent works that explore controllable toxicity mitigation in open-domain chatbots. Orthogonal approaches leverage decoding constraints and attribute control to steer generation (Dathathri et al., 2020; Krause et al., 2021).\n\nOur approach unifies classifier guidance with constraint-aware decoding under a single probabilistic framework, enabling plug-and-play safety across backbone models.",
    "reason": "Uses the phrase 'many recent works' without providing citations to any of them, violating rule (d).",
    "start": 328,
    "end": 430,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works have shown that prompt-tuning outperforms full fine-tuning for low-resource NER.",
    "document": "Introduction\n\nNamed entity recognition (NER) in low-resource settings remains challenging due to limited labeled data and domain drift between training and deployment environments. Parameter-efficient adaptation methods aim to leverage large pretrained language models while updating only a small subset of parameters to reduce overfitting and computational cost. Recent works have shown that prompt-tuning outperforms full fine-tuning for low-resource NER. Motivated by these trends, we explore a constrained optimization view of prompt selection that balances task performance with calibration.\n\nWe evaluate our approach across several domains with limited supervision and analyze its robustness under varying prompt budgets. Our results suggest that carefully selected prompts can close much of the gap to full fine-tuning while requiring fewer updates and less compute.",
    "reason": "Unsupported claim because it references unspecified 'recent works' demonstrating a result without providing citations (definition d).",
    "start": 364,
    "end": 457,
    "label": "Unsupported claim"
  },
  {
    "span": "The first shared task on counterfactual reasoning for QA standardized the evaluation with a 3,000-example test set",
    "document": "Introduction\n\nCounterfactual reasoning in question answering has drawn growing interest, as models must identify minimal edits that alter answers while maintaining narrative coherence. Benchmarks that stress counterfactual understanding can better reveal brittle decision paths and reliance on spurious cues.\n\nThe first shared task on counterfactual reasoning for QA standardized the evaluation with a 3,000-example test set and provided adversarially constructed contrast pairs. Subsequent studies report that counterfactual training improves robustness to distribution shift. Yet, the construction methods and annotation protocols vary widely across datasets, complicating cross-benchmark comparison.\n\nWe propose a controlled counterfactual generation pipeline that explicitly balances semantic preservation and causal intervention, enabling consistent evaluation across diverse QA formats.",
    "reason": "References a specific shared task and dataset setup without citing the task organizers or dataset paper, violating rule (a).",
    "start": 310,
    "end": 424,
    "label": "Unsupported claim"
  },
  {
    "span": "there are many recent works that explore cross-lingual summarization with minimal supervision",
    "document": "Related Work\n\nCross-lingual summarization seeks to produce concise summaries in a target language given source documents in another language. Early research focused on pipeline approaches that coupled machine translation with monolingual summarization, but this often propagated translation errors and led to stylistic inconsistency. With the maturation of multilingual pretraining, end-to-end models capable of joint translation and summarization have become attractive. However, the scarcity of high-quality parallel summarization data and the variability of discourse structures across languages remain key challenges.\n\nIn commercial applications such as global news monitoring and multilingual customer support, latency and cost constraints demand techniques that reduce reliance on extensive parallel corpora. Consequently, semi- and weakly supervised approaches have drawn growing attention in both academia and industry. In particular, there are many recent works that explore cross-lingual summarization with minimal supervision, emphasizing pseudo-parallel construction and alignment-free objectives. Yet the field lacks a consensus on evaluation practices: metrics are unevenly applied across languages, and reference summaries vary widely in length and style.\n\nOur study contributes a standardized evaluation protocol and a multilingual validation set curated for consistency in style and compression ratio. We also analyze the trade-offs between translation fidelity and summary salience under limited supervision.",
    "reason": "The span claims the existence of 'many recent works' without providing any citations to support the assertion, violating the requirement to cite prior work at first mention.",
    "start": 943,
    "end": 1036,
    "label": "Unsupported claim"
  },
  {
    "span": "Self-supervised pretraining for multilingual ASR has advanced with wav2vec 2.0 and XLSR for cross-lingual representation sharing (Baevski et al., 2020; Conneau et al., 2021). Large-scale weakly supervised models like Whisper demonstrate robustness to noise and accents (Radford et al., 2023). CTC/attention hybrid decoders and RNN-T have been explored for multilingual decoding (Graves et al., 2006; Kim et al., 2017; Rao et al., 2017). Domain adaptation uses language-specific adapters and feature normalization (Bapna et al., 2019; Chen et al., 2021).",
    "document": "Related Work\n\nMultilingual automatic speech recognition (ASR) seeks to leverage shared acoustic-phonetic structure while handling language-specific pronunciation, script, and morphology differences.\n\nSelf-supervised pretraining for multilingual ASR has advanced with wav2vec 2.0 and XLSR for cross-lingual representation sharing (Baevski et al., 2020; Conneau et al., 2021). Large-scale weakly supervised models like Whisper demonstrate robustness to noise and accents (Radford et al., 2023). CTC/attention hybrid decoders and RNN-T have been explored for multilingual decoding (Graves et al., 2006; Kim et al., 2017; Rao et al., 2017). Domain adaptation uses language-specific adapters and feature normalization (Bapna et al., 2019; Chen et al., 2021).\n\nOur work explores parameter-efficient adapter routing to improve low-resource languages without retraining the full encoder.",
    "reason": "The span summarizes prior methods but does not explain their relation to the paper's adapter routing approach or specify the unresolved issue it addresses (definition a and c).",
    "start": 200,
    "end": 753,
    "label": "Lacks synthesis"
  },
  {
    "span": "Time series forecasting has progressed from classical ARIMA and exponential smoothing to deep learning with recurrent and temporal convolutional networks (Box and Jenkins, 1970; Hyndman et al., 2008; Borovykh et al., 2017; Bai et al., 2018). Decomposition-based architectures and trend-seasonality modeling improve interpretability (Oreshkin et al., 2019; Oreshkin et al., 2020; Qin et al., 2017). Transformer variants aim to handle long-range dependencies via sparse attention, auto-correlation, and patching strategies (Zhou et al., 2021; Wu et al., 2021; Nie et al., 2022; Liu et al., 2023).",
    "document": "Related Work\n\nLong-horizon forecasting in multivariate time series is essential for capacity planning, energy management, and traffic control. Models must capture scale, seasonality, and cross-variable interactions under distribution shift.\n\nTime series forecasting has progressed from classical ARIMA and exponential smoothing to deep learning with recurrent and temporal convolutional networks (Box and Jenkins, 1970; Hyndman et al., 2008; Borovykh et al., 2017; Bai et al., 2018). Decomposition-based architectures and trend-seasonality modeling improve interpretability (Oreshkin et al., 2019; Oreshkin et al., 2020; Qin et al., 2017). Transformer variants aim to handle long-range dependencies via sparse attention, auto-correlation, and patching strategies (Zhou et al., 2021; Wu et al., 2021; Nie et al., 2022; Liu et al., 2023).\n\nOur approach introduces frequency-aware tokenization and multi-scale attention to stabilize long-horizon predictions under covariate shift.",
    "reason": "The span aggregates prior forecasting methods and Transformer variants without relating them to the authors' aims or explaining what deficiency remains, thereby lacking synthesis.",
    "start": 242,
    "end": 836,
    "label": "Lacks synthesis"
  },
  {
    "span": "On the Librispeech test-clean split, recent end-to-end systems report WER below 2% without LM fusion.",
    "document": "Introduction\n\nEnd-to-End Speech Recognition. End-to-end ASR with CTC, attention, and transducer objectives has simplified training and deployment while closing the gap to hybrid HMM systems (Graves et al., 2006; Chan et al., 2016; He et al., 2019). Self-supervised pretraining from raw audio further improves low-resource and noisy-condition performance (Baevski et al., 2020; Hsu et al., 2021). On the Librispeech test-clean split, recent end-to-end systems report WER below 2% without LM fusion. Complementary techniques like SpecAugment, shallow fusion, and confidence calibration remain standard in production pipelines (Park et al., 2019; Kannan et al., 2018). We propose a monotonic chunkwise joint training objective that improves long-utterance stability with negligible decoding overhead.",
    "reason": "Provides a specific benchmark result without any citation or evidence, which requires supporting references.",
    "start": 396,
    "end": 497,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent shared tasks have focused on low-resource translation for Indo-Aryan languages.",
    "document": "Introduction\n\nNeural machine translation (NMT) has achieved impressive performance in high-resource languages with large-scale parallel corpora (Bahdanau et al., 2015; Vaswani et al., 2017). For low-resource settings, transfer learning, multilingual training, and data augmentation are widely explored (Zoph et al., 2016; Neubig and Hu, 2018). Recent shared tasks have focused on low-resource translation for Indo-Aryan languages. We investigate how adapter-based multilingual fine-tuning can close the gap without overfitting to scarce data.",
    "reason": "Mentions 'recent shared tasks' without citing the tasks or their reports (rule d).",
    "start": 344,
    "end": 430,
    "label": "Unsupported claim"
  },
  {
    "span": "Causal inference in observational healthcare data has leveraged propensity scores, inverse probability weighting, and doubly robust estimators (Rosenbaum and Rubin, 1983; Robins et al., 2000; Bang and Robins, 2005). Representation learning approaches seek balanced covariates via adversarial learning and matching (Johansson et al., 2016; Shalit et al., 2017; Hassanpour and Greiner, 2019). Sequence models estimate treatment effects over time with recurrent and transformer architectures (Lim et al., 2018; Bica et al., 2020; Nie et al., 2021).",
    "document": "Introduction\n\nEstimating treatment effects from observational healthcare records is essential for decision support when randomized trials are infeasible. However, confounding, missingness, and time-varying policies complicate identification and estimation.\n\nCausal inference in observational healthcare data has leveraged propensity scores, inverse probability weighting, and doubly robust estimators (Rosenbaum and Rubin, 1983; Robins et al., 2000; Bang and Robins, 2005). Representation learning approaches seek balanced covariates via adversarial learning and matching (Johansson et al., 2016; Shalit et al., 2017; Hassanpour and Greiner, 2019). Sequence models estimate treatment effects over time with recurrent and transformer architectures (Lim et al., 2018; Bica et al., 2020; Nie et al., 2021).\n\nWe propose a counterfactual sequence model with missingness-aware balancing.\n\nWe test on multiple ICU datasets and report improvements in PEHE and ATE error.",
    "reason": "Violates (a) and (b): summarizes prior streams of work without synthesizing their shortcomings or stating the specific gap before presenting the contribution.",
    "start": 258,
    "end": 803,
    "label": "Lacks synthesis"
  },
  {
    "span": "Most traffic sign datasets fail to include nighttime conditions.",
    "document": "Related Work\n\nTraffic sign recognition has advanced rapidly with the availability of annotated benchmarks and convolutional architectures (Ciresan et al., 2012; Sermanet and LeCun, 2011). Public benchmarks commonly provide diverse sign categories and high-resolution annotations, enabling robust detection and classification pipelines under daylight scenes (Houben et al., 2013; Stallkamp et al., 2012). Data augmentation and domain adaptation further improve robustness to weather and viewpoint changes (Mogelmose et al., 2012).\n\nMost traffic sign datasets fail to include nighttime conditions. As a result, models trained on these corpora often struggle with glare, sensor noise, and low-luminance artifacts, limiting deployment in 24/7 intelligent transportation systems. We address this by curating a balanced, multi-country corpus emphasizing nocturnal driving scenarios and evaluating domain generalization from day to night.",
    "reason": "Generalization about the coverage of existing datasets requires citations to the datasets and evidence; no supporting references are provided.",
    "start": 531,
    "end": 595,
    "label": "Unsupported claim"
  },
  {
    "span": "Kipf and Welling (2017) propose GCNs for semi-supervised node classification. Hamilton et al. (2017) sample neighborhoods in GraphSAGE. Velickovic et al. (2018) introduce GAT with attention on edges. Ying et al. (2018) use hierarchical pooling in DiffPool.",
    "document": "Related Work\n\nGraph neural networks (GNNs) extend deep learning to non-Euclidean structures, enabling representation learning over nodes, edges, and subgraphs. Central design choices include neighborhood aggregation, attention mechanisms, and pooling strategies for hierarchical tasks.\n\nKipf and Welling (2017) propose GCNs for semi-supervised node classification. Hamilton et al. (2017) sample neighborhoods in GraphSAGE. Velickovic et al. (2018) introduce GAT with attention on edges. Ying et al. (2018) use hierarchical pooling in DiffPool.\n\nOur contribution is an uncertainty-aware aggregator that adapts receptive fields based on epistemic estimates, improving robustness on sparse and heterophilic graphs.",
    "reason": "The span enumerates seminal GNN papers without transitions or explaining how aggregation, sampling, attention, and pooling relate or contrast. The relationship is only implied, creating abrupt shifts across multiple sentences (criterion a, b, c).",
    "start": 287,
    "end": 543,
    "label": "Coherence"
  },
  {
    "span": "Transformer-based models have quickly surpassed classical baselines like ARIMA on retail demand forecasting datasets",
    "document": "Introduction\n\nTime series forecasting under intermittent demand and promotion effects is central to retail operations. Classical models such as ARIMA and exponential smoothing remain competitive, but recent approaches leverage deep sequence models to capture long-range dependencies and exogenous variables (Bandara et al., 2020).\n\nTransformer-based models have quickly surpassed classical baselines like ARIMA on retail demand forecasting datasets. However, their gains can diminish under severe data sparsity and cold-start conditions. We address these limitations by coupling hierarchical reconciliation with representation sharing across related series.",
    "reason": "This is a comparative performance claim about prior work that should be supported with citations to studies demonstrating it (rule a).",
    "start": 332,
    "end": 448,
    "label": "Unsupported claim"
  },
  {
    "span": "Smith et al. (2020) introduce a copy-augmented transformer for knowledge-grounded chat. Li and Zhao (2019) pre-train on Reddit to improve response diversity. Xu et al. (2021) align dialog states with retrieved triples from ConceptNet. Chen et al. (2020) fine-tune BART with persona conditioning.",
    "document": "Related Work\n\nKnowledge-grounded dialogue generation has evolved from template-based systems to neural architectures that attempt to ground responses in external sources such as knowledge graphs or retrieved passages. Early approaches focused on retrieval-augmented encoders that incorporate entity mentions and linked triples to reduce hallucination and improve factuality.\n\nSmith et al. (2020) introduce a copy-augmented transformer for knowledge-grounded chat. Li and Zhao (2019) pre-train on Reddit to improve response diversity. Xu et al. (2021) align dialog states with retrieved triples from ConceptNet. Chen et al. (2020) fine-tune BART with persona conditioning.\n\nIn contrast, our approach emphasizes controllable grounding signals that allow the system to selectively cite and verify evidence before generation. We also consider uncertainty estimation to defer or clarify answers when retrieved knowledge is sparse or inconsistent.",
    "reason": "The cited works are listed in consecutive sentences without transitions or explanation of how each relates to the others or to a common theme. The relationship is implied but not made explicit, creating abrupt shifts between studies (criterion a and b), and the issue spans multiple sentences (criterion c).",
    "start": 376,
    "end": 671,
    "label": "Coherence"
  },
  {
    "span": "users prefer explanations that are contrastive and counterfactual.",
    "document": "Related Work\n\nExplainable AI in Interactive Systems\n\nHuman-centered evaluation of explanations examines usefulness, trust, and decision efficacy. In behavioral studies, users prefer explanations that are contrastive and counterfactual. Interface designs that support selective detail and interactive what-if analysis tend to improve task performance. Our work integrates explanation generation with preference learning to tailor content to user goals and cognitive load.",
    "reason": "Presents a prior empirical finding about user preferences without citing the supporting studies (rule b).",
    "start": 169,
    "end": 235,
    "label": "Unsupported claim"
  },
  {
    "span": "Several methods treat text as covariates via bag-of-words or embeddings to adjust for confounding (Roberts et al., 2020; Veitch et al., 2021; Keith et al., 2020). In our approach, we consider textual proxies.",
    "document": "Related Work\n\nCausal inference with high-dimensional unstructured data poses conceptual and computational hurdles. Text has been incorporated to approximate confounders, mediate effects, or define outcomes, but each role entails distinct assumptions. Several methods treat text as covariates via bag-of-words or embeddings to adjust for confounding (Roberts et al., 2020; Veitch et al., 2021; Keith et al., 2020). In our approach, we consider textual proxies. Other studies leverage topic models for interpretability or use representation learning tailored for balance (Paul and Dredze, 2012; Johansson et al., 2016).\n\nWe study policy documents where unobserved institutional factors may correlate with both text and decisions.",
    "reason": "The span lists prior methods and then states the authors' approach in generic terms without connecting to a precise deficiency in existing work or clarifying how their approach differs (definition b) and lacks articulated motivation (definition c).",
    "start": 251,
    "end": 459,
    "label": "Lacks synthesis"
  },
  {
    "span": "(Nguyen 2021)",
    "document": "Related Work\n\nContinual learning methods address catastrophic forgetting through replay, regularization, and dynamic architectures (Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017). Lightweight replay buffers are effective when privacy or storage constraints limit access to original data (Aljundi et al., 2019). Parameter isolation methods assign disjoint subspaces or masks to tasks, but can overfit when tasks are numerous (Serra et al., 2018). Recent surveys (Nguyen 2021) discuss the trade-offs among these approaches but focus less on low-shot regimes. Our work complements this by introducing an uncertainty-aware rehearsal strategy that prioritizes boundary examples during replay (Zhao and Wang, 2022).",
    "reason": "Missing comma between author and year in parenthetical author–year citation; should be \"(Nguyen, 2021)\".",
    "start": 469,
    "end": 482,
    "label": "Format"
  },
  {
    "span": "Benchmarks such as CodeSearchNet, FunCom, and CONCODE have enabled ML research on code summarization, with encoder-decoder baselines based on RNNs and Transformers (Husain et al., 2019; LeClair and McMillan, 2019; Iyer et al., 2018; Ahmad et al., 2020).",
    "document": "Introduction\n\nNeural Code Summarization\nAutomatically generating natural language summaries of source code helps developers navigate large repositories. Datasets and models have evolved rapidly with the advent of transformer architectures.\n\nBenchmarks and Baselines\nBenchmarks such as CodeSearchNet, FunCom, and CONCODE have enabled ML research on code summarization, with encoder-decoder baselines based on RNNs and Transformers (Husain et al., 2019; LeClair and McMillan, 2019; Iyer et al., 2018; Ahmad et al., 2020).\n\nThis Work\nWe study cross-repository generalization for code summarization under domain shift.",
    "reason": "Lists datasets and baselines without articulating how they relate to the authors’ focus on domain shift or what shortcomings motivate the study (definition a/c).",
    "start": 266,
    "end": 519,
    "label": "Lacks synthesis"
  },
  {
    "span": "Most prior art on automated feedback generation evaluates on only two publicly available MOOC datasets.",
    "document": "Introduction\n\nAutomated feedback generation aims to provide timely, personalized guidance to learners at scale. Natural language generation approaches must balance correctness, specificity, and pedagogical usefulness.\n\nMost prior art on automated feedback generation evaluates on only two publicly available MOOC datasets. This narrow focus limits generalization and risks overfitting to dataset-specific artifacts.\n\nWe introduce a cross-course benchmark that spans diverse subjects and assignment types, and propose an evaluation framework that measures both linguistic quality and learning outcomes via simulated interventions.",
    "reason": "This is a claim about the scope of prior evaluations involving datasets without citing the works or datasets, violating rules (a) and (b).",
    "start": 219,
    "end": 322,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent studies show that epsilon values above 8 provide no meaningful privacy.",
    "document": "Introduction\n\nDifferential privacy (DP) provides formal guarantees limiting the influence of any single example on the output of a computation (Dwork et al., 2006). In machine learning, DP-SGD and related mechanisms add calibrated noise to gradients or objective terms to bound privacy loss under composition (Abadi et al., 2016; Mironov, 2017). The practical trade-off between privacy and utility depends on the dataset, model capacity, and number of training steps.\n\nRecent studies show that epsilon values above 8 provide no meaningful privacy. However, interpreting epsilon requires context about the threat model, composition, and post-processing, and different applications tolerate different privacy budgets. We therefore report a range of budgets and analyze utility/privacy Pareto frontiers for common NLP benchmarks.\n\nRelated Work\n\nWorks on privacy in language models have examined memorization, exposure metrics, and privacy amplification by subsampling (Carlini et al., 2019; Brown et al., 2022; Feldman and Zhang, 2020). Techniques for private fine-tuning and prompt learning seek stronger utility under fixed budgets (Li et al., 2021; Yu et al., 2022). Our contribution is an evaluation protocol that standardizes accounting and compares multiple optimizers under identical clipping regimes.",
    "reason": "Invokes 'recent studies' to make a strong quantitative claim about privacy without citing any specific studies (rule d).",
    "start": 469,
    "end": 547,
    "label": "Unsupported claim"
  },
  {
    "span": "Safety in conversational agents has been explored through taxonomies of risks and harms (Bender et al., 2021; Weidinger et al., 2022), detoxification via controlled generation and decoding constraints (Gehman et al., 2020; Krause et al., 2021; Welbl et al., 2021), post hoc filtering and classifiers (Dinan et al., 2019; Xu et al., 2020), and reinforcement learning from human feedback to align outputs (Stiennon et al., 2020; Bai et al., 2022). Existing benchmarks measure toxicity, bias, and adherence to safety policies across domains (Sun et al., 2022; Vidgen et al., 2021; Dinan et al., 2022). We curate a new dataset of safety-critical dialogs spanning medical, legal, and financial settings.",
    "document": "Introduction\n\nLarge language models deployed in open-domain conversation must satisfy safety requirements while preserving helpfulness. Ensuring safe behavior across diverse domains and intents remains a moving target as models and use cases evolve.\n\nSafety in conversational agents has been explored through taxonomies of risks and harms (Bender et al., 2021; Weidinger et al., 2022), detoxification via controlled generation and decoding constraints (Gehman et al., 2020; Krause et al., 2021; Welbl et al., 2021), post hoc filtering and classifiers (Dinan et al., 2019; Xu et al., 2020), and reinforcement learning from human feedback to align outputs (Stiennon et al., 2020; Bai et al., 2022). Existing benchmarks measure toxicity, bias, and adherence to safety policies across domains (Sun et al., 2022; Vidgen et al., 2021; Dinan et al., 2022). We curate a new dataset of safety-critical dialogs spanning medical, legal, and financial settings.\n\nWe further propose an evaluation protocol that stresses domain transfer and prompt-injection robustness, reporting results for both base and instruction-tuned models.",
    "reason": "The paragraph enumerates prior work and then announces a dataset contribution without articulating the gap in existing taxonomies, methods, or benchmarks that the new dataset addresses.",
    "start": 251,
    "end": 949,
    "label": "Lacks synthesis"
  },
  {
    "span": "It is widely known that continued training on in-domain data causes catastrophic forgetting in neural machine translation.",
    "document": "Related Work\n\nDomain adaptation in neural machine translation (NMT) often proceeds via continued training (fine-tuning) on in-domain parallel data to improve relevance and terminology (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Other approaches include multi-domain learning, mixture-of-experts, and data selection to balance domain coverage (Kobus et al., 2017; Britz et al., 2017; Wang et al., 2019).\n\nIt is widely known that continued training on in-domain data causes catastrophic forgetting in neural machine translation. To address this, regularization-based methods penalize deviation from out-of-domain parameters, while rehearsal strategies interleave out-of-domain batches to preserve general capability. We revisit these solutions under constrained in-domain regimes and propose a curriculum that leverages term frequency to prioritize stability.\n\nOur experiments cover IT, medical, and legal domains with both high- and low-resource language pairs, reporting cross-domain BLEU and terminology accuracy.",
    "reason": "The sentence presents a prior-work claim ('widely known' effect) without citing any evidence; claims about established phenomena should be supported by references (violates rule b).",
    "start": 417,
    "end": 539,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent studies have proposed spectral methods, deep autoencoders, and contrastive objectives for anomaly detection in graphs (Xu et al., 2018; Ding et al., 2019; Song et al., 2021). Others incorporate topology-aware attention or reconstruction losses to flag suspicious nodes (Zheng and Li, 2020; Park et al., 2022).",
    "document": "Related Work\n\nGraph anomaly detection has gained traction due to applications in fraud detection, cybersecurity, and infrastructure monitoring. Methods vary in how they exploit structural and attribute information and in whether they require labels. Recent studies have proposed spectral methods, deep autoencoders, and contrastive objectives for anomaly detection in graphs (Xu et al., 2018; Ding et al., 2019; Song et al., 2021). Others incorporate topology-aware attention or reconstruction losses to flag suspicious nodes (Zheng and Li, 2020; Park et al., 2022). While dynamic settings introduce additional complexity, streaming techniques and incremental embeddings have also been explored (Chen et al., 2020; Rao et al., 2021).\n\nIn this work, we consider node-level irregularities in attributed graphs constructed from enterprise event logs over multiple weeks.",
    "reason": "The span lists prior approaches without connecting them to the paper's aims, gap, or argument (definition a), and does not articulate the authors' perspective or motivation (definition c).",
    "start": 250,
    "end": 566,
    "label": "Lacks synthesis"
  },
  {
    "span": "The MIMIC-IV dataset has been extensively used for narrative generation in the ICU.",
    "document": "Related Work\n\nClinical text generation has explored summarizing patient trajectories and producing discharge instructions from structured and unstructured EHR data (Pivovarov and Elhadad, 2015; Huang et al., 2020). Prior work has also examined biomedical controllability and factual consistency to mitigate hallucinations in clinical settings (Zhang et al., 2020; Krishna et al., 2021). The MIMIC-IV dataset has been extensively used for narrative generation in the ICU. We contribute a section-aware decoder that grounds generated prose in time-stamped clinical events to improve factuality.",
    "reason": "First mention of a specific dataset and a claim about its prior use lacks a citation to the dataset and supporting studies, violating rule (a).",
    "start": 387,
    "end": 470,
    "label": "Unsupported claim"
  },
  {
    "span": "the ImageNet-LT benchmark",
    "document": "Related Work\n\nLong-tailed recognition addresses severe class imbalance where head classes dominate the training sample (Kang et al., 2020; Zhou et al., 2020). Methods include reweighting, decoupled training, and representation learning with balanced classifiers (Menon et al., 2021; Kim et al., 2020). We report results on the ImageNet-LT benchmark and Places-LT to measure performance under natural long-tailed distributions. Recent advances also leverage self-supervised pretraining to mitigate tail underfitting (He et al., 2020; Zhai et al., 2022).",
    "reason": "First mention of a specific benchmark dataset lacks a citation to its originating paper, which should be provided on first mention.",
    "start": 323,
    "end": 348,
    "label": "Unsupported claim"
  },
  {
    "span": "The ogbn-products dataset contains over 2.4 million edges and 47 features per node",
    "document": "Introduction\n\nScalable Graph Representation Learning Graph neural networks (GNNs) enable learning over relational data by iteratively aggregating neighborhood information (Kipf and Welling, 2017; Hamilton et al., 2017). However, message passing can introduce oversmoothing and high memory costs on large graphs (Li et al., 2018; Chen et al., 2018). The ogbn-products dataset contains over 2.4 million edges and 47 features per node, illustrating the computational challenges for standard GNN layers.\n\nOur Contributions We introduce a blockwise sampler with adaptive neighborhood truncation that preserves label signal while reducing variance in mini-batch training. Additionally, we propose a residual gated aggregation that mitigates oversmoothing in deeper stacks.",
    "reason": "This sentence gives specific dataset statistics at first mention without a citation, which should be referenced.",
    "start": 349,
    "end": 431,
    "label": "Unsupported claim"
  },
  {
    "span": "Several recent works demonstrate that prompt-tuning dramatically improves zero-shot dialogue state tracking.",
    "document": "Related Work\n\nDialogue state tracking (DST). Early neural DST models leverage delexicalization and semantic dictionaries to map user utterances to slot-value pairs (Mrkšić et al., 2017; Zhong et al., 2018). With the release of large-scale benchmarks like MultiWOZ, pre-trained encoders became dominant for turn-level reasoning (Budzianowski et al., 2018; Hosseini-Asl et al., 2020). Cross-lingual DST typically relies on multilingual encoders and translation-based augmentation to reduce annotation costs (Schuster et al., 2019; Zhu et al., 2020).\n\nPrompting and parameter-efficient tuning. Prompting methods reformulate tasks as masked or generative completions to better align with pre-training objectives (Schick and Schütze, 2021; Gao et al., 2021). Parameter-efficient approaches such as adapters and low-rank updates offer competitive performance with far fewer trainable parameters (Houlsby et al., 2019; Hu et al., 2022). Several recent works demonstrate that prompt-tuning dramatically improves zero-shot dialogue state tracking. Despite this promise, multilingual transfer with prompts remains underexplored, and prior studies often rely on English-centric templates.\n\nWe build on multilingual encoders and fixed prompts to improve zero-shot DST, focusing on robust slot transfer across typologically diverse languages.",
    "reason": "Claims the existence and effect of 'several recent works' without citing any of them.",
    "start": 930,
    "end": 1038,
    "label": "Unsupported claim"
  },
  {
    "span": "According to industry surveys, 72% of production dialogue systems rely on retrieval-augmented generation.",
    "document": "Introduction\n\nEnterprise deployment of conversational assistants increasingly emphasizes factuality and controllability. Retrieval-augmented generation (RAG) has emerged as a practical approach to ground responses in proprietary knowledge bases while maintaining linguistic flexibility. According to industry surveys, 72% of production dialogue systems rely on retrieval-augmented generation. Despite its popularity, rigorous evaluations of grounding fidelity and update latency under evolving knowledge remain scarce.\n\nWe contribute BenchRAG, a suite of scenarios reflecting realistic knowledge updates, and provide human and automatic measures for grounding fidelity and recency.",
    "reason": "This is a specific quantitative claim based on 'industry surveys' but provides no citation or evidence for the 72% figure.",
    "start": 287,
    "end": 392,
    "label": "Unsupported claim"
  },
  {
    "span": "In (Garcia and Lee, 2020)",
    "document": "Related Work\n\nCross-lingual representation learning has explored alignment at the word, sentence, and document levels. In (Garcia and Lee, 2020) a dual-encoder approach aligns multilingual sentence embeddings using contrastive objectives, while concurrent work improves token-level alignment via masked language modeling (Kumar et al., 2019; Zhao and Xu, 2021). Subsequent studies integrated visual grounding to further constrain cross-lingual mapping (Mori and Chen, 2022).\n\nRecent analyses show that alignment quality depends on both lexical coverage and domain similarity (Artetxe et al., 2018; Conneau et al., 2020). Building on this, we investigate domain-aware fine-tuning that adaptively reweights examples to mitigate domain shift during transfer (Li and Park, 2021).\n\nOur study complements retrieval-based approaches for multilingual QA (Asai et al., 2021) and translation (Tran et al., 2019), emphasizing robustness under distribution shifts common in web-scale corpora.",
    "reason": "Wrong citation style with misplaced parentheses after a preposition; should be 'In Garcia and Lee (2020)'.",
    "start": 119,
    "end": 144,
    "label": "Format"
  },
  {
    "span": "Region-based features from object detectors dominate early VQA systems (Anderson et al., 2018). Cross-modal attention fuses tokens across modalities (Lu et al., 2019). Large-scale image–text pretraining yields transferable representations (Radford et al., 2021). Compositional generalization remains challenging (Hudson and Manning, 2019).",
    "document": "Related Work\n\nVisual question answering (VQA) integrates vision and language through joint modeling of images and text. Approaches differ in feature extraction, fusion, and pretraining strategies.\n\nRegion-based features from object detectors dominate early VQA systems (Anderson et al., 2018). Cross-modal attention fuses tokens across modalities (Lu et al., 2019). Large-scale image–text pretraining yields transferable representations (Radford et al., 2021). Compositional generalization remains challenging (Hudson and Manning, 2019).\n\nWe investigate whether structure-aware fusion improves systematic generalization without sacrificing accuracy on in-domain splits.",
    "reason": "The sentences move across distinct themes (features, fusion, pretraining, generalization) without transitions or clarifying their relationships, creating an abrupt sequence of citations lacking explicit connections.",
    "start": 198,
    "end": 537,
    "label": "Coherence"
  },
  {
    "span": "BLEU has been conclusively shown to correlate poorly with human adequacy judgments for conversational text.",
    "document": "Related Work\n\nAutomatic metrics for machine translation evaluation have evolved from surface n-gram overlap to embedding- and model-based estimators. While BLEU remains widely reported due to its simplicity and historical usage, alternative metrics such as METEOR, CHRF, and newer learned metrics like COMET and BLEURT have demonstrated stronger associations with human judgments in many settings. Domain-specific evaluation is especially important for conversational and low-resource scenarios where literal overlap is less informative.\n\nBLEU has been conclusively shown to correlate poorly with human adequacy judgments for conversational text. Recent studies instead advocate reference-free or quality estimation approaches that leverage multilingual encoders to assess adequacy and fluency without access to references. Despite these advances, there remains substantial variance across language pairs, domains, and evaluation protocols.",
    "reason": "This is a claim about prior empirical findings (correlation with human judgments) but provides no citation; such claims must be supported by references (rules a and e).",
    "start": 539,
    "end": 646,
    "label": "Unsupported claim"
  },
  {
    "span": "there are many recent works that explore multimodal fact-checking for social media.",
    "document": "Introduction\n\nThe rapid dissemination of misinformation on social media has sparked significant interest in automated fact-checking. Early systems focused almost exclusively on textual claims, leveraging claim retrieval and stance detection to approximate veracity. As social platforms increasingly foreground images, videos, and memes, a purely textual pipeline becomes insufficient for robust verification.\n\nIn response to this shift, there are many recent works that explore multimodal fact-checking for social media. These efforts commonly pair visual content with associated captions or comments, using cross-modal attention to tie evidence to claims. Despite promising progress, most models assume the availability of well-aligned text–image pairs and neglect domain drift across platforms and languages.\n\nThis paper proposes a retrieval-augmented, cross-modal architecture that couples vision–language pretraining with document-level evidence aggregation. We also introduce a hard-negative mining strategy that reflects real-world feed composition. Our experiments show consistent gains across varied rumor categories and image–text correlation levels.\n\nWe release code and a reproducible evaluation protocol to foster comparability and rigorous error analysis.",
    "reason": "The sentence invokes 'recent works' without providing any citations to specific studies, violating the requirement that mentions of recent works be supported with references (rule d).",
    "start": 437,
    "end": 520,
    "label": "Unsupported claim"
  },
  {
    "span": "Large language models pretrained on code achieve strong zero-shot program synthesis (Chen et al., 2021). Automatic unit test generation has been proposed to validate outputs (Fraser and Arcuri, 2011). Static analysis detects potential defects in generated code (Johnson and Wagner, 2017).",
    "document": "Related Work\n\nProgram synthesis has advanced rapidly with large language models trained on source code. Large language models pretrained on code achieve strong zero-shot program synthesis (Chen et al., 2021). Automatic unit test generation has been proposed to validate outputs (Fraser and Arcuri, 2011). Static analysis detects potential defects in generated code (Johnson and Wagner, 2017). Work on feedback-driven decoding introduces constraints during generation (Nye et al., 2021). Our framework unifies execution feedback and static checks into a single iterative refinement loop.",
    "reason": "The span lists code LLMs, test generation, and static analysis as disjoint sentences without transitions or explicit connections, leaving the relationship between the cited works implicit and unclear.",
    "start": 104,
    "end": 392,
    "label": "Coherence"
  },
  {
    "span": "Industry reports indicate that phishing is responsible for over 90% of data breaches.",
    "document": "Introduction\n\nEmail remains a primary vector for organizational compromise, with attackers leveraging social engineering to bypass technical defenses. Security awareness training and automated filtering aim to reduce exposure to malicious content.\n\nIndustry reports indicate that phishing is responsible for over 90% of data breaches. If accurate, this figure underscores the need for proactive, user-centric defenses that can prevent credential theft and malware delivery.\n\nWe investigate a layered detection framework combining content analysis, sender reputation, and user-in-the-loop verification to reduce successful phishing attempts.",
    "reason": "This quantitative claim appeals to unspecified 'industry reports' without citing any source, leaving the statistic unsupported.",
    "start": 249,
    "end": 334,
    "label": "Unsupported claim"
  },
  {
    "span": "there are many recent works that explore this topic",
    "document": "Related Work\n\nTask-oriented dialogue systems aim to track user goals and update belief states throughout a conversation. Early approaches relied on rule-based trackers and generative models (Henderson, 2014; Williams et al., 2017), while later systems transitioned to neural encoders that jointly model dialogue context and system acts (Rastogi et al., 2019). There has also been significant interest in schema-guided dialogue and zero-shot generalization of slots using pre-trained language models.\n\nDespite this progress, there are many recent works that explore this topic, focusing on schema-guided dialogue and cross-lingual transfer. Several lines of research attempt to leverage multilingual pre-training to improve transfer across languages and domains. Others investigate the use of retrieval-augmented generation to better incorporate knowledge bases into dialogue state tracking. Our work situates within this line by emphasizing robust slot value grounding under domain shifts.",
    "reason": "The phrase invokes 'recent works' without citing any papers, violating the requirement that mentions of recent works be supported with citations (rule d).",
    "start": 524,
    "end": 575,
    "label": "Unsupported claim"
  },
  {
    "span": "(Clark, 2019; and Patel, 2020)",
    "document": "Related Work\n\nModern causal inference methods combine flexible function approximators with identification strategies to estimate heterogeneous treatment effects (Athey and Imbens, 2016; Shalit et al., 2017). Debiased machine learning reduces finite-sample bias through orthogonalization and sample splitting (Chernozhukov et al., 2018). Several surveys discuss practical considerations for covariate shift and overlap diagnostics (Pearl, 2009; Imbens and Rubin, 2015). Evidence on cross-domain transportability has been mixed (Clark, 2019; and Patel, 2020), motivating benchmarks with explicit dataset shifts.\n",
    "reason": "Incorrect coordination inside a parenthetical citation: \"and\" should not be used with a semicolon-delimited list. It should be formatted as \"(Clark, 2019; Patel, 2020)\".",
    "start": 526,
    "end": 556,
    "label": "Format"
  },
  {
    "span": "A/B tests show a 12% reduction in next-word prediction latency in production keyboards.",
    "document": "Introduction\n\nFederated learning (FL) enables on-device personalization while keeping raw user data private, making it attractive for mobile text entry and next-word prediction. By periodically aggregating model updates across clients, FL can adapt to individual typing styles and evolving vocabularies without centralized data collection.\n\nA/B tests show a 12% reduction in next-word prediction latency in production keyboards. Alongside latency gains, practitioners report improved keypress savings and higher user satisfaction when personalization is coupled with lightweight distillation.\n\nDespite these advantages, FL introduces systems challenges, including stragglers, non-iid data distributions, and stringent resource constraints. We present an adaptive scheduler that co-optimizes client selection and quantized aggregation to maintain model quality under tight energy budgets.",
    "reason": "Presents a precise quantitative performance improvement as an empirical claim without providing evidence or a citation to the reported A/B tests.",
    "start": 341,
    "end": 428,
    "label": "Unsupported claim"
  },
  {
    "span": "In practice, hospitals rarely comply with synchronous rounds due to network volatility.",
    "document": "Introduction\n\nFederated Learning in Medical Imaging. Sharing raw images is often prohibited by regulation, motivating federated optimization across institutions (Sheller et al., 2020; Li et al., 2020). Methods address data heterogeneity via personalization and robust aggregation (Fallah et al., 2020; Karimireddy et al., 2020). In practice, hospitals rarely comply with synchronous rounds due to network volatility. Asynchrony, dropouts, and stragglers thus play a central role in system performance.",
    "reason": "Claims a real-world practice pattern without any empirical citation or evidence (violates rule b).",
    "start": 329,
    "end": 416,
    "label": "Unsupported claim"
  },
  {
    "span": "Compared to CNNs, ViTs are intrinsically more robust to l_p attacks.",
    "document": "Introduction\n\nVision transformers (ViTs) have transformed image recognition by applying self-attention to patch embeddings, enabling models to capture long-range dependencies efficiently. While ViTs often match or surpass convolutional neural networks (CNNs) on clean accuracy, their adversarial robustness remains an active area of investigation with mixed results reported in the literature.\n\nCompared to CNNs, ViTs are intrinsically more robust to l_p attacks. This claim, if true, has important implications for model selection in safety-critical applications, but the evidence is confounded by differences in training recipes, data augmentation, and regularization practices.\n\nWe present a controlled robustness study that aligns training budgets, data augmentations, and model capacities across architectures. Our benchmark covers common l_p threat models and adaptive attacks. We further analyze attention map smoothness and frequency bias to shed light on architectural factors associated with robustness.\n\nRelated Work\n\nRobustness analyses span adversarial training, certified defenses, and architectural inductive biases. Studies comparing CNNs and ViTs often conflate pretraining scale and augmentation strategies, motivating the standardized evaluation protocol we introduce.",
    "reason": "Claims a comparative property of prior architectures without any citation to support it.",
    "start": 395,
    "end": 463,
    "label": "Unsupported claim"
  },
  {
    "span": "Previous benchmarks indicate that Sim2Real transfer typically fails without domain randomization.",
    "document": "Introduction\n\nBridging the gap between simulation and real-world control remains a central challenge in robot learning. Reinforcement learning (RL) methods can learn complex policies in high-fidelity simulators but often degrade when transferred to physical systems due to visual and dynamics mismatch (OpenAI et al., 2019; Andrychowicz et al., 2020). Domain randomization and adaptation techniques aim to improve robustness by exposing policies to diverse simulated conditions (Tobin et al., 2017; Peng et al., 2018).\n\nPrevious benchmarks indicate that Sim2Real transfer typically fails without domain randomization. Despite this, the cost of extensive randomization can be prohibitive, and the optimal diversity schedule is not well understood.\n\nWe propose a curriculum-based randomization strategy that incrementally expands variability, reducing training time while preserving transfer performance on two manipulation tasks.",
    "reason": "Cites 'previous benchmarks' and a general failure claim with no references to specific studies or benchmarks (rule d).",
    "start": 520,
    "end": 617,
    "label": "Unsupported claim"
  },
  {
    "span": "(ibid.)",
    "document": "Introduction\n\nKnowledge tracing models estimate a learner's evolving mastery over concepts to support personalized education (Piech et al., 2015; Pandey and Karypis, 2019). Neural approaches treat skill acquisition as a hidden state process updated by interactions over time (Ghosh et al., 2020).\n\nRecent work augments tracing with concept graphs and attention mechanisms to capture prerequisites and transfer between related skills (Zhao and Wang, 2021; Cheng et al., 2022). Some studies argue that explicit uncertainty modeling improves early-stage recommendations by avoiding overconfident updates (Liu and Hart, 2020). In contrast to (ibid.), we propose an evaluation protocol that separates question difficulty from concept difficulty via hierarchical calibration.\n\nOur results show improved calibration and next-response prediction across three public datasets.",
    "reason": "Improper use of 'ibid.' in an author–year reference list. 'Ibid.' is a footnote-based convention and should be replaced with a proper author–year citation (e.g., Author, Year) consistent with the rest of the text.",
    "start": 638,
    "end": 645,
    "label": "Format"
  },
  {
    "span": "In prior work, models were trained on GitHub code filtered with permissive licenses.",
    "document": "Introduction\n\nProgram synthesis models map natural language intent to executable code, enabling automation of repetitive tasks and assisting developers (Rossi and Tan, 2020). Scaling these models depends critically on the availability of large, high-quality code corpora paired with natural language (Miller et al., 2021).\n\nIn prior work, models were trained on GitHub code filtered with permissive licenses.\n\nHowever, such corpora contain duplicates, near-duplicates, and low-quality samples that can inflate evaluation metrics. We introduce a deduplicated, unit-tested dataset with explicit license metadata and report its effects on both accuracy and robustness.",
    "reason": "Describes a specific dataset construction/training setup from previous work without citing the corresponding paper(s).",
    "start": 324,
    "end": 408,
    "label": "Unsupported claim"
  },
  {
    "span": "there have been many recent works exploring controllable text simplification",
    "document": "Introduction\n\nText simplification aims to rewrite text so that it is easier to read while preserving meaning. Beyond generic simplification, practitioners increasingly seek to steer outputs with explicit controls over style and content. Over the last few years, there have been many recent works exploring controllable text simplification, focusing on knobs such as length, lexical complexity, paraphrastic diversity, and syntactic operations. However, most existing systems either require extensive parallel data or rely on indirect proxies for control signals. In this paper, we propose a framework that directly optimizes for control fidelity while maintaining semantic faithfulness and fluency.",
    "reason": "Mentions 'recent works' without citing any specific papers at first mention, violating requirement (d) and (a).",
    "start": 262,
    "end": 338,
    "label": "Unsupported claim"
  },
  {
    "span": "Several recent surveys conclude that adversarial training remains the most effective defense against strong white-box attacks.",
    "document": "Introduction\n\nNeural networks are notoriously vulnerable to adversarial perturbations, spurring defenses that range from robust optimization to randomized smoothing and certified verification (Madry et al., 2018; Cohen et al., 2019; Gowal et al., 2021). Robustness evaluations emphasize worst-case performance under threat models that reflect realistic attacker knowledge and constraints (Carlini et al., 2019).\n\nDespite a proliferation of methods, progress often hinges on evaluation rigor and transferability across datasets and architectures. Several recent surveys conclude that adversarial training remains the most effective defense against strong white-box attacks. However, training costs, accuracy-robustness trade-offs, and data-dependence motivate complementary strategies. We study hybrid schemes that combine adversarial objectives with invariance-inducing augmentations.",
    "reason": "The sentence references 'recent surveys' and asserts a specific conclusion without citing any of these surveys, violating the requirement to provide sources for such claims.",
    "start": 546,
    "end": 672,
    "label": "Unsupported claim"
  },
  {
    "span": "A large body of recent works explores session-based recommendation with transformers.",
    "document": "Related Work\n\nSession-based recommendation predicts the next item from short interaction sequences without long-term user histories. Early approaches used Markov chains and neighborhood methods to model co-occurrence signals (Shani et al., 2002; Linden et al., 2003). Recurrent neural networks later captured sequential dependencies and item transitions (Hidasi et al., 2016; Li et al., 2017), with attention mechanisms improving long-range context modeling (Kang and McAuley, 2018).\n\nA large body of recent works explores session-based recommendation with transformers. Despite these advances, challenges remain in handling noisy clicks and data sparsity; we address both via contrastive pretraining and context-adaptive masking.",
    "reason": "The statement refers to 'recent works' but provides no citations; such mentions require supporting references.",
    "start": 485,
    "end": 570,
    "label": "Unsupported claim"
  },
  {
    "span": "It is widely known that message passing fails to capture long-range effects in organic radicals.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a standard tool for molecular property prediction, modeling atoms as nodes and bonds as edges (Gilmer et al., 2017; Kearnes et al., 2016). Message passing networks aggregate local neighborhoods but may struggle with quantum or non-local interactions (Wu et al., 2018). It is widely known that message passing fails to capture long-range effects in organic radicals. Recent advances introduce global attention, spectral methods, and positional encodings to mitigate these limitations (Ying et al., 2021; Brossard et al., 2021). Our work complements these by injecting physics-inspired distance features.",
    "reason": "Makes a niche domain-specific claim presented as common knowledge but provides no evidence or citation (rule b).",
    "start": 324,
    "end": 420,
    "label": "Unsupported claim"
  },
  {
    "span": "(Klein and Rush, 2017; Sennrich et al., 2016",
    "document": "Related Work\n\nNeural machine translation (NMT) architectures progressed rapidly with attention-based encoder-decoders (Bahdanau et al., 2015) and subword tokenization to mitigate out-of-vocabulary issues (Sennrich et al., 2016). Efficient toolkits and benchmarks further accelerated adoption (Klein and Rush, 2017; Sennrich et al., 2016 across multiple language pairs and domains. Subsequent transformer-based models improved both speed and accuracy (Vaswani et al., 2017; Ott et al., 2018).\n\nWe focus on low-resource adaptation, building on multilingual transfer (Aharoni et al., 2019) and data augmentation via back-translation (Edunov et al., 2018).",
    "reason": "Missing closing parenthesis in a parenthetical multi-citation; the citation list should be properly enclosed like \"(Klein and Rush, 2017; Sennrich et al., 2016)\".",
    "start": 292,
    "end": 336,
    "label": "Format"
  },
  {
    "span": "Graph neural networks for molecular property prediction include message passing networks, attention-based models, and 3D-aware architectures (Gilmer et al., 2017; Kearnes et al., 2016; Hu et al., 2020; Schutt et al., 2018). Recent datasets and benchmarks provide large-scale evaluations across tasks such as solubility and toxicity (Wu et al., 2018; Hu et al., 2021; Klicpera et al., 2020). Self-supervised pretraining on molecular graphs leverages context prediction and contrastive objectives (Sun et al., 2020; You et al., 2020; Rong et al., 2020).",
    "document": "Related Work\n\nMolecular Representations\nRepresentations for small molecules range from handcrafted fingerprints to learned graph embeddings. The choice of representation mediates what chemical signals are available to downstream predictors and shapes generalization to new scaffolds.\n\nMolecular Property Prediction with GNNs\nGraph neural networks for molecular property prediction include message passing networks, attention-based models, and 3D-aware architectures (Gilmer et al., 2017; Kearnes et al., 2016; Hu et al., 2020; Schutt et al., 2018). Recent datasets and benchmarks provide large-scale evaluations across tasks such as solubility and toxicity (Wu et al., 2018; Hu et al., 2021; Klicpera et al., 2020). Self-supervised pretraining on molecular graphs leverages context prediction and contrastive objectives (Sun et al., 2020; You et al., 2020; Rong et al., 2020).\n\nUncertainty Estimation and OOD Generalization\nA parallel thread studies calibrated uncertainty and out-of-distribution robustness for drug discovery. Techniques include ensembles, Bayesian layers, and distributionally robust training.\n\nOur Focus\nWe target generalization under scaffold splits with limited labels by learning structure-aware invariances across scaffolds.",
    "reason": "The span enumerates prior works and categories without connecting them to the paper’s problem or clarifying what gap remains, thus lacking synthesis per (a) and (c).",
    "start": 325,
    "end": 876,
    "label": "Lacks synthesis"
  },
  {
    "span": "(Smith et al. 2020)",
    "document": "Related Work\n\nTime-series anomaly detection spans probabilistic forecasting, reconstruction-based methods, and contrastive learning (Chen and Li, 2017; Rahman et al., 2019). Forecasting residuals often reveal deviations from learned dynamics but can be brittle under covariate shift (Torres and Ahmad, 2020). Autoencoder approaches capture manifold structure yet may overfit periodic patterns (Vega and Zhou, 2020).\n\nRecently, contrastive objectives align temporal views to learn seasonality-robust representations (Smith et al. 2020). Hybrid models combine predictive heads with contrastive pretraining to reduce false positives on recurring spikes (Ilyas and Wong, 2021). We build on these ideas and add an adaptive temperature schedule to balance hard and easy negatives.\n\nIn streaming settings, memory-aware replay mitigates catastrophic forgetting when concepts drift (Park and Liu, 2022).",
    "reason": "Missing comma between author and year in a parenthetical citation; it should be '(Smith et al., 2020)'.",
    "start": 515,
    "end": 534,
    "label": "Format"
  },
  {
    "span": "The SemEval 2020 shared task on cause-effect extraction released over 10,000 annotated pairs.",
    "document": "Introduction\n\nCausal relation extraction underpins downstream applications such as decision support and scientific discovery. Benchmark datasets vary widely in domain, annotation guidelines, and granularity of causal expressions. The SemEval 2020 shared task on cause-effect extraction released over 10,000 annotated pairs. Such resources have catalyzed supervised learning approaches, yet cross-domain generalization remains limited. We investigate domain-adaptive pretraining and prompt-based learning to reduce annotation sensitivity while maintaining precision on out-of-domain texts.\n\nRelated Work\n\nEarly methods relied on pattern mining and bootstrapping with seed lexicons. Neural sequence tagging and span classification improved recall, but error analysis highlights boundary ambiguity and lexical sparsity. Recent interest in causal event graphs has motivated joint modeling of entities, triggers, and discourse relations. Our work complements these trends by emphasizing data efficiency and domain robustness.",
    "reason": "Mentions a specific shared task and gives a detailed dataset size without citing the task overview or dataset paper.",
    "start": 230,
    "end": 323,
    "label": "Unsupported claim"
  },
  {
    "span": "The LibriSpeech benchmark is largely considered solved by end-to-end systems under clean test conditions.",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) has advanced rapidly with sequence-to-sequence models and the adoption of self-attention encoders. These systems benefit from large-scale pretraining and domain-specific fine-tuning. Standard evaluation uses word error rate (WER) on datasets comprising both read and spontaneous speech.\n\nLibriSpeech has become a central benchmark for English ASR, featuring train, dev, and test splits under clean and other conditions. Alongside LibriSpeech, far-field and noisy corpora remain challenging, especially for robust ASR in real-world environments.\n\nThe LibriSpeech benchmark is largely considered solved by end-to-end systems under clean test conditions. However, performance gaps remain on noisy and conversational test sets, underscoring the need for architectures that generalize beyond studio-quality recordings.",
    "reason": "Asserts a community-wide conclusion about a specific benchmark being 'solved' without citing any evidence or studies, which is a niche, contested claim needing support (rule b).",
    "start": 606,
    "end": 711,
    "label": "Unsupported claim"
  },
  {
    "span": "Brown et al.",
    "document": "Introduction\n\nFew-Shot and In-Context Learning\n\nIn-context learning demonstrates that sufficiently large autoregressive models can perform tasks from a handful of examples without parameter updates. Brown et al. showed that scaling model and data yields emergent capabilities, inspiring studies on prompt design (Liu et al., 2021) and calibration (Zhao et al., 2021). Subsequent work explored instruction tuning (Sanh et al., 2022) and chain-of-thought prompting (Wei et al., 2022), while retrieval augmentation (Lewis et al., 2020) mitigates knowledge gaps. However, robustness to prompt permutations and domain shift remains limited (Min et al., 2022), motivating methods that combine demonstrations with constraints (Sun et al., 2022).",
    "reason": "Narrative citation missing the publication year; it should be Brown et al. (2020).",
    "start": 199,
    "end": 211,
    "label": "Format"
  },
  {
    "span": "Propensity score methods attempt to balance covariates between treated and control units (Rosenbaum and Rubin, 1983). Instrumental variable approaches identify causal effects using exogenous variation (Angrist et al., 1996). Difference-in-differences compares pre/post trends to estimate treatment impacts (Card and Krueger, 1994).",
    "document": "Related Work\n\nEstimating causal effects from observational data requires strong assumptions and careful design. Propensity score methods attempt to balance covariates between treated and control units (Rosenbaum and Rubin, 1983). Instrumental variable approaches identify causal effects using exogenous variation (Angrist et al., 1996). Difference-in-differences compares pre/post trends to estimate treatment impacts (Card and Krueger, 1994). Synthetic controls and matching refinements further reduce bias (Abadie et al., 2010; Stuart, 2010). We target scalable matching with learned representations under covariate shift.",
    "reason": "The span enumerates three different causal identification strategies back-to-back without transitions or any explicit explanation of how they relate to each other, causing abrupt topic shifts and poor coherence.",
    "start": 112,
    "end": 443,
    "label": "Coherence"
  },
  {
    "span": "Automated program repair with neural models includes sequence-to-sequence translation from buggy to fixed code (Tufano et al., 2019; Chen et al., 2019). Graph-based approaches encode program structure using ASTs or data-flow graphs (Allamanis et al., 2018; Hellendoorn et al., 2020). Pretrained code models leverage large corpora for improved patch generation (Feng et al., 2020; Ahmad et al., 2021). Retrieval-augmented pipelines mine similar fixes to guide patch search (Lutellier et al., 2020; Jiang et al., 2021).",
    "document": "Related Work\nNeural automated program repair (APR) seeks to synthesize patches for buggy code by learning from large-scale repositories and benchmarks. Key challenges include ensuring compilability, semantic correctness, and generalization beyond training distributions.\n\nAutomated program repair with neural models includes sequence-to-sequence translation from buggy to fixed code (Tufano et al., 2019; Chen et al., 2019). Graph-based approaches encode program structure using ASTs or data-flow graphs (Allamanis et al., 2018; Hellendoorn et al., 2020). Pretrained code models leverage large corpora for improved patch generation (Feng et al., 2020; Ahmad et al., 2021). Retrieval-augmented pipelines mine similar fixes to guide patch search (Lutellier et al., 2020; Jiang et al., 2021).\n\nWe target specification drift across repositories and propose a constraint-guided decoder that enforces type and API consistency while integrating a verifier to filter semantic regressions.",
    "reason": "The span enumerates existing avenues and citations but does not explain their relation to the current work or articulate what is missing; thus it lacks synthesis.",
    "start": 272,
    "end": 789,
    "label": "Lacks synthesis"
  },
  {
    "span": "Most recent efforts adopt soft prompts because they require fewer parameters and converge faster",
    "document": "Related Work\n\nPrompting and parameter-efficient tuning techniques adapt large language models without full fine-tuning (Lester et al., 2021; Li and Liang, 2021; Hu et al., 2022). Discrete prompts can be optimized via gradient-free search or initialized from templates (Shin et al., 2020; Gao et al., 2021). Most recent efforts adopt soft prompts because they require fewer parameters and converge faster. Alternatives such as adapters and LoRA modify a small number of internal weights while preserving the base model (Houlsby et al., 2019; Hu et al., 2022). We study when prompt length and initialization matter for few-shot generalization.",
    "reason": "Claims what 'most recent efforts' do and why, without citations to those efforts.",
    "start": 307,
    "end": 403,
    "label": "Unsupported claim"
  },
  {
    "span": "However, there are no comprehensive surveys on prompt calibration in low-resource languages.",
    "document": "Introduction\n\nPrompt-based learning has become a central paradigm for adapting large language models to downstream tasks (Brown et al., 2020; Schick and Schütze, 2021). Subsequent work studies prompt engineering, automatic prompt search, and calibration techniques to reduce label bias (Gao et al., 2021; Zhao et al., 2021). While much of this literature focuses on English and a few high-resource languages, multilingual prompt design remains underexplored (Winata et al., 2021).\n\nHowever, there are no comprehensive surveys on prompt calibration in low-resource languages. This gap makes it difficult for practitioners to select appropriate techniques that are robust to morphological complexity and data scarcity. In this paper, we systematically evaluate calibration strategies across typologically diverse, low-resource settings and propose an adaptive prior adjustment method that leverages unlabeled corpora.",
    "reason": "Sweeping claim about the absence of prior surveys should be supported with citations or evidence; as a claim about prior work, it is currently unsupported.",
    "start": 482,
    "end": 574,
    "label": "Unsupported claim"
  },
  {
    "span": "the SemEval-2015 Task 12 dataset",
    "document": "Related Work\n\nAspect-based sentiment analysis (ABSA) has been studied extensively, with early pipelines focusing on rule-based extraction followed by supervised polarity classification (Hu and Liu, 2004; Jiang et al., 2011). Neural models unified aspect extraction and sentiment prediction using sequence labeling and attention mechanisms (Wang et al., 2016; Ma et al., 2017). In our comparison, we evaluate on the SemEval-2015 Task 12 dataset and a Twitter ABSA corpus to assess domain robustness. Recent advances also incorporate syntactic priors via graph neural networks that exploit dependency trees (Zhang et al., 2019).",
    "reason": "First mention of a shared task/dataset lacks a citation to the task description paper, which should be cited on first mention.",
    "start": 411,
    "end": 443,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an automated essay scoring task trained on TOEFL essays and achieved near-human reliability.",
    "document": "Related Work\n\nAutomated essay scoring (AES) has progressed from handcrafted features capturing grammar, coherence, and vocabulary to neural encoders that jointly learn content and surface indicators (Yannakoudakis et al., 2011; Taghipour and Ng, 2016; Dong et al., 2017). Recent work leverages pretrained transformers and domain-adaptive fine-tuning to improve cross-prompt generalization and reduce bias (Mayfield and Black, 2020; Rodriguez et al., 2021).\n\nBERT was used in an automated essay scoring task trained on TOEFL essays and achieved near-human reliability. Concurrent research examines fairness and robustness in AES, seeking to mitigate demographic and prompt-induced biases while maintaining validity and interpretability (Saha and Eisenstein, 2021; Perry and Luo, 2022). Our approach adds content-sensitive calibration to align model scores with human raters across prompts.",
    "reason": "The span describes a specific prior setup (BERT on TOEFL AES with near-human reliability) without any citation, matching example (iii) and rule (a).",
    "start": 458,
    "end": 567,
    "label": "Unsupported claim"
  },
  {
    "span": "Lee & Park (2020)",
    "document": "Introduction\n\nMultimodal grounding has benefited from vision-language pretraining (Tan and Bansal, 2019; Lu et al., 2019). Following Lee & Park (2020), we evaluate cross-modal retrieval under fine-grained alignment, extending prior benchmarks (Young et al., 2014; Plummer et al., 2017).\n\nUnlike earlier alignment losses (Faghri et al., 2018), our approach incorporates region-level prompts to guide attention (Kamath et al., 2021; Ni et al., 2021).",
    "reason": "Wrong conjunction style in narrative citation; in narrative text, 'and' should be used instead of '&' (i.e., 'Lee and Park (2020)').",
    "start": 133,
    "end": 150,
    "label": "Format"
  },
  {
    "span": "Shen and Patel (2017) investigated grammar induction for semantic parsing. Rojas et al. (2019) used neural decoders with constrained decoding. Tan and Wu (2021) studied weak supervision via denotations. Diaz and Ahmed (2022) proposed schema linking heuristics for unseen databases.",
    "document": "Related Work\n\nSemantic parsing translates natural language into executable programs over structured data such as tables or knowledge bases. Recent work explores grammar design, constrained decoding, supervision signals, and schema generalization.\n\nShen and Patel (2017) investigated grammar induction for semantic parsing. Rojas et al. (2019) used neural decoders with constrained decoding. Tan and Wu (2021) studied weak supervision via denotations. Diaz and Ahmed (2022) proposed schema linking heuristics for unseen databases.\n\nSeveral benchmarks emphasize compositional generalization and cross-domain transfer (Long et al., 2023). Our approach unifies constrained decoding with schema-aware pretraining via a joint objective.",
    "reason": "The span lists separate works with no transitions or explicit ties among grammar induction, constrained decoding, weak supervision, and schema linking, making their relationships implicit and reducing coherence.",
    "start": 248,
    "end": 529,
    "label": "Coherence"
  },
  {
    "span": "Adaptive adjacency learning replaces fixed graphs with attention over sensor pairs (Wu et al., 2019). Temporal convolution captures multi-scale periodicity in traffic flows (Yu et al., 2018). Transformer self-attention models long-range dependencies across timesteps (Vaswani et al., 2017; Li et al., 2021). Data augmentation via time warping improves robustness to distribution shifts (Um et al., 2017). Spatio-temporal residual connections stabilize deep forecasting networks (Zhang et al., 2020).",
    "document": "Related Work\n\nGraph-based Traffic Forecasting\n\nAccurate traffic forecasting requires modeling spatial correlations among sensors and temporal dynamics across multiple horizons. Graph neural networks (GNNs) naturally capture spatial structure, while sequence models capture temporal dependencies (Li et al., 2018; Yu et al., 2018). Recent work explores adaptive graph construction, attention mechanisms, and hybrid convolutional-recurrent architectures to improve performance in urban-scale datasets.\n\nAdaptive adjacency learning replaces fixed graphs with attention over sensor pairs (Wu et al., 2019). Temporal convolution captures multi-scale periodicity in traffic flows (Yu et al., 2018). Transformer self-attention models long-range dependencies across timesteps (Vaswani et al., 2017; Li et al., 2021). Data augmentation via time warping improves robustness to distribution shifts (Um et al., 2017). Spatio-temporal residual connections stabilize deep forecasting networks (Zhang et al., 2020).\n\nWe instead propose a contrastive pretraining objective on masked sensor trajectories that learns topology-aware embeddings reusable across cities, reducing labeled data needs while maintaining horizon-wise accuracy.",
    "reason": "The sentences cite different techniques without transitions and do not clarify how each method relates to the others, leaving the connections implicit and causing abrupt shifts between topics.",
    "start": 501,
    "end": 1000,
    "label": "Coherence"
  },
  {
    "span": "The Atari-57 benchmark has become the de facto standard for measuring generalization",
    "document": "Related Work\n\nGeneralization in deep reinforcement learning (RL) is commonly assessed through performance across diverse tasks under a shared training protocol. The Atari-57 benchmark has become the de facto standard for measuring generalization, offering a heterogeneous suite of games that stress-test exploration, credit assignment, and planning. Despite progress, agents often overfit to visual statistics of the training distribution, leading to brittle policies. Our work addresses this gap by introducing representation-level interventions that encourage environment-invariant abstractions.",
    "reason": "Asserts the status of a benchmark without citing sources that establish this consensus, violating (a) and (b).",
    "start": 161,
    "end": 245,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays with prompt-specific heads.",
    "document": "Related Work\n\nAutomated essay scoring (AES) has progressed from handcrafted features with linear models to neural encoders that learn holistic representations of writing quality (Shermis and Burstein, 2013; Ke et al., 2019). Pretrained language models have been adapted to scoring with pairwise ranking losses and calibration strategies (Uto et al., 2020; Wang et al., 2021). BERT was used in an AES task trained on essays with prompt-specific heads. Concurrently, domain adaptation techniques mitigate prompt shift and genre variation using adversarial or contrastive objectives (Jin et al., 2020; Mayfield and Black, 2020). We build on pretrained encoders but emphasize robust cross-prompt generalization.",
    "reason": "This sentence references a specific prior setup (BERT with prompt-specific heads in AES) without citing the work; per the definition, mentions of a previous setup/task use must include a citation.",
    "start": 376,
    "end": 450,
    "label": "Unsupported claim"
  },
  {
    "span": "There are many recent works that explore multi-speaker meeting summarization.",
    "document": "Related Work\n\nDialogue summarization has been studied in meeting, customer-service, and open-domain settings. Early extractive approaches relied on lexical cues and discourse relations (Murray and Carenini, 2008; Riedhammer et al., 2010). With the advent of large pre-trained encoders, neural abstractive models incorporate role-aware and turn-level structures to handle conversational dynamics (Zhang et al., 2021; Chen and Yang, 2020). There are many recent works that explore multi-speaker meeting summarization. Resources such as AMI and ICSI meetings enable supervised training and evaluation (Carletta, 2005; Janin et al., 2003). In contrast to prior systems tailored to specific datasets, our approach aims to be domain-agnostic by leveraging discourse markers and topic segmentation.",
    "reason": "Mentions 'recent works' without providing citations to any of them (rule d).",
    "start": 438,
    "end": 515,
    "label": "Unsupported claim"
  },
  {
    "span": "recent works have explored multi-hop retrieval for conversational QA",
    "document": "Related Work\n\nConversational question answering (CQA) extends single-turn QA by modeling dialog context and coreference across turns (Choi et al., 2018; Reddy et al., 2019). Early systems adapted reading comprehension models with hierarchical context encoders and turn-level attention (Qu et al., 2019; Min et al., 2020). In open-domain settings, retrieval-augmented methods combine dense passage retrieval with generative readers (Karpukhin et al., 2020; Lewis et al., 2020). In particular, recent works have explored multi-hop retrieval for conversational QA, emphasizing evidence aggregation across dialog turns and passages. However, these methods often struggle with error propagation from retrieval to reasoning. We propose a jointly trained retriever-reader that conditions on dialog state to reduce cascading errors.",
    "reason": "Uses the phrase 'recent works' to claim multi-hop retrieval has been explored for CQA but provides no citations for those works (rule d).",
    "start": 492,
    "end": 560,
    "label": "Unsupported claim"
  },
  {
    "span": "It is well known that BM25 underperforms on dense argumentative queries compared to neural retrievers.",
    "document": "Related Work\n\nDocument retrieval traditionally relied on bag-of-words ranking with tf-idf and BM25, which remain strong baselines for ad hoc retrieval (Robertson and Zaragoza, 2009). Neural re-ranking with pre-trained transformers substantially improves effectiveness by modeling contextual semantics and long-range dependencies (Nogueira and Cho, 2019; Lin et al., 2021). It is well known that BM25 underperforms on dense argumentative queries compared to neural retrievers. Recent work on hybrid retrieval combines lexical and dense signals to address vocabulary mismatch while maintaining precision on rare entities (Gao et al., 2021; Formal et al., 2021). Our method extends hybridization with structure-aware expansions for argumentative discourse units.\n",
    "reason": "Asserts a field-specific, nontrivial performance relationship without citing evidence, violating rule (b).",
    "start": 373,
    "end": 475,
    "label": "Unsupported claim"
  },
  {
    "span": "Most prior work focuses on provider fairness rather than consumer fairness.",
    "document": "Introduction\n\nFairness in recommender systems encompasses equity for both item providers (e.g., sellers, artists) and consumers (e.g., users receiving recommendations). Algorithmic interventions often address exposure bias and popularity skew. Most prior work focuses on provider fairness rather than consumer fairness. This imbalance motivates our study of calibrated exposure across user groups, aiming to reduce disparate treatment while maintaining utility.\n\nWe formalize exposure constraints and propose an optimization framework that trades off accuracy and group-level parity.\n",
    "reason": "A comparative claim about the literature needs citations to representative prior works to substantiate the imbalance.",
    "start": 244,
    "end": 319,
    "label": "Unsupported claim"
  },
  {
    "span": "Prior shared tasks have shown that back-translation is the most effective strategy for all low-resource language pairs.",
    "document": "Introduction\n\nData augmentation is central to neural machine translation (NMT) when parallel corpora are scarce. Techniques such as back-translation, noising, and multilingual transfer have been widely explored to improve performance in low-resource settings (Sennrich et al., 2016; Edunov et al., 2018; Neubig and Hu, 2018). Recent multilingual pre-training further facilitates cross-lingual transfer for languages with limited resources (Conneau and Lample, 2019; Xue et al., 2021).\n\nPrior shared tasks have shown that back-translation is the most effective strategy for all low-resource language pairs. Nevertheless, the effectiveness of each augmentation technique can vary depending on domain mismatch, tokenization choices, and the amount of monolingual data. We revisit these factors by proposing a budgeted augmentation framework that selects among multiple strategies based on a small validation set.\n\nRelated Work\n\nSemi-supervised NMT uses synthetic data generated from target-side monolingual corpora to augment training (Sennrich et al., 2016; Hoang et al., 2018). Self-training and iterative back-translation have been studied to reduce noise and improve consistency (He et al., 2020; Zhou et al., 2021). Task reports on low-resource benchmarks emphasize careful evaluation protocols and resource accounting (Guzmán et al., 2019; Barrault et al., 2020). Our contributions differ by proposing a unified selection criterion across augmentation methods under fixed computational budgets.",
    "reason": "References 'shared tasks' and makes a strong performance claim without citing any specific shared task reports or papers (rule a and rule d).",
    "start": 486,
    "end": 605,
    "label": "Unsupported claim"
  },
  {
    "span": "Most previous rumor detection studies rely exclusively on Twitter threads and ignore cross-platform signals.",
    "document": "Introduction\n\nRumor detection aims to identify unverified or false claims that spread rapidly on social media platforms. Early studies emphasize the temporal and conversational dynamics of rumor spread, modeling how posts and replies evolve over time to determine veracity. Prior work highlights that linguistic cues, user credibility, and propagation structure can be predictive features for rumor classification, alongside stance analysis in threaded discussions.\n\nMost previous rumor detection studies rely exclusively on Twitter threads and ignore cross-platform signals. While this focus has facilitated the creation of benchmark datasets and standardized evaluation settings, it limits the ecological validity of deployed systems, which must handle content spanning multiple platforms, modalities, and languages.\n\nIn this paper, we introduce a cross-platform rumor detection benchmark that aggregates linked content from microblogs, forums, and messaging channels. Our contributions are threefold: we assemble a dataset of matched events across platforms, propose an architecture that fuses text, URL metadata, and propagation graphs, and present a robust evaluation protocol that accounts for event-level generalization. We show that incorporating cross-platform features improves detection performance and mitigates overfitting to platform-specific artifacts.\n\nRelated Work\n\nRumor classification has been studied through stance-based pipelines, propagation-tree kernels, and neural text encoders. Several datasets provide annotations at the post or thread level and have been widely used to benchmark methods. Despite these advances, generalization beyond a single platform remains underexplored, motivating our cross-platform perspective.",
    "reason": "Claims a generalization about prior work without providing any supporting citations at the first mention of that claim.",
    "start": 467,
    "end": 575,
    "label": "Unsupported claim"
  },
  {
    "span": "Early graph-based approaches for molecular property prediction rely on message passing and neural fingerprints (Duvenaud et al., 2015; Kearnes et al., 2016; Gilmer et al., 2017). Later studies explore attention and pooling mechanisms (Veličković et al., 2018; Ying et al., 2018), as well as expressive graph isomorphism tests (Xu et al., 2019; Morris et al., 2019). Recent works introduce equivariant architectures for 3D molecules (Schütt et al., 2018; Thomas et al., 2018; Anderson et al., 2019) and multi-task learning setups (Ramsundar et al., 2017; Yang et al., 2019).",
    "document": "Related Work\n\nTraditional cheminformatics pipelines employ engineered fingerprints and kernel methods to predict molecular properties, with ECFP and Tanimoto kernels being the most prominent choices (Rogers and Hahn, 2010; Riniker and Landrum, 2013). The advent of deep learning shifted attention to learned representations over molecular graphs.\n\nEarly graph-based approaches for molecular property prediction rely on message passing and neural fingerprints (Duvenaud et al., 2015; Kearnes et al., 2016; Gilmer et al., 2017). Later studies explore attention and pooling mechanisms (Veličković et al., 2018; Ying et al., 2018), as well as expressive graph isomorphism tests (Xu et al., 2019; Morris et al., 2019). Recent works introduce equivariant architectures for 3D molecules (Schütt et al., 2018; Thomas et al., 2018; Anderson et al., 2019) and multi-task learning setups (Ramsundar et al., 2017; Yang et al., 2019).\n\nIn parallel, uncertainty estimation and calibration for molecular models have been studied using Bayesian approximations, ensembles, and temperature scaling (Hernandez-Lobato et al., 2014; Lakshminarayanan et al., 2017; Minderer et al., 2021). Data augmentation with scaffold splitting and conformation sampling has shown benefits for generalization (Hu et al., 2019; Stokes et al., 2020).\n\nOur method builds on graph neural networks but targets structure-aware uncertainty propagation under distribution shifts, evaluating on challenging scaffold splits and out-of-domain assays.",
    "reason": "Violates (a): lists prior works without connecting them to the paper’s argument or explaining their relevance to the proposed method.",
    "start": 348,
    "end": 921,
    "label": "Lacks synthesis"
  },
  {
    "span": " (O'Neil 2016)",
    "document": "Introduction\n\nAlgorithmic decision-making has raised concerns about fairness, accountability, and transparency in high-stakes domains (Barocas et al., 2019; Selbst et al., 2019). Public awareness has grown with several prominent critiques of opaque predictive systems (Crawford, 2021). Many regulatory proposals focus on documentation and auditability to ensure due process and contestability (Mitchell et al., 2019). A broader societal critique was popularized in (O'Neil 2016), highlighting systemic risks of unregulated models.",
    "reason": "Missing comma between author and year in the parenthetical citation. It should be formatted as \"(O'Neil, 2016)\".",
    "start": 464,
    "end": 478,
    "label": "Format"
  },
  {
    "span": "In the 2017 MS COCO detection challenge, top teams trained with focal loss and very large batch sizes.",
    "document": "Introduction\n\nObject detection has seen remarkable progress through region-based and single-shot paradigms, with improvements driven by stronger backbones, multi-scale features, and better loss functions. Benchmarks such as PASCAL VOC and MS COCO standardize evaluation with average precision at multiple IoU thresholds.\n\nCompetition settings have often catalyzed advances by encouraging large-scale training and extensive ablations. Memory-efficient implementations and distributed training have enabled practitioners to push image resolution and batch size.\n\nIn the 2017 MS COCO detection challenge, top teams trained with focal loss and very large batch sizes. Building on these insights, we investigate whether scaling batch size and adjusting loss curvature benefits smaller models under tighter compute constraints.",
    "reason": "Makes a specific historical claim about a particular competition's winning methods without any citation to official reports or team papers (rule a and b).",
    "start": 561,
    "end": 663,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent clinical NER models exploit contextual embeddings from domain-adapted transformers (He et al., 2020; Albright et al., 2021), span-level classification with boundary detectors (Ribeiro et al., 2021), and multi-task learning with assertion status and entity linking (Singh et al., 2022; Park et al., 2023).",
    "document": "Introduction\n\nExtracting medical entities from clinical narratives underpins downstream applications such as cohort selection and pharmacovigilance. While supervised models achieve strong performance on benchmark corpora, domain shift and annotation variability remain practical challenges.\n\nRecent clinical NER models exploit contextual embeddings from domain-adapted transformers (He et al., 2020; Albright et al., 2021), span-level classification with boundary detectors (Ribeiro et al., 2021), and multi-task learning with assertion status and entity linking (Singh et al., 2022; Park et al., 2023). Several datasets cover different note types and labeling schemes, including discharge summaries and radiology reports (Chen et al., 2019; Luo et al., 2020).\n\nWe develop a lightweight adaptation procedure for low-resource hospitals using weak signals from routinely collected metadata.",
    "reason": "The span merely catalogs prior techniques with citations and does not connect them to the authors’ approach or identify a concrete gap, thus lacking synthesis (definition a and b).",
    "start": 292,
    "end": 603,
    "label": "Lacks synthesis"
  },
  {
    "span": "(Lee, 2015; Chen, 2017",
    "document": "Related Work\n\nData augmentation has long been used to improve generalization in vision and language tasks. Early techniques focused on label-preserving geometric transforms in images (Shorten and Khoshgoftaar, 2019), while text augmentation leveraged synonym replacement and back-translation (Fadaee et al., 2017). For regularization, prior studies (Lee, 2015; Chen, 2017 propose mixup-style interpolation and noise injection to smooth decision boundaries. More recent approaches learn augmentation policies automatically via reinforcement learning (Cubuk et al., 2019) or gradient-based search (Hataya et al., 2020). Our method unifies policy learning with uncertainty-aware selection to target rare but informative regions of the input space.\n",
    "reason": "Missing closing parenthesis in a multi-citation; should be '(Lee, 2015; Chen, 2017)'.",
    "start": 349,
    "end": 371,
    "label": "Format"
  },
  {
    "span": "there are many recent works that investigate multilingual preference optimization for LLMs",
    "document": "Introduction\n\nAligning large language models (LLMs) to human intent has rapidly progressed with preference-based optimization methods such as reinforcement learning from human feedback and direct preference optimization (Ziegler et al., 2019; Ouyang et al., 2022; Rafailov et al., 2023). While early studies focused on English, growing interest has shifted toward broader linguistic coverage to ensure equitable performance across languages and regions.\n\nDespite progress in cross-lingual pretraining and instruction tuning (Conneau et al., 2020; Chung et al., 2023), there are many recent works that investigate multilingual preference optimization for LLMs. However, systematic evaluations of how preference data composition, annotator language background, and cultural norms interact remain scarce. We study these factors by constructing a multilingual preference dataset spanning 22 languages and analyzing transfer patterns under varying annotation regimes.\n\nOur contributions are threefold: we introduce a multilingual preference dataset, propose a calibrated mixing strategy for preference batches, and present an evaluation suite isolating cultural and linguistic effects. We find that balanced preference batches mitigate language dominance and improve minority-language helpfulness without harming safety.",
    "reason": "The sentence invokes 'recent works' without providing any citations to specific studies, violating rule d about unsupported mentions of recent work.",
    "start": 568,
    "end": 658,
    "label": "Unsupported claim"
  },
  {
    "span": "Transformer-based encoders were first introduced to program repair in 2019.",
    "document": "Related Work\n\nAutomated program repair (APR) aims to suggest correct patches for buggy code with minimal human intervention. Earlier approaches employed pattern mining and constraint solving, while recent methods leverage neural models to learn edit distributions directly from code corpora.\n\nTransformer-based encoders were first introduced to program repair in 2019. Subsequent works integrated syntax-aware attention and pretraining on large codebases, enabling better generalization across repositories and languages.\n\nOur method builds on sequence-to-sequence editing but augments it with structured prompts derived from static analysis alerts, improving both precision and interpretability.",
    "reason": "This historical claim assigns a first introduction date to a method without referencing the corresponding paper, violating rule (b) and (a) regarding claims about prior work.",
    "start": 293,
    "end": 368,
    "label": "Unsupported claim"
  },
  {
    "span": "Zhou et al. (2016) apply BiLSTMs with attention for protein-protein interactions. Peng et al. (2017) use graph LSTMs over dependency trees. Beltagy et al. (2019) pretrain SciBERT on scientific corpora. Yao et al. (2019) integrate UMLS concepts with distant supervision.",
    "document": "Related Work\n\nBiomedical relation extraction has benefited from neural architectures that capture syntax and domain knowledge. However, integrating symbolic biomedical ontologies with contextual representations remains challenging due to coverage gaps and noise in distant supervision.\n\nZhou et al. (2016) apply BiLSTMs with attention for protein-protein interactions. Peng et al. (2017) use graph LSTMs over dependency trees. Beltagy et al. (2019) pretrain SciBERT on scientific corpora. Yao et al. (2019) integrate UMLS concepts with distant supervision.\n\nOur method unifies ontology linking with span-level contrastive objectives, enabling end-to-end training that reduces error propagation from entity normalization.",
    "reason": "The four studies are mentioned back-to-back with no transitions or explicit comparison, leaving unclear how sequence, graph, pretraining, and ontology-based approaches relate. This creates abrupt shifts and implied relations across multiple sentences (criterion a, b, c).",
    "start": 287,
    "end": 556,
    "label": "Coherence"
  },
  {
    "span": "State-of-the-art NER systems typically use transformer encoders.",
    "document": "Related Work\n\nNamed entity recognition (NER) has evolved from CRF-based sequence labeling with handcrafted features to neural architectures with character-level encoders (Lample et al., 2016; Ma and Hovy, 2016). Contextualized pretraining substantially improves performance across domains and languages (Peters et al., 2018; Devlin et al., 2019). State-of-the-art NER systems typically use transformer encoders. Complementary techniques such as span-based decoding, gazetteer integration, and adversarial training further reduce errors on rare and out-of-domain entities (Yan et al., 2019; Jie and Lu, 2019).\n",
    "reason": "Asserts the characteristics of state-of-the-art systems without citing representative SOTA papers to substantiate the claim (rule a/d).",
    "start": 347,
    "end": 411,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors reported a 15% reduction in model drift when using secure aggregation.",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative training without centralizing sensitive data, a particularly attractive property for healthcare applications. However, heterogeneity in client data distributions can exacerbate model drift and destabilize convergence. In a previous study, the authors reported a 15% reduction in model drift when using secure aggregation. Building on this observation, we investigate whether personalized optimization can further mitigate drift while preserving privacy guarantees.\n\nWe consider hospital networks with varying patient demographics and propose a stratified client sampling scheme to reduce inter-round variance. Our experiments analyze the trade-off between personalization strength and global performance on clinical prediction tasks.\n",
    "reason": "Mentions a specific prior study and quantitative results without providing a citation.",
    "start": 278,
    "end": 381,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from grades 6–12 with holistic scoring",
    "document": "Related Work\n\nAutomated essay scoring (AES) systems have progressed from handcrafted features and regression to neural encoders that model discourse structure and argumentation. Transfer learning from large language models has shown promise, especially when paired with rubric-aware objectives.\n\nBERT was used in an AES task trained on essays from grades 6–12 with holistic scoring, highlighting the benefit of contextualized representations for noisy student writing. Other studies investigate prompt-specific adapters and content-driven feedback generation, but the extent of cross-prompt generalization remains debated.\n\nOur work introduces a calibration layer to align model confidence with scorer uncertainty, reducing overconfidence on out-of-distribution prompts.",
    "reason": "Claims the use of a specific model and dataset setup in prior work without any citation to the corresponding paper, violating rule (a) and (e-iii).",
    "start": 296,
    "end": 381,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors claim that pointer-generator networks fail to handle factual consistency in news.",
    "document": "Related Work\n\nNeural abstractive summarization has progressed from sequence-to-sequence models with attention to pre-trained encoder-decoder architectures (See et al., 2017; Lewis et al., 2020). While copy mechanisms improve faithfulness to source wording, they often struggle with reasoning over dispersed facts and coreference (Kryscinski et al., 2020). In a previous study, the authors claim that pointer-generator networks fail to handle factual consistency in news. Subsequent research measures factuality with QA-based probes and entity-level constraints, and proposes training objectives that penalize unsupported statements (Durmus et al., 2020; Goyal and Durrett, 2021). Our approach complements these efforts with post-edit reranking guided by document-grounded entailment.\n",
    "reason": "References a prior specific study and its claim without providing a citation, violating rule (a).",
    "start": 356,
    "end": 470,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent studies tackle robustness in visual question answering through adversarial training (Jiang et al., 2020; Ganju et al., 2021), calibrated uncertainty estimation (Kuleshov et al., 2018; Minderer et al., 2021), and out-of-distribution detection (Hendrycks and Gimpel, 2017; Wang et al., 2020).",
    "document": "Related Work\n\nVisual question answering (VQA) systems must generalize beyond biases in training datasets and provide reliable predictions under distribution shift. As evaluations broaden to new benchmarks and stress tests, the community has examined both model architectures and training protocols for improving robustness.\n\nRecent studies tackle robustness in visual question answering through adversarial training (Jiang et al., 2020; Ganju et al., 2021), calibrated uncertainty estimation (Kuleshov et al., 2018; Minderer et al., 2021), and out-of-distribution detection (Hendrycks and Gimpel, 2017; Wang et al., 2020). Additional work explores dataset curation to reduce language priors (Agrawal et al., 2018; Goyal et al., 2017) and counterfactual augmentation (Ross et al., 2020; Kaushik et al., 2020).\n\nOur approach introduces a training-free procedure that adjusts answer distributions post hoc using question-only signals. We compare against adversarially trained and calibrated baselines on three VQA benchmarks.",
    "reason": "The span summarizes categories of prior work without articulating their connection to the paper’s approach or identifying a gap, meeting criterion (a).",
    "start": 325,
    "end": 622,
    "label": "Lacks synthesis"
  },
  {
    "span": "Lexicon-based approaches map words to affective scores and aggregate them at the message level (Mohammad and Turney, 2013; Buechel and Hahn, 2018). Multimodal fusion models combine text, image, and audio signals to infer affective states (Zadeh et al., 2017; Tsai et al., 2019). Sarcasm and irony have also been studied as confounds to sentiment polarity (Ghosh and Veale, 2016; Filatova, 2012).",
    "document": "Related Work\n\nEmotion recognition from social media text has evolved from lexicon-driven scoring to deep neural architectures that ingest large corpora and contextual signals. Early statistical methods emphasized bag-of-words features with linear classifiers, while recent work leverages pre-trained transformers and domain adaptation to handle informal, noisy user-generated content. Researchers have also considered temporal and user-level signals to capture dynamics and personalization effects.\n\nLexicon-based approaches map words to affective scores and aggregate them at the message level (Mohammad and Turney, 2013; Buechel and Hahn, 2018). Multimodal fusion models combine text, image, and audio signals to infer affective states (Zadeh et al., 2017; Tsai et al., 2019). Sarcasm and irony have also been studied as confounds to sentiment polarity (Ghosh and Veale, 2016; Filatova, 2012).\n\nNeural methods dominate current benchmarks with contextual encoders fine-tuned for emotion labels and ordinal intensities. Transfer learning from large conversational corpora has proven beneficial, as has multitask learning that jointly predicts sentiment, stance, and emotion categories. Distant supervision strategies align emoji or reaction signals to textual content to scale supervision, albeit with increased label noise.\n\nOur work builds on robust fine-tuning with adaptive layer freezing and frequency-aware regularization to mitigate overfitting in small, imbalanced datasets. We further incorporate user-level priors to calibrate predictions when personalization signals are available, and we assess generalization across platforms and event domains.",
    "reason": "The span lists three distinct areas (lexicons, multimodal fusion, sarcasm) in consecutive sentences without transitions or an explicit explanation of how they relate to each other or to a common thread, creating abrupt, unconnected citations.",
    "start": 500,
    "end": 895,
    "label": "Coherence"
  },
  {
    "span": "Exposure-based debiasing adjusts ranking to equalize item opportunity (Singh and Joachims, 2018). Propensity scoring corrects for selection bias in implicit feedback (Schnabel et al., 2016). Counterfactual reasoning estimates outcomes under alternative slates (Bonner and Vasile, 2018). Long-term simulation reveals feedback loops in user satisfaction (Chaney et al., 2018). Multi-objective optimization tunes relevance and fairness trade-offs (Yao and Huang, 2017).",
    "document": "Related Work\n\nFairness in Recommender Systems\n\nFair recommendation has drawn increasing attention due to the risk of disparate exposure among items, providers, and user groups. Recent surveys organize this space by fairness notions (individual vs. group), stakeholders (users, creators, platforms), and system stages (data collection, ranking, feedback) (Ekstrand et al., 2022; Mehrotra and Celis, 2021). Practical deployments must balance utility with fairness under logging bias and delayed impacts.\n\nExposure-based debiasing adjusts ranking to equalize item opportunity (Singh and Joachims, 2018). Propensity scoring corrects for selection bias in implicit feedback (Schnabel et al., 2016). Counterfactual reasoning estimates outcomes under alternative slates (Bonner and Vasile, 2018). Long-term simulation reveals feedback loops in user satisfaction (Chaney et al., 2018). Multi-objective optimization tunes relevance and fairness trade-offs (Yao and Huang, 2017).\n\nBeyond ranking-time interventions, data-centric approaches explore diversified collection policies and active learning to reduce bias in logged interactions (Jiang et al., 2019). Our approach integrates counterfactual estimation with constrained optimization to enforce target exposure while maintaining estimated utility under observed and counterfactual distributions.",
    "reason": "Each sentence references a different technique with no connective tissue or explanation of how they build on each other. The abrupt listing implies relationships but does not state them, resulting in unclear coherence.",
    "start": 503,
    "end": 969,
    "label": "Coherence"
  },
  {
    "span": "CMU-MOSI is widely used for benchmarking multimodal sentiment models.",
    "document": "Related Work\n\nMultimodal sentiment analysis integrates textual, acoustic, and visual cues to infer affect (Poria et al., 2017; Zadeh et al., 2017). Fusion strategies range from early concatenation to tensor-based and graph-based methods (Tsai et al., 2019; Mai et al., 2020). CMU-MOSI is widely used for benchmarking multimodal sentiment models. In addition, generalization across speakers and domains has been studied through domain-adversarial training and meta-learning (Wang et al., 2019; Tzinis et al., 2021). Our work targets robustness under missing modalities via uncertainty-aware fusion.",
    "reason": "The sentence mentions a specific dataset as a benchmark without citing it at first mention; per the definition, datasets should be cited when first introduced.",
    "start": 276,
    "end": 345,
    "label": "Unsupported claim"
  },
  {
    "span": "The Numenta Anomaly Benchmark (NAB) is a commonly used dataset for streaming anomaly detection.",
    "document": "Introduction\n\nDetecting anomalies in streaming time series is central to applications ranging from industrial monitoring to cybersecurity. Benchmarks that provide labeled temporal anomalies help standardize evaluation across methods. The Numenta Anomaly Benchmark (NAB) is a commonly used dataset for streaming anomaly detection. Other corpora emphasize domain-specific signals such as machine vibrations or network flows, but often lack consistent annotation schemas.\n\nWe introduce a window-aware evaluation framework that accounts for detection latency and false alarm clustering, and we demonstrate its utility across synthetic and real-world streams.",
    "reason": "First mention of a specific dataset lacks a citation to the dataset or its description.",
    "start": 234,
    "end": 329,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent surveys consistently report that reconstruction-based methods underperform forecasting-based models.",
    "document": "Related Work\n\nTime-series anomaly detection methods can be broadly categorized into reconstruction-based and forecasting-based approaches. Reconstruction-based models (e.g., autoencoders) learn typical patterns and flag deviations, whereas forecasting-based models predict future values and detect errors as anomalies. Recent surveys consistently report that reconstruction-based methods underperform forecasting-based models. Nonetheless, reconstruction objectives remain attractive due to their simplicity and applicability in non-stationary environments.\n\nWe revisit reconstruction with a hybrid loss that incorporates predictive signals while preserving interpretability.\n",
    "reason": "Claims about conclusions from 'recent surveys' require citations to those surveys.",
    "start": 319,
    "end": 426,
    "label": "Unsupported claim"
  },
  {
    "span": "[Hernandez et al., 2021)",
    "document": "Related Work\n\nBenchmarking multimodal retrieval has focused on aligning visual and textual embeddings in a shared space (Frome et al., 2013; Kiros et al., 2015). Recent studies [Hernandez et al., 2021) expand evaluations to multilingual captions and noisy web data, while others introduce stronger negative sampling strategies (Radford et al., 2021). However, performance remains sensitive to domain shift and annotation sparsity.",
    "reason": "Mismatched brackets: the citation opens with a square bracket and closes with a parenthesis. It should be (Hernandez et al., 2021) or [Hernandez et al., 2021] consistently, depending on the required style.",
    "start": 177,
    "end": 201,
    "label": "Format"
  },
  {
    "span": "as is common practice, we use the QQPairs benchmark to measure paraphrase identification",
    "document": "Introduction\n\nParaphrase identification underpins numerous NLP applications, from deduplication to question answering. Though supervised models achieve high accuracy on in-domain pairs, they often fail to generalize across domains and lexical variations. To enable comparability with prior studies, as is common practice, we use the QQPairs benchmark to measure paraphrase identification and report standard accuracy and F1.\n\nWe introduce a contrastive calibration layer that reduces sensitivity to spurious lexical overlap. In addition, we perform stress tests on antonymy, negation, and passive-voice transformations to evaluate robustness beyond the benchmark.",
    "reason": "Asserts a community 'common practice' and introduces a specific dataset without providing citations to prior works or to the benchmark itself.",
    "start": 299,
    "end": 387,
    "label": "Unsupported claim"
  },
  {
    "span": "Previous shared tasks on conversational search have standardized the evaluation protocol around nDCG@3.",
    "document": "Related Work\n\nConversational search aims to retrieve documents or passages through multi-turn interactions while maintaining context and intent disambiguation (Culpepper et al., 2018; Dalton et al., 2020). Approaches include hierarchical reranking, query reformulation, and mixed-initiative strategies that integrate retrieval with dialogue (Humeau et al., 2020; Qu et al., 2020). Evaluation metrics and turn-based pooling protocols vary across datasets, creating comparability challenges.\n\nPrevious shared tasks on conversational search have standardized the evaluation protocol around nDCG@3. In this work, we revisit these practices and propose a conversation-aware pooling strategy that accounts for dependency across turns, enabling more reliable model comparison.",
    "reason": "Refers to prior shared tasks and a specific evaluation convention without citing the tasks or guidelines; such claims require citations.",
    "start": 491,
    "end": 594,
    "label": "Unsupported claim"
  },
  {
    "span": "Classical consensus protocols such as Paxos and Raft ensure safety under partial failures (Lamport, 2001; Ongaro and Ousterhout, 2014), while viewstamped replication and Zab provide leader-based replication semantics (Liskov and Cowling, 2012; Junqueira et al., 2011). More recent approaches explore Byzantine fault tolerance for adversarial settings (Castro and Liskov, 1999; Gupta et al., 2020).",
    "document": "Related Work\n\nReliable distributed systems rely on consensus to replicate state across nodes in the presence of failures and asynchrony. Decades of research have produced protocols with trade-offs in latency, throughput, and fault assumptions.\n\nClassical consensus protocols such as Paxos and Raft ensure safety under partial failures (Lamport, 2001; Ongaro and Ousterhout, 2014), while viewstamped replication and Zab provide leader-based replication semantics (Liskov and Cowling, 2012; Junqueira et al., 2011). More recent approaches explore Byzantine fault tolerance for adversarial settings (Castro and Liskov, 1999; Gupta et al., 2020).\n\nWe introduce an implementation strategy that reduces write tail latency in geo-distributed deployments.",
    "reason": "The span summarizes protocols and citations without articulating how they relate to the proposed strategy or which limitation motivates the new work (definition a and c).",
    "start": 245,
    "end": 642,
    "label": "Lacks synthesis"
  },
  {
    "span": "Saliency-based explanations such as gradient maps (Simonyan et al., 2014), Grad-CAM (Selvaraju et al., 2017), and integrated gradients (Sundararajan et al., 2017) are widely used in medical imaging classifiers.",
    "document": "Related Work\n\nInterpretability is critical for clinical deployment of deep learning in medical imaging. Clinicians require faithful and stable explanations to assess model reliability and potential failure modes.\n\nSaliency-based explanations such as gradient maps (Simonyan et al., 2014), Grad-CAM (Selvaraju et al., 2017), and integrated gradients (Sundararajan et al., 2017) are widely used in medical imaging classifiers. Other directions include concept-based explanations (Kim et al., 2018), counterfactuals (Ghosh et al., 2019), and evaluation protocols for explanation faithfulness (Adebayo et al., 2018; Hooker et al., 2019).\n\nWe introduce a stability-aware explanation objective and assess its effect on downstream clinician decision support in retrospective studies.",
    "reason": "The span lists popular techniques without relating them to the paper's aims or explaining what shortcoming the new objective addresses, aligning with (a) and (c).",
    "start": 214,
    "end": 424,
    "label": "Lacks synthesis"
  },
  {
    "span": "Sim-to-real transfer for navigation leverages domain randomization, system identification, and adaptive policies to bridge visual and dynamics gaps (Tobin et al., 2017; Peng et al., 2018; James et al., 2019). Auxiliary objectives and privileged information are also used to stabilize training in simulation (Andrychowicz et al., 2020; Rusu et al., 2017). We adopt a modular architecture with learned perception and classical planning.",
    "document": "Introduction\n\nLearning-based navigation policies are appealing for their adaptability but often struggle when moving from simulation to real-world environments. Bridging the sim-to-real gap requires strategies that address mismatches in both sensing and dynamics.\n\nSim-to-real transfer for navigation leverages domain randomization, system identification, and adaptive policies to bridge visual and dynamics gaps (Tobin et al., 2017; Peng et al., 2018; James et al., 2019). Auxiliary objectives and privileged information are also used to stabilize training in simulation (Andrychowicz et al., 2020; Rusu et al., 2017). We adopt a modular architecture with learned perception and classical planning.\n\nWe demonstrate improved zero-shot deployment across three indoor environments and analyze failure modes with respect to lighting and texture shifts.",
    "reason": "The span lists existing approaches and then immediately states the authors’ choice without articulating why prior work is insufficient or how the choice addresses a specific gap (criterion b).",
    "start": 265,
    "end": 699,
    "label": "Lacks synthesis"
  },
  {
    "span": "Recent works have shown that ROUGE severely underestimates factual consistency in long-form summaries.",
    "document": "Introduction\n\nAutomatic summarization systems are typically evaluated with reference-based metrics such as ROUGE and BLEU, and more recently with embedding-based measures that attempt to capture semantic similarity. While these metrics offer inexpensive, repeatable evaluation, they do not directly assess factual consistency or faithfulness to the source document.\n\nRecent works have shown that ROUGE severely underestimates factual consistency in long-form summaries. Practitioners have therefore turned to question-answering based proxies and human assessment protocols to better capture factuality. However, these approaches are costly and difficult to scale across domains.\n\nIn this paper, we introduce an efficient factual consistency estimator designed for long-form summarization. We analyze its agreement with expert judgments across multiple domains and show that it can replace a substantial fraction of human evaluations without degrading reliability.",
    "reason": "The sentence asserts findings by 'recent works' about ROUGE and factual consistency but provides no citations to support the claim, violating the requirement to cite prior studies at first mention.",
    "start": 367,
    "end": 469,
    "label": "Unsupported claim"
  },
  {
    "span": "In (Klein and Rao, 2018)",
    "document": "Introduction\n\nPre-trained language models have transformed document retrieval by providing dense representations that capture semantic similarity beyond keyword overlap (Karpukhin et al., 2020; Xiong et al., 2021). In (Klein and Rao, 2018) the authors explored hybrid sparse-dense indices for web search, but their method required heavy feature engineering. Subsequent work reduced memory overhead via product quantization (Huang et al., 2020) and learned indexing structures (Luan et al., 2021). Our approach complements these efforts by aligning supervised contrastive objectives with corpus-level constraints, improving calibration and tail recall (Zhan et al., 2021; Formal et al., 2021). We also introduce a reranker that conditions on query-document pairs with cross-attention (Nogueira and Cho, 2020) while enforcing diversity at inference time.",
    "reason": "Wrong citation style: preposition directly before a parenthetical citation. It should be phrased in text as in Klein and Rao (2018) or the preposition removed before (Klein and Rao, 2018).",
    "start": 215,
    "end": 239,
    "label": "Format"
  },
  {
    "span": "Prior clinical studies have demonstrated up to a 25% improvement in AUROC when switching from centralized to federated training.",
    "document": "Background and Motivation\n\nFederated learning (FL) enables multiple hospitals to collaboratively train predictive models without centralizing patient data. This paradigm is attractive for clinical risk prediction tasks where regulatory, ethical, and infrastructural constraints limit cross-institutional data sharing. Prior clinical studies have demonstrated up to a 25% improvement in AUROC when switching from centralized to federated training. Despite these reported gains, practical deployments must contend with substantial heterogeneity in patient populations, measurement practices, and label distributions.\n\nIn this work, we examine FL for early sepsis prediction across a consortium of mid-sized hospitals with non-identical EHR schemas. We introduce a schema-adaptive encoder and a stratified aggregation protocol that mitigates client-drift induced by site-specific covariates. We compare performance under varying coverage of vital signs and labs, and conduct ablations on aggregation cadence, personalization layers, and differential privacy noise scales.\n\nOur results indicate that schema adaptation and robust aggregation substantially narrow the gap between sites with low-resource documentation practices and those with highly standardized EHR pipelines. We also provide a qualitative analysis of feature importance shifts across demographics to inform downstream auditing and model governance.",
    "reason": "This is a quantitative claim about results from 'prior clinical studies' (25% improvement in AUROC) without providing any citations or evidence, violating rule (a) and (b).",
    "start": 318,
    "end": 446,
    "label": "Unsupported claim"
  },
  {
    "span": "The TAC 2011 and DUC datasets have long been considered unreliable for evaluating abstractive methods due to annotation bias.",
    "document": "Introduction\n\nAutomatic summarization has transitioned from extractive techniques to fully abstractive generation with the advent of large pre-trained language models. Evaluation, however, remains contentious, especially when model outputs diverge lexically from references yet preserve factual content. The TAC 2011 and DUC datasets have long been considered unreliable for evaluating abstractive methods due to annotation bias. This observation has motivated alternative evaluation protocols emphasizing factuality, coverage, and faithfulness beyond surface n-gram overlap. In this work, we introduce a reference-lite evaluation suite focused on entity grounding and event consistency, designed to capture semantic adequacy without penalizing legitimate paraphrases.\n\nRelated Work\n\nTraditional metrics like ROUGE correlate imperfectly with human judgments for abstractive systems, spurring research into learned metrics, question-answering-based evaluation, and factual consistency checkers. However, these metrics are sensitive to domain shifts and adversarial phrasing. Our approach complements existing metrics by leveraging structured information extraction and calibration against human-labeled factuality judgments.",
    "reason": "Makes a judgment about widely used datasets without citing sources to support the claim (criterion b and a: first mention of datasets should be cited).",
    "start": 304,
    "end": 429,
    "label": "Unsupported claim"
  },
  {
    "span": "Graph contrastive learning methods augment graphs via node dropping, edge perturbation, and subgraph sampling, optimizing InfoNCE between multiple views (You et al., 2020; Hassani and Ahmadi, 2020; Zhu et al., 2021). We adopt a similar training recipe using node- and edge-level augmentations.",
    "document": "Related Work\n\nSelf-Supervised Learning on Graphs\n\nLearning from graphs without labels relies on structural priors and pretext tasks that capture topology and features. Contrastive and generative approaches have both shown promise for node and graph-level tasks.\n\nGraph contrastive learning methods augment graphs via node dropping, edge perturbation, and subgraph sampling, optimizing InfoNCE between multiple views (You et al., 2020; Hassani and Ahmadi, 2020; Zhu et al., 2021). We adopt a similar training recipe using node- and edge-level augmentations.\n\nGenerative objectives, such as masked node feature recovery or edge prediction, offer complementary signals and can reduce reliance on heavy augmentations (Hu et al., 2020; Rong et al., 2020).",
    "reason": "The authors state they adopt similar training without explaining why existing methods are insufficient or how their approach differs, thus not articulating a gap or perspective.",
    "start": 263,
    "end": 556,
    "label": "Lacks synthesis"
  },
  {
    "span": "Recent audits reveal error rates up to 35% for dark-skinned women in commercial face analysis systems.",
    "document": "Introduction\n\nConcerns about bias in face analysis have prompted calls for demographic disaggregation and transparency. Performance disparities across skin tone and gender categories raise ethical and legal questions for deployment.\n\nRecent audits reveal error rates up to 35% for dark-skinned women in commercial face analysis systems. Such disparities indicate limitations in training data coverage and potential issues with annotation practices and evaluation protocols.\n\nWe present a benchmark focused on balanced demographic representation and propose a training procedure that mitigates disparity while preserving overall accuracy.",
    "reason": "This is a specific empirical statistic attributed to prior audits but lacks citations to the studies or reports that produced the figure.",
    "start": 234,
    "end": 336,
    "label": "Unsupported claim"
  },
  {
    "span": "Self-supervised learning has advanced rapidly with contrastive and non-contrastive objectives. Contrastive methods such as SimCLR and MoCo learn invariances through instance discrimination (Chen et al., 2020; He et al., 2020), while non-contrastive approaches like BYOL, Barlow Twins, and VICReg avoid negatives via redundancy reduction or predictor heads (Grill et al., 2020; Zbontar et al., 2021; Bardes et al., 2022). Cluster-based variants like SwAV and DINO align representations with online codebooks or self-distilled targets (Caron et al., 2020; Caron et al., 2021). Several studies adapt these techniques to medical images, reporting improvements on classification and segmentation benchmarks (Azizi et al., 2021; Sriram et al., 2021; Chaitanya et al., 2020).",
    "document": "Introduction\n\nLabel scarcity is a persistent challenge in medical imaging, where expert annotations are expensive and disease prevalence is often imbalanced. Self-supervised learning offers a promising pathway to leverage large repositories of unlabeled scans.\n\nSelf-supervised learning has advanced rapidly with contrastive and non-contrastive objectives. Contrastive methods such as SimCLR and MoCo learn invariances through instance discrimination (Chen et al., 2020; He et al., 2020), while non-contrastive approaches like BYOL, Barlow Twins, and VICReg avoid negatives via redundancy reduction or predictor heads (Grill et al., 2020; Zbontar et al., 2021; Bardes et al., 2022). Cluster-based variants like SwAV and DINO align representations with online codebooks or self-distilled targets (Caron et al., 2020; Caron et al., 2021). Several studies adapt these techniques to medical images, reporting improvements on classification and segmentation benchmarks (Azizi et al., 2021; Sriram et al., 2021; Chaitanya et al., 2020).\n\nWe present MedMix, a pretraining strategy that integrates anatomy-aware augmentations and slice-order prediction to better capture clinically relevant structures in volumetric data, and we benchmark across three modalities with limited labels.",
    "reason": "The paragraph inventories self-supervised methods and their medical adaptations without explaining how they relate to the authors' objectives or what specific shortcomings motivate the new approach.",
    "start": 262,
    "end": 1030,
    "label": "Lacks synthesis"
  },
  {
    "span": "Classical autoregressive models capture linear dependencies (Box and Jenkins, 1970). Graph-based forecasters exploit sensor topology (Wu et al., 2019). Anomaly detection identifies rare deviations from normal patterns (Chandola et al., 2009).",
    "document": "Related Work\n\nTime series forecasting has evolved from classical statistical models to deep learning architectures that capture complex temporal and cross-series dependencies (Hyndman and Athanasopoulos, 2018; Lim and Zohren, 2021). Recent work emphasizes scalable probabilistic forecasting and structure-aware approaches for multivariate sensors and spatiotemporal graphs (Salinas et al., 2020; Yu et al., 2018).\n\nClassical autoregressive models capture linear dependencies (Box and Jenkins, 1970). Graph-based forecasters exploit sensor topology (Wu et al., 2019). Anomaly detection identifies rare deviations from normal patterns (Chandola et al., 2009). Advances in calibration and uncertainty quantification further improve decision-making under risk (Lakshminarayanan et al., 2017; Tagasovska and Lopez-Paz, 2019).\n\nWe propose a unified framework that performs joint forecasting and anomaly-aware calibration on sensor networks with dynamic topology changes.",
    "reason": "The span lists disparate topics—classical AR models, graph forecasting, and anomaly detection—without explaining their connections or transitions, making the relationships between sentences unclear.",
    "start": 415,
    "end": 657,
    "label": "Coherence"
  },
  {
    "span": "Nguyen, 2017)",
    "document": "Introduction\n\nCross-lingual transfer relies on shared subword vocabularies and multilingual pretraining to bridge language gaps (Conneau and Lample, 2019). Prior work by Nguyen, 2017) highlights the importance of language-specific adapters to prevent negative transfer, which later studies corroborate with modular architectures (Pfeiffer et al., 2020; Ansell et al., 2021). Despite these advances, zero-shot performance still lags in low-resource settings.",
    "reason": "The citation has a closing parenthesis but no opening one. It should be Nguyen (2017) in narrative form or (Nguyen, 2017) if used parenthetically.",
    "start": 170,
    "end": 183,
    "label": "Format"
  },
  {
    "span": "Ganin et al. (2016) proposed domain-adversarial neural networks with gradient reversal. Tzeng et al. (2017) studied adversarial alignment in deep feature space. Ben-David et al. (2010) provided a theoretical framework using HΔH-divergence. Pan and Yang (2010) surveyed transfer learning.",
    "document": "Introduction\n\nUnsupervised domain adaptation seeks to transfer knowledge from a labeled source to an unlabeled target domain. Methods range from adversarial training to moment matching and contrastive alignment.\n\nGanin et al. (2016) proposed domain-adversarial neural networks with gradient reversal. Tzeng et al. (2017) studied adversarial alignment in deep feature space. Ben-David et al. (2010) provided a theoretical framework using HΔH-divergence. Pan and Yang (2010) surveyed transfer learning.\n\nIn contrast, we consider semi-supervised domain adaptation with a small labeled target set.",
    "reason": "Each sentence cites a separate work but does not explain how they relate or build on one another. There are no transitions, and the connections between adversarial methods, theory, and surveys are only implied, leading to poor coherence across multiple sentences.",
    "start": 213,
    "end": 500,
    "label": "Coherence"
  },
  {
    "span": "There are many recent works that explore cross-modal attention for meeting summarization.",
    "document": "Related Work\n\nMultimodal meeting summarization aims to distill long, conversational meetings that include audio, video, and textual artifacts (Murray et al., 2005; Janin et al., 2003; Carletta et al., 2007). Early efforts relied on text-only cues such as discourse structure and speaker turns (McKeown et al., 2005; Gillick and Favre, 2009). More recently, neural models incorporate acoustic and visual features to capture emphasis, turn-taking, and slide transitions (Chen et al., 2021; Li et al., 2022). There are many recent works that explore cross-modal attention for meeting summarization. Our approach extends these lines by introducing modality-adaptive sparsity to focus on salient channels.",
    "reason": "Mentions 'recent works' without citing any of them (definition d and example i).",
    "start": 506,
    "end": 595,
    "label": "Unsupported claim"
  },
  {
    "span": "(Mendez et. al., 2014)",
    "document": "Related Work\n\nEnd-to-end speech recognition advanced with attention-based encoders (Park and Zhou, 2016) and CTC-augmented hybrids (Rao and Kim, 2017). Self-supervised pretraining on unlabeled audio (Liu and Brett, 2020) improves low-resource performance, while conformer blocks enhance locality modeling (Jain and Malik, 2021). For domain robustness, style augmentation and noise simulation are standard (Ghosh and Iyer, 2019). Punctuation restoration has been approached as a sequence tagging task (Silva et al., 2018; (Mendez et. al., 2014)), yet deployment often suffers from latency constraints. We propose a streaming-compatible decoder with adaptive lookahead that preserves accuracy while meeting real-time requirements.",
    "reason": "Misformatted 'et al.' abbreviation as 'et. al.' inside a citation; should be '(Mendez et al., 2014)'.",
    "start": 521,
    "end": 543,
    "label": "Format"
  },
  {
    "span": "The widely used RadiGraph-XL dataset contains over 1.2 million labeled radiographs.",
    "document": "Introduction\n\nDeep learning for diagnostic imaging has benefited from large-scale datasets annotated for multiple pathologies. However, label noise, class imbalance, and domain heterogeneity can confound training and evaluation. The widely used RadiGraph-XL dataset contains over 1.2 million labeled radiographs. While its scale is advantageous, the labels are often derived from report mining, introducing uncertainties that must be modeled.\n\nIn this paper, we present a robust training framework that explicitly accounts for label reliability through hierarchical uncertainty modeling. We show improved calibration and downstream triage performance across several clinical tasks.",
    "reason": "Unsupported claim because it states specific statistics about a dataset without providing a citation at first mention (definition a and b).",
    "start": 229,
    "end": 312,
    "label": "Unsupported claim"
  },
  {
    "span": "Large pre-trained models for code, such as CodeBERT, GraphCodeBERT, and PLBART, have been applied to vulnerability detection and repair (Feng et al., 2020; Guo et al., 2021; Ahmad et al., 2021).",
    "document": "Related Work\n\nNeural code understanding. Pre-trained language models for source code capture lexical and structural regularities by jointly modeling code and natural language (Kanade et al., 2020; Feng et al., 2020). Large pre-trained models for code, such as CodeBERT, GraphCodeBERT, and PLBART, have been applied to vulnerability detection and repair (Feng et al., 2020; Guo et al., 2021; Ahmad et al., 2021). Graph-enhanced approaches encode control and data flow, while sequence-to-sequence models support conditional generation tasks (Allamanis et al., 2018; Guo et al., 2021).\n\nIn contrast, our focus is on calibrating uncertainty for risk-aware triage, enabling auditors to prioritize code regions with high expected utility under labeling budgets.",
    "reason": "The span lists models and applications without clarifying their limitations or how they compare to the paper’s uncertainty-calibration focus, failing to synthesize prior work with the current study (criterion a).",
    "start": 217,
    "end": 411,
    "label": "Lacks synthesis"
  },
  {
    "span": "(O'Neil et. al., 2016)",
    "document": "Related Work\n\nSemantic segmentation progressed from patch-based classifiers to fully convolutional architectures with encoder–decoder designs (Ramos and Xu, 2015; Rao et al., 2017). Multi-scale feature aggregation and attention further improved boundary precision (Kim and Soto, 2019).\n\nEnsemble distillation has been used to compress heavy backbones into real-time models (Perez and Long, 2020). Earlier, (O'Neil et. al., 2016) described a pyramid pooling strategy that enhances global context without excessive compute. Building on these ideas, we propose an efficient decoder that exploits cross-level gating to preserve fine details.\n\nOur approach is complementary to dynamic inference schemes that skip computation adaptively (Lin and Patel, 2021).",
    "reason": "Misspelled 'et al.' in the citation; it should be 'et al.' not 'et. al.': '(O'Neil et al., 2016)'.",
    "start": 406,
    "end": 428,
    "label": "Format"
  },
  {
    "span": "Self-supervised visual representation learning has advanced rapidly with contrastive and non-contrastive frameworks. Contrastive approaches encourage instance discrimination with large negative sets (He et al., 2020; Chen et al., 2020). Momentum encoders stabilize training across views (Wu and Zhang, 2020). Non-contrastive methods avoid explicit negatives by using prediction asymmetry (Grill et al., 2020; Lee and Huang, 2021).",
    "document": "Related Work\n\nWe consider the problem of learning robust image features without manual labels. Self-supervised learning (SSL) constructs pretext tasks that induce invariances aligned with downstream semantics.\n\nSelf-supervised visual representation learning has advanced rapidly with contrastive and non-contrastive frameworks. Contrastive approaches encourage instance discrimination with large negative sets (He et al., 2020; Chen et al., 2020). Momentum encoders stabilize training across views (Wu and Zhang, 2020). Non-contrastive methods avoid explicit negatives by using prediction asymmetry (Grill et al., 2020; Lee and Huang, 2021).\n\nAugmentation policies play a critical role in shaping invariances, with color jitter, cropping, and blurring commonly used to define positive pairs (Tian et al., 2020; Wang and Qi, 2021). There is also work on multi-crop strategies and prototypes (Caron et al., 2020; Sun and Yu, 2021).\n\nDownstream transfer is typically evaluated on classification and detection benchmarks following linear probing or fine-tuning protocols (Kornblith et al., 2019; Luo et al., 2021).",
    "reason": "The span summarizes families of SSL methods and cites prior work but does not connect them to the authors’ aims, limitations addressed, or the gap motivating their study (criteria a and c).",
    "start": 211,
    "end": 641,
    "label": "Lacks synthesis"
  },
  {
    "span": "FedAvg aggregates client updates with weighted averaging (McMahan et al., 2017). FedProx adds a proximal term to handle heterogeneity (Li et al., 2020). pFedMe solves a bi-level objective for personalization (Dinh et al., 2020). Secure aggregation hides individual updates with cryptographic masks (Bonawitz et al., 2017).",
    "document": "Related Work\n\nFederated learning (FL) enables training over decentralized data while aiming to preserve privacy. A central challenge is client heterogeneity, which has motivated both modified optimization objectives and personalization strategies to improve on-device performance.\n\nFedAvg aggregates client updates with weighted averaging (McMahan et al., 2017). FedProx adds a proximal term to handle heterogeneity (Li et al., 2020). pFedMe solves a bi-level objective for personalization (Dinh et al., 2020). Secure aggregation hides individual updates with cryptographic masks (Bonawitz et al., 2017).\n\nOur work introduces a sample-efficient personalization layer that adapts to user distributions while maintaining compatibility with secure aggregation protocols.",
    "reason": "The span mixes aggregation, optimization, personalization, and cryptographic protocols with no transitions or stated relationships, making the connections between works unclear.",
    "start": 282,
    "end": 604,
    "label": "Coherence"
  },
  {
    "span": "Most recent works adopt contrastive learning objectives for multimodal retrieval.",
    "document": "Related Work\n\nMultimodal Retrieval and Alignment\n\nAligning images and text has been studied through cross-modal embedding learning and attention-based fusion. Early approaches learned separate encoders with a ranking loss to place matched pairs nearby in a joint space (Frome et al., 2013; Kiros et al., 2014). Subsequent methods incorporated attention to better focus on salient regions and phrases (Nam et al., 2017; Lee et al., 2018). Most recent works adopt contrastive learning objectives for multimodal retrieval. Large-scale pretraining on web-scale image–text pairs has further improved zero-shot transfer and robustness (Radford et al., 2021; Jia et al., 2021), while fine-grained region-word alignment continues to benefit downstream retrieval (Li et al., 2021). Despite these advances, negative sampling strategies and hard example mining remain open questions for robust generalization.",
    "reason": "Uses the phrase most recent works without providing citations to those works, violating rule d and requiring evidence or references.",
    "start": 438,
    "end": 519,
    "label": "Unsupported claim"
  },
  {
    "span": "Over 70% of industrial recommender systems now rely on graph neural networks.",
    "document": "Introduction\n\nRecommender systems have embraced representation learning to capture complex user-item interactions. Graph-based modeling offers a natural way to incorporate heterogeneous relations such as co-consumption, social ties, and item attributes.\n\nOver 70% of industrial recommender systems now rely on graph neural networks. Motivated by this trend, we propose a scalable graph contrastive learning framework that mitigates popularity bias via topology-aware negatives.\n\nWe analyze robustness under cold-start conditions and demonstrate improvements in both ranking quality and calibration across diverse domains, including media, retail, and education.",
    "reason": "This is a quantitative claim about industry adoption without any evidence or citation, violating the definition that statistics require support.",
    "start": 255,
    "end": 332,
    "label": "Unsupported claim"
  },
  {
    "span": "There have been numerous recent works on multimodal sarcasm detection.",
    "document": "Related Work\n\nSarcasm detection in text has been explored with feature-engineered classifiers (e.g., lexical, syntactic, and pragmatic cues) and, more recently, neural architectures (Johnson et al., 2017; Li and Xu, 2019; Duarte et al., 2020). Transformer-based models pre-trained on large corpora have further advanced performance on benchmark datasets by leveraging context and user history (Chen et al., 2021; Park and Lin, 2022).\n\nThere have been numerous recent works on multimodal sarcasm detection. In parallel, multimodal sentiment analysis has investigated the fusion of visual and textual signals to better capture affect and intent (Kumar et al., 2020; Sato and Nguyen, 2021). Several studies on meme understanding suggest that visual incongruity paired with textual cues can signal sarcasm, motivating joint modeling of images and captions (Rao et al., 2021; Gupta and Shah, 2022). Our work differs by proposing a contrastive objective that aligns cross-modal cues with discourse-level context.",
    "reason": "The span claims the existence of 'recent works' in multimodal sarcasm detection without providing citations at the point of first mention, violating rule (d).",
    "start": 435,
    "end": 505,
    "label": "Unsupported claim"
  },
  {
    "span": "Most prior work focuses on English newswire and ignores low-resource languages.",
    "document": "Introduction\n\nAbstractive summarization aims to produce concise, faithful synopses that capture the salient content of long documents. Neural encoder–decoder architectures with attention have advanced the state of the art, yet challenges persist regarding factual consistency and domain adaptation.\n\nWhile some progress has been made on scientific and conversational domains, cross-lingual summarization remains underexplored. Most prior work focuses on English newswire and ignores low-resource languages. To bridge this gap, we investigate multilingual pretraining and task-specific alignment strategies that target data-scarce settings.",
    "reason": "Generalizes about the scope of prior work without citing surveys or representative studies to substantiate the claim (rule b, e).",
    "start": 427,
    "end": 506,
    "label": "Unsupported claim"
  },
  {
    "span": "Garcia & Ito (2022)",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) combines acoustic and language modeling within unified architectures such as CTC and attention-based encoders (Graves et al., 2006; Chan et al., 2016). Hybrid transducer models improve streaming performance (Graves, 2012; He et al., 2019). Garcia & Ito (2022) introduce a conformer-based decoder for long-form transcription, while contemporaneous work explores self-supervised pretraining for low-resource languages (Baevski et al., 2020; Hsu et al., 2021).\n",
    "reason": "Ampersand used in narrative citation; in author–year narrative style it should be 'Garcia and Ito (2022)'.",
    "start": 300,
    "end": 319,
    "label": "Format"
  },
  {
    "span": "Matrix factorization was first used for top-N recommendation under implicit feedback.",
    "document": "Related Work\n\nTop-N recommendation under implicit feedback is typically tackled with collaborative filtering, where only positive interactions (clicks, views, purchases) are observed and non-interactions are ambiguous (Zhao and He, 2018). Early approaches modeled user-item affinities with heuristic confidence weights and negative sampling strategies (Liu et al., 2017).\n\nMatrix factorization was first used for top-N recommendation under implicit feedback.\n\nSubsequent work introduced pairwise ranking losses and Bayesian regularization to better capture relative preferences (Kumar and Rao, 2019). Deep learning methods further enhanced representation capacity by incorporating content and sequence signals (Fang et al., 2020).",
    "reason": "Asserts a historical 'first use' claim about a method without citing the originating work.",
    "start": 373,
    "end": 458,
    "label": "Unsupported claim"
  },
  {
    "span": "there has been a surge of recent works on multilingual abstractive summarization",
    "document": "Introduction\n\nAbstractive summarization has progressed rapidly with neural encoder–decoder architectures (Rush et al., 2015; See et al., 2017) and pretrained transformers that model long-range dependencies (Vaswani et al., 2017; Liu and Lapata, 2019). Despite these advances, generating faithful and concise summaries across languages remains challenging due to data scarcity and morphological diversity. In particular, there has been a surge of recent works on multilingual abstractive summarization, yet consistent evaluation protocols and datasets are still emerging. Prior research has explored cross-lingual transfer via multilingual encoders and pivot-based translation strategies, while others study language-specific constraints in low-resource settings. We build on these trends by proposing a parameter-efficient approach that leverages shared subword vocabularies and contrastive alignment for content selection.",
    "reason": "Mentions 'recent works' without providing any citations to those works, violating the requirement to cite prior literature at first mention.",
    "start": 420,
    "end": 500,
    "label": "Unsupported claim"
  },
  {
    "span": "Online A/B tests report a 12% average lift in click-through rate when deploying policy-gradient recommenders in production.",
    "document": "Reinforcement Learning for Recommendation\n\nRecommender systems increasingly incorporate sequential decision-making to optimize long-term engagement. While off-policy evaluation methods approximate counterfactual effects from logged data, online experimentation remains the gold standard for validating impact. Online A/B tests report a 12% average lift in click-through rate when deploying policy-gradient recommenders in production. However, these gains are sensitive to reward shaping, exploration strategies, and guardrail constraints on diversity and safety.\n\nWe study a constrained policy optimization approach that balances engagement with coverage and novelty objectives. Our method leverages conservative value estimates and per-user risk budgets to avoid regressions in sensitive segments.\n\nThrough simulation and offline replay on real logs, we show improvements in proxy metrics and reduced variance in estimator bias. We also discuss deployment considerations including warm-start policies, safety overrides, and incremental rollout plans.",
    "reason": "This sentence makes a specific quantitative claim (12% average lift) about prior online A/B tests without citing any studies or reports, violating rule (a) and (b).",
    "start": 310,
    "end": 433,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES task trained on prompt-specific eight-way scoring",
    "document": "Related Work\n\nAutomated essay scoring (AES) has a long history of feature-based methods (Attali and Burstein, 2006; Shermis and Burstein, 2013). Neural approaches adopt CNNs and RNNs to learn holistic representations (Taghipour and Ng, 2016; Dong et al., 2017), and pre-trained language models have further improved performance (Uto et al., 2020; Rodriguez et al., 2021). BERT was used in an AES task trained on prompt-specific eight-way scoring. Other studies explore cross-prompt transfer and domain adaptation to mitigate data scarcity (Xia et al., 2020; Yin et al., 2020). We extend this line by incorporating rubric-aware signals during fine-tuning.",
    "reason": "Specific prior setup is claimed without citing the study that used it.",
    "start": 372,
    "end": 445,
    "label": "Unsupported claim"
  },
  {
    "span": "(Nguyen et al. 2022)",
    "document": "Related Work\n\nCalibration techniques aim to align confidence with accuracy in classification models (Guo et al., 2017; Kull et al., 2019). Recent works propose temperature scaling for transformers (Desai and Durrett, 2020) and investigate distribution shift effects (Ovadia et al., 2019). Empirical results show benefits in text classification (Nguyen et al. 2022) and NER (Ortiz, 2021), though generative tasks remain underexplored (Kumar and Liang, 2022).\n\nWe build on these findings by integrating uncertainty-aware decoding for sequence labeling.",
    "reason": "Missing comma between author group and year in a parenthetical citation; it should be '(Nguyen et al., 2022)'.",
    "start": 344,
    "end": 364,
    "label": "Format"
  },
  {
    "span": "Johnson et al. 2",
    "document": "Introduction\n\nData augmentation for natural language processing aims to expand labeled corpora without costly annotation (Feng et al., 2021; Longpre et al., 2020). Back-translation and paraphrasing are effective for classification and QA (Sennrich et al., 2016; Yu et al., 2018; Wei and Zou, 2019). Johnson et al. 2 showed that meaning-preserving transformations can improve robustness to synonym substitutions, while adversarial rephrasing exposes brittleness in NLI models (Jin et al., 2020; Ribeiro et al., 2020). We propose a controllable augmentation pipeline that preserves discourse structure.",
    "reason": "Wrong use of footnote-like notation; should include a year (e.g., 'Johnson et al. (2019)') or be formatted as a proper footnote.",
    "start": 299,
    "end": 315,
    "label": "Format"
  },
  {
    "span": "A previous study showed that transfer learning surpasses lexicon baselines on Amharic.",
    "document": "Introduction\n\nSentiment analysis in low-resource languages often relies on cross-lingual transfer and weak supervision due to scarcity of labeled data (Artetxe and Schwenk, 2019; Conneau et al., 2020). Multilingual encoders have reduced annotation costs by aligning representations across languages and tasks.\n\nA previous study showed that transfer learning surpasses lexicon baselines on Amharic. Building on this, we explore parameter-efficient adapters and bilingual dictionaries to further improve performance with minimal supervision.",
    "reason": "The sentence refers to a specific prior study and its finding without providing a citation, which is an unsupported claim per rule (b).",
    "start": 311,
    "end": 397,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES task trained on the ASAP dataset with prompt-specific heads.",
    "document": "Automated Essay Scoring: Background\n\nAutomated essay scoring (AES) aims to predict holistic or trait-specific scores that align with human raters. Traditional approaches rely on handcrafted features capturing syntax, discourse structure, and lexical diversity. Recent neural architectures have shifted focus to contextual representations and end-to-end learning from raw text. BERT was used in an AES task trained on the ASAP dataset with prompt-specific heads. While pretraining can capture rich linguistic knowledge, domain adaptation to specific prompts and rubrics remains a key challenge.\n\nAnother obstacle is rater inconsistency: human scores may vary due to rubric interpretation, time pressure, or rater expertise. This motivates uncertainty-aware training objectives and calibration metrics that reflect plausible score ranges rather than point estimates.\n\nWe propose a prompt-aware encoder with multitask heads that jointly predict holistic and trait scores and regularize with inter-rater disagreement. Our experiments on multiple prompts show improved quadratic weighted kappa and better calibration under distribution shift.",
    "reason": "This sentence describes a specific experimental setup involving BERT and the ASAP dataset without citing the source study, matching example (iii) and violating rule (a).",
    "start": 377,
    "end": 461,
    "label": "Unsupported claim"
  },
  {
    "span": "There are many recent works that explore test-time adaptation in medical imaging.",
    "document": "Introduction\n\nMedical image analysis models often suffer from performance degradation when deployed across scanners, hospitals, or patient populations that differ from the training distribution. Unsupervised domain adaptation and self-training have been explored to mitigate this gap (Tzeng et al., 2017; Ganin et al., 2016). More recently, test-time adaptation has emerged as a promising paradigm that adapts models at inference without accessing source data, leveraging objectives such as entropy minimization or consistency regularization (Wang et al., 2021; Sun et al., 2020). There are many recent works that explore test-time adaptation in medical imaging. However, existing approaches typically assume stable data acquisition protocols and limited shifts, which may not hold in routine clinical workflows.\n\nIn this work, we propose a lightweight, memory-bounded test-time adaptation method for 3D segmentation that adjusts normalization statistics and a small subset of parameters using uncertainty-aware objectives. Our approach builds on the observation that volumetric scans provide spatial redundancy, enabling confident pseudo-labels in structurally consistent regions. We evaluate on multi-center MRI and CT datasets with diverse acquisition protocols, demonstrating improvements over source-only and batch-norm adaptation baselines.",
    "reason": "Mentions 'many recent works' without providing citations at the first mention, violating the requirement to cite prior work when referenced.",
    "start": 581,
    "end": 662,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from the ASAP dataset.",
    "document": "Introduction\n\nAutomatic Essay Scoring (AES) aims to predict holistic or trait-specific scores for student essays, reducing grader workload while maintaining reliability (Williamson et al., 2012; Shermis and Burstein, 2013). Early AES models relied on hand-crafted features such as lexical richness, syntactic variety, and discourse structure (Attali and Burstein, 2006; Taghipour and Ng, 2016). Neural architectures have since become dominant, especially transformer encoders that can capture long-range dependencies and discourse cues (Devlin et al., 2019). BERT was used in an AES task trained on essays from the ASAP dataset. Despite progress, cross-prompt generalization remains challenging due to topic leakage and prompt-specific stylistic cues (Uto et al., 2020). We propose a domain-adversarial training framework to improve robustness across prompts without sacrificing on-prompt accuracy.",
    "reason": "This statement mentions a specific model (BERT) applied to a particular dataset/task (ASAP) without citing any source at first mention of the dataset and prior work.",
    "start": 559,
    "end": 628,
    "label": "Unsupported claim"
  },
  {
    "span": "(RoBERTa Liu et al. (2019))",
    "document": "Related Work\n\nPretrained transformers have become the dominant backbone in NLP due to their scalability and strong transfer (Vaswani et al., 2017; Devlin et al., 2019). Variants improve performance via larger batches, more data, and dynamic masking, as in (RoBERTa Liu et al. (2019)) and DeBERTa (He et al., 2021). Domain-adaptive pretraining further tailors representations to specialized corpora (Gururangan et al., 2020). While these methods excel on in-domain benchmarks, robustness under distribution shift remains uneven (Hendrycks et al., 2020; Koh et al., 2021). Our work studies parameter-efficient tuning to mitigate shift without full retraining.",
    "reason": "Double-parenthesis/wrong embedding: the model name and citation are combined as \"(RoBERTa Liu et al. (2019))\". It should be either narrative, e.g., \"RoBERTa (Liu et al., 2019)\", or purely parenthetical, e.g., \"(Liu et al., 2019)\"—not parentheses around another parenthetical year.",
    "start": 256,
    "end": 283,
    "label": "Format"
  },
  {
    "span": "Neural question answering over knowledge graphs has progressed from embedding-based link prediction models (TransE, DistMult, ComplEx) to neural semantic parsers that map questions to logical forms (Bordes et al., 2013; Yang et al., 2015; Dettmers et al., 2018; Liang et al., 2017). Differentiable reasoning with graph neural networks and message passing has been explored for multi-hop queries (Hamilton et al., 2018; Sun et al., 2019). Weak supervision using question-answer pairs has been used to avoid expensive annotations (Chen et al., 2019; Lan and Jiang, 2020). We build upon these ideas to design a hybrid retriever-reasoner for complex questions.",
    "document": "Introduction\n\nAnswering natural language questions over knowledge graphs (KGQA) requires mapping text to entities and relations while performing multi-hop reasoning over incomplete and noisy graphs.\n\nNeural question answering over knowledge graphs has progressed from embedding-based link prediction models (TransE, DistMult, ComplEx) to neural semantic parsers that map questions to logical forms (Bordes et al., 2013; Yang et al., 2015; Dettmers et al., 2018; Liang et al., 2017). Differentiable reasoning with graph neural networks and message passing has been explored for multi-hop queries (Hamilton et al., 2018; Sun et al., 2019). Weak supervision using question-answer pairs has been used to avoid expensive annotations (Chen et al., 2019; Lan and Jiang, 2020). We build upon these ideas to design a hybrid retriever-reasoner for complex questions.\n\nWe evaluate on WebQuestionsSP and ComplexWebQuestions, reporting gains in F1 and calibration.",
    "reason": "The span lists prior work and then states the contribution but does not make explicit what limitation remains or how the proposed hybrid addresses a specific gap (definition b).",
    "start": 200,
    "end": 856,
    "label": "Lacks synthesis"
  },
  {
    "span": "According to industry reports, 78% of e-commerce queries are ambiguous.",
    "document": "Introduction\n\nUnderstanding user intent in e-commerce search is challenging due to short, telegraphic queries and diverse product taxonomies (Bai et al., 2019; Kim and Lee, 2020). Query understanding systems often combine lexical retrieval with neural rerankers and intent classifiers to disambiguate attribute constraints and facet preferences (Zamani et al., 2018; Nogueira and Cho, 2019).\n\nAccording to industry reports, 78% of e-commerce queries are ambiguous. To address ambiguity, prior work has used interactive clarification questions and multi-stage retrieval to elicit user preferences (Aliannejadi et al., 2019; Zamani et al., 2020). We propose a few-shot intent disambiguation model that jointly predicts attribute slots and suggests minimal clarification prompts.",
    "reason": "The span provides a precise statistic ('78%') attributed vaguely to 'industry reports' without a verifiable citation, violating rule (b).",
    "start": 393,
    "end": 464,
    "label": "Unsupported claim"
  },
  {
    "span": "The widely used 'CrowdEvents-2019' dataset contains over 50,000 annotated emergencies",
    "document": "Related Work\n\nEvent detection in social media supports timely response in crisis informatics, enabling early identification of emergencies and resource allocation. Prior research has explored keyword bursts, topic shifts, and graph propagation to detect and track emerging incidents. More recent neural approaches model temporal context and multimodal signals to reduce false alarms in noisy streams.\n\nDatasets for crisis event detection vary in scope and annotation rigor. Some focus on natural disasters, while others include man-made incidents and public health alerts. The widely used 'CrowdEvents-2019' dataset contains over 50,000 annotated emergencies and provides labels for event type, severity, and veracity. Benchmarks on this dataset have spurred development of architectures that combine sequence modeling with credibility assessment.\n\nHowever, generalization remains limited: models trained on one platform or region often degrade when applied elsewhere. To address this, we introduce a domain-adaptive pretraining scheme that leverages weakly labeled data across platforms and aligns representations through contrastive objectives.",
    "reason": "The dataset mention includes a specific name and statistics without any citation to its source, which should be cited at first mention.",
    "start": 573,
    "end": 658,
    "label": "Unsupported claim"
  },
  {
    "span": "there are many recent works exploring the ethical implications of conversational agents in education",
    "document": "Introduction\n\nConversational Agents in Education\n\nDialogue systems are increasingly deployed as tutors, writing assistants, and peer-learning partners. Prior studies report gains in engagement and formative feedback when conversational agents tailor responses to learner goals (Graesser et al., 2005; Nye et al., 2014). However, there are unresolved challenges in calibration, feedback timing, and safeguarding against hallucinated content in open-ended tasks. In particular, there are many recent works exploring the ethical implications of conversational agents in education, covering issues of privacy, consent, and fairness to multilingual learners. Our work complements this line by evaluating transparency interventions during high-stakes writing support.",
    "reason": "Uses a vague reference to many recent works without citing any of them, violating rule d regarding recent works needing citations.",
    "start": 476,
    "end": 576,
    "label": "Unsupported claim"
  },
  {
    "span": "Ergonomic risk in human–robot collaboration has been quantified using RULA, REBA, and EMG-based proxies (McAtamney and Corlett, 1993; Hignett and McAtamney, 2000; Antwi-Afari et al., 2018). Control strategies to reduce worker strain include trajectory optimization, shared control, and admittance control with intent estimation (Mainprice et al., 2016; Losey et al., 2018; Peternel et al., 2017).",
    "document": "Introduction\n\nAs robots enter human-centric workspaces, reducing physical strain and preventing musculoskeletal disorders are critical. Real-time perception of human posture and adaptation of robot behavior can mitigate risk without sacrificing task throughput.\n\nErgonomic risk in human–robot collaboration has been quantified using RULA, REBA, and EMG-based proxies (McAtamney and Corlett, 1993; Hignett and McAtamney, 2000; Antwi-Afari et al., 2018). Control strategies to reduce worker strain include trajectory optimization, shared control, and admittance control with intent estimation (Mainprice et al., 2016; Losey et al., 2018; Peternel et al., 2017).\n\nHowever, prior work typically optimizes ergonomics offline or relies on lab-grade sensors. We present a vision-only, on-robot estimator of ergonomic risk coupled with a receding-horizon controller that enforces safety margins online.",
    "reason": "The span lists measures and control methods without explaining their limitations or how the proposed approach differs, so it lacks synthesis per (a) and (b).",
    "start": 263,
    "end": 659,
    "label": "Lacks synthesis"
  },
  {
    "span": "It is well known that STL decomposition fails under regime shifts",
    "document": "Introduction\n\nTime series anomaly detection in operational telemetry must cope with seasonality, trends, and abrupt distributional changes. Classical approaches often remove seasonality and trend before applying residual-based detectors. It is well known that STL decomposition fails under regime shifts, motivating robust alternatives that adapt to changing periodicities and levels.\n\nRelated Work\n\nRecent methods leverage probabilistic state space models, deep seasonal-trend forecasters, and self-supervised representations that capture regime changes. Evaluation remains difficult due to label sparsity and the heterogeneity of anomalies.",
    "reason": "States a domain-specific generalization ('well known') without any citation supporting the claim.",
    "start": 238,
    "end": 303,
    "label": "Unsupported claim"
  },
  {
    "span": "According to industry reports, 60% of deployed models experience drift within the first month.",
    "document": "Introduction\nMonitoring machine learning systems in production is critical for maintaining performance under changing data. Model degradation can arise from covariate shift, concept drift, or feedback loops introduced by the system’s own predictions (Sculley et al., 2015; Breck et al., 2017). Detecting and responding to these shifts requires robust, low-latency telemetry.\nAccording to industry reports, 60% of deployed models experience drift within the first month. Existing solutions rely on supervised retraining schedules or unsupervised alarms based on feature statistics, but lack principled uncertainty estimates. We propose a drift-aware calibration framework that couples conformal prediction with adaptive retraining triggers.\nOur experiments simulate deployment on three real-world datasets, showing improved stability and reduced false alarms.",
    "reason": "This is a quantitative claim attributed to external reports but no citation is provided. Under rule (b), statistics and external report findings must be supported with references.",
    "start": 375,
    "end": 469,
    "label": "Unsupported claim"
  },
  {
    "span": "the largest open QA dataset in the biomedical domain contains over 5 million question–answer pairs.",
    "document": "Introduction\n\nDomain-specific question answering in biomedicine demands precise grounding in curated knowledge sources and robust handling of technical terminology. To our knowledge, the largest open QA dataset in the biomedical domain contains over 5 million question–answer pairs. Despite its scale, coverage gaps persist for rare diseases and newly introduced therapeutics. We address these gaps by synthesizing weakly supervised QA pairs from structured knowledge graphs and aligning them with controlled vocabularies for improved retrieval and answer normalization.\n",
    "reason": "The sentence asserts a dataset size and a superlative claim without providing any citation or evidence.",
    "start": 183,
    "end": 282,
    "label": "Unsupported claim"
  },
  {
    "span": "Smith et al., 2019)",
    "document": "Introduction\n\nNeural program synthesis has increasingly leveraged large pretrained models to guide search (Devlin et al., 2019; Austin et al., 2021). Yet, benchmarks inflate performance by allowing spurious matches between specifications and outputs, as argued by Smith et al., 2019) and reinforced by follow-up ablations (Chen et al., 2021). Recent datasets emphasize semantic constraints and unit tests (Odena and Sutton, 2020). We contribute a diagnostic suite that isolates reasoning from memorization.",
    "reason": "Missing opening parenthesis for a parenthetical citation, resulting in an unbalanced citation 'Smith et al., 2019)'.",
    "start": 264,
    "end": 283,
    "label": "Format"
  },
  {
    "span": "Several recent works introduce non-autoregressive translation that reaches comparable quality to autoregressive baselines at a fraction of the latency.",
    "document": "Introduction\n\nNeural machine translation (NMT) has been dominated by encoder–decoder architectures with attention, later unified under the Transformer framework. Autoregressive decoding provides strong accuracy but incurs latency proportional to sequence length due to token-by-token generation.\n\nTo mitigate latency, alternative decoding paradigms and distillation strategies have been explored. These approaches aim to parallelize generation while retaining linguistic fidelity and robustness across domains.\n\nSeveral recent works introduce non-autoregressive translation that reaches comparable quality to autoregressive baselines at a fraction of the latency. However, such methods often rely on heavy knowledge distillation and perform unevenly across languages, leaving room for improvements in fluency and adequacy.",
    "reason": "Mentions 'several recent works' and specific comparative performance without providing any citations to those works (rule d and a).",
    "start": 512,
    "end": 663,
    "label": "Unsupported claim"
  },
  {
    "span": "KPI datasets from large web companies typically contain 1–2% anomalies.",
    "document": "Introduction\n\nTime series anomaly detection underpins reliable operation of large-scale online services. Models must detect rare but impactful deviations while keeping false alarms manageable for human operators.\n\nKPI datasets from large web companies typically contain 1–2% anomalies. This extreme class imbalance motivates precision-oriented objectives and robust thresholding under nonstationarity.\n\nWe propose a scenario-adaptive detector that combines seasonal-trend decomposition with conformalized residual scoring. Experiments cover synthetic stress tests and real KPI streams with controlled drift injections.",
    "reason": "Provides a specific prevalence statistic without any citation or supporting evidence; per the definition, statistical claims require sources.",
    "start": 214,
    "end": 285,
    "label": "Unsupported claim"
  },
  {
    "span": "The CityDrive dataset has become the de facto benchmark for urban panoptic segmentation.",
    "document": "Related Work\n\nPanoptic segmentation unifies semantic and instance segmentation by predicting both class labels and object identities for every pixel in an image (Johnson and Patel, 2019). In urban scenes, this task is particularly challenging due to fine-grained categories and heavy occlusions (Nguyen et al., 2020).\n\nThe CityDrive dataset has become the de facto benchmark for urban panoptic segmentation.\n\nPrior approaches include top-down methods that detect instances before labeling pixels (Chen et al., 2019) and bottom-up methods that first partition the image into regions and then assign categories (Garcia and Luo, 2021). Recent transformer-based models have further improved long-range context aggregation (Wang et al., 2022).",
    "reason": "Claims a specific dataset is the de facto benchmark without citing the dataset; first mention of a dataset requires a citation.",
    "start": 319,
    "end": 407,
    "label": "Unsupported claim"
  },
  {
    "span": "Our community has recently converged on using AUROC for fairness evaluation.",
    "document": "Introduction\n\nFairness in machine learning encompasses group and individual notions measured through metrics such as demographic parity, equalized odds, and calibration (Hardt et al., 2016; Kleinberg et al., 2017). Recent work emphasizes trade-offs between utility and fairness and the importance of reporting uncertainty (Mitchell et al., 2019; Corbett-Davies and Goel, 2018). Our community has recently converged on using AUROC for fairness evaluation. However, AUROC does not directly capture disparities across protected groups and can obscure harmful errors under class imbalance.\n\nWe argue for metric suites that decompose performance by subgroup and threshold, and we provide confidence intervals for each metric using stratified bootstrap. We also introduce a reporting template aligned with model cards to standardize fairness documentation (Mitchell et al., 2019).",
    "reason": "This claim describes a recent community-wide trend without citing sources demonstrating such convergence.",
    "start": 378,
    "end": 454,
    "label": "Unsupported claim"
  },
  {
    "span": "According to industry estimates, 70% of users abandon signup flows longer than two steps.",
    "document": "Introduction\n\nDesigning frictionless onboarding is a key objective in consumer applications, where small increases in task completion can translate to significant growth (Sauro and Lewis, 2016; Tullis and Albert, 2013). Prior HCI research has emphasized progressive disclosure and adaptive assistance to reduce cognitive load during form entry.\n\nAccording to industry estimates, 70% of users abandon signup flows longer than two steps. This motivates interface designs that minimize upfront requirements and defer non-critical information to post-onboarding stages.",
    "reason": "This numerical statistic is presented without a source or evidence, making it an unsupported claim.",
    "start": 346,
    "end": 435,
    "label": "Unsupported claim"
  },
  {
    "span": "Wang et al., 2020)",
    "document": "Introduction\n\nTemporal convolutional models have shown promise for long-range forecasting in energy systems (Bai et al., 2018; Lim et al., 2021). Classical ARIMA and VAR baselines remain competitive under strong seasonality (Hyndman and Athanasopoulos, 2018). We revisit probabilistic forecasting with quantile objectives and group-level regularization (Salinas et al., 2019; Rangapuram et al., 2018). Prior hierarchical reconciliation approaches assume coherent bottom-up signals, which is rarely satisfied in practice. As argued by Wang et al., 2020) the risk of error amplification grows with hierarchy depth; our method mitigates this by learning reconciliation operators jointly with the forecaster. We evaluate across retail, traffic, and electricity datasets, demonstrating gains in WIS and CRPS under distribution shift.",
    "reason": "Missing opening parenthesis for a parenthetical citation; should be (Wang et al., 2020).",
    "start": 534,
    "end": 552,
    "label": "Format"
  },
  {
    "span": "in (Kumar et al., 2018)",
    "document": "Related Work\n\nSelf-supervised pretraining has reshaped feature learning in computer vision and speech (He et al., 2020; Schneider et al., 2019). This trend was first discussed in (Kumar et al., 2018) and later extended to cross-modal setups (Radford et al., 2021). However, fine-tuning stability remains a concern when labeled data are scarce (Dodge et al., 2020). We focus on calibration-aware adaptation using a small validation budget.\n\nPrior work on domain adaptation spans adversarial alignment (Ganin et al., 2016), moment matching (Zellinger et al., 2017), and optimal transport (Courty et al., 2017). Our approach complements these by regularizing the learning dynamics without requiring target labels.",
    "reason": "Wrong citation style with a preposition preceding a parenthetical citation. It should be narrative style: 'in Kumar et al. (2018)' rather than 'in (Kumar et al., 2018)'.",
    "start": 176,
    "end": 199,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claim that prompt tuning matches full fine-tuning on translation.",
    "document": "Related Work\n\nParameter-efficient learning has emerged as a promising strategy to adapt large language models without updating all parameters (Houlsby et al., 2019; Pfeiffer et al., 2021). In natural language processing, soft prompts and adapters reduce memory and computation while maintaining competitive accuracy on downstream tasks (Li and Liang, 2021; Hu et al., 2022). In a previous study, the authors claim that prompt tuning matches full fine-tuning on translation. Our work investigates when this parity holds for low-resource neural machine translation by systematically varying data size and domain shift.",
    "reason": "This sentence references a specific prior study and its claim but provides no citation, violating rule (a) and example (ii).",
    "start": 375,
    "end": 473,
    "label": "Unsupported claim"
  },
  {
    "span": "Smith et al., 2021)",
    "document": "Introduction\n\nCausal representation learning aims to disentangle factors of variation to enable robust generalization (Schölkopf et al., 2021). While synthetic benchmarks have driven rapid progress, real-world validation remains limited (Locatello et al., 2019). As shown in Smith et al., 2021) and related studies (Rubenstein and Tishby, 2020), mis-specified inductive biases can mask causal structure under observational settings.\n\nWe propose an interventional diagnostic that perturbs generative mechanisms while preserving low-level statistics, revealing whether learned features capture causal factors or merely correlate with proxies. Our analysis highlights the importance of modularity and sparsity constraints during training.",
    "reason": "Missing opening parenthesis for a parenthetical year; should be 'Smith et al. (2021)'.",
    "start": 275,
    "end": 294,
    "label": "Format"
  },
  {
    "span": "Graph learning on heterogeneous networks employs relation-specific transformations (Schmidt et al., 2019), metapath-based aggregation (Dong et al., 2020), and type-aware attention mechanisms (Fang and Yu, 2021; Qiao et al., 2022).",
    "document": "Introduction\n\nLearning on heterogeneous information networks (HINs) requires capturing semantics across node and edge types. Tasks such as classification and link prediction benefit from architectures that respect type-specific structure while sharing information between related components.\n\nGraph learning on heterogeneous networks employs relation-specific transformations (Schmidt et al., 2019), metapath-based aggregation (Dong et al., 2020), and type-aware attention mechanisms (Fang and Yu, 2021; Qiao et al., 2022). Some works investigate parameter efficiency through shared projections and low-rank adapters (Wei et al., 2023).\n\nWe develop a compact HIN encoder with improved scalability.",
    "reason": "The span lists prior categories and citations but does not explain their limitations or how they inform the compact encoder, lacking explicit gap articulation and author perspective (definition a and c).",
    "start": 293,
    "end": 523,
    "label": "Lacks synthesis"
  },
  {
    "span": "It is widely accepted that contrastive learning consistently outperforms supervised baselines on small datasets.",
    "document": "Related Work\nRepresentation learning with contrastive objectives has shown strong transfer across vision and language tasks by maximizing agreement under data augmentations (Chen et al., 2020; He et al., 2020). Subsequent works refine negative sampling, debias instance frequency, and incorporate multi-view priors (Roberts and Lee, 2021; Tan et al., 2021).\nIt is widely accepted that contrastive learning consistently outperforms supervised baselines on small datasets. However, evidence remains mixed across domains and augmentation regimes, motivating a closer look at data efficiency. We contribute a controlled study that disentangles sample size, augmentation strength, and objective temperature while holding architecture constant.",
    "reason": "This general claim about prior empirical findings lacks citations to support it. According to rule (b), field-specific assertions about performance trends should be cited.",
    "start": 358,
    "end": 470,
    "label": "Unsupported claim"
  },
  {
    "span": "There are widely used bias benchmarks for coreference resolution.",
    "document": "Related Work\n\nFairness evaluation in NLP has progressed from word-level bias assessments to task-specific diagnostics covering sentiment analysis, toxicity detection, and coreference. Methods range from counterfactual data augmentation to adversarial debiasing and calibration-aware training.\n\nThere are widely used bias benchmarks for coreference resolution. Parallel efforts curate templates for masked language models to probe stereotype associations across protected attributes.\n\nWe contribute a counterfactual evaluation suite that jointly varies semantic content and discourse structure, revealing interactions between gender, number agreement, and syntactic salience in coreference models.",
    "reason": "Mentions 'widely used bias benchmarks' (datasets/tasks) without citing them at first mention; violates rule a).",
    "start": 294,
    "end": 359,
    "label": "Unsupported claim"
  },
  {
    "span": "GraphBench-XL is known to contain approximately 12% mislabeled edges.",
    "document": "Introduction\n\nLearning over knowledge graphs requires robustness to incomplete and noisy structure. Link prediction methods often assume clean supervision, yet practical datasets include annotation errors and sparsity.\n\nGraphBench-XL is known to contain approximately 12% mislabeled edges. This level of noise can bias embedding learning and lead to overconfident predictions on corrupted triples.\n\nWe introduce a noise-aware training objective that estimates edge reliability jointly with representation learning, improving resilience to mislabeled relations without requiring clean validation sets.",
    "reason": "This sentence provides a specific quantitative statistic about a dataset without any citation or evidence, violating the definition that statistics must be supported.",
    "start": 220,
    "end": 289,
    "label": "Unsupported claim"
  },
  {
    "span": "A prior CTF challenge introduced the first dataset of real-world firmware backdoors.",
    "document": "Introduction\n\nAnalyzing embedded firmware is critical for securing the software supply chain, yet large-scale datasets remain limited due to proprietary constraints and device heterogeneity (Costin et al., 2014; Chen et al., 2016). Recent research focuses on scalable emulation, symbolic execution, and learning-based triage to surface vulnerabilities at scale (David et al., 2016; Feng et al., 2020). Public corpora for malware classification and binary analysis exist, but they seldom target embedded backdoors specifically (Raff et al., 2018).\n\nA prior CTF challenge introduced the first dataset of real-world firmware backdoors. Building on this premise, we develop an automated pipeline for function-level backdoor localization and propose evaluation metrics tailored to control-flow manipulations common in embedded code.",
    "reason": "Claiming a 'first dataset' from a prior event requires a precise citation; none is given.",
    "start": 548,
    "end": 632,
    "label": "Unsupported claim"
  },
  {
    "span": "there are many recent works that explore this topic",
    "document": "Introduction\n\nLow-resource machine translation (MT) aims to overcome data scarcity by leveraging transfer, multilingual pretraining, and data augmentation. While high-resource pairs benefit from billions of tokens, many language pairs lack parallel corpora, leading researchers to explore unsupervised and semi-supervised techniques. In particular, there are many recent works that explore this topic, spanning back-translation, dual learning, multilingual adapters, and active data selection.\n\nRelated Work\n\nPretrained sequence-to-sequence models have been adapted via vocabulary expansion, lightweight adapter layers, and lexicon induction. Data augmentation strategies such as noising, pivoting, and mined bitext have also been extensively discussed in the literature.",
    "reason": "Mentions 'many recent works' without citing any of them.",
    "start": 349,
    "end": 400,
    "label": "Unsupported claim"
  },
  {
    "span": "Self-supervised pretraining for medical imaging has adopted contrastive objectives, masked image modeling, and multi-instance schemes to reduce label dependence (Zeng et al., 2021; Azizi et al., 2021; He et al., 2022). Cross-modal pretraining aligns images with reports to transfer semantic structure (Zhang et al., 2020; Srinivasan et al., 2022). Large curated datasets like MIMIC-CXR and CheXpert enable scaling these approaches (Johnson et al., 2019; Irvin et al., 2019).",
    "document": "Related Work\n\nMedical imaging tasks often face limited annotations and domain shift across sites and devices, making representation learning crucial (Esteva et al., 2019; Oakden-Rayner et al., 2020). Recent approaches leverage pretraining and transfer to reduce reliance on expert labels.\n\nSelf-supervised pretraining for medical imaging has adopted contrastive objectives, masked image modeling, and multi-instance schemes to reduce label dependence (Zeng et al., 2021; Azizi et al., 2021; He et al., 2022). Cross-modal pretraining aligns images with reports to transfer semantic structure (Zhang et al., 2020; Srinivasan et al., 2022). Large curated datasets like MIMIC-CXR and CheXpert enable scaling these approaches (Johnson et al., 2019; Irvin et al., 2019).\n\nDownstream evaluation typically targets classification, segmentation, and localization tasks, with transfer across modalities such as X-ray, CT, and ultrasound (Raghu et al., 2019; Taleb et al., 2020).",
    "reason": "The span lists categories and datasets without indicating how they motivate or inform the authors' approach, thereby lacking synthesis in the sense of definition a and c.",
    "start": 290,
    "end": 764,
    "label": "Lacks synthesis"
  },
  {
    "span": "BERT was used in an AES task trained on essays with 5 prompt-specific regressors.",
    "document": "Related Work\n\nAutomated Essay Scoring (AES) research spans hand-crafted feature pipelines, neural sequence models, and pretrained language models. Feature-based systems relied on surface fluency, grammar, and discourse indicators, whereas neural approaches capture broader semantic coherence.\n\nBERT was used in an AES task trained on essays with 5 prompt-specific regressors. Other configurations include multi-task setups that predict rubric dimensions alongside holistic scores and adversarial training to reduce prompt leakage. However, scoring drift across prompts and domains remains a challenge, especially when rubrics encode stylistic conventions rather than content mastery.\n\nOur approach treats prompts as latent factors within a shared scorer, regularized by counterfactual prompt augmentation. We demonstrate gains in cross-prompt generalization and fairness-sensitive slices such as ESL writers and non-standard dialects.\n\nWe release an evaluation suite that disentangles prompt memorization from genuine writing proficiency.",
    "reason": "This sentence describes a specific setup of an AES task involving BERT and '5 prompt-specific regressors' without citing the source that introduced or evaluated this configuration (example iii; rule a/b).",
    "start": 294,
    "end": 375,
    "label": "Unsupported claim"
  },
  {
    "span": "In (Nguyen and Perez, 2018)",
    "document": "Introduction\n\nMeta-learning aims to rapidly adapt models to new tasks using limited data (Finn et al., 2017; Rusu et al., 2019). In (Nguyen and Perez, 2018), a gradient-based method is presented to improve adaptation stability via layer-wise learning rates. Subsequent work extends these ideas with probabilistic formulations and task augmentation (Grant et al., 2018; Antoniou et al., 2019).\n\nParallel research investigates metric-based approaches that learn similarity measures across episodes (Vinyals et al., 2016; Snell et al., 2017). Hybrid strategies combine gradient-based and metric-based techniques to leverage complementary strengths (Oreshkin et al., 2018; Sung et al., 2018).",
    "reason": "Wrong citation style: preposition placed before a parenthetical citation. Should be narrative style 'In Nguyen and Perez (2018)' or remove 'In' and keep '(Nguyen and Perez, 2018)'.",
    "start": 129,
    "end": 156,
    "label": "Format"
  },
  {
    "span": "Miller et al. 2",
    "document": "Introduction\n\nPersonalized Recommender Systems\n\nLearning from implicit feedback requires careful treatment of exposure and popularity bias (Joachims et al., 2017; Schnabel et al., 2016). Miller et al. 2 argue that counterfactual estimators reduce bias in ranking metrics, while subsequent work integrated inverse propensity scoring into deep recommenders (Saito, 2020). Graph-based models (Ying et al., 2018) propagate signals across users and items, and contrastive learning leverages augmentations to improve robustness (Xie et al., 2020). Despite progress, generalization to cold-start users and items remains a challenge (Vartak et al., 2017).",
    "reason": "Improper use of a footnote-style numeral instead of a proper citation with a year; should include the year (e.g., Miller et al., 2019) or use a properly formatted footnote.",
    "start": 187,
    "end": 202,
    "label": "Format"
  },
  {
    "span": "((Chen et al., 2019))",
    "document": "Introduction\n\nNeural response generation has advanced rapidly with the availability of large pretrained encoders and decoders that can be adapted for dialogue. Earlier systems focused on retrieval or template matching, but recent work has shown that conditional generation can capture richer pragmatic cues (Rao and Patel, 2021; Kim and Ortega, 2022). Safety and controllability, however, remain open challenges when models are deployed in zero-shot or few-shot regimes (Singh et al., 2020).\n\nWe adopt a transformer-based architecture with lightweight adapters to balance efficiency and performance (Garcia and Sun, 2021). For parameter-efficient tuning, we follow prior work on prompt-conditioned decoders (Almeida et al., 2022) and evaluate on multi-domain conversations.\n\nWhile earlier studies report gains from scaling model size, we find that structure-aware pretraining helps more consistently across domains ((Chen et al., 2019)). We also observe that curriculum-style sampling improves robustness to topic shifts (Vega et al., 2023). Our contributions include a unified training recipe and an analysis of transfer across conversational intents.",
    "reason": "Double parentheses around a parenthetical citation. Correct APA-style parenthetical citation should be (Chen et al., 2019), not ((Chen et al., 2019)).",
    "start": 915,
    "end": 936,
    "label": "Format"
  },
  {
    "span": "The WMT18 Biomedical Translation Task established benchmark test sets for English–German and English–French.",
    "document": "Introduction\n\nDomain adaptation in machine translation remains challenging due to terminology drift and limited in-domain parallel data (Koehn and Knowles, 2017; Chu and Wang, 2018). Recent evaluations highlight the sensitivity of neural systems to genre mismatches and rare terms (Freitag et al., 2022; Kocmi et al., 2022). The WMT18 Biomedical Translation Task established benchmark test sets for English–German and English–French. Subsequent work has explored lexicon-constrained decoding and glossary integration to handle domain-specific terminology (Song et al., 2019; Post and Vilar, 2018). We propose a retrieval-augmented adapter that targets biomedical term translation without retraining the full model.",
    "reason": "The sentence mentions a specific shared task and its benchmarks but provides no citation; per the definition, shared tasks must be cited at first mention.",
    "start": 325,
    "end": 433,
    "label": "Unsupported claim"
  },
  {
    "span": "The TREC Deep Learning Track has popularized large-scale passage ranking benchmarks.",
    "document": "Related Work\n\nNeural information retrieval has advanced from interaction-focused architectures to dual-encoder dense retrievers that enable efficient nearest neighbor search (Guo et al., 2016; Xiong et al., 2021). MS MARCO introduced large-scale supervision for passage ranking and QA, catalyzing improvements in training and evaluation protocols (Nguyen et al., 2016).\n\nThe TREC Deep Learning Track has popularized large-scale passage ranking benchmarks. Concurrently, hard-negative mining and cross-encoder distillation have become standard techniques to close the gap between efficiency and effectiveness (Hofstätter et al., 2020; Qu et al., 2021).",
    "reason": "The sentence asserts an influence of a specific venue without any citation to support it, which is an unsupported claim per rule (b).",
    "start": 371,
    "end": 455,
    "label": "Unsupported claim"
  },
  {
    "span": "Exploration in reinforcement learning includes intrinsic motivation via prediction error or information gain (Pathak et al., 2017; Burda et al., 2019), count-based and pseudo-count bonuses (Bellemare et al., 2016; Ostrovski et al., 2017), ensemble disagreement (Osband et al., 2016; Shyam et al., 2019), and curiosity driven by successor representations (Janz et al., 2019). We apply intrinsic rewards to continuous-control robotics tasks.",
    "document": "Related Work\n\nEfficient exploration remains a bottleneck in sparse-reward and long-horizon reinforcement learning problems. Numerous strategies have been proposed to encourage informative behaviors when extrinsic feedback is limited.\n\nExploration in reinforcement learning includes intrinsic motivation via prediction error or information gain (Pathak et al., 2017; Burda et al., 2019), count-based and pseudo-count bonuses (Bellemare et al., 2016; Ostrovski et al., 2017), ensemble disagreement (Osband et al., 2016; Shyam et al., 2019), and curiosity driven by successor representations (Janz et al., 2019). We apply intrinsic rewards to continuous-control robotics tasks.\n\nOur experiments cover MuJoCo and manipulation benchmarks with ablations on reward scaling and normalization.\n",
    "reason": "The span enumerates prior approaches and then states the authors' application choice without articulating what is missing in existing methods or why their setting requires a new approach, satisfying (b).",
    "start": 235,
    "end": 674,
    "label": "Lacks synthesis"
  },
  {
    "span": "(Chen et. al., 2018)",
    "document": "Related Work\n\nCalibration techniques for neural classifiers include temperature scaling, isotonic regression, and Bayesian approximations (Guo et al., 2017; Kuleshov and Liang, 2015). For sequence models, label smoothing can trade accuracy for calibration benefits (Müller et al., 2019). In multilingual settings, shared subword vocabularies complicate uncertainty estimation due to token frequency imbalances (Kumar and Li, 2020). Prior work (Chen et. al., 2018) proposed feature-space ensembling to reduce variance during inference, but did not analyze token-level calibration under domain shift. Our method introduces span-aware temperature scaling that accounts for contextual ambiguity and exposure bias (Santos and Rivera, 2021).",
    "reason": "Typographical error in citation: uses \"et. al.\" instead of the correct \"et al.\"; should be \"(Chen et al., 2018)\".",
    "start": 443,
    "end": 463,
    "label": "Format"
  },
  {
    "span": "CTC-based models align frames to labels with monotonic constraints (Graves et al., 2006). Sequence-to-sequence models learn attention-based alignments (Chan et al., 2016). Self-training leverages unlabeled audio with pseudo-labels (Kahn et al., 2020). End-to-end Transformers improve long-context modeling (Gulati et al., 2020).",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) replaces hand-engineered pipelines with neural models trained directly from audio to text. Advances include alignment mechanisms, architectural changes, and semi-supervised learning.\n\nCTC-based models align frames to labels with monotonic constraints (Graves et al., 2006). Sequence-to-sequence models learn attention-based alignments (Chan et al., 2016). Self-training leverages unlabeled audio with pseudo-labels (Kahn et al., 2020). End-to-end Transformers improve long-context modeling (Gulati et al., 2020).\n\nWe study how alignment choices interact with semi-supervised training under domain shift.",
    "reason": "The four sentences present different ASR directions without transitions or explicit relationships, making the linkage among the cited works and to the preceding summary unclear.",
    "start": 244,
    "end": 572,
    "label": "Coherence"
  },
  {
    "span": "The SemEval-2021 Task X used BERT-base as the official baseline.",
    "document": "Related Work\n\nStance detection has benefited from pretraining and intermediate-task transfer, with datasets spanning political debates, vaccine discussions, and rumor veracity. Differences in annotation schema and label granularity complicate cross-dataset transfer, prompting standardized shared tasks that provide common evaluation protocols.\n\nThe SemEval-2021 Task X used BERT-base as the official baseline. Later work expanded to domain-adaptive pretraining and multi-target stance, introducing auxiliary objectives for claim detection and evidence retrieval.\n\nWe benchmark a suite of pretrained encoders under consistent preprocessing and hyperparameter settings, highlighting the impact of lexical normalization and label smoothing. Our cross-domain experiments reveal substantial performance variability tied to topic drift and author style.",
    "reason": "References a specific shared task and its baseline configuration without citing the task or its official report.",
    "start": 346,
    "end": 410,
    "label": "Unsupported claim"
  },
  {
    "span": "Dual-encoder architectures encode code and query separately and score similarity with inner products (Gu et al., 2018; Cambronero et al., 2019). Traditional IR baselines such as BM25 remain competitive with careful tokenization (Robertson and Zaragoza, 2009). Multilingual code corpora present unique tokenization challenges (Wang et al., 2021; Lauriola et al., 2022).",
    "document": "Related Work\n\nNeural code search aims to retrieve relevant code snippets given natural language queries. Dense retrieval with learned encoders has become the de facto approach, yet performance depends on representation quality, negative sampling, and cross-lingual generalization in multilingual repositories.\n\nDual-encoder architectures encode code and query separately and score similarity with inner products (Gu et al., 2018; Cambronero et al., 2019). Traditional IR baselines such as BM25 remain competitive with careful tokenization (Robertson and Zaragoza, 2009). Multilingual code corpora present unique tokenization challenges (Wang et al., 2021; Lauriola et al., 2022).\n\nHard negative mining, in-batch negatives, and multi-vector representations have been explored to improve retrieval quality. Joint training with code summarization or docstring generation can enrich the shared space. Domain-specific pretraining further helps capture API usage patterns and coding idioms.\n\nWe present a multilingual code-search model with subword- and identifier-aware tokenization, trained with adversarial hard negatives and language-balanced batches. Our experiments cover repositories across programming languages and demonstrate consistent improvements over strong baselines.",
    "reason": "The span lists dual encoders, BM25 competitiveness, and multilingual tokenization challenges as separate statements without transitions or explicit explanation of how they relate to each other, resulting in poor coherence.",
    "start": 311,
    "end": 679,
    "label": "Coherence"
  },
  {
    "span": "(Nguyen, 2021,; Patel et al., 2019)",
    "document": "Introduction\n\nLarge-scale pretraining has transformed sequence modeling by enabling effective transfer with limited task-specific data (Chen and Rao, 2018; Silva et al., 2020). Cross-lingual objectives further align representations across languages to improve zero-shot generalization (Park and Ahmed, 2021).\n\nRecent studies explore domain-adaptive continued pretraining on specialized corpora (Nguyen, 2021,; Patel et al., 2019), while parameter-efficient tuning reduces compute and storage overhead (Kim and Flores, 2022). We synthesize these directions via a lightweight adapter stack and demonstrate strong gains on low-resource benchmarks.\n\nOur ablations compare masking strategies and adapter depths to isolate the sources of improvement (Diaz and Huang, 2022).",
    "reason": "Incorrect punctuation in a multiple-citation parenthetical: extraneous comma before semicolon. Should be '(Nguyen, 2021; Patel et al., 2019)'.",
    "start": 394,
    "end": 429,
    "label": "Format"
  },
  {
    "span": "Prior studies have explored neural approaches for feedback generation in writing, programming, and mathematics (Xiong et al., 2018; Piech et al., 2015; Wang et al., 2020; Nguyen et al., 2021).",
    "document": "Introduction\n\nAutomated formative feedback can improve learning outcomes by providing timely guidance at scale (Shute, 2008). Recent advances in language and program modeling open opportunities for personalized, context-aware feedback across domains. Prior studies have explored neural approaches for feedback generation in writing, programming, and mathematics (Xiong et al., 2018; Piech et al., 2015; Wang et al., 2020; Nguyen et al., 2021). However, institutions face diverse curricula, heterogeneous rubrics, and equity considerations when deploying such systems.\n\nWe present a framework that adapts feedback policies to course-specific rubrics using small labeled samples and unlabeled interaction logs, with safeguards against content leakage and demographic bias.",
    "reason": "The span enumerates prior work across domains without explaining their methods, limitations, or connection to the presented framework, lacking synthesis and author perspective (criteria a and c).",
    "start": 251,
    "end": 443,
    "label": "Lacks synthesis"
  },
  {
    "span": "Garcia et al. 1",
    "document": "Related Work\n\nContrastive learning has become a standard tool for visual representation learning (Chen et al., 2020; He et al., 2020). Garcia et al. 1 introduce a hard-negative mining strategy to balance informative and noisy pairs in large-scale training. Other works adopt momentum encoders and memory banks to stabilize the objective (Wu et al., 2018; He et al., 2020).\n\nSeveral studies explore debiased and supervised variants of contrastive objectives to mitigate sampling artifacts (Robinson et al., 2021; Khosla et al., 2020). Multi-view and multi-modal extensions further improve invariance and transfer (Tian et al., 2020; Radford et al., 2021).",
    "reason": "Improper footnote-style notation in place of a proper citation; include the year (e.g., 'Garcia et al. (2021)') or use a correctly formatted footnote (e.g., 'Garcia et al.^1') alongside a standard citation.",
    "start": 135,
    "end": 150,
    "label": "Format"
  },
  {
    "span": "Previous studies report that back-translation yields up to 3 BLEU improvement on low-resource pairs.",
    "document": "Related Work\n\nNeural machine translation (NMT) has achieved strong results in high-resource scenarios, yet performance degrades rapidly when parallel data is scarce. To mitigate data sparsity, researchers have proposed leveraging monolingual corpora via synthetic parallel data generation and self-training.\n\nBack-translation, in which target-side monolingual data is translated into the source language to augment training, has become a standard technique. Previous studies report that back-translation yields up to 3 BLEU improvement on low-resource pairs. In addition, iterative refinement and noise injection have been examined to enhance synthetic data diversity, but the trade-off between quality and quantity remains an open question in extremely low-resource settings.",
    "reason": "Claims a quantified improvement from 'previous studies' without any citations to those studies (rule b, e).",
    "start": 458,
    "end": 558,
    "label": "Unsupported claim"
  },
  {
    "span": "Constraint-based approaches test conditional independencies to infer equivalence classes (Spirtes et al., 2000; Colombo and Maathuis, 2014). Score-based methods optimize a global objective under acyclicity constraints (Chickering, 2002; Zheng et al., 2018). Functional causal models exploit additive noise and nonlinearity assumptions (Hoyer et al., 2009; Peters et al., 2014). Our method extends score-based modeling with a neural parametrization.",
    "document": "Introduction\n\nCausal Discovery from Observational Data\n\nUnderstanding causal structure enables robust prediction and counterfactual reasoning. In the absence of interventions, causal discovery leverages statistical properties and structural assumptions to infer directed acyclic graphs.\n\nConstraint-based approaches test conditional independencies to infer equivalence classes (Spirtes et al., 2000; Colombo and Maathuis, 2014). Score-based methods optimize a global objective under acyclicity constraints (Chickering, 2002; Zheng et al., 2018). Functional causal models exploit additive noise and nonlinearity assumptions (Hoyer et al., 2009; Peters et al., 2014). Our method extends score-based modeling with a neural parametrization.\n\nRecent advances relax acyclicity constraints with continuous surrogates and leverage representation learning to handle high-dimensional settings (Zheng et al., 2018; Lachapelle et al., 2020).",
    "reason": "The text moves from listing categories of methods to stating an extension without specifying the deficiency in prior score-based models that the extension addresses.",
    "start": 288,
    "end": 736,
    "label": "Lacks synthesis"
  },
  {
    "span": "To the best of our knowledge, we are the first to align audio and code for programming-by-demonstration.",
    "document": "Introduction\n\nProgramming-by-demonstration aims to infer executable programs from user-provided signals such as GUI traces, speech, or sketches. Multimodal supervision can reduce the ambiguity inherent in demonstrations by grounding symbols in perceptual input.\n\nTo the best of our knowledge, we are the first to align audio and code for programming-by-demonstration. We present a sequence-to-sequence model that aligns spoken task descriptions with API call sequences, supported by a small set of paired demonstrations.\n\nOur experiments evaluate generalization to unseen APIs and robustness to disfluent speech, with ablations quantifying the contribution of cross-modal alignment.",
    "reason": "Makes a novelty/priority claim about being the first without providing a survey or citations to substantiate the absence of prior work, violating rule (b).",
    "start": 263,
    "end": 367,
    "label": "Unsupported claim"
  },
  {
    "span": "(2020, Patel et al.)",
    "document": "Related Work\n\nDomain generalization seeks to learn invariances that transfer to unseen environments (Muandet et al., 2013; Gulrajani and Lopez-Paz, 2021). More recent baselines (2020, Patel et al.) improve robustness by mixing feature statistics, while complementary approaches leverage meta-learning (Li et al., 2018; Balaji et al., 2018).\n\nOur framework introduces an invariance prior that regularizes the predictor without domain labels at training time.",
    "reason": "Incorrect author–year order within the parenthetical citation; it should be '(Patel et al., 2020)'.",
    "start": 177,
    "end": 197,
    "label": "Format"
  },
  {
    "span": "Multimodal grounding has benefited from contrastive pretraining that aligns images and text in a shared space (Radford et al., 2021; Jia et al., 2021). Cross-modal fusion architectures integrate visual tokens into language backbones for downstream tasks (Li et al., 2021; Kim et al., 2021). Vision-language pretraining with captioning and matching objectives improves transfer (Wang et al., 2022; Alayrac et al., 2022). Instruction tuning further adapts models for open-ended tasks (InstructBLIP, 2023; LLaVA, 2023).",
    "document": "Introduction\nGrounding language in perception is essential for robust multimodal reasoning and interaction. Recent pretraining paradigms scale across web-curated data to acquire broad visual-linguistic knowledge, but grounding to fine-grained spatial references and compositional semantics remains challenging.\n\nMultimodal grounding has benefited from contrastive pretraining that aligns images and text in a shared space (Radford et al., 2021; Jia et al., 2021). Cross-modal fusion architectures integrate visual tokens into language backbones for downstream tasks (Li et al., 2021; Kim et al., 2021). Vision-language pretraining with captioning and matching objectives improves transfer (Wang et al., 2022; Alayrac et al., 2022). Instruction tuning further adapts models for open-ended tasks (InstructBLIP, 2023; LLaVA, 2023).\n\nWe propose a region-conditioned alignment module that learns bidirectional references between phrases and object proposals using synthetic grounding supervision, improving spatial reasoning and compositional generalization on RefCOCO and NLVR2.",
    "reason": "The span summarizes prior work and cites multiple lines but provides no commentary on their limitations or how they relate to the proposed method; author motivation is not articulated.",
    "start": 312,
    "end": 828,
    "label": "Lacks synthesis"
  },
  {
    "span": "The ConvAI2 competition standardized the evaluation of persona consistency.",
    "document": "Introduction\n\nMaintaining persona consistency is crucial for engaging open-domain dialogue agents (Li et al., 2016; Zhang et al., 2018). Public datasets such as Persona-Chat enable training models that condition responses on profile-like attributes and style descriptors (Zhang et al., 2018). The ConvAI2 competition standardized the evaluation of persona consistency. Despite these advances, existing metrics overemphasize lexical overlap with profile sentences and under-reward pragmatic coherence across turns. We introduce a multi-view evaluation protocol that combines entailment-based persona adherence, dialog-level factual consistency, and human judgments of naturalness.\n\nRelated Work\n\nPersona-grounded dialogue leverages retrieval, generative modeling, or hybrids to ensure persona conformity (Wolf et al., 2019; Roller et al., 2021). Consistency has been modeled through latent traits, speaker embeddings, and memory networks (Qian et al., 2018; Zhang et al., 2020). Evaluation remains challenging due to sparse supervision and the multiplicity of valid responses (Mehri and Eskenazi, 2020).",
    "reason": "First mention of a specific competition's role in standardizing evaluation is made without citing the competition or relevant reports.",
    "start": 293,
    "end": 368,
    "label": "Unsupported claim"
  },
  {
    "span": "As observed in previous work, test-time repair is more effective than training-time data augmentation for generalization.",
    "document": "Introduction\n\nNeural program synthesis aims to generate source code from specifications such as input–output examples, natural language, or partial programs (Devlin et al., 2017; Chen et al., 2021). Recent transformer models fine-tuned on code corpora have shown strong few-shot capabilities, yet generalization to out-of-distribution specifications remains limited (Austin et al., 2021). To mitigate this, researchers explore structured decoding, constraint satisfaction, and repair mechanisms that iteratively modify candidate programs (Gupta et al., 2017; Le et al., 2020).\n\nAs observed in previous work, test-time repair is more effective than training-time data augmentation for generalization. Nonetheless, the interplay between repair strategies and search budgets is under-studied, particularly in low-resource languages and DSLs with strict semantic constraints.\n\nWe propose a verifier-guided repair policy learned via offline reinforcement learning, yielding improved robustness under distribution shift.",
    "reason": "Makes a comparative claim about prior findings without citing the specific studies that reported it.",
    "start": 578,
    "end": 699,
    "label": "Unsupported claim"
  },
  {
    "span": "Johnson 2015",
    "document": "Related Work\n\nData augmentation has become a key regularization tool in machine learning. Classical approaches include noise injection and feature-space perturbations (Bishop, 1995; Simard et al., 1998). In NLP, back-translation and synonym replacement have been widely adopted (Sennrich et al., 2016; Wei and Zou, 2019). Several methods (Johnson 2015; Wang et al., 2018; Brown, 2021) explored task-specific transformations guided by syntactic parses.\n\nRecent work advocates learned augmentation policies that adapt to the target distribution (Cubuk et al., 2019; Chen et al., 2020). Our study complements these approaches by examining how augmentation interacts with model capacity and label noise, providing guidelines for selecting strategies under resource constraints.",
    "reason": "Missing comma between author and year in a parenthetical citation; should be '(Johnson, 2015)'.",
    "start": 339,
    "end": 351,
    "label": "Format"
  },
  {
    "span": "Chen et al. 2",
    "document": "Related Work\n\nCalibration of predicted probabilities has been studied in both computer vision and NLP. Temperature scaling is a strong post-hoc baseline (Guo et al., 2017), while label smoothing can reduce overconfidence during training (Muller et al., 2019). Chen et al. 2 proposed a classwise scaling scheme but did not evaluate robustness under domain shift, which later work addressed (Kumar et al., 2019).\n\nIn contrast, our approach jointly optimizes calibration and accuracy with a margin-aware objective (Ferrer et al., 2021), following recommendations from prior evaluation protocols (Naeini et al., 2015).",
    "reason": "Wrong use of footnotes/numbering in place of a proper citation; should include the year or be reformatted as a proper footnote or author-year citation (e.g., \"Chen et al. (2020)\" or a numeric footnote).",
    "start": 260,
    "end": 273,
    "label": "Format"
  },
  {
    "span": "[Nguyen et al., 2021]",
    "document": "Introduction\n\nFederated learning (FL) enables on-device model training without centralizing raw data (McMahan et al., 2017; Kairouz et al., 2021). Privacy-preserving mechanisms include secure aggregation and differential privacy (Bonawitz et al., 2017; Abadi et al., 2016). Some methods rely on secure aggregation [Nguyen et al., 2021] to mitigate server-side inference attacks, while others target client-side robustness through adversarial training (Nasr et al., 2019). Personalization and heterogeneity remain open challenges (Arivazhagan et al., 2019; Dinh et al., 2020).\n",
    "reason": "Wrong bracket style for an author–year system; square brackets '[...]' should be parentheses '(Nguyen et al., 2021)'.",
    "start": 314,
    "end": 335,
    "label": "Format"
  },
  {
    "span": "The Netflix Prize established matrix factorization as the dominant paradigm for recommendation.",
    "document": "Related Work\n\nRecommender systems have evolved from heuristic neighborhood methods to learned representations that capture user–item interactions. Latent factor models offer compact embeddings that can be regularized and combined with side information.\n\nThe Netflix Prize established matrix factorization as the dominant paradigm for recommendation. Subsequent industrial deployments reportedly refined these models with implicit feedback handling and temporal dynamics.\n\nMore recently, sequence-aware recommenders and transformer-based architectures have gained traction. Our approach integrates sequential modeling with calibrated uncertainty estimates for cold-start scenarios.",
    "reason": "The sentence references a specific competition and claims its impact on the field without any supporting citation to the competition or to works establishing matrix factorization’s dominance.",
    "start": 254,
    "end": 349,
    "label": "Unsupported claim"
  },
  {
    "span": "To address confounding, researchers have proposed propensity scoring, inverse propensity weighting, doubly robust estimation, and instrumental variables (Rosenbaum and Rubin, 1983; Schnabel et al., 2016; Wang et al., 2020; Hartford et al., 2017).",
    "document": "Introduction\n\nEstimating causal effects of recommendations from observational logs is challenging due to selection bias and unobserved confounding. Learning policies that optimize counterfactual outcomes requires careful identification and robust estimation.\n\nTo address confounding, researchers have proposed propensity scoring, inverse propensity weighting, doubly robust estimation, and instrumental variables (Rosenbaum and Rubin, 1983; Schnabel et al., 2016; Wang et al., 2020; Hartford et al., 2017). Parallel streams adapt uplift modeling and heterogeneous treatment effect estimation to ranking (Athey and Imbens, 2016; Kallus, 2020).\n\nOur work studies bandit feedback with delayed outcomes but does not explicitly state how existing estimators fall short in this setting.",
    "reason": "The span enumerates techniques without linking them to the delayed-feedback setting or stating the unresolved gap, failing to synthesize prior work with the paper’s aims (definition a, b, c).",
    "start": 260,
    "end": 506,
    "label": "Lacks synthesis"
  },
  {
    "span": "The MMSD-21 shared task defined a standard benchmark for this problem",
    "document": "Related Work\n\nMultimodal sarcasm detection (MSD) leverages text and visual cues to interpret non-literal meaning, which is particularly challenging in social media contexts. Prior text-only sarcasm work explored lexical incongruity and sentiment shifts (Tsochantaridis et al., 2010; Davidov et al., 2010), and multimodal extensions incorporate image features or visual sentiment (Cai et al., 2019; Schifanella et al., 2016). Transformer-based encoders with cross-modal attention have recently improved fusion quality (Lu et al., 2019; Tan and Bansal, 2019).\n\nThe MMSD-21 shared task defined a standard benchmark for this problem, catalyzing a wave of competitive systems that explored late fusion, co-attention, and contrastive objectives. Subsequent work reports that robust training against label noise common in social media further improves generalization across platforms. In contrast to prior fusion-centric approaches, we propose a calibration-first strategy that normalizes modality confidence before cross-attention, which we show to be crucial under class imbalance.\n\nWe also examine transfer from sarcasm to irony detection, building on insights from multimodal sentiment analysis (Zadeh et al., 2017), and evaluate cross-domain robustness between Twitter and Instagram datasets.",
    "reason": "The sentence references a specific shared task and claims it established a benchmark without providing a citation; first mentions of shared tasks must be cited (violates rule a).",
    "start": 559,
    "end": 628,
    "label": "Unsupported claim"
  },
  {
    "span": "Kim et al.",
    "document": "Introduction\n\nPretrained language models have become the foundation for many NLP tasks due to their ability to capture contextual meaning (Devlin et al., 2019; Liu et al., 2019). Brown and Lee (2021) argue that task-specific prompts can further enhance transfer.\n\nAs argued by Kim et al., prompt calibration is crucial when adapting to low-resource settings, but few works examine calibration under distribution shift (Garcia and Zhou, 2020; Sun et al., 2022). We revisit calibration with a focus on stability and sample efficiency.",
    "reason": "Narrative citation missing the year; it should be 'Kim et al. (YEAR)'.",
    "start": 277,
    "end": 287,
    "label": "Format"
  },
  {
    "span": "Cost-based optimizers estimate cardinalities with statistics such as histograms (Selinger et al., 1979), sketch-based summaries (Cormode and Muthukrishnan, 2005), and learned models (Kraska et al., 2018; Kipf et al., 2019), while join ordering heuristics include dynamic programming, greedy enumeration, and randomized search (Ioannidis and Kang, 2010; Marcus and Papaemmanouil, 2018).",
    "document": "Related Work\n\nQuery optimization is central to relational databases, where the optimizer must choose a plan with near-optimal cost under uncertainty in data distributions and correlations. Errors in cardinality estimates lead to suboptimal plan choices and latency spikes.\n\nCost-based optimizers estimate cardinalities with statistics such as histograms (Selinger et al., 1979), sketch-based summaries (Cormode and Muthukrishnan, 2005), and learned models (Kraska et al., 2018; Kipf et al., 2019), while join ordering heuristics include dynamic programming, greedy enumeration, and randomized search (Ioannidis and Kang, 2010; Marcus and Papaemmanouil, 2018).\n\nWe introduce a calibration layer that adjusts model predictions on the fly using feedback from partial executions.",
    "reason": "This span compiles prior techniques but does not relate them to the proposed calibration layer or clarify which limitations persist, hence lacking synthesis (definition a and b).",
    "start": 274,
    "end": 659,
    "label": "Lacks synthesis"
  },
  {
    "span": "Mikolov [2013]",
    "document": "Introduction\n\nDistributed representations underpin many NLP systems. Word embeddings popularized the notion of semantic similarity in vector spaces (Turney and Pantel, 2010; Baroni et al., 2014). Mikolov [2013] introduced efficient training objectives that scale to large corpora, paving the way for contextual models (Peters et al., 2018; Devlin et al., 2019). Our work examines whether subword information improves rare-word generalization beyond standard tokenization (Sennrich et al., 2016).\n\nWe hypothesize that incorporating character-level signals can bridge gaps observed in morphologically rich languages (Heinzerling and Strube, 2018).",
    "reason": "Wrong bracket style for author-year; should use parentheses in APA-like author-year formats, e.g., \"Mikolov (2013)\" or \"(Mikolov, 2013)\" depending on context.",
    "start": 196,
    "end": 210,
    "label": "Format"
  },
  {
    "span": "Previous studies have conclusively shown that dynamic pruning always improves inference latency without hurting accuracy",
    "document": "Related Work\n\nModel compression techniques aim to reduce the computational cost of deep neural networks while preserving accuracy. Pruning, quantization, and knowledge distillation are among the most widely explored approaches. Static pruning removes weights based on saliency criteria prior to deployment, whereas dynamic pruning selectively activates substructures conditioned on the input.\n\nPrevious studies have conclusively shown that dynamic pruning always improves inference latency without hurting accuracy, motivating widespread interest in conditional computation. Nonetheless, measuring latency gains accurately is nontrivial due to hardware variability, kernel fusion, and batch-size effects. Furthermore, dynamic policies can introduce control-flow divergence that undermines throughput on certain accelerators.\n\nOur work focuses on hardware-aware dynamic pruning that optimizes both computational sparsity and memory bandwidth. We present a compiler-assisted schedule that co-designs gating policies with operator layouts to ensure stable speedups across devices.",
    "reason": "The claim attributes a strong, general conclusion to 'previous studies' without providing any citations, which is an unsupported claim about prior work.",
    "start": 394,
    "end": 514,
    "label": "Unsupported claim"
  },
  {
    "span": "RLHF reduces hallucinations by 37% on average across instruction-following benchmarks",
    "document": "Introduction\n\nAligning large language models with human preferences has become central to improving helpfulness and safety. Reinforcement learning from human feedback (RLHF) introduces a reward signal based on annotated preferences to steer generations toward desired behaviors. RLHF reduces hallucinations by 37% on average across instruction-following benchmarks, motivating widespread adoption in commercial systems.\n\nRelated Work\n\nRecent pipelines combine supervised fine-tuning on curated instructions, reward modeling on paired comparisons, and proximal policy optimization variants. Alternative alignment strategies include constitutional guidance and direct preference optimization, each differing in data requirements and stability.",
    "reason": "Claims a specific quantitative effect (37% on average) without any supporting citation or evidence.",
    "start": 279,
    "end": 364,
    "label": "Unsupported claim"
  },
  {
    "span": "Prior studies consistently report that temporal attention outperforms recurrent models for multi-horizon traffic prediction.",
    "document": "Related Work\n\nTraffic forecasting methods have evolved from classical time series baselines to deep spatiotemporal models that capture road network structure. Graph convolutional networks (GCNs) and their temporal variants (DCRNN, STGCN) model spatial dependencies using sensor graphs while handling temporal dynamics with recurrent or convolutional modules (Li et al., 2018; Yu et al., 2018; Wu et al., 2019). Attention mechanisms have been introduced to better capture long-range temporal patterns and variable periodicities (Zhou et al., 2018; Vaswani et al., 2017, applied to traffic by later works).\n\nPrior studies consistently report that temporal attention outperforms recurrent models for multi-horizon traffic prediction. However, empirical evidence is often confounded by differences in horizon definitions, input window sizes, and evaluation protocols. We therefore advocate for a unified benchmarking setup that fixes these confounders across models and horizons, and we introduce a calibration step to align sensor-wise error scales before graph propagation.\n\nOur work complements graph structure learning approaches (Franceschi et al., 2019) by focusing on temporal aggregation quality under distribution shifts caused by incidents and weather variability.",
    "reason": "The sentence makes a claim about consensus in prior studies without providing any citations to support it (violates rule b: niche-specific comparative claim requires evidence).",
    "start": 606,
    "end": 730,
    "label": "Unsupported claim"
  },
  {
    "span": "(Nguyen et al., 2021",
    "document": "Related Work\n\nGraph neural networks (GNNs) have advanced semi-supervised node classification by propagating information over edges (Kipf and Welling, 2017; Hamilton et al., 2017). Robustness to structural noise has been studied through adversarial training and edge pruning (Zügner et al., 2018; Jin et al., 2020). Data-efficient pretraining on graphs uses masked attribute modeling and context prediction (Hu et al., 2020; You et al., 2020). Recent work proposes graph adapters to quickly personalize models for new graphs (Ma et al., 2022). However, calibration of uncertainty in noisy graphs remains underexplored (Strehl and Ghosh, 2021). We build upon uncertainty-aware message passing (Ng et al., 2020) and extend it with distributional edge weights inspired by Bayesian treatments (Nguyen et al., 2021 to better handle missing links and spurious connections.",
    "reason": "Missing closing parenthesis in a parenthetical citation; should be (Nguyen et al., 2021).",
    "start": 788,
    "end": 808,
    "label": "Format"
  },
  {
    "span": "BERT was first applied to automated essay scoring in a holistic rubric setup using domain adaptation",
    "document": "Introduction\n\nAutomated essay scoring (AES) aims to provide reliable, scalable estimates of writing proficiency. Traditional AES systems rely on hand-crafted features such as lexical diversity, syntactic complexity, and error rates, which can be brittle across prompts and genres. With the advent of deep contextualized representations, neural approaches have begun to replace feature engineering with end-to-end learning.\n\nDespite rapid progress, generalization across prompts and demographic groups remains a core challenge. Cross-prompt transfer often suffers due to distribution shifts in topic, style, and expected discourse structure. To mitigate these issues, researchers have turned to pretraining, domain adaptation, and calibration techniques.\n\nBERT was first applied to automated essay scoring in a holistic rubric setup using domain adaptation, demonstrating that contextual embeddings can capture higher-level discourse signals beyond surface features. Subsequently, work on pairwise ranking and ordinal regression has attempted to better align model outputs with the ordered nature of score levels.\n\nIn this paper, we study robust calibration under domain shift and propose a prompt-aware Bayesian calibration layer that preserves ordinal relationships while adapting to changing score distributions.",
    "reason": "This sentence references a specific prior application and setup ('first applied' and 'domain adaptation') without citing the study responsible, which is a claim about prior work requiring citation.",
    "start": 755,
    "end": 855,
    "label": "Unsupported claim"
  },
  {
    "span": "Paredes et al.",
    "document": "Related Work\n\nResearch on weak supervision has evolved rapidly over the past decade, with methods that combine heuristic labeling functions and generative label models to reduce annotation costs (Sato and Liu, 2019; Kim and Ortega, 2020). Programmatic labeling has been shown to scale supervision signals across domains such as information extraction and text classification (Garcia and Zhou, 2021). A recent survey by Paredes et al. provides a taxonomy of noise-aware training strategies and evaluates their applicability in low-resource settings. In contrast, our approach focuses on calibrating the aggregation of conflicting heuristics using a task-aware prior (Miller and Shah, 2022). We also build on theoretical insights into noise-robust loss functions (Iqbal and Chen, 2018), but differ in how we integrate uncertainty estimates into downstream fine-tuning.\n\nBeyond text, cross-modal weak supervision has leveraged complementary signals from images and audio (Barker and Xu, 2021), though these methods typically assume access to co-occurrence patterns unavailable in our target domain.",
    "reason": "Narrative citation missing year; should be formatted as a narrative citation with the year, e.g., \"Paredes et al. (2021)\" instead of just \"Paredes et al.\".",
    "start": 419,
    "end": 433,
    "label": "Format"
  },
  {
    "span": "Doe, 2021 et al.",
    "document": "Introduction\n\nKnowledge Graph Completion\n\nLink prediction approaches exploit structural patterns and semantics to infer missing edges (Bordes et al., 2013; Trouillon et al., 2016). Recent transformer-based encoders integrate textual descriptions for improved generalization (Yao et al., 2019). Doe, 2021 et al. investigate multi-hop reasoning with reinforcement learning, while complementary work studies rule induction with differentiable logic (Rocktäschel and Riedel, 2017). Despite gains, challenges remain in handling long-tail relations and temporal dynamics (Goel et al., 2020).",
    "reason": "Misordered citation elements in a narrative citation; should be Doe et al. (2021) or (Doe et al., 2021).",
    "start": 294,
    "end": 310,
    "label": "Format"
  },
  {
    "span": "The MS-COCO test split has known annotation leaks that inflate BLEU scores.",
    "document": "Introduction\n\nImage captioning benchmarks have catalyzed progress in grounded language generation, but evaluation practices lag behind modeling advances. Standard metrics prioritize surface n-gram overlap and may not reflect semantic adequacy or grounding fidelity. The MS-COCO test split has known annotation leaks that inflate BLEU scores. As a result, small gains reported by successive models can be difficult to interpret in terms of real-world improvements.\n\nRelated Work\n\nWork on evaluation has proposed reference-free metrics, learned alignment scores, and task-oriented proxies such as retrieval performance. On the modeling side, cross-modal transformers with region features and end-to-end vision encoders continue to improve sample efficiency and compositional generalization.\n\nOur Contributions\n\nWe re-examine evaluation protocols with stratified sampling, deduplicated references, and entity-grounded metrics. We also release a leakage-controlled split to support more reliable comparisons.",
    "reason": "Claims a specific dataset issue (annotation leaks inflating BLEU) without citing evidence, violating rule (b) and (a).",
    "start": 266,
    "end": 341,
    "label": "Unsupported claim"
  },
  {
    "span": "(Patel 2016)",
    "document": "Related Work\n\nProgram synthesis approaches range from enumerative search to neural-guided techniques (Gulwani, 2011; Ellis et al., 2018). Neural program induction benefits from latent execution traces and differentiable interpreters (Reed and De Freitas, 2016; Graves et al., 2016). Grammar-constrained decoding (Patel 2016) has been used to ensure syntactic validity, while type systems and semantic constraints improve correctness (Polikarpova et al., 2016; Parisotto et al., 2017).\n",
    "reason": "Missing comma between author and year in parenthetical citation; it should be '(Patel, 2016)'.",
    "start": 312,
    "end": 324,
    "label": "Format"
  },
  {
    "span": "The original DistilGPT was shown to match its teacher on GLUE",
    "document": "Related Work\n\nKnowledge distillation transfers knowledge from a large teacher to a compact student, often by matching soft targets or intermediate representations (Hinton et al., 2015; Gou et al., 2021). Distillation has been widely adopted to reduce inference latency and memory cost in transformer-based NLP models without substantial loss in quality.\n\nThe original DistilGPT was shown to match its teacher on GLUE, motivating research on task-agnostic compression for generative models. Subsequent methods extend distillation with data augmentation, contrastive objectives, and layer-wise alignment (Sun et al., 2019). In this work, we investigate data-efficient distillation under domain shift.",
    "reason": "Claims about benchmark results for a specific model require a citation to the study reporting them (rule a).",
    "start": 355,
    "end": 416,
    "label": "Unsupported claim"
  },
  {
    "span": "Neural combinatorial approaches encompass pointer networks, attention-based reinforcement learning, and learning-to-branch in MILP (Vinyals et al., 2015; Bello et al., 2017; Kool et al., 2019; Khalil et al., 2016; Gasse et al., 2019).",
    "document": "Related Work\n\nCombinatorial optimization has seen growing interest from the machine learning community, particularly in leveraging neural function approximators to learn heuristics that generalize across instances. Methods vary in their reliance on supervision, search, and problem structure.\n\nNeural combinatorial approaches encompass pointer networks, attention-based reinforcement learning, and learning-to-branch in MILP (Vinyals et al., 2015; Bello et al., 2017; Kool et al., 2019; Khalil et al., 2016; Gasse et al., 2019). Classical baselines include greedy construction, local search, and Lagrangian relaxations (Johnson and McGeoch, 1997; Applegate et al., 2003). Recent works also investigate hybrid solvers that interleave neural policies with exact routines (Nair et al., 2020; Huang et al., 2022).\n\nIn this paper we evaluate performance on routing and assignment tasks with variable budgets and cost structures.",
    "reason": "The span lists prior approaches and citations but does not relate them to the authors’ evaluation setting or indicate what is missing in these methods, providing no explicit connection to the paper’s argument (definition a, c).",
    "start": 294,
    "end": 528,
    "label": "Lacks synthesis"
  },
  {
    "span": "To the best of our knowledge, this is the first work to report human-parity results on biomedical NER.",
    "document": "Introduction\n\nBiomedical named entity recognition (NER) underpins curation, literature mining, and knowledge graph construction. Despite strong performance from domain-adapted transformers, gaps remain in boundary detection and normalization across diverse subdomains.\n\nTo the best of our knowledge, this is the first work to report human-parity results on biomedical NER. We achieve this by combining span-level contrastive pretraining with lexicon-aware decoding and uncertainty calibration for abstention.\n\nWe evaluate on multiple corpora, compare to expert annotations, and analyze disagreements to characterize residual errors and annotation ambiguity.",
    "reason": "Claims novelty and human-parity status relative to prior work but provides no citations to prior state-of-the-art systems or human performance baselines.",
    "start": 270,
    "end": 372,
    "label": "Unsupported claim"
  },
  {
    "span": "(Perez et. al., 2014)",
    "document": "Related Work\n\nData augmentation is widely used in computer vision to improve robustness and generalization, with techniques ranging from geometric transforms to learned policies (Shorten and Khoshgoftaar, 2019; Cubuk et al., 2019). Style transfer has also been leveraged to diversify texture statistics without altering semantics (Gatys et al., 2016). Early work on color jitter and random cropping demonstrated strong baselines for classification and detection (He et al., 2016). Recent approaches automate augmentation policy search to match task characteristics (Perez et. al., 2014).",
    "reason": "Incorrect abbreviation in the citation: \"et. al.\" should be \"et al.\" without a period after \"et\". Correct form is \"(Perez et al., 2014)\".",
    "start": 565,
    "end": 586,
    "label": "Format"
  },
  {
    "span": "A previous study reported that pretrained decoder-only models struggle with out-of-domain repositories.",
    "document": "Related Work\n\nAutomated code summarization aims to generate descriptive natural language comments for source code. Pretrained language models have achieved strong results by leveraging large-scale code corpora and natural language documentation. Encoder–decoder architectures and decoder-only models have both been explored for sequence-to-sequence generation of summaries.\n\nA previous study reported that pretrained decoder-only models struggle with out-of-domain repositories. While anecdotal evidence suggests domain drift degrades performance, systematic evaluations of cross-repository generalization remain scarce.\n\nWe address this gap by constructing a cross-domain benchmark spanning libraries, applications, and build systems from multiple programming ecosystems. We propose retrieval-augmented prompting to ground summaries in project-specific context and introduce evaluation metrics that penalize hallucinated APIs. Experiments demonstrate improved generalization without additional fine-tuning.\n\nRecent Work\n\nContext-aware summarization has incorporated static analysis, call graphs, and snippet retrieval. Our approach complements these directions by integrating lightweight retrieval at inference time.",
    "reason": "Mentions a prior study and its findings without providing a citation to identify the source.",
    "start": 375,
    "end": 478,
    "label": "Unsupported claim"
  },
  {
    "span": "According to industry reports, 62% of mobile apps have adopted on-device learning in some form.",
    "document": "Introduction\n\nFederated learning enables training global models without centralizing user data, thereby mitigating privacy risks while leveraging distributed signals (McMahan et al., 2017). Subsequent work improves communication efficiency via update compression, client selection, and partial participation (Konečný et al., 2016; Sattler et al., 2019). According to industry reports, 62% of mobile apps have adopted on-device learning in some form. Despite this apparent uptake, practical deployments face challenges including non-IID data, stragglers, and privacy accounting under composition. We propose an adaptive server-side optimizer with variance reduction tailored to extreme client heterogeneity.\n",
    "reason": "Presents a specific statistic attributed to unspecified 'industry reports' without citation, violating rule (b).",
    "start": 354,
    "end": 449,
    "label": "Unsupported claim"
  },
  {
    "span": "(Johnson, 2019.",
    "document": "Related Work\n\nTheoretical analyses connect generalization to margin distributions and complexity measures (Neyshabur et al., 2018; Bartlett et al., 2017). The margin-based perspective continues to influence practical bounds and algorithms (Johnson, 2019. Follow-up work unifies margin and PAC-Bayesian viewpoints to offer tighter guarantees (Dziugaite and Roy, 2017; Nagarajan and Kolter, 2019).\n\nOptimistic rates and stability-based analyses further clarify the role of overparameterization (Hardt et al., 2016; Kuzborskij and Lampert, 2018). Together, these developments inform training heuristics and regularization choices used in modern deep learning.",
    "reason": "Missing closing parenthesis in the parenthetical citation; should be '(Johnson, 2019)'.",
    "start": 239,
    "end": 254,
    "label": "Format"
  },
  {
    "span": "Patel et al. 1",
    "document": "Introduction\n\nProgram synthesis from natural language aims to map user intents to executable code sequences (Yin and Neubig, 2017; Chen et al., 2021). Weak supervision via execution results has been explored to reduce annotation costs (Iyer et al., 2018). Neural decoders benefit from structural constraints that reflect grammar productions (Dong and Lapata, 2016). Recent surveys Patel et al. 1 emphasize the role of intermediate representations in improving generalization to unseen tasks. Building on these insights, we introduce a constrained decoding framework with learned repair operators that enforce semantic type checks during generation, improving robustness to spurious correlations (Wang and Liang, 2016; Akyurek and Andreas, 2021).",
    "reason": "Wrong use of footnote-style marker appended to an author citation without a year; should include a year (e.g., Patel et al., 2020) or be formatted as a proper footnote.",
    "start": 381,
    "end": 395,
    "label": "Format"
  },
  {
    "span": "According to industry surveys, 78% of contact centers now deploy intent detection models.",
    "document": "Introduction\n\nTask-oriented dialogue systems depend critically on robust intent detection to route user requests and trigger appropriate backend actions. While academic benchmarks focus on accuracy and few-shot generalization, production systems must handle heterogeneous traffic and strict latency constraints.\n\nAccording to industry surveys, 78% of contact centers now deploy intent detection models. This rapid adoption underscores the need for scalable training pipelines and reliable uncertainty estimates when models are exposed to novel intents and out-of-domain inputs.\n\nIn this paper, we investigate calibration techniques and show that selective prediction can reduce escalation costs without sacrificing user experience.",
    "reason": "Presents a precise statistic from surveys without any citation or evidence (violates rule b).",
    "start": 313,
    "end": 402,
    "label": "Unsupported claim"
  },
  {
    "span": "Brown et al., (2016)",
    "document": "Introduction\n\nStochastic optimization under non-convex objectives has benefited from variance reduction and adaptive stepsizes (Johnson and Zhang, 2013; Kingma and Ba, 2015). Brown et al., (2016) show that iterate averaging can stabilize training in the presence of heavy-tailed noise. However, momentum schemes can amplify bias when gradients are poorly scaled (Wilson et al., 2017).\n\nWe propose a curvature-aware preconditioner with decoupled averaging to balance stability and speed.",
    "reason": "Wrong punctuation in a narrative citation: an extraneous comma before the year in 'Brown et al., (2016)'. It should be 'Brown et al. (2016)'.",
    "start": 175,
    "end": 195,
    "label": "Format"
  },
  {
    "span": "many recent works have explored conversational recommendation",
    "document": "Introduction\n\nConversational recommendation (CR) systems interactively elicit user preferences through dialogue, enabling finer control over item selection and explanation. Unlike static recommenders, CR must reason over evolving contexts and uncertainty, balancing exploration and exploitation in natural language.\n\nMotivated by these challenges, many recent works have explored conversational recommendation via reinforcement learning, belief tracking, and language-guided retrieval. However, the majority of approaches optimize task success on narrow domains, limiting generalization to new item catalogs and conversation styles. Moreover, evaluation protocols vary widely, complicating comparisons across systems.\n\nIn this paper, we propose a modular CR framework that unifies uncertainty-aware preference elicitation with retrieval-augmented generation, and we introduce standardized evaluation recipes for multi-domain settings.",
    "reason": "The phrase references a body of prior research ('many recent works') without citing any of the relevant papers, violating the requirement to cite recent works at first mention.",
    "start": 348,
    "end": 409,
    "label": "Unsupported claim"
  },
  {
    "span": "In the latest CodeXGLUE competition, transformer decoders dominated the leaderboard.",
    "document": "Introduction\n\nProgram Synthesis and Code Generation. Pretrained language models have achieved strong results on code summarization, translation, and generation by leveraging large-scale code corpora (Kanade et al., 2020; Feng et al., 2020). Benchmark suites such as HumanEval and MBPP surfaced reasoning gaps in few-shot settings (Chen et al., 2021; Austin et al., 2021). In the latest CodeXGLUE competition, transformer decoders dominated the leaderboard. Concurrently, constrained decoding and execution-guided search reduce syntactic and runtime errors (Yin and Neubig, 2017; Zhang et al., 2022). We focus on execution-aware pretraining signals that align model likelihood with pass@k performance without relying on external verifiers at inference.",
    "reason": "References a specific competition result without citing the competition or a report, violating the requirement to cite prior events or shared tasks.",
    "start": 372,
    "end": 456,
    "label": "Unsupported claim"
  },
  {
    "span": "Previous studies consistently report that game-based learning improves retention by 30–50% across subjects.",
    "document": "Introduction\n\nDigital game-based learning has been proposed as a means to increase student engagement and deepen conceptual understanding through interactive tasks and immediate feedback. Educators often adopt game elements such as points, levels, and narrative framing to motivate participation.\n\nPrevious studies consistently report that game-based learning improves retention by 30–50% across subjects. Reported benefits are said to persist across age groups and instructional settings, suggesting that the approach generalizes beyond specific curricula.\n\nThis paper investigates whether these purported benefits translate to large introductory courses with heterogeneous student backgrounds. We design a multi-week intervention and measure effects on retention, transfer, and time-on-task.",
    "reason": "The sentence cites a quantitative effect size ('30–50%') attributed to prior studies but does not provide any references to substantiate the statistic.",
    "start": 298,
    "end": 405,
    "label": "Unsupported claim"
  },
  {
    "span": "Personalized federated learning approaches aim to adapt a global model to heterogeneous clients via model partitioning, meta-learning, or proximal regularization (Arivazhagan et al., 2019; Fallah et al., 2020; Li et al., 2020). In this paper, we propose a simple client-adaptive reweighting scheme that improves local performance.",
    "document": "Introduction\n\nFederated Learning and Heterogeneity\nFederated learning enables collaborative model training without centralizing data, but client heterogeneity in data distributions and resources challenges global convergence and fairness. Personalized approaches attempt to tailor models to clients while leveraging cross-client transfer.\n\nPrior Personalization Methods\nPersonalized federated learning approaches aim to adapt a global model to heterogeneous clients via model partitioning, meta-learning, or proximal regularization (Arivazhagan et al., 2019; Fallah et al., 2020; Li et al., 2020). In this paper, we propose a simple client-adaptive reweighting scheme that improves local performance.\n\nOverview of Our Approach\nWe introduce a training objective that assigns client-specific weights to gradients based on locally estimated utility, and we evaluate across vision and language benchmarks under synthetic and real non-IID splits.",
    "reason": "Transitions from a summary of prior work directly to stating the contribution without explicitly identifying the gap or why existing methods are insufficient (definition b).",
    "start": 370,
    "end": 700,
    "label": "Lacks synthesis"
  },
  {
    "span": "In DSTC9, organizers favored automatic metrics over human evaluation to rank submissions.",
    "document": "Introduction\n\nEvaluation of open-domain dialogue systems remains contentious due to the weak correlation of reference-based automatic metrics with human judgments. Shared tasks and challenges attempt to standardize evaluation, but design choices vary widely across years and tracks.\n\nHybrid protocols that combine crowd-sourced pairwise comparisons with automatic screening have emerged, yet transparency about procedures and significance testing is inconsistent. This variability complicates cross-paper comparisons and meta-analyses.\n\nIn DSTC9, organizers favored automatic metrics over human evaluation to rank submissions. Our work advocates for a standardized evaluation card that documents selection criteria, number of judgments, and statistical tests to improve reproducibility.",
    "reason": "Makes a specific claim about the procedures of a particular shared task (DSTC9) without citing an official overview or report, requiring a citation at first mention (rule a).",
    "start": 537,
    "end": 626,
    "label": "Unsupported claim"
  },
  {
    "span": "The FB15k-237 dataset removes inverse relations specifically to prevent test leakage.",
    "document": "Related Work\n\nKnowledge graph completion benchmarks typically evaluate link prediction under closed-world assumptions. Dataset design choices—relation cardinality, entity degree distribution, and leakage—strongly affect reported performance. The FB15k-237 dataset removes inverse relations specifically to prevent test leakage. This change has motivated model families that rely less on trivial symmetry detection and more on compositional reasoning. However, benchmark saturation and narrow relation types can still inflate gains. Our work extends evaluation to multi-source graphs with heterogeneous schemas to test generalization beyond curated splits.",
    "reason": "States a specific property and motivation of a well-known dataset without citing the dataset paper or documentation.",
    "start": 242,
    "end": 327,
    "label": "Unsupported claim"
  },
  {
    "span": "multilingual models outperform monolingual baselines",
    "document": "Introduction\n\nNamed entity recognition (NER) in low-resource languages suffers from data scarcity and domain mismatch. Cross-lingual transfer aims to leverage supervision from high-resource languages via shared subword vocabularies and aligned representations.\n\nA common observation is that multilingual models outperform monolingual baselines in zero-shot transfer, particularly when languages share scripts or lexical borrowings. Yet, gains are inconsistent across domains, and performance degrades sharply on code-switched or noisy data.\n\nWe revisit cross-lingual NER with a focus on subword granularity and character-level augmentation, demonstrating improved robustness on noisy social media text in under-represented languages.",
    "reason": "This statement summarizes prior empirical findings about model performance without citing supporting studies.",
    "start": 291,
    "end": 343,
    "label": "Unsupported claim"
  },
  {
    "span": "Ren et al. (2015) present Faster R-CNN for real-time detection. Redmon and Farhadi (2018) propose YOLOv3 with multi-scale predictions. Lin et al. (2017) introduce FPN to fuse features across layers. Bochkovskiy et al. (2020) optimize inference with Darknet.",
    "document": "Related Work\n\nObject detection methods have progressed rapidly from region-based approaches to single-shot detectors. While accuracy on standard benchmarks has improved, the design trade-offs between speed, robustness, and data efficiency remain central to practical deployment.\n\nRen et al. (2015) present Faster R-CNN for real-time detection. Redmon and Farhadi (2018) propose YOLOv3 with multi-scale predictions. Lin et al. (2017) introduce FPN to fuse features across layers. Bochkovskiy et al. (2020) optimize inference with Darknet.\n\nOur work targets the low-data regime by introducing consistency regularization across multi-resolution feature maps, complementing prior work that scales models and training sets to achieve gains.",
    "reason": "The span enumerates four papers back-to-back without articulating how they build on each other or differ, and offers no transitions or connective claims to clarify relevance. This yields abrupt topic shifts between architectures (criterion a and b) across multiple sentences (criterion c).",
    "start": 280,
    "end": 537,
    "label": "Coherence"
  },
  {
    "span": "many recent works have proposed reference-free metrics for open-domain dialogue",
    "document": "Introduction\n\nEvaluating open-domain dialogue systems remains challenging due to the diversity of valid responses and the weak correlation between surface-level overlap and human judgments. Traditional reference-based metrics such as BLEU and ROUGE often fail to capture conversational quality dimensions like engagement, coherence, and safety. To address these gaps, many recent works have proposed reference-free metrics for open-domain dialogue, aiming to estimate quality directly from the conversation context and response. Despite reported gains, the community lacks consensus on which aspects of quality these metrics reliably capture and how robust they are across domains and styles.\n\nIn this paper, we introduce DiaQual, a holistic evaluation suite that decomposes dialogue quality into coherence, grounding, safety, and user-centricity. We provide standardized prompts, human-annotated benchmarks, and a calibration protocol to relate automatic scores to interpretable human labels.",
    "reason": "The sentence claims the existence of 'many recent works' proposing reference-free metrics but provides no citations to support or exemplify them, violating the requirement to cite recent works.",
    "start": 368,
    "end": 447,
    "label": "Unsupported claim"
  },
  {
    "span": "in (Lopez and Meyer, 2018)",
    "document": "Introduction\n\nMultilingual named entity recognition (NER) has benefited from cross-lingual transfer and multilingual pretraining, reducing annotation costs in low-resource settings (Pires et al., 2019; Hu et al., 2020). We follow the taxonomy proposed in (Conneau et al., 2020) for cross-lingual representation learning and examine alignment-based versus alignment-free methods. We also categorize prior work in (Lopez and Meyer, 2018) to delineate dictionary-based, projection-based, and joint-training paradigms. Despite progress, robustness across typologically diverse languages remains limited (Artetxe et al., 2020; Lauscher et al., 2020).\n",
    "reason": "Wrong citation style for narrative context; it should be 'in Lopez and Meyer (2018)' rather than 'in (Lopez and Meyer, 2018)'.",
    "start": 409,
    "end": 435,
    "label": "Format"
  },
  {
    "span": "(Clark and Gardner, 2018",
    "document": "Related Work\n\nOpen-Domain Question Answering\n\nRetriever-reader frameworks decompose the task into evidence retrieval and answer extraction (Chen et al., 2017; Karpukhin et al., 2020). Datasets such as SQuAD (Rajpurkar et al., 2016) and MultiRC (Khashabi et al., 2018) fostered extractive models, while open-domain settings rely on large corpora and dense retrievers (Lee et al., 2019). Reading strategies like multi-hop reasoning (Yang et al., 2018) and iterative retrieval (Qi et al., 2019) improve coverage. Notably, transferable readers trained on SQuAD generalize poorly without domain adaptation (Clark and Gardner, 2018 and follow-up work report this gap). Generative models (Lewis et al., 2020) mitigate annotation constraints by producing abstractive answers.",
    "reason": "Missing closing parenthesis in a parenthetical citation; it should be (Clark and Gardner, 2018).",
    "start": 601,
    "end": 625,
    "label": "Format"
  },
  {
    "span": "(Baker et al. 2015)",
    "document": "Related Work\n\nPolicy gradient methods enable direct optimization of expected returns and have scaled to complex control tasks (Sutton et al., 2000; Schulman et al., 2015). Trust-region and proximal updates stabilize training by constraining policy divergence (Schulman et al., 2015; Schulman et al., 2017). Actor-critic variants improve sample efficiency with learned baselines (Mnih et al., 2016; Haarnoja et al., 2018).\n\nExploration strategies range from count-based bonuses to intrinsic motivation driven by prediction errors (Bellemare et al., 2016; Pathak et al., 2017). Model-based RL further reduces sample complexity via learned dynamics and planning (Wang et al., 2019; Schrittwieser et al., 2020). For continuous control, distributional critics and entropy regularization provide more stable gradients (Haarnoja et al., 2018; Dabney et al., 2018). Benchmarks commonly follow the MuJoCo protocol (Todorov et al., 2012). A comparative analysis in (Baker et al. 2015) emphasized the role of advantage normalization across tasks.\n",
    "reason": "Missing comma between author and year in a parenthetical citation; should be '(Baker et al., 2015)'.",
    "start": 955,
    "end": 974,
    "label": "Format"
  },
  {
    "span": "In (Kumar et al., 2021)",
    "document": "Related Work\n\nMultimodal pretraining has accelerated progress in tasks that require aligning text and vision. Early cross-modal architectures learned joint embeddings from paired image–text data (Frome et al., 2013; Kiros et al., 2015). Transformer-based models further improved transfer by leveraging large-scale corpora and contrastive objectives (Radford et al., 2021; Jia et al., 2021). In (Kumar et al., 2021), the authors present a video-language model that conditions temporal attention on subtitles, reporting gains on retrieval benchmarks. Subsequent work integrates momentum encoders and masked modeling to stabilize pretraining (He et al., 2020; Bao et al., 2022). Despite these advances, domain shift across video genres and narration styles remains a challenge (Wang et al., 2022).",
    "reason": "Wrong citation style: a narrative construction incorrectly uses a parenthetical citation. It should be phrased as narrative, e.g., \"In Kumar et al. (2021)\" instead of \"In (Kumar et al., 2021)\".",
    "start": 391,
    "end": 414,
    "label": "Format"
  },
  {
    "span": "Our review found that most existing pancreas segmentation models rely on U-Net backbones with minor tweaks.",
    "document": "Related Work\n\nMedical image segmentation in abdominal CT faces challenges due to low contrast, anatomical variability, and limited annotations. Various architectures have been proposed to segment organs and lesions, often building on encoder–decoder designs with skip connections. Our review found that most existing pancreas segmentation models rely on U-Net backbones with minor tweaks. Nevertheless, performance varies widely across cohorts, suggesting that design choices in loss functions, data preprocessing, and post-processing may play a larger role than architectural novelty. Motivated by this, we systematically study training dynamics and regularization in a backbone-agnostic manner, providing a strong and reproducible baseline for pancreas segmentation.\n\nWe also examine test-time augmentation and uncertainty estimation to provide calibrated predictions suited for clinical workflows.",
    "reason": "Claims a summary of prior literature ('our review found...') without providing citations or evidence (criterion b).",
    "start": 281,
    "end": 388,
    "label": "Unsupported claim"
  },
  {
    "span": "there are many recent works that explore this topic",
    "document": "Related Work\n\nLow-resource automatic speech recognition (ASR) commonly relies on transfer learning, multilingual pretraining, and data augmentation to compensate for scarce labeled audio (Besacier et al., 2014; Tong et al., 2017; Pratap et al., 2020). Self-supervised acoustic models have narrowed the performance gap between high- and low-resource settings (Baevski et al., 2020; Hsu et al., 2021). However, there are many recent works that explore this topic, and it remains unclear which strategies are complementary. We focus on sample-efficient fine-tuning with pronunciation lexicon constraints and lightweight adapters.",
    "reason": "The phrase references 'many recent works' without providing citations; per the definition, mentions of recent work must be accompanied by references.",
    "start": 409,
    "end": 460,
    "label": "Unsupported claim"
  },
  {
    "span": "The WMT19 Quality Estimation shared task formalized sentence-level QE with gap tags.",
    "document": "Introduction\n\nQuality Estimation (QE) seeks to predict the quality of machine translation outputs without access to reference translations. It has become central to workflows that prioritize post-editing and triage low-quality outputs. The WMT19 Quality Estimation shared task formalized sentence-level QE with gap tags. While this formulation has driven progress on segment-level prediction, document-level consistency and uncertainty calibration remain underexplored.\n\nWe propose a context-aware QE framework that models cross-sentence dependencies and exposes well-calibrated uncertainty estimates, enabling better downstream decision-making in translation pipelines.",
    "reason": "The sentence mentions a specific shared task and its contribution but provides no citation to the task proceedings or overview paper, which is required at first mention.",
    "start": 236,
    "end": 320,
    "label": "Unsupported claim"
  },
  {
    "span": "Vision-language pretraining has rapidly advanced with dual-encoder contrastive models, generative sequence-to-sequence models, and unified architectures (Radford et al., 2021; Jia et al., 2021; Wang et al., 2021; Li et al., 2022). Training corpora span web-scale image-text pairs, curated caption datasets, and synthetic renderings (Schuhmann et al., 2021; Changpinyo et al., 2021; Nguyen et al., 2022).",
    "document": "Introduction\n\nLearning aligned representations of images and text enables zero-shot recognition, retrieval, and multimodal reasoning. Scaling pretraining has delivered impressive cross-domain generalization, but robustness to distribution shift and spurious correlations remains a concern.\n\nVision-language pretraining has rapidly advanced with dual-encoder contrastive models, generative sequence-to-sequence models, and unified architectures (Radford et al., 2021; Jia et al., 2021; Wang et al., 2021; Li et al., 2022). Training corpora span web-scale image-text pairs, curated caption datasets, and synthetic renderings (Schuhmann et al., 2021; Changpinyo et al., 2021; Nguyen et al., 2022).\n\nWe study targeted debiasing under weak supervision by coupling causal intervention prompts with attribute-balanced resampling. Our method reduces reliance on shortcut cues while preserving zero-shot accuracy.",
    "reason": "The span lists VL pretraining families and datasets but does not connect them to the stated robustness concerns or clarify the debiasing gap addressed by the paper, thus lacking synthesis (criterion a and b).",
    "start": 291,
    "end": 694,
    "label": "Lacks synthesis"
  },
  {
    "span": "Low-resource ASR has leveraged multilingual pretraining, cross-lingual transfer, and self-supervised speech models such as wav2vec 2.0 and HuBERT (Conneau et al., 2021; Baevski et al., 2020; Hsu et al., 2021). Data augmentation using speed perturbation, noise injection, and SpecAugment further improves robustness (Ko et al., 2015; Park et al., 2019).",
    "document": "Introduction\n\nBuilding accurate ASR systems for low-resource languages remains difficult due to limited transcribed audio and domain variability. Recent approaches focus on transferring knowledge from resource-rich settings and reducing supervision requirements.\n\nLow-resource ASR has leveraged multilingual pretraining, cross-lingual transfer, and self-supervised speech models such as wav2vec 2.0 and HuBERT (Conneau et al., 2021; Baevski et al., 2020; Hsu et al., 2021). Data augmentation using speed perturbation, noise injection, and SpecAugment further improves robustness (Ko et al., 2015; Park et al., 2019).\n\nSemi-supervised and pseudo-labeling pipelines exploit unlabeled audio to scale training without costly annotation (Kahn et al., 2020; Xu et al., 2021).",
    "reason": "The paragraph summarizes techniques and citations without articulating the gap the paper targets or the authors’ motivation, failing to synthesize prior work with the current study (criteria a and c).",
    "start": 264,
    "end": 616,
    "label": "Lacks synthesis"
  },
  {
    "span": "Several recent works have demonstrated that curriculum learning dramatically improves multilingual ASR.",
    "document": "Related Work\n\nMultilingual automatic speech recognition (ASR) aims to share parameters across languages to reduce labeling costs and improve low-resource performance. Early studies leveraged bottleneck features and universal phoneme sets, while more recent systems rely on end-to-end transformers and conformers with subword units. Data augmentation (e.g., speed perturbation) and self-supervised pretraining on unlabeled audio have further improved robustness.\n\nSeveral recent works have demonstrated that curriculum learning dramatically improves multilingual ASR. However, the exact form of the curriculum—by language family, signal-to-noise ratio, or utterance length—remains an open question. Our study isolates these factors and analyzes transfer dynamics among typologically diverse languages.\n\nIn contrast to monolingual curricula, our approach schedules utterances jointly across languages using a unified difficulty proxy derived from CTC alignment stability. We report gains on low- and mid-resource languages and ablations on scheduling temperature and update frequency.",
    "reason": "Asserts the existence of multiple recent works and a qualitative effect without citing any of those studies.",
    "start": 463,
    "end": 566,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors showed that curriculum learning improves data-to-text generation",
    "document": "Related Work\n\nData-to-text generation converts structured inputs into coherent natural language. Neural models with copy and content planning mechanisms have been proposed to improve fidelity and fluency (Lebret et al., 2016; Wiseman et al., 2017; Puduppully et al., 2019). Pretraining on large text corpora followed by task-specific fine-tuning further boosts performance (Raffel et al., 2020; Kale and Rastogi, 2020). In a previous study, the authors showed that curriculum learning improves data-to-text generation, which motivates our staged training schedule. We also consider controllable decoding to balance correctness and diversity (Dathathri et al., 2020).\n",
    "reason": "Mentions a specific prior study and its finding about curriculum learning but fails to cite the study (example ii).",
    "start": 420,
    "end": 517,
    "label": "Unsupported claim"
  },
  {
    "span": "The 2021 Low-Resource ASR Challenge standardized the evaluation protocol for streaming recognition.",
    "document": "Related Work\n\nEnd-to-end ASR in low-resource settings benefits from multilingual pretraining, data augmentation, and self-training (Watanabe et al., 2018; Kahn et al., 2020; Chen et al., 2021). Streaming recognition further imposes latency constraints that motivate chunk-wise attention and transducer-based models (Graves, 2012; He et al., 2019).\n\nThe 2021 Low-Resource ASR Challenge standardized the evaluation protocol for streaming recognition. Subsequent studies have compared non-streaming and streaming trade-offs using shared corpora and metrics to study accuracy-latency Pareto fronts (Rosenberg and Prabhavalkar, 2021; Shi et al., 2022). We extend this line by introducing an alignment-regularized transducer that improves early emission without sacrificing word error rate.",
    "reason": "The span references a specific shared task/competition and its claimed standardization impact without providing a citation, violating rule (a).",
    "start": 349,
    "end": 448,
    "label": "Unsupported claim"
  },
  {
    "span": "We use the widely adopted FB15k-237 split to avoid test leakage.",
    "document": "Related Work\n\nKnowledge graph completion evaluates the ability to predict missing links given observed triples. Early datasets suffered from inverse relation leakage, prompting the community to adopt revised splits to ensure a cleaner evaluation. We use the widely adopted FB15k-237 split to avoid test leakage. Our experiments compare translational distance models, bilinear factorization, and message-passing GNNs under a standardized negative sampling protocol.\n\nWe also report results on complementary benchmarks to assess robustness across relation types and graph densities.",
    "reason": "Introduces a specific dataset/split without providing the required citation on first mention (criterion a).",
    "start": 247,
    "end": 311,
    "label": "Unsupported claim"
  },
  {
    "span": "Miller et al. 3",
    "document": "Related Work\n\nInteractive visualization systems have embraced progressive analytics to keep users in the loop during long computations (Fekete and Primet, 2016; Moritz et al., 2017). Miller et al. 3 report that early, approximate previews guide user attention and reduce abandonment, aligning with findings on human-in-the-loop steering (Amershi et al., 2014). Subsequent studies formalize utility-aware sampling and confidence cues to convey uncertainty (Kale et al., 2019; Correll, 2019). We build on these insights with an operator-agnostic scheduler that optimizes perceived latency and informativeness under strict resource budgets.\n",
    "reason": "Wrong use of footnotes/numbering in an author–year style; should include the year (e.g., 'Miller et al. (YEAR)') or be reformatted as a proper footnote.",
    "start": 183,
    "end": 198,
    "label": "Format"
  },
  {
    "span": "According to industry reports, 60% of IoT devices use MQTT.",
    "document": "Introduction\n\nResource-constrained IoT deployments depend on lightweight publish-subscribe protocols to exchange telemetry with cloud services. According to industry reports, 60% of IoT devices use MQTT. Despite its ubiquity, default configurations often neglect authentication and encryption, leaving brokers vulnerable to abuse.",
    "reason": "Presents a specific statistic attributed to reports without citing any source, which requires evidence.",
    "start": 144,
    "end": 203,
    "label": "Unsupported claim"
  },
  {
    "span": "As is well known in sociolinguistics, code-switching on social media predominantly follows the Matrix Language Frame model.",
    "document": "Introduction\n\nCode-switching poses challenges for tokenization, tagging, and dependency parsing due to rapid language alternation and nonstandard orthography (Solorio and Liu, 2008; Molina et al., 2016). Studies have investigated syntactic constraints and community norms that shape switch points in bilingual discourse (Poplack, 1980; Myers-Scotton, 1993). As is well known in sociolinguistics, code-switching on social media predominantly follows the Matrix Language Frame model. We contribute a new dataset with fine-grained switch tags and propose a tagging architecture that conditions on predicted matrix language.",
    "reason": "Claims a field-wide, niche theoretical assertion without citing supporting sociolinguistic studies (definition b).",
    "start": 358,
    "end": 481,
    "label": "Unsupported claim"
  },
  {
    "span": "In the original SQuAD v3.0 setup, unanswerable questions were balanced by adversarial paraphrases.",
    "document": "Introduction\n\nOpen-domain question answering (QA) has evolved from extractive reading comprehension to systems that combine retrieval and generation. Benchmark datasets have catalyzed progress by providing standardized evaluation and challenging phenomena such as unanswerability and multi-hop reasoning.\n\nA central challenge is distinguishing answerable from unanswerable queries when passages contain plausible but incorrect distractors. In the original SQuAD v3.0 setup, unanswerable questions were balanced by adversarial paraphrases. This setting requires models to rely on fine-grained discourse cues rather than shallow lexical overlap.\n\nWe revisit unanswerability by proposing a denoising framework that masks answer spans and paraphrastic distractors during pretraining. Our contributions include a new contrastive objective for calibrating abstention and a study of calibration metrics beyond exact match and F1.",
    "reason": "Claims a specific dataset version and design detail about how unanswerable questions were created without citing the dataset or a paper describing that setup.",
    "start": 440,
    "end": 538,
    "label": "Unsupported claim"
  },
  {
    "span": "Sutton and Barto 2",
    "document": "Introduction\n\nReinforcement learning (RL) provides a general framework for sequential decision making under uncertainty (Puterman, 1994; Kaelbling et al., 1996). Policy gradient and actor–critic methods scale RL to high-dimensional control (Sutton et al., 2000; Mnih et al., 2016). We follow Sutton and Barto 2 for foundational terminology and notational conventions, and adopt the standard definition of return and value functions. In robotics, sample efficiency and safety constraints remain central challenges (Kober et al., 2013; Levine et al., 2016).",
    "reason": "Wrong use of footnote/numbering in place of a year: \"Sutton and Barto 2\" incorrectly appends a superscript-like number instead of providing the year or a proper footnote. It should read \"Sutton and Barto (2018)\" or be formatted as an actual footnote.",
    "start": 292,
    "end": 310,
    "label": "Format"
  },
  {
    "span": "The Natural Questions dataset has become the de facto standard for open-domain QA evaluation.",
    "document": "Related Work\n\nOpen-domain question answering (QA) systems retrieve evidence from large corpora and synthesize answers to factoid and descriptive queries. Benchmarks have driven progress by standardizing evaluation protocols and enabling apples-to-apples comparisons across retrieval and reader models.\n\nThe Natural Questions dataset has become the de facto standard for open-domain QA evaluation. Nevertheless, differences in retrieval depth, passage selection, and answer normalization complicate direct comparison of reported scores.\n\nOur study introduces a unified evaluation harness with transparent retrieval and normalization settings, enabling reproducible comparisons across readers and retrievers.",
    "reason": "Asserts benchmark dominance for a named dataset without citing the dataset paper or surveys supporting the 'de facto standard' claim.",
    "start": 303,
    "end": 396,
    "label": "Unsupported claim"
  },
  {
    "span": "[Hernandez et al., 2016]",
    "document": "Related Work\n\nOptimization strategies for deep reinforcement learning have explored adaptive step sizes and target networks to stabilize training (Mnih et al., 2015; Lillicrap et al., 2016). Trust-region and proximal methods constrain policy updates to prevent collapse (Schulman et al., 2015; Schulman et al., 2017). Off-policy corrections via importance sampling enable better sample reuse (Espeholt et al., 2018). While entropy regularization improves exploration (Haarnoja et al., 2018), tuning remains sensitive to reward scaling. Prior work reported that gradient clipping reduces variance in advantage estimates [Hernandez et al., 2016], but results vary across tasks (Keller et al., 2019). We propose schedule-aware clipping with dual averaging to adaptively trade off bias and variance during training.",
    "reason": "Wrong bracket style for in-text citation in APA-like context; should use parentheses (Hernandez et al., 2016) instead of square brackets.",
    "start": 619,
    "end": 643,
    "label": "Format"
  },
  {
    "span": "Fast gradient methods craft perturbations via loss linearization (Goodfellow et al., 2015). Patch-wise adversaries occlude salient regions to fool classifiers (Brown et al., 2017). Certifiable defenses bound worst-case loss within Lp balls (Cohen et al., 2019). Smoothing inputs with randomized noise improves stability (Lecuyer et al., 2019). Token mixing in Vision Transformers changes attack surfaces (Dosovitskiy et al., 2021).",
    "document": "Related Work\n\nAdversarial Robustness for Vision Transformers\n\nThe study of adversarial robustness has evolved from gradient-based attacks and empirical defenses to certified guarantees. While most early work targeted convolutional networks, recent advances in Vision Transformers (ViTs) introduce different inductive biases and failure modes (Dosovitskiy et al., 2021; Bhojanapalli et al., 2021). Understanding how tokenization and self-attention interact with adversarial perturbations is crucial for robust deployment.\n\nFast gradient methods craft perturbations via loss linearization (Goodfellow et al., 2015). Patch-wise adversaries occlude salient regions to fool classifiers (Brown et al., 2017). Certifiable defenses bound worst-case loss within Lp balls (Cohen et al., 2019). Smoothing inputs with randomized noise improves stability (Lecuyer et al., 2019). Token mixing in Vision Transformers changes attack surfaces (Dosovitskiy et al., 2021).\n\nWe propose a token-level adversarial training scheme with patch masking and Lipschitz regularization that better aligns with ViT architecture, providing improved robustness under both pixel and patch attacks.",
    "reason": "The span lists attacks and defenses without transitions or explicit relations, then jumps to ViT token mixing without connecting it to prior methods, leading to poor coherence.",
    "start": 522,
    "end": 953,
    "label": "Coherence"
  },
  {
    "span": "Earlier competitions on adversarial QA focused on short passages.",
    "document": "Introduction\n\nAdversarial question answering benchmarks stress-test models by exposing weaknesses in robustness and calibration (Jia and Liang, 2017; Wallace et al., 2019). Recent datasets introduce perturbations and counterfactuals to assess generalization beyond lexical overlap (Kaushik et al., 2020). Earlier competitions on adversarial QA focused on short passages. We instead design a long-context adversarial benchmark that requires multi-hop retrieval across full documents.",
    "reason": "This mentions prior competitions but provides no citations to them, violating rule (a) concerning first mentions of shared tasks or competitions.",
    "start": 305,
    "end": 370,
    "label": "Unsupported claim"
  },
  {
    "span": "Prompting methods for program synthesis include few-shot prompting with exemplars, chain-of-thought rationales, self-consistency sampling, and tool-augmented prompting such as REPL execution and search (Brown et al., 2020; Wei et al., 2022; Wang et al., 2022; Schick et al., 2023).",
    "document": "Related Work\n\nLarge language models have shown promise in generating code from natural language. Despite improved capabilities, generation reliability remains a challenge in the presence of long-range dependencies and underspecified instructions.\n\nPrompting methods for program synthesis include few-shot prompting with exemplars, chain-of-thought rationales, self-consistency sampling, and tool-augmented prompting such as REPL execution and search (Brown et al., 2020; Wei et al., 2022; Wang et al., 2022; Schick et al., 2023).\n\nWe introduce Constraint-Guided Decoding, which integrates symbolic specifications into the decoding process using differentiable constraint hints and iterative repair.",
    "reason": "The span merely enumerates prompting techniques without connecting them to the proposed constraint-guided decoding or identifying the shortcomings that motivate it (criterion a/c).",
    "start": 248,
    "end": 529,
    "label": "Lacks synthesis"
  },
  {
    "span": "There has been a surge of interest in multimodal code understanding in the past two years.",
    "document": "Related Work\n\nAutomatic code summarization aims to generate natural-language descriptions of source code, often leveraging structural cues from abstract syntax trees and data flow (Allamanis et al., 2018; Ahmad et al., 2020). Pre-trained models trained on large code corpora and paired docstrings advance the state of the art (Feng et al., 2020; Wang et al., 2021). There has been a surge of interest in multimodal code understanding in the past two years. This trend integrates execution traces, UI screenshots, and repository metadata to improve comprehension beyond code-only signals.",
    "reason": "Claims a trend of 'surge of interest' and 'recent' work without citing representative papers (rule d).",
    "start": 366,
    "end": 456,
    "label": "Unsupported claim"
  },
  {
    "span": "The widely-used CORA-Graph benchmark has 2,708 nodes and 7 classes.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become the de facto approach for semi-supervised node classification on citation and social graphs. Architectural variations emphasize neighborhood aggregation depth, positional encodings, and decoupled propagation.\n\nThe widely-used CORA-Graph benchmark has 2,708 nodes and 7 classes. Although widely adopted, the dataset’s homophily and small size can lead to over-optimistic generalization and limit conclusions about real-world graphs.\n\nBeyond canonical citation graphs, larger and more diverse benchmarks have been proposed to assess robustness to heterophily and distribution shift.",
    "reason": "States specific dataset statistics and characteristics without citing the dataset source (violates rule b).",
    "start": 265,
    "end": 332,
    "label": "Unsupported claim"
  },
  {
    "span": "Domain adaptation for automatic speech recognition includes feature-space normalization, adversarial training, and self-training on unlabeled target speech (Saon et al., 2013; Shinohara, 2016; Karita et al., 2021). Accent and dialect robustness is improved via data augmentation, pronunciation modeling, and meta-learning (Jain et al., 2018; Jotti et al., 2020; Klejch et al., 2022). Self-supervised pretraining like wav2vec 2.0 provides transferable representations (Baevski et al., 2020).",
    "document": "Related Work\n\nVariation in speakers, channels, and domains poses persistent challenges to ASR systems, especially in low-resource and accented settings (Gales and Young, 2007; Stoian et al., 2020). Recent methods leverage unlabeled data and robust optimization to close the gap.\n\nDomain adaptation for automatic speech recognition includes feature-space normalization, adversarial training, and self-training on unlabeled target speech (Saon et al., 2013; Shinohara, 2016; Karita et al., 2021). Accent and dialect robustness is improved via data augmentation, pronunciation modeling, and meta-learning (Jain et al., 2018; Jotti et al., 2020; Klejch et al., 2022). Self-supervised pretraining like wav2vec 2.0 provides transferable representations (Baevski et al., 2020).\n\nEvaluation increasingly considers word error rate across multiple accents and domains, as well as calibration of confidence scores for downstream use (Park et al., 2019; Karandikar et al., 2021).",
    "reason": "The span lists techniques and findings without describing how they motivate or contrast with the present study, fitting definition a and c.",
    "start": 280,
    "end": 770,
    "label": "Lacks synthesis"
  },
  {
    "span": "In (Miller, 2020)",
    "document": "Related Work\n\nNeural dialogue systems have evolved from encoder–decoder architectures to retrieval-augmented models that ground responses in external knowledge (Dinan et al., 2019; Lewis et al., 2020). In (Miller, 2020), retrieval-augmented generation was extended with differentiable memory to improve factual consistency. Concurrently, prompt-based approaches adapt pretrained LMs for few-shot conversational tasks (Schick and Schütze, 2021; Gao et al., 2021), but their sensitivity to template phrasing remains a challenge.",
    "reason": "Wrong citation style: the preposition 'In' should not precede a parenthetical citation; it should be a narrative citation like 'In Miller (2020)'.",
    "start": 202,
    "end": 219,
    "label": "Format"
  },
  {
    "span": "Over 70% of industrial deployments rely on rule-based baselines",
    "document": "Introduction\n\nNatural language understanding (NLU) technologies are increasingly adopted in enterprise applications, from customer support chatbots to document understanding workflows. Despite progress, practical deployments face constraints such as data privacy, latency, and model interpretability (Rastogi et al., 2020; Ackerman et al., 2021). Over 70% of industrial deployments rely on rule-based baselines, highlighting the gap between research prototypes and production requirements. This paper studies hybrid neuro-symbolic approaches that preserve interpretability while improving accuracy over keyword and pattern rules.\n",
    "reason": "Presents a precise statistic about industry adoption without citing any survey or evidence backing the number (definition: statistics without citation).",
    "start": 347,
    "end": 410,
    "label": "Unsupported claim"
  },
  {
    "span": "In the eighth Dialog System Technology Challenge, systems were pre-screened with BLEU before human judging.",
    "document": "Introduction\n\nEvaluating open-domain dialogue systems remains challenging due to the weak correlation between reference-based automatic metrics and human judgments (Liu et al., 2016; Derby et al., 2020). Shared tasks and competitions have helped consolidate evaluation protocols but also surfaced methodological pitfalls, particularly around metric-driven triaging of submissions.\n\nIn the eighth Dialog System Technology Challenge, systems were pre-screened with BLEU before human judging. While such filtering reduces annotation cost, it risks discarding systems that could perform well in human evaluations. Our study proposes a cost-aware human-in-the-loop selection scheme that avoids overreliance on brittle automatic metrics.",
    "reason": "Mentions a specific shared task procedure without citing the corresponding DSTC report or paper; first mentions of shared tasks should be cited.",
    "start": 382,
    "end": 489,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT Devlin et al., 2019",
    "document": "Related Work\n\nPretrained encoders have transformed many NLP tasks. BERT Devlin et al., 2019 established strong baselines for token-level and sequence-level classification, while RoBERTa refined training dynamics (Liu et al., 2019). Span-based pretraining improved question answering (Clark and Gardner, 2018), and long-document transformers extended context windows (Beltagy et al., 2020).\n\nWe build upon these models by incorporating task-adaptive pretraining objectives (Gururangan et al., 2020) and efficient fine-tuning strategies (Houlsby et al., 2019).",
    "reason": "Citation not properly delimited; when referring to the model, it should be \"BERT (Devlin et al., 2019)\" or a narrative form like \"Devlin et al. (2019) introduced BERT.\"",
    "start": 67,
    "end": 91,
    "label": "Format"
  },
  {
    "span": "Most existing work agrees that pointer-generators struggle with factual consistency.",
    "document": "Introduction\n\nAbstractive summarization systems must balance fluency with factual faithfulness. Early neural models suffered from repetition and content drift, motivating architectures that explicitly copy source spans while allowing novel phrasing. Most existing work agrees that pointer-generators struggle with factual consistency. Yet, the mechanisms underlying factual errors—such as entity swaps and temporal hallucinations—are still poorly characterized across domains.\n\nRelated Work\n\nSubsequent research has examined constrained decoding, coverage penalties, and post-hoc factuality reranking. Retrieval-augmented approaches attempt to ground generation in evidence, while alignment-based metrics target entity-level fidelity. Despite these innovations, general-purpose faithfulness metrics remain imperfect proxies for human judgments.\n\nContributions\n\nWe provide a taxonomy of factual error types and propose a lightweight calibration head that conditions generation on extracted entailment signals. We evaluate on news, biomedical, and legal corpora to assess cross-domain robustness.",
    "reason": "Asserts a consensus in prior work without citing any sources, breaching rule (d) and example (i).",
    "start": 250,
    "end": 334,
    "label": "Unsupported claim"
  },
  {
    "span": "Self-supervised pretraining on raw audio learns robust representations (Schneider et al., 2019). Contrastive losses separate positive and negative pairs (van den Oord et al., 2018). Semi-supervised CTC leverages untranscribed speech (Kahn et al., 2020). End-to-end transducers integrate acoustic and language models (Graves, 2012).",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) has benefited from large-scale self-supervised pretraining and efficient decoding architectures. While recent advances reduce label reliance, performance under domain shift and low-resource conditions remains challenging.\n\nSelf-supervised pretraining on raw audio learns robust representations (Schneider et al., 2019). Contrastive losses separate positive and negative pairs (van den Oord et al., 2018). Semi-supervised CTC leverages untranscribed speech (Kahn et al., 2020). End-to-end transducers integrate acoustic and language models (Graves, 2012).\n\nWe propose uncertainty-aware pseudo-label selection for semi-supervised ASR and combine it with transducer training, achieving improvements on noisy and accented speech without additional annotations.",
    "reason": "The span presents four separate statements about pretraining, contrastive learning, semi-supervised CTC, and transducers without connecting them. The lack of transitions and explicit relations makes the sequence abrupt and incoherent.",
    "start": 283,
    "end": 614,
    "label": "Coherence"
  },
  {
    "span": "The widely used UrbanSound8K dataset contains numerous mislabeled clips.",
    "document": "Related Work\n\nEnvironmental sound classification (ESC) has advanced through the use of convolutional and transformer-based architectures trained on mel-spectrogram inputs (Piczak, 2015; Salamon and Bello, 2017; Kong et al., 2020). Benchmark datasets such as ESC-50 and UrbanSound8K have enabled standardized comparisons across methods (Piczak, 2015; Salamon et al., 2014). The widely used UrbanSound8K dataset contains numerous mislabeled clips. Such label noise can confound reported improvements and limit the reproducibility of results across implementations.\n\nRecent works propose noise-robust training objectives and semi-supervised learners that leverage larger unlabeled corpora (Kumar and Raj, 2021; Fonseca et al., 2022). In contrast, our approach focuses on dataset-centric mitigation, introducing cross-split relabeling and quality-controlled augmentation to improve label fidelity without changing model capacity.",
    "reason": "This statement asserts a quality issue with a specific dataset without citing evidence or a source documenting the mislabels.",
    "start": 373,
    "end": 445,
    "label": "Unsupported claim"
  },
  {
    "span": "TransE variants dominate link prediction benchmarks across standard knowledge graphs.",
    "document": "Related Work\n\nKnowledge graph embedding (KGE) methods learn vector representations of entities and relations to enable link prediction and completion (Nickel et al., 2016). Translational distance models like TransE and its extensions capture relational patterns via simple geometric operations (Bordes et al., 2013). Bilinear and tensor factorization approaches such as DistMult, ComplEx, and TuckER improve modeling of symmetric and antisymmetric relations (Yang et al., 2015; Trouillon et al., 2016; Balazevic et al., 2019).\n\nRecent advances incorporate logical constraints and rule-based reasoning to improve generalization and interpretability (Rocktaschel et al., 2015; Ren and Leskovec, 2020). Despite these developments, optimization stability and proper negative sampling remain crucial for performance (Sun et al., 2019). TransE variants dominate link prediction benchmarks across standard knowledge graphs. In contrast, we explore margin-free training with adaptive self-adversarial sampling to improve convergence for both translational and bilinear families.\n\nWe evaluate on FB15k-237, WN18RR, and YAGO subsets, reporting filtered metrics and calibration analyses.",
    "reason": "The sentence asserts a comparative claim about prior methods' dominance on benchmarks without citing any studies or leaderboards; such claims about prior results require citations.",
    "start": 831,
    "end": 916,
    "label": "Unsupported claim"
  },
  {
    "span": "(Patel et al. 2016)",
    "document": "Related Work\n\nGraph neural networks (GNNs) have advanced node classification and link prediction by integrating message passing with learned aggregation (Kipf and Welling, 2017; Hamilton et al., 2017). Extensions to dynamic graphs address temporal dependencies and evolving topology (Trivedi et al., 2019; Rossi et al., 2020). Prior work noted oversmoothing in deep architectures and proposed residual or normalization remedies (Oono and Suzuki, 2019; Rong et al., 2020). Our approach builds on spectral regularization and skip connections (Patel et al. 2016) to stabilize deeper GNNs without sacrificing expressivity (Chen et al., 2020b).",
    "reason": "Missing comma between author and year in a parenthetical citation; should be “(Patel et al., 2016)”.",
    "start": 540,
    "end": 559,
    "label": "Format"
  },
  {
    "span": "Previous studies unanimously report that temporal attention outperforms gated recurrent units in all urban networks.",
    "document": "Related Work\n\nTraffic Forecasting with Graph Neural Networks. Spatial-temporal forecasting has been approached with diffusion convolutions (Li et al., 2018), attention mechanisms (Zheng et al., 2020), and spatio-temporal transformers (Cai et al., 2020). Prior work often compares recurrent units and attention for temporal modeling. Previous studies unanimously report that temporal attention outperforms gated recurrent units in all urban networks. However, these methods differ in graph construction and training regimes, making results hard to compare.",
    "reason": "Makes a sweeping claim about prior studies' results without citations or evidence (violates rule b and d).",
    "start": 333,
    "end": 449,
    "label": "Unsupported claim"
  },
  {
    "span": "According to industry reports, end-to-end transducers now account for over 70% of production ASR deployments.",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) simplifies the conventional hybrid pipeline by directly mapping acoustics to word sequences. CTC, attention-based encoder–decoders, and transducer architectures are prominent paradigms (Graves et al., 2013; Chan et al., 2016; Kim et al., 2017). According to industry reports, end-to-end transducers now account for over 70% of production ASR deployments. Despite these advances, robustness to noise and domain shift remains a critical challenge (Lippmann, 1997; Ko et al., 2015).",
    "reason": "Presents a quantitative statistic attributed to 'industry reports' without citing any source (rule b).",
    "start": 305,
    "end": 414,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works explore few-shot causal discovery in multivariate time series.",
    "document": "Related Work\n\nCausal discovery from time series seeks to infer directed relationships under temporal dependencies and potential confounders. Classical methods rely on Granger causality and constraint-based approaches, while modern neural methods combine representation learning with invariance principles. Recent works explore few-shot causal discovery in multivariate time series. Despite promising results, the literature lacks systematic evaluations across varying sequence lengths, sampling rates, and intervention regimes.\n\nWe address these gaps by introducing a synthetic-to-real transfer protocol and a suite of diagnostics for stability under limited supervision.",
    "reason": "The phrase 'Recent works explore...' makes a claim about prior literature but provides no citations to specific studies, violating the requirement to cite recent works.",
    "start": 306,
    "end": 381,
    "label": "Unsupported claim"
  },
  {
    "span": "Self-supervised representation learning for vision includes contrastive methods that discriminate instances (MoCo, SimCLR) (He et al., 2020; Chen et al., 2020), non-contrastive methods like BYOL and SimSiam that avoid negative pairs (Grill et al., 2020; Chen and He, 2021), and vision transformer pretraining with DINO, iBOT, and masked autoencoders (Caron et al., 2021; Zhou et al., 2021; He et al., 2022). Multi-view augmentations and invariance to color, cropping, and blur are standard design choices (Tian et al., 2020; Jing and Tian, 2021).",
    "document": "Related Work\n\nUnlabeled visual data can be exploited via pretext tasks that induce semantic structure in learned features without requiring manual annotations.\n\nSelf-supervised representation learning for vision includes contrastive methods that discriminate instances (MoCo, SimCLR) (He et al., 2020; Chen et al., 2020), non-contrastive methods like BYOL and SimSiam that avoid negative pairs (Grill et al., 2020; Chen and He, 2021), and vision transformer pretraining with DINO, iBOT, and masked autoencoders (Caron et al., 2021; Zhou et al., 2021; He et al., 2022). Multi-view augmentations and invariance to color, cropping, and blur are standard design choices (Tian et al., 2020; Jing and Tian, 2021).\n\nWe introduce a minimal predictive coding loss that aligns invariances with downstream robustness to distribution shifts.",
    "reason": "The span compiles methods and design choices but does not relate them to the proposed loss or explain the specific shortcoming the paper aims to resolve (definition a and c).",
    "start": 161,
    "end": 707,
    "label": "Lacks synthesis"
  },
  {
    "span": "(Lopez and Kim, 2022)",
    "document": "Related Work\n\nEvaluation metrics for text generation trade off fluency, adequacy, and faithfulness. N-gram overlap measures correlate weakly with human judgments on abstractive tasks (Papineni et al., 2002; Lin, 2004). Learned metrics trained on human preferences improve correlation but can be domain-sensitive (Zhou and Small, 2021).\n\nCalibrated reference-free metrics aim to reduce reference bias by modeling semantic similarity and factual consistency jointly (Chen and Zhang, 2022). Recent studies also propose task-specific probes for contradiction and entity drift (Park et al., 2023). We adopt a multi-facet evaluation following (Lopez and Kim, 2022) with additional controls for prompt sensitivity.\n\nOur analysis reveals that metric disagreements are often driven by shallow lexical cues, motivating a composite scoring scheme.",
    "reason": "Incorrect connector inside a parenthetical citation. In APA style, within parentheses with two authors, use an ampersand: (Lopez & Kim, 2022), not (Lopez and Kim, 2022).",
    "start": 637,
    "end": 658,
    "label": "Format"
  },
  {
    "span": "Privacy in federated learning is commonly addressed with secure aggregation, differential privacy, and homomorphic encryption (Bonawitz et al., 2017; McMahan et al., 2018; Gentry, 2009). Client- and server-side perturbation mechanisms trade off utility and privacy under various threat models (Abadi et al., 2016; Kairouz et al., 2021).",
    "document": "Introduction\n\nFederated learning enables collaborative model training without centralizing raw data. However, updates can still leak sensitive information about participants. Robust privacy accounting and efficient protocols are therefore essential for practical deployments.\n\nPrivacy in federated learning is commonly addressed with secure aggregation, differential privacy, and homomorphic encryption (Bonawitz et al., 2017; McMahan et al., 2018; Gentry, 2009). Client- and server-side perturbation mechanisms trade off utility and privacy under various threat models (Abadi et al., 2016; Kairouz et al., 2021).\n\nWe present AdaBudget-DP, a communication- and privacy-adaptive training scheme that dynamically allocates privacy budgets across rounds using loss curvature signals. Experiments on image and text benchmarks show improved accuracy under the same total privacy loss compared to fixed-allocation baselines.",
    "reason": "The span only catalogs existing privacy mechanisms and trade-offs without articulating how they relate to the proposed approach or what specific shortcoming remains (criteria a and c).",
    "start": 277,
    "end": 613,
    "label": "Lacks synthesis"
  },
  {
    "span": "Gilmer et al. (2017) introduced message passing neural networks for molecular property prediction. Kearnes et al. (2016) built graph convolutional models for molecules. Yang et al. (2019) curated MoleculeNet benchmarks for evaluation. Duvenaud et al. (2015) used differentiable fingerprints.",
    "document": "Related Work\n\nMolecular property prediction has benefited from graph-based deep learning, where atoms and bonds are modeled as nodes and edges. Recent advances emphasize scalable architectures and data curation practices.\n\nGilmer et al. (2017) introduced message passing neural networks for molecular property prediction. Kearnes et al. (2016) built graph convolutional models for molecules. Yang et al. (2019) curated MoleculeNet benchmarks for evaluation. Duvenaud et al. (2015) used differentiable fingerprints.\n\nWhile our approach also operates on molecular graphs, we focus on uncertainty calibration across distribution shifts.",
    "reason": "The span lists four papers in separate sentences without transitions or explicit relationships among them. It is unclear how each cited work connects to the others or to the preceding context, creating abrupt, unlinked statements across multiple sentences.",
    "start": 223,
    "end": 514,
    "label": "Coherence"
  },
  {
    "span": "Most prior work treats heterophily as noise that should be smoothed away.",
    "document": "Introduction\n\nGraph neural networks (GNNs) commonly assume homophily: connected nodes tend to share labels or attributes, enabling neighborhood aggregation to reinforce useful signals. However, many real-world networks exhibit heterophily, where edges more often connect dissimilar nodes, challenging standard message-passing designs.\n\nMost prior work treats heterophily as noise that should be smoothed away. This perspective has motivated stronger low-pass filters and residual connections that preserve local features while attenuating high-frequency components.\n\nA contrasting line of inquiry advocates explicit modeling of relation types, signed edges, or role-based structures to leverage heterophily rather than suppress it. In this paper, we unify these intuitions under a spectral view and propose an architecture that adaptively mixes complementary frequency bands based on learned topology-aware gates.",
    "reason": "Makes a broad claim about the stance of prior literature without citing any representative studies, which requires citations in related work.",
    "start": 336,
    "end": 409,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors claim that contrastive pretraining eliminates the need for labels",
    "document": "Introduction\n\nSelf-supervised learning (SSL) has reshaped representation learning by replacing costly manual annotations with pretext tasks that exploit raw data structure. Contrastive approaches, in particular, have shown strong transfer to downstream tasks, often rivaling fully supervised baselines. In a previous study, the authors claim that contrastive pretraining eliminates the need for labels, suggesting that with sufficient scale, SSL can obviate manual supervision entirely.\n\nWhile SSL reduces labeling demands, its benefits can vary widely across domains, data regimes, and evaluation metrics. For example, fine-grained classification and structured prediction tasks may still benefit from modest amounts of labeled data to disambiguate subtle categories. Moreover, the stability of SSL under distribution shift and limited compute budgets remains an open question.\n\nThis paper systematically evaluates the extent to which contrastive pretraining reduces supervised data requirements across vision and speech tasks. We introduce a unified protocol for subsampling labels, measure calibration under shift, and assess robustness to corrupted views.\n",
    "reason": "Refers to a specific prior study and claims its finding without citing it, violating rule a.",
    "start": 303,
    "end": 401,
    "label": "Unsupported claim"
  },
  {
    "span": "Back-translation generates synthetic parallel data from target-side monolingual text (Sennrich et al., 2016). Contextual word replacement augments sentences using masked language models (Kobayashi, 2018). Prompt-based generation uses instructions to synthesize labeled examples (Schick and Schütze, 2021).",
    "document": "Related Work\n\nData augmentation has become a key strategy for improving low-resource NLP performance. Back-translation generates synthetic parallel data from target-side monolingual text (Sennrich et al., 2016). Contextual word replacement augments sentences using masked language models (Kobayashi, 2018). Prompt-based generation uses instructions to synthesize labeled examples (Schick and Schütze, 2021). Mixup and manifold interpolations have also been studied for text (Guo et al., 2020). We investigate label-preserving augmentation under strict semantic fidelity constraints.",
    "reason": "The span lists three augmentation techniques in separate sentences without transitions or clarifying how they relate to one another, making the connection between the cited works implicit and the flow abrupt.",
    "start": 102,
    "end": 407,
    "label": "Coherence"
  },
  {
    "span": "Message passing neural networks have been extended with attention mechanisms, higher-order neighborhoods, and subgraph pooling to improve molecular property prediction (Gilmer et al., 2017; Veličković et al., 2018; Ying et al., 2021). Other lines incorporate 3D geometry via distance matrices or equivariant layers (Schütt et al., 2018; Thomas et al., 2018). Benchmarks such as MoleculeNet and PCQM4M have driven architectural comparisons (Wu et al., 2018; Hu et al., 2020).",
    "document": "Related Work\n\nGraph neural networks have become a dominant paradigm for learning on molecular graphs, enabling end-to-end prediction of physical, biological, and pharmacokinetic properties (Gilmer et al., 2017; Wu et al., 2018). Advances in pooling, normalization, and training regimes have steadily improved performance across standard datasets.\n\nMessage passing neural networks have been extended with attention mechanisms, higher-order neighborhoods, and subgraph pooling to improve molecular property prediction (Gilmer et al., 2017; Veličković et al., 2018; Ying et al., 2021). Other lines incorporate 3D geometry via distance matrices or equivariant layers (Schütt et al., 2018; Thomas et al., 2018). Benchmarks such as MoleculeNet and PCQM4M have driven architectural comparisons (Wu et al., 2018; Hu et al., 2020).\n\nRecent studies also explore pretraining on large compound libraries using masked atom tasks and contrastive objectives to enhance sample efficiency (Hu et al., 2020; You et al., 2020). Interpretability methods estimate atom and bond attributions to rationalize predictions (Pope et al., 2019; Ying et al., 2019).",
    "reason": "The span lists developments and benchmarks without explaining how they inform, motivate, or contrast with the authors' work, thus lacking synthesis per definition a and c.",
    "start": 348,
    "end": 822,
    "label": "Lacks synthesis"
  },
  {
    "span": "Garcia et. al. (2015)",
    "document": "Introduction\n\nNeural machine translation (NMT) shifted the field from phrase-based systems to end-to-end modeling with attention (Bahdanau et al., 2015; Sutskever et al., 2014). Transformer architectures advanced state of the art by enabling parallel sequence processing (Vaswani et al., 2017). Domain adaptation remains a key problem for low-resource and specialized corpora (Chu and Wang, 2018; Freitag and Al-Onaizan, 2016). Garcia et. al. (2015) examine terminology consistency under domain shift, motivating our focus on lexicon-constrained decoding.",
    "reason": "Incorrect abbreviation punctuation in the citation: \"et. al.\" is wrong; it should be \"et al.\" without a period after \"et\".",
    "start": 428,
    "end": 449,
    "label": "Format"
  },
  {
    "span": "Recent studies propose numerous variants of graph neural networks for molecular property prediction, including message passing networks (Gilmer et al., 2017), graph attention mechanisms (Velickovic et al., 2018; Chen et al., 2019), spectral approaches (Defferrard et al., 2016; Kipf and Welling, 2017), and 3D geometry-aware models that incorporate distances and angles (Schütt et al., 2018; Klicpera et al., 2020). Self-supervised pretraining on large chemical corpora has also been explored to improve downstream performance (Hu et al., 2019; You et al., 2020).",
    "document": "Related Work\n\nMachine learning for molecular property prediction has transitioned from hand-crafted fingerprints to neural message passing on molecular graphs. Representations that respect permutation invariance and the symmetries of 3D conformations have been particularly influential. We study supervised prediction under low-data regimes where inductive bias and multi-scale structure matter.\n\nRecent studies propose numerous variants of graph neural networks for molecular property prediction, including message passing networks (Gilmer et al., 2017), graph attention mechanisms (Velickovic et al., 2018; Chen et al., 2019), spectral approaches (Defferrard et al., 2016; Kipf and Welling, 2017), and 3D geometry-aware models that incorporate distances and angles (Schütt et al., 2018; Klicpera et al., 2020). Self-supervised pretraining on large chemical corpora has also been explored to improve downstream performance (Hu et al., 2019; You et al., 2020).\n\nDespite rich progress, existing surveys largely catalog architectures without resolving how to integrate topology, geometry, and electronic features under tight label budgets. In contrast, our approach introduces a hierarchy of equivariant updates tied to atomic environments and uses contrastive objectives to transfer across endpoints.",
    "reason": "The span lists prior GNN variants and pretraining work without explaining how they relate to the authors' problem setting or clarifying the specific gap their method addresses, thus lacking synthesis per (a) and (b).",
    "start": 397,
    "end": 960,
    "label": "Lacks synthesis"
  },
  {
    "span": "Zhao and Kim (2021) fine-tuned dialog models for tutoring strategies. Ahmed et al. (2020) proposed knowledge-grounded responses using curriculum policies. Park and Silva (2019) explored empathy-conditioned generation. Lin et al. (2022) examined assessment alignment with rubric-guided decoding.",
    "document": "Related Work\n\nEducational chatbots combine conversational modeling with pedagogy, requiring content accuracy, supportive tone, and alignment with learning objectives. Prior work explores instructional strategies, grounding, and assessment.\n\nZhao and Kim (2021) fine-tuned dialog models for tutoring strategies. Ahmed et al. (2020) proposed knowledge-grounded responses using curriculum policies. Park and Silva (2019) explored empathy-conditioned generation. Lin et al. (2022) examined assessment alignment with rubric-guided decoding.\n\nRecent studies emphasize formative feedback and explainability to foster metacognition (Gordon et al., 2023). We extend this line by coupling rubric alignment with verifiable content grounding.",
    "reason": "The span lists four citations in succession with no transitions or explanation of how tutoring strategies, grounding, empathy, and assessment relate, making the connections between works implicit and abrupt.",
    "start": 241,
    "end": 535,
    "label": "Coherence"
  },
  {
    "span": "Early neural TTS systems demonstrated near-human naturalness in single-speaker settings.",
    "document": "Introduction\n\nText-to-speech (TTS) synthesis has progressed rapidly with neural architectures that jointly model prosody, articulation, and acoustic detail. While multi-speaker and cross-lingual synthesis remain challenging, single-speaker pipelines have seen dramatic improvements. Early neural TTS systems demonstrated near-human naturalness in single-speaker settings. However, these systems often relied on large amounts of high-quality recordings and were sensitive to mismatches between training and inference conditions.\n\nWe propose a data-efficient TTS approach that leverages self-supervised speech representations and pitch-contour augmentation, reducing reliance on extensive labeled corpora while preserving naturalness and intelligibility.",
    "reason": "Claims specific outcomes of prior work ('near-human naturalness') without any citation.",
    "start": 283,
    "end": 371,
    "label": "Unsupported claim"
  },
  {
    "span": "Medical image segmentation has been advanced by U-Net and its variants with residual blocks, attention gates, dilated convolutions, and more recently Transformer-based encoders and self-supervised pretraining (Ronneberger et al., 2015; Oktay et al., 2018; Chen et al., 2021; Zhou et al., 2022).",
    "document": "Related Work\n\nAccurate delineation of anatomical structures is essential for computer-assisted diagnosis and treatment planning. Limited labels, domain shift across scanners, and class imbalance complicate robust segmentation.\n\nMedical image segmentation has been advanced by U-Net and its variants with residual blocks, attention gates, dilated convolutions, and more recently Transformer-based encoders and self-supervised pretraining (Ronneberger et al., 2015; Oktay et al., 2018; Chen et al., 2021; Zhou et al., 2022).\n\nOur approach, MixTeacher, uses semi-supervised consistency with uncertainty-aware mixing to leverage unlabeled volumes and improve generalization under cross-site shifts.",
    "reason": "The span enumerates prior architectures without relating them to the semi-supervised, cross-site generalization focus of the paper or identifying the specific gap (criterion a/c).",
    "start": 228,
    "end": 522,
    "label": "Lacks synthesis"
  },
  {
    "span": "Garcia et al. 4",
    "document": "Introduction\n\nMeasuring online toxicity requires robust annotation schemes and context-aware models that capture pragmatic cues (Lopez et al., 2018). Early datasets focused on keyword filters, which exhibit poor recall on implicit abuse (Singh and Rao, 2019). Contextual encoders improved detection but still struggle with domain shift across platforms (Chen and Patel, 2021).\n\nGarcia et al. 4 introduced a multilingual corpus with fine-grained offense categories and annotator rationale spans. Subsequent studies leveraged this resource to train multitask models that jointly predict labels and rationales (Huang and Kim, 2020). Our work extends rationale supervision with counterfactual augmentations that debias label shortcuts.\n\nWe also compare cross-lingual transfer approaches based on adapters (Nguyen et al., 2021) and show that lightweight conditioning improves zero-shot performance.",
    "reason": "Wrong use of footnote-style superscript without proper formatting or year; should include the year as an author–year citation (e.g., 'Garcia et al. (2020)') or be reformatted as a proper footnote.",
    "start": 378,
    "end": 393,
    "label": "Format"
  },
  {
    "span": "Most recent works fine-tune CodeBERT with copy attention for code summarization.",
    "document": "Related Work\n\nNeural code summarization leverages sequence-to-sequence models and pretrained code-language encoders to generate natural language descriptions of source code (Allamanis et al., 2018; Ahmad et al., 2020; Feng et al., 2020). Structural encoders that exploit ASTs and data-flow graphs further improve faithfulness by injecting program semantics (LeClair et al., 2019; Hellendoorn et al., 2020).\n\nMost recent works fine-tune CodeBERT with copy attention for code summarization. However, copying can amplify exposure bias and lead to brittle outputs when identifiers are obfuscated or rare.\n\nWe revisit copying through a constrained decoding lens, coupling a pointer mechanism with type-aware constraints and demonstrating improved robustness under identifier perturbations.",
    "reason": "Asserts a trend in 'recent works' and a specific modeling choice without citing representative papers (rule d and example i).",
    "start": 408,
    "end": 488,
    "label": "Unsupported claim"
  },
  {
    "span": "Prior studies have shown that back-translation yields 3–5 BLEU improvements in low-resource settings.",
    "document": "Related Work\n\nNeural machine translation in low-resource scenarios often compensates for limited parallel data by leveraging monolingual corpora. Techniques such as back-translation, multilingual transfer, and data augmentation via noising are frequently used to bolster performance when supervision is scarce.\n\nPrior studies have shown that back-translation yields 3–5 BLEU improvements in low-resource settings. However, these gains vary widely with the quality of the reverse model and the domain mismatch between synthetic and real source sentences. Furthermore, evaluation typically focuses on a few language pairs, leaving uncertainty about generalization to typologically distant languages.\n\nOur work revisits back-translation under a fixed compute budget, presenting an efficiency-accuracy trade-off analysis across multiple data scales and language pairs.",
    "reason": "Reports a specific performance range attributed to prior studies without providing citations to the studies or datasets, which requires evidence.",
    "start": 312,
    "end": 413,
    "label": "Unsupported claim"
  },
  {
    "span": "As reported by prior work, sim-to-real transfer succeeds when domain randomization covers at least 30% of the target variation.",
    "document": "Introduction\n\nReinforcement learning for robotics often relies on simulation to enable scalable data collection, but transferring policies to the real world remains difficult due to modeling errors and sensor noise. Domain randomization and system identification are common strategies to bridge the gap, while offline reinforcement learning and imitation learning reduce unsafe on-robot exploration. Policy architectures range from end-to-end visuomotor controllers to modular pipelines with learned perception and classical control.\n\nAs reported by prior work, sim-to-real transfer succeeds when domain randomization covers at least 30% of the target variation. Recent benchmarks standardize tasks and metrics for grasping, manipulation, and locomotion, facilitating reproducible comparisons across methods and hardware. Our method complements domain randomization with uncertainty-aware adaptation at deployment time.",
    "reason": "This sentence asserts a quantitative threshold based on prior work without citing any source; such statistical claims must be supported by citations (rules a and e).",
    "start": 535,
    "end": 662,
    "label": "Unsupported claim"
  },
  {
    "span": "The i2b2 dataset remains the most widely used benchmark for clinical NER.",
    "document": "Introduction\n\nClinical named entity recognition (NER) supports downstream applications such as cohort selection, pharmacovigilance, and clinical decision support. Despite privacy constraints, de-identified corpora have enabled progress by providing standardized annotation schemas and evaluation splits.\n\nThe i2b2 dataset remains the most widely used benchmark for clinical NER. Yet, domain shifts across institutions and note types can substantially degrade performance, underscoring the need for methods that better generalize across care settings. Additionally, reliance on token-level F1 can obscure entity boundary errors that have meaningful clinical implications.\n\nWe propose a domain-robust NER framework that integrates structure-aware pretraining with uncertainty-aware inference. Our experiments evaluate generalization under cross-hospital transfer and low-resource annotation budgets.",
    "reason": "Asserts the prominence of a specific dataset without citing any survey, usage statistics, or original dataset paper on first mention.",
    "start": 305,
    "end": 378,
    "label": "Unsupported claim"
  },
  {
    "span": "over 80% of industrial vision datasets are weakly labeled",
    "document": "Related Work\n\nSelf-supervised learning (SSL) has emerged as a dominant paradigm for visual representation learning by leveraging pretext objectives to reduce reliance on labeled data (Chen et al., 2020; He et al., 2020; Caron et al., 2021). Recent advances extend SSL to multimodal settings, pairing images with text or audio to improve generalization under distribution shifts (Radford et al., 2021; Alayrac et al., 2022).\n\nIn practical deployments, annotation costs and privacy constraints drive organizations toward noisy, incomplete supervision. Anecdotal evidence suggests that over 80% of industrial vision datasets are weakly labeled, motivating robust pretraining strategies that tolerate significant label noise. Yet, most benchmarks assume clean labels or curated web supervision, limiting external validity.\n\nWe focus on pretraining objectives that explicitly model uncertainty in supervisory signals and evaluate them on controlled noise-injection protocols, bridging the gap between curated academic benchmarks and real-world weak supervision.",
    "reason": "The statistic 'over 80%' is presented without any citation or evidence, making it an unsupported quantitative claim under rule b (niche-specific statistic) and the general definition.",
    "start": 583,
    "end": 640,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from grades 6–12.",
    "document": "Related Work\n\nAutomated essay scoring (AES) has progressed from handcrafted features to neural encoders that model coherence, content coverage, and discourse. Transfer learning from large language models has further improved cross-prompt generalization.\n\nBERT was used in an AES task trained on essays from grades 6–12. Approaches vary in how they align rubric dimensions to latent representations and how they mitigate positional bias across prompts.\n\nDespite improvements in Pearson correlation with human scores, fairness and transparency remain pressing issues for deployment in educational settings.",
    "reason": "Describes a specific prior setup using BERT in AES without providing a citation (violates rule a and b).",
    "start": 255,
    "end": 319,
    "label": "Unsupported claim"
  },
  {
    "span": "The CoNLL 2012 coreference dataset includes gold speaker annotations for all dialogues.",
    "document": "Related Work\n\nCoreference resolution has progressed from mention-pair models to end-to-end neural systems that jointly detect and link mentions (Ng and Cardie, 2002; Clark and Manning, 2016; Lee et al., 2017). The OntoNotes 5.0 corpus and the CoNLL shared tasks have provided large-scale benchmarks spanning multiple genres (Pradhan et al., 2012). The CoNLL 2012 coreference dataset includes gold speaker annotations for all dialogues. Our approach augments span representations with conversational cues and cross-turn discourse markers to improve resolution in dialogue-heavy domains.",
    "reason": "Makes a specific dataset claim without providing a citation to support it (definition a and ii).",
    "start": 348,
    "end": 435,
    "label": "Unsupported claim"
  },
  {
    "span": "Existing automatic metrics for code generation include exact match, BLEU, CodeBLEU, and execution-based Pass@k (Papineni et al., 2002; Ren et al., 2020; Chen et al., 2021). Several works also evaluate functional correctness via test suites and static analysis (Austin et al., 2021; Mastropaolo et al., 2021). We propose a new metric in this space.",
    "document": "Related Work\n\nEvaluation of code generation models hinges on both surface similarity and semantic correctness. The community has explored lexical, structural, and execution-grounded metrics.\n\nExisting automatic metrics for code generation include exact match, BLEU, CodeBLEU, and execution-based Pass@k (Papineni et al., 2002; Ren et al., 2020; Chen et al., 2021). Several works also evaluate functional correctness via test suites and static analysis (Austin et al., 2021; Mastropaolo et al., 2021). We propose a new metric in this space.\n\nRecent datasets provide richer unit tests and multi-turn problem statements to stress reasoning and robustness of program synthesis systems (Hendrycks et al., 2021; Li et al., 2022).",
    "reason": "The text lists prior metrics and then immediately asserts a new metric without stating why existing metrics are insufficient or how the new one addresses a specific gap, matching criterion b for lack of synthesis.",
    "start": 192,
    "end": 539,
    "label": "Lacks synthesis"
  },
  {
    "span": "GraphSAGE introduced the idea of inductive representation learning for graphs.",
    "document": "Related Work\n\nGraph neural networks (GNNs) enable learning over relational structures by aggregating information from neighborhoods. Early architectures targeted transductive settings, where all nodes are observed during training. GraphSAGE introduced the idea of inductive representation learning for graphs. This shift made it possible to generalize to unseen nodes and dynamic graphs by sampling and aggregating local neighborhoods. Subsequent advances refined aggregation functions, normalized message passing, and addressed oversmoothing. Our work extends inductive GNNs with sparsity-aware training and curriculum sampling for large-scale heterogeneous graphs.",
    "reason": "References a specific prior method and its core contribution without providing a citation to the original paper.",
    "start": 231,
    "end": 309,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent benchmarks report state-of-the-art F1 above 90% on zero-shot intent detection.",
    "document": "Introduction\n\nIntent detection systems are typically trained with supervised data for a fixed ontology. In realistic scenarios, though, intents evolve and new intents appear frequently, motivating zero-shot methods that generalize to unseen labels using descriptions or examples.\n\nRecent benchmarks report state-of-the-art F1 above 90% on zero-shot intent detection. These results raise questions about dataset design, label granularity, and the extent to which lexical overlap between intent names and utterances inflates performance.\n\nWe introduce a challenge suite with adversarially paraphrased intents, compositional labels, and domain transfer. Our approach uses representation debiasing and contrastive calibration to reduce overreliance on surface cues.\n\nWe analyze error modes across semantic similarity regimes and report robust improvements under strict evaluation protocols.",
    "reason": "The sentence references 'recent benchmarks' and specific performance numbers but provides no citations to those benchmarks (rule d).",
    "start": 281,
    "end": 366,
    "label": "Unsupported claim"
  },
  {
    "span": "Differential privacy (Dwork et al., 2006) provides a formal guarantee by bounding the change in output distributions when a single record varies. Mechanisms such as the Gaussian mechanism and DP-SGD add calibrated noise to ensure (ε, δ)-DP (Abadi et al., 2016). In NLP, DP has been applied to language modeling and text classification with varying utility costs (McMahan et al., 2018; Yu et al., 2021).",
    "document": "Introduction\n\nPrivacy in Language Technologies\nLarge-scale language models are often trained on sensitive text, raising concerns about membership inference and memorization. Differential privacy offers principled protections during training.\n\nBackground and Applications\nDifferential privacy (Dwork et al., 2006) provides a formal guarantee by bounding the change in output distributions when a single record varies. Mechanisms such as the Gaussian mechanism and DP-SGD add calibrated noise to ensure (ε, δ)-DP (Abadi et al., 2016). In NLP, DP has been applied to language modeling and text classification with varying utility costs (McMahan et al., 2018; Yu et al., 2021).\n\nOverview\nWe evaluate privacy-utility trade-offs on multi-domain corpora and analyze memorization under varying noise budgets.",
    "reason": "Provides background and citations but does not connect them to a concrete research problem, motivation, or the authors' approach (definition c/a).",
    "start": 271,
    "end": 673,
    "label": "Lacks synthesis"
  },
  {
    "span": "IEMOCAP is the de facto benchmark for multimodal emotion recognition.",
    "document": "Related Work\n\nSpeech Emotion Recognition (SER) aims to infer affective states from acoustic and linguistic cues (Schuller et al., 2011). Traditional approaches rely on prosodic features with SVMs, while recent models exploit spectrogram CNNs and end-to-end transformers (Tzirakis et al., 2018; Pepino et al., 2021). IEMOCAP is the de facto benchmark for multimodal emotion recognition. Other corpora such as RAVDESS and CREMA-D offer controlled conditions but limited spontaneous interactions (Livingstone and Russo, 2018; Cao et al., 2014). Our work focuses on cross-corpus generalization through self-supervised pretraining on unlabeled conversational data.\n\nIntroduction\n\nWe propose a dual-stream architecture that aligns acoustic and lexical embeddings via contrastive objectives, improving robustness to speaker and channel shift. Experiments indicate gains over strong baselines across five datasets.",
    "reason": "Asserts a field-wide status of a specific dataset without providing a supporting citation at first mention.",
    "start": 316,
    "end": 385,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT was used in an AES setup with prompt-aware adapters to score essays across eight prompts.",
    "document": "Introduction\n\nAutomated essay scoring (AES) seeks to produce reliable, prompt-sensitive scores using models that generalize across topics and writing styles. Pretrained language models have become the de facto backbone for AES due to their strong contextual understanding and ease of fine-tuning. BERT was used in an AES setup with prompt-aware adapters to score essays across eight prompts. While such designs aim to balance cross-prompt transfer and prompt specificity, they may overfit to prompt artifacts when training data is scarce. We therefore propose a calibration strategy that disentangles prompt features from general writing quality signals.\n\nRelated Work\n\nAES approaches have historically combined surface features, syntactic complexity, and discourse indicators with regression models. Neural systems improved performance through hierarchical encoders and auxiliary learning objectives. Recent efforts emphasize robustness to adversarial edits and fairness across demographic groups. However, prompt leakage and scoring drift remain open challenges for deployed systems.",
    "reason": "Describes a specific experimental setup and prior use of BERT with prompt-aware adapters without citing the study that introduced or evaluated it.",
    "start": 297,
    "end": 391,
    "label": "Unsupported claim"
  },
  {
    "span": "In a previous study, the authors claim that graph pooling improves interpretability without sacrificing accuracy.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have achieved strong results on molecular property prediction, recommendation, and social network analysis (Kipf and Welling, 2017; Hamilton et al., 2017). To capture hierarchical structure, numerous graph pooling operators have been proposed, including top-k pooling and differentiable pooling variants (Gao and Ji, 2019; Ying et al., 2018). Interpretability for GNNs has been examined through subgraph explanations and feature attribution (Yuan et al., 2020; Luo et al., 2020). In a previous study, the authors claim that graph pooling improves interpretability without sacrificing accuracy. However, empirical evidence across domains is mixed, and evaluation protocols vary widely.\n\nWe revisit hierarchical pooling with standardized datasets and metrics, and propose a calibration procedure for pooling selection that maintains fidelity while limiting over-smoothing.",
    "reason": "References a 'previous study' and its claim without citing the study, which is required when mentioning prior work.",
    "start": 523,
    "end": 636,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent benchmarks show that retrieval-augmented models reduce hallucination by 30%.",
    "document": "Introduction\n\nLarge language models (LLMs) can generate factually inconsistent statements, prompting research on attribution and grounding (Maynez et al., 2020; Ji et al., 2023). Retrieval-augmented generation (RAG) conditions decoding on retrieved documents to improve factuality and answer recall (Lewis et al., 2020; Izacard and Grave, 2021). Recent benchmarks show that retrieval-augmented models reduce hallucination by 30%. Complementary strategies include faithfulness-aware decoding and post-hoc verification modules (Kadavath et al., 2022; Manakul et al., 2023).\n",
    "reason": "Presents a quantitative statistic about 'recent benchmarks' without citing any source; per (d) and (b) such claims require specific references.",
    "start": 346,
    "end": 429,
    "label": "Unsupported claim"
  },
  {
    "span": "Surveyed practitioners overwhelmingly prefer simpler models over deep learning for fairness reasons.",
    "document": "Related Work\n\nAlgorithmic fairness has been studied through metrics such as demographic parity, equalized odds, and calibration (Hardt et al., 2016; Kleinberg et al., 2017). Practical mitigation includes pre-, in-, and post-processing methods (Feldman et al., 2015; Agarwal et al., 2018). Adoption in industry depends on interpretability, regulatory compliance, and organizational incentives (Holstein et al., 2019). Surveyed practitioners overwhelmingly prefer simpler models over deep learning for fairness reasons. Yet, empirical evidence on the trade-offs between transparency and accuracy in high-stakes domains is mixed and context-dependent.\n\nWe contribute a multi-institutional study comparing interpretable generalized additive models to deep ensembles on credit and hiring datasets, with human-in-the-loop audits to assess perceived fairness.",
    "reason": "Reports findings from a survey without citing the survey source, which is necessary when referencing prior empirical studies.",
    "start": 417,
    "end": 517,
    "label": "Unsupported claim"
  },
  {
    "span": "Transformer encoders pretrained on protein sequences have advanced contact prediction and structure modeling (Rives et al., 2019; Rao et al., 2020; Elnaggar et al., 2021).",
    "document": "Introduction\n\nProtein structure prediction benefits from advances in representation learning on large unlabeled sequence corpora. Self-supervised objectives capture evolutionary regularities that transfer to downstream tasks including contact prediction and function annotation.\n\nTransformer encoders pretrained on protein sequences have advanced contact prediction and structure modeling (Rives et al., 2019; Rao et al., 2020; Elnaggar et al., 2021). Complementary efforts integrate multiple sequence alignments and template information (Senior et al., 2020; Baek et al., 2021) and explore geometric inductive biases (Jing et al., 2021; Ingraham et al., 2019).\n\nWe introduce a pretraining scheme that conditions masked language modeling on predicted local geometry to improve single-sequence structure inference.",
    "reason": "The span reports progress and cites prior work but does not state how it relates to the paper's contribution or what gap persists, aligning with (a) and (c).",
    "start": 280,
    "end": 451,
    "label": "Lacks synthesis"
  },
  {
    "span": "Exploration bonuses derived from prediction error or novelty have been widely studied, including count-based methods, pseudo-counts, and curiosity-driven signals (Bellemare et al., 2016; Ostrovski et al., 2017; Pathak et al., 2017). Random Network Distillation (Burda et al., 2019) and episodic memory approaches (Savinov et al., 2018) further encourage agents to visit unseen states. For extremely sparse tasks, strategies like Go-Explore separate exploration and robustification (Ecoffet et al., 2021).",
    "document": "Related Work\n\nSparse-reward reinforcement learning poses significant challenges due to the need for targeted exploration and long-horizon credit assignment (Ecoffet et al., 2021; Badia et al., 2020). A large body of research has developed intrinsic motivation signals and structured curricula to address these issues.\n\nExploration bonuses derived from prediction error or novelty have been widely studied, including count-based methods, pseudo-counts, and curiosity-driven signals (Bellemare et al., 2016; Ostrovski et al., 2017; Pathak et al., 2017). Random Network Distillation (Burda et al., 2019) and episodic memory approaches (Savinov et al., 2018) further encourage agents to visit unseen states. For extremely sparse tasks, strategies like Go-Explore separate exploration and robustification (Ecoffet et al., 2021).\n\nHierarchical and goal-conditioned RL offer alternative strategies to structure exploration through subgoals and relabeling methods (Vezhnevets et al., 2017; Andrychowicz et al., 2017). Environment design and training pipelines, including resets and demonstrations, can also mitigate exploration challenges (Nair et al., 2018; Zhang et al., 2020).",
    "reason": "The span compiles prior techniques but does not articulate how they relate to the authors' approach or what specific limitation motivates the present study, violating a and c.",
    "start": 319,
    "end": 823,
    "label": "Lacks synthesis"
  },
  {
    "span": "A previous study reported a 20% reduction in MAE when using adaptive adjacency matrices.",
    "document": "Related Work\n\nTraffic forecasting with graph neural networks (GNNs) models spatiotemporal dependencies among sensors or road segments. Early methods used fixed, distance-based adjacency matrices with temporal sequence models (Yu et al., 2018; Li et al., 2018). Subsequent work introduced dynamic or learnable graphs to capture time-varying connectivity and hidden correlations (Bai et al., 2020; Wu et al., 2019).\n\nA previous study reported a 20% reduction in MAE when using adaptive adjacency matrices. While learnable graphs can enhance flexibility, they introduce additional parameters and stability concerns under distribution shifts. Our method regularizes the learned topology with a physics-informed prior and evaluates robustness under sensor dropout.\n\nBenchmarks commonly used for evaluation include METR-LA and PEMS datasets, with standardized train/validation/test splits (Li et al., 2018; Guo et al., 2019). We adopt these protocols and augment them with adverse-event scenarios to examine deployment resilience.",
    "reason": "Mentions 'a previous study' with a specific quantitative result but provides no citation to that study (rule e.ii).",
    "start": 415,
    "end": 503,
    "label": "Unsupported claim"
  },
  {
    "span": "(Li et al. 2020)",
    "document": "Related Work\n\nContrastive learning has been applied to sentence embeddings with notable gains (Kiros et al., 2015; Gao et al., 2021). In multilingual scenarios, alignment losses encourage cross-lingual consistency (Feng et al., 2022). Prior work (Li et al. 2020) explored hard negative mining but reported instability without temperature tuning (Robinson et al., 2021).\n\nOur method introduces curriculum-aware negatives to stabilize training while preserving diversity in sampled pairs (Chuang et al., 2020).",
    "reason": "Missing comma between author and year in a parenthetical citation; should be \"(Li et al., 2020)\".",
    "start": 246,
    "end": 262,
    "label": "Format"
  },
  {
    "span": "most recent works adopt dense passage retrievers for scalability.",
    "document": "Related Work\n\nOpen-Domain Question Answering\n\nOpen-domain question answering (ODQA) aims to answer questions using a large external corpus without structured supervision. Following early neural readers that paired with TF-IDF retrieval, most recent works adopt dense passage retrievers for scalability. Retriever-reader pipelines have become the de facto approach, where a neural retriever selects candidate passages and a neural reader extracts or generates answers. Recent generative readers aggregate evidence across multiple passages, while extractive readers focus on span selection within a single passage. Despite substantial progress, retriever training signals and reader robustness to noise remain open problems.",
    "reason": "Claims a trend about recent works without citing any supporting papers (rule d and b).",
    "start": 237,
    "end": 302,
    "label": "Unsupported claim"
  },
  {
    "span": "Vargas et al. (2019) introduced a message passing framework for molecular property prediction. Kim and Ortega (2021) explored equivariant networks for modeling 3D conformations. Patel et al. (2020) studied data augmentation with scaffold splitting. Zhou and Liang (2022) proposed contrastive pretraining on unlabeled molecules.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become central to molecular machine learning due to their ability to capture relational structure and local chemistry. Prior efforts address representation power, inductive bias for 3D geometry, and data-efficient training. We summarize representative lines of work.\n\nVargas et al. (2019) introduced a message passing framework for molecular property prediction. Kim and Ortega (2021) explored equivariant networks for modeling 3D conformations. Patel et al. (2020) studied data augmentation with scaffold splitting. Zhou and Liang (2022) proposed contrastive pretraining on unlabeled molecules.\n\nBeyond architectures, researchers investigate uncertainty estimation for safer screening (Ghosh et al., 2023) and multi-task transfer for low-resource assays (Reddy and Xu, 2021). Our approach integrates equivariant layers with uncertainty-aware objectives for robust molecular ranking.",
    "reason": "The span enumerates distinct works as isolated statements without transitions or an explicit thread linking geometry, augmentation, and pretraining; the relationship among these citations is implied but not stated, reducing coherence.",
    "start": 316,
    "end": 643,
    "label": "Coherence"
  },
  {
    "span": "Our approach builds on the widely adopted CoQA baseline where span extraction is paired with dialog state modeling.",
    "document": "Introduction\n\nConversational question answering extends machine reading by requiring models to resolve coreference, track context, and handle ellipsis across turns. Early single-turn QA models underperform when confronted with multi-turn phenomena, motivating architectures that maintain dialog state.\n\nOur approach builds on the widely adopted CoQA baseline where span extraction is paired with dialog state modeling. We enhance this setup with a lightweight memory mechanism and uncertainty-aware decoding to mitigate error accumulation over long conversations.\n\nWe evaluate on multiple conversational QA datasets and report improved performance in settings with sparse conversational cues.",
    "reason": "Mentions a specific baseline associated with a known dataset and claims it is widely adopted without citing the baseline paper or dataset, violating rule (a) and (d).",
    "start": 303,
    "end": 418,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works have conclusively shown that ViTs surpass CNNs under strong PGD attacks.",
    "document": "Introduction\n\nThe robustness of image classifiers to adversarial perturbations remains a central challenge in reliable computer vision (Szegedy et al., 2014; Madry et al., 2018). Vision Transformers (ViTs) have emerged as competitive alternatives to convolutional networks across classification and detection tasks (Dosovitskiy et al., 2021; Touvron et al., 2021). Recent works have conclusively shown that ViTs surpass CNNs under strong PGD attacks. However, it is unclear whether these gains persist when training data are scarce or when models are evaluated under adaptive threat models with tuned step sizes and restarts. We systematically compare ViTs and CNNs across data regimes and threat configurations, and introduce a hybrid token-aggregation scheme that improves certified and empirical robustness.\n\nRelated Work\n\nAdversarial training and regularization have been extensively studied for CNNs (Madry et al., 2018; Rice et al., 2020). Robustness properties of transformers have been explored through frequency analysis and token smoothing (Shaffer et al., 2022; Bhojanapalli et al, 2021). Data-efficient ViTs rely on distillation and strong augmentations (Touvron et al., 2021; Steiner et al., 2022).",
    "reason": "The claim about 'recent works' demonstrating ViT superiority under PGD is not accompanied by citations at first mention.",
    "start": 365,
    "end": 450,
    "label": "Unsupported claim"
  },
  {
    "span": "Message passing neural networks aggregate neighbor features (Gilmer et al., 2017). Graph isomorphism networks increase expressiveness with sum aggregators (Xu et al., 2019). Equivariant models respect 3D symmetries for molecules (Satorras et al., 2021). Contrastive pretraining improves molecular representations (You et al., 2020).",
    "document": "Related Work\n\nPredicting molecular properties with graph neural networks (GNNs) has advanced through improved architectures and training objectives. Key ideas span message passing, expressive aggregation, geometric inductive biases, and pretraining on large unlabeled corpora.\n\nMessage passing neural networks aggregate neighbor features (Gilmer et al., 2017). Graph isomorphism networks increase expressiveness with sum aggregators (Xu et al., 2019). Equivariant models respect 3D symmetries for molecules (Satorras et al., 2021). Contrastive pretraining improves molecular representations (You et al., 2020).\n\nWe build on these insights by coupling 3D-equivariant message passing with task-adaptive contrastive objectives, enabling better generalization across scaffolds and data-scarce targets.",
    "reason": "The span is a sequence of unconnected sentences about distinct techniques (MPNNs, GIN, equivariant models, contrastive pretraining), without transitions or an explicit explanation of their relationships. This results in abrupt shifts and weak coherence.",
    "start": 278,
    "end": 610,
    "label": "Coherence"
  },
  {
    "span": "Multilingual pretraining transfers representations across languages (Conneau et al., 2020). Back-translation improves data scarcity by generating synthetic pairs (Sennrich et al., 2016). Character-level models are robust to noisy orthography (Lee et al., 2017). Pivot-based translation uses an intermediate high-resource language (Utiyama and Isahara, 2007).",
    "document": "Related Work\n\nLow-resource machine translation remains challenging due to limited parallel corpora and domain mismatch. Solutions leverage cross-lingual sharing, synthetic data, and architectural choices tailored to morphology and noise.\n\nMultilingual pretraining transfers representations across languages (Conneau et al., 2020). Back-translation improves data scarcity by generating synthetic pairs (Sennrich et al., 2016). Character-level models are robust to noisy orthography (Lee et al., 2017). Pivot-based translation uses an intermediate high-resource language (Utiyama and Isahara, 2007).\n\nWe build on multilingual encoders while incorporating lexicon-constrained back-translation to better capture rare terminology in technical domains.",
    "reason": "The span presents four techniques in isolation without transitions or explicit connections, leaving the reader to infer relationships among multilingual pretraining, back-translation, character models, and pivoting.",
    "start": 239,
    "end": 597,
    "label": "Coherence"
  },
  {
    "span": "recent audits report demographic parity gaps above 30% in commercial hiring systems",
    "document": "Related Work\n\nAlgorithmic fairness in employment screening has drawn increasing regulatory scrutiny, prompting research on disparate impact mitigation and transparent model reporting (Barocas and Selbst, 2016; Raji et al., 2020). Techniques span pre-processing reweighting, in-processing constraints, and post-processing calibration (Hardt et al., 2016; Agarwal et al., 2018).\n\nDespite methodological advances, recent audits report demographic parity gaps above 30% in commercial hiring systems. Yet many studies rely on synthetic benchmarks or proprietary datasets, complicating external validation and reproducibility.\n\nWe release a semi-synthetic, legally compliant benchmark built from public resumes and job postings, and we propose an evaluation protocol that jointly measures selection utility and multiple group fairness criteria.",
    "reason": "This sentence claims specific audit findings with a quantitative figure but gives no citation, violating rule d (recent works must be cited) and the general definition for unsupported statistics.",
    "start": 411,
    "end": 494,
    "label": "Unsupported claim"
  },
  {
    "span": "The benchmark contains over 120,000 annotated radiology reports collected across six hospitals.",
    "document": "Introduction\n\nAutomated summarization of radiology reports has gained traction as hospitals adopt electronic health records at scale. Prior work has explored extractive methods (e.g., sentence selection) and abstractive sequence-to-sequence models to generate concise study impressions from verbose findings (see, for example, models based on encoder–decoder transformers for medical text). Clinical summarization is challenging due to domain-specific terminology, implicit reasoning, and the need to preserve critical findings.\n\nWe study self-supervised pretraining objectives tailored to medical discourse and evaluate transfer to report summarization. We compare contrastive learning approaches with masked language modeling using de-identified corpora and assess fidelity to clinical facts using automatic metrics and expert review.\n\nThe benchmark contains over 120,000 annotated radiology reports collected across six hospitals. We release standardized splits and a codebase to encourage reproducible comparisons. Our contributions include (1) a pretraining analysis across objectives and corpora size, (2) a thorough evaluation on chest and abdominal imaging subsets, and (3) an error taxonomy highlighting common failure modes.\n\nRelated Work\n\nGeneral-purpose summarization has advanced with large-scale pretraining and instruction tuning. In the clinical domain, prior efforts have focused on discharge summaries and progress notes, with limited attention to imaging reports. We build on this line of research by emphasizing factual consistency and clinical utility in radiology.",
    "reason": "Presents a precise dataset size and collection scope for a benchmark without providing a citation to the dataset or source.",
    "start": 838,
    "end": 933,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent competitions have shifted toward transductive episodic evaluation with open-set distractors.",
    "document": "Introduction\n\nFew-shot image classification evaluates a model's ability to generalize to novel classes from a handful of labeled examples. Early metric-based methods such as matching and prototype networks introduced episodic training to mimic test conditions, while optimization-based approaches learned rapid adaptation via meta-learning. Data augmentation, self-supervised pretraining, and stronger backbones have all contributed to steady improvements under the standard N-way, K-shot protocol.\n\nRecent competitions have shifted toward transductive episodic evaluation with open-set distractors. Concurrently, benchmarking efforts have emphasized cross-domain generalization where the base and novel classes differ significantly in style and content, highlighting the brittleness of overfitted meta-learners. Our work follows this direction by evaluating under both closed- and open-set conditions across multiple domains.",
    "reason": "This sentence refers to 'recent competitions' and specific evaluation shifts without citing any competitions or reports; mentions of recent works and shared tasks require citations (rules a and d).",
    "start": 500,
    "end": 599,
    "label": "Unsupported claim"
  },
  {
    "span": "Garcia et al., 2021",
    "document": "Related Work\n\nInterpretability techniques increasingly focus on faithfulness and robustness. Garcia et al., 2021 show that attention weights poorly correlate with gradient-based importance across architectures. Follow-up research examines causal interventions to assess feature influence, yet results vary with proxy tasks and training objectives.\n\nWe contribute a benchmark that couples counterfactual evaluation with ground-truth causal graphs to assess explanation faithfulness.",
    "reason": "Narrative citation places a comma before the year instead of using parentheses; it should be Garcia et al. (2021).",
    "start": 93,
    "end": 112,
    "label": "Format"
  },
  {
    "span": "The UCF-101 dataset was originally collected from YouTube cooking channels.",
    "document": "Introduction\n\nAction recognition has progressed rapidly with the availability of large-scale video datasets and 3D convolutional architectures (Carreira and Zisserman, 2017; Feichtenhofer et al., 2019). Benchmarks such as HMDB-51 and Kinetics have standardized evaluation across diverse motion categories (Kuehne et al., 2011; Kay et al., 2017). The UCF-101 dataset was originally collected from YouTube cooking channels. Despite strong performance on trimmed clips, models still struggle with long-form temporal reasoning and domain shift (Hara et al., 2018; Girdhar and Ramanan, 2020).\n",
    "reason": "States a specific provenance detail about a named dataset without citing a source; per (a) dataset descriptions should include a citation at first mention.",
    "start": 346,
    "end": 421,
    "label": "Unsupported claim"
  },
  {
    "span": "We pretrain on Common Crawl as our primary corpus",
    "document": "Introduction\n\nLarge-scale pretraining on web text produces general-purpose language models that transfer effectively to downstream tasks. Corpus composition and filtering critically influence lexical diversity, toxicity, and domain balance, shaping emergent capabilities and biases. We pretrain on Common Crawl as our primary corpus and apply lightweight deduplication to reduce redundancy.\n\nOverview\n\nWe study data size scaling laws, toxicity mitigation via filtering, and downstream effects on summarization and QA.",
    "reason": "Introduces a specific dataset (Common Crawl) at first mention without citing its source or documentation (violates guideline a).",
    "start": 283,
    "end": 332,
    "label": "Unsupported claim"
  },
  {
    "span": "Prompt-based tuning reformulates classification as masked-token prediction and often reduces the gap between pre-training and fine-tuning (Schick and Schütze, 2021; Gao et al., 2021). We follow the categorization that separates discrete prompts from continuous prompts (Lester et al., 2021; Liu et al., 2021). Few works have examined zero-shot cross-lingual generalization with prompts (Winata et al., 2021; Ponti et al., 2022).",
    "document": "Introduction\n\nPrompting techniques have reshaped how pre-trained language models are adapted to downstream tasks by aligning objectives with pre-training. For multilingual understanding, prompts offer a lightweight interface that could alleviate data scarcity in low-resource languages. Despite this promise, effective cross-lingual prompt design remains underexplored.\n\nPrompt-based tuning reformulates classification as masked-token prediction and often reduces the gap between pre-training and fine-tuning (Schick and Schütze, 2021; Gao et al., 2021). We follow the categorization that separates discrete prompts from continuous prompts (Lester et al., 2021; Liu et al., 2021). Few works have examined zero-shot cross-lingual generalization with prompts (Winata et al., 2021; Ponti et al., 2022).\n\nPrior work on cross-lingual transfer has leveraged multilingual encoders, alignment losses, and translation-based augmentation. Template engineering and verbalizer selection are also known to strongly influence prompt outcomes. Nevertheless, systematic studies of transfer behaviors across scripts and morphology are limited.\n\nWe present a framework for prompt selection and calibration that jointly optimizes source-task fit and target-language priors. Our experiments cover typologically diverse languages and analyze sensitivity to tokenization, verbalizers, and template semantics.",
    "reason": "The span strings together statements about prompt-based tuning, a taxonomy of prompt types, and a claim about cross-lingual work without transitions that explain how each relates to the previous point, reducing coherence across the cited works.",
    "start": 371,
    "end": 799,
    "label": "Coherence"
  },
  {
    "span": "(Brown, 2018, Clark et al., 2019)",
    "document": "Related Work\n\nPretrained language models have transformed information extraction, with early work demonstrating gains from contextual embeddings. Existing baselines (Brown, 2018, Clark et al., 2019) compare span-based and sequence-labeling paradigms, but differ in sampling and evaluation protocols, complicating head-to-head comparisons. Later studies introduced prompt-based tuning for few-shot adaptation, albeit with sensitivity to template wording and verbalizers.\n\nOur framework standardizes evaluation across paradigms and introduces semantics-aware prompts to reduce template brittleness.",
    "reason": "Improper delimiter within multiple citations; semicolons should separate distinct works in APA-like author–year style, e.g., (Brown, 2018; Clark et al., 2019).",
    "start": 165,
    "end": 198,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on essays from state writing assessments.",
    "document": "Introduction\n\nAutomated essay scoring (AES) aims to predict human-assigned scores for student writing with consistency and fairness. Early AES systems relied on hand-engineered features such as length, lexical diversity, and syntactic complexity (Attali and Burstein, 2006). Recent neural approaches leverage pre-trained language models to capture discourse and semantic coherence (Ushio et al., 2022; Ke and Ng, 2019). Despite progress, concerns persist about domain shift across prompts and unintended bias (Yannakoudakis et al., 2011; Madnani and Cahill, 2018). BERT was used in an AES task trained on essays from state writing assessments. However, the generalizability of such models to unseen prompts remains underexplored.\n\nWe introduce a prompt-agnostic training framework with calibration via score normalization and counterfactual token masking to disentangle content from form.",
    "reason": "Mentions a specific prior application of BERT to AES and dataset provenance without providing a citation to the study.",
    "start": 565,
    "end": 643,
    "label": "Unsupported claim"
  },
  {
    "span": "CRISPR off-target prediction is now considered a solved problem in practice.",
    "document": "Related Work\n\nPredicting CRISPR Off-Target Effects\n\nComputational models predict off-target cleavage by learning sequence and chromatin features associated with Cas nuclease activity. Early classifiers used handcrafted mismatch counts and position penalties, while modern approaches apply deep CNNs to capture motif dependencies and genome context (Doench et al., 2016; Hsu et al., 2013; Kim et al., 2019). CRISPR off-target prediction is now considered a solved problem in practice. Nonetheless, discrepancies between in vitro assays and in vivo outcomes persist, and cellular context can significantly alter editing efficiency (Listgarten et al., 2018). We address generalization under domain shift by incorporating assay-aware embeddings.",
    "reason": "Makes a strong field-wide claim about the status of a niche problem without providing supporting citations or evidence, violating rule b.",
    "start": 407,
    "end": 483,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent approaches to graph anomaly detection include reconstruction-based methods that learn to compress normal patterns (Zheng et al., 2019; Fan et al., 2020), contrastive learning that separates anomalies from inliers through augmented views (Velickovic et al., 2019; Zhu et al., 2021), community and subgraph scoring techniques (Akoglu et al., 2015; Perozzi et al., 2014), and spectral or matrix factorization tools (Shi et al., 2016; Ma et al., 2021). Temporal extensions incorporate recurrent or attention modules to track evolving structures (Ribeiro et al., 2020; Dang et al., 2021).",
    "document": "Related Work\n\nAnomaly detection on graphs aims to identify unusual nodes, edges, or subgraphs that deviate from regular connectivity or attribute patterns. Applications include fraud detection, intrusion detection, and quality control in sensor networks.\n\nRecent approaches to graph anomaly detection include reconstruction-based methods that learn to compress normal patterns (Zheng et al., 2019; Fan et al., 2020), contrastive learning that separates anomalies from inliers through augmented views (Velickovic et al., 2019; Zhu et al., 2021), community and subgraph scoring techniques (Akoglu et al., 2015; Perozzi et al., 2014), and spectral or matrix factorization tools (Shi et al., 2016; Ma et al., 2021). Temporal extensions incorporate recurrent or attention modules to track evolving structures (Ribeiro et al., 2020; Dang et al., 2021).\n\nOur method targets dynamic transaction graphs, introducing calibration mechanisms that stabilize anomaly scores under diurnal patterns and sudden load spikes.",
    "reason": "The span summarizes categories of prior work with citations but does not connect them to the authors' problem setting or articulate what is missing that the proposed method addresses.",
    "start": 256,
    "end": 846,
    "label": "Lacks synthesis"
  },
  {
    "span": "Scalable GCN variants reduce neighborhood explosion via sampling (Hamilton et al., 2017; Chen et al., 2018). Negative sampling strategies affect ranking quality in implicit feedback settings (Rendle et al., 2009; Zhang et al., 2013). Knowledge graph embeddings incorporate side information into recommenders (Wang et al., 2019).",
    "document": "Related Work\n\nGraph-based recommender systems model user–item interactions as bipartite graphs and propagate signals with message passing (Ying et al., 2018). Subsequent works address scalability and cold-start challenges while improving ranking metrics on large catalogs.\n\nScalable GCN variants reduce neighborhood explosion via sampling (Hamilton et al., 2017; Chen et al., 2018). Negative sampling strategies affect ranking quality in implicit feedback settings (Rendle et al., 2009; Zhang et al., 2013). Knowledge graph embeddings incorporate side information into recommenders (Wang et al., 2019). Our approach unifies sampling with semantic constraints derived from side information to improve long-tail recommendations.\n",
    "reason": "The paragraph lists three distinct themes without clarifying their relationships; the movement from scalability to negative sampling to knowledge graphs is abrupt and unconnected.",
    "start": 274,
    "end": 602,
    "label": "Coherence"
  },
  {
    "span": "satellite-based nowcasting has reduced short-term precipitation forecast errors by as much as 30%",
    "document": "Introduction\n\nPrecipitation Nowcasting Short-term forecasting (0–2 hours) supports critical decisions in aviation, flood control, and urban mobility. Radar extrapolation and physics-based models have complementary strengths but struggle with rapid convective initiation (Sun et al., 2014; Ravuri et al., 2021). Recently, satellite-based nowcasting has reduced short-term precipitation forecast errors by as much as 30%, highlighting the value of high-cadence geostationary observations for cloud dynamics.\n\nOur Contributions We present a multi-scale encoder that fuses infrared channels with radar mosaics via cross-attention, and we introduce a temporal curriculum to stabilize training on rapidly evolving storms.",
    "reason": "This quantitative improvement claim about prior work lacks a citation to the study demonstrating the 30% error reduction, which must be referenced.",
    "start": 321,
    "end": 418,
    "label": "Unsupported claim"
  },
  {
    "span": "Most prior work relies on back-translation as the primary data augmentation technique for low-resource machine translation.",
    "document": "Related Work\n\nLow-resource machine translation (MT) leverages transfer learning, multilingual training, and data augmentation to compensate for data scarcity (Johnson et al., 2017; Neubig and Hu, 2018). Back-translation popularized the use of synthetic target-side data and has become a strong baseline in medium- and high-resource settings (Sennrich et al., 2016). Unsupervised and semi-supervised methods further reduce dependence on labeled data by exploiting monolingual corpora (Lample et al., 2018; Artetxe et al., 2019).\n\nMost prior work relies on back-translation as the primary data augmentation technique for low-resource machine translation. Nonetheless, its effectiveness can degrade when monolingual corpora are also limited or domain-mismatched. Alternatives include noising objectives, pivoting, and bilingual lexicon induction, but a comprehensive comparison across truly low-resource pairs is still lacking.\n\nWe contribute a systematic study of augmentation strategies under realistic low-resource constraints, controlling for domain and monolingual data size across five language pairs with typological diversity.",
    "reason": "Asserts what 'most prior work' does without supporting citations to representative studies (rule d and example i).",
    "start": 529,
    "end": 652,
    "label": "Unsupported claim"
  },
  {
    "span": "The CMU-MOSI dataset has been widely criticized for annotation inconsistencies.",
    "document": "Related Work\n\nMultimodal sentiment analysis combines linguistic, acoustic, and visual cues to predict affect and opinion intensity. Early fusion methods concatenate features, while late fusion aligns modality-specific predictions, with both approaches benefiting from cross-modal attention (Zadeh et al., 2016; Tsai et al., 2019). The CMU-MOSI dataset has been widely criticized for annotation inconsistencies. To mitigate label noise and limited scale, larger corpora and semi-supervised strategies have been introduced, alongside evaluations emphasizing speaker independence and demographic balance (Bagher Zadeh et al., 2018; Hazarika et al., 2020). We contribute a noise-aware calibration technique that improves temporal localization of sentiment cues across modalities.\n",
    "reason": "Makes a critical claim about a specific dataset without providing any supporting citations, violating rule (a).",
    "start": 331,
    "end": 410,
    "label": "Unsupported claim"
  },
  {
    "span": "There is a growing body of research on federated learning for ASR.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) models typically require access to large volumes of speech data, which may be sensitive and difficult to centralize. Federated learning offers a way to train models collaboratively across clients while keeping raw data local. There is a growing body of research on federated learning for ASR. These efforts explore personalization, communication-efficient optimization, and privacy-preserving adaptation.\n\nWe propose a noise-robust aggregation method tailored to heterogeneous audio conditions, demonstrating improved word error rates under realistic client variability and bandwidth constraints.",
    "reason": "Unsupported claim because it asserts broad prior work ('growing body of research') without citing any representative studies (definition d).",
    "start": 275,
    "end": 341,
    "label": "Unsupported claim"
  },
  {
    "span": "Demographic parity is the most commonly adopted criterion in deployed decision systems.",
    "document": "Related Work\n\nFairness in machine learning encompasses group metrics such as demographic parity and equalized odds, as well as individual notions of similarity-based treatment (Dwork et al., 2012; Hardt et al., 2016; Kusner et al., 2017). Practical mitigations include reweighting, adversarial debiasing, and post-processing calibration (Kamiran and Calders, 2012; Zhang et al., 2018; Pleiss et al., 2017).\n\nDemographic parity is the most commonly adopted criterion in deployed decision systems. However, its appropriateness depends on base rate differences and downstream utility, motivating context-specific metric selection.\n\nWe study metric selection under resource constraints, proposing a multi-objective framework that trades off accuracy, calibration, and fairness while providing actionable model cards for stakeholders.",
    "reason": "Makes a prevalence claim about deployment practices without any empirical citation or survey evidence (rule b).",
    "start": 408,
    "end": 495,
    "label": "Unsupported claim"
  },
  {
    "span": "(Brown et. al., 2019)",
    "document": "Related Work\n\nRobot learning from demonstration (LfD) benefits from combining trajectory priors with policy optimization to generalize across tasks (Argall et al., 2009; Chernova and Veloso, 2014). Recent work learns latent skill embeddings that enable fast task adaptation (Hausman et al., 2018; Pertsch et al., 2021). For contact-rich manipulation, hybrid models integrate learned policies with analytic controllers to maintain stability (Zhu et al., 2019). Prior studies (Brown et. al., 2019) explored meta-imitation to further reduce supervision. We extend this direction with uncertainty-calibrated reward relabeling to improve data efficiency in sparse-reward settings.\n",
    "reason": "Incorrect abbreviation formatting 'et. al.'; should be 'et al.' resulting in '(Brown et al., 2019)'.",
    "start": 474,
    "end": 495,
    "label": "Format"
  },
  {
    "span": "Transformer-XL has become the de facto baseline for character-level language modeling on PTB.",
    "document": "Introduction\n\nCharacter-level language modeling probes a model's ability to capture fine-grained orthographic and morphological patterns while handling long-range dependencies. Benchmarks such as PTB at the character level remain popular due to their compact size and historical adoption. Transformer-XL has become the de facto baseline for character-level language modeling on PTB. Nevertheless, differences in preprocessing, vocabulary normalization, and early stopping criteria complicate direct comparisons.\n\nRelated Work\n\nBeyond architectural changes, regularization strategies such as adaptive dropout and dynamic evaluation have been explored to maintain generalization under data scarcity. Recent studies also consider byte-level tokenization and multilingual corpora to reduce domain overfitting.\n\nContributions\n\nWe introduce a unified evaluation harness that standardizes preprocessing and reporting. Our experiments compare recurrence, memorization, and calibration effects across transformer variants under matched conditions.",
    "reason": "Asserts a community-wide baseline status for a model on a specific dataset without citation, triggering rule (b) and (d).",
    "start": 289,
    "end": 382,
    "label": "Unsupported claim"
  },
  {
    "span": "Most prior studies rely on proprietary datasets and undisclosed preprocessing pipelines.",
    "document": "Related Work\n\nDeep learning has substantially improved organ and lesion segmentation across modalities such as MRI and CT, with encoder–decoder architectures remaining the dominant design (Huang et al., 2020; Patel et al., 2021). Subsequent refinements introduced attention, multi-scale fusion, and topology-aware losses to address small-structure sensitivity (Gao et al., 2022; Lin and Chen, 2022).\n\nReproducibility, however, is a persistent challenge in medical image segmentation. Public benchmarks exist for select organs and pathologies, yet coverage is uneven across scanners and institutions (Rossi et al., 2021). Most prior studies rely on proprietary datasets and undisclosed preprocessing pipelines. This lack of standardization hinders meta-analysis and fair comparison of methods. Our work focuses on transparent preprocessing, fixed data splits, and rigorous reporting to facilitate replicability and comparison.",
    "reason": "It generalizes about the literature using 'Most prior studies' without citing any sources or surveys to substantiate the claim.",
    "start": 621,
    "end": 709,
    "label": "Unsupported claim"
  },
  {
    "span": "We adopt the U-Net architecture for medical image segmentation",
    "document": "Related Work\n\nMedical Image Segmentation\n\nMedical image segmentation has advanced through convolutional architectures that capture multiscale context. We adopt the U-Net architecture for medical image segmentation to model fine-grained boundaries while maintaining resolution through skip connections. Variants with attention gates, residual blocks, and dilated convolutions have been proposed to improve sensitivity to small structures. Semi-supervised and self-supervised pretraining further reduce annotation requirements.",
    "reason": "First mention of a specific prior architecture (U-Net) lacks a citation to the original work (rule a).",
    "start": 151,
    "end": 213,
    "label": "Unsupported claim"
  },
  {
    "span": "It is widely accepted that chrF++ correlates better with human judgments than BLEU on morphologically rich languages",
    "document": "Introduction\n\nAutomatic evaluation metrics are indispensable in machine translation (MT), but their correlation with human judgments can vary substantially across language pairs and domains (Mathur et al., 2020). While BLEU remains popular due to legacy and ease of computation, character-level and learned metrics have gained ground.\n\nIt is widely accepted that chrF++ correlates better with human judgments than BLEU on morphologically rich languages. However, results can be sensitive to tokenization, reference diversity, and domain mismatch. We re-examine these findings under controlled settings and propose a calibration method that reduces variance across datasets.",
    "reason": "This is a field-specific assertion about metric correlation that should be supported with citations to comparative studies (rule b).",
    "start": 336,
    "end": 452,
    "label": "Unsupported claim"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nOffline reinforcement learning aims to learn policies from static logs without additional environment interaction. Conservative objectives reduce overestimation of out-of-distribution actions (Reddy and Kaur, 2020), while uncertainty-aware penalties constrain the policy to the data support (Lin and Ortega, 2021). Model-based variants learn dynamics to enable value improvement with penalty terms (Gupta and Shah, 2022). Although empirical protocols have converged on author–year style citations (e.g., Kumar and Rao, 2019; Jensen et al., 2021), some results are reported using numeric references like [12], which complicates cross-paper comparison in this literature. We introduce a unified evaluation suite that normalizes datasets and metrics, enabling fair benchmarking of conservative and uncertainty-based methods.",
    "reason": "Wrong citation style: numeric bracket '[12]' appears in a context that otherwise uses author–year formatting.",
    "start": 617,
    "end": 621,
    "label": "Format"
  },
  {
    "span": "a previous study showed that contrastive learning improves sample efficiency in retinal OCT",
    "document": "Related Work\n\nSelf-Supervised Learning in Medical Imaging Self-supervised pretraining has demonstrated strong transfer to medical imaging tasks, particularly when labeled data are scarce (Zhuang et al., 2019; Azizi et al., 2021). Visual encoders trained with instance discrimination or masked image modeling often outperform supervised pretraining on downstream segmentation and classification (He et al., 2020; Chen et al., 2020). In ophthalmology, a previous study showed that contrastive learning improves sample efficiency in retinal OCT, motivating our choice of pretext tasks.\n\nDomain Adaptation and Limited Labels Methods that adapt models across institutions and devices reduce performance drops due to scanner shifts (Ganin et al., 2016; Tzeng et al., 2017). Mixed supervision using small expert-labeled sets and large pseudo-labeled pools further mitigates annotation costs (Bortsova et al., 2019). We combine self-supervised pretraining with selective fine-tuning to balance generalization and label efficiency.",
    "reason": "This is a claim about prior work and a specific finding without any citation to identify the study, which should be cited at first mention.",
    "start": 450,
    "end": 541,
    "label": "Unsupported claim"
  },
  {
    "span": "BERT has been used for biomedical named entity recognition with a CRF decoding layer.",
    "document": "Related Work\n\nBiomedical named entity recognition (BioNER) identifies spans such as diseases, chemicals, and genes in scientific text, often serving as a precursor to relation extraction and knowledge base construction (Smith et al., 2008). Early approaches relied on CRFs with hand-crafted lexical and orthographic features (Lafferty et al., 2001). With the advent of contextualized embeddings, pre-trained language models have substantially improved BioNER performance across multiple corpora (Peters et al., 2018; Lee et al., 2020).\n\nSpan-level modeling, soft lexicon integration, and boundary-aware objectives further refine precision at entity edges (Jie and Lu, 2019; Luo and Yang, 2020). BERT has been used for biomedical named entity recognition with a CRF decoding layer. Our work focuses on low-resource adaptation via parameter-efficient tuning and dictionary-constrained Viterbi decoding to reduce hallucinated entity boundaries.\n\nWe evaluate on BC5CDR, NCBI-Disease, and JNLPBA, reporting both span-level and entity-type calibration metrics.",
    "reason": "This sentence describes a specific prior modeling setup (BERT with a CRF layer for BioNER) without providing a citation; such mentions of specific prior configurations require references.",
    "start": 695,
    "end": 780,
    "label": "Unsupported claim"
  },
  {
    "span": "there are many recent works that explore multimodal sarcasm detection",
    "document": "Introduction\n\nSarcasm detection aims to identify utterances whose literal meaning diverges from the speaker's intent. Prior studies have shown that lexical cues alone are often insufficient, motivating models that incorporate visual and acoustic modalities alongside text. Despite progress, robust generalization across domains and platforms remains challenging, and there are many recent works that explore multimodal sarcasm detection. In this paper, we propose a compact fusion architecture that balances accuracy with deployability on resource-constrained devices.\n\nRelated Work\n\nEarly sarcasm detection relied on lexical and pragmatic features derived from sentiment incongruity. Neural architectures introduced contextualized representations and attention over user history. Multimodal approaches further integrate image regions and prosodic cues to capture non-literal signals. Our work differs by emphasizing low-latency fusion with uncertainty calibration for real-time applications.",
    "reason": "Mentions 'many recent works' without providing any citations to support the claim (violates guideline d; vague prior work reference lacks evidence).",
    "start": 367,
    "end": 436,
    "label": "Unsupported claim"
  },
  {
    "span": "Recent works demonstrate that prompt-tuning consistently outperforms full fine-tuning across a wide range of text classification benchmarks.",
    "document": "Introduction\n\nPre-trained language models have reshaped the landscape of text understanding by decoupling general knowledge acquisition from task-specific adaptation. Among adaptation strategies, prompt-based methods aim to align inputs with the model's pre-training objectives rather than modifying the model parameters extensively. Despite substantial interest in prompt engineering and lightweight adapters, the trade-offs between flexibility and stability remain underexplored. Recent works demonstrate that prompt-tuning consistently outperforms full fine-tuning across a wide range of text classification benchmarks. However, these claims have not been analyzed with respect to robustness under distribution shift or calibration in low-data regimes. In this paper, we revisit parameter-efficient adaptation and propose a hybrid prompting approach that interpolates between soft templates and learned instruction heads. We report results on sentiment analysis, topic categorization, and natural language inference, and we analyze sensitivity to label imbalance and prompt lexical variation.\n\nRelated Work\n\nParameter-efficient tuning has been explored through adapters, prefix tuning, and soft prompts, each emphasizing minimal parameter updates while retaining accuracy. Prior studies have compared adapter-based and prefix-based methods under controlled conditions, but the interaction of prompt design with class imbalance and calibration has received less attention.",
    "reason": "The phrase \"Recent works demonstrate...\" makes a claim about prior literature without providing any citations; mentions of recent works must be supported by references.",
    "start": 482,
    "end": 622,
    "label": "Unsupported claim"
  },
  {
    "span": "The 2019 shared task on hate speech detection standardized label definitions across platforms.",
    "document": "Introduction\n\nToxic language detection spans a spectrum from abusive expressions to incitement and threats, yet annotation practices vary across communities and platforms. Generalization remains limited when models trained on one platform are deployed on another with different norms and user demographics. The 2019 shared task on hate speech detection standardized label definitions across platforms. This purported standardization is frequently invoked to justify cross-domain evaluation but is rarely scrutinized for construct validity.\n\nRelated Work\n\nStudies have compared lexicon-based baselines with contextual encoders and contrastive pretraining, highlighting sensitivity to identity terms and dialectal variation. Data statements and rater training protocols have been proposed to mitigate bias. However, disagreement remains high, particularly for borderline cases involving sarcasm and reclaimed slurs.\n\nOur Approach\n\nWe assemble a multi-platform corpus with harmonized annotation guidelines and expert adjudication. We analyze disagreement patterns and propose an uncertainty-aware training regimen that explicitly models rater variance.",
    "reason": "Mentions a specific shared task and attributes standardization to it without providing a citation, violating rule (a).",
    "start": 307,
    "end": 401,
    "label": "Unsupported claim"
  },
  {
    "span": "Soft prompts have been optimized with gradient-based methods (Lester et al., 2021). Prompt selection can be framed as discrete search over templates (Shin et al., 2020). Chain-of-thought prompting improves multi-step reasoning (Wei et al., 2022b). Safety-oriented prompting reduces toxic generations (Dinan et al., 2022).",
    "document": "Related Work\n\nPrompting and instruction-following in large language models have been extensively studied to elicit capabilities from pre-trained parameters (Brown et al., 2020; Liu et al., 2021; Gao et al., 2021). Subsequent work explores how natural language instructions can unify multitask training and improve generalization (Sanh et al., 2022; Wei et al., 2022a). Beyond performance, researchers have examined calibration and faithfulness of prompted outputs (Kumar et al., 2019; Zhao et al., 2021).\n\nSoft prompts have been optimized with gradient-based methods (Lester et al., 2021). Prompt selection can be framed as discrete search over templates (Shin et al., 2020). Chain-of-thought prompting improves multi-step reasoning (Wei et al., 2022b). Safety-oriented prompting reduces toxic generations (Dinan et al., 2022).\n\nOur work investigates how structure-aware prompting affects compositional generalization, focusing on cross-task transfer under limited demonstrations.",
    "reason": "The sentences list four distinct lines of work with no transitions or explanation of how they relate to each other or to the preceding paragraph, making the connection between cited works abrupt and implicit.",
    "start": 506,
    "end": 827,
    "label": "Coherence"
  },
  {
    "span": "Recent works have shown that deeper GNNs do not necessarily improve performance due to oversmoothing.",
    "document": "Related Work\n\nGraph neural networks (GNNs) propagate neighborhood information through message passing, enabling representation learning on non-Euclidean structures (Kipf and Welling, 2017; Hamilton et al., 2017). While architectural innovations address heterophily and scalability (Chen et al., 2018; Bo et al., 2021), depth scaling remains a central challenge. Recent works have shown that deeper GNNs do not necessarily improve performance due to oversmoothing. Approaches to mitigate this include residual connections, normalization, and decoupled propagation (Li et al., 2019; Klicpera et al., 2019). We contribute a stability-regularized training objective that constrains the spectrum of the propagation operator to reduce smoothing-induced collapse.",
    "reason": "Uses the phrase 'Recent works have shown' to describe prior findings without citing the works, violating the requirement to support such claims with references.",
    "start": 362,
    "end": 463,
    "label": "Unsupported claim"
  },
  {
    "span": "The FLORES-200 benchmark is now the de facto standard for evaluating massively multilingual MT.",
    "document": "Introduction\n\nMassively multilingual machine translation (MMMT) aims to cover hundreds of languages with a single model (Aharoni et al., 2019; Fan et al., 2021). Evaluation across many low-resource languages remains challenging due to data quality and segmentation issues (Guzmán et al., 2019; Costa-jussà et al., 2022). The FLORES-200 benchmark is now the de facto standard for evaluating massively multilingual MT. Alternatives such as Tatoeba and JW300 offer broader coverage but vary in domain and sentence alignment quality (Artetxe and Schwenk, 2019; Agić and Vulić, 2019). In this work, we report results on a diverse set of public test suites.",
    "reason": "Asserts the status of a benchmark without providing a supporting citation.",
    "start": 321,
    "end": 416,
    "label": "Unsupported claim"
  },
  {
    "span": "Pratap et al. (2020) leveraged wav2vec 2.0 style pretraining for low-resource ASR. Park et al. (2019) used SpecAugment for robustness in speech recognition. Kahn et al. (2020) examined self-training for semi-supervised ASR. He et al. (2021) studied multilingual transfer for cross-lingual speech models.",
    "document": "Introduction\n\nLow-resource automatic speech recognition benefits from self-supervised pretraining, data augmentation, semi-supervision, and transfer across languages. However, integrating these techniques efficiently remains challenging.\n\nPratap et al. (2020) leveraged wav2vec 2.0 style pretraining for low-resource ASR. Park et al. (2019) used SpecAugment for robustness in speech recognition. Kahn et al. (2020) examined self-training for semi-supervised ASR. He et al. (2021) studied multilingual transfer for cross-lingual speech models.\n\nWe explore parameter-efficient adapters for cross-lingual adaptation.",
    "reason": "The span enumerates several approaches without connecting them or explaining their interplay. There are no transitions between sentences, and the relationships among pretraining, augmentation, self-training, and transfer are left implicit, yielding poor coherence across multiple sentences.",
    "start": 239,
    "end": 542,
    "label": "Coherence"
  },
  {
    "span": "Zhou et al. (2021) propose Informer with ProbSparse attention. Lim et al. (2021) develop TFT with variable selection networks. Salinas et al. (2020) model demand with DeepAR autoregressive RNNs. Wu et al. (2021) design Autoformer with decomposition layers.",
    "document": "Related Work\n\nLong-horizon time-series forecasting has driven innovations in attention mechanisms, temporal decomposition, and multi-horizon supervision. These methods aim to balance expressivity with computational tractability and robustness to nonstationarity.\n\nZhou et al. (2021) propose Informer with ProbSparse attention. Lim et al. (2021) develop TFT with variable selection networks. Salinas et al. (2020) model demand with DeepAR autoregressive RNNs. Wu et al. (2021) design Autoformer with decomposition layers.\n\nWe complement these approaches by introducing frequency-domain priors that stabilize training under regime shifts, improving calibration without increasing asymptotic complexity.",
    "reason": "The span sequentially lists four methods without transitions or explicit statements about their comparative goals or differences, making the connections abrupt and implicit. The coherence problem appears across multiple sentences (criterion a, b, c).",
    "start": 264,
    "end": 520,
    "label": "Coherence"
  },
  {
    "span": "The CMU-MOSI dataset has become the de facto benchmark for multimodal sentiment classification.",
    "document": "Introduction\n\nMultimodal Sentiment Analysis. Understanding opinion expressed through language, vision, and acoustics has received growing attention with the rise of video platforms. Foundational works study multimodal fusion via tensor factorization, late fusion, and attention (Zadeh et al., 2017; Tsai et al., 2019; Mai et al., 2020). The CMU-MOSI dataset has become the de facto benchmark for multimodal sentiment classification. Larger corpora and pretraining on non-parallel multimodal data have further improved generalization (Bagher Zadeh et al., 2018; Wu et al., 2021). Despite progress, models still struggle with cross-domain transfer and sarcasm (Ghosal et al., 2020). We revisit fusion with a modality-adaptive gating network that reduces over-reliance on text while preserving complementary cues from vision and audio.",
    "reason": "Mentions a specific dataset and asserts its benchmark status without citing the dataset or supporting evidence, which should be cited on first mention.",
    "start": 337,
    "end": 432,
    "label": "Unsupported claim"
  }
]