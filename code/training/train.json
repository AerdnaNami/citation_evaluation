[
  {
    "span": "Defects4J has been extended to include Python projects.",
    "document": "Related Work\n\nBenchmarking automated program repair (APR) requires standardized bug datasets with ground-truth patches. Defects4J has long served as the de facto benchmark for Java, enabling reproducible comparisons across APR techniques. Follow-up efforts broadened the scope of defects, patch types, and project diversity to better reflect real-world failures. Defects4J has been extended to include Python projects. Parallel lines of work introduce multilingual bug corpora and synthetic mutation operators to scale data availability. Our study focuses on cross-language generalization and examines how repair patterns learned in one language transfer to another.",
    "reason": "Claims a specific extension to a named dataset without providing a citation (rule a).",
    "start": 363,
    "end": 418,
    "label": "Unsupported_claim"
  },
  {
    "span": "Contrastive learning with instance discrimination and momentum encoders has become a standard recipe (He et al., 2020; Chen et al., 2020), while generative and masked-token objectives focus on reconstructive signals (Gidaris et al., 2018; Bao et al., 2021). Vision transformers have been adapted to both paradigms with competitive results (Dosovitskiy et al., 2021; Xie et al., 2021).",
    "document": "Related Work\n\nSelf-supervised learning (SSL) for visual representation has advanced rapidly, enabling strong performance with limited labels. Methods can be broadly categorized into contrastive, clustering, and reconstruction-based approaches, often differing in augmentation policies, architectural backbones, and pretraining compute.\n\nContrastive learning with instance discrimination and momentum encoders has become a standard recipe (He et al., 2020; Chen et al., 2020), while generative and masked-token objectives focus on reconstructive signals (Gidaris et al., 2018; Bao et al., 2021). Vision transformers have been adapted to both paradigms with competitive results (Dosovitskiy et al., 2021; Xie et al., 2021). Building on these insights, we train a compact ViT with a masked image modeling objective tailored to mobile inference constraints.\n\nWe evaluate downstream on classification, detection, and segmentation, reporting favorable accuracy–latency trade-offs compared to strong SSL and supervised baselines.\n",
    "reason": "The span lists categories and representative works without explaining how they motivate the proposed approach or what gap persists; the next sentence jumps to the contribution without explicit synthesis.",
    "start": 337,
    "end": 721,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Brown et al., 2020; Radford et al., 2019;.",
    "document": "Introduction\n\nLarge language models (LLMs) trained on web-scale corpora have demonstrated impressive few-shot and zero-shot capabilities. Autoregressive objectives combined with scaling strategies enable broad generalization without task-specific finetuning (Radford et al., 2019; Brown et al., 2020). However, deployment remains challenging due to latency, memory, and safety considerations (Bender et al., 2021; Weidinger et al., 2022). Recent work explores instruction tuning and preference optimization to better align models with human intent (Ouyang et al., 2022; Bai et al., 2022). Evidence from (Brown et al., 2020; Radford et al., 2019;. indicates that emergent abilities correlate with parameter count, motivating efficient scaling techniques.",
    "reason": "Improper punctuation inside the parenthetical: an extraneous semicolon before the period and no closing parenthesis.",
    "start": 603,
    "end": 646,
    "label": "Format"
  },
  {
    "span": "(Garcia et al. 2016)",
    "document": "Introduction\n\nTime-series anomaly detection under weak supervision is critical for forecasting reliability in real-world systems (Laptev et al., 2015; Blázquez-García et al., 2021). Prior work (Garcia et al. 2016) has explored shapelet-based features and scalable nearest-neighbor models, though these approaches struggle with non-stationarity and missing data. Self-supervised objectives such as forecasting and masked reconstruction have been proposed to learn robust representations (Eldele et al., 2021; Tonekaboni et al., 2021). We build on these ideas with a change-point aware contrastive loss that focuses on regime boundaries.\n\nOur empirical study spans industrial sensors and physiological signals with diverse sampling rates.",
    "reason": "Missing comma before the year in author–year citation: should be '(Garcia et al., 2016)'.",
    "start": 193,
    "end": 213,
    "label": "Format"
  },
  {
    "span": "Prior surveys agree that exposure disparity is the dominant fairness issue in music recommendation.",
    "document": "Related Work\n\nAlgorithmic fairness in recommender systems spans user-centric and provider-centric concerns, including disparate error rates, calibration, popularity bias, and exposure allocation (Burke, 2017; Ekstrand et al., 2018). Ranking fairness methods seek to balance utility with equitable exposure by optimizing position-based metrics under fairness constraints (Biega et al., 2018; Singh and Joachims, 2018).\n\nIn creative domains, recommendation can influence labor markets and cultural diversity, making provider-side fairness particularly salient for artists and labels (Mehrotra et al., 2018). Prior surveys agree that exposure disparity is the dominant fairness issue in music recommendation. Nonetheless, practitioner adoption remains limited due to unclear business trade-offs and the lack of counterfactual evaluation protocols.\n\nWe address this gap by proposing a counterfactual exposure auditing framework that estimates artist-level treatment effects under historical logging policies, enabling actionable diagnostics for production systems.",
    "reason": "This statement claims consensus among prior surveys without citing any specific surveys, making it an unsupported claim about the literature.",
    "start": 606,
    "end": 705,
    "label": "Unsupported_claim"
  },
  {
    "span": "and there are many recent works that explore this topic",
    "document": "Related Work\n\nMultilingual named entity recognition (mNER) seeks to identify entities across languages with limited annotated data. Cross-lingual transfer via multilingual encoders (Pires et al., 2019; Conneau et al., 2020) and annotation projection (Ni et al., 2017) are common strategies. Knowledge distillation transfers supervision from a teacher to a student to reduce latency and memory footprint (Hinton et al., 2015; Sanh et al., 2019), and there are many recent works that explore this topic. In addition, lexicon and gazetteer augmentation has been shown to improve recall, particularly for low-resource languages (Pan et al., 2017; Rahimi et al., 2019).\n\nOur work distills a multilingual teacher into a language-adaptive student that leverages character-level subword priors. We further introduce example-level temperature scaling based on entity uncertainty to avoid overconfident distillation targets.",
    "reason": "Uses the phrase 'many recent works' to claim activity in the area but provides no citations to those recent works.",
    "start": 445,
    "end": 500,
    "label": "Unsupported_claim"
  },
  {
    "span": "there are many recent works that leverage transformer decoders for code summarization",
    "document": "Introduction\n\nNeural code summarization aims to generate natural language descriptions for source code, assisting developers in program comprehension and maintenance. Early neural approaches modeled source code as sequences or structured representations, leveraging sequence-to-sequence architectures and abstract syntax trees (Iyer et al., 2016; LeClair and McMillan, 2019). More recently, pretraining over code and natural language has improved generalization, with models such as CodeBERT and GraphCodeBERT demonstrating strong performance on downstream code intelligence tasks (Ahmad et al., 2020; Feng et al., 2020). However, there are many recent works that leverage transformer decoders for code summarization. Despite these advances, most methods still struggle with out-of-project generalization and limited exposure to repository-level context.\n\nTo address these limitations, we propose a hierarchical retrieval-augmented decoder that conditions on cross-file dependencies and library usage. Our approach integrates project-aware retrieval with a lightweight adaptation of pretrained code encoders, and we evaluate on multiple public benchmarks under realistic cross-repository splits.",
    "reason": "Mentions 'recent works' and a methodological trend without any supporting citations; claims about prior work require references.",
    "start": 631,
    "end": 716,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prompting and few-shot learning enable large language models to generate code across common program synthesis benchmarks (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022).",
    "document": "Introduction\n\nProgram synthesis with large language models (LLMs) has advanced rapidly due to scale, datasets, and improved training objectives. Despite impressive performance, reliability and alignment with specifications remain active concerns.\n\nPrompting and few-shot learning enable large language models to generate code across common program synthesis benchmarks (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022). Augmentations such as test-time sampling, reranking, and constraint checking can further improve pass rates (Li et al., 2022; Le et al., 2022). Retrieval-augmented generation injects relevant snippets and API references to ground predictions in existing codebases (Borgeaud et al., 2021; Yao et al., 2022).\n\nVerification and repair pipelines combine static analysis, execution feedback, and search to refine initial generations (Lemieux et al., 2018; Chen et al., 2022). Nevertheless, the interplay between specification coverage and model calibration is not fully characterized, and benchmarks may not capture real-world edge cases.\n\nThis context frames ongoing efforts to improve correctness and robustness in code generation systems.",
    "reason": "The span states a trend with citations but does not relate it to the authors’ approach, nor does it specify which shortcomings or gaps their work targets.",
    "start": 248,
    "end": 432,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Exploration strategies include count-based bonuses, pseudo-counts for high-dimensional inputs, intrinsic curiosity modules, random network distillation, and information-gain objectives (Strehl and Littman, 2008; Bellemare et al., 2016; Pathak et al., 2017; Burda et al., 2019; Houthooft et al., 2016).",
    "document": "Introduction\n\nEffective exploration remains a core challenge in reinforcement learning (RL), especially in sparse-reward and deceptive environments. Agents must discover informative trajectories while controlling for noise, stochasticity, and function approximation errors. A rich body of work has proposed algorithmic incentives to steer behavior toward uncertain or novel states.\n\nExploration strategies include count-based bonuses, pseudo-counts for high-dimensional inputs, intrinsic curiosity modules, random network distillation, and information-gain objectives (Strehl and Littman, 2008; Bellemare et al., 2016; Pathak et al., 2017; Burda et al., 2019; Houthooft et al., 2016). Despite these advances, performance can vary widely across tasks and hyperparameters, complicating deployment.\n\nWe introduce E3B, an ensemble-based epistemic exploration bonus that estimates uncertainty via bootstrap disagreement under a shared feature backbone. E3B is simple to implement and robust to reward scaling.\n",
    "reason": "The span lists prior exploration methods without linking them to a specific shortcoming or to the motivation for the proposed method; it does not articulate the authors’ perspective.",
    "start": 383,
    "end": 684,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In a previous study, the authors claim that CTC models underperform attention-based models on noisy speech.",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) has evolved from CTC-based models to attention and transducer architectures that jointly learn acoustic and language representations (Graves et al., 2006; Chorowski et al., 2015; Watanabe et al., 2017). Data augmentation, multi-condition training, and robust feature extraction are commonly used to mitigate noise-induced degradation (Park et al., 2019).\n\nIn a previous study, the authors claim that CTC models underperform attention-based models on noisy speech. We revisit this assertion by controlling for encoder capacity, training data size, and decoding strategy across noise conditions.",
    "reason": "Mentions a 'previous study' and its claim without providing a citation (rule b/ii: prior work mention lacking citation).",
    "start": 416,
    "end": 523,
    "label": "Unsupported_claim"
  },
  {
    "span": "According to industry reports, more than 70% of customer queries can be answered with FAQ retrieval.",
    "document": "Introduction\n\nCustomer support systems increasingly combine retrieval and generation to deliver instant answers. While retrieval ensures factual consistency, generation can tailor responses to user context. Determining when to retrieve versus generate remains a key decision point, influenced by coverage of the knowledge base and ambiguity in user intent.\n\nAccording to industry reports, more than 70% of customer queries can be answered with FAQ retrieval. Motivated by this observation, we propose a hybrid controller that routes queries to retrieval or generation using uncertainty signals and entailment checks, thereby improving response latency and accuracy.\n\nWe evaluate on multi-domain logs and report performance across intent categories and novelty levels.",
    "reason": "Provides a specific statistic attributed to 'industry reports' without citing any source.",
    "start": 358,
    "end": 458,
    "label": "Unsupported_claim"
  },
  {
    "span": "Nguyen et al.",
    "document": "Related Work\n\nCross-lingual question answering benefits from shared subword vocabularies and alignment objectives across languages. Nguyen et al. demonstrate that joint training with multilingual readers and bilingual lexicon induction improves low-resource performance. Building on this, Artetxe et al. (2019) propose unsupervised machine translation to bootstrap distant pairs, while Conneau et al. (2020) introduce language-agnostic sentence embeddings that aid transfer. Nonetheless, performance varies widely with typological distance and script differences.",
    "reason": "Narrative citation missing year; should appear as 'Nguyen et al. (YEAR)'.",
    "start": 132,
    "end": 145,
    "label": "Format"
  },
  {
    "span": "EDA perturbs sentences via synonym replacement and word swaps to expand labeled data (Wei and Zou, 2019). Back-translation generates paraphrases by translating to and from pivot languages (Sennrich et al., 2016). Pre-trained language models improve downstream tasks with large-scale unsupervised objectives (Devlin et al., 2019). Mixup interpolates examples in input or embedding space to regularize models (Zhang et al., 2018).",
    "document": "Related Work\n\nData Augmentation for NLP\n\nAugmentation is a practical strategy to improve generalization, especially in low-resource regimes where labeled data is expensive. Methods range from lexical and syntactic perturbations to semantic-preserving paraphrases and model-based transformations. Recent work also explores task-agnostic augmentation policies that can be transferred across tasks.\n\nLexical, Semantic, and Model-based Approaches\n\nEDA perturbs sentences via synonym replacement and word swaps to expand labeled data (Wei and Zou, 2019). Back-translation generates paraphrases by translating to and from pivot languages (Sennrich et al., 2016). Pre-trained language models improve downstream tasks with large-scale unsupervised objectives (Devlin et al., 2019). Mixup interpolates examples in input or embedding space to regularize models (Zhang et al., 2018).\n\nPolicy Learning for Augmentation\n\nPolicy-search approaches optimize augmentation choices with reinforcement learning or Bayesian optimization, aiming to balance diversity with label consistency.\n\nOur Work\n\nWe propose a controllable augmentation framework that constrains semantic drift using entailment signals and calibrates label preservation with uncertainty-aware filtering.",
    "reason": "The span lists heterogeneous methods (EDA, back-translation, pretraining, mixup) without clarifying their relationships or categorical differences; transitions are missing and the relevance of pretraining to augmentation is not made explicit.",
    "start": 444,
    "end": 872,
    "label": "Coherence"
  },
  {
    "span": "Recent works have shown near-human performance on note de-identification, even in low-resource institutions.",
    "document": "Introduction\n\nElectronic health records contain rich clinical narratives that often include protected health information (PHI). Automated de-identification is a critical step for enabling secondary use of clinical text while preserving patient privacy. Early approaches relied on rules and dictionaries, later augmented by sequence labeling models such as CRFs and LSTMs. With the advent of large language models, transformer-based systems have rapidly improved extraction accuracy and robustness in diverse clinical settings. Recent works have shown near-human performance on note de-identification, even in low-resource institutions. Nevertheless, most prior evaluations are conducted on curated corpora with carefully balanced entity distributions, and generalization to real-world hospital notes remains an open question. In this paper, we investigate domain adaptation strategies and error calibration for de-identification models and analyze failure modes across institution-specific documentation patterns.\n",
    "reason": "The sentence asserts findings by 'recent works' and a performance level without citing any specific studies, violating the requirement to cite prior work at first mention (rule d and a).",
    "start": 527,
    "end": 635,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT-style encoders have been used in automated program repair systems to rank patch candidates generated by search-based methods.",
    "document": "Related Work\n\nAutomated program repair (APR) techniques span search-based patch synthesis, constraint solving, and learning-guided ranking. Neural approaches increasingly leverage code pretraining to capture syntax and semantics beyond token-level patterns. BERT-style encoders have been used in automated program repair systems to rank patch candidates generated by search-based methods. Other lines of work exploit execution traces and test coverage signals to refine candidate selection. Our method introduces contrastive defect-context pretraining that conditions on failure signatures for improved patch prioritization.",
    "reason": "States a specific prior application of BERT-style models in APR without citing any supporting papers; per rule (a) first mentions require citations.",
    "start": 258,
    "end": 388,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Li, 2019, Chen, 2020)",
    "document": "Related Work\n\nMeta-learning for adaptation. Gradient-based meta-learning accelerates adaptation to new domains and tasks by optimizing for rapid update dynamics (Finn et al., 2017). In NLP, episodic training has been applied to relation extraction, achieving consistent gains (Li, 2019, Chen, 2020). However, these improvements diminish under severe label shift, motivating distributionally robust objectives.\n",
    "reason": "Incorrect separator for multiple citations: uses a comma instead of a semicolon between distinct sources. Should be \"(Li, 2019; Chen, 2020)\".",
    "start": 276,
    "end": 298,
    "label": "Format"
  },
  {
    "span": "Mnih et al. (2015) popularize deep Q-learning for Atari. Bellemare et al. (2016) propose count-based exploration for large state spaces. Pathak et al. (2017) introduce curiosity-driven exploration. Badia et al. (2020) develop agent57 with exploratory mechanisms.",
    "document": "Related Work\n\nExploration in Deep Reinforcement Learning\n\nWe overview techniques encouraging agents to discover informative states. Mnih et al. (2015) popularize deep Q-learning for Atari. Bellemare et al. (2016) propose count-based exploration for large state spaces. Pathak et al. (2017) introduce curiosity-driven exploration. Badia et al. (2020) develop agent57 with exploratory mechanisms. Alternative approaches leverage posterior sampling and ensemble uncertainty (Osband et al., 2016; O'Donoghue et al., 2018).\n\nWe pursue scalable exploration bonuses that are stable under function approximation.",
    "reason": "Different exploration methods are enumerated without clarifying their connections or differences, causing abrupt shifts between approaches.",
    "start": 132,
    "end": 394,
    "label": "Coherence"
  },
  {
    "span": "Inverse propensity scoring corrects exposure bias in implicit feedback (Schnabel et al., 2016). Deconfounded recommenders treat exposures as latent confounders (Wang et al., 2020). Uplift modeling estimates individual treatment effects for interventions (Radcliffe and Surry, 2011). Bandit feedback enables counterfactual learning from logged data (Swaminathan and Joachims, 2015).",
    "document": "Related Work\n\nCausal inference has gained traction in recommender systems to address selection and exposure biases inherent in observational logs. Approaches adapt tools from counterfactual estimation, instrumental variables, and uplift modeling to improve out-of-distribution generalization.\n\nInverse propensity scoring corrects exposure bias in implicit feedback (Schnabel et al., 2016). Deconfounded recommenders treat exposures as latent confounders (Wang et al., 2020). Uplift modeling estimates individual treatment effects for interventions (Radcliffe and Surry, 2011). Bandit feedback enables counterfactual learning from logged data (Swaminathan and Joachims, 2015).\n\nMore recent studies integrate structural causal models for recommender evaluation (Liang et al., 2016) and exploit slate-level propensities for ranking (Joachims et al., 2017). Our work bridges uplift estimation with exposure modeling to produce intervention-aware rankings under limited exploration.",
    "reason": "The span enumerates four causal approaches without transitions or explanation of how they relate (e.g., IPS vs. deconfounding vs. uplift vs. bandits), resulting in abrupt, implied connections across sentences.",
    "start": 294,
    "end": 675,
    "label": "Coherence"
  },
  {
    "span": "Brown et al., (2021)",
    "document": "Related Work\n\nContextual prompting has emerged as a simple yet strong approach to adapt large language models to downstream tasks. As shown by Brown et al., (2021), in-context learning can match or exceed fine-tuning on select benchmarks when sufficient demonstrations are provided. Follow-up studies explored automatic prompt induction (Liu et al., 2022) and mixture-of-prompts routing to improve task generalization (Chen and Zhao, 2023).",
    "reason": "Incorrect narrative citation punctuation: should be “Brown et al. (2021)” without the comma before the parenthetical year.",
    "start": 143,
    "end": 163,
    "label": "Format"
  },
  {
    "span": "According to (Ng et al., 1999)",
    "document": "Related Work\n\nReward shaping is a well-studied technique for accelerating reinforcement learning by augmenting the reward function with auxiliary signals (Mataric, 1994; Randløv and Alstrøm, 1998). Potential-based shaping preserves the optimal policy under certain conditions. According to (Ng et al., 1999), shaping functions derived from potential differences guarantee policy invariance, which motivated a long line of work on value-based and model-based shaping (Grzes and Kudenko, 2008; Wiewiora, 2003).\n\nIn deep reinforcement learning, shaping has been applied to sparse-reward environments, curriculum learning, and hierarchical control (Mnih et al., 2015; Andrychowicz et al., 2017; Kulkarni et al., 2016). Nevertheless, designing shaping signals that generalize across tasks remains challenging. We study representation-aligned shaping that encourages state abstractions consistent with task structure, complementing prior work on potential-based methods and intrinsic motivation (Pathak et al., 2017; Burda et al., 2019).",
    "reason": "Wrong citation style: narrative use with 'According to' should place the year after the authors as Ng et al. (1999), not as a parenthetical after the preposition.",
    "start": 277,
    "end": 307,
    "label": "Format"
  },
  {
    "span": "Intrinsic motivation encourages novelty-seeking (Pathak et al., 2017). Optimism in the face of uncertainty provides theoretical guarantees (Jaksch et al., 2010). Curiosity can be misled by stochasticity (Burda et al., 2019). Model-based bonuses drive information gain (Houthooft et al., 2016).",
    "document": "Related Work\n\nExploration in reinforcement learning. Balancing exploration and exploitation is essential for sample-efficient control, with both theoretical and practical approaches advancing rapidly (Auer et al., 2002; Osband et al., 2016). Empirical methods often rely on intrinsic rewards, entropy regularization, or explicit uncertainty estimation (Bellemare et al., 2016; Pathak et al., 2017).\n\nIntrinsic motivation encourages novelty-seeking (Pathak et al., 2017). Optimism in the face of uncertainty provides theoretical guarantees (Jaksch et al., 2010). Curiosity can be misled by stochasticity (Burda et al., 2019). Model-based bonuses drive information gain (Houthooft et al., 2016). We reconcile these strands by introducing an optimism-regularized intrinsic reward that discounts stochastic distractions using epistemic uncertainty.",
    "reason": "The span moves between different exploration strategies with no connective tissue or explanation of their relationships. The lack of transitions makes the connections among the cited works unclear and the flow abrupt.",
    "start": 400,
    "end": 693,
    "label": "Coherence"
  },
  {
    "span": "Neural program synthesis maps specifications to code with sequence-to-sequence models (Devlin et al., 2017). Static analysis can verify safety properties of programs (Cousot and Cousot, 1977). Robust training mitigates reliance on spurious cues (Perez et al., 2019). Domain-specific languages constrain the search space (Polozov and Gulwani, 2015).",
    "document": "Related Work\n\nProgram Synthesis Paradigms. Program synthesis methods include enumerative search in a DSL, constraint-based solving, and learning-based approaches that condition on input-output examples or natural language descriptions (Gulwani, 2011; Solar-Lezama, 2008; Devlin et al., 2017). Recent advances combine neural priors with symbolic constraints to navigate large search spaces efficiently (Balog et al., 2017; Nye et al., 2021).\n\nLearning, Verification, and Robustness. Neural program synthesis maps specifications to code with sequence-to-sequence models (Devlin et al., 2017). Static analysis can verify safety properties of programs (Cousot and Cousot, 1977). Robust training mitigates reliance on spurious cues (Perez et al., 2019). Domain-specific languages constrain the search space (Polozov and Gulwani, 2015). In this work, we study verifier-guided decoding that integrates symbolic checks during generation to improve correctness without sacrificing diversity.",
    "reason": "The sentences jump across neural synthesis, static analysis, robustness, and DSL design without articulating how these facets interact, lacking transitions to tie the cited works into a coherent narrative.",
    "start": 482,
    "end": 830,
    "label": "Coherence"
  },
  {
    "span": "(Li et al., 2016;; Zhang and Wang, 2017; Chen, 2018)",
    "document": "Related Work\n\nNeural machine translation (NMT) has progressed rapidly with attention-based encoder–decoder models (Bahdanau et al., 2015; Vaswani et al., 2017). While subword segmentation alleviates out-of-vocabulary issues (Sennrich et al., 2016), rare morphology and low-resource settings remain difficult (Fadaee et al., 2017).\n\nTransfer learning and multilingual pre-training improve cross-lingual transfer (Aharoni et al., 2019; Conneau and Lample, 2019), and back-translation continues to be a strong data augmentation method (Sennrich et al., 2016). However, domain mismatch degrades translation quality (Chu and Wang, 2018), necessitating adaptation strategies (Li et al., 2016;; Zhang and Wang, 2017; Chen, 2018) and uncertainty-aware decoding (Stahlberg and Byrne, 2019).\n\nWe propose a simple yet effective domain-consistent fine-tuning scheme that regularizes encoder representations and yields robust gains on out-of-domain test sets.",
    "reason": "There is a duplicate semicolon in a multi-citation; it should be a single semicolon between citations.",
    "start": 669,
    "end": 721,
    "label": "Format"
  },
  {
    "span": "the Natural Questions corpus",
    "document": "Introduction\n\nMachine reading comprehension has advanced rapidly with large-scale datasets that pair passages with questions and answers. Early benchmarks such as SQuAD (Rajpurkar et al., 2016) enabled span-based extraction from short paragraphs, while RACE (Lai et al., 2017) emphasized multi-sentence reasoning over exam-style passages. More recently, real-world information-seeking behavior has been studied using search logs and long-form documents, as in the Natural Questions corpus, which introduces real-user queries and long documents. Despite these improvements, many models still overfit to annotation artifacts and fail to generalize to out-of-domain settings.",
    "reason": "Mentions a specific dataset without providing a citation to the resource describing it.",
    "start": 460,
    "end": 488,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Chen et al., 2018",
    "document": "Introduction\n\nDialogue response selection has progressed from heuristic retrieval to neural bi-encoder and cross-encoder architectures (Henderson et al., 2019; Humeau et al., 2020). Early neural ranking methods (Chen et al., 2018 proposed kernel-based interactions but suffered from limited context windows.\n\nSubsequent work incorporated hierarchical encoders and dense passage retrieval to improve recall in multi-turn settings (Karpukhin et al., 2020; Xiong et al., 2021). Our work builds on these advances by introducing a conversation-level cache that preserves discourse entities across turns.",
    "reason": "Missing closing parenthesis in the citation; should be '(Chen et al., 2018)'.",
    "start": 211,
    "end": 229,
    "label": "Format"
  },
  {
    "span": "For low-resource machine translation, back-translation (Sennrich et al., 2016), unsupervised learning (Lample et al., 2018; Artetxe et al., 2018), multilingual transfer (Johnson et al., 2017; Neubig and Hu, 2018), and data augmentation via noising or paraphrasing (Fadaee et al., 2017; Vaibhav et al., 2019) have been widely explored.",
    "document": "Introduction\n\nBuilding machine translation systems for low-resource languages remains challenging due to limited parallel corpora, domain mismatch, and morphological complexity. Methods that can leverage monolingual data and cross-lingual transfer are particularly promising.\n\nFor low-resource machine translation, back-translation (Sennrich et al., 2016), unsupervised learning (Lample et al., 2018; Artetxe et al., 2018), multilingual transfer (Johnson et al., 2017; Neubig and Hu, 2018), and data augmentation via noising or paraphrasing (Fadaee et al., 2017; Vaibhav et al., 2019) have been widely explored.\n\nIn our study, we consider dialectal variants with sparse parallel data and evaluate across domain shifts. We later introduce a lexicon-constrained pre-training objective that leverages mined bilingual lexemes.\n",
    "reason": "Violates (a) and (c): the sentence lists approaches without connecting them to the authors' stance, gap, or motivation, offering no synthesis with the current work.",
    "start": 277,
    "end": 611,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(RoBERTa Liu et al. (2019))",
    "document": "Related Work\n\nPretrained language models have transformed NLP benchmarks (Peters et al., 2018; Devlin et al., 2019). Large-scale masked language modeling yields strong representations for downstream tasks (Liu et al., 2019; Clark et al., 2020).\n\nFor domain adaptation, continued pretraining on in-domain corpora is effective (Gururangan et al., 2020). Several works report gains with RoBERTa (RoBERTa Liu et al. (2019)) on specialized datasets, yet the trade-offs with vocabulary updates remain unclear.\n\nWe examine vocabulary freezing versus re-tokenization for biomedical and legal corpora, analyzing effects on rare-term coverage.",
    "reason": "Nested/compound parentheses and model name included inside a parenthetical author–year citation; should be “RoBERTa (Liu et al., 2019)” or simply “(Liu et al., 2019)”.",
    "start": 392,
    "end": 419,
    "label": "Format"
  },
  {
    "span": "Knowledge graph completion has been extensively studied with translational embeddings (Bordes et al., 2013; Lin et al., 2015), semantic matching models (Yang et al., 2015; Trouillon et al., 2016), and graph neural approaches that propagate neighborhood signals (Schlichtkrull et al., 2018; Vashishth et al., 2020).",
    "document": "Related Work\n\nKnowledge graph completion. Completing missing facts in knowledge graphs is a long-standing problem. Knowledge graph completion has been extensively studied with translational embeddings (Bordes et al., 2013; Lin et al., 2015), semantic matching models (Yang et al., 2015; Trouillon et al., 2016), and graph neural approaches that propagate neighborhood signals (Schlichtkrull et al., 2018; Vashishth et al., 2020). Recent methods incorporate textual descriptions and pre-trained language models to better capture semantics (Yao et al., 2019; Wang et al., 2021).\n\nInductive and few-shot settings. Inductive link prediction addresses entities unseen at training time using subgraph encoders and meta-learning strategies (Teru et al., 2020; Zhang and Yao, 2022). Few-shot variants rely on relation prototypes and adaptation across tasks (Xiong et al., 2018).\n\nTraining recipes. Loss functions, negative sampling schemes, and regularization critically affect performance and efficiency across benchmarks such as FB15k-237 and WN18RR (Ruffinelli et al., 2020).\n",
    "reason": "The span catalogs categories of KGC methods without articulating the shortcomings that motivate the current work or how it differentiates from those models (criteria a and b).",
    "start": 115,
    "end": 429,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Rao, 2015;;Chung et al.,2018)",
    "document": "Related Work\n\nAcoustic word embeddings map variable-length speech segments into fixed-dimensional vectors. Early dynamic time warping baselines were replaced by discriminative Siamese networks (Synnaeve et al., 2014). Subsequent studies investigated proxy classification losses (Rao, 2015;;Chung et al.,2018) to learn class-independent distances. Our method adapts these objectives to semi-supervised settings with consistency regularization.",
    "reason": "Formatting errors in a parenthetical list: double semicolon and missing space after the comma before the year. Should read “(Rao, 2015; Chung et al., 2018)”.",
    "start": 278,
    "end": 308,
    "label": "Format"
  },
  {
    "span": "(Lee et al., 2019; Kim and Park, 2020,)",
    "document": "Introduction\n\nLarge-scale vision datasets have driven progress in recognition and detection, but long-tailed distributions challenge standard empirical risk minimization (Krizhevsky et al., 2012; He et al., 2016). Rebalancing strategies, including class-aware sampling and deferred reweighting, improve minority-class performance (Cui et al., 2019; Kang et al., 2020). Open-set recognition further complicates evaluation by introducing out-of-distribution samples at test time (Scheirer et al., 2013). Recent benchmarks standardize splits for long-tailed and open-set settings (Lee et al., 2019; Kim and Park, 2020,) yet reporting practices vary across works, obscuring fair comparisons. We propose a unified protocol with calibrated metrics and release code to reproduce all baselines.\n\nRelated Work\n\nUncertainty estimation with deep ensembles and energy-based scores has shown promise for detecting unknowns (Lakshminarayanan et al., 2017; Liu et al., 2020).",
    "reason": "Extraneous comma before the closing parenthesis in a multiple-citation parenthetical. Should be \"(Lee et al., 2019; Kim and Park, 2020)\".",
    "start": 577,
    "end": 616,
    "label": "Format"
  },
  {
    "span": "Recent time-series anomaly detection approaches include autoencoders, variational autoencoders, generative adversarial networks, forecasting-based residual models, and seasonal-trend decomposition heuristics (Li et al., 2018; Audibert et al., 2020; Malhotra et al., 2016; Xu et al., 2021).",
    "document": "Introduction\n\nDetecting anomalies in multivariate sensor streams is crucial for industrial monitoring, where false alarms and missed detections carry significant costs.\n\nRecent time-series anomaly detection approaches include autoencoders, variational autoencoders, generative adversarial networks, forecasting-based residual models, and seasonal-trend decomposition heuristics (Li et al., 2018; Audibert et al., 2020; Malhotra et al., 2016; Xu et al., 2021). These methods often benchmark on synthetic or semi-synthetic datasets with limited coverage of real-world noise patterns.\n\nWe propose STAG, a structure-aware temporal graph model that encodes cross-sensor dependencies via dynamic edges and calibrates residual scores through conformal prediction. We evaluate STAG on three industrial datasets with annotated maintenance logs.",
    "reason": "The span enumerates prior techniques without clarifying how they compare to or motivate the proposed structure-aware model.",
    "start": 170,
    "end": 459,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(2019, Zhang et al.)",
    "document": "Related Work\n\nGraph neural networks (GNNs) generalize convolution to non-Euclidean domains by aggregating information over neighborhoods (Kipf and Welling, 2017; Hamilton et al., 2017). Variants address oversmoothing and scalability via architectural and training changes (Xu et al., 2019; Chen et al., 2018). Attention-based message passing enables adaptive weighting across edges (Velickovic et al., 2018). Recent work explores graph transformers and positional encodings for long-range dependencies (Dwivedi et al., 2021; Ying et al., 2021).\n\nContrastive learning on graphs improves representations without labels (You et al., 2020; Hassani and Khasahmadi, 2020), and robustness is studied under structure perturbations (Zügner et al., 2018). Notably, improvements in training stability were reported in (2019, Zhang et al.), which we build upon with a scalable normalization layer.",
    "reason": "Wrong citation ordering inside parentheses; should follow author-year format (e.g., \"(Zhang et al., 2019)\"), not year-first.",
    "start": 807,
    "end": 827,
    "label": "Format"
  },
  {
    "span": "(Chen et al., 2017; Yang et al., 2018,",
    "document": "Related Work\n\nOpen-domain question answering builds pipelines that retrieve and read evidence from large corpora. Early neural readers were trained over fixed passages (Chen et al., 2016), while later systems jointly optimized retrieval and reading (Karpukhin et al., 2020; Guu et al., 2020). A line of work investigates dense retrievers coupled with generative readers (Lewis et al., 2020). Despite progress, training remains data-hungry, and domain adaptation is brittle (Lin et al., 2019). Prior survey papers (Chen et al., 2017; Yang et al., 2018, discuss transferable QA skills across tasks and domains, but leave open questions about calibration and justification generation.",
    "reason": "Missing closing parenthesis and trailing comma inside the parenthetical citation list; should be '(Chen et al., 2017; Yang et al., 2018)'.",
    "start": 513,
    "end": 551,
    "label": "Format"
  },
  {
    "span": "Continual reinforcement learning has been studied through regularization-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017), replay buffers (Rolnick et al., 2019; Chaudhry et al., 2019), parameter isolation (Rusu et al., 2016), meta-learning (Finn et al., 2017), and curriculum generation (Narvekar et al., 2020). Benchmarks include Procgen, Atari, and DMControl suites.",
    "document": "Related Work\n\nAgents deployed in the real world must learn across tasks without catastrophic forgetting while leveraging prior skills. Continual RL poses this challenge in non-stationary environments with evolving reward functions and dynamics. We consider an online setting with bounded memory and limited interaction budgets.\n\nContinual reinforcement learning has been studied through regularization-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017), replay buffers (Rolnick et al., 2019; Chaudhry et al., 2019), parameter isolation (Rusu et al., 2016), meta-learning (Finn et al., 2017), and curriculum generation (Narvekar et al., 2020). Benchmarks include Procgen, Atari, and DMControl suites.\n\nWe develop a prioritized episodic memory with policy-aware sampling and an uncertainty-gated consolidation rule that scales to long task sequences.",
    "reason": "The span lists method categories and benchmarks without analyzing their limitations or linking them to the paper’s constraints (bounded memory, online setting), hence lacking synthesis per (a) and (c).",
    "start": 329,
    "end": 709,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Zhou et al. (2018) constructed knowledge-grounded conversation datasets with annotated facts. Dinan et al. (2019) introduced Wizard of Wikipedia for knowledge selection and generation. Lewis et al. (2020) explored retrieval-augmented generation for open-domain tasks. Shuster et al. (2021) developed BlenderBot with retrieval-augmented modules.",
    "document": "Related Work\n\nKnowledge-grounded dialogue systems aim to produce responses that are both fluent and supported by external evidence. A common pipeline retrieves relevant knowledge and conditions a generator on the retrieved content, balancing factuality and coherence.\n\nZhou et al. (2018) constructed knowledge-grounded conversation datasets with annotated facts. Dinan et al. (2019) introduced Wizard of Wikipedia for knowledge selection and generation. Lewis et al. (2020) explored retrieval-augmented generation for open-domain tasks. Shuster et al. (2021) developed BlenderBot with retrieval-augmented modules.\n\nWe propose adaptive retrieval with uncertainty-aware gating to mitigate retrieval noise and reduce hallucinations in multi-turn settings.",
    "reason": "The cited works appear in a sequence with no connective phrases or explicit comparisons, making their relationships and relevance to each other unclear (issues a and b).",
    "start": 269,
    "end": 613,
    "label": "Coherence"
  },
  {
    "span": "To reduce computation on edge devices, prior work explores pruning (Han et al., 2016; He et al., 2018), quantization (Jacob et al., 2018; Nagel et al., 2020), neural architecture search (Tan et al., 2019; Cai et al., 2020), and operator-level optimizations (Vasilache et al., 2018; Chen et al., 2018). We propose a mixed-precision method.",
    "document": "Related Work\n\nDeploying deep networks on edge hardware requires meeting strict latency and power budgets while maintaining acceptable accuracy. Methods target both algorithmic and systems-level improvements.\n\nTo reduce computation on edge devices, prior work explores pruning (Han et al., 2016; He et al., 2018), quantization (Jacob et al., 2018; Nagel et al., 2020), neural architecture search (Tan et al., 2019; Cai et al., 2020), and operator-level optimizations (Vasilache et al., 2018; Chen et al., 2018). We propose a mixed-precision method.\n\nWe later detail our calibration protocol for assigning bit widths and compare against uniform quantization under realistic memory constraints on microcontrollers.\n",
    "reason": "Violates (b): presents the authors' contribution immediately after listing prior work without explaining the shortcoming in existing approaches that the method addresses.",
    "start": 209,
    "end": 547,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Lopez et al., 2018)",
    "document": "Related Work\n\nActive learning aims to reduce annotation costs by selecting informative examples (Settles, 2009). In (Lopez et al., 2018) we replicate uncertainty sampling baselines and show that committee-based strategies can be unstable under class imbalance. Subsequent studies propose diversity-aware selection (Gao and Chen, 2020) and gradient-based utilities (Riley et al., 2021), reporting improvements on low-resource intent classification. Our study complements these efforts by combining uncertainty and consistency signals for acquisition while controlling for training-time confounds.",
    "reason": "Wrong citation style with a leading preposition; should be narrative form \"In Lopez et al. (2018), we...\" or rephrased to avoid parentheses after \"In\".",
    "start": 116,
    "end": 136,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on essays from multiple prompts",
    "document": "Related Work\n\nAutomated Essay Scoring (AES) evaluates writing quality by predicting holistic or trait-specific scores. Traditional approaches rely on engineered lexical, syntactic, and discourse features, while neural methods leverage pretrained language models to capture richer semantics.\n\nBERT was used in an AES task trained on essays from multiple prompts, with subsequent work exploring domain adaptation across prompts and robustness to adversarially perturbed inputs. Cross-prompt generalization remains challenging due to prompt-specific content cues and annotation inconsistencies.\n\nOur study examines content-style disentanglement via contrastive learning, aiming to preserve rubric-aligned style indicators while reducing overreliance on prompt-specific content. We report gains on cross-prompt transfers and evaluate calibration under distribution shifts.",
    "reason": "This is a specific claim about prior work using BERT for AES without providing a citation, violating rule a and example iii.",
    "start": 292,
    "end": 360,
    "label": "Unsupported_claim"
  },
  {
    "span": "The widely used MIMIC-III clinical notes contain 58% negated findings.",
    "document": "Introduction\n\nAutomated detection of clinical events from electronic health records is complicated by domain-specific language, negation, and temporality. Robust handling of negation is essential for downstream tasks such as cohort selection and pharmacovigilance.\n\nThe widely used MIMIC-III clinical notes contain 58% negated findings. This prevalence motivates architectures that explicitly model scope and cue uncertainty rather than relying solely on token-level heuristics. We propose a structured prediction approach that integrates cue detection with span-level scope inference and test it on multiple hospital domains.\n\nWe further analyze model calibration and provide guidelines for transferring negation models across institutions with minimal annotation effort.",
    "reason": "Presents a specific statistic about a niche clinical dataset without any citation or supporting evidence (violates b and e).",
    "start": 266,
    "end": 336,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Johnson and Ruiz, 2017)",
    "document": "Related Work\n\nBayesian optimization for hyperparameter tuning has been adapted to deep architectures via surrogate models and acquisition functions (Snoek et al., 2012). Multi-fidelity extensions reduce evaluation cost by leveraging cheaper proxies such as lower epochs or reduced data (Kandasamy et al., 2017). In transfer settings, prior knowledge can shape priors over functions (Johnson and Ruiz, 2017), enabling faster convergence across related tasks.",
    "reason": "Within-parentheses citation should use '&' instead of 'and' in APA-style author–date: '(Johnson & Ruiz, 2017)'.",
    "start": 382,
    "end": 406,
    "label": "Format"
  },
  {
    "span": "Prior work has shown that using graph convolutions improves few-shot classification.",
    "document": "Related Work\n\nFew-shot learning aims to generalize from limited labeled data by leveraging inductive biases and auxiliary structure. Metric-based approaches compare query instances to prototype representations, while optimization-based methods adapt model parameters to new tasks with minimal updates.\n\nPrior work has shown that using graph convolutions improves few-shot classification.\n\nMore recent approaches combine relational reasoning with contrastive objectives, often integrating transductive inference to exploit the structure among queries. Despite these advances, performance remains sensitive to the choice of support examples and class imbalance.",
    "reason": "Asserts findings from prior work without citing any specific papers to support the claim (rule b).",
    "start": 303,
    "end": 387,
    "label": "Unsupported_claim"
  },
  {
    "span": "Wang et al. 1",
    "document": "Introduction\n\nNeural machine translation (NMT) has benefited from advances in attention mechanisms, larger corpora, and multilingual pretraining. Despite these improvements, low-resource languages continue to lag due to data scarcity and domain mismatch. Wang et al. 1 propose leveraging a shared subword vocabulary and adapter modules to facilitate transfer from high-resource to low-resource pairs, reporting gains on several families. Complementary approaches incorporate back-translation and dual learning to exploit monolingual data when parallel corpora are limited (Sennrich et al., 2016; He et al., 2016).",
    "reason": "Improper footnote-style marker without a corresponding footnote and missing year; it should be a proper citation, e.g., “Wang et al. (2020)”, or use a correctly formatted footnote.",
    "start": 255,
    "end": 268,
    "label": "Format"
  },
  {
    "span": "[Smith et al., 2015]",
    "document": "Introduction\n\nScaling object detectors has historically relied on stronger backbones and better normalization (He et al., 2016; Woo et al., 2018). Prior work [Smith et al., 2015] suggests that multi-scale features are essential for detecting small objects, leading to feature pyramid networks and their variants (Lin et al., 2017; Ghiasi et al., 2019). Recent transformers integrate multi-scale attention to capture long-range dependencies (Carion et al., 2020; Zhu et al., 2021). We revisit the role of scale priors by introducing a lightweight octave aggregation module that improves performance with minimal overhead.\n\nExperiments on COCO and aerial imagery datasets demonstrate consistent gains.",
    "reason": "Wrong citation style: author–year citation formatted with square brackets as if numeric; should be a parenthetical citation '(Smith et al., 2015)'.",
    "start": 158,
    "end": 178,
    "label": "Format"
  },
  {
    "span": "See et al.",
    "document": "Introduction\n\nAbstractive text summarization aims to generate concise summaries that capture the salient content of long documents. Neural sequence-to-sequence models have become the dominant approach due to their ability to learn powerful conditional language models from large corpora (Nallapati et al., 2016; Paulus et al., 2018). Pointer-generator networks, first popularized by See et al., remain widely used for news summarization because they can copy rare tokens from the source while still generating fluent abstractions. Later extensions improved factual consistency and coverage (Chen and Liu, 2020; Zhao et al., 2022), while reinforcement learning objectives further aligned model outputs with human-written summaries (Narayan et al., 2018).\n\nDespite these advances, controlling the content selection process and mitigating hallucinations remain key challenges. Recent work investigates constrained decoding and evidence grounding (Cao et al., 2018; Dong et al., 2021), showing that explicit content planning can reduce unsupported statements. In this paper, we revisit controllable summarization and propose a planning module that balances extractive anchors with abstractive rephrasing.",
    "reason": "Narrative citation missing year; APA-style narrative citations should include the year, e.g., 'See et al. (2017)'.",
    "start": 383,
    "end": 393,
    "label": "Format"
  },
  {
    "span": "Regularization-based methods penalize changes to important parameters (Kirkpatrick et al., 2017; Zenke et al., 2017). Rehearsal strategies replay exemplars or synthesize data (Lopez-Paz and Ranzato, 2017; Shin et al., 2017). Parameter-isolation allocates task-specific capacity (Rusu et al., 2016; Yoon et al., 2018). We build on these ideas to design our approach.",
    "document": "Related Work\n\nContinual learning seeks models that acquire new skills without catastrophically forgetting prior knowledge. Approaches can be grouped by the mechanism used to protect old tasks while learning new ones.\n\nRegularization-based methods penalize changes to important parameters (Kirkpatrick et al., 2017; Zenke et al., 2017). Rehearsal strategies replay exemplars or synthesize data (Lopez-Paz and Ranzato, 2017; Shin et al., 2017). Parameter-isolation allocates task-specific capacity (Rusu et al., 2016; Yoon et al., 2018). We build on these ideas to design our approach.\n\nRecent benchmarks emphasize realistic, non-stationary data streams and compute constraints. We evaluate under these conditions and report standard forgetting and plasticity metrics.",
    "reason": "The span catalogs three families of methods and then states the authors build on them, but it does not articulate the specific deficiency being addressed or how their method synthesizes or departs from prior techniques.",
    "start": 218,
    "end": 583,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Levy et al.",
    "document": "Introduction\n\nFew-shot learning aims to enable models to generalize from only a handful of labeled examples by leveraging prior experience across tasks (Finn et al., 2017; Rusu et al., 2019). Despite strong performance on benchmarks, many approaches still overfit to meta-training distributions and fail to adapt robustly in the presence of domain shift (Triantafillou et al., 2019; Dhillon et al., 2020). As shown by Levy et al., meta-learning with task-conditioned regularization can mitigate such overfitting by aligning task priors during adaptation, yet the sensitivity to label noise remains poorly understood. Our work revisits this question with a focus on calibration and uncertainty estimation in low-data regimes.\n\nRelated work spans metric-based methods that learn embedding spaces (Snell et al., 2017; Sung et al., 2018), gradient-based adaptation (Finn et al., 2017; Nichol et al., 2018), and Bayesian formulations that capture task uncertainty (Gordon et al., 2019). We complement this body by analyzing how uncertainty-aware objectives transfer across tasks with varying support-set sizes and noise profiles.",
    "reason": "Narrative citation missing the publication year; in APA-style narrative form it should appear as 'Levy et al. (YEAR)'.",
    "start": 418,
    "end": 429,
    "label": "Format"
  },
  {
    "span": "As noted in a previous study, users tend to overfit to interface cues.",
    "document": "Introduction\n\nInteractive machine learning systems rely on human feedback to guide model updates. While such systems can quickly adapt to new tasks, they also introduce behavioral biases that skew both model and user behavior.\n\nAs noted in a previous study, users tend to overfit to interface cues.\n\nUnderstanding these dynamics is essential for designing interfaces that elicit informative feedback rather than encourage shortcut learning. We therefore investigate how visual affordances and prompt phrasing influence labeling quality under time pressure.",
    "reason": "Mentions a 'previous study' and its finding without identifying or citing the study (rule b).",
    "start": 228,
    "end": 298,
    "label": "Unsupported_claim"
  },
  {
    "span": "Graph-based recommenders leverage message passing architectures such as GraphSAGE (Hamilton et al., 2017), GAT (Velickovic et al., 2018), NGCF (Wang et al., 2019), and LightGCN (He et al., 2020) to propagate collaborative signals over user–item graphs. Subsequent work incorporates self-supervised objectives and contrastive learning to improve robustness and sparsity handling (Sun et al., 2020; Xia et al., 2021; Yu et al., 2022).",
    "document": "Related Work\n\nRecommender systems increasingly adopt graph-structured representations to capture higher-order interactions beyond co-occurrence signals. By modeling users and items as nodes and interactions as edges, graph learning methods can exploit neighborhood structure and long-range dependencies that are difficult to capture with shallow matrix factorization.\n\nGraph-based recommenders leverage message passing architectures such as GraphSAGE (Hamilton et al., 2017), GAT (Velickovic et al., 2018), NGCF (Wang et al., 2019), and LightGCN (He et al., 2020) to propagate collaborative signals over user–item graphs. Subsequent work incorporates self-supervised objectives and contrastive learning to improve robustness and sparsity handling (Sun et al., 2020; Xia et al., 2021; Yu et al., 2022).\n\nBeyond the core encoders, line of work explores scalable sampling, negative mining, and debiasing to counter popularity skew (Zheng et al., 2021; Wei et al., 2021). Temporal dynamics are modeled with sequence-aware graph architectures that account for evolving preferences (Wu et al., 2019; Qiu et al., 2020).\n\nOur study focuses on learning under severe interaction sparsity and cold-start with minimal side information. We propose a structure-regularized contrastive objective that explicitly discourages shortcut popularity paths while preserving neighborhood agreement, yielding consistent gains on sparse public and proprietary datasets.",
    "reason": "The span summarizes and lists prior GNN recommender approaches and enhancements, but it does not connect these works to the authors' problem setting, gap, or contribution, nor does it state a perspective. It lacks synthesis per (a) and (c).",
    "start": 369,
    "end": 801,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Zhang and Liu, 2017)",
    "document": "Introduction\n\nPretrained language models have transformed text classification by enabling effective fine-tuning with limited labels (Howard and Ruder, 2018; Devlin et al., 2019). Domain shift, however, degrades performance when the training and test distributions diverge (Ben-David et al., 2010; Blitzer et al., 2006). Unsupervised domain adaptation reduces this gap via representation alignment (Ganin and Lempitsky, 2015; Tzeng et al., 2017).\n\nIn (Zhang and Liu, 2017) the authors survey sentiment adaptation methods and highlight pivot-based techniques. Subsequent work adapts these ideas to contextual embeddings using virtual adversarial training and consistency regularization (Miyato et al., 2017; Xie et al., 2020). We build on these advances with a contrastive objective that aligns class-conditional features across domains without target labels.\n\nOur contributions include a theoretical bound on target risk under class-conditional alignment and an empirical study across reviews, social media, and news.",
    "reason": "Incorrect citation style: the capitalized preposition and parentheses around the citation should be narrative style, e.g., “In Zhang and Liu (2017) …”.",
    "start": 447,
    "end": 471,
    "label": "Format"
  },
  {
    "span": "(O'Neil et al., 2020))",
    "document": "Introduction\n\nReinforcement learning (RL) has enabled robots to acquire complex skills from interaction (Levine et al., 2016; Kalashnikov et al., 2018). However, sample inefficiency and sim-to-real gaps remain pressing issues. Recent work shows that offline datasets can bootstrap policy learning, as shown in (O'Neil et al., 2020)) with large robot logs. We propose a hybrid offline-to-online protocol that exploits uncertainty estimates to guide exploration.\n\nWe evaluate on manipulation benchmarks and real-robot rollouts, comparing against strong offline RL baselines (Kumar et al., 2020; Kostrikov et al., 2021).",
    "reason": "Extraneous closing parenthesis resulting in “))”; should have a single closing parenthesis.",
    "start": 310,
    "end": 332,
    "label": "Format"
  },
  {
    "span": "Kipf and Welling (2017) introduced GCNs for semi-supervised classification on graphs. Veličković et al. (2018) proposed GATs with attention over neighborhoods. Dwivedi et al. (2021) studied positional encodings for graph transformers.",
    "document": "Related Work\n\nGraph neural networks (GNNs) learn representations by propagating information along edges. Recent work explores architectures, training regimes, and inductive biases to improve expressivity and scalability.\n\nKipf and Welling (2017) introduced GCNs for semi-supervised classification on graphs. Veličković et al. (2018) proposed GATs with attention over neighborhoods. Dwivedi et al. (2021) studied positional encodings for graph transformers. Xu et al. (2019) analyzed the expressive power of GNNs via the Weisfeiler-Lehman test.\n\nWe extend positional message passing by integrating spectrum-aware attention with learned diffusion kernels.",
    "reason": "Although the papers are related to GNNs, the span provides no explanation of how the works connect or why positional encodings follow GCN/GAT; transitions and explicit relationships are missing.",
    "start": 222,
    "end": 456,
    "label": "Coherence"
  },
  {
    "span": "Differential privacy in federated learning has been explored using output perturbation at the server (Geyer et al., 2017; Sun et al., 2019), per-example gradient clipping with Gaussian noise (Abadi et al., 2016; McMahan et al., 2018), client-level privacy via randomized participation (Bittau et al., 2017; Kairouz et al., 2021), secure aggregation to mask updates (Bonawitz et al., 2017), and privacy amplification by subsampling (Balle et al., 2018; Wang et al., 2019). Accounting methods include advanced composition, RDP, and zCDP (Dwork et al., 2010; Mironov, 2017; Bun and Steinke, 2016).",
    "document": "Introduction\n\nFederated learning enables collaborative training over decentralized data while keeping raw examples on device. However, model updates can leak sensitive information, motivating formal privacy guarantees.\n\nDifferential privacy in federated learning has been explored using output perturbation at the server (Geyer et al., 2017; Sun et al., 2019), per-example gradient clipping with Gaussian noise (Abadi et al., 2016; McMahan et al., 2018), client-level privacy via randomized participation (Bittau et al., 2017; Kairouz et al., 2021), secure aggregation to mask updates (Bonawitz et al., 2017), and privacy amplification by subsampling (Balle et al., 2018; Wang et al., 2019). Accounting methods include advanced composition, RDP, and zCDP (Dwork et al., 2010; Mironov, 2017; Bun and Steinke, 2016).\n\nDespite progress, practitioners face utility degradation, fairness concerns across heterogeneous clients, and opaque privacy accounting under partial participation. Our work targets these challenges by combining client-adaptive clipping with stratified sampling and a tight RDP accountant tuned for skewed participation.",
    "reason": "The span enumerates prior approaches and accounting methods without explaining their limitations or how the current work relates, thus lacking synthesis per (a) and (b).",
    "start": 220,
    "end": 814,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT-based clinical NER models have surpassed expert annotators on discharge summaries.",
    "document": "Introduction\n\nClinical named entity recognition (NER) supports downstream tasks such as cohort identification and phenotyping, where high recall is essential (Wang et al., 2018; Stubbs and Uzuner, 2015). Domain-adaptive pretraining on clinical notes has led to gains over general-domain models, especially for medication and problem entities (Alsentzer et al., 2019; Huang et al., 2019). Weak supervision and distant labeling further reduce annotation costs (Ratner et al., 2017; Fries et al., 2020).\n\nBERT-based clinical NER models have surpassed expert annotators on discharge summaries. Nevertheless, many systems struggle with abbreviations, historical mentions, and negated findings. We introduce a negation- and temporality-aware tagging architecture with span-level calibration to improve deployment readiness in EHR settings.",
    "reason": "Asserts performance exceeding human experts on a specific document type without evidence or citation.",
    "start": 502,
    "end": 589,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent shared tasks have demonstrated that end-to-end transducers outperform hybrid systems on noisy speech.",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) models have simplified training pipelines by jointly optimizing acoustic and language components. Among these, transducer architectures have gained popularity for streaming scenarios.\n\nRecent shared tasks have demonstrated that end-to-end transducers outperform hybrid systems on noisy speech. However, these gains are sensitive to augmentation schemes and the availability of domain-matched noise profiles.\n\nWe examine data augmentation strategies for robust streaming ASR, assessing trade-offs between latency, accuracy, and compute under diverse noise conditions.",
    "reason": "References 'recent shared tasks' and a performance comparison without citing the specific shared tasks or their reports.",
    "start": 245,
    "end": 353,
    "label": "Unsupported_claim"
  },
  {
    "span": "the M4 dataset is the standard benchmark for time series forecasting",
    "document": "Introduction\n\nAccurate time series forecasting underpins decision-making in finance, energy, and retail. Classical methods such as ARIMA and ETS remain competitive for univariate series, while deep learning approaches show promise in capturing nonlinearities and cross-series information. Evaluation practices vary widely across domains, complicating comparisons.\n\nTo enable consistent assessment, the community increasingly reports results on unified collections of real-world series. In particular, the M4 dataset is the standard benchmark for time series forecasting, spanning diverse frequencies and domains with well-defined accuracy metrics. Nevertheless, reporting conventions differ on seasonal adjustments and aggregation strategies.\n\nWe propose a frequency-aware forecasting architecture with shared embeddings and per-frequency adapters, and we standardize preprocessing to ensure fair comparisons across methods.",
    "reason": "Claims a dataset is the 'standard benchmark' without citing the benchmark or associated challenge paper (definition a/b).",
    "start": 501,
    "end": 569,
    "label": "Unsupported_claim"
  },
  {
    "span": "Policy gradient methods optimize parameterized controllers directly in continuous action spaces (Schulman et al., 2015; Lillicrap et al., 2016). Model-based approaches learn dynamics for data-efficient planning and control (Deisenroth and Rasmussen, 2011; Chua et al., 2018). Sim-to-real research focuses on transferring policies trained in simulation to physical platforms via domain randomization or adaptation (Tobin et al., 2017; Peng et al., 2018).",
    "document": "Introduction\n\nReinforcement learning (RL) for robotic control aims to produce policies that are both sample efficient and robust in the presence of real-world uncertainties. Challenges include limited interaction budgets, safety constraints during exploration, and the reality gap between simulators and physical systems.\n\nPolicy gradient methods optimize parameterized controllers directly in continuous action spaces (Schulman et al., 2015; Lillicrap et al., 2016). Model-based approaches learn dynamics for data-efficient planning and control (Deisenroth and Rasmussen, 2011; Chua et al., 2018). Sim-to-real research focuses on transferring policies trained in simulation to physical platforms via domain randomization or adaptation (Tobin et al., 2017; Peng et al., 2018).\n\nDespite rapid progress, combining rapid adaptation with safety guarantees remains difficult, particularly when dynamics shift outside the support of training data. In this work, we introduce SafeAda-RL, a framework that couples uncertainty-aware dynamics models with proactive safety shielding and meta-adaptation. We provide theoretical safety certificates under bounded model error and demonstrate safe, fast adaptation across manipulation and locomotion tasks on real hardware.",
    "reason": "The span lists three strands of prior RL work with citations but does not connect them to the authors' goals, articulate what is missing, or state how their approach relates. It lacks synthesis under (a) and (c).",
    "start": 323,
    "end": 776,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Several techniques accelerate diffusion sampling, such as non-Markovian samplers, learned solvers, and consistency distillation (Song et al., 2021; Liu et al., 2022; Salimans and Ho, 2022; Luo et al., 2023; Zhang and Chen, 2023).",
    "document": "Related Work\n\nDiffusion models and sampling efficiency\n\nScore-based diffusion models achieve state-of-the-art image generation quality but require many sampling steps, limiting real-time applications. This has sparked interest in methods that reduce the number of function evaluations while preserving fidelity and diversity.\n\nSeveral techniques accelerate diffusion sampling, such as non-Markovian samplers, learned solvers, and consistency distillation (Song et al., 2021; Liu et al., 2022; Salimans and Ho, 2022; Luo et al., 2023; Zhang and Chen, 2023).\n\nQuality-speed trade-offs\n\nWhile acceleration improves wall-clock time, recent investigations highlight degradation in coverage and increased mode drop when steps are aggressively reduced. There is also sensitivity to noise schedules and discretization schemes that complicates deployment across datasets and architectures.\n\nPositioning\n\nWe target the quality-speed frontier by examining solver stability under distribution shifts in noise schedules and proposing regularization that improves robustness of few-step samplers.",
    "reason": "The span lists acceleration approaches without explaining how they compare, what remains unresolved, or how the present work addresses a specific gap, demonstrating lack of synthesis (criteria a and b).",
    "start": 327,
    "end": 556,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[23]",
    "document": "Introduction\n\nAbstractive summarization has shifted from sequence-to-sequence baselines to pretrained encoder–decoder models that better capture discourse and factual consistency (See et al., 2017; Lewis et al., 2020; Zhang et al., 2020). Despite progress, hallucination remains a persistent issue when models overgeneralize beyond evidence in the source (Maynez et al., 2020; Kryscinski et al., 2020). As shown in [23], constrained decoding can mitigate unsupported content, but often at the cost of fluency.\n\nWe address this trade-off by introducing a retrieval-conditioned planner that structures content selection before generation. Our planner integrates citation-aware constraints during beam search, reducing unsupported claims while preserving readability. We evaluate across news and scientific corpora with both automatic and human factuality metrics.",
    "reason": "Numeric bracket citation style \"[23]\" is inconsistent with the surrounding author–year style and should be converted to an author–year citation.",
    "start": 415,
    "end": 419,
    "label": "Format"
  },
  {
    "span": "LibriSpeech offers approximately 1,000 hours of read English speech and is the de facto benchmark for ASR pretraining.",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) systems increasingly rely on large-scale corpora and self-supervised pretraining to achieve competitive word error rates. Despite advances in model capacity and optimization, data diversity and label quality remain key determinants of downstream performance.\n\nLibriSpeech offers approximately 1,000 hours of read English speech and is the de facto benchmark for ASR pretraining. Other corpora such as Common Voice and TED-LIUM complement LibriSpeech by introducing varied speakers, recording conditions, and speaking styles. However, domain mismatch between read speech and conversational settings often degrades recognition accuracy.\n\nThis paper investigates domain-aware pretraining objectives that improve transfer to spontaneous speech while maintaining strong performance on read-speech benchmarks.",
    "reason": "First mention of the dataset and benchmark status lacks a supporting citation.",
    "start": 320,
    "end": 438,
    "label": "Unsupported_claim"
  },
  {
    "span": "state-of-the-art results on the CoNLL NER benchmark.",
    "document": "Introduction\n\nNamed entity recognition (NER) is a core sequence labeling task that benefits from contextualized encoders and span-level decoding. While transformer-based models have improved NER in high-resource settings, they often rely on token-level independence assumptions that limit span consistency. We introduce a span-consistent decoder trained with contrastive objectives to better separate overlapping and nested mentions. Our method achieves state-of-the-art results on the CoNLL NER benchmark. We further analyze error types to show gains on boundary precision and rare entity categories.",
    "reason": "A state-of-the-art performance claim and first mention of a benchmark require citations and evidence (numbers, baselines), which are missing.",
    "start": 454,
    "end": 506,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Kumar, 2019; Lopez, 2020",
    "document": "Introduction\n\nReinforcement learning (RL) methods such as policy gradients and actor-critic algorithms have achieved notable successes in continuous control and games (Sutton et al., 1999; Williams, 1992; Mnih et al., 2016). Intrinsic motivation and curiosity-driven exploration alleviate sparse-reward issues (Bellemare et al., 2016; Pathak et al., 2017; Burda et al., 2019). Model-based RL can be more sample-efficient but is sensitive to model bias (Ha & Schmidhuber, 2018; Janner et al., 2019). Offline RL introduces additional challenges due to extrapolation error and distributional drift, motivating conservative objectives and uncertainty estimation (Fujimoto et al., 2019; Kumar et al., 2020). In this work, we target robust value estimation under dataset shift, drawing on prior work on conservative policy learning and pessimistic Q-regularization (Kumar, 2019; Lopez, 2020 while also examining bootstrapped ensembles for uncertainty quantification (Osband et al., 2016; Buckman et al., 2018).",
    "reason": "Missing closing parenthesis in a parenthetical citation.",
    "start": 859,
    "end": 884,
    "label": "Format"
  },
  {
    "span": "Bilinear pooling enhances fine-grained interactions between vision and language (Kim et al., 2018). New datasets increase compositional complexity (Hudson and Manning, 2019). Transformer-based encoders scale to larger input contexts (Lu et al., 2019).",
    "document": "Related Work\n\nVisual question answering (VQA) requires joint reasoning over images and text. Progress has come from better multimodal fusion, larger datasets, and improved training recipes.\n\nBilinear pooling enhances fine-grained interactions between vision and language (Kim et al., 2018). New datasets increase compositional complexity (Hudson and Manning, 2019). Transformer-based encoders scale to larger input contexts (Lu et al., 2019). We focus on answer calibration and selective prediction for VQA.\n\nOur calibration layer estimates instance-wise confidence by distilling logits into temperature-scaled scores.",
    "reason": "The works are listed without explaining their relationships; transitions are omitted and the connection from fusion methods to datasets to transformers is not made explicit.",
    "start": 191,
    "end": 442,
    "label": "Coherence"
  },
  {
    "span": "MSRA",
    "document": "Introduction\n\nChinese named entity recognition (NER) poses unique challenges due to the absence of explicit word boundaries and the prevalence of nested entities. Two dominant paradigms exist: character-level sequence labeling and span-based classification, each with trade-offs in modeling local composition and long-range context. To provide a comprehensive evaluation, we conduct experiments on OntoNotes, MSRA, and Weibo, and we analyze the impact of pretraining granularity on boundary detection. We also report results under cross-domain transfer.",
    "reason": "Mentions a specific dataset without citing its original source paper or release, which is required at first mention (rule a).",
    "start": 409,
    "end": 413,
    "label": "Unsupported_claim"
  },
  {
    "span": "ConceptNet",
    "document": "Related Work\n\nCommonsense reasoning remains a persistent bottleneck for language understanding systems. A common strategy is to enrich text encoders with external knowledge graphs that provide relational structure for everyday concepts. Among these resources, ConceptNet and ATOMIC are frequently referenced for their breadth of commonsense relations. Our approach augments a pretrained encoder with node embeddings retrieved from these graphs via entity linking, then regularizes training with relation-aware contrastive losses to encourage multi-hop consistency.",
    "reason": "First mention of a specific knowledge base appears without a citation to its source, violating rule (a).",
    "start": 260,
    "end": 270,
    "label": "Unsupported_claim"
  },
  {
    "span": "BLEU and METEOR correlate weakly with human judgments (Liu et al., 2016). Toxicity filters reduce harmful outputs (Gehman et al., 2020). Retrieval-augmented generation grounds responses in documents (Lewis et al., 2020). Persona consistency remains a challenge (Zhang et al., 2018).",
    "document": "Related Work\n\nOpen-domain dialogue systems have progressed from sequence-to-sequence models to large language models with improved fluency and knowledge access (Vinyals and Le, 2015; Roller et al., 2021). Alongside modeling, evaluation remains difficult due to the diversity of valid responses and safety constraints in real deployments (See et al., 2019; Dinan et al., 2019).\n\nBLEU and METEOR correlate weakly with human judgments (Liu et al., 2016). Toxicity filters reduce harmful outputs (Gehman et al., 2020). Retrieval-augmented generation grounds responses in documents (Lewis et al., 2020). Persona consistency remains a challenge (Zhang et al., 2018).\n\nOur approach addresses faithfulness by conditioning on evidence passages while integrating a calibrated safety classifier during decoding.",
    "reason": "The span lists evaluation metrics, safety filters, retrieval augmentation, and persona consistency without connective language or an explanation of how each relates to the others, causing an abrupt and incoherent flow.",
    "start": 378,
    "end": 660,
    "label": "Coherence"
  },
  {
    "span": "The CodeBench dataset comprises 10,000 programming tasks spanning 20 domains.",
    "document": "Related Work\n\nProgram synthesis from natural language has benefited from large code corpora and instruction-tuned models that map specifications to executable solutions. Benchmarks evaluate generalization across problem distributions and test robustness to adversarial inputs.\n\nThe CodeBench dataset comprises 10,000 programming tasks spanning 20 domains. While recent methods improve pass@k under constrained execution budgets, differences in sandboxing, time limits, and test coverage complicate fair comparison.\n\nWe introduce a normalized evaluation suite with unified execution constraints and per-domain diagnostics to better assess cross-domain generalization.",
    "reason": "Claims specific dataset statistics (size and domains) without providing a citation (rule a/b).",
    "start": 278,
    "end": 355,
    "label": "Unsupported_claim"
  },
  {
    "span": "The FEVER shared task established fact verification as a standard benchmark.",
    "document": "Introduction\n\nAutomated fact verification evaluates a system's ability to retrieve evidence and decide whether a claim is supported, refuted, or unverifiable. Interest in this problem has surged alongside concerns about misinformation and the need for trustworthy AI systems. The FEVER shared task established fact verification as a standard benchmark. Subsequent datasets expanded beyond encyclopedic knowledge to news, science, and multimodal evidence, introducing challenges in domain adaptation and explainability. Our work focuses on cross-domain generalization with retrieval-augmented verifiers.",
    "reason": "Mentions a specific shared task without providing a citation to it.",
    "start": 276,
    "end": 352,
    "label": "Unsupported_claim"
  },
  {
    "span": "over 40% of recent fact-checking systems rely on Wikipedia as the primary source.",
    "document": "Related Work\n\nAutomated fact-checking pipelines typically combine claim retrieval, evidence selection, and veracity classification. Evidence sources range from encyclopedic resources to news articles and scientific literature.\n\nOur survey of prior systems indicates that over 40% of recent fact-checking systems rely on Wikipedia as the primary source. While convenient, this focus can bias evaluations towards encyclopedic claims and limit generalization to emerging news.\n\nWe address this gap by constructing a multi-source evidence retriever that integrates newswire and policy documents alongside encyclopedic texts.",
    "reason": "Provides a precise statistic about prior work uptake without citing supporting evidence or a survey (rule b, e).",
    "start": 271,
    "end": 352,
    "label": "Unsupported_claim"
  },
  {
    "span": "Dosovitskiy et al. (2020) introduced the Vision Transformer for image classification. Chen et al. (2021) adapt ViT to 3D medical volumes with tokens-to-token strategies. UNet remains a strong baseline for segmentation (Ronneberger et al., 2015). Self-supervised pretraining on radiology archives improves downstream tasks (Azizi et al., 2021).",
    "document": "Related Work\n\nDeep learning for medical imaging has progressed from CNN backbones to hybrid architectures that leverage global context. We cover transformers, segmentation baselines, and self-supervision for data efficiency.\n\nDosovitskiy et al. (2020) introduced the Vision Transformer for image classification. Chen et al. (2021) adapt ViT to 3D medical volumes with tokens-to-token strategies. UNet remains a strong baseline for segmentation (Ronneberger et al., 2015). Self-supervised pretraining on radiology archives improves downstream tasks (Azizi et al., 2021).\n\nNevertheless, limited annotations and domain shifts across scanners continue to hinder generalization, motivating our semi-supervised adaptation framework.",
    "reason": "The sequence moves from ViT to 3D adaptations to UNet and then to self-supervision without clarifying how these lines of work connect; transitions and explicit relationships across the sentences are missing.",
    "start": 226,
    "end": 569,
    "label": "Coherence"
  },
  {
    "span": "Equalized odds constrains false positive and false negative rates across groups (Hardt et al., 2016). Causal approaches model counterfactual fairness (Kusner et al., 2017). Calibration aims for predicted probabilities to match frequencies (Guo et al., 2017).",
    "document": "Related Work\n\nFairness in Machine Learning. Algorithmic decisions can propagate or amplify societal biases, motivating formal definitions and interventions to assess and mitigate harm.\n\nDefinitions and Interventions. Group-based criteria, causal notions, and individual-level guarantees provide different lenses on fairness, often leading to trade-offs in practice.\n\nKey Formulations. Equalized odds constrains false positive and false negative rates across groups (Hardt et al., 2016). Causal approaches model counterfactual fairness (Kusner et al., 2017). Calibration aims for predicted probabilities to match frequencies (Guo et al., 2017).\n\nPractical Considerations. In real-world pipelines, shifting distributions, selective labels, and feedback loops complicate evaluation and deployment of fairness interventions.",
    "reason": "The span lists three fairness concepts without articulating their relationships or transitions, leaving the reader unsure how they connect or compare, which creates a coherence issue.",
    "start": 385,
    "end": 643,
    "label": "Coherence"
  },
  {
    "span": "Early neural approaches relied on fixed fingerprints with feed-forward layers (Rogers and Hahn, 2010). Graph-based models then became dominant, including spectral GCNs (Bruna et al., 2014; Defferrard et al., 2016), spatial GCNs (Kipf and Welling, 2017), message passing networks (Gilmer et al., 2017), attention-based GNNs (Veličković et al., 2018), and variants that incorporate edge features or virtual nodes (Hu et al., 2019; Li et al., 2019). Self-supervised pretraining for molecules has also been explored via context prediction, masking, and contrastive objectives (Hu et al., 2020; You et al., 2020; Sun et al., 2020).",
    "document": "Introduction\n\nPredicting molecular properties from structure is central to drug discovery and materials science. Traditional cheminformatics often relies on human-engineered descriptors and fingerprints, which struggle to capture long-range interactions and subtle stereochemical effects. Neural graph encoders promise to learn task-relevant features directly from molecular graphs, reducing the need for manual feature design and enabling multi-task learning across endpoints.\n\nEarly neural approaches relied on fixed fingerprints with feed-forward layers (Rogers and Hahn, 2010). Graph-based models then became dominant, including spectral GCNs (Bruna et al., 2014; Defferrard et al., 2016), spatial GCNs (Kipf and Welling, 2017), message passing networks (Gilmer et al., 2017), attention-based GNNs (Veličković et al., 2018), and variants that incorporate edge features or virtual nodes (Hu et al., 2019; Li et al., 2019). Self-supervised pretraining for molecules has also been explored via context prediction, masking, and contrastive objectives (Hu et al., 2020; You et al., 2020; Sun et al., 2020).\n\nDespite these advances, performance often degrades under scaffold shift and when labels are scarce or noisy. In this paper, we introduce a scaffold-aware pretraining and calibration framework that couples masked node recovery with conformer-consistent contrastive learning and post-hoc temperature scaling. Our contributions are: (1) a new contrastive objective that aligns 2D graphs with 3D conformers; (2) a scaffold-balanced batching scheme for robust pretraining; and (3) a label-noise aware calibration routine that improves reliability under distribution shift.",
    "reason": "Lists and briefly describes prior methods without explaining how they relate to the proposed approach or identifying a concrete gap; no synthesis with the paper's aims (criterion a/c).",
    "start": 479,
    "end": 1105,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Park et al., (2017)",
    "document": "Introduction\n\nDeep learning has transformed recommender systems by enabling expressive user–item interactions. Early matrix factorization established low-rank structure (Koren et al., 2009), while neural approaches model higher-order signals (He et al., 2017). Attention mechanisms capture sequential context (Kang and McAuley, 2018), and graph-based methods propagate preferences across item relations (Wang et al., 2019). Park et al., (2017) present a hybrid model that fuses content and collaborative information. Our work focuses on calibrated uncertainty for recommendation under exposure bias, extending propensity-based corrections (Schnabel et al., 2016).",
    "reason": "Incorrect punctuation for narrative citation; should be 'Park et al. (2017)' without the comma before the parenthesis.",
    "start": 424,
    "end": 443,
    "label": "Format"
  },
  {
    "span": "Model-agnostic meta-learning optimizes initializations for rapid adaptation (Finn et al., 2017). Prototypical networks classify by distances to class prototypes (Snell et al., 2017). Reptile performs first-order meta-updates for simplicity (Nichol et al., 2018). Data augmentation improves generalization via synthetic tasks (Rajendran et al., 2020).",
    "document": "Related Work\n\nFew-shot learning methods aim to generalize to new classes from limited examples. Meta-learning approaches and representation learning have emerged as leading strategies for this setting.\n\nModel-agnostic meta-learning optimizes initializations for rapid adaptation (Finn et al., 2017). Prototypical networks classify by distances to class prototypes (Snell et al., 2017). Reptile performs first-order meta-updates for simplicity (Nichol et al., 2018). Data augmentation improves generalization via synthetic tasks (Rajendran et al., 2020).\n\nWe study how task-conditioned augmentation interacts with gradient-based meta-learning in cross-domain evaluation.",
    "reason": "The works are presented as isolated statements with no transitions or explanation of their interrelationships, leaving the connection between meta-learning algorithms and augmentation implied rather than explicit.",
    "start": 203,
    "end": 553,
    "label": "Coherence"
  },
  {
    "span": "Kim et al.",
    "document": "Introduction\n\nRobust domain adaptation remains a central challenge for sequence labeling. As shown by Kim et al., adversarial objectives can reduce representation shift between source and target corpora, but they often underperform when label distributions diverge widely. To address this, we propose a conditional alignment mechanism that explicitly conditions on task-specific prototypes, building on insights from prototype learning (Snell et al., 2017) and information bottleneck regularization.",
    "reason": "Narrative citation missing year: should be 'Kim et al. (YEAR)' when used in running text.",
    "start": 102,
    "end": 112,
    "label": "Format"
  },
  {
    "span": "Per-FedAvg adapts local models with task-specific heads (Fallah et al., 2020). Ditto adds a proximal regularizer for robustness (Li et al., 2021). Secure aggregation protects gradients from the server (Bonawitz et al., 2017). Gradient sparsification reduces communication cost (Aji and Heafield, 2017).",
    "document": "Related Work\n\nPersonalization in Federated Learning\n\nFederated learning often faces client heterogeneity due to non-iid data, motivating approaches that personalize models to local distributions. Personalization methods adjust model layers, introduce mixture-of-experts, or fine-tune client-specific parameters while sharing global priors.\n\nMethods and System Considerations\n\nPer-FedAvg adapts local models with task-specific heads (Fallah et al., 2020). Ditto adds a proximal regularizer for robustness (Li et al., 2021). Secure aggregation protects gradients from the server (Bonawitz et al., 2017). Gradient sparsification reduces communication cost (Aji and Heafield, 2017).\n\nOur Contribution\n\nWe present a meta-learned representation with client-conditioned adapters and a bi-level optimization that balances global and local objectives. We analyze convergence under partial participation and report improvements on vision and text benchmarks.",
    "reason": "The sentences juxtapose personalization algorithms with privacy and communication efficiency techniques without transitions or explanation. The relevance of secure aggregation and gradient sparsification to personalization is not made explicit, causing an abrupt shift and reducing coherence.",
    "start": 376,
    "end": 678,
    "label": "Coherence"
  },
  {
    "span": "Gulwani (2011) explored inductive program synthesis from examples. Devlin et al. (2017) introduced DeepCoder for learning to write programs. Chen et al. (2021) presented Codex for general-purpose code generation.",
    "document": "Related Work\n\nProgram synthesis methods range from symbolic search guided by specifications to neural approaches that learn from large corpora. Key challenges include constraining search spaces, incorporating weak and noisy supervision, and ensuring correctness with respect to formal specifications.\n\nSymbolic and neural synthesis\n\nGulwani (2011) explored inductive program synthesis from examples. Devlin et al. (2017) introduced DeepCoder for learning to write programs. Chen et al. (2021) presented Codex for general-purpose code generation. Ellis et al. (2019) proposed DreamCoder with wake-sleep for libraries of programs.\n\nVerification and repair\n\nComplementary threads integrate SMT solving, abstract interpretation, and counterexample-guided inductive synthesis to provide correctness guarantees or automated repair, but scaling these methods to large, real-world tasks remains difficult.\n\nOur contribution bridges statistical learning with lightweight constraint checking, enabling faster search while preserving semantic constraints on synthesized code fragments.",
    "reason": "The span lists three works with no transitions or explicit articulation of how they relate (e.g., progression from symbolic to neural, or trade-offs), creating abrupt connections. This is a multi-sentence coherence issue per (a) and (b).",
    "start": 333,
    "end": 545,
    "label": "Coherence"
  },
  {
    "span": "Most recent neural ranking models rely on cross-encoder architectures.",
    "document": "Related Work\nNeural information retrieval has progressed from representation-focused bi-encoders to interaction-heavy architectures that model fine-grained token alignments. Cross-encoders jointly encode query and document pairs, often achieving higher accuracy at the cost of throughput, while bi-encoders enable efficient approximate nearest neighbor search.\n\nMost recent neural ranking models rely on cross-encoder architectures. However, latency constraints in production search motivate hybrid strategies that combine fast retrieval with re-ranking. Our approach explores an intermediate design that preserves much of the interaction capacity while remaining compatible with large-scale retrieval.",
    "reason": "This broad claim about 'most recent' models is a literature claim that requires citations to representative works; none are given (violates d).",
    "start": 362,
    "end": 432,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prompting has been used to steer pretrained LMs toward downstream tasks (Brown et al., 2020; Gao et al., 2021). Prefix-tuning freezes base parameters while optimizing continuous prompts (Li and Liang, 2021). Instruction tuning aligns models to follow natural language requests (Wei et al., 2022). Retrieval-augmented generation attaches an external memory (Lewis et al., 2020).",
    "document": "Introduction\n\nLarge Language Models and Prompt-Based Learning\n\nPretrained language models have transformed NLP by enabling few-shot and zero-shot transfer through the use of prompts. However, the space of prompting techniques varies widely in how they adapt parameters, leverage supervision, and access external knowledge.\n\nPrompt Design and Lightweight Adaptation\n\nPrompting has been used to steer pretrained LMs toward downstream tasks (Brown et al., 2020; Gao et al., 2021). Prefix-tuning freezes base parameters while optimizing continuous prompts (Li and Liang, 2021). Instruction tuning aligns models to follow natural language requests (Wei et al., 2022). Retrieval-augmented generation attaches an external memory (Lewis et al., 2020).\n\nOur Perspective\n\nWe study stability of prompt methods across domains with distribution shift, focusing on calibration under label imbalance. We complement prior work by analyzing sensitivity to template perturbations and context length, and by proposing a robust prompt ensembling strategy with adaptive selection.",
    "reason": "Multiple cited works are placed consecutively with no explicit transitions connecting them or explaining their relationships. The shift from parameter-efficient prompting to instruction tuning and then to retrieval is abrupt, leaving the reader to infer how each technique relates to the others.",
    "start": 366,
    "end": 743,
    "label": "Coherence"
  },
  {
    "span": "(Nguyen 2020)",
    "document": "Related Work\n\nLow-resource machine translation benefits from transfer learning, data augmentation, and multilingual pretraining. Back-translation remains a strong baseline for synthetic target-side data (Sennrich et al., 2016), while round-trip consistency improves stability (He et al., 2016). Multilingual encoders enable parameter sharing across families (Conneau et al., 2020), and adapter layers allow specialization with few parameters (Bapna and Firat, 2019). Several studies report gains from vocabulary alignment and shared subwords (Nguyen 2020), but these approaches can underperform when morphology diverges substantially.\n\nOur approach complements these methods by introducing a lexicon-constrained objective that leverages bilingual dictionaries without requiring parallel sentences.",
    "reason": "Missing comma between author and year in a parenthetical citation; should be (Nguyen, 2020).",
    "start": 542,
    "end": 555,
    "label": "Format"
  },
  {
    "span": "Previous studies have shown that character-level CRFs outperform word-level models on agglutinative languages.",
    "document": "Related Work\nNamed entity recognition (NER) in low-resource settings poses challenges due to sparse supervision, domain shift, and complex morphology. Approaches span multilingual transfer, distant supervision, and subword modeling. While contextual encoders have improved performance, morphology-aware modeling remains important for languages with rich inflection and compounding.\nPrevious studies have shown that character-level CRFs outperform word-level models on agglutinative languages. This observation has motivated hybrid architectures that incorporate character, subword, and morpheme representations, often with constraints to encourage label consistency.\nDespite these advances, many methods rely on resources that are scarce in truly low-resource contexts, such as morphological analyzers or large parallel corpora. Our work focuses on resource-lean techniques that can be applied with only raw text and minimal annotations.",
    "reason": "Claims a result from prior studies without providing any citation to the studies.",
    "start": 382,
    "end": 492,
    "label": "Unsupported_claim"
  },
  {
    "span": "Consistency regularization has proven effective for SSL in speech (Park et al., 2020; Xu et al., 2020; Chen et al., 2021).",
    "document": "Introduction\nSemi-supervised learning (SSL) reduces the need for labeled speech by leveraging large corpora of unlabeled audio. A key challenge is extracting robust representations that transfer across domains and speakers.\n\nRelated Work\nConsistency regularization has proven effective for SSL in speech (Park et al., 2020; Xu et al., 2020; Chen et al., 2021). Pseudo-labeling approaches generate targets from teacher models or EMA students (Kahn et al., 2020; Zhang et al., 2021). Self-supervised pretraining learns general acoustic features via contrastive or masked objectives (Baevski et al., 2020; Hsu et al., 2021). We benchmark on Librispeech and cross-corpus settings.",
    "reason": "The span lists a technique and citations but does not explain how these works inform the present study, which limitations persist, or how the authors’ method differs.",
    "start": 238,
    "end": 360,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Chen et al., 2018",
    "document": "Related Work\n\nGraph neural networks (GNNs) generalize convolution to irregular structures, enabling end-to-end learning on graphs for tasks such as node classification and link prediction (Kipf and Welling, 2017; Hamilton et al., 2017). Attention-based architectures capture long-range dependencies and heterogeneous neighborhoods more effectively than fixed filters (Velickovic et al., 2018; Wang et al., 2019). Early message-passing frameworks (Chen et al., 2018 established a template for iterative aggregation, but they struggled with oversmoothing in deep stacks (Li et al., 2018). Subsequent techniques introduced residual connections, normalization, and decoupled propagation to alleviate degradation (Klicpera et al., 2019; Oono and Suzuki, 2020). We build on scalable sampling strategies (Chen et al., 2018b; Zeng et al., 2020) and explore adaptive receptive fields that adjust to graph sparsity patterns, improving both efficiency and accuracy in large-scale benchmarks.\n",
    "reason": "Missing closing parenthesis in a parenthetical citation; should be \"+)\" at the end, e.g., \"(Chen et al., 2018)\".",
    "start": 446,
    "end": 464,
    "label": "Format"
  },
  {
    "span": "over 7,000 languages worldwide, yet fewer than 2% have any ASR resources",
    "document": "Introduction\n\nLow-resource automatic speech recognition (ASR) faces data scarcity, domain mismatch, and orthographic variation challenges (Besacier et al., 2014; Ardila et al., 2020). Although multilingual self-supervised pretraining reduces labeled data needs (Baevski et al., 2020; Conneau et al., 2021), most languages remain underserved in public benchmarks and toolchains. There are over 7,000 languages worldwide, yet fewer than 2% have any ASR resources, underscoring the urgency of scalable methods that minimize supervision and leverage community contributions.\n\nWe investigate a semi-supervised pipeline combining wav2vec-style pretraining, pseudo-labeling with confidence filtering, and pronunciation lexicon induction. Experiments on three underrepresented Bantu languages demonstrate consistent WER reductions compared to supervised baselines.",
    "reason": "Includes precise global statistics about languages and ASR coverage without citing any source, which requires evidence per rule (b).",
    "start": 388,
    "end": 460,
    "label": "Unsupported_claim"
  },
  {
    "span": "[28]",
    "document": "Introduction\n\nLearning to rank for question answering commonly integrates features such as term proximity, semantic similarity, and click feedback (Metzler and Bruce Croft, 2007; Xiong et al., 2017). Neural retrieval augments sparse signals with dense representations trained via contrastive objectives (Karpukhin et al., 2020; Qu et al., 2021). Our method borrows negative sampling strategies from information retrieval [28] but adapts the temperature and queue size to the multi-hop setting (Asai et al., 2020).",
    "reason": "Numeric bracketed citation used in a context that otherwise follows author–year style.",
    "start": 421,
    "end": 425,
    "label": "Format"
  },
  {
    "span": "McMahan et al. (2017) introduced FedAvg for on-device learning. Bonawitz et al. (2017) developed secure aggregation for federated settings. Differential privacy has been integrated with FL (Geyer et al., 2017). Personalized federated optimization was proposed by Hanzely and Richtárik (2020).",
    "document": "Related Work\n\nFederated Learning and Privacy\n\nFederated learning (FL) enables collaborative training across clients without centralizing raw data. This setting raises challenges around communication efficiency, privacy protection, and statistical heterogeneity across clients.\n\nMcMahan et al. (2017) introduced FedAvg for on-device learning. Bonawitz et al. (2017) developed secure aggregation for federated settings. Differential privacy has been integrated with FL (Geyer et al., 2017). Personalized federated optimization was proposed by Hanzely and Richtárik (2020).\n\nOrthogonal enhancements address compression, client sampling, and robustness to byzantine behavior. Our approach complements these by decoupling personalization from privacy accounting through adaptive partial aggregation.",
    "reason": "The span lists four contributions that target different aspects of FL (optimization, cryptography, privacy, personalization) without articulating how they relate or transition from one to the next. The abrupt sequence results in weak coherence (criteria a and b).",
    "start": 278,
    "end": 570,
    "label": "Coherence"
  },
  {
    "span": "In a previous study, the authors claim that immediate feedback halves student error rates in introductory programming.",
    "document": "Introduction\nIntelligent tutoring systems for programming aim to provide timely, targeted support that accelerates learning while reducing instructor burden. Feedback timing and granularity are central design choices that influence student engagement and knowledge retention.\nIn a previous study, the authors claim that immediate feedback halves student error rates in introductory programming. While this result is suggestive, it remains unclear how feedback latency interacts with task difficulty and prior knowledge.\nWe investigate feedback schedules that trade off immediacy and reflection, evaluating their effects on short-term performance and long-term retention across multiple cohorts.",
    "reason": "Mentions a 'previous study' and a specific claimed effect without citing the study.",
    "start": 276,
    "end": 394,
    "label": "Unsupported_claim"
  },
  {
    "span": "Zhou et al. (2021) proposed TS-TCC for time-series contrastive learning. Eldele et al. (2021) developed SleepTransformer for sleep staging. Woo et al. (2022) examined augmentation policies for sensor data. Rajpurkar et al. (2017) released a dataset for arrhythmia detection.",
    "document": "Introduction\n\nTime-series representation learning aims to build robust features for downstream tasks with limited labels across domains like healthcare, wearables, and industrial monitoring. Augmentations and contrastive objectives are central to recent progress, but their effectiveness can be sensitive to sampling rates, channel correlations, and nonstationarity. Moreover, evaluation protocols often conflate domain shift with label imbalance, obscuring the benefits of learned invariances.\n\nZhou et al. (2021) proposed TS-TCC for time-series contrastive learning. Eldele et al. (2021) developed SleepTransformer for sleep staging. Woo et al. (2022) examined augmentation policies for sensor data. Rajpurkar et al. (2017) released a dataset for arrhythmia detection.\n\nWe contribute a domain-adaptive augmentation search that conditions on signal spectra, combined with a stability-regularized objective that resists spurious temporal shortcuts, yielding consistent gains across ECG, EEG, and HAR benchmarks.",
    "reason": "The span mixes methods, applications, augmentation studies, and a dataset without articulating their connections. There are no transitions or explanations showing how these works inform one another.",
    "start": 496,
    "end": 770,
    "label": "Coherence"
  },
  {
    "span": "Sennrich et al. (2016) introduce back-translation to exploit monolingual data. Fadaee et al. (2017) propose rare word substitution to improve lexical coverage. Kobayashi (2018) generates paraphrases by replacing words using gradients. Xie et al. (2020) mixup-style augmentation regularizes sequence models.",
    "document": "Related Work\n\nData Augmentation for Neural Machine Translation\n\nAugmentation mitigates overfitting and domain mismatch by synthesizing diverse training pairs. Techniques vary in how they leverage monolingual corpora, encourage lexical diversity, or smooth decision boundaries. Sennrich et al. (2016) introduce back-translation to exploit monolingual data. Fadaee et al. (2017) propose rare word substitution to improve lexical coverage. Kobayashi (2018) generates paraphrases by replacing words using gradients. Xie et al. (2020) mixup-style augmentation regularizes sequence models. The effectiveness of each method depends on language pair, model capacity, and noise control.\n\nWe extend back-translation with curriculum-aware noise schedules and bilingual lexical constraints to better target low-frequency phenomena.",
    "reason": "Papers are listed in succession with no transitions or explanation of how lexical substitution, paraphrasing, and mixup relate to or differ from back-translation, resulting in an abrupt sequence lacking explicit connections.",
    "start": 277,
    "end": 583,
    "label": "Coherence"
  },
  {
    "span": "(RoBERTa Liu et al. (2019))",
    "document": "Introduction\n\nPretrained language models are widely used for code summarization due to their strong sequence modeling capabilities. Early encoder-decoder architectures relied on recurrent networks (Iyer et al., 2016), later supplanted by transformers with masked pretraining. We adopt a strong baseline based on (RoBERTa Liu et al. (2019)) and adapt it to source code via structure-aware adapters that exploit abstract syntax trees.\n\nRelated Work\n\nDomain-adaptive pretraining for code (Kanade et al., 2020) and retrieval-augmented generation (LeClair et al., 2020) have shown complementary benefits. Our adapter design reduces catastrophic forgetting relative to naive fine-tuning (Howard and Ruder, 2018).",
    "reason": "Mixed citation style with nested and misplaced parentheses. It should be “RoBERTa (Liu et al., 2019)” in narrative form, or simply “(Liu et al., 2019)” if referring to the paper, without the double parentheses.",
    "start": 312,
    "end": 339,
    "label": "Format"
  },
  {
    "span": "Causal perspectives on recommendation introduce potential outcomes, inverse propensity scoring, doubly robust estimation, and structural models to debias exposure and selection effects (Schnabel et al., 2016; Wang et al., 2020; Bonner and Vasile, 2018; Saito, 2020; Pearl, 2009).",
    "document": "Related Work\n\nImplicit-feedback recommenders suffer from confounding due to exposure mechanisms and user self-selection. While causal methods offer principled corrections, practical deployment requires robust estimation under misspecified propensities. We consider off-policy evaluation and learning with partially observed logging.\n\nCausal perspectives on recommendation introduce potential outcomes, inverse propensity scoring, doubly robust estimation, and structural models to debias exposure and selection effects (Schnabel et al., 2016; Wang et al., 2020; Bonner and Vasile, 2018; Saito, 2020; Pearl, 2009).\n\nWe propose a calibration-regularized doubly robust objective that remains stable when logged propensities are noisy and incomplete, improving both estimation and ranking quality.",
    "reason": "The span merely enumerates causal techniques without explaining their limitations under partially observed logging or how they relate to the proposed solution (criteria a and b).",
    "start": 334,
    "end": 613,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Smith, 2021 et al.",
    "document": "Introduction\n\nAbstractive summarization has shifted toward pretrained encoder–decoder models fine-tuned with coverage and factuality objectives (Lewis et al., 2020; Xu et al., 2021). Reliance on imitation learning can entrench exposure bias, prompting research into reinforcement learning with factuality rewards (Narayan et al., 2018). Smith, 2021 et al. highlight the limits of n-gram overlap metrics for evaluating factual consistency and propose entity-centric measures.\n\nWe build on this line by integrating a contradiction-aware verifier into the training loop and assessing summaries with faithfulness-oriented metrics.",
    "reason": "Incorrect ordering/style in narrative citation; the year is inserted after the author surname within the author list. Should be 'Smith et al. (2021)'.",
    "start": 337,
    "end": 355,
    "label": "Format"
  },
  {
    "span": "Morales et al.",
    "document": "Related Work\n\nBiomedical entity normalization relies on mapping surface forms to canonical identifiers. Dictionary-based methods remain competitive but suffer from ambiguity and coverage issues (Leaman and Lu, 2016; Tutubalina et al., 2018). As noted by Morales et al., recent neural ranking models improve disambiguation by integrating context-sensitive encoders and candidate priors. Concurrent efforts incorporate ontology structure and synonym expansion to enhance recall (Sung et al., 2020; Ji et al., 2020).\n\nIn contrast to prior work, our approach combines mention-level context with concept-level descriptions and leverages contrastive pretraining to align mentions and entities across corpora.\n",
    "reason": "Narrative citation missing year; should be formatted as 'Morales et al. (YEAR)'.",
    "start": 254,
    "end": 268,
    "label": "Format"
  },
  {
    "span": "Recent works have focused on semi-supervised semantic segmentation in urban scenes.",
    "document": "Related Work\n\nSemantic segmentation has advanced rapidly with fully convolutional networks (Long et al., 2015), encoder–decoder architectures (Ronneberger et al., 2015), and atrous spatial pyramid pooling (Chen et al., 2017; Chen et al., 2018). Large-scale datasets and challenges have catalyzed progress in both accuracy and robustness (Cordts et al., 2016; Zhao et al., 2017; Chen et al., 2019).\n\nRecent works have focused on semi-supervised semantic segmentation in urban scenes. While pseudo-labeling and consistency regularization have proven effective in classification, segmentation introduces dense prediction challenges that require spatially aware perturbations and uncertainty modeling. Concurrently, cross-domain adaptation tackles shifts between synthetic and real data (Hoffman et al., 2018; Zhang et al., 2019), yet remains sensitive to label noise.\n\nOur approach bridges semi-supervised and domain-adaptive training via class-balanced consistency losses and prototype alignment.",
    "reason": "The phrase 'Recent works' asserts prior literature without providing any citations (violates rule d).",
    "start": 399,
    "end": 482,
    "label": "Unsupported_claim"
  },
  {
    "span": "Federated averaging aggregates on-device models to form a global model without centralizing data (McMahan et al., 2017). Differentially private stochastic gradient descent provides formal privacy guarantees by perturbing updates (Abadi et al., 2016). Secure aggregation ensures the server only observes encrypted client updates (Bonawitz et al., 2017). Personalization methods adapt the global model to local user distributions (Smith et al., 2017).",
    "document": "Related Work\n\nFederated Learning and Privacy\n\nFederated learning (FL) enables collaborative training across clients while keeping raw data local, addressing increasing concerns about data sovereignty and regulation. A core challenge in FL is achieving utility under heterogeneous data while preserving strong privacy guarantees. Research threads include aggregation algorithms, privacy mechanisms, communication efficiency, and personalization.\n\nPrivacy-Preserving Mechanisms in FL\n\nFederated averaging aggregates on-device models to form a global model without centralizing data (McMahan et al., 2017). Differentially private stochastic gradient descent provides formal privacy guarantees by perturbing updates (Abadi et al., 2016). Secure aggregation ensures the server only observes encrypted client updates (Bonawitz et al., 2017). Personalization methods adapt the global model to local user distributions (Smith et al., 2017).\n\nCommunication and Systems\n\nBeyond privacy, reducing uplink/downlink cost and straggler effects is crucial. Compression and client selection, along with adaptive participation, improve scalability in cross-device settings.\n\nOur Contribution\n\nWe unify client-level differential privacy with secure aggregation under partial participation, providing a utility analysis that accounts for heterogeneity and communication constraints.",
    "reason": "The span enumerates four topics (FedAvg, DP-SGD, secure aggregation, and personalization) as isolated facts with no transitions or explanation of how they interact in a single FL pipeline; the relationships are only implied, reducing coherence.",
    "start": 483,
    "end": 932,
    "label": "Coherence"
  },
  {
    "span": "(Rahman, 2020",
    "document": "Related Work\n\nOff-policy reinforcement learning improves sample efficiency by reusing data collected under older policies, but suffers from extrapolation error and distributional shift (Fujimoto et al., 2019; Kumar et al., 2019). Conservative value estimation methods and implicit Q regularization aim to address these issues (Kostrikov et al., 2021; Nair et al., 2020). Prior work (Rahman, 2020 introduced uncertainty-aware targets to temper bootstrapping on out-of-distribution actions, while others incorporated behavior cloning penalties to constrain updates (Fujimoto and Gu, 2021). We build on these insights by unifying conservative backups with behavior-aware exploration, yielding stable improvements on sparse-reward tasks (Andrychowicz et al., 2017; Ecoffet et al., 2021).",
    "reason": "Missing closing parenthesis in a parenthetical citation.",
    "start": 382,
    "end": 395,
    "label": "Format"
  },
  {
    "span": "Recent studies have applied temporal graph networks and attention mechanisms to traffic prediction (Wu et al., 2019; Bai et al., 2020; Li and Zhu, 2021). These methods model spatial dependencies via graph convolutions and temporal patterns with recurrent units or temporal convolutions.",
    "document": "Related Work\n\nTraffic forecasting has long been approached with statistical models that assume linear dynamics and stationarity, such as ARIMA and VAR, but these assumptions often fail in congested urban networks. With the rise of deep learning, researchers moved toward neural architectures that better capture nonlinearities and spatiotemporal dependencies.\n\nRecent studies have applied temporal graph networks and attention mechanisms to traffic prediction (Wu et al., 2019; Bai et al., 2020; Li and Zhu, 2021). These methods model spatial dependencies via graph convolutions and temporal patterns with recurrent units or temporal convolutions. In parallel, sequence-to-sequence models and encoder–decoder frameworks have shown promise in multi-horizon prediction of traffic speeds and volumes across large sensor networks (Ma et al., 2015; Yao et al., 2018). Hybrid approaches have also explored dynamic adjacency estimation to reflect evolving traffic conditions (Yu et al., 2018; Pan et al., 2021).\n\nAdditionally, data augmentation and transfer learning techniques have been investigated to cope with sensor sparsity and domain shifts across cities (Zhang et al., 2019; Gao et al., 2021). External factors such as weather, events, and incidents have been integrated through feature fusion to further enhance prediction accuracy (Hoang et al., 2019; Chen et al., 2020).\n\nThe above lines of work collectively illustrate the evolution from static, local models to dynamic, graph-based architectures for urban traffic forecasting.",
    "reason": "The span lists prior approaches and what they generally do but does not connect them to the paper's aims, articulate a gap, or explain how the cited methods relate to the authors' argument or contribution.",
    "start": 361,
    "end": 647,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent works combine self-supervised features with beamforming to improve multi-microphone denoising.",
    "document": "Related Work\n\nSpeech enhancement has progressed from spectral subtraction and Wiener filtering to deep neural architectures that operate in time, frequency, or hybrid domains (Boll, 1979; Luo and Mesgarani, 2019). Self-supervised representations have shown promise for robust speech processing under noisy conditions.\n\nRecent works combine self-supervised features with beamforming to improve multi-microphone denoising. Despite promising trends, these methods often require carefully matched pretraining corpora and microphone array geometries. Our approach integrates geometry-aware positional encoding with feature-space consistency regularization to enhance generalization across arrays.",
    "reason": "Claims a trend in recent literature without providing any citations; mentions of 'recent works' require specific references.",
    "start": 319,
    "end": 420,
    "label": "Unsupported_claim"
  },
  {
    "span": "In a previous study, the authors claim that pairwise ranking losses outperform pointwise losses on implicit feedback.",
    "document": "Related Work\n\nLearning to rank from implicit feedback underpins many large-scale recommender systems. Pointwise methods model click probabilities with logistic or Poisson losses, while pairwise and listwise approaches directly compare clicked with unclicked items to optimize ranking quality (Rendle et al., 2009; Xia et al., 2008). In a previous study, the authors claim that pairwise ranking losses outperform pointwise losses on implicit feedback. Subsequent research investigates debiasing strategies for position and exposure bias using inverse propensity weighting and counterfactual risk minimization (Joachims et al., 2017; Schnabel et al., 2016). Our method integrates pairwise training with calibrated propensity models to reduce bias.",
    "reason": "References a specific prior study and its claim but does not cite the study.",
    "start": 333,
    "end": 450,
    "label": "Unsupported_claim"
  },
  {
    "span": "In the Natural Questions shared task, top systems rely on hybrid sparse-dense retrievers.",
    "document": "Introduction\n\nOpen-domain question answering (ODQA) connects information retrieval with machine reading to answer questions over large corpora. Modern systems typically adopt a retriever-reader pipeline, where the retriever must balance recall and efficiency to surface evidence passages.\n\nIn the Natural Questions shared task, top systems rely on hybrid sparse-dense retrievers. While hybridization offers robustness across query types, it introduces new challenges in index maintenance and late fusion calibration, especially under domain drift.\n\nWe present a retriever with adaptive fusion that optimizes combination weights per-query using uncertainty signals derived from retrieval proxies, leading to improved recall at fixed latency budgets.\n",
    "reason": "This sentence references a specific shared task and claims what 'top systems' do without citing the shared task or the systems, which requires citations.",
    "start": 290,
    "end": 379,
    "label": "Unsupported_claim"
  },
  {
    "span": "(O'Neil 2015)",
    "document": "Related Work\n\nFairness in ranking extends classification fairness notions to position-biased exposure models. Statistical parity constraints were adapted to ranked lists (O'Neil 2015) and later extended to exposure-based objectives (Singh and Joachims, 2018). Optimization-based approaches incorporate group constraints into learning-to-rank losses (Celis et al., 2020), while post-processing methods re-order results to satisfy fairness criteria (Biega et al., 2020). Recent work studies individual fairness via similarity metrics (Zehlike et al., 2020).",
    "reason": "Missing comma between author and year in the parenthetical citation; should be \"(O'Neil, 2015)\".",
    "start": 170,
    "end": 183,
    "label": "Format"
  },
  {
    "span": "In the SemEval Irony Detection shared task, character-level CNNs were the most competitive models.",
    "document": "Related Work\n\nIrony and sarcasm detection have been studied across social media platforms, with datasets capturing diverse linguistic cues such as hyperbole, polarity flips, and pragmatic contrasts. Shared tasks have accelerated progress by providing standardized evaluation settings. In the SemEval Irony Detection shared task, character-level CNNs were the most competitive models. Subsequent work shifted toward pre-trained transformers, incorporating context beyond the target sentence, such as conversation threads and user histories. Despite these advances, robustness to domain shift (e.g., from Twitter to Reddit) remains a persistent challenge.\n",
    "reason": "References a specific shared task outcome without citing the task overview or system reports.",
    "start": 285,
    "end": 383,
    "label": "Unsupported_claim"
  },
  {
    "span": "Safety in human-robot collaboration builds on standards for collaborative operation and power-and-force limiting (ISO 10218-1; ISO/TS 15066), perception for human pose and intent estimation (Romero et al., 2020; Patel et al., 2021), motion planning with proxemics and chance constraints (Liu et al., 2018; Pham et al., 2020), and formal verification of safety envelopes (Althoff et al., 2010; Herbert et al., 2017). Runtime monitors and risk indices are also explored (Lasota et al., 2017; Villani et al., 2018).",
    "document": "Related Work\n\nHuman-robot collaboration (HRC) requires predicting and respecting human motion while maintaining task efficiency. Achieving this balance involves perception, planning, and assurance components.\n\nSafety in human-robot collaboration builds on standards for collaborative operation and power-and-force limiting (ISO 10218-1; ISO/TS 15066), perception for human pose and intent estimation (Romero et al., 2020; Patel et al., 2021), motion planning with proxemics and chance constraints (Liu et al., 2018; Pham et al., 2020), and formal verification of safety envelopes (Althoff et al., 2010; Herbert et al., 2017). Runtime monitors and risk indices are also explored (Lasota et al., 2017; Villani et al., 2018).\n\nOur approach unifies predictive safety via probabilistic human motion models with control barrier certificates that adaptively scale conservatism based on real-time intent uncertainty.",
    "reason": "The span lists standards and technical strands without drawing connections to the authors’ approach or stating the unresolved issue, thus lacking synthesis per (a) and (c).",
    "start": 210,
    "end": 722,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Clark, 2019,; Jones, 2020)",
    "document": "Related Work\n\nSemi-supervised learning. Consistency regularization and pseudo-labeling are two prevalent paradigms for exploiting unlabeled data. Augmentation robustness plays a critical role: stronger perturbations improve generalization but may destabilize training without confidence thresholds. Prior comparisons have highlighted the benefits of sharpening targets and distribution alignment across domains (Clark, 2019,; Jones, 2020).\n\nTeacher–student frameworks. Exponential moving average teachers stabilize targets across epochs, while multi-view consistency boosts performance when views capture complementary invariances. However, confirmation bias can accumulate when label noise is high, necessitating curriculum-aware sampling.",
    "reason": "Malformed multi-citation punctuation: extra comma before semicolon. Correct APA form is '(Clark, 2019; Jones, 2020)'.",
    "start": 411,
    "end": 438,
    "label": "Format"
  },
  {
    "span": "Early rule-based systems like NegEx established the de facto approach for clinical negation detection.",
    "document": "Introduction\n\nAccurate extraction of clinical conditions from electronic health records (EHRs) requires distinguishing asserted findings from negated or speculative mentions. Negation detection has therefore become a core preprocessing step in clinical NLP pipelines, enabling downstream tasks such as cohort identification, adverse event monitoring, and clinical decision support. While modern neural models can learn contextual cues directly from data, clinical notes are heterogeneous and often terse, motivating approaches that explicitly model negation cues and their scopes. Early rule-based systems like NegEx established the de facto approach for clinical negation detection. Subsequent research explored adaptations to languages beyond English, domain portability, and syntactic heuristics to handle long-distance dependencies. More recently, sequence labeling models and pretrained encoders have been applied to incorporate broader context and handle ambiguous cues such as \"denies\" or templates that list conditions as present or absent. However, comprehensive evaluations across institutions remain limited, and the impact of annotation guidelines on reported performance is not well understood. In this work, we present a cross-domain study of negation detection that contrasts deterministic cue–scope rules with contextual sequence tagging, analyzing errors by cue type, distance, and section of note. We further release an annotation schema aligned with common clinical documentation patterns to facilitate reproducible comparisons.",
    "reason": "Mentions a specific prior system (NegEx) and claims its influence without providing a citation; first mentions of named methods/datasets should be cited (rule a).",
    "start": 581,
    "end": 683,
    "label": "Unsupported_claim"
  },
  {
    "span": "Evasion attacks perturb inputs to fool classifiers (Szegedy et al., 2014). Poisoning attacks corrupt training data (Biggio et al., 2012). Certified defenses bound worst-case risk (Cohen et al., 2019). Adversarial training improves robustness empirically (Madry et al., 2018).",
    "document": "Related Work\n\nAdversarial robustness research spans attack taxonomies, defense mechanisms, and certified guarantees. Understanding the trade-offs among accuracy, robustness, and efficiency is crucial for deployment in safety-critical domains. We aim to reconcile empirical and certified robustness under realistic threat models.\n\nEvasion attacks perturb inputs to fool classifiers (Szegedy et al., 2014). Poisoning attacks corrupt training data (Biggio et al., 2012). Certified defenses bound worst-case risk (Cohen et al., 2019). Adversarial training improves robustness empirically (Madry et al., 2018).\n\nHowever, few approaches provide unified evaluations across evasion and poisoning with distribution shifts and constrained budgets. We propose a benchmark and a modular defense that adapts to threat uncertainty.",
    "reason": "The span lists attack and defense categories in isolation without transitions or explanation of how they relate to one another, leading to abrupt, incoherent progression.",
    "start": 330,
    "end": 605,
    "label": "Coherence"
  },
  {
    "span": "Anderson et al. (2018) released R2R for vision-and-language navigation. Fried et al. (2018) improved navigation with speaker-follower models. Shridhar et al. (2020) examined ALFRED for instruction following in household tasks. Gan et al. (2020) explored procedural reasoning in multi-step environments.",
    "document": "Introduction\n\nEmbodied agents that follow natural language instructions must ground semantics in visual perception and action. Progress has been catalyzed by datasets and models that couple navigation or manipulation objectives with free-form language, enabling study of generalization to new goals and scenes. Despite advances, agents often overfit to spurious cues and fail to handle compositional instructions.\n\nAnderson et al. (2018) released R2R for vision-and-language navigation. Fried et al. (2018) improved navigation with speaker-follower models. Shridhar et al. (2020) examined ALFRED for instruction following in household tasks. Gan et al. (2020) explored procedural reasoning in multi-step environments.\n\nWe introduce a benchmark that disentangles spatial grounding from pragmatic inference by systematically varying referential expressions and action affordances. Our approach trains a dual-world model that forecasts both egocentric observations and instruction-conditioned progress, improving robustness to unseen layouts.",
    "reason": "The span jumps between datasets and modeling approaches across different tasks without clarifying their relationships. There are no transitions explaining how R2R relates to ALFRED or how speaker-follower connects to procedural reasoning.",
    "start": 415,
    "end": 717,
    "label": "Coherence"
  },
  {
    "span": "[Chen et al., 2020)",
    "document": "Related Work\n\nMitigating social bias in NLP spans intrinsic and extrinsic evaluations. Word embedding debiasing revealed linear subspaces encoding stereotypes (Bolukbasi et al., 2016), while contextual models show persistent bias (May et al., 2019). Dataset curation frameworks emphasize documentation and ethics (Bender and Friedman, 2018). [Chen et al., 2020) propose counterfactual data augmentation for toxicity detection, complementing adversarial training (Zhao et al., 2018). We unify these directions via a measurement–optimization loop with standardized bias benchmarks.",
    "reason": "Mismatched brackets in the citation; uses '[' with a closing ')'.",
    "start": 342,
    "end": 361,
    "label": "Format"
  },
  {
    "span": "Sequential models like LSTMs predict patient trajectories (Lipton et al., 2016). Polygenic risk scores aggregate genetic variants (Khera et al., 2018). Attention mechanisms provide interpretability (Choi et al., 2016). Differential privacy limits patient re-identification (Dwork, 2006).",
    "document": "Related Work\n\nPredictive modeling with electronic health records (EHR) has used recurrent and convolutional architectures to capture temporal patterns in diagnoses, medications, and labs (Rajkomar et al., 2018; Ma et al., 2017). Regularization and uncertainty estimation have been explored to manage label sparsity and distribution shift across institutions (Leibig et al., 2017; Zhang et al., 2020).\n\nSequential models like LSTMs predict patient trajectories (Lipton et al., 2016). Polygenic risk scores aggregate genetic variants (Khera et al., 2018). Attention mechanisms provide interpretability (Choi et al., 2016). Differential privacy limits patient re-identification (Dwork, 2006).\n\nWe propose a calibrated temporal model for early warning that leverages visit-level uncertainty while remaining compatible with standard EHR coding systems.",
    "reason": "The span juxtaposes EHR sequence models with genomics (polygenic risk scores), interpretability, and privacy in rapid succession without transitions or clarifying the relationships, resulting in poor coherence.",
    "start": 402,
    "end": 689,
    "label": "Coherence"
  },
  {
    "span": "Controllable text generation has been pursued via attribute classifiers, control codes, and plug-and-play steering of pretrained decoders (Hu et al., 2017; Keskar et al., 2019; Dathathri et al., 2020). Discrete and soft constraints enforce style, toxicity, or topicality with varying trade-offs in fluency and control strength (Krause et al., 2020; Liu et al., 2021).",
    "document": "Introduction\n\nLarge language models can produce fluent text but often fail to respect desired attributes such as style or safety. Controllability methods aim to steer generation without sacrificing coherence or coverage.\n\nControllable text generation has been pursued via attribute classifiers, control codes, and plug-and-play steering of pretrained decoders (Hu et al., 2017; Keskar et al., 2019; Dathathri et al., 2020). Discrete and soft constraints enforce style, toxicity, or topicality with varying trade-offs in fluency and control strength (Krause et al., 2020; Liu et al., 2021).\n\nWe present posterior-guided decoding, which calibrates token-level preferences using a lightweight attribute posterior estimated from counterfactual samples. The approach composes multiple controls, maintains diversity, and requires no retraining of the base model.",
    "reason": "The span lists categories of controllability methods and attributes without relating them to the paper’s approach or articulating a concrete gap, so it lacks synthesis per (a) and (c).",
    "start": 222,
    "end": 589,
    "label": "Lacks_synthesis"
  },
  {
    "span": "the M4 dataset contains over 100,000 series",
    "document": "Related Work\n\nTime-Series Forecasting Benchmarks\n\nProgress in forecasting has been driven by systematic evaluation on diverse collections of real-world series. Classical statistical baselines, global models, and deep sequence architectures have all benefited from large-scale benchmarks that include a variety of seasonalities and horizons.\n\nAmong these resources, the M4 dataset contains over 100,000 series and is frequently used to assess the generalization of global forecasting models across domains. Researchers have also introduced domain-specific corpora for energy, traffic, and retail to evaluate scenario-dependent performance.\n\nOur experiments complement these efforts by focusing on cold-start categories and by stress-testing models under abrupt regime changes.",
    "reason": "Provides a specific statistic about a well-known dataset at first mention without any citation or source.",
    "start": 365,
    "end": 408,
    "label": "Unsupported_claim"
  },
  {
    "span": "On CIFAR-10-C, most recent methods report over 85% accuracy under moderate corruption.",
    "document": "Introduction\n\nEvaluating robustness to distribution shift is essential for deploying vision models in safety-critical settings. Common image corruption benchmarks test resilience to noise, blur, weather, and digital artifacts. On CIFAR-10-C, most recent methods report over 85% accuracy under moderate corruption. However, these aggregate numbers can obscure severe weaknesses on specific corruption types. We argue for per-corruption analysis and propose a calibration-aware training scheme that improves worst-case performance.",
    "reason": "Claims performance levels for 'most recent methods' on a named benchmark without providing citations (definition a, d, and e).",
    "start": 227,
    "end": 313,
    "label": "Unsupported_claim"
  },
  {
    "span": "Large language models have been leveraged for automated program repair via prompt-based code generation, iterative refinement, and search over candidate patches (Chen et al., 2021; Fried et al., 2022; Zhu et al., 2022; Xia et al., 2023).",
    "document": "Related Work\n\nAutomated program repair with LLMs. Code LLMs can propose patches conditioned on failing tests or bug summaries. Large language models have been leveraged for automated program repair via prompt-based code generation, iterative refinement, and search over candidate patches (Chen et al., 2021; Fried et al., 2022; Zhu et al., 2022; Xia et al., 2023). Some systems combine static analysis signals or unit-test feedback to filter candidates.\n\nEvaluation practices. Benchmarks vary in domain, test quality, and patch correctness criteria, leading to inconsistent comparisons.\n\nWe present a repair framework that couples test synthesis with verification-guided decoding to improve patch validity across repositories.\n",
    "reason": "The span summarizes prior approaches without clarifying their shortcomings or how they relate to the proposed framework; it lacks an explicit gap or perspective, meeting the lack of synthesis criteria.",
    "start": 127,
    "end": 364,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT was used in an AES task trained on essays from multiple prompts with adversarial data augmentation.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) aims to predict holistic or analytic scores for student writing at scale and with high reliability. Early systems relied on handcrafted lexical, syntactic, and discourse features combined with linear or tree-based models. With the rise of large pre-trained encoders, neural approaches have become prevalent for both prompt-specific and cross-prompt scoring. BERT was used in an AES task trained on essays from multiple prompts with adversarial data augmentation. Recent methods also explore domain adaptation across prompts, calibration to reduce score inflation, and robustness to off-topic or incoherent inputs. Despite progress, generalization to unseen prompts and resilience against strategic writing remain open challenges.\n\nIn this work, we build on neural AES architectures and examine strategies to improve cross-prompt transfer without sacrificing interpretability. We also analyze how model confidence relates to rubric-aligned criteria such as organization, development, and language use.",
    "reason": "Claims a specific prior setup (BERT used for AES with adversarial augmentation across prompts) without citing the study that introduced it.",
    "start": 402,
    "end": 506,
    "label": "Unsupported_claim"
  },
  {
    "span": "For tabular credit risk modeling, post-hoc explanations such as SHAP and LIME are widely used (Ribeiro et al., 2016; Lundberg and Lee, 2017; Molnar, 2020).",
    "document": "Related Work\nRegulatory scrutiny in finance has heightened interest in explainable AI (XAI) for credit scoring. Models must be both accurate and interpretable to satisfy audit and compliance requirements.\n\nFor tabular credit risk modeling, post-hoc explanations such as SHAP and LIME are widely used (Ribeiro et al., 2016; Lundberg and Lee, 2017; Molnar, 2020). Global surrogates and rule lists have also been explored (Fugelstad et al., 2019; Lakkaraju et al., 2020), while counterfactuals provide actionable recourse (Ustun et al., 2019; Karimi et al., 2020). We conduct experiments on standard credit datasets and assess stability under data drifts.",
    "reason": "The sentence summarizes tools and citations without clarifying how these methods relate to the paper’s goals, what limitations they have in credit risk, or the motivation for the authors’ approach.",
    "start": 206,
    "end": 361,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Vaswani et al., 2017)",
    "document": "Related Work\n\nTransformer architectures have reshaped sequence modeling by emphasizing self-attention over recurrence (Bahdanau et al., 2015; Parikh et al., 2016). In (Vaswani et al., 2017) the Transformer is introduced as a fully attention-based encoder-decoder, enabling parallel training and long-range dependency modeling. Subsequent work extends this framework to language modeling (Radford et al., 2019) and vision tasks (Dosovitskiy et al., 2021). However, lightweight variants still struggle on low-resource devices (Touvron et al., 2021), motivating structure-aware compression strategies.",
    "reason": "Wrong citation style: the preposition “In” should not precede a parenthetical citation. It should be “In Vaswani et al. (2017), …” or simply “(Vaswani et al., 2017)”.",
    "start": 164,
    "end": 189,
    "label": "Format"
  },
  {
    "span": "The M5 competition has demonstrated the superiority of gradient boosting methods",
    "document": "Introduction\n\nLarge-scale demand forecasting is pivotal for inventory management and supply chain optimization. Benchmark competitions have accelerated progress by providing standardized datasets and evaluation protocols.\n\nThe M5 competition has demonstrated the superiority of gradient boosting methods. Motivated by these observations, many practitioners adopt tree-based ensembles with elaborate feature engineering for retail demand prediction.\n\nWe revisit this practice by introducing a probabilistic transformer with hierarchical reconciliation and quantile-aware losses, showing improved accuracy and calibrated uncertainty on retail datasets without heavy feature engineering.\n",
    "reason": "References a specific competition and a comparative conclusion without citing any source; per rule (a) and (b), first mentions of a competition and claims about its findings require citations.",
    "start": 223,
    "end": 303,
    "label": "Unsupported_claim"
  },
  {
    "span": "Vision Transformers scale with dataset and model size to achieve strong performance (Dosovitskiy et al., 2020). MoCo maintains a momentum encoder for contrastive learning (He et al., 2020). Clustering-based pretraining discovers semantically coherent groups (Caron et al., 2018).",
    "document": "Related Work\n\nSelf-Supervised Learning for Visual Recognition\n\nSelf-supervised pretraining reduces annotation costs by learning from raw images via surrogate tasks or instance discrimination. Early work explored context prediction and colorization (Doersch et al., 2015; Zhang et al., 2016). Contrastive methods and distillation without negatives now dominate large-scale pretraining (Chen et al., 2020; Grill et al., 2020). Vision Transformers scale with dataset and model size to achieve strong performance (Dosovitskiy et al., 2020). MoCo maintains a momentum encoder for contrastive learning (He et al., 2020). Clustering-based pretraining discovers semantically coherent groups (Caron et al., 2018). Transfer learning studies examine linear probe and fine-tuning across tasks (Kornblith et al., 2019; Kolesnikov et al., 2020).",
    "reason": "The span enumerates different approaches and architectures without transitions or an explanation of their interconnections, making the relation between the cited works unclear.",
    "start": 425,
    "end": 704,
    "label": "Coherence"
  },
  {
    "span": "BERT was used in an early fake news shared task trained on headlines only.",
    "document": "Related Work\n\nAutomated misinformation detection has evolved from stylometric and lexicon-based approaches to deep neural architectures that incorporate content, source, and social context signals. Early systems often focused on headline-body stance, while later models integrated propagation patterns and user credibility.\n\nShared tasks and benchmarks have played a significant role in standardizing evaluation for rumor verification and fake news classification. BERT was used in an early fake news shared task trained on headlines only. More recent efforts explore multimodal misinformation with images and videos, raising new challenges in cross-modal grounding and adversarial robustness.\n\nOur study focuses on robustness to domain shift in news content. We propose a modular pipeline that separates claim detection from veracity assessment and introduce a domain-adaptive pretraining regime that leverages weak supervision from archival news sources.",
    "reason": "Mentions a specific shared task setup and the use of a particular model without any citation (violates rule a; first mention of a shared task and setup requires a reference).",
    "start": 465,
    "end": 539,
    "label": "Unsupported_claim"
  },
  {
    "span": "Baker et al., (2014)",
    "document": "Introduction\n\nProgram synthesis from examples has leveraged neural sequence models and symbolic priors to generalize from limited supervision (Devlin et al., 2017; Ellis et al., 2018). Baker et al., (2014) argued that Bayesian program induction can capture strong inductive biases for compositional reasoning, inspiring hybrid neuro-symbolic methods (Creswell et al., 2020; Nye et al., 2021). Recent approaches integrate differentiable interpreters and constraint solvers to balance expressivity with verifiability (Lamb et al., 2020; Bunel et al., 2018).\n\nDespite impressive benchmarks, generalization to out-of-distribution specifications remains limited (Lake and Baroni, 2018; Hendrycks et al., 2021). Advances in search-guided training and curriculum design have improved sample efficiency, but scaling to realistic DSLs requires better pruning and uncertainty estimation (Balog et al., 2017; Shi et al., 2020).\n\nWe propose a lightweight posterior regularizer that shapes the synthesis distribution toward semantically minimal edits, reducing spurious programs while preserving completeness. Our experiments cover string transformation and list-processing DSLs under perturbations and noisy I/O pairs.",
    "reason": "Superfluous comma before the parenthetical year in a narrative citation; should be 'Baker et al. (2014)'.",
    "start": 185,
    "end": 205,
    "label": "Format"
  },
  {
    "span": "End-to-end models have surpassed hybrid HMM-DNN systems on LibriSpeech in both clean and noisy conditions.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN pipelines to fully neural end-to-end architectures, including CTC, attention-based encoder–decoders, and transducers. End-to-end models have surpassed hybrid HMM-DNN systems on LibriSpeech in both clean and noisy conditions. Nevertheless, their performance can degrade under domain shift and low-resource accents.\n\nTo address these issues, we investigate pronunciation-aware adapters that inject lexicon structure into end-to-end models without sacrificing differentiability. Our approach augments the encoder with phoneme-informed residual modules and employs a mixed CTC/attention loss.\n\nWe evaluate on LibriSpeech and cross-corpus settings, analyzing WER, robustness to additive noise, and adaptation to unseen speakers.",
    "reason": "Asserts a comparative performance claim on a specific dataset without citing supporting studies.",
    "start": 204,
    "end": 310,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Kim and Lee, 2020",
    "document": "Introduction\n\nGraph neural networks (GNNs) generalize convolutional operations to non-Euclidean domains and have achieved state-of-the-art performance on node classification, link prediction, and graph classification tasks (Bronstein et al., 2017; Hamilton et al., 2017). Early pooling methods (Kim and Lee, 2020 simplified hierarchical structure by repeatedly coarsening the graph, but suffered from oversmoothing on deep stacks. Subsequent approaches introduced attention-based pooling and learned cluster assignments to preserve task-relevant structure (Ying et al., 2018; Lee et al., 2019).\n\nWe propose an adaptive pooling layer that estimates uncertainty-aware assignments, which improves robustness on noisy and heterophilic graphs. Experiments on seven benchmarks show consistent gains over prior art while maintaining linear scalability.",
    "reason": "Missing closing parenthesis in a parenthetical citation; should be '(Kim and Lee, 2020)'.",
    "start": 294,
    "end": 312,
    "label": "Format"
  },
  {
    "span": "(Lopez and Kim, 2017",
    "document": "Introduction\n\nDomain adaptation for neural machine translation has leveraged data selection, fine-tuning, and multi-domain training to mitigate distribution shift (Chu and Wang, 2018; Britz et al., 2017). Recent work explores control tokens and mixture-of-experts to encode domain signals and avoid catastrophic forgetting (Johnson et al., 2017; Gu et al., 2019). Semi-supervised objectives (Lopez and Kim, 2017 combine back-translation with consistency regularization across domains, yielding improvements in low-resource settings. We complement these ideas with a curriculum that orders batches by domain similarity and uncertainty, leading to more robust convergence on noisy corpora (Sennrich et al., 2016; Edunov et al., 2018). Our experiments cover news, biomedical, and patent test sets with both in-domain and out-of-domain evaluations.\n",
    "reason": "Missing closing parenthesis in a parenthetical citation; it should be “(Lopez and Kim, 2017)” with both opening and closing parentheses.",
    "start": 391,
    "end": 411,
    "label": "Format"
  },
  {
    "span": "Robust ASR has been improved through multi-condition training and data augmentation such as SpecAugment and noise injection (Ko et al., 2015; Park et al., 2019), front-end enhancement and beamforming (Erdogan et al., 2015; Heymann et al., 2016), and self-supervised pretraining on unlabeled audio (Schneider et al., 2019; Baevski et al., 2020).",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) in the wild must contend with noise, reverberation, and domain mismatch. Robustness techniques span data curation, architectural modifications, and training objectives.\n\nRobust ASR has been improved through multi-condition training and data augmentation such as SpecAugment and noise injection (Ko et al., 2015; Park et al., 2019), front-end enhancement and beamforming (Erdogan et al., 2015; Heymann et al., 2016), and self-supervised pretraining on unlabeled audio (Schneider et al., 2019; Baevski et al., 2020).\n\nOur approach jointly adapts representations and decoders via test-time entropy minimization with consistency regularization, requiring no access to source-domain data during deployment. We assess performance across multiple noisy corpora and accents.\n",
    "reason": "The span aggregates prior robustness strategies without tying them to the proposed test-time adaptation or specifying what unresolved issue remains.",
    "start": 219,
    "end": 563,
    "label": "Lacks_synthesis"
  },
  {
    "span": "most recent works build a two-stage retriever-reader system to tackle the problem.",
    "document": "Related Work\n\nOpen-Domain Question Answering\n\nIn this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).\n\nGenerative Readers\n\nCompared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.\n\nPointer-Generator Network\n\nPointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. It has been frequently used in natural language tasks like summarization (Gu et al., 2016;See et al.,  Gehrmann et al., 2018) and neural machine translation (Luong et al., 2014;Gu et al., 2017), but its application to ODQA has been less explored.\n\n ",
    "start": 311,
    "end": 392,
    "label": "Unsupported_claim"
  },
  {
    "span": "FiD model",
    "document": "Related Work\n\nOpen-Domain Question Answering\n\nIn this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).\n\nGenerative Readers\n\nCompared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.\n\nPointer-Generator Network\n\nPointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. It has been frequently used in natural language tasks like summarization (Gu et al., 2016;See et al.,  Gehrmann et al., 2018) and neural machine translation (Luong et al., 2014;Gu et al., 2017), but its application to ODQA has been less explored.\n\n ",
    "start": 1719,
    "end": 1727,
    "label": "Unsupported_claim"
  },
  {
    "span": "Raffel et al., 2019)",
    "document": "Related Work\n\nOpen-Domain Question Answering\n\nIn this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).\n\nGenerative Readers\n\nCompared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.\n\nPointer-Generator Network\n\nPointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. It has been frequently used in natural language tasks like summarization (Gu et al., 2016;See et al.,  Gehrmann et al., 2018) and neural machine translation (Luong et al., 2014;Gu et al., 2017), but its application to ODQA has been less explored.\n\n ",
    "start": 863,
    "end": 882,
    "label": "Format"
  },
  {
    "span": "See et al.,",
    "document": "Related Work\n\nOpen-Domain Question Answering\n\nIn this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).\n\nGenerative Readers\n\nCompared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.\n\nPointer-Generator Network\n\nPointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. It has been frequently used in natural language tasks like summarization (Gu et al., 2016;See et al.,  Gehrmann et al., 2018) and neural machine translation (Luong et al., 2014;Gu et al., 2017), but its application to ODQA has been less explored.\n\n ",
    "start": 1914,
    "end": 1924,
    "label": "Format"
  },
  {
    "span": "The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs.",
    "document": "Related Work\n\nOpen-Domain Question Answering\n\nIn this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).\n\nGenerative Readers\n\nCompared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.\n\nPointer-Generator Network\n\nPointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. It has been frequently used in natural language tasks like summarization (Gu et al., 2016;See et al.,  Gehrmann et al., 2018) and neural machine translation (Luong et al., 2014;Gu et al., 2017), but its application to ODQA has been less explored.\n\n ",
    "start": 394,
    "end": 735,
    "label": "Coherence"
  },
  {
    "span": "and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.",
    "document": "Related Work\n\nOpen-Domain Question Answering\n\nIn this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).\n\nGenerative Readers\n\nCompared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.\n\nPointer-Generator Network\n\nPointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. It has been frequently used in natural language tasks like summarization (Gu et al., 2016;See et al.,  Gehrmann et al., 2018) and neural machine translation (Luong et al., 2014;Gu et al., 2017), but its application to ODQA has been less explored.\n\n ",
    "start": 1225,
    "end": 1857,
    "label": "Coherence"
  },
  {
    "span": "Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).",
    "document": "Related Work\n\nOpen-Domain Question Answering\n\nIn this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).\n\nGenerative Readers\n\nCompared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.\n\nPointer-Generator Network\n\nPointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. It has been frequently used in natural language tasks like summarization (Gu et al., 2016;See et al.,  Gehrmann et al., 2018) and neural machine translation (Luong et al., 2014;Gu et al., 2017), but its application to ODQA has been less explored.\n\n ",
    "start": 263,
    "end": 981,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Compared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.",
    "document": "Related Work\n\nOpen-Domain Question Answering\n\nIn this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).\n\nGenerative Readers\n\nCompared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.\n\nPointer-Generator Network\n\nPointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. It has been frequently used in natural language tasks like summarization (Gu et al., 2016;See et al.,  Gehrmann et al., 2018) and neural machine translation (Luong et al., 2014;Gu et al., 2017), but its application to ODQA has been less explored.\n\n ",
    "start": 1004,
    "end": 1857,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Tanaka 2018)",
    "document": "Related Work\n\nRepresentation learning for time series seeks to extract features invariant to temporal distortions and noise. Classical methods rely on DTW and SAX-based indexes for similarity search (Keogh and Ratanamahatana, 2005; Lin et al., 2007). Recently, contrastive and forecasting-based pretraining have gained traction (Franceschi et al., 2019; Eldele et al., 2021). Prior work has analyzed the role of seasonality and trend decomposition (Smith, 2016; Tanaka 2018) in improving downstream accuracy. We complement these approaches with an augmentation pipeline that preserves alignment while diversifying local patterns.",
    "reason": "Missing comma between author and year in a parenthetical citation; should be 'Tanaka, 2018'.",
    "start": 462,
    "end": 474,
    "label": "Format"
  },
  {
    "span": "García et al. 2",
    "document": "Related Work\n\nGraph neural networks (GNNs) generalize convolution to irregular structures (Bronstein et al., 2017). Spectral methods leverage Laplacian operators for localized filtering, while spatial methods aggregate neighborhood information (Hamilton et al., 2017). García et al. 2 extend attention mechanisms to multi-hop neighborhoods, but their approach incurs substantial memory overhead on dense graphs. Recent works propose sampling strategies to mitigate this cost (Chen et al., 2018; Zeng et al., 2019).",
    "reason": "Improper footnote-like usage in place of a proper citation; should include the publication year or be formatted as a proper footnote. For example, “García et al. (YEAR)” or “García et al. [footnote number]” with corresponding footnote content.",
    "start": 269,
    "end": 284,
    "label": "Format"
  },
  {
    "span": "Code2Vec learns distributed representations from AST paths (Alon et al., 2019). Static taint analysis tracks information flow in programs (Sabelfeld and Myers, 2003). Pre-training on masked tokens improves defect prediction (Wang et al., 2020).",
    "document": "Related Work\n\nLearning from Source Code. Neural models exploit structural and semantic cues via ASTs, control-flow graphs, and token sequences to support tasks like summarization, code search, and bug detection (Allamanis et al., 2018; Feng et al., 2020).\n\nStatic Analysis and ML for Bugs. Traditional program analyses detect classes of bugs with soundness guarantees but limited scalability, while data-driven methods capture statistical regularities yet may miss rare patterns (Johnson and Ernst, 2013; Pradel and Sen, 2018). Code2Vec learns distributed representations from AST paths (Alon et al., 2019). Static taint analysis tracks information flow in programs (Sabelfeld and Myers, 2003). Pre-training on masked tokens improves defect prediction (Wang et al., 2020). We aim to combine structural invariants with learned representations.\n\nHybrid Approaches. Recent hybrid systems use analysis-derived features as inputs to neural networks or constrain generation with symbolic checks (Hellendoorn et al., 2020; Bielik et al., 2016). Our framework distills path- and flow-sensitive signals into a transformer encoder for precise bug localization.",
    "reason": "The span lists Code2Vec, taint analysis, and masked-token pre-training with no connective explanation; the relationships are implied but not stated, and transitions are absent.",
    "start": 528,
    "end": 772,
    "label": "Coherence"
  },
  {
    "span": "(Smith et al. 2017)",
    "document": "Related Work\n\nPolicy optimization methods such as PPO and TRPO remain popular due to their stability and sample efficiency (Schulman et al., 2015; Schulman et al., 2017). Model-based reinforcement learning complements these by improving data efficiency through learned dynamics (Chua et al., 2018; Janner et al., 2019). Exploration via intrinsic rewards has further expanded the frontier in sparse-reward tasks (Pathak et al., 2017; Burda et al., 2019). Prior work on hierarchical RL emphasizes temporal abstraction for long-horizon credit assignment (Bacon et al., 2017; Nachum et al., 2018). However, theoretical guarantees for stability under approximation errors remain limited (Smith et al. 2017), and empirical protocols vary widely across domains.\n\nOur contribution is a trust-region regularizer derived from conservative policy improvement with an adaptive penalty, closing the gap between empirical performance and theoretical constraints in continuous control.",
    "reason": "Missing comma between authors and year inside a parenthetical citation; APA-style requires \"(Smith et al., 2017)\".",
    "start": 682,
    "end": 701,
    "label": "Format"
  },
  {
    "span": "Since 2020, Transformers have dominated sequential recommendation.",
    "document": "Related Work\n\nSequential recommendation models user behaviors as ordered event streams to predict the next item of interest. Early approaches relied on Markov chains and factorization machines, followed by recurrent and convolutional architectures that capture short-term dynamics. More recently, attention mechanisms have been introduced to address long-range dependencies and variable interaction patterns.\n\nSince 2020, Transformers have dominated sequential recommendation. Despite their strong performance, these models can be computationally expensive and sensitive to sparsity in long tails. In this study, we revisit architectural trade-offs and propose a hybrid design that combines sparse attention with lightweight gating, aiming to preserve accuracy while reducing training cost.\n\nWe evaluate our approach on multiple public datasets with standardized preprocessing and report ablations on sequence length, sparsity, and cold-start severity.",
    "reason": "The sentence asserts a field-wide trend ('have dominated') without providing citations to surveys, leaderboards, or representative studies, making it an unsupported claim.",
    "start": 410,
    "end": 476,
    "label": "Unsupported_claim"
  },
  {
    "span": "Lee et al. 2",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) has rapidly advanced with sequence-to-sequence and transducer models (Graves, 2012; Graves, 2013; Hannun et al., 2019). Large-scale self-supervised pretraining further reduces labeled data requirements (Baevski et al., 2020; Hsu et al., 2021). A hybrid HMM-DNN pipeline remains competitive according to Lee et al. 2, but end-to-end architectures simplify optimization and deployment while achieving comparable accuracy. Our work studies contextual biasing mechanisms that do not require external language models (Zeyer et al., 2019).",
    "reason": "Wrong use of footnote-style numeral with an author mention; should include a year (e.g., \"Lee et al. (YEAR)\") or be formatted as a proper footnote, not \"Lee et al. 2\".",
    "start": 363,
    "end": 375,
    "label": "Format"
  },
  {
    "span": " (BERT Devlin et al. (2019))",
    "document": "Introduction\n\nMorality helps humans discern right from wrong. Pluralist moral philosophers argue that human morality can be represented, understood, and explained by a finite number of irreducible basic elements, referred to as moral values (Graham et al., 2013). The difference in our preferences over moral values explains how and why we think differently. For instance, both conservatives and liberals may agree that individual welfare is important. However, a conservative, who cherishes the values of freedom and independence, may believe that taxes should be decreased to attain more individual welfare. In contrast, a liberal, who cherishes the values of community and care, may believe that taxes should be increased to obtain welfare (Graham et al., 2009).\n\nIt is crucial to understand human morality to develop beneficial AI (Soares and Fallenstein, 2017;Russell, 2019). As artificial agents live and operate among humans (Akata et al., 2020), they must be able to comprehend and recognize the moral values that drive the differences in human behavior (Gabriel, 2020). The ability to understand moral rhetoric can be instrumental for, e.g., facilitating human-agent trust (Chhogyal et al., 2019;Mehrotra et al., 2021) and engineering value-aligned sociotechnical systems (Murukannaiah et al., 2020;Serramia et al., 2020;Montes and Sierra, 2021).\n\nThere are survey instruments to estimate individual value profiles (Schwartz, 2012;Graham et al., 2013). However, reasoning about moral values is challenging for humans (Le Dantec et al., 2009;Pommeranz et al., 2012). Further, in practical applications, e.g., to conduct meaningful conversations (Tigunova et al., 2019) or to identify online trends (Mooijman et al., 2018), artificial agents should be able to understand moral rhetoric on the fly.\n\nThe growing capabilities of natural language processing (NLP) enable the estimation of moral rhetoric from discourse Mooijman et al., 2018;Rezapour et al., 2019;Hoover et al., 2020;Araque et al., 2020). Value classifiers can be used to identify the moral values underlying a piece of text on the fly. For instance, Mooijman et al. (2018) show that detecting moral values from tweets can predict violent protests.\n\nExisting value classifiers are evaluated on a specific dataset, without re-training or testing the classifier on a different dataset. This shows the ability of the classifier to predict values from text, but not the ability to transfer the learned knowledge across datasets. A critical aspect of moral values is that they are intrinsically linked to the domain under discussion (Pommeranz et al., 2012;Liscio et al., 2021). Moral value expressions may take different forms in different domains. For example, in the driving domain, the value of safety concerns speed limits and seat belts, but in the COVID-19 domain, safety concerns social distancing and face masks. Thus, a word (broadly, language) may trigger different moral rhetoric in different domains. For example, in a libertarian blog, the word 'taxes' may be linked to the authority values, but in a socialist blog it may be linked to the community values. Then, it is crucial for a value classifier to recognize domain-specific connotations of moral rhetoric.\n\nCollecting and annotating a sufficient amount of training examples in each domain is expensive and time consuming. To reduce the need for new annotated examples, we can pretrain classifiers with similar available annotated data and transfer the acquired knowledge to a novel task-a practice known as transfer learning (Ruder, 2019). Despite the benefits, transfer learning poses wellknown challenges, including: (1) generalizability: how well does a classifier perform on novel data? (2) transferability: how well is knowledge transferred from one domain to another? and (3) catastrophic forgetting: to what extent is knowledge of a previous domain lost after training in a new domain? These challenges are crucial for value classification because of its domain-specific nature.\n\nWe perform the first comprehensive crossdomain evaluation of a value classifier. We employ the Moral Foundation Twitter Corpus (Hoover et al., 2020), consisting of seven datasets spanning different socio-political areas, annotated with the value taxonomy of the Moral Foundation Theory (Graham et al., 2013). Treating each dataset as a domain, we train a deep learning model (BERT Devlin et al. (2019)) in four training settings to evaluate the value classifier's generalizability, transferability, and catastrophic forgetting.\n\nOur experiments show that (1) a value classifier can generalize to novel domains, especially when trained on a varied array of domains, (2) initializing a classifier with examples from different domains improves performance in novel domains even when little training data is available in the novel domains, (3) catastrophic forgetting occurs even when training on a small portion of data from the novel domain, and its impact must be considered when training on a novel domain, and (4) in the large majority of cases, in all considered training settings, there is at least one annotator that agrees with the model predictions. These results provide insights to researchers and practitioners on estimating moral values in different domains.\n\n ",
    "start": 4396,
    "end": 4424,
    "label": "Format"
  },
  {
    "span": "Nguyen et. al. (2020)",
    "document": "Related Work\n\nNeural machine translation (NMT) has advanced rapidly with Transformer architectures (Vaswani et al., 2017). Domain adaptation methods include fine-tuning, data selection, and multi-domain training (Chu and Wang, 2018; Wang et al., 2020). Nguyen et. al. (2020) introduce instance weighting for domain-robust NMT, showing improvements on medical and legal corpora. Complementary to this, test-time adaptation leverages pseudo-labels to reduce domain mismatch (Sun et al., 2020). Our approach combines instance reweighting with uncertainty-aware decoding.",
    "reason": "Incorrect abbreviation formatting; should be Nguyen et al. (2020) without the period after et and with a period after al.",
    "start": 253,
    "end": 274,
    "label": "Format"
  },
  {
    "span": "prior studies have proven that gradient inversion can always reconstruct images from a single batch",
    "document": "Related Work\n\nFederated learning enables training models across decentralized data silos without centralizing raw data. Privacy concerns arise because gradient updates can leak sensitive information about local examples. A growing body of research analyzes leakage channels and proposes defenses such as secure aggregation, differential privacy, and gradient compression.\n\nSeveral attacks reconstruct inputs or membership attributes from shared gradients, highlighting nontrivial privacy risks. Building on this line of inquiry, prior studies have proven that gradient inversion can always reconstruct images from a single batch, which has raised alarms for vision applications.\n\nDefensive strategies trade off utility for privacy, and practical deployments must navigate accuracy loss, communication overhead, and regulatory constraints. Our contribution is a selective masking mechanism that targets gradients with high memorization potential, guided by a calibration phase that estimates per-parameter sensitivity. We combine this with lightweight secure aggregation to reduce leakage without fully homomorphic encryption.\n\nWe evaluate on image classification and medical imaging tasks under realistic participation patterns. Results indicate strong reductions in attack success rates with modest drops in accuracy, and we provide sensitivity analyses under varying client heterogeneity.",
    "reason": "Overgeneralized claim about proven ability of gradient inversion attacks, with no citation to substantiate the assertion.",
    "start": 529,
    "end": 628,
    "label": "Unsupported_claim"
  },
  {
    "span": "Hinton et al. (2015) introduced distillation with soft targets. Sanh et al. (2019) released DistilBERT as a compact transformer. Jiao et al. (2020) presented TinyBERT with multi-stage distillation. Wang et al. (2020) studied patient knowledge distillation for transformers.",
    "document": "Related Work\n\nModel Compression and Distillation\nLarge transformers impose latency and memory costs, motivating compression via pruning, quantization, and knowledge distillation (KD) (Han et al., 2016; Cheng et al., 2017). KD transfers knowledge from a teacher to a student model using soft targets, intermediate features, or attention maps.\n\nDistillation for NLP Transformers\nSeveral lines adapt KD to language understanding and generation tasks but differ in objectives and layers matched. Hinton et al. (2015) introduced distillation with soft targets. Sanh et al. (2019) released DistilBERT as a compact transformer. Jiao et al. (2020) presented TinyBERT with multi-stage distillation. Wang et al. (2020) studied patient knowledge distillation for transformers.\n\nTask-Aware and Dynamic KD\nRecent work explores curriculum scheduling and task-aware teachers to improve transfer efficiency (Sun et al., 2020; Rashid et al., 2021). Our approach adapts layer-wise pairing based on entropy to reduce overfitting.",
    "reason": "The span presents a sequence of cited works with no explicit ties or contrasts, leaving unclear how each method builds upon the prior or differs. The transitions are absent, resulting in low coherence.",
    "start": 492,
    "end": 765,
    "label": "Coherence"
  },
  {
    "span": "(Brown et al., 2020))",
    "document": "Related Work\n\nOpen-domain question answering typically combines dense retrieval with generative readers (Karpukhin et al., 2020; Lewis et al., 2020). Recent large language models have further improved zero-shot accuracy by leveraging parametric knowledge (Brown et al., 2020). However, reliance on parametric memory makes systems brittle when evidence contradicts memorized facts (Si et al., 2021). Prior work has explored attribution to sources (Rashkin et al., 2021) and retrieval-augmented generation (Guu et al., 2020; Izacard and Grave, 2021) to tackle hallucination. We follow the latter line with a focus on calibrating the reader given uncertain retrieval signals (Brown et al., 2020)).",
    "reason": "Extra closing parenthesis in the parenthetical citation; should be (Brown et al., 2020).",
    "start": 672,
    "end": 693,
    "label": "Format"
  },
  {
    "span": "We observe that more than 70% of failure cases arise from small objects under 16x16 pixels.",
    "document": "Introduction\n\nVision Transformers (ViTs) have recently achieved competitive accuracy in object detection by leveraging global attention and multi-scale features. However, as models grow deeper and wider, performance gains can saturate under compute and memory constraints typical of edge deployments.\n\nA key challenge for ViT-based detectors is robustness to scale variation and cluttered scenes. We observe that more than 70% of failure cases arise from small objects under 16x16 pixels. This motivates architectural changes that improve fine-grained localization without sacrificing global context.\n\nWe propose a lightweight cross-scale attention module that conditions high-resolution features on coarse semantic tokens. Experiments demonstrate consistent improvements on standard detection benchmarks under strict FLOP and latency budgets.\n",
    "reason": "The sentence presents a specific quantitative statistic about failure cases without supporting evidence, citation, or a pointer to a figure/table.",
    "start": 397,
    "end": 488,
    "label": "Unsupported_claim"
  },
  {
    "span": "Kipf and Welling (2017) show semi-supervised node classification with GCNs. Ying et al. (2019) propose GNNExplainer for explanations. Xu et al. (2019) study the expressiveness of GINs. Yuan et al. (2020) survey explainability for graphs.",
    "document": "Related Work\n\nGraph Neural Networks and Explainability\n\nWe review predictive GNNs and methods for interpreting them. Kipf and Welling (2017) show semi-supervised node classification with GCNs. Ying et al. (2019) propose GNNExplainer for explanations. Xu et al. (2019) study the expressiveness of GINs. Yuan et al. (2020) survey explainability for graphs. Causal feature attribution on graphs is an emerging direction (Lin et al., 2021; Agarwal et al., 2022).\n\nWe aim to produce faithful, stable subgraph rationales with uncertainty estimates.",
    "reason": "The sentences shift among classification, explanation, and expressiveness results without transitions, and the relationships between these contributions are not made explicit.",
    "start": 117,
    "end": 354,
    "label": "Coherence"
  },
  {
    "span": "Unsupervised time-series anomaly detection leverages reconstruction-based autoencoders, probabilistic VAEs, GAN critics, and predictive models to flag deviations (Malhotra et al., 2016; An and Cho, 2015; Schlegl et al., 2017; Hundman et al., 2018). Recent work applies Transformers and graph temporal networks to capture long-range and cross-sensor dependencies (Wu et al., 2020; Chen et al., 2021).",
    "document": "Related Work\n\nDetecting anomalies in multivariate time series is critical for monitoring industrial systems and cyber-physical infrastructure. Models must capture normal dynamics and identify rare deviations with minimal supervision.\n\nUnsupervised time-series anomaly detection leverages reconstruction-based autoencoders, probabilistic VAEs, GAN critics, and predictive models to flag deviations (Malhotra et al., 2016; An and Cho, 2015; Schlegl et al., 2017; Hundman et al., 2018). Recent work applies Transformers and graph temporal networks to capture long-range and cross-sensor dependencies (Wu et al., 2020; Chen et al., 2021).\n\nWe propose a contrastive forecasting framework with adaptive seasonal differencing and topology-aware augmentations, improving precision–recall under regime changes. On three public benchmarks and a proprietary sensor suite, our method outperforms recent Transformer baselines.",
    "reason": "This span summarizes model families and recent architectural trends but does not connect them to the proposed contrastive framework or specify unresolved issues, thus lacking synthesis (a, c).",
    "start": 235,
    "end": 634,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Temporal knowledge graph reasoning models incorporate time via interval- and event-based embeddings (Jiang et al., 2016; Goel et al., 2020; Xu et al., 2020). Time series forecasting considers autoregressive and transformer-based predictors (Zhou et al., 2021; Lim et al., 2021). Static knowledge graph embeddings optimize translational or bilinear objectives (Bordes et al., 2013; Trouillon et al., 2016).",
    "document": "Related Work\n\nTemporal Knowledge Graphs and Inference\n\nTemporal knowledge graphs (TKGs) extend static KGs with timestamps to represent evolving facts and enable time-aware querying (Leblay and Chekol, 2018; Trivedi et al., 2017). Methods differ in how they encode temporal order, duration, and recurrence while maintaining scalability.\n\nTemporal knowledge graph reasoning models incorporate time via interval- and event-based embeddings (Jiang et al., 2016; Goel et al., 2020; Xu et al., 2020). Time series forecasting considers autoregressive and transformer-based predictors (Zhou et al., 2021; Lim et al., 2021). Static knowledge graph embeddings optimize translational or bilinear objectives (Bordes et al., 2013; Trouillon et al., 2016).\n\nRecent work also explores graph neural architectures with temporal message passing and memory modules for extrapolating future edges (Wu et al., 2020; Zhu et al., 2021). However, principled evaluation protocols that separate interpolation from extrapolation across time remain underdeveloped.",
    "reason": "The paragraph jumps from temporal KG embeddings to generic time series forecasting and then to static KGE without transitions, failing to articulate how these areas connect, which reduces coherence across the cited works.",
    "start": 337,
    "end": 742,
    "label": "Coherence"
  },
  {
    "span": "MS MARCO contains over 8.8 million passages and approximately one million queries.",
    "document": "Introduction\n\nDense retrieval has emerged as a strong alternative to sparse term-matching for passage search. Training such models typically relies on large-scale datasets that provide question–passage pairs and relevance annotations. MS MARCO contains over 8.8 million passages and approximately one million queries. Despite broad adoption, the community lacks consensus on how to balance recall and latency when deploying dense retrievers in real-world systems. We examine index size, negative sampling, and late interaction as practical levers for efficient retrieval.\n",
    "reason": "Provides specific dataset statistics without citing the dataset paper or official documentation at first mention.",
    "start": 235,
    "end": 317,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Nguyen et al.), 2016",
    "document": "Introduction\n\nEstimating causal effects from observational data requires strong identification assumptions. Propensity score methods (Rosenbaum and Rubin, 1983) and doubly robust estimators (Bang and Robins, 2005) are widely applied. Several methods exist (Nguyen et al.), 2016 that adjust for unobserved confounding via instrumental variables (Angrist and Imbens, 1995), though sensitivity remains a concern (Rosenbaum, 2002).",
    "reason": "Year placed outside the parenthetical citation; should be “(Nguyen et al., 2016)”.",
    "start": 256,
    "end": 277,
    "label": "Format"
  },
  {
    "span": "A number of studies have reported F1 exceeding 90% under 5-shot conditions.",
    "document": "Introduction\n\nFew-shot named entity recognition (NER) seeks to generalize label semantics from a handful of annotated examples, reducing the cost of domain adaptation (Fritzler et al., 2019; Yang and Katiyar, 2020). Meta-learning and prompt-based techniques have recently shown promise by leveraging label descriptions and contextual cues to bridge domain gaps (Cui et al., 2021; Hou et al., 2020). Nevertheless, performance remains sensitive to entity boundary ambiguity and label imbalance.\n\nBenchmarks typically organize episodes with support and query sets sampled across domains to mimic real deployment scenarios (Ding et al., 2021). Transfer from general-domain encoders to biomedical or financial text often leads to brittle span detection and poor calibration (Liu et al., 2022). A number of studies have reported F1 exceeding 90% under 5-shot conditions. However, these reports frequently rely on entity type leakage or constrained evaluation settings that differ from realistic open-domain conditions.\n\nWe propose a contrastive calibration framework that decouples span detection from type assignment, coupled with an episode-level selection strategy to reduce label leakage effects.",
    "reason": "The claim references prior empirical results in the literature but provides no citations or evidence supporting the specific performance numbers.",
    "start": 789,
    "end": 864,
    "label": "Unsupported_claim"
  },
  {
    "span": "Secure aggregation masks client updates using cryptographic protocols (Bonawitz et al., 2017). Client drift arises from non-IID data and slows convergence (Karimireddy et al., 2020).",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative model training without centralizing raw data, but real-world deployments must address privacy, communication efficiency, and statistical heterogeneity. The interplay among these concerns determines practical performance.\n\nPrivacy-preserving FL incorporates secure aggregation and differential privacy to reduce information leakage from updates (Bonawitz et al., 2017; McMahan et al., 2018). Compression and sparsification techniques reduce bandwidth while maintaining accuracy (Konecnỳ et al., 2016; Alistarh et al., 2017). Secure aggregation masks client updates using cryptographic protocols (Bonawitz et al., 2017). Client drift arises from non-IID data and slows convergence (Karimireddy et al., 2020). Mitigation strategies include variance reduction, proximal terms, and personalization layers (Li et al., 2020; Deng et al., 2020).\n\nWe introduce a privacy-accounted personalized optimizer that jointly controls sensitivity and drift by adapting proximal strength to client-specific gradient variance.",
    "reason": "The span lists secure aggregation and client drift back-to-back without bridging why a cryptographic mechanism is discussed alongside an optimization/statistical issue, making the connection unclear.",
    "start": 584,
    "end": 766,
    "label": "Coherence"
  },
  {
    "span": "Mori et al. 2",
    "document": "Related Work\n\nDocument layout understanding combines visual structure with linguistic cues to segment and label pages (Xu et al., 2020; Li et al., 2021). Early systems relied on hand-crafted rules over connected components, while modern approaches use multimodal transformers to fuse token embeddings and region features (Appalaraju et al., 2021; Garncarek et al., 2021). Mori et al. 2 propose a hierarchical tagging scheme for scientific articles, but their evaluation conflates figures and tables, complicating cross-paper comparisons.\n\nSubsequent research introduced richer region-graph interactions and pretraining on scanned corpora, improving robustness to OCR noise (Huang et al., 2022; Wang et al., 2021). Weak supervision via distant labels from PDF metadata has scaled training to millions of pages (Zhong et al., 2019). Despite progress, domain shifts across publishers and languages limit generalization (Pfitzmann et al., 2022).\n\nWe unify text, vision, and geometry through a decoupled encoder with adaptive fusion, emphasizing calibration and error localization. Our benchmarks include diverse document types, with careful harmonization of label spaces to ensure comparability across datasets.",
    "reason": "Improper use of a footnote-like numeral with an author-year style; should include a year (e.g., 'Mori et al. (2020)') or be formatted as a proper footnote.",
    "start": 372,
    "end": 385,
    "label": "Format"
  },
  {
    "span": "A broad set of defenses has been investigated, including adversarial training (Madry et al., 2018), randomized smoothing (Cohen et al., 2019), certified defenses via convex relaxations (Wong and Kolter, 2018), and input transformations such as JPEG compression or bit-depth reduction (Guo et al., 2018).",
    "document": "Related Work\n\nResearch on adversarial robustness in computer vision has examined both attack generation and model defenses. While gradient-based attacks expose brittle decision boundaries, certified methods aim to provide formal guarantees against norm-bounded perturbations.\n\nA broad set of defenses has been investigated, including adversarial training (Madry et al., 2018), randomized smoothing (Cohen et al., 2019), certified defenses via convex relaxations (Wong and Kolter, 2018), and input transformations such as JPEG compression or bit-depth reduction (Guo et al., 2018).\n\nIn this study, we explore a training-time regularizer that aligns margin growth with representation compression, aiming to improve both empirical and certified robustness. Our method is orthogonal to attack procedures and can be combined with standard adversarial training.",
    "reason": "Summarizes prior defenses as a list without connecting them to the current study, clarifying limitations, or positioning the new method relative to these defenses.",
    "start": 277,
    "end": 580,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The CNN/DailyMail dataset remains the de facto benchmark for single-document news summarization.",
    "document": "Introduction\n\nAbstractive text summarization aims to produce concise summaries that capture the salient content of a source article. Early neural approaches leveraged sequence-to-sequence models with attention to learn content selection and surface realization jointly, and subsequent work introduced pointer mechanisms to better copy factual details from the source while reducing repetition. Transformer-based encoders and decoders further improved faithfulness and fluency by modeling long-range dependencies with self-attention and large-scale pretraining. The CNN/DailyMail dataset remains the de facto benchmark for single-document news summarization. Although larger web-scale corpora have recently become available, news summarization continues to be a stress test for controllability and factual consistency. In this work, we investigate whether lightweight alignment objectives can improve entity grounding without increasing model size, focusing on robustness to headline-style inputs and distribution shift.",
    "reason": "First mention of a specific dataset used as a benchmark lacks a citation.",
    "start": 561,
    "end": 657,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior work shows that exposure disparity is the primary driver of popularity bias.",
    "document": "Introduction\n\nFairness in recommender systems encompasses concerns about how items and providers receive exposure and how users experience diversity. Popularity bias often leads to feedback loops where already-popular items become more visible and thus more frequently consumed.\n\nPrior work shows that exposure disparity is the primary driver of popularity bias. Interventions typically focus on re-ranking to balance utility and fairness, but the causal mechanisms underlying exposure dynamics remain underexplored.\n\nWe propose a counterfactual exposure estimator that disentangles algorithmic effects from user preference shifts, enabling principled fairness interventions.",
    "reason": "Asserts a specific conclusion attributed to 'prior work' without providing citations to that literature.",
    "start": 280,
    "end": 362,
    "label": "Unsupported_claim"
  },
  {
    "span": "Garcia et al. 1",
    "document": "Introduction\n\nSafety in reinforcement learning (RL) focuses on avoiding catastrophic failures during exploration and deployment (Amodei et al., 2016; Garcıa and Fernández, 2015). Constrained RL provides a principled framework for trading off reward and risk via cost limits (Altman, 1999; Achiam et al., 2017).\n\nModel-based approaches can reduce unsafe exploration by predicting hazards, but they are sensitive to model bias and compounding errors (Deisenroth and Rasmussen, 2011; Janner et al., 2019). Off-policy evaluation and conservative policy updates also mitigate risk in practice (Kumar et al., 2020; Levine et al., 2020).\n\nRecent surveys such as Garcia et al. 1 summarize shielding, recovery, and verification methods, yet do not address nonstationary environments. We extend this line of work by formulating risk as a distributional criterion with adaptive thresholds (Bellemare et al., 2017; Dabney et al., 2018).",
    "reason": "Wrong use of footnotes: 'Garcia et al. 1' uses a bare superscript-like number instead of a proper year or a formatted footnote; should be 'Garcia et al. (YEAR)' or a correctly formatted footnote.",
    "start": 655,
    "end": 670,
    "label": "Format"
  },
  {
    "span": "(Li, 2018: Chen et al., 2019)",
    "document": "Related Work\n\nKnowledge distillation compresses large teacher models into compact students through soft-target matching and auxiliary objectives (Hinton et al., 2015; Gou et al., 2021). Techniques vary from logit-based to feature-based and relational distillation (Romero et al., 2015; Tung and Mori, 2019). Recent studies investigate data-free settings using generative replay (Micaelli and Storkey, 2019; Fang et al., 2020) and task-specific augmentations (Li, 2018: Chen et al., 2019). However, stability under distribution shift and calibration effects remain underexplored (Müller et al., 2019; Stacey et al., 2021).\n\nOur work introduces calibration-aware distillation that jointly optimizes for accuracy and well-calibrated confidence.",
    "reason": "Uses a colon to separate multiple citations in a single parenthetical; standard styles require a semicolon, e.g., “(Li, 2018; Chen et al., 2019)”.",
    "start": 458,
    "end": 487,
    "label": "Format"
  },
  {
    "span": "Message passing neural networks treat bonds as edges and update atom embeddings through iterative neighborhood aggregation, leading to strong results on quantum mechanics benchmarks and molecular property prediction. Attention-based GNNs introduce learned edge weights that allow nodes to attend selectively to their neighbors. DimeNet variants incorporate directional information to better capture angular dependencies, while SchNet uses continuous-filter convolutions over inter-atomic distances. More recently, graph transformers employ global self-attention with structural encodings to model long-range interactions. In this work we develop a compact encoder that combines local message passing with lightweight spectral features.",
    "document": "Related Work\n\nMolecular property prediction has been driven by the development of graph neural architectures that aim to capture both local chemistry and global topology. Early approaches relied on fixed fingerprints and kernel methods, but neural encoders have steadily displaced handcrafted representations due to their flexibility and end-to-end training.\n\nMessage passing neural networks treat bonds as edges and update atom embeddings through iterative neighborhood aggregation, leading to strong results on quantum mechanics benchmarks and molecular property prediction. Attention-based GNNs introduce learned edge weights that allow nodes to attend selectively to their neighbors. DimeNet variants incorporate directional information to better capture angular dependencies, while SchNet uses continuous-filter convolutions over inter-atomic distances. More recently, graph transformers employ global self-attention with structural encodings to model long-range interactions. In this work we develop a compact encoder that combines local message passing with lightweight spectral features.\n\nWe evaluate models on established datasets spanning physical chemistry, biophysics, and ADMET endpoints and adopt standard train/validation/test splits to ensure comparability with prior studies.",
    "reason": "The paragraph inventories prior GNN approaches and then immediately introduces the authors' method without articulating what limitation in the surveyed work motivates their design or how their approach addresses a specific gap.",
    "start": 360,
    "end": 1095,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Quantization for efficient large language model inference has examined post-training quantization, quantization-aware training, and mixed-precision kernels (Banner et al., 2019; Dettmers et al., 2022; Frantar et al., 2023). Methods vary in calibration data requirements, per-channel vs. per-tensor granularity, and handling of outliers (Xiao et al., 2022; Yao et al., 2022).",
    "document": "Related Work\n\nScaling large language models has intensified the need for memory- and latency-efficient inference. Quantization reduces numerical precision to shrink model footprint and increase throughput, but aggressive bitwidth reductions risk accuracy loss from activation outliers and distribution shifts.\n\nQuantization for efficient large language model inference has examined post-training quantization, quantization-aware training, and mixed-precision kernels (Banner et al., 2019; Dettmers et al., 2022; Frantar et al., 2023). Methods vary in calibration data requirements, per-channel vs. per-tensor granularity, and handling of outliers (Xiao et al., 2022; Yao et al., 2022).\n\nIn contrast, we introduce a token-aware dynamic rescaling mechanism that tracks heavy-tail statistics online and selectively refines scale factors for rare activation patterns, achieving 4-bit weight/activation inference with minimal degradation on instruction-following tasks.",
    "reason": "The span catalogs categories and design axes of prior quantization methods without synthesizing how these differences motivate the new approach or what specific limitation is targeted.",
    "start": 311,
    "end": 685,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Kusner et al. (2017) defined counterfactual fairness based on structural causal models. Kilbertus et al. (2017) analyzed path-specific effects to block unfair influence. Pearl (2009) formalized causal inference with structural equations and do-calculus. Nabi and Shpitser (2018) proposed methods to repair unfair causal pathways.",
    "document": "Related Work\n\nAlgorithmic fairness often requires reasoning about causal pathways that transmit information from sensitive attributes to decisions. Causal formalisms can differentiate permissible from impermissible influence and guide the design of interventions.\n\nKusner et al. (2017) defined counterfactual fairness based on structural causal models. Kilbertus et al. (2017) analyzed path-specific effects to block unfair influence. Pearl (2009) formalized causal inference with structural equations and do-calculus. Nabi and Shpitser (2018) proposed methods to repair unfair causal pathways.\n\nWe operationalize path constraints via differentiable surrogates that can be incorporated into end-to-end training while providing post-hoc certificates.",
    "reason": "The span presents four works with no transitions and without clarifying their relationships; the reader must infer connections, reducing coherence (issues a and b).",
    "start": 265,
    "end": 594,
    "label": "Coherence"
  },
  {
    "span": "Smith et al., 2020)",
    "document": "Related Work\n\nKnowledge graph embedding methods model entities and relations in continuous spaces to enable link prediction and reasoning (Bordes et al., 2013; Wang et al., 2017). Inductive formulations extend these ideas to unseen entities by leveraging textual or structural features (Hamaguchi et al., 2017; Teru et al., 2020). However, transductive models remain dominant due to their strong closed-world assumptions and scalability (Sun et al., 2019; Smith et al., 2020). Recent advances incorporate graph neural networks to propagate neighborhood information (Schlichtkrull et al., 2018; Vashishth et al., 2020), while hybrid methods fuse text encoders with structured signals (Xiong et al., 2020; Yao et al., 2019). Our work complements encoder-decoder formulations that explicitly model relation directionality and sparsity (Rossi et al., 2021).",
    "reason": "Parenthetical citation is missing the opening parenthesis; it should be (Smith et al., 2020).",
    "start": 456,
    "end": 475,
    "label": "Format"
  },
  {
    "span": "Recent competitions show that graph neural networks dominate molecular property prediction benchmarks.",
    "document": "Related Work\n\nMolecular property prediction has benefited from learned representations that capture both local chemical environments and global graph topology. Approaches span message passing, spectral methods, and 3D-aware architectures, with increasing attention to incorporating geometric and quantum-mechanical signals. Benchmarking practices typically evaluate models across a suite of properties and data splits to assess generalization.\n\nRecent competitions show that graph neural networks dominate molecular property prediction benchmarks. Yet, the extent to which dominance depends on scaffold splits, label noise, and conformer availability remains under-explored. We revisit this question with controlled ablations and report findings on robustness to distribution shifts.",
    "reason": "Claims evidence from 'recent competitions' without citing any specific competition or results.",
    "start": 445,
    "end": 547,
    "label": "Unsupported_claim"
  },
  {
    "span": "Neural abstractive summarization has progressed through sequence-to-sequence with attention (Rush et al., 2015; Nallapati et al., 2016), copy-augmented pointer-generator networks (See et al., 2017), coverage mechanisms to reduce repetition (Tu et al., 2016; See et al., 2017), and Transformer-based encoders-decoders (Vaswani et al., 2017; Liu and Lapata, 2019). Pretraining with large language models further improves factuality and fluency (Zhang et al., 2020; Lewis et al., 2020; Dong et al., 2019), and reinforcement learning optimizes task-specific rewards (Paulus et al., 2018; Narayan et al., 2018).",
    "document": "Related Work\n\nText summarization research spans extractive and abstractive paradigms. Extractive methods select salient units directly from the source (Yasunaga et al., 2017; Narayan et al., 2018b), while abstractive systems generate novel phrasings that can compress and paraphrase content (Nallapati et al., 2016). Evaluation typically relies on lexical overlap (ROUGE) and increasingly on semantic and factuality metrics (Kryscinski et al., 2020; Maynez et al., 2020).\n\nNeural abstractive summarization has progressed through sequence-to-sequence with attention (Rush et al., 2015; Nallapati et al., 2016), copy-augmented pointer-generator networks (See et al., 2017), coverage mechanisms to reduce repetition (Tu et al., 2016; See et al., 2017), and Transformer-based encoders-decoders (Vaswani et al., 2017; Liu and Lapata, 2019). Pretraining with large language models further improves factuality and fluency (Zhang et al., 2020; Lewis et al., 2020; Dong et al., 2019), and reinforcement learning optimizes task-specific rewards (Paulus et al., 2018; Narayan et al., 2018).\n\nContemporary work also explores controllability, content planning, and grounding to reduce hallucination (Huang et al., 2020; Narayan et al., 2021). In this work, we investigate a planning-then-generation approach with document-level constraints and contrastive training for factual alignment.\n",
    "reason": "The span lists prior models and techniques without explaining how they relate to the authors' goals or approach, and it does not articulate a gap or perspective, matching definition (a) and (c).",
    "start": 473,
    "end": 1079,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Smith et al., 2019]",
    "document": "Introduction\n\nSelf-supervised visual pretraining learns transferable representations without manual labels by solving proxy tasks such as instance discrimination and clustering (He et al., 2020; Caron et al., 2020). When fine-tuned for detection and segmentation, these models rival fully supervised pretraining on large-scale labeled datasets, as shown by (Smith et al., 2019] and subsequent work exploring multi-crop and momentum encoders. Nevertheless, domain-specific pretraining remains crucial in medical and satellite imagery where texture and scale differ markedly from natural images (Azizi et al., 2021; Tao et al., 2022).\n\nWe investigate cross-domain adaptation by aligning intermediate feature statistics and demonstrate consistent improvements across three target domains with limited annotations.",
    "reason": "Mismatched bracket in a parenthetical citation; opening parenthesis '(' is closed with ']' instead of ')'.",
    "start": 358,
    "end": 377,
    "label": "Format"
  },
  {
    "span": "Koedinger et al. (1997) outlined principles of intelligent tutoring systems. Piech et al. (2015) modeled student learning with recurrent neural networks. Brooks et al. (2014) surveyed analytics in MOOCs.",
    "document": "Introduction\n\nEducational technology research spans intelligent tutoring systems (ITS), learning analytics, and adaptive assessment. A unifying goal is to infer latent knowledge states and deliver timely feedback that improves learning outcomes while respecting constraints on instructor time and privacy.\n\nBackground and prior work\n\nKoedinger et al. (1997) outlined principles of intelligent tutoring systems. Piech et al. (2015) modeled student learning with recurrent neural networks. Brooks et al. (2014) surveyed analytics in MOOCs. Corbett and Anderson (1995) proposed knowledge tracing as a cognitive model.\n\nOpen challenges\n\nDespite progress, model generalization across courses and domains is poor, and opaque predictions hinder adoption. Granularity of skills and label noise further complicate evaluation.\n\nWe present a hierarchical, interpretable knowledge tracing model that adapts to course structure and provides actionable explanations for instructors, validated across multiple MOOCs and small classroom settings.",
    "reason": "The sentences list works from different subareas (ITS, RNN-based tracing, MOOC analytics) with no transitions or explanation of how they relate, causing abrupt, unconnected mentions. This is a multi-sentence coherence issue per (a) and (b).",
    "start": 334,
    "end": 537,
    "label": "Coherence"
  },
  {
    "span": "Deep Q-Networks approximate action-value functions from high-dimensional inputs (Mnih et al., 2015). Slot filling uses recurrent networks to extract semantic frames (Henderson et al., 2014). Policy gradient methods optimize dialogue policies with stochastic objectives (Williams, 1992). Pretrained conversational models improve language understanding and generation (Zhang et al., 2020).",
    "document": "Related Work\n\nReinforcement Learning for Task-Oriented Dialogue\n\nTask-oriented dialogue agents require robust state tracking, policy optimization, and natural language generation to achieve user goals. Reinforcement learning (RL) is often used to optimize long-term success under sparse, delayed rewards.\n\nRL Algorithms, NLU/NLG, and Pretraining\n\nDeep Q-Networks approximate action-value functions from high-dimensional inputs (Mnih et al., 2015). Slot filling uses recurrent networks to extract semantic frames (Henderson et al., 2014). Policy gradient methods optimize dialogue policies with stochastic objectives (Williams, 1992). Pretrained conversational models improve language understanding and generation (Zhang et al., 2020).\n\nOur Approach\n\nWe propose an offline RL framework that leverages pretrained encoders for state tracking and uses conservative policy updates to mitigate distributional shift in logged dialogues.",
    "reason": "The span abruptly switches between RL control (DQN, policy gradient), an NLU component (slot filling), and pretraining for NLG without explaining their interrelations in a dialogue system pipeline, reducing coherence.",
    "start": 347,
    "end": 734,
    "label": "Coherence"
  },
  {
    "span": "(Diaz, 2014, Kumar, 2015)",
    "document": "Related Work\n\nComparative evaluations of semi-supervised objectives highlight trade-offs between consistency regularization and pseudo-labeling confidence thresholds. Several studies report that performance depends strongly on data distribution shifts (Diaz, 2014, Kumar, 2015), whereas others emphasize the role of calibration techniques to mitigate confirmation bias (Thakur & Verma, 2019; Lee et al., 2020). Multi-dataset benchmarks have been proposed to standardize settings and reduce cherry-picking (Clark et al., 2021).\n\nOur work contributes a unified evaluation protocol and analyzes failure modes under varying label scarcity.",
    "reason": "Multiple sources inside one parenthetical citation are incorrectly separated by a comma; they should be separated by a semicolon, e.g., \"(Diaz, 2014; Kumar, 2015)\".",
    "start": 252,
    "end": 277,
    "label": "Format"
  },
  {
    "span": "Recent works have explored hierarchical attention for stance detection.",
    "document": "Introduction\n\nStance detection aims to identify the author's position toward a given target in social media posts and news comments. Early approaches relied on feature engineering over lexical and syntactic cues, while neural models have shifted the field toward end-to-end architectures that better capture context and pragmatics. Despite progress, modeling implicit targets and handling domain shift remain open challenges. Recent works have explored hierarchical attention for stance detection. In this paper, we build on target-aware contextualization and propose a contrastive training objective that encourages separation between pro and con stances under weak supervision.",
    "reason": "This is a claim about prior work (“recent works”) without any supporting citations, violating rule (d) and (a).",
    "start": 426,
    "end": 497,
    "label": "Unsupported_claim"
  },
  {
    "span": "SLAM methods estimate pose and map jointly (Durrant-Whyte and Bailey, 2006). Imitation learning scales with teleoperation data (Pomerleau, 1989). Model predictive control handles constraints in real time (Camacho and Bordons, 2007). Semantic mapping adds object-level labels (Bowman et al., 2017).",
    "document": "Related Work\n\nAutonomous navigation integrates perception, planning, and control under uncertainty. Classical pipelines decouple these components, whereas learning-based systems blur boundaries to share representations. We review relevant foundations to situate our learning-to-plan approach.\n\nSLAM methods estimate pose and map jointly (Durrant-Whyte and Bailey, 2006). Imitation learning scales with teleoperation data (Pomerleau, 1989). Model predictive control handles constraints in real time (Camacho and Bordons, 2007). Semantic mapping adds object-level labels (Bowman et al., 2017).\n\nAlthough each area advances a key capability, few works combine semantics-aware mapping with predictive safety constraints learned from demonstrations. Our method unifies these elements in a differentiable planning framework.",
    "reason": "The span strings together disparate subfields (SLAM, imitation, MPC, semantic mapping) without transitions or explanation of how they build on one another, reducing coherence.",
    "start": 294,
    "end": 591,
    "label": "Coherence"
  },
  {
    "span": "Zheng et al. 2",
    "document": "Related Work\n\nActive learning strategies for text classification typically rely on uncertainty sampling, diversity sampling, or hybrids that trade off informativeness and coverage (Settles, 2009; Ash et al., 2020). Margin-based methods are easy to implement but can over-focus on outliers in skewed datasets (Siddhant and Lipton, 2018). Zheng et al. 2 propose an entropy-regularized selector that stabilizes acquisition across rounds. We compare against core-set selection and BALD-style methods (Sener and Savarese, 2018; Gal and Ghahramani, 2016) and examine label bias introduced by annotator disagreement.",
    "reason": "Wrong use of footnote-style number in place of a proper citation year; should include a year or be reformatted as a proper footnote.",
    "start": 337,
    "end": 351,
    "label": "Format"
  },
  {
    "span": "Faster R-CNN introduced region proposal networks for end-to-end detection (Ren et al., 2015). Mosaic augmentation composes four images into one training sample (Bochkovskiy et al., 2020). Scale jittering changes input resolutions across iterations (Singh and Davis, 2018). RetinaNet addresses class imbalance with focal loss (Lin et al., 2017).",
    "document": "Related Work\n\nModern object detection has progressed through architectural innovations and training strategies that improve accuracy and efficiency. Two-stage detectors leverage proposals to refine bounding boxes, while one-stage models streamline inference with dense predictions.\n\nFaster R-CNN introduced region proposal networks for end-to-end detection (Ren et al., 2015). Mosaic augmentation composes four images into one training sample (Bochkovskiy et al., 2020). Scale jittering changes input resolutions across iterations (Singh and Davis, 2018). RetinaNet addresses class imbalance with focal loss (Lin et al., 2017).\n\nSubsequent advances explore multi-scale feature pyramids (Lin et al., 2017), transformer-based detectors with set prediction (Carion et al., 2020), and decoupled heads for classification and localization (Chen et al., 2019). Our approach focuses on data-centric improvements, learning augmentation policies that adapt to dynamic class imbalance and image content, which complements architectural gains.",
    "reason": "The span mixes detector architectures with various augmentations and a loss function but provides no transitions or explanation of how these works are related, creating an abrupt, disjointed sequence.",
    "start": 283,
    "end": 627,
    "label": "Coherence"
  },
  {
    "span": "Human performance on SQuAD 2.0 has plateaued around 90 F1.",
    "document": "Related Work\n\nReading comprehension benchmarks such as SQuAD 1.1 and 2.0, NewsQA, and Natural Questions have driven rapid improvements in extractive and generative QA systems (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Trischler et al., 2017; Kwiatkowski et al., 2019). Large-scale pretrained encoders and span predictors, including BERT and RoBERTa, substantially improved F1 over prior RNN-based architectures (Devlin et al., 2019; Liu et al., 2019). More recent approaches emphasize calibrating abstention, adversarial robustness, and answer verification (Jia and Liang, 2017; Kamath et al., 2020).\n\nHuman performance on SQuAD 2.0 has plateaued around 90 F1. Motivated by the remaining gap between model and human calibration, we study confidence modeling under distribution shift and propose a lightweight verifier that improves selective QA without sacrificing overall accuracy.",
    "reason": "Claims a specific human-performance figure on a benchmark without citing an authoritative source, which requires evidence.",
    "start": 608,
    "end": 666,
    "label": "Unsupported_claim"
  },
  {
    "span": "Counterfactual data augmentation reduces spurious correlations (Zmigrod et al., 2019). Adversarial training removes protected attribute information (Zhang et al., 2018). Debiased embeddings alter vector geometry (Bolukbasi et al., 2016). Calibration aligns predicted probabilities with outcomes (Guo et al., 2017).",
    "document": "Related Work\n\nFairness in NLP. Bias can enter at data, representation, and objective levels, yielding disparate performance across demographic groups (Blodgett et al., 2020). Mitigation typically targets one or more stages through preprocessing, in-processing, or post-processing interventions (Hardt et al., 2016; Zhang et al., 2018).\n\nCounterfactual data augmentation reduces spurious correlations (Zmigrod et al., 2019). Adversarial training removes protected attribute information (Zhang et al., 2018). Debiased embeddings alter vector geometry (Bolukbasi et al., 2016). Calibration aligns predicted probabilities with outcomes (Guo et al., 2017). We unify counterfactual augmentation with adversarial invariance while preserving calibrated probabilities via group-aware temperature scaling.",
    "reason": "The span lists four techniques with citations but lacks transitions or an explanation of their interrelations. The abrupt shifts between augmentation, adversarial training, embeddings, and calibration create a coherence problem.",
    "start": 337,
    "end": 651,
    "label": "Coherence"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become a standard tool for relational data, unifying message passing with learnable aggregation (Kipf and Welling, 2017; Hamilton et al., 2017). While most prior work reports improvements using author–date citations, some studies in recommender systems adopt a numeric style [12], leading to inconsistencies within mixed-format manuscripts. Our study focuses on calibration of GNN outputs under distribution shift (Ovadia et al., 2019; Liu et al., 2020) and proposes topology-aware temperature scaling for robust uncertainty estimation.",
    "reason": "Inconsistent citation style: numeric bracketed citation used in a context that otherwise follows author–date format.",
    "start": 323,
    "end": 327,
    "label": "Format"
  },
  {
    "span": "Chen et al., 2018)",
    "document": "Related Work\n\nNeural Machine Translation and Data Selection\n\nBack-translation established a strong baseline for leveraging monolingual data (Sennrich et al., 2016), and subsequent work refined sampling and noise schedules (Edunov et al., 2018; Caswell et al., 2019). Curriculum strategies were also considered Chen et al., 2018) alongside data selection for domain adaptation (van der Wees et al., 2017) and lexicon-aware constraints (Arthur et al., 2016). More recent approaches integrate multilingual pretraining to improve low-resource transfer (Conneau and Lample, 2019; Conneau et al., 2020).\n",
    "reason": "Missing opening parenthesis in a parenthetical citation: should be '(Chen et al., 2018)'.",
    "start": 310,
    "end": 328,
    "label": "Format"
  },
  {
    "span": "(Martins et al. 2017)",
    "document": "Introduction\n\nNeural machine translation (NMT) benefits from subword segmentation and transformer architectures that capture long-range dependencies (Klein and Schuster, 2018; Rao and Demir, 2019). Prior work (Martins et al. 2017) shows that coverage penalties mitigate under-translation, while auxiliary alignment objectives can improve adequacy (Liu and Perez, 2020; Wang and Soto, 2021). Domain adaptation remains challenging due to vocabulary drift and style shift (Huang and Silva, 2020).\n\nWe propose a dual-decoder architecture that jointly models adequacy and fluency with shared attention, enabling better control over coverage and paraphrastic variation in low-resource settings.",
    "reason": "Missing comma before the year in a parenthetical citation. Should be formatted as \"(Martins et al., 2017)\".",
    "start": 209,
    "end": 230,
    "label": "Format"
  },
  {
    "span": "According to (Jones and Lee, 2015)",
    "document": "Introduction\n\nPrivacy-Preserving Machine Learning\n\nGrowing deployment of ML systems raises concerns about privacy and data protection in sensitive domains. According to (Jones and Lee, 2015), differential privacy provides worst-case guarantees on individual influence, and subsequent work adapts these guarantees to learning algorithms (Dwork et al., 2016; Abadi et al., 2016). Beyond privacy, secure aggregation and federated optimization address communication and threat models in decentralized training (Konečný et al., 2016; Bonawitz et al., 2017).\n",
    "reason": "Wrong citation style for narrative form: the phrase 'According to' should be followed by a narrative citation, e.g., 'According to Jones and Lee (2015)'.",
    "start": 156,
    "end": 190,
    "label": "Format"
  },
  {
    "span": "The CNN/DailyMail dataset is the most widely used benchmark for news summarization.",
    "document": "Related Work\n\nAbstractive summarization has benefited from sequence-to-sequence models with attention and, more recently, pretrained encoder–decoder architectures. Much of this progress has been evaluated on large-scale news corpora, where article-lead bias and extractive baselines pose meaningful challenges to fair comparison. The CNN/DailyMail dataset is the most widely used benchmark for news summarization. However, shifts toward long-document and dialog summarization require methods that generalize beyond headline-style abstracts. Our study focuses on mitigating hallucination through constrained decoding while preserving fluency.",
    "reason": "This sentence asserts a prevalence statistic about a specific dataset without a citation to support it, violating rule (a) and (b).",
    "start": 330,
    "end": 413,
    "label": "Unsupported_claim"
  },
  {
    "span": "Johnson et al. 2",
    "document": "Introduction\n\nRepresentation learning for graphs has progressed from shallow embedding methods to deep message-passing networks (Perozzi et al., 2014; Kipf and Welling, 2017). Johnson et al. 2 report contrary findings on the benefit of deeper architectures in sparse citation networks, attributing performance drops to oversmoothing. Our study reevaluates this claim with calibrated early stopping and residual connections (Li et al., 2018; Chen et al., 2020) and examines the role of positional encodings (Dwivedi et al., 2021).\n",
    "reason": "Wrong use of footnotes/numbering: the trailing \"2\" suggests a footnote marker but is used in place of a proper year or footnote formatting.",
    "start": 176,
    "end": 192,
    "label": "Format"
  },
  {
    "span": "Zhao et al., 2020)",
    "document": "Related Work\n\nKnowledge-grounded dialogue models integrate external corpora to generate factual responses (Dinan et al., 2019; Zhou et al., 2020). Retrieval-augmented generation has become a dominant paradigm due to its flexibility and scalability (Lewis et al., 2020). For grounding selection, margin-based ranking losses improve retrieval quality, as shown by Zhao et al., 2020) and later refined with cross-encoder rerankers (Han et al., 2021). To mitigate hallucination, recent work introduces factuality constraints during decoding (Kryscinski et al., 2020; Shuster et al., 2021).",
    "reason": "Mismatched parenthesis: missing opening parenthesis before a parenthetical citation.",
    "start": 362,
    "end": 380,
    "label": "Format"
  },
  {
    "span": "Back-translation consistently yields over 5 BLEU improvements in truly low-resource settings.",
    "document": "Introduction\n\nNeural machine translation in low-resource scenarios remains challenging due to limited parallel data and domain mismatch. A widely adopted strategy is to leverage monolingual corpora via data augmentation and semi-supervised learning. Among these methods, back-translation has emerged as a strong baseline that synthesizes source-side text by translating target-side monolingual data using a reverse model. Back-translation consistently yields over 5 BLEU improvements in truly low-resource settings.\n\nDespite its popularity, the effectiveness of back-translation depends on the noise profile of the reverse model and the filtering of synthetic pairs. Further, for morphologically rich languages, synthetic source quality and tokenization choices can significantly affect gains. This paper investigates how quality-aware filtering and curriculum schedules improve low-resource NMT beyond standard back-translation pipelines.",
    "reason": "Asserts a quantitative performance gain ('over 5 BLEU') from prior work without any citation or evidence.",
    "start": 422,
    "end": 515,
    "label": "Unsupported_claim"
  },
  {
    "span": "Jumper et al. (2021) presented AlphaFold2 with attention and invariant point networks. Simons et al. (1997) developed Rosetta ab initio folding. Jones et al. (2012) advanced contact prediction using coevolutionary signals. Lin et al. (2022) introduced ESMFold based on protein language models.",
    "document": "Related Work\n\nProtein structure prediction has rapidly advanced through deep learning and statistical coevolution, leading to near-experimental accuracy for many targets. Approaches vary in how they integrate sequence, evolutionary, and geometric priors.\n\nJumper et al. (2021) presented AlphaFold2 with attention and invariant point networks. Simons et al. (1997) developed Rosetta ab initio folding. Jones et al. (2012) advanced contact prediction using coevolutionary signals. Lin et al. (2022) introduced ESMFold based on protein language models.\n\nWe focus on data-efficient fine-tuning of structure predictors under limited MSAs, orthogonal to architecture innovations summarized above.",
    "reason": "The span enumerates disparate methods across decades without clarifying their relationships or transitions, making the connection between sentences and works unclear.",
    "start": 256,
    "end": 549,
    "label": "Coherence"
  },
  {
    "span": "The SNLI corpus contains 570k sentence pairs",
    "document": "Related Work\n\nNatural Language Inference (NLI) evaluates a model's ability to determine entailment, contradiction, or neutrality between a premise and a hypothesis. The task has become a litmus test for sentence understanding, spurring the development of models that range from attention-based architectures to pretrained transformers.\n\nThe SNLI corpus contains 570k sentence pairs and has been complemented by other datasets that test different phenomena, such as multi-genre coverage and adversarial examples. Despite impressive accuracies, recent studies reveal annotation artifacts and lexical biases that models exploit.\n\nOur work addresses these issues by incorporating debiased training objectives and counterfactual data augmentation to improve robustness across NLI benchmarks.",
    "reason": "Provides a specific dataset statistic on first mention without a citation to the dataset paper (definition a/b).",
    "start": 337,
    "end": 381,
    "label": "Unsupported_claim"
  },
  {
    "span": "End-to-end ASR replaces hybrid HMM-DNN pipelines (Graves et al., 2014). SpecAugment improves robustness to input variability (Park et al., 2019). Self-supervised pretraining leverages unlabeled speech (Baevski et al., 2020).",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) has moved from hybrid systems to end-to-end models that unify acoustic, pronunciation, and language modeling. Robustness and data efficiency are critical for deployment in diverse acoustic conditions.\n\nEnd-to-end ASR replaces hybrid HMM-DNN pipelines (Graves et al., 2014). SpecAugment improves robustness to input variability (Park et al., 2019). Self-supervised pretraining leverages unlabeled speech (Baevski et al., 2020). External language models can further refine outputs via shallow fusion (Kannan et al., 2018). Yet, the combined impact of augmentation and pretraining on low-resource domains remains understudied.\n\nWe systematically vary augmentation strength and pretraining checkpoints across resource levels, providing guidance for practitioners targeting noisy, low-data regimes.",
    "reason": "The span lists three contributions in ASR in succession without clarifying how they connect or build on each other. The lack of transitions and explicit relationships reduces coherence across the sentences.",
    "start": 251,
    "end": 475,
    "label": "Coherence"
  },
  {
    "span": "Radford et al. (2021) introduced CLIP for image–text contrastive learning. Owens and Efros (2018) studied audio–visual correspondence. Bertasius et al. (2021) proposed TimeSformer for video understanding with transformers.",
    "document": "Related Work\n\nMultimodal learning integrates heterogeneous signals to improve representation quality and task performance. Contrastive objectives and transformer architectures have become central tools in learning across modalities.\n\nRadford et al. (2021) introduced CLIP for image–text contrastive learning. Owens and Efros (2018) studied audio–visual correspondence. Bertasius et al. (2021) proposed TimeSformer for video understanding with transformers. Lei et al. (2021) presented VATT as a unified architecture for video, audio, and text.\n\nWe focus on cross-modal pretraining with masked modeling and alignment constraints tailored for fine-grained temporal grounding.",
    "reason": "The span lists disparate multimodal topics (image–text, audio–visual, video transformers) without transitions or explicit explanation of their relationships, resulting in abrupt shifts between works.",
    "start": 234,
    "end": 456,
    "label": "Coherence"
  },
  {
    "span": "The Atari 100k benchmark has become the de facto standard for evaluating data-efficient RL agents.",
    "document": "Related Work\n\nSample-efficient reinforcement learning has progressed rapidly with model-based methods, contrastive auxiliary losses, and improved data augmentation. The Atari 100k benchmark has become the de facto standard for evaluating data-efficient RL agents. While impressive gains have been reported on a subset of games, generalization across titles and action spaces remains inconsistent. Our approach targets representation stability under sparse rewards and limited interaction budgets.",
    "reason": "Introduces a specific benchmark and a field-wide status claim without citation (definition a and b).",
    "start": 165,
    "end": 263,
    "label": "Unsupported_claim"
  },
  {
    "span": "[12]",
    "document": "Related Work\n\nUncertainty estimation in deep learning spans Bayesian approximations, ensembling, and calibration techniques (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017). According to [12], temperature scaling offers a simple post-hoc calibration method but fails under distribution shift. Subsequent studies combine calibration with robustness objectives to improve out-of-distribution performance (Guo et al., 2017; Ovadia et al., 2019).\n\nWe complement prior work by introducing a shift-aware calibrator that conditions on features predictive of domain change.\n",
    "reason": "Numeric bracket citation used in an author–year context; should be replaced with a proper author–year citation.",
    "start": 196,
    "end": 200,
    "label": "Format"
  },
  {
    "span": "Time-series anomaly detection methods range from statistical baselines (ARIMA, STL, robust z-scores) to tree ensembles (Isolation Forest), deep autoencoders, VAEs, sequence-to-sequence predictors, and GAN-based detectors for complex temporal patterns (Box et al., 2015; Liu et al., 2008; Malhotra et al., 2016; Xu et al., 2018). Benchmarks report strong performance on industrial sensors, IT telemetry, and financial data.",
    "document": "Related Work: Anomaly Detection in Time Series\n\nDetecting unusual behaviors in temporal data is critical for maintaining service reliability and safety in cyber-physical systems. Challenges include non-stationarity, seasonality, and rare-event scarcity.\n\nTime-series anomaly detection methods range from statistical baselines (ARIMA, STL, robust z-scores) to tree ensembles (Isolation Forest), deep autoencoders, VAEs, sequence-to-sequence predictors, and GAN-based detectors for complex temporal patterns (Box et al., 2015; Liu et al., 2008; Malhotra et al., 2016; Xu et al., 2018). Benchmarks report strong performance on industrial sensors, IT telemetry, and financial data.\n\nWe develop a detector that combines seasonal-trend decomposition with probabilistic forecasting under uncertainty.",
    "reason": "The span lists categories of methods and domains but does not connect them to the proposed detector or identify any limitation or gap to be addressed (criteria a and c).",
    "start": 255,
    "end": 677,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Lopez et al., 2018;Kim and Rao, 2020,",
    "document": "Introduction\n\nDomain adaptation for text classification seeks to transfer knowledge from a labeled source domain to an unlabeled or sparsely labeled target domain. Early approaches employed instance reweighting and pivot features (Huang and Paul, 2017; Blitzer et al., 2006). More recent work has studied adversarial training and meta-learning for robust transfer, but consensus on the best strategy remains elusive. Importance weighting and discrepancy minimization are among the most widely adopted methods (Lopez et al., 2018;Kim and Rao, 2020, with additional research exploring feature alignment and representation invariance (Hoffman et al., 2019; Tzeng et al., 2020). In this paper, we revisit these assumptions and propose a unified objective that balances invariance with target-specific adaptation.",
    "reason": "Parenthetical citation is missing the closing parenthesis and lacks a space after the semicolon; it also ends with a dangling comma. Should be formatted like “(Lopez et al., 2018; Kim and Rao, 2020)”",
    "start": 509,
    "end": 547,
    "label": "Format"
  },
  {
    "span": "(Brown et al., 2020",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative training without centralizing raw data, improving privacy and compliance (McMahan et al., 2017; Kairouz et al., 2021). However, client heterogeneity in data distributions and system capabilities complicates optimization and evaluation (Li et al., 2020a; Karimireddy et al., 2020).\n\nPersonalization strategies tailor global models to client-specific distributions via meta-learning, multi-task objectives, or mixture-of-experts (Fallah et al., 2020; Arivazhagan et al., 2019). Robust aggregation mitigates the influence of outliers and adversaries (Blanchard et al., 2017; Yin et al., 2018). Communication efficiency is further improved through sparsification and quantization (Suresh et al., 2017; Haddadpour et al., 2021).\n\nBenchmarking practices vary widely across works, with inconsistent datasets and client partitioning schemes (Brown et al., 2020, leading to difficulties in reproducibility and fair comparison (Caldas et al., 2018; He et al., 2020). We address this by releasing a standardized suite with realistic non-IID splits and scripted system conditions.",
    "reason": "Missing closing parenthesis in a parenthetical citation.",
    "start": 893,
    "end": 912,
    "label": "Format"
  },
  {
    "span": "Human agreement on sarcasm detection rarely exceeds 0.6 kappa.",
    "document": "Introduction\n\nSarcasm detection remains challenging due to context dependence, pragmatic inference, and cultural variation. While neural architectures have improved performance on benchmark datasets, substantial disagreements persist among annotators, especially when surface cues are ambiguous.\n\nHuman agreement on sarcasm detection rarely exceeds 0.6 kappa. This low agreement complicates the interpretation of model accuracy and raises questions about ceiling effects and label reliability. Moreover, the prevalence of sarcasm differs substantially across domains, such as product reviews versus microblogs, which can bias evaluation if not carefully controlled.\n\nWe propose a multi-view annotation protocol that captures context windows, speaker intent, and audience reaction signals. Our approach yields more consistent labels and enables models to disentangle literal content from pragmatic cues.",
    "reason": "A quantitative claim about inter-annotator agreement is made without providing evidence or references (violates rule b).",
    "start": 297,
    "end": 359,
    "label": "Unsupported_claim"
  },
  {
    "span": "Miller et al. 1",
    "document": "Introduction\n\nEvaluation protocols for document retrieval vary widely across datasets, making cross-paper comparisons challenging. Prior surveys, notably by Miller et al. 1, have cataloged metrics and test collections used in ad hoc retrieval and QA. Subsequent updates emphasize standardized splits and statistical testing (Robertson, 2019; Yang et al., 2020), while new benchmarks encourage robust evaluation under domain shift (Clark et al., 2021).\n\nIn our experiments, we adopt common retrieval metrics and report statistical significance following established recommendations (Sakai, 2018).",
    "reason": "Improper footnote-like usage attached to an author name without a year; should either include the year as a proper citation (e.g., \"Miller et al. (YEAR)\") or be formatted as a proper footnote/endnote.",
    "start": 157,
    "end": 172,
    "label": "Format"
  },
  {
    "span": "SpecAugment, TTS-based synthesis, speed perturbation, and noise injection are commonly used to expand training data, while back-translation-like methods generate pseudo-labels from unlabeled speech (Park et al., 2019; Kharitonov et al., 2020; Ko et al., 2015; Chen et al., 2021). Self-training and consistency regularization further leverage untranscribed audio to improve robustness (Xu et al., 2020; Baevski et al., 2020).",
    "document": "Introduction\n\nBuilding accurate automatic speech recognition (ASR) systems in low-resource settings is challenging due to limited labeled data and domain mismatch. Data augmentation and semi-supervised learning are standard levers to mitigate scarcity.\n\nSpecAugment, TTS-based synthesis, speed perturbation, and noise injection are commonly used to expand training data, while back-translation-like methods generate pseudo-labels from unlabeled speech (Park et al., 2019; Kharitonov et al., 2020; Ko et al., 2015; Chen et al., 2021). Self-training and consistency regularization further leverage untranscribed audio to improve robustness (Xu et al., 2020; Baevski et al., 2020).\n\nIn this work, we explore augmentation policies optimized per language and domain under compute constraints. We present a search procedure that balances word error rate with training cost and demonstrate gains on three low-resource corpora.",
    "reason": "The span lists techniques and citations without connecting them to the paper’s proposed method or stating what specific limitation remains, aligning with (a) and (b).",
    "start": 254,
    "end": 678,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Prior studies demonstrate that sequence-level distillation is always superior to frame-level methods across ASR benchmarks.",
    "document": "Related Work\n\nKnowledge distillation has emerged as a powerful paradigm for compressing automatic speech recognition (ASR) models by transferring knowledge from a large teacher to a smaller student. Early ASR distillation focused on frame-level supervision, while later work explored sequence-level objectives aligned with end-to-end training. Prior studies demonstrate that sequence-level distillation is always superior to frame-level methods across ASR benchmarks. Beyond objective choice, researchers investigate teacher ensembles, temperature scaling, and curriculum schedules to stabilize training and improve word error rates. Despite these advances, little is known about how distillation interacts with streaming constraints and on-device latency budgets.\n",
    "reason": "This sweeping claim about 'prior studies' and universal superiority lacks any citation to the studies in question (rule a and b).",
    "start": 344,
    "end": 467,
    "label": "Unsupported_claim"
  },
  {
    "span": "Our method ranked second on the SemEval sarcasm detection competition.",
    "document": "Related Work\n\nSarcasm detection has been studied in both sentence-level and conversation-level settings, with datasets spanning Twitter, Reddit, and product reviews. Approaches range from lexical and pragmatic cues to contextual user embeddings and pretraining on large conversational corpora. Our method ranked second on the SemEval sarcasm detection competition. While leaderboard-oriented optimization has driven progress, generalization across domains and platforms remains limited, motivating approaches that emphasize robustness and transfer.",
    "reason": "Claims a competition ranking without citing the specific SemEval task, year, or leaderboard (rule a/e).",
    "start": 294,
    "end": 364,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Chen et al., 2020)",
    "document": "Related Work\n\nResearch on graph-based semi-supervised classification has a long history, with early surveys by Zhu and Goldberg (2009) and Chapelle et al. (2006). More recent work explores consistency training for robust learning signals (Miyato et al., 2018; Xie et al., 2020). In (Chen et al., 2020), the authors propose a neighborhood interpolation scheme that reduces confirmation bias during pseudo-labeling. Similar ideas have been examined for text classification using pre-trained encoders (Gururangan et al., 2020), but domain shift remains a challenge (Ben-David et al., 2010). Our study extends this line of research by integrating curriculum schedules for unlabeled data selection.",
    "reason": "Wrong citation style: a preposition is followed by a parenthetical citation. It should be narrative style: \"In Chen et al. (2020)\".",
    "start": 279,
    "end": 301,
    "label": "Format"
  },
  {
    "span": "According to industry reports, more than 70% of commercial recommenders now deploy bandit algorithms.",
    "document": "Introduction\n\nInteractive recommendation must balance exploration and exploitation to personalize content under uncertainty. Bandit algorithms provide a principled framework for online learning with partial feedback, enabling systems to adapt to non-stationary user preferences. Despite their theoretical appeal, practical deployment raises concerns around delayed feedback, exposure bias, and safety constraints.\n\nAccording to industry reports, more than 70% of commercial recommenders now deploy bandit algorithms. This rapid adoption underscores the need for methods that maintain performance while satisfying business and fairness constraints. We present a conservative off-policy optimization approach with calibrated uncertainty estimates for safe exploration.",
    "reason": "Presents a precise statistic and source category ('industry reports') without citing any specific report.",
    "start": 415,
    "end": 516,
    "label": "Unsupported_claim"
  },
  {
    "span": "Shallow fusion and cold fusion have been adopted to mitigate mismatches (Gulcehre et al., 2015; Kannan et al., 2018). Contextual biasing methods add on-the-fly phrases to recognition (Pundak et al., 2018; Chen et al., 2019).",
    "document": "Related Work\n\nDomain adaptation in automatic speech recognition (ASR) seeks to reduce performance degradation when models are deployed in conditions different from training. Early approaches emphasized feature-space normalization and speaker adaptation to compensate for channel and acoustic variability (Gales, 1998; Woodland, 2001). More recent neural end-to-end models rely on additional textual or acoustic resources to close the gap between source and target domains.\n\nData selection and importance weighting aim to curate training subsets that better match the target distribution (Axelrod et al., 2011; Hsu and Glass, 2018). Unsupervised adaptation via pseudo-labeling has also proven effective when labeled target audio is scarce (Kahn et al., 2020; Xu et al., 2020). Shallow fusion and cold fusion have been adopted to mitigate mismatches (Gulcehre et al., 2015; Kannan et al., 2018). Contextual biasing methods add on-the-fly phrases to recognition (Pundak et al., 2018; Chen et al., 2019). Meta-learning frameworks further attempt to quickly adapt to new accents or noise settings by learning update rules that generalize across domains (Finn et al., 2017; Klejch et al., 2022).\n\nWhile semi-supervised learning leverages large unlabeled corpora to reduce word error rates, practical deployment often requires fast adaptation during streaming inference (Hegde et al., 2021; Wang et al., 2021). Our work differs by focusing on budget-aware selection of adaptation utterances that are maximally informative under latency constraints.",
    "reason": "Two adjacent sentences cite distinct techniques (LM fusion vs. contextual biasing) without transitions or an explicit relationship to each other or to the preceding data selection thread, causing abrupt topic shifts.",
    "start": 776,
    "end": 1000,
    "label": "Coherence"
  },
  {
    "span": "Self-supervised pretraining learns universal acoustic representations (Baevski et al., 2020). Pronunciation lexicons reduce homophone errors (Gales and Young, 2008). Multilingual training transfers phonetic knowledge across languages (Conneau et al., 2021). Data augmentation with SpecAugment improves robustness (Park et al., 2019).",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) in low-resource settings suffers from limited transcribed data, domain mismatch, and noisy conditions. Recent advances in self-supervision and cross-lingual learning promise significant gains, but combining techniques effectively remains nontrivial due to interaction effects and resource constraints.\n\nSelf-supervised pretraining learns universal acoustic representations (Baevski et al., 2020). Pronunciation lexicons reduce homophone errors (Gales and Young, 2008). Multilingual training transfers phonetic knowledge across languages (Conneau et al., 2021). Data augmentation with SpecAugment improves robustness (Park et al., 2019).\n\nWe explore a unified recipe that integrates self-supervised features with multilingual adapters and lexicon-informed decoding tailored to extremely small labeled corpora.",
    "reason": "The sentences list various methods without transitions or explicit links, leaving the relationships among the cited works implicit and causing abrupt topic shifts.",
    "start": 352,
    "end": 685,
    "label": "Coherence"
  },
  {
    "span": "Previous shared tasks on question generation have standardized evaluation protocols for answer-aware QG.",
    "document": "Related Work\n\nQuestion generation (QG) has evolved from rule-based systems to neural architectures that jointly consider passage and target answer spans. Evaluation practices vary across datasets and target applications, complicating fair comparison. Previous shared tasks on question generation have standardized evaluation protocols for answer-aware QG. Nevertheless, automatic metrics still struggle to capture answerability and pedagogical quality, motivating human-in-the-loop evaluation frameworks. Our method aligns generation objectives with downstream assessment criteria.",
    "reason": "This refers to shared tasks and their contributions without citing them, violating rule (a) and (d).",
    "start": 251,
    "end": 355,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Chen et al. 2019)",
    "document": "Related Work\n\nNeural abstractive summarization has progressed from sequence-to-sequence baselines to pretraining-enhanced encoders and decoders (Nallapati et al., 2016; See et al., 2017; Lewis et al., 2020). Pretrained encoder-decoder models achieve strong zero-shot and few-shot performance on diverse summarization benchmarks (Zhang et al., 2020; Raffel et al., 2020). Copy mechanisms and coverage penalties mitigate repetition and factual drift (See et al., 2017; Paulus et al., 2018). However, factuality remains a central challenge (Kryscinski et al., 2020; Durmus et al., 2020), and evaluation metrics can be misleading (Lin, 2004; Bhandari et al., 2020). Content selection and constrained decoding improved faithfulness (Chen et al. 2019) but do not fully resolve hallucination under distribution shift.\n",
    "reason": "Missing comma before the year in a parenthetical author–year citation. It should be \"(Chen et al., 2019)\".",
    "start": 727,
    "end": 745,
    "label": "Format"
  },
  {
    "span": "Johnson and Lee, 2020)",
    "document": "Related Work\n\nWeakly supervised segmentation has leveraged class activation maps and pseudo-label refinement to reduce annotation costs. Many approaches build on Johnson and Lee, 2020) to incorporate multi-scale consistency constraints and boundary-aware losses. Later extensions introduced cross-image consistency and curriculum re-labeling to mitigate confirmation bias (Patel et al., 2021; Zhou et al., 2022).",
    "reason": "Missing opening parenthesis in a parenthetical citation: should be “(Johnson and Lee, 2020)”.",
    "start": 162,
    "end": 184,
    "label": "Format"
  },
  {
    "span": "GraphSAGE samples neighborhood subgraphs to enable inductive learning (Hamilton et al., 2017). BPR optimizes pairwise preference ranking with implicit feedback (Rendle et al., 2009). PinSage scales GNNs with random walks for web-scale catalogs (Ying et al., 2018). Attention mechanisms help weight interactions (Velickovic et al., 2018).",
    "document": "Related Work\n\nGraph-based recommender systems leverage user-item interaction graphs and side information to capture higher-order connectivity and improve recommendation quality (Wu et al., 2020). Approaches range from matrix factorization on bipartite graphs to graph neural networks (GNNs) that propagate signals across neighborhoods.\n\nGraphSAGE samples neighborhood subgraphs to enable inductive learning (Hamilton et al., 2017). BPR optimizes pairwise preference ranking with implicit feedback (Rendle et al., 2009). PinSage scales GNNs with random walks for web-scale catalogs (Ying et al., 2018). Attention mechanisms help weight interactions (Velickovic et al., 2018).\n\nSubsequent research explores disentangling user intents (Ma et al., 2019), mitigating popularity bias (Abdollahpouri et al., 2017), and incorporating knowledge graphs (Wang et al., 2019). Self-supervised objectives such as contrastive augmentation improve robustness under sparsity (You et al., 2020). Our method complements these advances by unifying neighborhood sampling with counterfactual training objectives to address exposure bias in large-scale graph recommenders.",
    "reason": "The span abruptly jumps among neighborhood sampling, matrix factorization loss (BPR), web-scale GNNs, and attention without articulating their relationships or transitions, resulting in unclear coherence.",
    "start": 337,
    "end": 674,
    "label": "Coherence"
  },
  {
    "span": "DSTC9 introduced a knowledge-grounded track with human ratings on topical adequacy and factuality.",
    "document": "Introduction and Related Work\n\nOpen-domain dialogue research has benefited from shared tasks that provide standardized datasets and evaluation protocols. While automatic metrics remain imperfect correlates of human judgments, community evaluations help benchmark progress across modeling strategies.\n\nDSTC9 introduced a knowledge-grounded track with human ratings on topical adequacy and factuality. Parallel efforts in other challenges have emphasized multi-turn consistency and safety, yet reproducibility of human evaluation remains an ongoing concern due to differences in annotation schemes and worker pools.\n\nOur study proposes a unified evaluation recipe combining targeted human prompts with reference-free estimators to better capture coherence and groundedness across domains.",
    "reason": "Mentions a specific shared task and track details without citation at first mention.",
    "start": 301,
    "end": 399,
    "label": "Unsupported_claim"
  },
  {
    "span": "Reinforcement learning has been applied to recommender systems through bandits, policy gradients, and offline RL to optimize long-term metrics (Zhou et al., 2020; Chen et al., 2021; Ie et al., 2019; Ma et al., 2020).",
    "document": "Related Work\n\nRecommender systems traditionally optimize immediate engagement via supervised learning and point-wise losses, which may overlook delayed user satisfaction. RL offers a framework for sequential decision making.\n\nReinforcement learning has been applied to recommender systems through bandits, policy gradients, and offline RL to optimize long-term metrics (Zhou et al., 2020; Chen et al., 2021; Ie et al., 2019; Ma et al., 2020).\n\nNevertheless, evaluation with logged data is challenging due to bias and extrapolation error. We study conservative policy learning with counterfactual evaluation tailored to recommendation.",
    "reason": "Presents a list of RL applications with citations but does not connect them to the authors’ problem setting or identify a specific shortcoming in that span (criteria a and b).",
    "start": 226,
    "end": 442,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Techniques such as pruning, quantization, and knowledge distillation reduce computation and memory for edge deployment (Han et al., 2015; Jacob et al., 2018; Hinton et al., 2015).",
    "document": "Related Work\n\nEfficient deep learning on the edge. On-device inference faces tight energy and latency budgets. Techniques such as pruning, quantization, and knowledge distillation reduce computation and memory for edge deployment (Han et al., 2015; Jacob et al., 2018; Hinton et al., 2015). NAS and hardware-aware co-design further align architectures with device constraints (Tan et al., 2019; Cai et al., 2020).\n\nGap addressed. While many methods lower MACs and parameters, energy proportionality under dynamic workloads is less studied.\n\nThis paper. We propose an input-adaptive activation gating mechanism with per-layer energy controllers that optimizes energy-delay product under workload variability.",
    "reason": "The span enumerates broad efficiency techniques without relating them to the specific challenge of energy proportionality or the proposed approach, so it lacks synthesis.",
    "start": 111,
    "end": 290,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Most prior methods rely on handcrafted descriptors before the advent of GNNs.",
    "document": "Related Work\n\nPredicting molecular properties from structure is central to virtual screening and lead optimization. Historically, models operated on fixed fingerprints or descriptors derived from substructures, counts, and physicochemical features. Most prior methods rely on handcrafted descriptors before the advent of GNNs. With message passing neural networks, representations are learned end-to-end from molecular graphs, enabling better generalization across scaffolds.\n\nRecent advances incorporate 3D geometries, equivariant architectures, and pretraining on large unlabeled molecule corpora. However, improvements can be confounded by scaffold splits, label noise from quantum approximations, and distribution shift across assay conditions. We present a rigorous evaluation with matched data splits and uncertainty-aware calibration.",
    "reason": "Makes a broad claim about the predominant approach in prior work without any supporting citations.",
    "start": 249,
    "end": 326,
    "label": "Unsupported_claim"
  },
  {
    "span": "In healthcare imaging, federated learning has been explored for classification, segmentation, and detection across hospitals using CNN backbones. Works report gains from simple FedAvg, personalization layers to adapt client-specific features, and domain alignment through adversarial objectives. Additional studies investigate communication-efficient update compression, client selection strategies, and differential privacy mechanisms to protect labels and features.",
    "document": "Introduction\nMedical imaging models benefit from diverse data but face strict privacy constraints that limit centralization. Federated learning promises collaborative training without raw data sharing, yet heterogeneity in scanners, protocols, and patient populations poses major challenges.\n\nRelated Work\nFederated optimization methods build on averaging (e.g., FedAvg) and variants that mitigate client drift via proximal terms, momentum, or adaptive aggregation. Personalization strategies produce client-specific models through fine-tuning, meta-learning, and multi-task formulations.\nIn healthcare imaging, federated learning has been explored for classification, segmentation, and detection across hospitals using CNN backbones. Works report gains from simple FedAvg, personalization layers to adapt client-specific features, and domain alignment through adversarial objectives. Additional studies investigate communication-efficient update compression, client selection strategies, and differential privacy mechanisms to protect labels and features.\nCross-site validation protocols and simulated non-IID partitions are commonly used to benchmark methods, but reporting varies and often lacks standardized evaluation under real-world shift.\n\nOur Approach\nWe present a framework that adapts to heterogeneous institutions via uncertainty-aware aggregation and calibration across clients.",
    "reason": "The span summarizes a range of federated healthcare studies without articulating how they relate to this paper's specific goals or identifying a gap; it lacks synthesis per (a) and (c).",
    "start": 589,
    "end": 1056,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Johnson et al. 2",
    "document": "Related Work\n\nExploration in deep reinforcement learning (RL) balances the need to discover novel states with the exploitation of known rewards (Sutton and Barto, 2018). Intrinsic motivation assigns pseudo-rewards based on novelty or prediction errors (Bellemare et al., 2016; Pathak et al., 2017), while posterior sampling approximates Bayesian exploration (Osband et al., 2016).\n\nCount-based and density-model methods struggle in high-dimensional observations; representation learning mitigates this by compressing observations into more learnable features (Burda et al., 2019; Yarats et al., 2021). Johnson et al. 2 propose a hybrid curiosity-bonus with ensemble uncertainty, complementing entropy-regularized objectives that implicitly encourage exploration (Haarnoja et al., 2018).\n\nOur work integrates a temporal abstraction mechanism with uncertainty-aware bonuses, enabling efficient long-horizon exploration in sparse-reward environments.",
    "reason": "Wrong use of footnotes/numbering in an author-year citation; dangling superscript-like number should be a proper year or removed/reformatted.",
    "start": 602,
    "end": 618,
    "label": "Format"
  },
  {
    "span": "Transfer learning and back-translation improve low-resource translation (Sennrich et al., 2016; Zoph et al., 2016). Multilingual pretraining with mBART and mT5 provides powerful initialization (Liu et al., 2020; Xue et al., 2021). Large multilingual models like M2M-100 and NLLB scale to many directions (Fan et al., 2020; Team et al., 2022). We propose a new approach for low-resource MT.",
    "document": "Related Work\n\nLow-resource machine translation (MT) benefits from techniques that transfer knowledge from high-resource languages and exploit unlabeled monolingual data. Recent progress leverages multilingual pretraining and large-scale models that cover hundreds of language pairs.\n\nTransfer learning and back-translation improve low-resource translation (Sennrich et al., 2016; Zoph et al., 2016). Multilingual pretraining with mBART and mT5 provides powerful initialization (Liu et al., 2020; Xue et al., 2021). Large multilingual models like M2M-100 and NLLB scale to many directions (Fan et al., 2020; Team et al., 2022). We propose a new approach for low-resource MT.\n\nBeyond modeling, data curation and domain adaptation remain critical. We validate on standard low-resource benchmarks and report performance across zero-shot and few-shot settings.",
    "reason": "The span recites prior advances and immediately states a new approach without explicating the gap or how the new method differs, leaving the connection to the literature implicit.",
    "start": 284,
    "end": 673,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Nguyen et al., 2019)",
    "document": "Related Work\n\nOpen-domain dialogue has seen rapid advances with large pretrained language models, yet grounding in external knowledge remains challenging (Dinan et al., 2019; Roller et al., 2021). Retrieval-augmented methods address this by conditioning responses on passages fetched at inference time (Komeili et al., 2022; Shuster et al., 2022).\n\nIn (Nguyen et al., 2019) a retrieval-augmented generator is introduced with late fusion, while subsequent work explores tighter encoder integration and iterative retrieval (Guu et al., 2020; Lewis et al., 2020). Our model builds on this line by unifying retrieval and planning through a shared contrastive objective that aligns candidate knowledge with dialog intents.\n\nMemory-augmented methods (Weston et al., 2015; Miller et al., 2016) and knowledge-grounded datasets (Zhou et al., 2018; Gopalakrishnan et al., 2019) have also catalyzed progress, but robust grounding across topics and domains remains an open problem.",
    "reason": "Wrong citation style: leading preposition inside parentheses. Should be 'In Nguyen et al. (2019)' (narrative) or remove 'In' for a parenthetical cite.",
    "start": 349,
    "end": 373,
    "label": "Format"
  },
  {
    "span": "Sequential recommenders model click histories to infer short-term intent (Hidasi et al., 2015). Knowledge graph embeddings inject structured semantics into recommendation (Wang et al., 2019). Causal recommenders disentangle exposure from preference to debias learning signals (Wang et al., 2020).",
    "document": "Related Work\n\nPersonalization has advanced through deep sequence models that capture temporal dynamics in user behavior. Representation learning for users and items further benefits from side information, including text, images, and graphs.\n\nBias and confounding remain central challenges, as logged data reflects both user interest and system exposure policies. Recent methods address selection bias, position bias, and popularity bias with counterfactual estimators and instrumentation.\n\nSequential recommenders model click histories to infer short-term intent (Hidasi et al., 2015). Knowledge graph embeddings inject structured semantics into recommendation (Wang et al., 2019). Causal recommenders disentangle exposure from preference to debias learning signals (Wang et al., 2020).\n\nWe propose a unified architecture that conditions sequence modeling on exposure-aware representations learned from logged policies. Our evaluation isolates confounding via interventions on ranking positions and exposure distributions.",
    "reason": "The span lists three areas (sequential, KG-based, and causal recommenders) without explaining how they relate or transition from one to another, leading to abrupt topic shifts and implied relationships only (a, b).",
    "start": 490,
    "end": 786,
    "label": "Coherence"
  },
  {
    "span": "[Raghu et al., 2019]",
    "document": "Related Work\n\nTransfer learning has become standard practice in medical imaging, where labeled data can be scarce (Cheplygina et al., 2019). Pretraining on natural images followed by fine-tuning on target tasks often yields strong results (Yosinski et al., 2014), though domain shift may limit feature reuse. Recent studies examine when transfer helps in radiology [Raghu et al., 2019] and propose task-specific adapters to bridge representation gaps (Zoph et al., 2020).",
    "reason": "Inconsistent citation style: square brackets used for an author–year citation while the surrounding text uses parentheses. It should be “(Raghu et al., 2019)” to match the author–year parenthetical style.",
    "start": 365,
    "end": 385,
    "label": "Format"
  },
  {
    "span": "MS COCO",
    "document": "Introduction\n\nObject detection has progressed rapidly with the advent of deep convolutional and transformer-based architectures, yielding strong performance across a range of benchmarks and deployment scenarios. Advances in multi-scale feature fusion, improved label assignment, and richer data augmentation have further boosted average precision.\n\nPerformance comparisons are typically standardized on MS COCO and related datasets, with leaderboard gains often reflecting better long-tail recognition and localization quality rather than purely higher recall. Nonetheless, robust detection under distribution shift, small object regimes, and limited annotation budgets remains challenging.\n\nWe present a training strategy that decouples classification and localization curricula to emphasize difficult examples early while preserving calibration. Experiments demonstrate consistent improvements under varying image resolutions and annotation sparsity.",
    "reason": "The dataset 'MS COCO' is introduced without providing a citation at first mention, which should be cited per rule a.",
    "start": 403,
    "end": 410,
    "label": "Unsupported_claim"
  },
  {
    "span": "Kim and Lee",
    "document": "Introduction\n\nDistant supervision reduces annotation costs by aligning raw corpora with weak heuristics, but it introduces label noise that must be mitigated (Ratner et al., 2017; Bach et al., 2019). Kim and Lee propose a bootstrapped denoising strategy that alternates between confident pseudo-labeling and reweighting to improve generalization in low-resource settings. Building on this line, we study how curriculum schedules impact convergence under varying noise rates (Zhang et al., 2020; Arazo et al., 2020) and how confidence calibration interacts with sample selection (Guo et al., 2017).\n",
    "reason": "Narrative citation missing year: as a narrative reference, it should be formatted as \"Kim and Lee (YEAR)\" rather than just the authors' names.",
    "start": 200,
    "end": 211,
    "label": "Format"
  },
  {
    "span": "McMahan et al. (2017) propose federated averaging to aggregate local updates from edge devices. Differential privacy adds calibrated noise to gradients to protect individuals (Abadi et al., 2016). Secure aggregation protocols reduce leakage during transmission (Bonawitz et al., 2017). Communication-efficient compression techniques have also been explored (Konecný et al., 2016).",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative model training across a population of clients while keeping raw data on-device, thereby mitigating centralized data collection risks (Kairouz et al., 2021). A broad literature has emerged around objective design, aggregation robustness, privacy guarantees, and systems efficiency.\n\nMcMahan et al. (2017) propose federated averaging to aggregate local updates from edge devices. Differential privacy adds calibrated noise to gradients to protect individuals (Abadi et al., 2016). Secure aggregation protocols reduce leakage during transmission (Bonawitz et al., 2017). Communication-efficient compression techniques have also been explored (Konecný et al., 2016).\n\nBeyond privacy and efficiency, recent work studies heterogeneity and personalization. Methods address non-iid client distributions with clustered aggregation (Briggs et al., 2020), proximal regularization (Li et al., 2020), and meta-learning (Fallah et al., 2020). Robust FL resists malicious clients via Byzantine-resilient aggregators and anomaly detection (Blanchard et al., 2017; Pillutla et al., 2022). In contrast to these directions, our work focuses on quantifying trade-offs between fairness and accuracy under stratified participation, complementing prior advances in optimization and privacy.",
    "reason": "The four cited sentences list disparate FL topics (aggregation, DP, secure aggregation, compression) without transitions or explicit links between them, making the connection between works abrupt and unclear.",
    "start": 342,
    "end": 722,
    "label": "Coherence"
  },
  {
    "span": "There are many recent works that explore this topic.",
    "document": "Related Work\n\nData augmentation for relation extraction aims to improve generalization under limited supervision. Techniques include pattern-based bootstrapping (Snow et al., 2005), distant supervision with noise modeling (Mintz et al., 2009; Riedel et al., 2010), and augmentation via paraphrasing or back-translation (Yu et al., 2018). Strong pretrained encoders such as RoBERTa have further advanced state of the art (Liu et al., 2019).\n\nThere are many recent works that explore this topic. Nonetheless, a systematic comparison of augmentation strategies under strict low-shot constraints is lacking. We benchmark a suite of label-preserving and label-expanding transformations under a unified evaluation protocol.\n\nIntroduction\n\nOur study isolates the effect of augmentation from pretraining by holding the encoder fixed and varying only the data generation pipeline.",
    "reason": "Uses the vague phrase 'many recent works' without providing citations to any of these works.",
    "start": 441,
    "end": 493,
    "label": "Unsupported_claim"
  },
  {
    "span": "ViT demonstrates that pure transformer architectures can achieve strong performance on image classification (Dosovitskiy et al., 2020). U-Net remains a dominant architecture for biomedical segmentation tasks (Ronneberger et al., 2015). Self-attention mechanisms were initially explored for machine translation (Bahdanau et al., 2015). Swin Transformer introduces hierarchical attention for dense prediction (Liu et al., 2021).",
    "document": "Related Work\n\nMedical Image Segmentation\n\nDeep learning has transformed medical image segmentation by enabling end-to-end learning of complex anatomical structures. Early encoder-decoder networks established strong baselines by combining multiscale features with skip connections, yielding robust spatial detail recovery in segmentation maps. Subsequent work incorporated attention and multi-task objectives to handle class imbalance and sparse annotations typical of clinical datasets.\n\nTransformers in Vision\n\nTransformers have recently shown promise for vision tasks by leveraging global context via self-attention mechanisms. Hybrid CNN-Transformer models have been proposed to bridge locality and long-range dependencies, and adaptations for dense prediction focus on multi-scale tokenization and hierarchical processing.\n\nTransformers for Medical Imaging\n\nViT demonstrates that pure transformer architectures can achieve strong performance on image classification (Dosovitskiy et al., 2020). U-Net remains a dominant architecture for biomedical segmentation tasks (Ronneberger et al., 2015). Self-attention mechanisms were initially explored for machine translation (Bahdanau et al., 2015). Swin Transformer introduces hierarchical attention for dense prediction (Liu et al., 2021).\n\nOur Approach\n\nWe propose a hybrid architecture that retains U-Net's inductive biases while leveraging transformer blocks for global reasoning. We further introduce a token regularization objective tailored to boundary preservation and evaluate on multi-center MR datasets.",
    "reason": "The four sentences list disparate works (image classification ViT, biomedical U-Net, machine translation attention, and Swin for dense prediction) without transitions or explicit explanation of how each relates to the others or to medical segmentation, creating abrupt, unconnected shifts.",
    "start": 862,
    "end": 1288,
    "label": "Coherence"
  },
  {
    "span": "(Johnson et al. 2018)",
    "document": "Related Work\n\nFairness in classification has been studied through both pre-processing and in-processing techniques (Zemel et al., 2013; Hardt et al., 2016). According to (Johnson et al. 2018), demographic parity can conflict with individual fairness, necessitating trade-offs. We analyze this tension in the context of personalized recommenders and propose a constrained optimization framework.",
    "reason": "Missing comma between authors and year in a parenthetical citation; should be “(Johnson et al., 2018)”.",
    "start": 170,
    "end": 191,
    "label": "Format"
  },
  {
    "span": "See et al. (2017) introduced pointer-generator networks for abstractive summarization. Liu and Lapata (2019) pretrained transformers for extractive summarization. Narayan et al. (2018) proposed a reinforcement approach.",
    "document": "Related Work\n\nNeural text summarization has progressed rapidly with the advent of sequence-to-sequence modeling and pretrained transformers. Early abstractive methods built on attention-equipped encoder–decoder architectures, while extractive models emphasized sentence-level salience and global coherence. As the community moved toward larger pretraining regimes, hybrid methods began to blur the boundary between extraction and abstraction, but the interplay among architectural choices, training objectives, and factual consistency remains an active area of study.\n\nAbstractive and extractive methods\n\nSee et al. (2017) introduced pointer-generator networks for abstractive summarization. Liu and Lapata (2019) pretrained transformers for extractive summarization. Narayan et al. (2018) proposed a reinforcement approach. Kryscinski et al. (2020) examined factual consistency issues in neural summarization. Zhong et al. (2020) explored unsupervised summarization objectives.\n\nEvaluation and faithfulness\n\nBeyond ROUGE, researchers have sought measures that better capture salience, coherence, and factual grounding. Factuality metrics leveraging natural language inference and question answering have been proposed to detect hallucinations. Despite these advances, aligning training signals with evaluation desiderata remains challenging for long documents and specialized domains.\n\nOur work situates in this landscape by targeting controllable faithfulness signals during training while preserving the strengths of pretrained encoders. We focus on domain transfer to scientific articles where extractive cues are weak and factual precision is critical, and we evaluate with both automatic and human judgments.",
    "reason": "The three consecutive sentences list distinct works with no transitions or explicit relationships among them, leaving unclear how each relates to the others or to the surrounding discussion. This violates (a) and (b) and spans multiple sentences as required by (c).",
    "start": 605,
    "end": 824,
    "label": "Coherence"
  },
  {
    "span": "Reinforcement learning from human feedback (RLHF) has utilized preference modeling, reward modeling, and policy optimization with KL constraints (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022). Alternatives incorporate direct preference optimization and offline datasets for scalability (Rafailov et al., 2023; Touvron et al., 2023).",
    "document": "Introduction\n\nLarge language models can be steered toward helpful and safe behavior by aligning them with human preferences. RLHF has emerged as a key paradigm, yet it can be unstable and sample-inefficient.\n\nReinforcement learning from human feedback (RLHF) has utilized preference modeling, reward modeling, and policy optimization with KL constraints (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022). Alternatives incorporate direct preference optimization and offline datasets for scalability (Rafailov et al., 2023; Touvron et al., 2023).\n\nRecent analyses highlight reward hacking and over-optimization due to misspecified or brittle reward models. Moreover, alignment signals can drift across domains and interaction contexts.\n\nWe present a calibrated preference learning procedure that regularizes reward curvature and integrates uncertainty into policy improvement, reducing over-optimization while preserving helpfulness.",
    "reason": "The span summarizes RLHF techniques without explaining how they inform the proposed approach or specifying the precise limitation addressed, thereby lacking synthesis.",
    "start": 209,
    "end": 564,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Time-series anomaly detection methods span reconstruction-based autoencoders (Sakurada and Yairi, 2014; Zhang et al., 2019), forecasting-based detectors (Malhotra et al., 2015; Wu et al., 2021), probabilistic density estimators (Franceschi et al., 2019), isolation-based methods (Liu et al., 2008), and contrastive approaches (Tonekaboni et al., 2021; Eldele et al., 2021). Multivariate extensions add graph or attention modules to model cross-sensor dependencies (Chen et al., 2021; Deng and Hooi, 2021).",
    "document": "Introduction\n\nDetecting anomalies in sensor streams is crucial for predictive maintenance, cybersecurity, and healthcare monitoring. Real-world deployments demand timely detection, high precision to reduce false alarms, and resilience to nonstationarity, missing data, and concept drift.\n\nTime-series anomaly detection methods span reconstruction-based autoencoders (Sakurada and Yairi, 2014; Zhang et al., 2019), forecasting-based detectors (Malhotra et al., 2015; Wu et al., 2021), probabilistic density estimators (Franceschi et al., 2019), isolation-based methods (Liu et al., 2008), and contrastive approaches (Tonekaboni et al., 2021; Eldele et al., 2021). Multivariate extensions add graph or attention modules to model cross-sensor dependencies (Chen et al., 2021; Deng and Hooi, 2021).\n\nWe address delayed detection under drift by proposing a drift-aware conformal detector that pairs adaptive quantile calibration with change-point gated windows. Our method maintains uncertainty-aware thresholds and yields consistent lead time improvements across five industrial benchmarks with drifted regimes.",
    "reason": "Summarizes categories of prior methods without discussing their limitations regarding drift or how the proposed detector improves upon them; lacks explicit synthesis (criterion a/b).",
    "start": 289,
    "end": 794,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Several studies have shown that calibration can conflict with equalized odds.",
    "document": "Introduction\n\nAlgorithmic fairness formalizes trade-offs among statistical criteria such as equalized odds, demographic parity, and calibration (Hardt et al., 2016; Barocas et al., 2019). In high-stakes settings, post-hoc adjustments are often applied to improve group fairness while attempting to preserve overall accuracy (Pleiss et al., 2017).\n\nSeveral studies have shown that calibration can conflict with equalized odds. This tension motivates approaches that explicitly navigate fairness–utility trade-offs under distribution shift and label noise. We consider a risk-sensitive objective that preserves calibrated probabilities while regularizing group-specific error rates.\n\nWe provide theoretical guarantees for Pareto-frontier optimality and empirical results on credit and healthcare datasets.",
    "reason": "Asserting prior findings ('several studies have shown') without citing the studies is an unsupported claim (violates rule d).",
    "start": 348,
    "end": 425,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Kim, 2018, Li et al., 2020, Ahmed, 2021)",
    "document": "Related Work\n\nMeta-learning aims to acquire inductive biases that enable fast adaptation to new tasks with few examples (Finn et al., 2017; Rusu et al., 2019). In reinforcement learning, meta-gradients and context encoders have been explored to accelerate adaptation across task families with varying reward scales and dynamics. Recent meta-learning approaches (Kim, 2018, Li et al., 2020, Ahmed, 2021) suggest that structure-aware priors improve sample efficiency, but these methods often struggle under non-stationary reward noise.\n\nOur method introduces a noise-robust context encoder with uncertainty-weighted updates, leading to stable performance across a spectrum of task stochasticities.",
    "reason": "Wrong separator for multiple parenthetical citations; APA-style multiple citations should be separated by semicolons, e.g., '(Kim, 2018; Li et al., 2020; Ahmed, 2021)'.",
    "start": 361,
    "end": 402,
    "label": "Format"
  },
  {
    "span": "Rosenbaum and Rubin (1983) introduced propensity score matching. Breiman (2001) presented random forests. Athey and Imbens (2016) developed causal forests for heterogeneous treatment effects.",
    "document": "Related Work\n\nEstimating heterogeneous treatment effects (HTE) under observational data requires careful adjustment for confounding and flexible function approximation. Modern approaches combine identification strategies with machine learning.\n\nRosenbaum and Rubin (1983) introduced propensity score matching. Breiman (2001) presented random forests. Athey and Imbens (2016) developed causal forests for heterogeneous treatment effects. Doubly robust estimators integrate outcome and propensity models to reduce bias (Bang and Robins, 2005; Chernozhukov et al., 2018).\n\nIn online settings, uplift modeling and contextual bandits tailor treatments to maximize incremental outcomes (Zhao et al., 2017; Dimakopoulou et al., 2019).",
    "reason": "A general-purpose machine learning method (random forests) is inserted between two causally motivated techniques without clarifying why it is relevant or how it leads to causal forests.",
    "start": 245,
    "end": 436,
    "label": "Coherence"
  },
  {
    "span": "Message passing networks (Gilmer et al., 2017) encode bond-level interactions between atoms; Graph Isomorphism Networks (Xu et al., 2019) increase discriminative power through sum aggregation; D-MPNN focuses on directed edge messages to capture chemical semantics (Yang et al., 2019). Attention-based GNNs highlight salient substructures (Veličković et al., 2018), while graph transformers leverage global receptive fields without explicit message passing (Ying et al., 2021). Pretraining strategies include context prediction (Hu et al., 2020), contrastive augmentation (You et al., 2020), and masked node/edge modeling (Rong et al., 2020).",
    "document": "Related Work\n\nMolecular property prediction with graph neural networks has accelerated due to improved representations of atomic interactions and scalable training strategies in cheminformatics. A large body of work varies in how messages are constructed, aggregated, and regularized across molecular graphs.\n\nMessage passing networks (Gilmer et al., 2017) encode bond-level interactions between atoms; Graph Isomorphism Networks (Xu et al., 2019) increase discriminative power through sum aggregation; D-MPNN focuses on directed edge messages to capture chemical semantics (Yang et al., 2019). Attention-based GNNs highlight salient substructures (Veličković et al., 2018), while graph transformers leverage global receptive fields without explicit message passing (Ying et al., 2021). Pretraining strategies include context prediction (Hu et al., 2020), contrastive augmentation (You et al., 2020), and masked node/edge modeling (Rong et al., 2020).\n\nBeyond architectural choices, recent studies investigate multi-task objectives, scaffold-aware splitting, and augmentation with 3D conformers to reduce distribution shift (Walters and Barzilay, 2020; St. John et al., 2021). Hybrid pipelines couple learned embeddings with cheminformatic descriptors to stabilize learning in small-data regimes (Tsubaki et al., 2018).\n\nIn this paper, we explore data-efficient molecular learning under extreme label scarcity. Our approach is orthogonal to architectural innovation and can wrap around standard GNN backbones, but the details of this approach are outside the scope of this section.",
    "reason": "The span lists prior GNN variants and pretraining methods without connecting them to the paper's aims, offering no gap or author perspective (criteria a and c).",
    "start": 310,
    "end": 951,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Nguyen et al. 1",
    "document": "Related Work\n\nAbstractive summarization has evolved from encoder-decoder architectures with attention (See et al., 2017) to pretrained sequence-to-sequence models leveraging large-scale corpora (Lewis et al., 2020). Faithfulness remains a major issue, with numerous works proposing constraints and factuality rewards to mitigate hallucination (Cao et al., 2018; Kryscinski et al., 2020). Nguyen et al. 1 introduce a reranking approach that filters candidates by question-answer consistency, yielding improvements on CNN/DailyMail. However, their approach increases inference cost due to multiple forward passes.\n",
    "reason": "Wrong use of footnotes or numeric marker; should include the year (e.g., 'Nguyen et al. (YEAR)') or be properly formatted as a footnote, not 'Nguyen et al. 1'.",
    "start": 388,
    "end": 403,
    "label": "Format"
  },
  {
    "span": "(Li, 2019 and Wang, 2020)",
    "document": "Related Work\n\nFairness in machine learning encompasses group and individual notions, often in tension with accuracy under distribution shift (Dwork et al., 2012; Hardt et al., 2016; Corbett-Davies & Goel, 2018). Post-processing methods calibrate decision thresholds to meet fairness constraints, while pre-processing approaches learn debiased representations (Kamiran & Calders, 2012; Zemel et al., 2013). In-processing techniques embed constraints or regularizers directly in training (Zafar et al., 2017; Agarwal et al., 2018). Causal perspectives argue for interventions on data-generating mechanisms to address unresolved confounding (Kusner et al., 2017; Kilbertus et al., 2017). We relate our approach to prior work (Li, 2019 and Wang, 2020) and emphasize robustness to spurious correlations via distributionally robust optimization (Sagawa et al., 2020; Hashimoto et al., 2018).",
    "reason": "Incorrect separator for multiple citations inside parentheses; should use a semicolon: '(Li, 2019; Wang, 2020)'.",
    "start": 722,
    "end": 747,
    "label": "Format"
  },
  {
    "span": "Our survey reveals that over 70% of practitioners prefer static code analysis tools integrated into the IDE.",
    "document": "Introduction\n\nTool support for software quality assurance spans testing frameworks, linters, and static analysis platforms. While research has focused on improving precision and recall of bug detectors, the adoption of these tools is shaped by developer workflows and integration costs (Johnson et al., 2013; Kim and Ernst, 2007).\n\nOur survey reveals that over 70% of practitioners prefer static code analysis tools integrated into the IDE. Prior work on human factors in static analysis highlights alert fatigue, actionable feedback, and explainability as key determinants of sustained use (Christakis and Bird, 2016). Continuous integration pipelines further influence when and how developers act on alerts (Beller et al., 2017).\n\nWe present a mixed-methods study combining telemetry logs with interviews to quantify the impact of just-in-time explanations on alert triage time and fix rates across large repositories.",
    "reason": "Presents a precise statistic without citing a source or providing evidence for the claimed survey result.",
    "start": 332,
    "end": 440,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most e-commerce review datasets contain around 10% deceptive reviews.",
    "document": "Introduction\n\nOnline marketplaces rely on user reviews to guide purchasing decisions, yet deceptive opinion spam undermines trust and platform integrity. Most e-commerce review datasets contain around 10% deceptive reviews. Prior research typically frames deception detection as a supervised classification problem with linguistic and behavioral features. In this work, we revisit cross-domain generalization for deception detection using lightweight contrastive objectives.",
    "reason": "Presents a quantitative statistic without evidence or citation; per the definition, statistics require supporting sources.",
    "start": 154,
    "end": 223,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Khan et al. 2017)",
    "document": "Related Work\n\nObject detection has evolved from region-based methods to unified one-stage detectors with improved speed-accuracy trade-offs (Ren et al., 2015; Redmon et al., 2016; Liu et al., 2016). Feature pyramid networks enable multi-scale reasoning, while deformable convolutions better model geometric variability (Lin et al., 2017; Zhu et al., 2019). Anchor-free designs simplify training and reduce hyperparameter sensitivity (Tian et al., 2019). Early work (Khan et al. 2017) explored cascaded refinement for small objects, complemented by more recent transformer-based detectors that unify detection and relation modeling (Carion et al., 2020; Zhu et al., 2020).",
    "reason": "Missing comma between authors and year in parenthetical citation; should be \"(Khan et al., 2017)\".",
    "start": 465,
    "end": 483,
    "label": "Format"
  },
  {
    "span": "The CoNLL-03 dataset is known to contain annotation inconsistencies in the PER and ORG classes.",
    "document": "Related Work\n\nSequence labeling for named entity recognition commonly evaluates on newswire datasets such as CoNLL-03 and OntoNotes. The CoNLL-03 dataset is known to contain annotation inconsistencies in the PER and ORG classes. Recent approaches address label noise via confidence-based loss reweighting and consistency regularization, while others propose span-level calibration to improve threshold selection. Although pretrained encoders dominate leaderboard results, robust evaluation requires understanding how noise, domain shift, and label schemas interact. We study these factors by constructing controlled perturbations of entity boundaries and types.",
    "reason": "Makes a specific claim about dataset annotation issues without citing any analysis documenting these inconsistencies (rule b).",
    "start": 133,
    "end": 228,
    "label": "Unsupported_claim"
  },
  {
    "span": "Model-based reinforcement learning enables planning in ranking environments (Chen et al., 2019). Fairness auditing frameworks highlight exposure disparities across groups (Sapiezynski et al., 2019). Graph neural networks capture high-order user-item relations (Wang et al., 2019).",
    "document": "Related Work\n\nRecommendation systems must balance accuracy, efficiency, and societal concerns such as fairness and diversity. Recent trends explore interactive learning paradigms alongside structural modeling of user-item graphs.\n\nModel-based reinforcement learning enables planning in ranking environments (Chen et al., 2019). Fairness auditing frameworks highlight exposure disparities across groups (Sapiezynski et al., 2019). Graph neural networks capture high-order user-item relations (Wang et al., 2019). Our work studies counterfactual estimation for slate recommendation.\n\nWe build on logged bandit feedback to estimate policy improvement under position bias.",
    "reason": "The cited works jump from RL to fairness auditing to GNNs without articulating relationships or transitions, making the connection between them abrupt and unclear.",
    "start": 231,
    "end": 511,
    "label": "Coherence"
  },
  {
    "span": "CRF-based models dominated early work (Lafferty et al., 2001). ELMo and BERT pretraining boosted biomedical NER (Peters et al., 2018; Devlin et al., 2019). Domain-specific models like BioBERT and SciBERT improved further (Lee et al., 2020; Beltagy et al., 2019). Gazetteers remain useful in some settings (Campos et al., 2013).",
    "document": "Introduction\n\nBiomedical Named Entity Recognition\n\nBiomedical NER identifies mentions of genes, diseases, and chemicals in scientific text, enabling downstream curation and knowledge graph construction. Challenges include long-tail terminology, ambiguity, and domain shift across corpora.\n\nMethods and Resources\n\nCRF-based models dominated early work (Lafferty et al., 2001). ELMo and BERT pretraining boosted biomedical NER (Peters et al., 2018; Devlin et al., 2019). Domain-specific models like BioBERT and SciBERT improved further (Lee et al., 2020; Beltagy et al., 2019). Gazetteers remain useful in some settings (Campos et al., 2013).\n\nEvaluation Benchmarks\n\nCorpora such as BC5CDR, NCBI-Disease, and JNLPBA are widely used (Li et al., 2016; Doğan et al., 2014; Collier and Kim, 2004). Our work studies cross-corpus generalization with uncertainty estimation.",
    "reason": "The paragraph lists model classes and resources without explaining their connections or transitions (e.g., how pretraining relates to domain-specific models or gazetteers), leading to weak coherence across sentences.",
    "start": 313,
    "end": 640,
    "label": "Coherence"
  },
  {
    "span": "The WNLI subset is notoriously noisy and is typically excluded from evaluation.",
    "document": "Introduction\n\nGeneral language understanding benchmarks such as GLUE and SuperGLUE have catalyzed progress by standardizing evaluation across diverse tasks (Wang et al., 2018; Wang et al., 2019). While many tasks in these suites are well-curated, some subsets pose annotation and format challenges that complicate fair comparison. The WNLI subset is notoriously noisy and is typically excluded from evaluation. As models approach saturation on several GLUE tasks, researchers have turned to diagnostic datasets and adversarial testbeds to probe robustness (Ribeiro et al., 2020; Gardner et al., 2021).",
    "reason": "Makes a specific claim about a dataset’s quality and common evaluation practice without providing a supporting citation, violating rule (b).",
    "start": 331,
    "end": 410,
    "label": "Unsupported_claim"
  },
  {
    "span": "Time-series anomaly detection approaches include reconstruction-based autoencoders, predictive forecasting residuals, variational methods, and adversarial training (Malhotra et al., 2016; Hundman et al., 2018; Xu et al., 2018; Li et al., 2019). These methods span both univariate and multivariate settings with diverse architectural choices.",
    "document": "Introduction\n\nDetecting anomalies in time series is crucial for monitoring industrial systems, financial markets, and cloud services. The task is difficult due to non-stationarity, seasonality, and rare-event scarcity. Time-series anomaly detection approaches include reconstruction-based autoencoders, predictive forecasting residuals, variational methods, and adversarial training (Malhotra et al., 2016; Hundman et al., 2018; Xu et al., 2018; Li et al., 2019). These methods span both univariate and multivariate settings with diverse architectural choices.\n\nThis paper investigates anomaly detection under abrupt distribution shifts and delayed labels, evaluating a suite of training strategies and calibration techniques on benchmark datasets and proprietary telemetry.",
    "reason": "The span catalogs categories and citations but provides no synthesis or explanation of how these prior methods inform, contrast with, or motivate the present study.",
    "start": 219,
    "end": 560,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There are many recent works that explore multilingual pretraining for ASR",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) in low-resource languages remains challenging due to limited labeled audio, domain shift, and heterogeneous acoustic conditions. Transfer learning from high-resource languages is a common strategy, and self-supervised objectives further reduce reliance on transcriptions by exploiting large amounts of unlabeled speech.\n\nThere are many recent works that explore multilingual pretraining for ASR and show gains on typologically diverse language families. These results suggest that shared subword inventories and cross-lingual acoustic representations are beneficial, even when target languages have minimal annotated data.\n\nOur study complements this literature by isolating the effects of vocabulary granularity and by evaluating robustness under heavy code-switching and noisy channel conditions.",
    "reason": "Uses the vague phrase “recent works” to make a claim about prior literature without providing citations to any of those works.",
    "start": 370,
    "end": 443,
    "label": "Unsupported_claim"
  },
  {
    "span": "A previous study showed that entropy regularization always accelerates convergence in sparse-reward domains.",
    "document": "Related Work\n\nEntropy regularization encourages exploration and has been integrated into many policy gradient methods (Williams, 1992; Mnih et al., 2016). Maximum entropy RL formalizes a stochastic objective, leading to more robust policies and improved stability (Haarnoja et al., 2018; Abdolmaleki et al., 2018). In practice, temperature tuning is crucial to balance exploration and exploitation, and adaptive schemes have been proposed to automate this process (Zhang et al., 2020; Xie et al., 2021).\n\nIn sparse-reward settings, auxiliary signals and intrinsic motivation aim to mitigate exploration challenges (Pathak et al., 2017; Burda et al., 2019). A previous study showed that entropy regularization always accelerates convergence in sparse-reward domains. While empirical evidence supports benefits of higher entropy early in training, the general conditions under which entropy helps remain underexplored.\n\nOur work develops a theoretically grounded adaptive temperature controller that provably bounds performance degradation while preserving sufficient exploration. We complement analysis with benchmarks on Atari and continuous-control tasks featuring sparse rewards.",
    "reason": "Claims a result from a 'previous study' without citing that study, and uses an absolute qualifier ('always') that requires strong evidence.",
    "start": 657,
    "end": 765,
    "label": "Unsupported_claim"
  },
  {
    "span": "Deep approaches span LSTM autoencoders (Malhotra et al., 2016), variational models (An and Cho, 2015), temporal convolutional networks (Bai et al., 2018), and graph-based detectors for multivariate data (Deng and Hooi, 2021; Xu et al., 2021).",
    "document": "Related Work\n\nTime-series anomaly detection is critical for monitoring complex systems, where anomalies may be rare, contextual, and interdependent across signals. Modern methods leverage deep architectures to model temporal dynamics and cross-channel structure.\n\nDeep approaches span LSTM autoencoders (Malhotra et al., 2016), variational models (An and Cho, 2015), temporal convolutional networks (Bai et al., 2018), and graph-based detectors for multivariate data (Deng and Hooi, 2021; Xu et al., 2021).\n\nWe introduce a probabilistic forecasting model with structured low-rank attention that captures shared rhythms across channels and provides calibrated anomaly scores using conformal quantiles.",
    "reason": "Provides a list of prior deep methods without synthesizing their strengths/weaknesses or positioning the proposed model with respect to identified gaps.",
    "start": 264,
    "end": 506,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Miller and Rao, 2017)",
    "document": "Related Work\n\nOrganizational behavior research links workload, autonomy, and perceived fairness to burnout and turnover intentions (Maslach and Leiter, 2016; Colquitt et al., 2013). In software engineering teams, asynchronous communication and on-call rotations exacerbate these pressures, particularly in globally distributed settings (Treude et al., 2019). Prior work associates notification volume with stress and reduced focus (Miller and Rao, 2017), motivating interventions that batch alerts and prioritize high-urgency messages (Pielot et al., 2017).\n\nWe build on this literature by modeling personal alert tolerance and demonstrating that individualized batching policies reduce interrupt costs without delaying critical responses.",
    "reason": "Wrong conjunction in an APA-style parenthetical citation; within parentheses APA uses '&' instead of 'and', e.g., '(Miller & Rao, 2017)'.",
    "start": 431,
    "end": 453,
    "label": "Format"
  },
  {
    "span": "Johnson et al.",
    "document": "Related Work\n\nOffline reinforcement learning (RL) focuses on learning policies from static datasets without further environment interaction. Johnson et al. propose conservative policy iteration to reduce overestimation bias in value functions, inspiring a line of pessimistic algorithms for offline control. Subsequent methods formalize uncertainty penalties through behavior regularization and implicit constraints (Chen and Liu, 2020; Agarwal et al., 2021). Model-based variants learn dynamics to enable planning while maintaining conservatism (Park and Sun, 2022).\n\nDespite progress, performance degrades when datasets contain narrow coverage or high confounding. Our work combines uncertainty-aware value estimation with policy support constraints to address these issues in continuous control tasks.",
    "reason": "Narrative citation missing the publication year; should be formatted as “Johnson et al. (YEAR)”.",
    "start": 141,
    "end": 155,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on TOEFL essays with pairwise ranking as the objective.",
    "document": "Related Work\n\nAutomated essay scoring (AES) has a long history spanning hand-crafted features, regression models, and, more recently, neural encoders that operate on raw text. Transformer-based encoders have quickly become the de facto backbone in AES due to their ability to capture long-range dependencies and stylistic cues.\n\nBERT was used in an AES task trained on TOEFL essays with pairwise ranking as the objective. Other studies incorporate auxiliary signals such as discourse structure and prompt relevance, reporting improvements on cross-prompt generalization. Yet, concerns remain regarding construct validity, fairness across demographics, and the susceptibility of AES systems to adversarial perturbations.\n\nWe contribute a transparent evaluation protocol for AES that emphasizes robustness, aligns with rater guidelines, and quantifies the trade-off between text length, content relevance, and linguistic quality.",
    "reason": "This sentence describes a specific setup (task, dataset, objective) without citing the corresponding prior work (violates rule a and example iii).",
    "start": 329,
    "end": 421,
    "label": "Unsupported_claim"
  },
  {
    "span": "It is well known that graph neural networks become over-smoothed after only two or three message-passing layers.",
    "document": "Introduction\n\nGraph neural networks (GNNs) excel at learning from relational structure by iteratively exchanging information over edges. Despite their success, training deeper GNNs often leads to degraded performance.\n\nIt is well known that graph neural networks become over-smoothed after only two or three message-passing layers. This observation has motivated architectural modifications and regularizers that preserve node-level distinction across layers.",
    "reason": "Asserts a field-specific, nontrivial phenomenon as common knowledge without evidence or citation; per rule (b), such niche claims should be cited.",
    "start": 219,
    "end": 331,
    "label": "Unsupported_claim"
  },
  {
    "span": "GRU-based detectors have been widely used for industrial sensor streams.",
    "document": "Introduction\n\nTime series anomaly detection aims to identify unusual patterns that may indicate faults or security breaches. Classical approaches include statistical forecasting with residual thresholding, while recent neural methods model complex temporal dependencies with convolutional and recurrent architectures (Hundman et al., 2018; Lai et al., 2018). GRU-based detectors have been widely used for industrial sensor streams. Yet these methods often lack calibration and produce high false positive rates under distribution shifts (Ren et al., 2019). We propose a probabilistic detection framework with uncertainty-aware thresholds and domain adaptation.",
    "reason": "Makes a domain-specific claim about widespread use without any supporting citations.",
    "start": 359,
    "end": 431,
    "label": "Unsupported_claim"
  },
  {
    "span": "Neural abstractive summarizers often hallucinate content, leading to factual inconsistency (Cao et al., 2018; Kryscinski et al., 2019; Maynez et al., 2020). Post-hoc verification models and constrained decoding have been proposed to mitigate this issue (Zhou et al., 2021; Dong et al., 2020; Chen et al., 2021).",
    "document": "Introduction\n\nAbstractive summarization systems have achieved strong fluency and coverage but still produce content that departs from source facts. Understanding and mitigating factual errors is essential for trustworthy summarization in high-stakes domains such as news and medicine.\n\nNeural abstractive summarizers often hallucinate content, leading to factual inconsistency (Cao et al., 2018; Kryscinski et al., 2019; Maynez et al., 2020). Post-hoc verification models and constrained decoding have been proposed to mitigate this issue (Zhou et al., 2021; Dong et al., 2020; Chen et al., 2021).\n\nWe explore a retrieval-grounded decoding framework that conditions generation on evidence snippets identified at inference time. Our approach decouples faithfulness control from base model pretraining and yields consistent factuality gains on multiple datasets.",
    "reason": "The span summarizes prior work categories and cites papers but does not explain how these methods relate to each other or to the current study, nor does it identify a specific gap motivating the new approach.",
    "start": 286,
    "end": 597,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Trigeorgis et al. (2016) modeled raw audio with end-to-end architectures. Zadeh et al. (2017) constructed the MOSI dataset for multimodal sentiment analysis. Albanie et al. (2018) learned cross-modal embeddings for emotion recognition. Tsai et al. (2019) introduced multimodal transformers for aligned and unaligned data.",
    "document": "Related Work\n\nMultimodal Emotion Recognition\n\nEmotion recognition blends cues from speech, facial expressions, and text. Approaches differ in feature extraction, temporal modeling, and cross-modal fusion, often trading off robustness and latency. Dataset scale and annotation granularity significantly affect reported gains.\n\nTrigeorgis et al. (2016) modeled raw audio with end-to-end architectures. Zadeh et al. (2017) constructed the MOSI dataset for multimodal sentiment analysis. Albanie et al. (2018) learned cross-modal embeddings for emotion recognition. Tsai et al. (2019) introduced multimodal transformers for aligned and unaligned data.\n\nWe complement prior fusion strategies with a causal alignment module that reduces spurious correlations across modalities under distribution shift.",
    "reason": "The cited works are listed without transitions or explicit relations; the span mixes dataset creation and modeling papers without explaining their connection.",
    "start": 326,
    "end": 647,
    "label": "Coherence"
  },
  {
    "span": "several prior papers have proved that demographic parity is ill-suited for text classifiers",
    "document": "Related Work\n\nFairness in NLP encompasses both representational harms in learned embeddings and allocative harms in downstream decisions. Among group fairness notions, demographic parity, equalized odds, and calibration capture different trade-offs between utility and equity. In classification settings with language-mediated features, several prior papers have proved that demographic parity is ill-suited for text classifiers, due to correlations between linguistic markers and target labels. Alternatives such as conditional parity or counterfactual fairness have been proposed to address these limitations. Our study evaluates these criteria under distribution shift and label imbalance.",
    "reason": "Claims consensus and proof across 'several prior papers' without providing citations (rule b/d; prior work assertions require references).",
    "start": 337,
    "end": 428,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Brown and Lee, 2021)",
    "document": "Related Work\n\nNeural text summarization spans extractive and abstractive paradigms (Nallapati et al., 2017; See et al., 2017). Prior neural extractive models (Brown and Lee, 2021) rely on sentence-level scoring with pretraining, whereas encoder-decoder architectures increasingly dominate abstractive methods (Lewis et al., 2020; Raffel et al., 2020). Training objectives range from maximum likelihood to reinforcement learning (Paulus et al., 2018), and recent work explores factuality constraints (Kryscinski et al., 2020; Dong et al., 2020). We complement this literature with controllable extractive objectives (Nguyen & Chen, 2020) tailored to domain-specific constraints.",
    "reason": "Wrong conjunction in a parenthetical citation under APA-like style; within parentheses, multiple-author citations should use '&' instead of 'and', i.e., '(Brown & Lee, 2021)'.",
    "start": 158,
    "end": 179,
    "label": "Format"
  },
  {
    "span": "Pathak et al. (2017) proposed intrinsic curiosity based on forward dynamics. Burda et al. (2019) introduced random network distillation as a novelty measure. Bellemare et al. (2016) studied count-based exploration with pseudo-counts. Tang et al. (2017) extended pseudo-count exploration to high-dimensional settings.",
    "document": "Related Work\n\nExploration in reinforcement learning uses intrinsic rewards, uncertainty, and novelty estimation to guide agents in sparse-reward environments. Methods vary in their computational and statistical properties.\n\nPathak et al. (2017) proposed intrinsic curiosity based on forward dynamics. Burda et al. (2019) introduced random network distillation as a novelty measure. Bellemare et al. (2016) studied count-based exploration with pseudo-counts. Tang et al. (2017) extended pseudo-count exploration to high-dimensional settings.\n\nOur method integrates an uncertainty-aware novelty signal with policy regularization, targeting stability under non-stationary observations.",
    "reason": "The span lists exploration methods without transitions or explicit relationships among them, leaving the reader to infer the connections and reducing coherence.",
    "start": 224,
    "end": 540,
    "label": "Coherence"
  },
  {
    "span": "Medical named entity recognition in German has been scarcely studied.",
    "document": "Related Work\n\nClinical and biomedical named entity recognition (NER) enables downstream tasks such as cohort selection, adverse event detection, and clinical decision support. Most progress has focused on English due to resource availability, leaving other languages under-explored.\n\nMedical named entity recognition in German has been scarcely studied. Existing multilingual transfer approaches may underperform due to domain shift and terminological variation. We investigate domain-adaptive pretraining with synthetic perturbations to improve cross-lingual robustness for German clinical narratives.",
    "reason": "Broad, domain-specific claim about the state of prior work without supporting citations (definition b).",
    "start": 284,
    "end": 353,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Graham and Lee, 2020",
    "document": "Introduction\n\nCross-lingual transfer capitalizes on shared subword representations to enable zero-shot performance in new languages (Artetxe and Schwenk, 2019). However, lexical overlap alone does not guarantee syntactic or pragmatic transfer (Nguyen et al., 2021). Prior analysis attributes failures to mismatched discourse markers and idiomatic usage (Graham and Lee, 2020, while recent work emphasizes the role of task-specific adapters (Santos et al., 2022). We investigate whether lightweight alignment objectives reduce such mismatches without increasing inference latency.",
    "reason": "Missing closing parenthesis in a parenthetical citation; should be \"(Graham and Lee, 2020)\".",
    "start": 353,
    "end": 374,
    "label": "Format"
  },
  {
    "span": "BioBERT was originally trained on 18 billion words of PubMed",
    "document": "Introduction\n\nDomain-specific language models have become foundational for biomedical text mining, improving performance on named entity recognition, relation extraction, and question answering. Pretraining on in-domain corpora allows models to capture terminology and writing conventions particular to biomedicine.\n\nAmong such models, BioBERT and related variants extend general-purpose encoders with further pretraining on biomedical articles. BioBERT was originally trained on 18 billion words of PubMed, motivating downstream evaluations on entity-heavy datasets and clinical notes. Subsequent work has refined tokenization schemes, scaling strategies, and continual pretraining to handle concept drift as new literature emerges.\n\nDespite strong aggregate scores, specialized subdomains like genomics and radiology still exhibit coverage gaps. We introduce a curriculum pretraining strategy that emphasizes rare term acquisition using a difficulty-aware sampling process. The approach aims to boost performance on low-frequency concepts without degrading general biomedical understanding.\n\nWe evaluate across a suite of biomedical benchmarks and provide detailed error taxonomies to illuminate remaining challenges such as abbreviation ambiguity and nested entities.",
    "reason": "Claims a precise pretraining corpus size for a specific model without citing the original source.",
    "start": 446,
    "end": 506,
    "label": "Unsupported_claim"
  },
  {
    "span": "Neural program synthesis approaches map natural language to code using sequence-to-sequence models with attention and copy mechanisms (Yin and Neubig, 2017; Rabinovich et al., 2017; Dong and Lapata, 2018). Hybrid neuro-symbolic methods constrain decoding with grammars or type systems and integrate search (Parisotto et al., 2017; Allen et al., 2019; Nye et al., 2019).",
    "document": "Introduction\n\nMapping natural language intents to executable programs enables end-users to automate tasks without expert knowledge. Challenges include grounding ambiguous utterances, adhering to language semantics, and generalizing to unseen APIs.\n\nNeural program synthesis approaches map natural language to code using sequence-to-sequence models with attention and copy mechanisms (Yin and Neubig, 2017; Rabinovich et al., 2017; Dong and Lapata, 2018). Hybrid neuro-symbolic methods constrain decoding with grammars or type systems and integrate search (Parisotto et al., 2017; Allen et al., 2019; Nye et al., 2019).\n\nWe propose a retrieval-augmented decoder that conditions on executable exemplars and enforces API contracts through a lightweight type oracle. Experiments demonstrate gains on semantic parsing and code generation benchmarks.\n",
    "reason": "The span lists families of program synthesis methods without explaining how they relate to or motivate the retrieval-augmented decoder, leaving the gap implicit.",
    "start": 249,
    "end": 618,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Miller et al (2015)",
    "document": "Related Work\n\nEnd-to-end speech recognition replaced hand-engineered pipelines with neural acoustic and language modeling (Hinton et al., 2012; Graves et al., 2013). Attention-based encoder–decoders and transducer models further improved streaming and accuracy trade-offs (Chan et al., 2016; Graves, 2012; Graves, 2013). Self-supervised pretraining reduces labeled data requirements (Baevski et al., 2020; Schneider et al., 2019).\n\nFor noisy conditions, multi-condition training and robust feature learning are common (Seltzer et al., 2013; Narang et al., 2017). Miller et al (2015) demonstrate that noise-aware training can be combined with data augmentation to recover accuracy in far-field scenarios. Our approach complements these techniques by incorporating test-time uncertainty estimation.\n",
    "reason": "Missing period after \"al\" in a narrative citation; should be \"Miller et al. (2015)\".",
    "start": 563,
    "end": 582,
    "label": "Format"
  },
  {
    "span": "(Devlin et al., 2019;;",
    "document": "Related Work\n\nPretrained language models have transformed few-shot learning by enabling task-agnostic representations refined with lightweight adapters (Houlsby et al., 2019; Pfeiffer et al., 2021). Encoder-only models excel at understanding tasks, while encoder-decoder models are strong for text-to-text transfer (Raffel et al., 2020). Masked language modeling remains a common pretraining target (Devlin et al., 2019;; Liu et al., 2019), yet the interaction between masking schemes and adapter placement is underexplored. We investigate adapter routing conditioned on token uncertainty.",
    "reason": "Extra semicolon inside the parenthetical citation list; should be a single separator: '(Devlin et al., 2019; Liu et al., 2019)'.",
    "start": 399,
    "end": 421,
    "label": "Format"
  },
  {
    "span": "BLEU has been widely criticized for failing to capture adequacy and fluency in low-resource settings.",
    "document": "Introduction\n\nAutomatic evaluation remains central to machine translation (MT) research because human judgments are costly and slow to obtain. Despite decades of progress, many studies still rely on n-gram overlap metrics for benchmarking new systems and datasets. However, the community has increasingly emphasized the need to assess meaning preservation and linguistic quality beyond surface similarity.\n\nBLEU has been widely criticized for failing to capture adequacy and fluency in low-resource settings. Emerging alternatives aim to incorporate semantic representations and reference-free signals to better correlate with human preferences. In this work, we revisit these concerns by constructing a multilingual low-resource suite and comparing a set of correlation-oriented metrics under a unified protocol.",
    "reason": "Makes a broad field-wide critique without citing supporting studies; needs references to prior analyses (rules b, d).",
    "start": 407,
    "end": 508,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent graph neural network approaches model road networks as graphs and aggregate spatial features via neighborhood message passing (Li et al., 2018; Yu et al., 2018; Wu et al., 2019). Temporal dynamics are captured with recurrent or attention-based encoders layered atop the graph backbone (Zhao et al., 2019; Guo et al., 2019; Bai et al., 2020).",
    "document": "Related Work\n\nSpatio-temporal traffic forecasting seeks to predict future conditions from past observations on sensor networks. Early statistical models relied on linear assumptions that struggle with nonlinear congestion patterns and irregular sensor layouts.\n\nRecent graph neural network approaches model road networks as graphs and aggregate spatial features via neighborhood message passing (Li et al., 2018; Yu et al., 2018; Wu et al., 2019). Temporal dynamics are captured with recurrent or attention-based encoders layered atop the graph backbone (Zhao et al., 2019; Guo et al., 2019; Bai et al., 2020).\n\nDatasets such as METR-LA and PEMS-BAY have become standard benchmarks, with evaluation often reported in MAE and RMSE. However, event-driven disruptions and sensor outages introduce covariate shifts that remain underexplored. Our work investigates adaptive graph construction under distribution shift.",
    "reason": "Enumerates prior GNN and temporal models without connecting them to the paper's approach or stating a specific gap in that segment (criteria a and b).",
    "start": 262,
    "end": 610,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Neural radiance fields (NeRF) and derivatives model scenes with implicit volumetric representations, with extensions for fast rendering, dynamic scenes, and sparse views (Mildenhall et al., 2020; Barron et al., 2021; Garbin et al., 2021; Reiser et al., 2021; Müller et al., 2022). Many methods introduce multiresolution grids, hash encodings, or factorized decompositions to accelerate training and inference (Takikawa et al., 2021; Yu et al., 2021; Fridovich-Keil et al., 2022). We build on NeRF for outdoor scenes with variable illumination.",
    "document": "Introduction\n\nLearning continuous 3D scene representations from images has transformed novel view synthesis. However, outdoor environments present challenges such as variable lighting, large-scale geometry, and transient objects.\n\nNeural radiance fields (NeRF) and derivatives model scenes with implicit volumetric representations, with extensions for fast rendering, dynamic scenes, and sparse views (Mildenhall et al., 2020; Barron et al., 2021; Garbin et al., 2021; Reiser et al., 2021; Müller et al., 2022). Many methods introduce multiresolution grids, hash encodings, or factorized decompositions to accelerate training and inference (Takikawa et al., 2021; Yu et al., 2021; Fridovich-Keil et al., 2022). We build on NeRF for outdoor scenes with variable illumination.\n\nWe evaluate on driving datasets and report improvements in PSNR and relighting robustness.",
    "reason": "The span lists prior NeRF variants and then states the paper’s focus without clarifying what these methods lack for outdoor illumination or how the new approach addresses it; it lacks a synthesized argument.",
    "start": 231,
    "end": 774,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Privacy-preserving federated learning leverages secure aggregation, local differential privacy, and gradient perturbation to mitigate leakage risks (Bonawitz et al., 2017; Abadi et al., 2016; Geyer et al., 2017). Additional approaches consider client-side subsampling, encryption for server-side aggregation, and compression schemes to reduce both communication and exposure.",
    "document": "Related Work: Privacy in Federated Learning\n\nFederated learning enables model training across distributed clients without centralizing raw data. However, gradient updates can still reveal sensitive information about participants, motivating robust privacy mechanisms.\n\nPrivacy-preserving federated learning leverages secure aggregation, local differential privacy, and gradient perturbation to mitigate leakage risks (Bonawitz et al., 2017; Abadi et al., 2016; Geyer et al., 2017). Additional approaches consider client-side subsampling, encryption for server-side aggregation, and compression schemes to reduce both communication and exposure.\n\nAuditing and attack studies have demonstrated susceptibility to property inference and reconstruction in federated settings, spurring interest in stronger adversarial threat models and accountability protocols.\n\nWe present a federated training procedure with adaptive noise scaling and cohort-aware aggregation.",
    "reason": "The span lists techniques and references without explaining their limitations or how they relate to the authors' approach; there is no synthesis or stated motivation (criteria a and c).",
    "start": 269,
    "end": 644,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent progress includes graph convolutional recommenders, session-based graph models, and multi-relational message passing (Khan et al., 2018; Wu and Li, 2019; Xu et al., 2020; Chen et al., 2021; Park et al., 2022). These systems apply neighborhood aggregation, attention over item-item graphs, and higher-order connectivity to improve top-k accuracy across benchmarks (He et al., 2020; Jin et al., 2021).",
    "document": "Introduction\n\nLearning user and item representations from interaction graphs has become a central strategy in modern recommender systems. The graph perspective emphasizes the structural dependencies induced by co-consumption, temporal proximity, and side-information relations. By leveraging message passing operations, recommenders can propagate signals across higher-order neighborhoods and better capture collaborative effects beyond pairwise similarities.\n\nRecent progress includes graph convolutional recommenders, session-based graph models, and multi-relational message passing (Khan et al., 2018; Wu and Li, 2019; Xu et al., 2020; Chen et al., 2021; Park et al., 2022). These systems apply neighborhood aggregation, attention over item-item graphs, and higher-order connectivity to improve top-k accuracy across benchmarks (He et al., 2020; Jin et al., 2021). Despite these developments, practitioners still deploy factorization-based baselines due to their stability and ease of serving at scale.\n\nIn this paper we present LiteGNN, a sparse and training-efficient graph recommender designed for industrial latency budgets. We evaluate LiteGNN on three public datasets and one large-scale production corpus, and report improvements in NDCG and recall under fixed compute and memory envelopes.\n",
    "reason": "The span lists prior works and their features without explaining how they relate to the current paper’s aims or identifying a specific gap; it summarizes literature but does not synthesize it with the authors’ argument.",
    "start": 461,
    "end": 867,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Pretrained conversational models benefit from large-scale social media corpora (Zhang et al., 2020). Dialogue state tracking benchmarks evaluate slot accuracy (Wu et al., 2019). Knowledge-grounded response generation retrieves supporting passages (Dinan et al., 2019).",
    "document": "Introduction\n\nBuilding helpful conversational agents entails modeling context, grounding in external knowledge, and tracking user goals. Despite advances, agents often fail to maintain consistency and cite sources.\n\nPretrained conversational models benefit from large-scale social media corpora (Zhang et al., 2020). Dialogue state tracking benchmarks evaluate slot accuracy (Wu et al., 2019). Knowledge-grounded response generation retrieves supporting passages (Dinan et al., 2019). We introduce a framework that unifies grounding with uncertainty estimation.\n\nOur experiments span multi-domain goal-oriented and open-domain datasets with controlled hallucination tests.",
    "reason": "The three sentences present separate strands of dialogue research without transitions or explicit links, leaving their relevance to each other implicit and causing abrupt topic shifts.",
    "start": 216,
    "end": 484,
    "label": "Coherence"
  },
  {
    "span": "(Kumar and Singh, 2021",
    "document": "Introduction\n\nGraph representation learning has enabled state-of-the-art results on node classification and link prediction by combining message passing with attention mechanisms (Kipf and Welling, 2017; Veličković et al., 2018; Hamilton et al., 2017). Despite progress, models can oversmooth and fail under heterophily (Zhu et al., 2020; Pei et al., 2020).\n\nA line of work proposes adaptive propagation steps and personalized PageRank to mitigate oversmoothing (Klicpera et al., 2019; Li et al., 2019). Others incorporate subgraph sampling and relational inductive biases to scale to large graphs (You et al., 2020; Rossi et al., 2020).\n\nHowever, recent evidence suggests that positional encodings are necessary to capture long-range dependencies (Dwivedi et al., 2021; Rampášek et al., 2022; Kumar and Singh, 2021). In contrast to (Kumar and Singh, 2021, we introduce a topology-aware regularizer that aligns spectral frequencies across layers while preserving local semantics.\n\nWe evaluate on heterophilous benchmarks and report consistent improvements under scarce labels (Zhu et al., 2020; Pei et al., 2020).",
    "reason": "Missing closing parenthesis in a parenthetical citation: '(Kumar and Singh, 2021' should be '(Kumar and Singh, 2021)'.",
    "start": 833,
    "end": 855,
    "label": "Format"
  },
  {
    "span": "[24]",
    "document": "Introduction\n\nGraph Neural Networks for Semi-Supervised Learning\n\nGraph neural networks (GNNs) generalize convolution to non-Euclidean domains by aggregating information from node neighborhoods (Kipf and Welling, 2017; Hamilton et al., 2017). Variants address oversmoothing and scalability through architectural changes and sampling (Chen et al., 2018; Wu et al., 2019). Regularization techniques such as consistency training improve robustness under label scarcity (Xie et al., 2020; Liu et al., 2020). While prior work [24] proposes hierarchical pooling to capture graph structure, other studies emphasize positional encodings and spectral properties (Dwivedi et al., 2020; Kremer et al., 2022). We extend semi-supervised objectives with structure-aware pseudo-labeling to exploit unlabeled subgraphs.",
    "reason": "Wrong citation style: numeric bracket '[24]' used in an author–year context.",
    "start": 521,
    "end": 525,
    "label": "Format"
  },
  {
    "span": "the Yahoo S5 dataset contains anomalies curated by experts",
    "document": "Introduction\n\nTime series anomaly detection underpins monitoring in IT operations, finance, and manufacturing. Modern methods span reconstruction-based autoencoders, probabilistic forecasting with uncertainty thresholds, and representation learning with contrastive objectives.\n\nBenchmarking is complicated by heterogeneous data sources and inconsistent labeling practices. In this context, the Yahoo S5 dataset contains anomalies curated by experts, whereas other collections mix synthetic and weakly supervised labels.\n\nWe present a unified evaluation harness with leakage-resistant splits and propose a simple probabilistic calibration layer that harmonizes scores across models, yielding more reliable precision–recall trade-offs.",
    "reason": "Mentions a dataset property without providing a citation to support it per rule (a)/(b).",
    "start": 391,
    "end": 449,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Nguyen et al. 2020)",
    "document": "Related Work\n\nGraph neural networks (GNNs) have been extended with attention mechanisms to better capture long-range dependencies (Velickovic et al., 2018; Brody and Alon, 2021). In semi-supervised settings, consistency regularization on perturbed graphs improves label efficiency, as suggested by (Nguyen et al. 2020) and further explored by topology-aware augmentations (Huang and Sun, 2021). Our method integrates calibration-driven perturbations with a teacher–student framework to reduce confirmation bias (Li and He, 2022).",
    "reason": "Missing comma before the year in a parenthetical citation; it should be '(Nguyen et al., 2020)'.",
    "start": 298,
    "end": 318,
    "label": "Format"
  },
  {
    "span": "Recent works demonstrate that long-document transformers reliably outperform extractive baselines on news and scientific corpora.",
    "document": "Introduction\n\nAbstractive summarization has progressed rapidly with neural sequence-to-sequence architectures, attention mechanisms, and large-scale pretraining (See et al., 2017; Lewis et al., 2020; Zhang et al., 2020). While early methods struggled with factual consistency, pretrained encoder–decoder models have improved both fluency and content selection by conditioning on richer contextual representations. Long-context architectures such as sparse-attention Transformers have further enabled processing multi-thousand-token inputs by reducing the quadratic cost of standard attention (Beltagy et al., 2020; Zaheer et al., 2020).\n\nDespite these advances, modeling discourse structure and handling cross-document redundancy remain open challenges in long-document summarization. Recent works demonstrate that long-document transformers reliably outperform extractive baselines on news and scientific corpora. In this paper, we revisit these claims by conducting a controlled comparison that accounts for input truncation, training compute, and evaluation variance.",
    "reason": "Mentions 'recent works' with a comparative performance claim but provides no citations to those works (rule d; also a/b for prior work).",
    "start": 785,
    "end": 914,
    "label": "Unsupported_claim"
  },
  {
    "span": "The CNN/DailyMail dataset is the de facto benchmark for abstractive summarization.",
    "document": "Introduction\n\nAbstractive text summarization aims to generate concise summaries that capture the salient content of long documents. Neural encoder–decoder models with attention laid the groundwork for modern approaches (Rush et al., 2015; Chopra et al., 2016), and pointer-generator mechanisms improved factual grounding and coverage (See et al., 2017). More recently, pre-trained transformers have driven large gains by leveraging large-scale unlabeled corpora (Liu and Lapata, 2019; Lewis et al., 2020).\n\nThe CNN/DailyMail dataset is the de facto benchmark for abstractive summarization. Despite the progress enabled by large-scale pre-training, current systems still struggle with factual consistency and faithfulness (Kryściński et al., 2019), motivating approaches that incorporate reasoning or retrieval (Zhang et al., 2020; Xu et al., 2020). In this paper, we investigate a constrained decoding strategy that integrates entity-level constraints without sacrificing fluency.\n\nOur contributions are threefold: (1) we propose a constraint-aware decoding objective compatible with standard transformer summarizers; (2) we introduce an entity recall metric targeted at factual overlap; and (3) we demonstrate improvements across news and scientific domains.",
    "reason": "First mention of a specific dataset and a claim about its benchmark status lacks a supporting citation (violates rule a and b).",
    "start": 507,
    "end": 589,
    "label": "Unsupported_claim"
  },
  {
    "span": "Text as treatment or mediator has been modeled with embeddings and topic models to adjust for confounding (Roberts et al., 2020; Egami et al., 2018; Veitch et al., 2020; Pryzant et al., 2021).",
    "document": "Related Work\n\nEstimating causal effects with text arises in settings where interventions are communicated through language or where unstructured text contains confounders not captured by structured covariates. Aligning causal identification with high-dimensional representation learning is an active area of research.\n\nChallenges include post-treatment bias, measurement error from representation learning, and violations of ignorability when key confounders are missing or poorly encoded.\n\nText as treatment or mediator has been modeled with embeddings and topic models to adjust for confounding (Roberts et al., 2020; Egami et al., 2018; Veitch et al., 2020; Pryzant et al., 2021).\n\nOur approach introduces identification-aware representation learning that jointly optimizes balance and predictive sufficiency with a latent confounder simulator to stress-test ignorability assumptions.",
    "reason": "The span enumerates prior modeling approaches with citations but does not integrate their assumptions, limitations, or connection to the authors' method; it lacks synthesis and explicit motivation.",
    "start": 491,
    "end": 683,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Malhotra et al. (2016) model normal sequences with LSTM autoencoders. Hundman et al. (2018) forecast spacecraft telemetry for anomaly detection. Keogh et al. (2005) introduce discord discovery in time series. Su et al. (2019) propose robust streaming detection with seasonal decomposition.",
    "document": "Related Work\n\nTime Series Anomaly Detection\n\nAnomaly detection approaches include reconstruction-based criteria, predictive modeling, and distance-based discord discovery. Selection depends on stationarity, periodicity, and streaming constraints. Malhotra et al. (2016) model normal sequences with LSTM autoencoders. Hundman et al. (2018) forecast spacecraft telemetry for anomaly detection. Keogh et al. (2005) introduce discord discovery in time series. Su et al. (2019) propose robust streaming detection with seasonal decomposition. Industrial use cases require explainability and low-latency alarms under noisy conditions.\n\nOur method unifies forecasting and reconstruction via multi-horizon conformal scoring, enabling calibrated alerts with controlled false positive rates.",
    "reason": "The span lists disparate approaches without transitions or clarifying how reconstruction, forecasting, and discord methods relate or differ, leaving the reader to infer connections across heterogeneous paradigms.",
    "start": 247,
    "end": 536,
    "label": "Coherence"
  },
  {
    "span": "Most prior work treats hate speech detection as a binary classification task.",
    "document": "Related Work\n\nOnline abuse detection encompasses a spectrum of phenomena, including hate speech, harassment, and toxic language, often requiring nuanced, context-sensitive modeling (Waseem and Hovy, 2016; Fortuna and Nunes, 2018). Dataset construction strategies range from keyword filtering to community reporting, each introducing distinct biases (Davidson et al., 2017). Most prior work treats hate speech detection as a binary classification task. Recent approaches investigate finer-grained taxonomies, contextual signals (e.g., conversation history), and cross-domain adaptation to mitigate annotation artifacts and domain shift.\n",
    "reason": "Claims a common framing in the literature without citing representative studies.",
    "start": 374,
    "end": 451,
    "label": "Unsupported_claim"
  },
  {
    "span": "Gradient-based saliency attributes node-level importance by backpropagating through the message-passing layers (Pope et al., 2019; Ying et al., 2019). Subgraph extraction learns masks that highlight influential edges (Yuan et al., 2020). Counterfactual explanations alter graph structure to change predictions (Lucic et al., 2022). Prototype learning aligns latent representations with human-interpretable motifs (Zhang et al., 2022).",
    "document": "Related Work\n\nExplainability for Graph Neural Networks\nAs GNNs gain adoption in high-stakes domains, explaining their predictions has become critical (Ying et al., 2019; Baldassarre and Azizpour, 2019). Recent approaches differ in how they attribute importance, the granularity of explanations, and the constraints they impose to ensure fidelity or plausibility.\n\nPost-hoc and Self-explaining Methods\nGradient-based saliency attributes node-level importance by backpropagating through the message-passing layers (Pope et al., 2019; Ying et al., 2019). Subgraph extraction learns masks that highlight influential edges (Yuan et al., 2020). Counterfactual explanations alter graph structure to change predictions (Lucic et al., 2022). Prototype learning aligns latent representations with human-interpretable motifs (Zhang et al., 2022). Recent work also formalizes desiderata such as sparsity, stability, and faithfulness, yet trade-offs among them remain underexplored (Agarwal et al., 2022; Schlichtkrull et al., 2021).\n\nEvaluation Protocols\nEvaluation has relied on synthetic benchmarks with ground-truth rationales and real-world tasks with proxy metrics like fidelity or overlap with known motifs (Yuan et al., 2020; Luo et al., 2020). Our study proposes a unified evaluation harness that controls for graph size and label distribution while comparing methods under consistent perturbation budgets.",
    "reason": "The span lists four distinct explanation families in separate sentences without any connective tissue or explanation of their relationships, causing abrupt shifts and unclear coherence across the cited works.",
    "start": 401,
    "end": 835,
    "label": "Coherence"
  },
  {
    "span": "Early recommender systems relied on neighborhood-based collaborative filtering and matrix factorization (Sarwar et al., 2001; Koren et al., 2009). Deep learning then brought neural collaborative filtering and autoencoder variants (He et al., 2017; Sedhain et al., 2015). More recently, graph-based approaches such as NGCF, LightGCN, and PinSage propagate signals on user–item graphs to capture higher-order connectivity (Wang et al., 2019; He et al., 2020; Ying et al., 2018). Self-supervised objectives have further improved data efficiency for recommendation (Wu et al., 2021; Xia et al., 2022).",
    "document": "Introduction\n\nPersonalized recommendation underpins many online platforms by surfacing relevant content from massive catalogs. Accurate modeling must balance sparse feedback, shifting interests, and efficiency constraints in large-scale deployments.\n\nEarly recommender systems relied on neighborhood-based collaborative filtering and matrix factorization (Sarwar et al., 2001; Koren et al., 2009). Deep learning then brought neural collaborative filtering and autoencoder variants (He et al., 2017; Sedhain et al., 2015). More recently, graph-based approaches such as NGCF, LightGCN, and PinSage propagate signals on user–item graphs to capture higher-order connectivity (Wang et al., 2019; He et al., 2020; Ying et al., 2018). Self-supervised objectives have further improved data efficiency for recommendation (Wu et al., 2021; Xia et al., 2022).\n\nIn this paper, we propose a path-aware hypergraph recommender that unifies multi-behavior interactions and side information within a single message-passing framework. Our method models long-range collaborative semantics while maintaining scalability through sparse routing. We conduct experiments on three public benchmarks and a large industrial dataset, demonstrating consistent gains over strong graph-based and self-supervised baselines.\n\nOur contributions are: (1) a hypergraph formulation capturing typed multi-hop relations, (2) a path-aware propagation operator that selectively amplifies informative structures, and (3) a scalable training recipe with negative-sampling and curriculum scheduling.",
    "reason": "The span summarizes prior recommender methods and citations without connecting them to the paper's aims, gap, or argument, satisfying (a) and (c) of the definition.",
    "start": 251,
    "end": 848,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Graph neural networks have recently been applied to molecular property prediction with great success.",
    "document": "Related Work\n\nPredicting molecular properties is central to virtual screening and drug discovery. Traditional approaches rely on fixed fingerprints such as ECFP and physicochemical descriptors with tree-based models. Graph neural networks have recently been applied to molecular property prediction with great success. Subsequent work explores message passing schemes, attention mechanisms, and pretraining objectives tailored to chemical graphs. Nevertheless, label scarcity and distribution shift across assays limit generalization, motivating semi-supervised learning and multi-task training to share signal between related endpoints. Our work investigates scaffold-aware pretraining that aligns with medicinal chemistry priors.",
    "reason": "Asserts 'recent' applications and success without citing any of the corresponding studies (rule d).",
    "start": 217,
    "end": 318,
    "label": "Unsupported_claim"
  },
  {
    "span": "(RoBERTa Liu et al. (2019))",
    "document": "Introduction\n\nPretrained language models have transformed NLP by enabling effective transfer learning across a broad range of tasks (Peters et al., 2018; Devlin et al., 2019). While BERT introduced bidirectional masked pretraining, subsequent work improved training dynamics, data, and objectives to obtain stronger encoders (Clark et al., 2020; He et al., 2021). We build on this line by exploring domain-adaptive pretraining for low-resource settings.\n\nWe fine-tune competitive baselines, including BERT (Devlin et al., 2019) and (RoBERTa Liu et al. (2019)), on entity recognition and relation extraction benchmarks. Our approach augments masked language modeling with task-aware adapters that efficiently specialize representations without full fine-tuning (Houlsby et al., 2019; Pfeiffer et al., 2020). Extensive experiments across biomedical and legal corpora show robust gains under limited supervision.\n\nFinally, we analyze calibration and out-of-domain generalization, revealing that adapter-based specialization preserves uncertainty estimates better than full fine-tuning (Guo et al., 2017; Desai and Durrett, 2020).",
    "reason": "Double nesting of parentheses and mixing narrative and parenthetical forms; the correct form is either RoBERTa (Liu et al., 2019) in narrative or (Liu et al., 2019) in parenthetical, but not '(RoBERTa Liu et al. (2019))'.",
    "start": 532,
    "end": 559,
    "label": "Format"
  },
  {
    "span": "there are many recent works that exploit prototypical networks for intent detection",
    "document": "Introduction\n\nFew-shot intent detection seeks to recognize novel intents from only a handful of labeled examples. This problem arises in production dialog systems where new functionalities are frequently introduced and annotation is costly. Traditional supervised classifiers require hundreds of examples per intent to reach acceptable accuracy, which is infeasible in rapidly evolving domains. Metric-learning approaches have emerged as a promising alternative by learning embedding spaces where classes are separable with minimal data.\n\nIn this paper, we focus on prototype-based decision rules for few-shot intent detection in multilingual settings. We observe that cross-lingual transfer is hindered by inconsistent lexical cues and domain-specific synonyms. To address this, we introduce a transductive refinement step that leverages unlabeled query utterances to adapt prototype representations at test time. Our method is model-agnostic and can be layered on top of any encoder trained with episodic objectives.\n\nDespite a large body of work in low-resource intent classification, there is limited investigation into prototype adaptation under label shift. While metric-based methods are popular, there are many recent works that exploit prototypical networks for intent detection. However, to our knowledge, none explicitly incorporate transductive adjustments that correct for distributional drift between support and query sets in multilingual intents.\n\nWe evaluate our approach on multiple domains with both in-language and cross-lingual transfer, and we report consistent gains in the most challenging 1-shot and 2-shot regimes. We further perform ablations to isolate the contribution of our refinement step and analyze failure modes where intents are semantically entangled.",
    "reason": "Mentions many recent works using prototypical networks without providing any citations to support the claim.",
    "start": 1204,
    "end": 1287,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works analyze decoding strategies for code language models (Austin et al., 2021). Program repair has also been studied extensively (Monperrus, 2018). Few-shot prompting improves synthesis quality (Brown et al., 2020; Chen et al., 2021). Static analysis tools detect defects (Johnson et al., 2013).",
    "document": "Related Work\n\nLarge language models for code generation have benefited from advances in pretraining, dataset curation, and inference-time steering. Studies have examined the role of instruction tuning, retrieval augmentation, and test-time interventions to improve program correctness and pass rates on benchmarks.\n\nRecent works analyze decoding strategies for code language models (Austin et al., 2021). Program repair has also been studied extensively (Monperrus, 2018). Few-shot prompting improves synthesis quality (Brown et al., 2020; Chen et al., 2021). Static analysis tools detect defects (Johnson et al., 2013).\n\nBeyond inference-time techniques, data filtering and deduplication reduce memorization and improve generalization in code LMs (Allamanis, 2019; Li et al., 2022). Our approach focuses on integrating test-based feedback into decoding, contrasting with prior methods that rely solely on probabilistic signals.",
    "reason": "The four consecutive sentences list distinct areas (decoding, program repair, prompting, static analysis) without transitions or an explicit relationship among them, making the connection between cited works abrupt and unclear.",
    "start": 316,
    "end": 620,
    "label": "Coherence"
  },
  {
    "span": "Most existing benchmarks evaluate on D4RL-v2 using normalized scores averaged across 10 seeds.",
    "document": "Introduction\n\nOffline reinforcement learning (RL) aims to learn policies purely from logged data, avoiding unsafe online exploration. While standardized datasets have improved comparability, methodological differences in evaluation still lead to inconsistent claims about progress. Most existing benchmarks evaluate on D4RL-v2 using normalized scores averaged across 10 seeds. Unfortunately, reporting practices differ in terms of data preprocessing, policy selection criteria, and penalty terms for out-of-distribution actions.\n\nWe propose a transparent protocol with fixed preprocessing and unified early-stopping rules, and we release an evaluation harness to replicate results across algorithms and datasets.",
    "reason": "Asserts a common evaluation practice with specific details (D4RL-v2, 10 seeds) but does not provide citations to benchmarks or papers establishing this convention (definition a and b).",
    "start": 282,
    "end": 376,
    "label": "Unsupported_claim"
  },
  {
    "span": "There is a growing body of literature showing that multimodal models exhibit systematic biases across vision and language modalities.",
    "document": "Related Work\n\nMultimodal learning integrates visual and linguistic signals to improve grounding, reasoning, and generalization. Despite rapid progress, disparities can arise from dataset composition, annotation protocols, and pretraining objectives. There is a growing body of literature showing that multimodal models exhibit systematic biases across vision and language modalities. Recent efforts seek to measure these effects via counterfactual data augmentation, balanced benchmarks, and fairness-aware training objectives. Our contribution is a diagnostic suite that isolates modality-specific artifacts and quantifies robustness under targeted shifts.",
    "reason": "References a 'growing body of literature' without citing any representative studies, which is required for such claims.",
    "start": 250,
    "end": 383,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Alvarez and Kim, 2017",
    "document": "Introduction\n\nStability and Exploration in Reinforcement Learning\n\nPolicy gradient methods provide a principled framework for optimizing stochastic policies (Sutton et al., 2000; Schulman et al., 2015). Actor-critic architectures stabilize training through value estimation (Konda and Tsitsiklis, 2000), yet exploration remains difficult (Alvarez and Kim, 2017 across continuous control tasks with sparse rewards. Recent advances adopt entropy regularization (Haarnoja et al., 2018) and intrinsic motivation (Pathak et al., 2017) to address these challenges, while distributional critics improve sample efficiency (Bellemare et al., 2017).\n",
    "reason": "Missing closing parenthesis in a parenthetical citation: should be '(Alvarez and Kim, 2017)'.",
    "start": 338,
    "end": 360,
    "label": "Format"
  },
  {
    "span": "Smith et al., (2020)",
    "document": "Related Work\n\nPretrained language models provide strong initialization for downstream tasks, but effective adaptation requires careful regularization (Howard and Ruder, 2018; Devlin et al., 2019). Adapter-based tuning preserves base parameters while enabling task specialization (Houlsby et al., 2019; Pfeiffer et al., 2021).\n\nSmith et al., (2020) show that small adapters can match full fine-tuning on sentence classification, whereas parallel adapters further improve transfer (He and Chen, 2021). Our study complements these findings by analyzing adapter sparsity patterns across domains (Kumar and Zhao, 2022).",
    "reason": "Unnecessary comma before the narrative year; should be Smith et al. (2020).",
    "start": 327,
    "end": 347,
    "label": "Format"
  },
  {
    "span": "Existing work on protein contact prediction predominantly relies on evolutionary coupling signals.",
    "document": "Introduction\n\nAccurate prediction of residue-residue contacts provides structural constraints that aid protein folding and function annotation. Existing work on protein contact prediction predominantly relies on evolutionary coupling signals. While these approaches succeed for families with rich multiple sequence alignments, performance degrades for orphan proteins with few homologs (Jones et al., 2012; Seemayer et al., 2014). Recent end-to-end transformers leverage large-scale protein language modeling to infer long-range dependencies without alignments (Rives et al., 2021; Rao et al., 2021). We investigate hybrid architectures that integrate sequence-only features with sparse evolutionary information.",
    "reason": "Claims a dominant approach in prior work but provides no citations supporting the statement.",
    "start": 144,
    "end": 242,
    "label": "Unsupported_claim"
  },
  {
    "span": "Johnson et al., (2018)",
    "document": "Introduction\n\nOut-of-distribution detection for text classifiers often relies on energy scores or temperature scaling (Liu et al., 2020; Hsu et al., 2020). Following Johnson et al., (2018), we treat confidence calibration as a first-class objective during fine-tuning. Unlike prior work that adds post-hoc temperature, we learn task-specific margins jointly with the classifier (Guo et al., 2017).",
    "reason": "Incorrect narrative citation punctuation; should be 'Johnson et al. (2018)' without the comma before the parenthesis.",
    "start": 166,
    "end": 188,
    "label": "Format"
  },
  {
    "span": "Martinez et al., (2019)",
    "document": "Introduction\n\nRobustness to spurious correlations has been pursued via reweighting, invariant risk minimization, and counterfactual augmentation. Martinez et al., (2019) demonstrated that group-aware reweighting can recover worst-group accuracy when group labels are available. Follow-up work approximated group structure with clustering (Huang and Sato, 2020) or adversarial critics (Li and Nair, 2021). We investigate counterfactual resampling strategies that require no group labels and retain standard empirical risk training.",
    "reason": "Extraneous comma before the year in a narrative citation; should be \"Martinez et al. (2019)\".",
    "start": 146,
    "end": 169,
    "label": "Format"
  },
  {
    "span": "Wang et al. 1",
    "document": "Introduction\n\nVision Transformers (ViT) have shown that pure attention-based architectures can rival or surpass convolutional networks on large-scale image classification (Dosovitskiy et al., 2021; Touvron et al., 2021). As noted by Wang et al. 1, scaling depth without appropriate regularization leads to overfitting on mid-sized datasets, motivating stronger data augmentation and stochastic depth (He et al., 2016; Huang et al., 2016). Subsequent work explores hierarchical token aggregation to improve inductive bias (Liu et al., 2021; Wang et al., 2021). Our work focuses on parameter-efficient adapters that preserve pretrained features while enabling domain specialization (Houlsby et al., 2019; He et al., 2022).\n\nWe present extensive evaluations across fine-grained recognition and remote sensing benchmarks.",
    "reason": "Wrong use of footnote-like marker: 'Wang et al. 1' lacks a year and appears as a footnote index; should be a proper citation with year or a correctly formatted footnote.",
    "start": 233,
    "end": 246,
    "label": "Format"
  },
  {
    "span": "There has been a surge of recent works investigating controllable summarization for news and scientific articles.",
    "document": "Introduction\n\nAutomatic text summarization has advanced rapidly with neural architectures enabling both extractive and abstractive paradigms (Nallapati et al., 2016; See et al., 2017). Beyond faithfulness and coherence, practical applications increasingly demand controllability, such as specifying length, style, or focus dimensions in the generated summary. There has been a surge of recent works investigating controllable summarization for news and scientific articles. Yet, a consistent evaluation framework that isolates control fidelity from summary quality remains elusive, and metrics that capture both aspects are still underexplored. In this paper, we introduce a simple control interface that expresses user intents via discrete attributes and propose an evaluation protocol that factors control accuracy, faithfulness, and fluency. We further study how different control signals interact and whether control generalizes across domains.",
    "reason": "Mentions 'recent works' without providing citations to specific papers, violating the requirement to cite first mentions of prior work.",
    "start": 360,
    "end": 473,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Nguyen, 2020",
    "document": "Related Work\n\nDialog state tracking has evolved from rule-based pipelines to neural architectures that jointly model belief states and system actions (Henderson et al., 2014; Mrkšić et al., 2017; Wu et al., 2019). Pretrained language models improve robustness to paraphrases and out-of-vocabulary slots (Rastogi et al., 2020; Hosseini-Asl et al., 2020).\n\nRecent methods incorporate schema-guided signals and cross-domain transfer (Rastogi et al., 2020; Chen et al., 2020). Others model uncertainty with Bayesian dialogue policies to mitigate exposure bias (DeVault et al., 2011; Kottur et al., 2017). A concurrent line of work studies retrieval-augmented generation for knowledge-grounded responses (Dinan et al., 2019; Lewis et al., 2020; Nguyen, 2020). Despite progress, robust tracking under noisy ASR and domain drift remains challenging.\n\nWe focus on a modular approach that learns a compact belief representation via contrastive learning and calibrates uncertainty using conformal prediction (Angelopoulos and Bates, 2022; Romano et al., 2020).",
    "reason": "Missing closing parenthesis in a parenthetical citation; should be '(Nguyen, 2020)'.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "Kumar et al. 1",
    "document": "Introduction\n\nSafe exploration balances performance with constraints on risk, often formalized via cost critics or Lagrangian methods (Achiam et al., 2017; Chow et al., 2018). Building on early constrained policy optimization, Kumar et al. 1 demonstrated that conservative penalties can stabilize training in sparse-reward environments. Yet subsequent analyses highlighted that over-penalization leads to under-exploration (Eysenbach et al., 2020), motivating adaptive constraint schedules (Stooke et al., 2020).\n\nWe contribute a unified view of conservative updates and uncertainty-aware constraints, showing improved stability without excessive conservatism across continuous control benchmarks.",
    "reason": "Wrong use of footnotes: the superscript-like '1' is used instead of a proper bibliographic year or a formatted footnote; it should be a year or a correctly formatted footnote reference.",
    "start": 227,
    "end": 241,
    "label": "Format"
  },
  {
    "span": "[Chen et al., 2016; Park and Lee, 2018)",
    "document": "Related Work\n\nDocument-level relation extraction extends sentence-level models by aggregating evidence across multiple mentions. Early pipelines relied on distant supervision with sentence classifiers (Mintz et al., 2009; Hoffmann et al., 2011). Graph-based models propagate information along entity and coreference edges [Chen et al., 2016; Park and Lee, 2018) and attention mechanisms align token spans across sentences (Zhou et al., 2021). Recent approaches employ pre-trained encoders with structured decoders, yielding strong gains but higher computational cost (Wang et al., 2022).\n\nWe propose a lightweight cross-sentence aggregator that preserves performance while reducing inference latency.",
    "reason": "Mismatched bracket types in a single citation group: it opens with '[' and closes with ')'; it should use matching delimiters, e.g., \"(Chen et al., 2016; Park and Lee, 2018)\".",
    "start": 322,
    "end": 361,
    "label": "Format"
  },
  {
    "span": "Most sentiment analysis datasets contain at least 100k labeled examples.",
    "document": "Introduction\n\nSentiment analysis benchmarks have driven progress on document and sentence-level classification, with canonical datasets such as IMDB (Maas et al., 2011) and SST (Socher et al., 2013). Most sentiment analysis datasets contain at least 100k labeled examples. While large-scale resources enable effective fine-tuning of pretrained encoders, real-world applications often involve niche domains with sparse annotations and significant class imbalance. To bridge this gap, recent research explores weak supervision, domain adaptation, and active learning strategies to reduce reliance on extensive manual labeling. In this work, we focus on budget-constrained fine-tuning with principled selection of informative examples under limited labeling budgets.",
    "reason": "Presents a quantitative generalization about dataset sizes without evidence or citation (rule b).",
    "start": 200,
    "end": 272,
    "label": "Unsupported_claim"
  },
  {
    "span": "The FetchReach benchmark has become the de facto standard for evaluating goal-conditioned policies.",
    "document": "Related Work\n\nGoal-conditioned reinforcement learning enables policies to generalize across desired outcomes by conditioning on target states or observations (Schaul et al., 2015; Andrychowicz et al., 2017). Hindsight relabeling and contrastive representation learning have improved sample efficiency and generalization in sparse-reward settings (Andrychowicz et al., 2017; Laskin et al., 2020). The FetchReach benchmark has become the de facto standard for evaluating goal-conditioned policies. Beyond table-top reaching, several works consider multi-task manipulation suites and compositional goals to test scalability (Yu et al., 2020; Gupta et al., 2018).",
    "reason": "Mentions a specific benchmark at first use and makes a prevalence claim without citing any source.",
    "start": 396,
    "end": 495,
    "label": "Unsupported_claim"
  },
  {
    "span": "Self-supervised speech pretraining has explored contrastive predictive coding, masked acoustic modeling, and predictive quantization (van den Oord et al., 2018; Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2022). Large-scale pretraining on unlabelled audio yields strong ASR results after fine-tuning (Conneau et al., 2021; Chung et al., 2021).",
    "document": "Related Work\n\nEnd-to-end ASR models. Attention-based encoder–decoders and CTC/Transducer architectures have streamlined ASR pipelines by learning acoustic, pronunciation, and language modeling jointly. However, they require substantial labeled data for competitive performance.\n\nSelf-supervised pretraining. Self-supervised speech pretraining has explored contrastive predictive coding, masked acoustic modeling, and predictive quantization (van den Oord et al., 2018; Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2022). Large-scale pretraining on unlabelled audio yields strong ASR results after fine-tuning (Conneau et al., 2021; Chung et al., 2021).\n\nDomain and robustness. Recent studies investigate noise robustness, speaker variability, and low-resource adaptation by combining augmentation, multi-condition training, and multilingual pretraining, aiming to reduce data requirements and improve generalization.\n\nWe propose curriculum-regularized masking with domain-adaptive normalization to better transfer self-supervised representations to noisy, far-field ASR.",
    "reason": "The span summarizes self-supervised techniques and outcomes without clarifying how they inform or motivate the proposed method or what specific deficiency remains.",
    "start": 308,
    "end": 660,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[12]",
    "document": "Related Work\n\nUnsupervised domain adaptation aligns feature distributions between source and target domains (Gretton et al., 2012; Ganin et al., 2016). Moment matching minimizes higher-order discrepancies (Zellinger et al., 2017), while adversarial methods learn domain-invariant representations via discriminators (Tzeng et al., 2017; Long et al., 2018). Self-training with confidence thresholds helps when target labels are absent (Zou et al., 2019). Recent consistency-based approaches enforce agreement under augmentations (French et al., 2018; Berthelot et al., 2019). Despite the prevalence of author–year citations in this literature, some works rely on numeric indexing such as [12], which complicates cross-paper comparison without a unified bibliography. We adopt an author–year style and report standardized benchmarks to facilitate reproducibility.",
    "reason": "Numeric bracket citation is inconsistent with the author–year style used elsewhere; it should be replaced with an author–year citation.",
    "start": 686,
    "end": 690,
    "label": "Format"
  },
  {
    "span": "see Johnson and Patel, 2020",
    "document": "Related Work\n\nModern calibration methods include temperature scaling (Guo et al., 2017), focal losses (Lin et al., 2017), and label smoothing (Szegedy et al., 2016). For multiclass classification, post-hoc calibration often strikes the best trade-off between accuracy and confidence alignment. For more on temperature scaling, see Johnson and Patel, 2020 for a survey and practical guidelines. Recent advances also consider distribution shift (Ovadia et al., 2019) and selective prediction (Geifman and El-Yaniv, 2019).\n\nOur work introduces a data-efficient variant that preserves accuracy while improving ECE under label noise.",
    "reason": "Narrative pointer uses 'see' but omits proper citation formatting; should be 'see Johnson and Patel (2020)' or use a parenthetical '(Johnson and Patel, 2020)'.",
    "start": 327,
    "end": 354,
    "label": "Format"
  },
  {
    "span": "Behavior cloning learns policies from demonstration datasets (Pomerleau, 1989). Shared autonomy combines user control with robot assistance through assistance policies (Javdani et al., 2015). In-situ learning adapts models online during interaction (Losey et al., 2018).",
    "document": "Related Work\n\nLearning for Human–Robot Interaction\n\nData-driven methods in HRI balance sample efficiency, safety, and user experience. Approaches span offline imitation, interactive optimization, and adaptive control. Behavior cloning learns policies from demonstration datasets (Pomerleau, 1989). Shared autonomy combines user control with robot assistance through assistance policies (Javdani et al., 2015). In-situ learning adapts models online during interaction (Losey et al., 2018). Yet, how these paradigms trade off intervention frequency and task success across user skill levels is not well understood.\n\nUser-Centric Evaluation\n\nHRI studies report objective task metrics and subjective workload scales. We unify these measures to assess assistance quality under adaptive policies.",
    "reason": "The span abruptly juxtaposes three approaches without explaining their relationships or providing transitions, leaving connections implied rather than made explicit.",
    "start": 218,
    "end": 488,
    "label": "Coherence"
  },
  {
    "span": "Early VQA datasets and baseline models established multimodal reasoning benchmarks (Antol et al., 2015; Goyal et al., 2017). Attention-based fusion improved alignment between vision and language (Anderson et al., 2018; Lu et al., 2016). Transformer backbones and large-scale pretraining further advanced performance (Tan and Bansal, 2019; Kamath et al., 2021). In this paper, we introduce a new VQA architecture.",
    "document": "Related Work\n\nVisual Question Answering (VQA) combines image understanding with natural language reasoning. Progress in this area has been driven by large-scale datasets, stronger multimodal architectures, and pretraining on heterogeneous corpora.\n\nEarly VQA datasets and baseline models established multimodal reasoning benchmarks (Antol et al., 2015; Goyal et al., 2017). Attention-based fusion improved alignment between vision and language (Anderson et al., 2018; Lu et al., 2016). Transformer backbones and large-scale pretraining further advanced performance (Tan and Bansal, 2019; Kamath et al., 2021). In this paper, we introduce a new VQA architecture.\n\nIn addition, compositional reasoning and program-based methods attempt to improve robustness to dataset biases. We provide results on standard VQA benchmarks and conduct ablations of our components.",
    "reason": "After summarizing prior work, the span abruptly states the authors introduce a new architecture without explaining what gap remains or how the proposed method addresses a specific shortcoming.",
    "start": 249,
    "end": 661,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The MS MARCO passage ranking competition popularized cross-encoder architectures.",
    "document": "Related Work Neural ranking for open-domain search has evolved from dual-encoders relying on approximate nearest neighbor retrieval to cross-encoders that score query–document pairs with full attention (Karpukhin et al., 2020; Nogueira and Cho, 2019). Large-scale supervised signals and hard negative mining are central to achieving strong retrieval quality (Qu et al., 2021). The MS MARCO passage ranking competition popularized cross-encoder architectures. We revisit the balance between effectiveness and efficiency by distilling cross-encoder signals into late interaction models.",
    "reason": "Attributes influence to a specific competition without citing supporting evidence or reports, violating rule (a).",
    "start": 377,
    "end": 458,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Nguyen, 2021))",
    "document": "Related Work\n\nRecent advances in efficient Transformers target the quadratic cost of self-attention (Tay et al., 2020). Approaches include low-rank projections, kernelized attention, and sparse patterns guided by locality priors (Katharopoulos et al., 2020; Zaheer et al., 2020). As demonstrated by structured sparsity, blockwise attention can preserve accuracy while scaling to long sequences (Child et al., 2019). In addition, memory compression and chunking reduce overhead in encoder-only models (Beltagy et al., 2020). Recent efficient Transformers (Nguyen, 2021)) reduce cost via learned hashing to cluster tokens before attention.\n\nOur method complements these techniques with a differentiable selector that adaptively prunes attention heads conditioned on input complexity.",
    "reason": "Extra closing parenthesis. The citation should be \"(Nguyen, 2021)\" with a single closing parenthesis.",
    "start": 554,
    "end": 569,
    "label": "Format"
  },
  {
    "span": "Visual servoing uses image-space feedback to control end-effector motion (Hutchinson et al., 1996). Model predictive control optimizes trajectories under dynamics constraints (Tassa et al., 2014). Imitation learning clones expert demonstrations to learn policies (Pomerleau, 1989). Sim-to-real transfer mitigates the reality gap using domain randomization (Tobin et al., 2017).",
    "document": "Related Work\n\nRobotic manipulation can be approached through control-theoretic formulations and learning-based policies. Recent advances in perception and simulation have enabled increasingly complex tasks in unstructured environments.\n\nVisual servoing uses image-space feedback to control end-effector motion (Hutchinson et al., 1996). Model predictive control optimizes trajectories under dynamics constraints (Tassa et al., 2014). Imitation learning clones expert demonstrations to learn policies (Pomerleau, 1989). Sim-to-real transfer mitigates the reality gap using domain randomization (Tobin et al., 2017).\n\nOur method combines demonstration-driven pretraining with online adaptation to disturbances.",
    "reason": "The paragraph lists four distinct approaches without transitions or a clear narrative tying them together, making the relationship between control, learning, and sim-to-real methods unclear.",
    "start": 237,
    "end": 614,
    "label": "Coherence"
  },
  {
    "span": "Patel et al.",
    "document": "Introduction\n\nFew-shot classification aims to recognize novel classes from only a handful of labeled examples (Lake et al., 2015; Vinyals et al., 2016). Metric-based approaches learn embedding spaces where nearest-neighbor rules generalize across tasks (Snell et al., 2017; Sung et al., 2018). Optimization-based methods adapt model parameters to new tasks with minimal gradient steps (Finn et al., 2017; Nichol et al., 2018). Recently, transformers have shown promise for task-agnostic representation learning (Dosovitskiy et al., 2021; Touvron et al., 2021), especially when combined with episodic pre-training (Gidaris et al., 2019; Tian et al., 2020).\n\nHowever, relying solely on class prototypes can be brittle under distribution shift, label noise, and class-imbalance (Tseng et al., 2020; Bao et al., 2021). To address this, Patel et al. extend prototypical networks with distributional uncertainty modeling, achieving improvements on cluttered backgrounds and occlusions. Other work incorporates data augmentation via generative models to densify support sets (Antoniou et al., 2018; Zhao et al., 2020) or leverages self-supervision for improved invariances (Gidaris et al., 2019; Chen et al., 2020).\n\nWe unify uncertainty-aware embeddings with test-time adaptation that calibrates decision boundaries using unlabeled query points, bridging metric and semi-supervised paradigms (Lazarou et al., 2021; Zhang et al., 2021). Our benchmarks cover cross-domain transfer from natural to sketch datasets, highlighting when uncertainty helps and when it harms performance.",
    "reason": "Narrative citation missing year; it should include the year as 'Patel et al. (YEAR)'.",
    "start": 832,
    "end": 844,
    "label": "Format"
  },
  {
    "span": "Mnih et al. (2015) introduced deep Q-learning for control from pixels. Lillicrap et al. (2016) proposed a deterministic policy gradient method for continuous actions. Haarnoja et al. (2018) developed a maximum-entropy actor-critic to improve exploration. Andrychowicz et al. (2017) investigated hindsight relabeling for sparse reward tasks.",
    "document": "Related Work\n\nReinforcement learning for robotics spans from off-policy methods that leverage replay to model-based methods that exploit dynamics structure. Sim-to-real transfer and robust policy learning are core challenges due to dynamics mismatch and partial observability.\n\nMnih et al. (2015) introduced deep Q-learning for control from pixels. Lillicrap et al. (2016) proposed a deterministic policy gradient method for continuous actions. Haarnoja et al. (2018) developed a maximum-entropy actor-critic to improve exploration. Andrychowicz et al. (2017) investigated hindsight relabeling for sparse reward tasks.\n\nRecent efforts pursue domain randomization and adaptation for sim-to-real (Tobin et al., 2017; Peng et al., 2018) and representation learning to stabilize off-policy updates (Srinivas et al., 2020). Our work targets sample-efficient adaptation across dynamics perturbations with safety constraints, a setting that differs from pure exploration or pure transfer formulations.",
    "reason": "The span enumerates major RL methods without clarifying their relation to each other or to the robotics context introduced earlier. There are no transitions explaining how one method motivates the next, leading to coherence issues.",
    "start": 278,
    "end": 618,
    "label": "Coherence"
  },
  {
    "span": "Lee, 2017; Kim, 2019)",
    "document": "Related Work\n\nContinual learning studies how to acquire new tasks without catastrophic forgetting (Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017). Methods include regularization that preserves important weights, rehearsal buffers of past examples, and dynamic architectures (Zenke et al., 2017; Chaudhry et al., 2019; Rusu et al., 2016).\n\nTask-agnostic settings complicate reliance on explicit task boundaries, motivating change-point detection and online validation heuristics (Aljundi et al., 2019; Prabhu et al., 2020). Some works address distributional drift in a single evolving task, akin to test-time adaptation (Sun et al., 2020; Schneider et al., 2020). Prior evaluations often aggregate accuracy over sequences of benchmarks such as Split CIFAR and Permuted MNIST Lee, 2017; Kim, 2019), but omit calibration and uncertainty metrics.\n\nWe propose a rehearsal-efficient method that balances stability and plasticity via selective consolidation guided by Fisher-pruned subspaces.",
    "reason": "Missing opening parenthesis for a grouped parenthetical citation; should be “(Lee, 2017; Kim, 2019)” or reformatted as narrative.",
    "start": 782,
    "end": 803,
    "label": "Format"
  },
  {
    "span": "Constraint-based and score-based algorithms offer complementary solutions (Spirtes et al., 2000; Chickering, 2002; Kalisch and Bühlmann, 2007).",
    "document": "Introduction\n\nCausal discovery from observational data aims to recover causal structure under assumptions such as acyclicity and faithfulness. Approaches vary in how they exploit conditional independence, score maximization, or functional assumptions to infer edges and orientations.\n\nConstraint-based and score-based algorithms offer complementary solutions (Spirtes et al., 2000; Chickering, 2002; Kalisch and Bühlmann, 2007). Functional causal models and invariance-based techniques extend discovery beyond linear-Gaussian settings (Shimizu et al., 2006; Peters et al., 2016). Interventional datasets, when available, can improve identifiability and orient edges more reliably (Eberhardt, 2007; Hauser and Bühlmann, 2012).\n\nRecent advances incorporate deep generative models and variational objectives to relax parametric assumptions, enabling scalable discovery in high dimensions (Lachapelle et al., 2020; Zheng et al., 2018; Yu et al., 2019).",
    "reason": "The span simply names two broad families of methods with citations and does not connect them to the authors' argument, motivation, or the specific challenges the paper addresses.",
    "start": 285,
    "end": 428,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Nguyen 2017)",
    "document": "Related Work\n\nAbstractive summarization has progressed rapidly with encoder–decoder architectures trained on large-scale corpora (See et al., 2017; Lewis et al., 2020). Pointer mechanisms and coverage penalties mitigate repetition and promote factuality (Paulus et al., 2018; Gehrmann et al., 2018). Pre-training with sequence-to-sequence objectives further improves content selection and fluency (Dong et al., 2019; Zhang et al., 2020). While reinforcement learning can optimize non-differentiable metrics, stability remains a challenge (Narayan et al., 2018; Kryscinski et al., 2019). Extractive–abstractive hybrids attempt to combine strong content coverage with fluent generation (Liu and Lapata, 2019; Chen and Bansal, 2018). We compare to graph-based extractive selectors (Nguyen 2017) and evaluate on long-document settings where hierarchical attention better preserves discourse structure (Cohan et al., 2018).\n",
    "reason": "Missing comma between author and year in a parenthetical citation; APA style requires \"(Nguyen, 2017)\".",
    "start": 778,
    "end": 791,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors report a 12% absolute improvement from curriculum learning.",
    "document": "Introduction\n\nSemi-supervised medical image segmentation benefits from consistency regularization and pseudo-labeling strategies that exploit unlabeled scans. While architectural advances have plateaued, training curricula that schedule augmentation strength or mask difficulty can materially affect outcomes. In a previous study, the authors report a 12% absolute improvement from curriculum learning. However, evidence remains mixed across modalities and institutions, and external validation is often limited by privacy constraints and annotation variability.\n\nWe propose a slice-aware curriculum that progressively increases anatomical diversity by sampling from underrepresented regions during early training. Combined with shape priors and test-time augmentation, our approach aims to reduce false negatives in small structures. We evaluate on multi-center abdominal CT and cardiac MR cohorts with rigorous cross-site validation and report sensitivity-specificity trade-offs under clinically relevant thresholds.",
    "reason": "Mentions a 'previous study' and a specific performance gain without citing the work, which is required when referring to prior results.",
    "start": 310,
    "end": 402,
    "label": "Unsupported_claim"
  },
  {
    "span": "Neural program repair approaches encode buggy code and generate patches with sequence-to-sequence or graph-based models (Tufano et al., 2019; Chen et al., 2019; Dinella et al., 2020). Datasets such as Defects4J and ManySStuBs4J are standard benchmarks for evaluation (Just et al., 2014; Karampatsis et al., 2020).",
    "document": "Introduction\n\nAutomated program repair aims to localize faults and synthesize patches that restore correctness with respect to a test suite or specification. Learning-based approaches have shown promise by leveraging patterns from historical bug fixes.\n\nNeural program repair approaches encode buggy code and generate patches with sequence-to-sequence or graph-based models (Tufano et al., 2019; Chen et al., 2019; Dinella et al., 2020). Datasets such as Defects4J and ManySStuBs4J are standard benchmarks for evaluation (Just et al., 2014; Karampatsis et al., 2020).\n\nWe present a retrieval-augmented patch generator that conditions on semantically similar historical fixes and enforces type constraints during decoding, improving both plausibility and correctness rates across Java benchmarks.",
    "reason": "The span recites prior methods and benchmarks but does not connect them to the authors' approach, identify shortcomings, or motivate the contribution.",
    "start": 254,
    "end": 567,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Williams et al.) 2018",
    "document": "Introduction\n\nImitation learning enables robots to acquire policies from expert demonstrations without explicit reward engineering (Argall et al., 2009; Osa et al., 2018). Behavioral cloning scales easily but suffers from covariate shift, while inverse reinforcement learning addresses reward inference at higher sample cost (Ross et al., 2011; Ho and Ermon, 2016).\n\nRecent work integrates representation learning with temporal abstraction to improve sample efficiency in long-horizon tasks (Krishnan et al., 2017; Sharma et al., 2020). Robustness under changing dynamics remains a key obstacle to deployment (Hanna and Stone, 2017; Rajeswaran et al., 2017).\n\nAs noted by (Williams et al.) 2018, distribution mismatch between expert and learner trajectories can lead to compounding errors. We mitigate this effect via online aggregation and uncertainty-aware data selection (Ross et al., 2011; Czarnecki et al., 2019).",
    "reason": "Year placed outside the parenthetical citation: it should be 'Williams et al. (2018)' or '(Williams et al., 2018)'; '(Williams et al.) 2018' is a formatting error.",
    "start": 672,
    "end": 694,
    "label": "Format"
  },
  {
    "span": "(Nguyen et al., 2020",
    "document": "Related Work\n\nProbabilistic topic models extend beyond LDA to incorporate document structure and metadata (Blei et al., 2003; Roberts et al., 2014). Neural topic models improve flexibility but can suffer from posterior collapse (Srivastava and Sutton, 2017; Dieng et al., 2020). Guided priors have been shown to enhance interpretability, as shown in (Nguyen et al., 2020 where seed words anchor topics in low-resource settings. We build on these ideas with semi-supervised anchors and adaptive temperature schedules to stabilize training.",
    "reason": "Missing closing parenthesis in a parenthetical citation.",
    "start": 350,
    "end": 370,
    "label": "Format"
  },
  {
    "span": "(Smith and Lee, 2020)",
    "document": "Related Work\n\nEvent detection on social media often exploits distant supervision and temporal bursts to identify emerging topics (Meladianos et al., 2015; Hasan et al., 2019). User embeddings have been studied (Smith and Lee, 2020) to incorporate social homophily into classifiers, while graph propagation can further denoise weak signals (Rossi et al., 2021). Multi-modal cues from images and text improve robustness to linguistic noise (Zhang et al., 2018; Li et al., 2020). Our approach contrasts user- and content-level signals to reduce spurious correlations induced by platform-specific artifacts.\n\nWe evaluate on multiple crisis and protest datasets with manual event annotations.",
    "reason": "Wrong citation style inside parentheses: in APA-style parenthetical citations, '&' should be used; '(Smith and Lee, 2020)' should be '(Smith & Lee, 2020)'.",
    "start": 210,
    "end": 231,
    "label": "Format"
  },
  {
    "span": "Neural approaches to causal discovery have adopted continuous relaxations of acyclicity, score-based optimization, and variational formulations (Zheng et al., 2018; Yu et al., 2019; Lachapelle et al., 2020; Ng et al., 2020).",
    "document": "Related Work\n\nCausal discovery. Identifying directed acyclic graphs from observational data has a long history, with constraint- and score-based algorithms providing classical baselines. Neural approaches to causal discovery have adopted continuous relaxations of acyclicity, score-based optimization, and variational formulations (Zheng et al., 2018; Yu et al., 2019; Lachapelle et al., 2020; Ng et al., 2020). Extensions incorporate sparsity-inducing penalties and non-linear function classes.\n\nInterventions and robustness. Recent studies examine transfer under interventions and distribution shifts, aiming to preserve invariant mechanisms.\n\nWe introduce a hybrid objective that couples invariant risk minimization with acyclicity constraints for improved stability under shift.\n",
    "reason": "The span lists lines of work without explaining their assumptions, limitations, or relation to the proposed hybrid objective; it lacks synthesis and does not articulate the authors’ motivation.",
    "start": 187,
    "end": 411,
    "label": "Lacks_synthesis"
  },
  {
    "span": "a previous study shows that phoneme-aware pre-training improves low-resource ASR",
    "document": "Related Work\n\nSelf-supervised pre-training for speech has yielded strong gains in downstream recognition by leveraging large unlabeled audio corpora. Techniques that incorporate linguistic units, such as subword or phoneme targets, aim to bridge acoustic and textual representations. In particular, a previous study shows that phoneme-aware pre-training improves low-resource ASR, suggesting that intermediate linguistic supervision can serve as an effective inductive bias. Building on this intuition, we propose a multi-level alignment objective that jointly models acoustic frames, phonemes, and graphemes.",
    "reason": "References a specific prior study and its finding without citing the work, which is required for claims about previous results.",
    "start": 299,
    "end": 379,
    "label": "Unsupported_claim"
  },
  {
    "span": "Zhang et al.",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become a standard tool for spatiotemporal forecasting in transportation systems, enabling models to capture the relational structure of road networks and temporal dynamics simultaneously (Kipf and Welling, 2017; Wu et al., 2019). Early approaches relied on fixed graph filters, whereas recent methods learn adaptive connectivity to handle non-stationary traffic patterns (Li et al., 2018; Yu and Li, 2018). Similar to Zhang et al., we adopt a message-passing architecture that integrates historical speed and volume signals, but we also incorporate uncertainty estimation to improve robustness under distribution shift. Our work complements probabilistic forecasting studies (Salinas et al., 2020) by focusing on graph-aware calibration and evaluation under adverse weather regimes.",
    "reason": "Narrative citation missing year; should be formatted as 'Zhang et al. (YEAR)'.",
    "start": 466,
    "end": 478,
    "label": "Format"
  },
  {
    "span": "Chen et al. 3",
    "document": "Introduction\n\nWeak supervision aggregates noisy sources to produce training labels. As noted by Chen et al. 3, heuristic coverage and conflict are key determinants of label model quality. Subsequent work integrates program induction with weak labeling functions (Ratner et al., 2017; Varma and Ré, 2018). Our approach targets cross-domain portability by learning transferable source reliabilities.",
    "reason": "Wrong use of a footnote-like number after an author list. Should include the year (e.g., Chen et al., 2021) or be formatted as a proper footnote.",
    "start": 96,
    "end": 109,
    "label": "Format"
  },
  {
    "span": "Readability formulas estimate grade level using surface features (Kincaid et al., 1975). Automated essay scoring leverages neural coherence models (Taghipour and Ng, 2016). Argument mining identifies claims and premises in student writing (Stab and Gurevych, 2017).",
    "document": "Introduction\n\nAutomated assessment tools are increasingly used to support formative feedback in writing education. Early systems measured readability and lexical sophistication as proxies for difficulty (Kincaid et al., 1975; Pitler and Nenkova, 2008). Neural methods introduced representations that capture discourse and coherence, improving scoring reliability (Taghipour and Ng, 2016; Dong et al., 2017). Parallel work in argument mining aims to detect claims and premises to provide targeted feedback (Stab and Gurevych, 2017; Lippi and Torroni, 2016).\n\nReadability formulas estimate grade level using surface features (Kincaid et al., 1975). Automated essay scoring leverages neural coherence models (Taghipour and Ng, 2016). Argument mining identifies claims and premises in student writing (Stab and Gurevych, 2017). Despite progress, little is known about how models disentangle stylistic vs. argumentative quality in low-resource classroom settings. We present a multi-task framework with uncertainty calibration for actionable feedback.",
    "reason": "The span lists three areas—readability, essay scoring, and argument mining—without transitions or an explicit explanation of their interrelation, leaving the reader to infer connections and reducing coherence.",
    "start": 558,
    "end": 823,
    "label": "Coherence"
  },
  {
    "span": "Yao and Huang (2017) define fairness metrics for recommendations. Exposure allocation can be optimized with constrained reranking (Singh and Joachims, 2018). Calibrated recommendations seek to match user distributions (Steck, 2018). Popularity bias can distort long-tail coverage (Abdollahpouri et al., 2019).",
    "document": "Related Work\n\nFairness in recommender systems spans user-level parity, provider-side exposure, and platform-wide objectives. Approaches vary from pre-processing and reweighting to in-processing constraints and post-processing reranking, with ongoing debate about tradeoffs between accuracy and fairness.\n\nYao and Huang (2017) define fairness metrics for recommendations. Exposure allocation can be optimized with constrained reranking (Singh and Joachims, 2018). Calibrated recommendations seek to match user distributions (Steck, 2018). Popularity bias can distort long-tail coverage (Abdollahpouri et al., 2019).\n\nWe address these dimensions jointly by learning a slate-level objective that internalizes exposure fairness and calibration while explicitly regularizing popularity bias.",
    "reason": "The listed works are presented as isolated statements without transitions or explicit relationships, so the reader cannot see how metrics, reranking, calibration, and popularity bias connect; the coherence issue spans multiple sentences.",
    "start": 305,
    "end": 614,
    "label": "Coherence"
  },
  {
    "span": "Kiela et al. 1",
    "document": "Related Work\n\nVision-and-language learning connects textual semantics with visual grounding for tasks such as VQA and retrieval (Antol et al., 2015; Chen et al., 2015; Lu et al., 2016). Early multimodal embeddings align images and captions via joint objectives (Kiros et al., 2014; Frome et al., 2013). However, some studies argue that unimodal textual cues can dominate benchmarks, questioning the depth of grounding (Agrawal et al., 2018; Goyal et al., 2017). Kiela et al. 1 further highlight the need for more diagnostic datasets to isolate confounds.\n\nTransformer backbones with region features enabled stronger fusion (Anderson et al., 2018; Tan and Bansal, 2019), and end-to-end vision Transformers improved scaling (Dosovitskiy et al., 2021; Li et al., 2020). Despite progress, spurious correlations and partial grounding persist (Niu et al., 2021).",
    "reason": "Wrong use of footnote indicator without proper citation formatting; should include the publication year (e.g., \"Kiela et al. (2019)\") or be formatted as a proper numbered footnote.",
    "start": 462,
    "end": 476,
    "label": "Format"
  },
  {
    "span": "McMahan et al. (2017) developed FedAvg to aggregate client updates. Smith et al. (2018) studied multi-task personalization with MOCHA. Li et al. (2020) introduced FedProx to stabilize training with heterogeneous data. Kairouz et al. (2021) provided a comprehensive survey.",
    "document": "Related Work\n\nFederated Learning Foundations\nFederated learning (FL) enables training models across decentralized data silos while keeping raw data on devices (Konečný et al., 2016). Practical deployments must confront statistical heterogeneity, communication constraints, and privacy considerations (Bonawitz et al., 2019).\n\nPersonalization and Heterogeneity\nA central challenge in FL is to achieve global utility while accommodating client-specific distributions. Personalization layers, mixture-of-experts, and meta-learning have been explored to adapt global models to local needs (Arivazhagan et al., 2019; Fallah et al., 2020). McMahan et al. (2017) developed FedAvg to aggregate client updates. Smith et al. (2018) studied multi-task personalization with MOCHA. Li et al. (2020) introduced FedProx to stabilize training with heterogeneous data. Kairouz et al. (2021) provided a comprehensive survey.\n\nPrivacy and Robustness\nComplementary lines investigate differential privacy for client updates and robustness to poisoning attacks (Geyer et al., 2017; Blanchard et al., 2017). Our method focuses on heterogeneity but remains compatible with privacy and robustness techniques.",
    "reason": "The span enumerates multiple works without clarifying how they connect to each other or to the stated goal of personalization. Transitions are missing and the relationships are left implicit, causing an abrupt shift from the general problem statement to a list of disparate citations.",
    "start": 634,
    "end": 906,
    "label": "Coherence"
  },
  {
    "span": "Lee & Patel (2020)",
    "document": "Related Work\n\nAlgorithmic fairness in NLP examines disparities across demographic groups in both performance and downstream outcomes (Hanna and Boyd, 2019; Ortiz et al., 2021). Bias can arise from pretraining corpora, annotation practices, or deployment shifts (Chen and Alvarez, 2020).\n\nAs argued by Lee & Patel (2020), measurement choice critically shapes conclusions about fairness, and subsequent work proposed counterfactual evaluations and robust optimization (Mendez et al., 2022; Shah and Iqbal, 2023). We extend these efforts with a causal objective that targets group-conditional error parity under covariate shift.\n",
    "reason": "Ampersand used in a narrative citation; in narrative text it should be 'Lee and Patel (2020)'.",
    "start": 301,
    "end": 319,
    "label": "Format"
  },
  {
    "span": "Duvenaud et al. (2015) introduce neural fingerprints for molecules. Gilmer et al. (2017) propose message passing networks with edge updates. Yang et al. (2019) curate MoleculeNet benchmarks for fair comparison. Hu et al. (2020) pre-train GNNs on large unlabeled graphs.",
    "document": "Related Work\n\nGraph Neural Networks for Molecular Property Prediction\n\nLearning on molecular graphs has progressed rapidly with architectures that model atoms and bonds directly. Early work emphasized fixed descriptors, while recent methods learn task-specific representations end-to-end. Duvenaud et al. (2015) introduce neural fingerprints for molecules. Gilmer et al. (2017) propose message passing networks with edge updates. Yang et al. (2019) curate MoleculeNet benchmarks for fair comparison. Hu et al. (2020) pre-train GNNs on large unlabeled graphs. These approaches collectively shaped benchmarks, architectures, and training paradigms, though differences in supervision and data regimes complicate direct comparison.\n\nPretraining and transfer have emerged as key strategies to improve low-data performance. Contrastive and generative objectives have been explored alongside domain-specific augmentations. Our work focuses on exploiting pretraining signals tailored to reaction prediction, where conditional context differs from standard property prediction tasks.",
    "reason": "The span lists four works in sequence without transitions or explicit relationships among them; it is unclear how the benchmark paper relates to model proposals or pretraining, creating abrupt shifts between topics.",
    "start": 289,
    "end": 558,
    "label": "Coherence"
  },
  {
    "span": "(Gao et al., 2019]",
    "document": "Related Work\n\nMultimodal fusion. Strategies for combining vision and language signals include early fusion through joint embeddings and late fusion via ensembling (Kiros et al., 2014; Lu et al., 2019). Attention-based fusion has proven especially effective for grounding and reasoning (Anderson et al., 2018; Tan and Bansal, 2019). However, alignment errors in noisy web data remain a key challenge (Gao et al., 2019] and require robust pretraining objectives that tolerate mismatches (Jia et al., 2021; Singh et al., 2022).\n\nWe build on contrastive pretraining with noise-aware sampling that downweights ambiguous pairs while preserving coverage.",
    "reason": "Mismatched brackets: opening parenthesis with a closing square bracket; should consistently use parentheses, e.g., “(Gao et al., 2019)”.",
    "start": 399,
    "end": 417,
    "label": "Format"
  },
  {
    "span": "The Atari 2600 suite contains 57 games",
    "document": "Introduction\n\nBenchmarking deep reinforcement learning often relies on arcade-style games. The Atari 2600 suite contains 57 games and supports standardized evaluation with sticky actions and human-normalized scores. While DQN pioneered value-based learning on this suite (Mnih et al., 2015), Rainbow and distributional variants improved stability and data efficiency (Hessel et al., 2018; Bellemare et al., 2017). More recently, model-based agents exploit latent dynamics to plan (Schrittwieser et al., 2020). We propose a sample-efficient agent that unifies value-based exploration with imagination rollouts.",
    "reason": "Provides a benchmark statistic without a citation to the benchmark’s defining source.",
    "start": 91,
    "end": 129,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent surveys report that Transformer-based models dominate leaderboard performance",
    "document": "Introduction\n\nNeural code summarization translates source code into natural-language descriptions, aiding software maintenance and onboarding (Haque et al., 2020; LeClair and McMillan, 2019). Pretrained models over code tokens and AST representations have substantially improved sequence-to-sequence quality on Java and Python benchmarks (Kanade et al., 2020; Feng et al., 2020). Recent surveys report that Transformer-based models dominate leaderboard performance on CodeSearchNet-style datasets across multiple programming languages.\n\nHowever, prevailing evaluations often neglect cross-repository generalization and robustness to coding style variation. We propose a retrieval-augmented generator that conditions on semantic neighbors from large codebases and demonstrate consistent improvements on out-of-project and temporal holdout splits.",
    "reason": "Asserts findings from 'recent surveys' without citing any, violating rule (d) and requiring citations to specific surveys.",
    "start": 380,
    "end": 464,
    "label": "Unsupported_claim"
  },
  {
    "span": "Many datasets used for toxicity detection contain annotation artifacts.",
    "document": "Related Work on Bias in Toxicity Detection\nAutomated toxicity detection systems face challenges related to spurious correlations, demographic biases, and context sensitivity. Models often overfit to surface cues, leading to disparate performance across groups and contexts.\nMany datasets used for toxicity detection contain annotation artifacts. These artifacts can encourage shortcut learning and inflate benchmark scores without improving real-world reliability.\nWe propose a data curation protocol that explicitly targets such artifacts and show that models trained on the curated data exhibit improved robustness and fairness metrics.",
    "reason": "Makes a general claim about prior datasets without providing citations to studies documenting those artifacts.",
    "start": 274,
    "end": 345,
    "label": "Unsupported_claim"
  },
  {
    "span": "Previous studies have shown that message passing networks struggle to capture long-range interactions in proteins.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a standard tool for molecular property prediction (Gilmer et al., 2017; Yang et al., 2019). In structural biology, contact maps and residue graphs enable efficient modeling of proteins at scale. Previous studies have shown that message passing networks struggle to capture long-range interactions in proteins. To address this limitation, researchers have proposed augmentations such as attention over residue pairs and geometric priors (Ingraham et al., 2019; Jing et al., 2020). Our work complements these efforts by introducing an explicit pathway modeling module that conditions on predicted secondary structure.\n\nWe further situate our approach within energy-based formulations for protein modeling and discuss how global context can be injected efficiently during training.",
    "reason": "It references 'previous studies' and makes a technical claim about long-range interactions without citing any specific papers.",
    "start": 250,
    "end": 364,
    "label": "Unsupported_claim"
  },
  {
    "span": "The ATIS dataset contains 5,000 spoken utterances for airline travel.",
    "document": "Introduction\n\nSpoken language understanding (SLU) commonly evaluates intent classification and slot filling using benchmark datasets drawn from task-oriented domains. While newer corpora target multi-domain assistants and multi-turn interactions, classic single-domain resources remain widely used for comparative evaluation and error analysis. The ATIS dataset contains 5,000 spoken utterances for airline travel. We revisit SLU under low-resource conditions, studying how data augmentation and self-training affect robustness to ASR noise.",
    "reason": "States a specific dataset statistic without citing the dataset’s original publication (rule a/b).",
    "start": 345,
    "end": 414,
    "label": "Unsupported_claim"
  },
  {
    "span": "The first SemEval stance detection competition reported 73% macro-F1 as the winning score.",
    "document": "Introduction\n\nStance detection aims to identify an author's position toward a given target, and has been evaluated across domains such as politics, health, and social movements. Shared tasks have spurred rapid progress by standardizing data, metrics, and baselines. The first SemEval stance detection competition reported 73% macro-F1 as the winning score. However, model performance is highly sensitive to target specificity, label imbalance, and contextual cues, motivating more rigorous cross-domain evaluation and calibration techniques.",
    "reason": "Claims a specific statistic and references a shared task without providing a citation to the task report or paper (rule a).",
    "start": 266,
    "end": 356,
    "label": "Unsupported_claim"
  },
  {
    "span": "Multimodal misinformation detection combines textual and visual signals using early, late, or hybrid fusion (Wang et al., 2018; Khattar et al., 2019; Qi et al., 2021). Datasets such as Fakeddit, Weibo, and Twitter-based collections provide benchmarks (Nakamura et al., 2020; Ma et al., 2016; Boididou et al., 2015), and pre-trained transformers and vision backbones have become standard encoders (Devlin et al., 2019; Dosovitskiy et al., 2021).",
    "document": "Related Work\n\nMisinformation increasingly leverages persuasive visuals alongside text, making unimodal detectors brittle. Multimodal models promise better robustness by cross-validating claims across modalities.\n\nMultimodal misinformation detection combines textual and visual signals using early, late, or hybrid fusion (Wang et al., 2018; Khattar et al., 2019; Qi et al., 2021). Datasets such as Fakeddit, Weibo, and Twitter-based collections provide benchmarks (Nakamura et al., 2020; Ma et al., 2016; Boididou et al., 2015), and pre-trained transformers and vision backbones have become standard encoders (Devlin et al., 2019; Dosovitskiy et al., 2021).\n\nRecent work explores stance-aware reasoning and lightweight adaptation to new events. Yet, detecting semantically consistent but temporally or geographically mismatched media remains an open challenge.\n\nWe propose AlignCheck, which aligns textual claims with visual evidence along entity, location, and time axes, improving mismatch detection with minimal supervision.",
    "reason": "The span summarizes datasets, fusion strategies, and standard encoders without stating the specific deficiency that motivates the paper or connecting prior art to the proposed idea.",
    "start": 213,
    "end": 657,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Kumar et al. 2",
    "document": "Introduction\n\nLearning from demonstration (LfD) enables robots to acquire complex skills by observing expert behavior (Argall et al., 2009; Chernova and Thomaz, 2014). Policy learning with temporal abstraction improves compositionality and sample efficiency (Krishnan et al., 2017; Daniel et al., 2016). Recent approaches integrate language to specify goals and constraints, enhancing generalization to novel tasks (Tellex et al., 2011; Guhur et al., 2021).\n\nVisual imitation presents unique challenges due to partial observability and viewpoint mismatch (Yu et al., 2018; Smith et al., 2020). Contrastive representations aligned across egocentric and third-person views can mitigate these issues (Sermanet et al., 2018; Dwibedi et al., 2019). While curriculum learning stabilizes training on long-horizon tasks (Florensa et al., 2017), reward hacking and covariate shift remain problematic (Ross et al., 2011).\n\nOur framework unifies goal-conditioned policies with constraint-aware planners, drawing on safety formulations from control theory (Ames et al., 2019) and risk-sensitive RL (Tamar et al., 2015). We compare to modular pipelines that separate perception and control (James et al., 2019) and to end-to-end visuomotor policies (Levine et al., 2016). Notably, Kumar et al. 2 propose a hybrid approach that blends trajectory optimization with learned residual policies, but requires extensive tuning for stability. We instead introduce an adaptive scheme that auto-tunes residual magnitudes during training, reducing crashes and constraint violations in cluttered scenes.",
    "reason": "Improper footnote-like usage after an author group; should include a year or be formatted as a proper footnote, e.g., 'Kumar et al. (2020)' or a numbered footnote marker linked to a note.",
    "start": 1268,
    "end": 1282,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claim that temperature scaling above 5 hurts generalization.",
    "document": "Related Work\n\nKnowledge distillation compresses a high-capacity teacher into a smaller student via soft targets, often improving compact model accuracy and calibration. Temperature scaling modulates the softness of teacher outputs, affecting the balance between dark knowledge and over-smoothing. In a previous study, the authors claim that temperature scaling above 5 hurts generalization. Other lines explore data augmentation, intermediate feature matching, and contrastive objectives to enrich the student's representation space. Despite these advances, sensitivity to distillation hyperparameters persists, particularly under distribution shifts. We introduce an adaptive schedule for temperature and sample weighting that self-tunes based on student uncertainty, targeting stability across domains.",
    "reason": "Refers to a 'previous study' and its findings without providing a citation (rule a and e).",
    "start": 297,
    "end": 390,
    "label": "Unsupported_claim"
  },
  {
    "span": "Message passing neural networks, graph attention mechanisms, and equivariant architectures have been applied to molecular graphs with strong results across QM9, ZINC, and MoleculeNet benchmarks (Gilmer et al., 2017; Veličković et al., 2018; Schütt et al., 2018; Hu et al., 2020). Pretraining strategies such as context prediction, node masking, and graph-level contrastive learning have also been proposed (Sun et al., 2019; You et al., 2020; Hu et al., 2020b).",
    "document": "Related Work\n\nMolecular property prediction is a central task in computational chemistry and drug discovery. Graph neural networks (GNNs) offer a natural inductive bias by operating directly on molecular graphs and have set the pace on widely used benchmarks.\n\nMessage passing neural networks, graph attention mechanisms, and equivariant architectures have been applied to molecular graphs with strong results across QM9, ZINC, and MoleculeNet benchmarks (Gilmer et al., 2017; Veličković et al., 2018; Schütt et al., 2018; Hu et al., 2020). Pretraining strategies such as context prediction, node masking, and graph-level contrastive learning have also been proposed (Sun et al., 2019; You et al., 2020; Hu et al., 2020b).\n\nBeyond architecture design, data scarcity and distribution shift limit generalization to new chemotypes. Our study targets the few-shot regime where only a handful of labeled molecules are available for a novel assay.\n\nWe introduce a task-adaptive pretraining objective aligned with downstream endpoints and show improvements in cross-assay transfer relative to standard contrastive pretraining.",
    "reason": "The span summarizes prior architectures and pretraining methods but does not articulate how they connect to the present study or identify a gap, demonstrating (a) and (c).",
    "start": 261,
    "end": 722,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Back-translation became the standard practice for low-resource NMT after the WMT18 shared task.",
    "document": "Related Work\n\nData augmentation has been pivotal in neural machine translation (NMT), with back-translation serving as a primary technique to leverage monolingual corpora. Iterative refinement, noise scheduling, and dual learning extend this idea by improving synthetic source quality and training dynamics. Back-translation became the standard practice for low-resource NMT after the WMT18 shared task. Concurrently, multilingual pretraining and adapter modules have improved cross-lingual transfer in scarce-data regimes, while quality estimation informs filtering of synthetic pairs. Our work studies how synthetic data diversity interacts with domain shift, proposing an ensemble of generative priors for improved generalization.",
    "reason": "Claims a historical shift attributed to a specific shared task without any citation to the task report or relevant publications.",
    "start": 308,
    "end": 403,
    "label": "Unsupported_claim"
  },
  {
    "span": "In a previous study, the authors claim that contrastive decoding eliminates hallucinations in open-domain QA.",
    "document": "Introduction\n\nHallucination in open-domain question answering (QA) undermines user trust and limits deployment in high-stakes settings. In a previous study, the authors claim that contrastive decoding eliminates hallucinations in open-domain QA. While decoding-time interventions can reduce unsupported assertions, their impact depends on retrieval quality and calibration.\n\nWe revisit decoding strategies under controlled retrieval conditions and introduce diagnostics for attribution fidelity, separating errors due to retrieval, modeling, and decoding.",
    "reason": "Mentions a previous study and its claim without providing a citation to that study (rules a and e/ii).",
    "start": 136,
    "end": 245,
    "label": "Unsupported_claim"
  },
  {
    "span": "To the best of our knowledge, this is the first work to evaluate zero-shot summarization on Wolof.",
    "document": "Introduction\n\nAbstractive summarization has benefited from advances in sequence-to-sequence pretraining and large-scale datasets (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). Cross-lingual summarization has emerged to serve speakers of languages with limited labeled resources by transferring knowledge from high-resource languages (Zhu et al., 2019; Nguyen and Daumé, 2019). Multilingual encoder-decoder models, such as mBART and mT5, enable zero-shot transfer across typologically diverse languages (Tang et al., 2020; Xue et al., 2021). Low-resource African languages, however, remain substantially under-studied due to lack of task-specific annotations and domain-appropriate corpora (Adelani et al., 2021; Nekoto et al., 2020).\n\nWhile prior efforts primarily evaluate on European or East Asian languages, coverage of Niger-Congo languages is sparse, and existing evaluations rarely address discourse structure or factual consistency in low-resource settings. To the best of our knowledge, this is the first work to evaluate zero-shot summarization on Wolof. We build on multilingual pretraining and incorporate lexicon-aware constraints to improve content selection under vocabulary sparsity. We further release a small human-verified test set to facilitate future research.",
    "reason": "Novelty claim about being the first to evaluate zero-shot summarization on Wolof asserts a state of prior work without providing any citation or evidence.",
    "start": 977,
    "end": 1075,
    "label": "Unsupported_claim"
  },
  {
    "span": "Schneider et al. (2019) pre-trained CPC on unlabeled audio. Baevski et al. (2020) proposed wav2vec 2.0 with contrastive objectives. SpecAugment augments spectrograms during supervised training (Park et al., 2019). End-to-end transducers optimize streaming inference (Graves, 2012; He et al., 2019).",
    "document": "Related Work\n\nSelf-supervised learning has reshaped automatic speech recognition (ASR) by reducing labeled data requirements. Complementary techniques in data augmentation and sequence modeling have further improved robustness and latency for real-world applications.\n\nSchneider et al. (2019) pre-trained CPC on unlabeled audio. Baevski et al. (2020) proposed wav2vec 2.0 with contrastive objectives. SpecAugment augments spectrograms during supervised training (Park et al., 2019). End-to-end transducers optimize streaming inference (Graves, 2012; He et al., 2019).\n\nOur method unifies contrastive pretraining with augmentation-invariant objectives and evaluates streaming ASR under domain shift.",
    "reason": "The span presents a sequence of works across pretraining, augmentation, and decoding without transitions or clear relational statements, making the connections between them implicit at best.",
    "start": 269,
    "end": 567,
    "label": "Coherence"
  },
  {
    "span": "Kim et al.",
    "document": "Introduction\n\nRecent progress in low-resource dependency parsing has been driven by transfer learning and data augmentation. Earlier approaches emphasize multilingual pretraining to share syntactic priors across languages (Zhang et al., 2020; Ponti et al., 2021). However, purely pretraining-based methods often underperform when target domains diverge significantly from the sources (Ruder, 2019). Following the approach of Kim et al., we propose to combine synthetic treebank expansion with domain-adaptive fine-tuning to bridge this gap. Our method builds on contrastive objectives to maintain structural consistency while adapting to domain-specific patterns (Clark et al., 2019).\n\nRelated Work\n\nCross-lingual parsing has considered projection (Tiedemann, 2015) and pseudo-labeling (He et al., 2020). Data augmentation via perturbation has shown promise for robustness (Xie et al., 2020). We complement these with targeted syntactic augmentations and domain-aware sampling, which align with findings by Søgaard and Goldberg (2016) that task bias can be mitigated via curriculum design.",
    "reason": "Narrative citation missing year. “Kim et al.” should be formatted as a narrative citation with the year, e.g., “Kim et al. (2019)” or converted to a parenthetical form “(Kim et al., 2019)”.",
    "start": 425,
    "end": 435,
    "label": "Format"
  },
  {
    "span": "In the context of DP-SGD, many techniques adjust noise schedules, clipping strategies, or momentum (Abadi et al., 2016; Andrew et al., 2021; Bu et al., 2020; De et al., 2022).",
    "document": "Introduction\n\nDifferential privacy (DP) provides formal guarantees against leakage of individual training examples. In deep learning, the predominant mechanism is DP-SGD, which injects calibrated noise and clips per-sample gradients. Unfortunately, privacy comes with utility costs and optimization instability.\n\nIn the context of DP-SGD, many techniques adjust noise schedules, clipping strategies, or momentum (Abadi et al., 2016; Andrew et al., 2021; Bu et al., 2020; De et al., 2022).\n\nThis paper revisits gradient clipping through the lens of adaptive norms, proposing a curvature-aware clipping rule that reduces clipping bias while maintaining tight privacy accounting. We pair this with a simple variance-reduction schedule that preserves privacy budgets.\n\nOn image and language benchmarks, our approach improves accuracy at fixed epsilon and reduces training instability. We further analyze sensitivity to batch size and sampling rate to provide practical guidance.",
    "reason": "The span lists existing DP-SGD modifications but does not connect them to the paper's approach or clarify motivation, meeting (a) and (c).",
    "start": 313,
    "end": 488,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Garcia et al., 2018)",
    "document": "Introduction\n\nDomain adaptation seeks to transfer knowledge from a labeled source domain to an unlabeled or sparsely labeled target domain. Prior work has explored discrepancy minimization, adversarial learning, and representation alignment to reduce domain shift (Ben-David et al., 2010; Tzeng et al., 2017; Long et al., 2018). In (Garcia et al., 2018) a metric-learning objective is combined with adversarial training to align class-conditional distributions, demonstrating strong performance on visual benchmarks.\n\nDespite these advances, negative transfer persists when the source and target label spaces only partially overlap (Chu et al., 2020; Pan et al., 2020). To mitigate this, recent methods employ selective alignment via sparsity-inducing priors and uncertainty-aware distillation (Zou et al., 2019; Cao et al., 2019). We build on this line of work by introducing a confidence-calibrated discriminator that explicitly down-weights out-of-support source examples while preserving discriminative features in the shared subspace.",
    "reason": "Wrong citation style: preposition should introduce a narrative citation (e.g., \"in Garcia et al. (2018)\") rather than enclosing the authors and year in parentheses after \"In\".",
    "start": 329,
    "end": 353,
    "label": "Format"
  },
  {
    "span": "Classical protocols such as Paxos (Lamport, 1998) and Viewstamped Replication (Oki and Liskov, 1988) establish agreement in fail-stop environments, while Raft emphasizes understandability (Ongaro and Ousterhout, 2014) and EPaxos reduces commit latency via fast quorums (Moraru et al., 2013).",
    "document": "Related Work\n\nState machine replication relies on consensus to ensure fault tolerance and high availability in distributed systems. Over the past decades, protocols have evolved to address latency, throughput under contention, and ease of implementation.\n\nClassical protocols such as Paxos (Lamport, 1998) and Viewstamped Replication (Oki and Liskov, 1988) establish agreement in fail-stop environments, while Raft emphasizes understandability (Ongaro and Ousterhout, 2014) and EPaxos reduces commit latency via fast quorums (Moraru et al., 2013).\n\nOur approach introduces a contention-adaptive consensus layer that reconfigures quorum structure and leader roles based on online conflict signals, aiming to reduce tail latency without sacrificing safety.",
    "reason": "Enumerates well-known protocols and their properties without connecting them to the paper’s problem setting or motivating the need for the proposed design.",
    "start": 256,
    "end": 547,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Garcia and Lee, 2020)",
    "document": "Related Work\n\nData augmentation for end-to-end automatic speech recognition (ASR) has explored perturbations at the waveform and spectrogram levels. SpecAugment introduced time and frequency masking to improve robustness (Park et al., 2019), while vocal tract length perturbation addressed speaker variability (Jaitly and Hinton, 2013). In (Garcia and Lee, 2020), the authors demonstrate that semantically preserving text-to-speech backtranslation can close domain gaps for conversational ASR. Concurrently, multilingual pretraining has been shown to benefit low-resource languages via shared subword vocabularies (Singh et al., 2021). Self-supervised objectives such as contrastive predictive coding and masked acoustic modeling further reduce label requirements (Baevski et al., 2020). Our approach differs by unifying augmentation with consistency training, encouraging agreement across heterogeneous perturbations (Watanabe and Kim, 2022).",
    "reason": "Wrong citation style; narrative use should be 'In Garcia and Lee (2020)' rather than placing the authors in parentheses after 'In'.",
    "start": 337,
    "end": 362,
    "label": "Format"
  },
  {
    "span": "Recent work has explored zero- and few-shot prompting with large language models (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022). Prompt tuning and prefix-based methods adapt soft prompts without updating backbone parameters (Li and Liang, 2021; Lester et al., 2021). Dataset distillation compresses training sets into synthetic examples (Wang et al., 2018; Zhao et al., 2021).",
    "document": "Related Work\n\nPrompting and Few-Shot Generalization\n\nLarge language models (LLMs) have demonstrated strong few-shot capabilities via in-context learning, where a small number of exemplars are provided in the prompt to elicit task-relevant behavior (Brown et al., 2020). Subsequent studies explore the stability, calibration, and sensitivity of such prompting strategies across tasks and domains (Zhao et al., 2021; Holtzman et al., 2021; Webson and Pavlick, 2022).\n\nRecent work has explored zero- and few-shot prompting with large language models (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022). Prompt tuning and prefix-based methods adapt soft prompts without updating backbone parameters (Li and Liang, 2021; Lester et al., 2021). Dataset distillation compresses training sets into synthetic examples (Wang et al., 2018; Zhao et al., 2021).\n\nOther approaches manipulate the format or semantics of instructions to steer model behavior, including task decomposition, chain-of-thought prompting, and self-consistency decoding (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022). Parameter-efficient fine-tuning methods, such as adapters and LoRA, aim to reduce the training footprint while maintaining competitive performance (Houlsby et al., 2019; Hu et al., 2021). Despite progress, questions remain about when prompt-based methods outperform fine-tuning and how to systematically construct prompts for new domains.",
    "reason": "The third sentence about dataset distillation appears abruptly after two sentences on prompting and soft prompts without any transition or explanation of relevance, leaving the relationship between the cited works implicit.",
    "start": 466,
    "end": 870,
    "label": "Coherence"
  },
  {
    "span": "McMahan et al. (2017) introduce federated averaging for on-device learning. Kairouz et al. (2021) survey challenges in federated learning. Geyer et al. (2017) study differential privacy in federated settings. Karimireddy et al. (2020) analyze drift with control variates.",
    "document": "Related Work\n\nFederated Optimization and Privacy\n\nWe review approaches for learning across devices without centralizing raw data. McMahan et al. (2017) introduce federated averaging for on-device learning. Kairouz et al. (2021) survey challenges in federated learning. Geyer et al. (2017) study differential privacy in federated settings. Karimireddy et al. (2020) analyze drift with control variates. Systems work addresses heterogeneity and communication bottlenecks (Bonawitz et al., 2019; Li et al., 2020).\n\nOur method tackles sparse participation and non-IID data with adaptive client selection.",
    "reason": "Works are listed back-to-back with no connective tissue; it is unclear how the survey, privacy, and optimization lines relate or build on each other.",
    "start": 130,
    "end": 401,
    "label": "Coherence"
  },
  {
    "span": "recent works have explored cross-lingual adapters",
    "document": "Related Work\n\nParameter-efficient transfer learning has emerged as an alternative to full fine-tuning, enabling rapid adaptation via small modules while keeping the backbone frozen. In multilingual NLP, recent works have explored cross-lingual adapters to support low-resource transfer and zero-shot generalization without retraining the entire model. Other approaches leverage prompts or soft tuning to steer pretrained encoders toward new tasks. However, the interaction between adapter placement and subword segmentation across languages is still poorly understood.",
    "reason": "Vague reference to 'recent works' without any supporting citations, which must be provided (rule d).",
    "start": 203,
    "end": 252,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Johnson et al. 2021)",
    "document": "Introduction\n\nFormative feedback systems aim to provide timely guidance that helps students regulate their learning processes. Intelligent tutoring systems have shown benefits in mathematics and reading when feedback is specific and actionable (VanLehn, 2011; Nye, 2015). Recent studies report that model-generated hints can improve revision quality in writing tasks (Johnson et al. 2021), but the generality of these results beyond short prompts remains unclear.\n\nWe examine scalable feedback for long-form writing and show that content-planning hints yield larger gains than surface-level suggestions on coherence and argumentation quality.",
    "reason": "Missing comma before the year in an APA-style parenthetical citation; should be '(Johnson et al., 2021)'.",
    "start": 367,
    "end": 388,
    "label": "Format"
  },
  {
    "span": "In a previous study, researchers demonstrated that adversarial training improves calibration on out-of-distribution data.",
    "document": "Related Work\n\nModel calibration assesses the alignment between predicted probabilities and empirical accuracy. Common techniques include temperature scaling, label smoothing, and post-hoc calibration using held-out data. In a previous study, researchers demonstrated that adversarial training improves calibration on out-of-distribution data. Other works propose confidence-aware losses and ensembles to capture epistemic uncertainty. Despite these developments, calibration quality often degrades under dataset shift, prompting exploration of lightweight strategies that can be applied during fine-tuning without access to large validation sets.\n",
    "reason": "References a specific prior study and its findings without providing a citation to that study.",
    "start": 221,
    "end": 342,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent shared tasks have focused on hate speech detection across languages.",
    "document": "Related Work\n\nContent moderation research has increasingly addressed abusive language and hate speech detection, with growing attention to robustness, domain transfer, and fairness (Waseem and Hovy, 2016; Davidson et al., 2017). Multilingual settings pose additional challenges due to code-switching, dialectal variation, and scarcity of labeled data for low-resource languages.\n\nRecent shared tasks have focused on hate speech detection across languages. Despite the community interest, cross-lingual generalization remains limited, and evaluation protocols vary widely in label taxonomy, sampling, and preprocessing.\n\nWe survey multilingual datasets and propose a benchmark aligned on shared label definitions and consistent preprocessing. Our experiments with multilingual encoders and adapter-based transfer indicate that label harmonization reduces spurious language effects and improves cross-lingual F1.",
    "reason": "The mention of 'recent shared tasks' lacks citations to the specific shared tasks or workshop reports.",
    "start": 380,
    "end": 455,
    "label": "Unsupported_claim"
  },
  {
    "span": "[Smith, 2018]",
    "document": "Related Work\n\nWe situate our method within research on explainability for text classifiers. Prior surveys [Smith, 2018] emphasize fidelity-interpretability trade-offs, while more recent evaluations propose human-in-the-loop frameworks (Doshi-Velez and Kim, 2017; Jacovi and Goldberg, 2020). Unlike attribution-only methods, we incorporate counterfactual generation to capture model reliance on specific spans.",
    "reason": "Wrong bracket style for APA-like citations; should be (Smith, 2018) rather than [Smith, 2018].",
    "start": 106,
    "end": 119,
    "label": "Format"
  },
  {
    "span": "Prior studies have conclusively shown that model calibration alleviates demographic disparities.",
    "document": "Introduction\n\nMachine learning in medical imaging faces persistent performance disparities across demographic groups. These disparities can arise from dataset imbalance, domain shift, and spurious correlations. Clinically reliable deployment requires not only high average accuracy but also equitable error rates and well-calibrated uncertainty estimates across subpopulations.\n\nA variety of mitigation strategies have been explored, including reweighting, adversarial debiasing, and distributionally robust optimization. Prior studies have conclusively shown that model calibration alleviates demographic disparities. However, calibration methods themselves may interact with thresholding policies and disease prevalence, leading to complex trade-offs between sensitivity, specificity, and fairness metrics.\n\nRelated Work\n\nCalibration techniques range from temperature scaling and isotonic regression to focal loss and post-hoc Bayesian methods. Fairness interventions in imaging span pre-processing, in-processing, and post-processing. Yet, aligning calibration with group-specific operating points remains underexplored in clinical contexts.",
    "reason": "Claims conclusive evidence from prior studies without any references; this is a claim about prior work that requires citations under rule b.",
    "start": 522,
    "end": 618,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Lee et al., 2019",
    "document": "Related Work\n\nCross-lingual named entity recognition has progressed through annotation projection, multilingual pretraining, and adversarial alignment (Ni et al., 2017; Pires et al., 2019; Keung et al., 2019). Methods based on adversarial training (Lee et al., 2019 align latent spaces to reduce language shift, while data augmentation via machine translation helps bridge lexical gaps (Xie et al., 2018; Feng et al., 2020). Recent prompt-based approaches adapt multilingual encoders with minimal task-specific parameters (Winata et al., 2021; Zhao and Schütze, 2022).",
    "reason": "Missing closing parenthesis in a parenthetical citation; should be (Lee et al., 2019).",
    "start": 248,
    "end": 265,
    "label": "Format"
  },
  {
    "span": "Prior user studies have demonstrated that dark mode reduces eye strain by 30% on average.",
    "document": "Introduction\n\nDisplay theming has become a central component of user interface personalization, with dark mode widely available across operating systems and applications. Advocates argue that darker palettes improve comfort and battery life, while critics note potential drawbacks for readability and visual search, particularly in high-glare environments.\n\nPrior user studies have demonstrated that dark mode reduces eye strain by 30% on average. However, reported benefits may depend on ambient lighting, task type, and font rendering, and many evaluations lack longitudinal designs. In this paper, we conduct a controlled, multi-session experiment to measure comfort, performance, and preference across themes, and we analyze moderating effects of contrast ratio, luminance adaptation, and individual differences in visual acuity.",
    "reason": "Presents a specific quantitative effect size from 'user studies' without citing any empirical sources; statistics require evidence.",
    "start": 358,
    "end": 447,
    "label": "Unsupported_claim"
  },
  {
    "span": "Multimodal fusion architectures for VQA range from bilinear pooling and tensor decompositions to attention-based co-modulation (Fukui et al., 2016; Ben-Younes et al., 2017; Yu et al., 2018; Kim et al., 2018). Pretrained vision-language transformers have further improved alignment through large-scale contrastive and captioning objectives (Lu et al., 2019; Tan and Bansal, 2019; Li et al., 2020).",
    "document": "Introduction\n\nVisual question answering (VQA) requires compositional reasoning over images and natural language. While dataset scale and pretrained representations have advanced performance, systematic generalization and efficiency remain open challenges, particularly in low-data regimes.\n\nMultimodal fusion architectures for VQA range from bilinear pooling and tensor decompositions to attention-based co-modulation (Fukui et al., 2016; Ben-Younes et al., 2017; Yu et al., 2018; Kim et al., 2018). Pretrained vision-language transformers have further improved alignment through large-scale contrastive and captioning objectives (Lu et al., 2019; Tan and Bansal, 2019; Li et al., 2020).\n\nWe present a parameter-efficient adapter that disentangles content and relation features via sparse cross-attention, enabling rapid adaptation with few annotated samples. We validate on standard and compositional splits of VQA datasets.\n",
    "reason": "The span enumerates fusion and pretraining approaches without articulating their limitations relative to the proposed adapter or how it addresses a specific gap.",
    "start": 291,
    "end": 687,
    "label": "Lacks_synthesis"
  },
  {
    "span": "For molecular property prediction, message passing neural networks propagate features along bonds (Gilmer et al., 2017), while attention-based graph transformers capture long-range interactions (Ying et al., 2021; Kreuzer et al., 2021). 3D-aware models incorporate spatial coordinates or distance matrices (Schütt et al., 2018; Klicpera et al., 2020).",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become standard tools for modeling molecules as graphs of atoms and bonds. Advances include architectures that better capture long-range dependencies, encode stereochemistry, and integrate geometric information.\n\nFor molecular property prediction, message passing neural networks propagate features along bonds (Gilmer et al., 2017), while attention-based graph transformers capture long-range interactions (Ying et al., 2021; Kreuzer et al., 2021). 3D-aware models incorporate spatial coordinates or distance matrices (Schütt et al., 2018; Klicpera et al., 2020).\n\nOur work proposes a hybrid graph-field model that augments message passing with learned scalar and vector fields over molecular conformers, enabling direction-aware aggregation with improved sample efficiency on small datasets.",
    "reason": "Lists categories of prior models and citations but does not explain their limitations or how they inform the need for the proposed hybrid approach.",
    "start": 261,
    "end": 612,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Leaman and Lu (2016) addressed biomedical concept normalization with neural ranking. Wright et al. (2019) used UMLS synonyms to improve candidate generation. Wadden et al. (2019) introduced SciSpacy for biomedical named entity recognition. Rule-based normalization remains competitive in narrow domains (Pakhomov et al., 2011).",
    "document": "Related Work\n\nClinical entity linking maps mentions in clinical narratives to standardized ontologies, facing challenges like abbreviation resolution, polysemy, and domain shift across institutions. Methods combine mention detection, candidate generation, and disambiguation with knowledge resources such as UMLS.\n\nLeaman and Lu (2016) addressed biomedical concept normalization with neural ranking. Wright et al. (2019) used UMLS synonyms to improve candidate generation. Wadden et al. (2019) introduced SciSpacy for biomedical named entity recognition. Rule-based normalization remains competitive in narrow domains (Pakhomov et al., 2011).\n\nBeyond general clinical text, domain-adapted encoders and cross-institutional pretraining improve robustness (Peng et al., 2019; Si et al., 2021). We examine cross-site generalization for disambiguation with limited labeled supervision.",
    "reason": "The span mixes normalization, candidate generation, and NER systems without explaining their relationships or transitions, making the connections between cited works abrupt and unclear.",
    "start": 315,
    "end": 642,
    "label": "Coherence"
  },
  {
    "span": "Differentially private stochastic gradient descent adds calibrated noise to gradients (Abadi et al., 2016). PATE aggregates teacher votes to protect student labels (Papernot et al., 2017). Gradient clipping bounds sensitivity to limit privacy loss (Goodfellow et al., 2014). Federated averaging trains models across devices without centralizing data (McMahan et al., 2017).",
    "document": "Related Work\n\nPrivacy-preserving machine learning studies mechanisms that protect individual-level information while enabling learning. Techniques span algorithmic noise addition, secure aggregation, and decentralized optimization.\n\nDifferentially private stochastic gradient descent adds calibrated noise to gradients (Abadi et al., 2016). PATE aggregates teacher votes to protect student labels (Papernot et al., 2017). Gradient clipping bounds sensitivity to limit privacy loss (Goodfellow et al., 2014). Federated averaging trains models across devices without centralizing data (McMahan et al., 2017).\n\nWe explore adaptive privacy budgeting in cross-device federated settings with non-IID data.",
    "reason": "The paragraph juxtaposes DP mechanisms with federated learning without transitions or an explicit link, leaving the connection between privacy techniques and decentralization unclear.",
    "start": 233,
    "end": 606,
    "label": "Coherence"
  },
  {
    "span": "Wei and Zou (2019) propose easy data augmentation with simple text edits. Sennrich et al. (2016) use back-translation to augment parallel data. Kobayashi (2018) introduces contextual augmentation with a language model. Xie et al. (2020) propose unsupervised data augmentation with consistency training.",
    "document": "Related Work\n\nData augmentation in NLP targets robustness and sample efficiency by expanding training distributions. Techniques range from lexical perturbations to model-based paraphrasing and consistency objectives.\n\nWei and Zou (2019) propose easy data augmentation with simple text edits. Sennrich et al. (2016) use back-translation to augment parallel data. Kobayashi (2018) introduces contextual augmentation with a language model. Xie et al. (2020) propose unsupervised data augmentation with consistency training.\n\nRecent methods leverage pretrained LMs for semantically controlled edits and task-specific invariances. We contribute a calibration-aware augmentation strategy that preserves label semantics under strong perturbations.",
    "reason": "The span lists four techniques without transitions or explicit comparative framing, leaving the relationships and differences among them implicit.",
    "start": 218,
    "end": 520,
    "label": "Coherence"
  },
  {
    "span": "Smith et al.",
    "document": "Introduction\n\nUnsupervised domain adaptation (UDA) seeks to leverage labeled data from a source domain to improve performance on an unlabeled target domain. Classic theoretical analyses characterize the trade-off between source error and divergence across domains (Ben-David et al., 2010), motivating practical algorithms that minimize proxy measures of domain shift. Adversarial approaches align latent representations by confusing a domain discriminator (Ganin et al., 2016), while discrepancy-based methods reduce measures such as MMD and CORAL (Long et al., 2015; Sun and Saenko, 2016).\n\nDespite progress in vision, text UDA remains challenging due to lexical sparsity and shifting label semantics across corpora. Early neural methods adapted feature extractors with task-specific regularization (Glorot et al., 2011; Chen et al., 2012). More recently, pre-trained language models have been fine-tuned with alignment losses and pseudo-labeling to close the gap (Gururangan et al., 2020; Han and Eisenstein, 2021). Smith et al. showed that careful control of source-label imbalance is crucial when adapting sentiment classifiers across reviews, complementing techniques that reweight instances or calibrate confidence (Zhao et al., 2020; Kumar et al., 2022).\n\nWe build on this line by introducing a curriculum over subdomains identified through contrastive clustering, enabling gradual adaptation from near-source to far-target texts. Our experiments compare curriculum schedules and alignment objectives across four benchmark shifts, following protocols used in prior work (Li et al., 2018; Ruder and Plank, 2018).",
    "reason": "Narrative citation missing year; should be formatted as Smith et al. (YEAR) or converted to a parenthetical citation.",
    "start": 1018,
    "end": 1030,
    "label": "Format"
  },
  {
    "span": "Entity linking maps clinical mentions to standardized vocabularies (Leaman and Lu, 2016). SNOMED CT and UMLS provide terminologies (Bodenreider, 2004). Negation detection identifies absent findings (Chapman et al., 2001). Temporal reasoning orders clinical events (Pustejovsky et al., 2003).",
    "document": "Related Work\n\nClinical NLP systems support cohort discovery, decision support, and documentation by extracting structured information from notes. Robust pipelines must address ambiguity, variability, and clinical context.\n\nEntity linking maps clinical mentions to standardized vocabularies (Leaman and Lu, 2016). SNOMED CT and UMLS provide terminologies (Bodenreider, 2004). Negation detection identifies absent findings (Chapman et al., 2001). Temporal reasoning orders clinical events (Pustejovsky et al., 2003).\n\nWe address cross-institution generalization for entity normalization with contextualized linking and domain-adaptive pretraining on de-identified corpora.",
    "reason": "The span covers entity linking, terminologies, negation, and temporal reasoning without clarifying how they relate within a coherent pipeline, and lacks transitions between sentences.",
    "start": 223,
    "end": 514,
    "label": "Coherence"
  },
  {
    "span": "(Smith et al. 2020)",
    "document": "Introduction\n\nPersonalized recommendation balances relevance, novelty, and fairness under feedback loops. Matrix factorization methods (Koren et al., 2009) remain strong baselines, while deep models capture complex user–item interactions (Covington et al., 2016). Recent graph-based models (Smith et al. 2020) integrate social ties to improve cold-start performance. Our approach extends graph signals with calibrated uncertainty to handle sparse histories.",
    "reason": "Missing comma between authors and year in a parenthetical citation. Should be “(Smith et al., 2020)”.",
    "start": 290,
    "end": 309,
    "label": "Format"
  },
  {
    "span": "exposure to toxic replies reduces user trust by up to 40%",
    "document": "Related Work\n\nSafety in open-domain dialogue encompasses toxicity avoidance, hallucination control, and grounded knowledge usage (Dinan et al., 2019; Xu et al., 2021). Studies have investigated controllable generation using style transfer, reinforcement learning from human feedback, and safety classifiers in the loop (Keskar et al., 2019; Bai et al., 2022). Beyond technical metrics, user-centric evaluations highlight how exposure to harmful outputs affects perceptions of reliability and willingness to continue interactions; exposure to toxic replies reduces user trust by up to 40%.\n\nOur work complements model-centric safety by introducing a calibration-aware rejection mechanism that defers uncertain responses to retrieval or abstention, and we provide a human evaluation protocol focused on perceived trust and satisfaction.",
    "reason": "Presents a specific quantitative effect ('up to 40%') without any citation or evidence, which falls under unsupported statistical claims.",
    "start": 530,
    "end": 587,
    "label": "Unsupported_claim"
  },
  {
    "span": "The DSTC10 track standardized the use of BARTScore for ranking submissions.",
    "document": "Background and Related Work in Dialogue Evaluation\n\nAutomatic evaluation of open-domain dialogue remains challenging. N-gram overlap metrics like BLEU and ROUGE correlate poorly with human judgments in open-ended settings (Liu et al., 2016; Novikova et al., 2017). Learned reference-based scorers and embedding metrics such as BERTScore show improved but still inconsistent correlations (Zhang et al., 2020; Lowe et al., 2017). More recently, reference-free metrics that leverage pretrained language models have been proposed (Mehri and Eskenazi, 2020).\n\nShared tasks such as DSTC have catalyzed progress by providing common data and leaderboards (Yoshino et al., 2020). The DSTC10 track standardized the use of BARTScore for ranking submissions. However, without consistent human evaluation protocols, comparisons across years remain difficult.\n\nWe adopt a paired-comparison evaluation with uncertainty estimates to provide statistically meaningful system rankings.",
    "reason": "Mentions a specific shared task decision and tool adoption without citing the track overview or official report (rule a and b).",
    "start": 671,
    "end": 746,
    "label": "Unsupported_claim"
  },
  {
    "span": "A previous study claimed that injecting as few as 1% fake profiles can shift top-10 lists significantly.",
    "document": "Related Work\n\nRecommender systems are vulnerable to data poisoning, where adversaries manipulate training data to sway recommendations. Attack strategies range from random and bandwagon profiles to optimization-based poisoning that targets specific items or users. Defenses include anomaly detection, robust matrix factorization, and graph-based filtering.\n\nA previous study claimed that injecting as few as 1% fake profiles can shift top-10 lists significantly. Subsequent work has examined gray-box attackers that adapt to detectors, as well as defenses leveraging certification and influence functions. Yet, realistic evaluations often lack operational constraints such as rate limits and account aging, leading to overestimation of attack potency.\n\nIntroduction\n\nWe propose a defense that combines certified robustness guarantees with budget-aware anomaly scoring, providing protection under practical constraints while maintaining recommendation quality.",
    "reason": "References a specific result from prior work without providing a citation; per rule a and b, this requires a supporting reference.",
    "start": 358,
    "end": 462,
    "label": "Unsupported_claim"
  },
  {
    "span": "Neural abstractive models with copy mechanisms have improved lexical fidelity (See et al., 2017). Large-scale pretraining further boosts summarization quality by leveraging unlabeled corpora (Lewis et al., 2020). Reinforcement learning has been used to optimize non-differentiable metrics (Paulus et al., 2018). Multi-document settings introduce redundancy challenges (Fabbri et al., 2019).",
    "document": "Related Work\n\nText summarization research spans extractive and abstractive methods, with recent neural approaches dominating benchmarks. Early neural systems struggled with faithfulness and factual consistency, leading to techniques that constrain decoding or incorporate source copying.\n\nNeural abstractive models with copy mechanisms have improved lexical fidelity (See et al., 2017). Large-scale pretraining further boosts summarization quality by leveraging unlabeled corpora (Lewis et al., 2020). Reinforcement learning has been used to optimize non-differentiable metrics (Paulus et al., 2018). Multi-document settings introduce redundancy challenges (Fabbri et al., 2019). Our work explores controllable summarization with topical constraints.\n\nIn contrast to prior methods that learn implicit control signals, we introduce an explicit topic planner that conditions sentence-level content selection before surface realization.",
    "reason": "Multiple cited works are listed in adjacent sentences without transitions or explicit relationships; the shift from copy mechanisms to pretraining to RL to multi-document redundancy is abrupt and the connections are left implied.",
    "start": 289,
    "end": 679,
    "label": "Coherence"
  },
  {
    "span": "According to clinical statistics, 30% of ICU patients develop sepsis within 48 hours of admission",
    "document": "Introduction\n\nSepsis is a life-threatening dysregulated host response to infection and a leading cause of mortality in intensive care units (ICUs). Early detection enables timely intervention, which can drastically improve patient outcomes.\n\nAccording to clinical statistics, 30% of ICU patients develop sepsis within 48 hours of admission. This alarming incidence underscores the importance of accurate, real-time risk stratification tools that operate on routinely collected electronic health records.\n\nIn this work, we introduce a temporally aware model that integrates vital signs, labs, and treatments using a continuous-time encoder, enabling fine-grained predictions under irregular sampling. We further explore uncertainty estimation to flag high-risk predictions for clinician review.\n",
    "reason": "Presents a precise epidemiological statistic without evidence; per rule (b), numerical claims in a specialized domain must be supported by a citation.",
    "start": 242,
    "end": 339,
    "label": "Unsupported_claim"
  },
  {
    "span": "Message passing networks have become the dominant approach for molecular property prediction.",
    "document": "Related Work\n\nGraph representation learning has transformed computational chemistry by enabling end-to-end prediction directly from molecular graphs. Early approaches relied on fixed fingerprints, while neural methods learn task-specific features. Message passing networks have become the dominant approach for molecular property prediction. Extensions incorporate 3D geometry, equivariant mappings, and multi-task objectives to better capture stereochemistry and quantum effects. Datasets such as QM9 and MoleculeNet have driven progress, yet issues of scaffold split leakage and experimental noise persist. We build on these insights with an uncertainty-aware architecture and rigorous evaluation across challenging splits.\n",
    "reason": "Claims dominance of a particular model family in a niche area without citing supporting surveys or empirical studies (violates rule b).",
    "start": 248,
    "end": 341,
    "label": "Unsupported_claim"
  },
  {
    "span": "Personalization techniques include model interpolation (Mansour et al., 2020), meta-learning (Fallah et al., 2020), and clustered federated learning (Sattler et al., 2020).",
    "document": "Related Work\n\nFederated Learning\nFederated learning (FL) enables training across distributed clients without centralizing raw data, but non-iid distributions cause significant performance degradation for global models.\n\nPersonalized FL\nPersonalization techniques include model interpolation (Mansour et al., 2020), meta-learning (Fallah et al., 2020), and clustered federated learning (Sattler et al., 2020). Additional approaches introduce client-specific adapters or regularizers to bias client updates toward personalized optima.\n\nCommunication Efficiency\nOrthogonal to personalization, many studies reduce communication via compression, quantization, and partial participation, which can interact with personalization strategies in complex ways.",
    "reason": "The sentence enumerates prior personalization techniques without explaining their implications for the authors' setting, their shortcomings, or how the proposed method interfaces with them (criterion a and c).",
    "start": 236,
    "end": 408,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent work leverages large language models to propose candidate patches directly from failing tests (Chen et al., 2021; Jiang et al., 2022; Xia et al., 2023).",
    "document": "Introduction\n\nAutomated program repair (APR) aims to generate correct patches for buggy code with minimal developer intervention. Traditional APR methods combine fault localization with search over predefined patch templates or semantic transformations.\n\nLearning-based repair. Recent work leverages large language models to propose candidate patches directly from failing tests (Chen et al., 2021; Jiang et al., 2022; Xia et al., 2023). Other approaches incorporate execution traces, type constraints, and specification mining to improve patch validity and reduce overfitting (Long and Rinard, 2016; Bader et al., 2019; Le Goues et al., 2012).\n\nWe investigate repair in low-observability settings where only partial test coverage and coarse-grained failure signals are available, introducing a feedback shaping mechanism that steers generation without full traces.",
    "reason": "The span summarizes prior LLM-based APR at a high level without explaining how these methods succeed or fail in low-observability settings or how they inform the authors’ approach, lacking explicit synthesis.",
    "start": 278,
    "end": 437,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Most industrial recommender systems already rely on session-based models rather than user profiles.",
    "document": "Introduction\n\nRecommender systems balance short-term intent with long-term preferences, often under strict latency and privacy constraints. Session-based modeling has gained attention for capturing transient goals without persistent identifiers. Most industrial recommender systems already rely on session-based models rather than user profiles. Yet session signals are sparse and noisy, and observed gains can depend on re-ranking stages and candidate pools. We study hybrid architectures that reconcile session context with lightweight, privacy-preserving user representations.",
    "reason": "Broad claim about industry-wide practice requires evidence such as surveys, industry reports, or case studies, but none are cited.",
    "start": 246,
    "end": 345,
    "label": "Unsupported_claim"
  },
  {
    "span": "[12]",
    "document": "Related Work\n\nIn model-based reinforcement learning for robotics, learned dynamics enable data-efficient policy improvement (Deisenroth and Rasmussen, 2011; Chua et al., 2018). Uncertainty-aware planning (Gal and Ghahramani, 2016) and ensembles (Lakshminarayanan et al., 2017) improve robustness. While prior robotic manipulation studies rely on task-specific rewards (Levine et al., 2016), recent work explores self-supervised objectives (Pathak et al., 2017; Lynch et al., 2020). As shown in [12], combining latent planning with closed-loop control can further stabilize training under sensor noise. Our method builds on these ideas with a constraint-satisfying planner.\n\nWe evaluate on standard manipulation benchmarks and report gains over model-free baselines (Schulman et al., 2017).",
    "reason": "Wrong citation style: a numeric bracket citation '[12]' is inconsistent with the author–year style used elsewhere in the document.",
    "start": 494,
    "end": 498,
    "label": "Format"
  },
  {
    "span": "We follow the common practice of using the OntoNotes 5.0 splits for training and evaluation.",
    "document": "Related Work\n\nNamed entity recognition (NER) benchmarks span newswire, biomedical, and social media domains. While early systems relied on hand-crafted features, modern approaches use pre-trained encoders with task-specific heads and soft lexicons.\n\nWe follow the common practice of using the OntoNotes 5.0 splits for training and evaluation. Prior research has examined label consistency and cross-domain transfer, showing that distributional mismatch can significantly degrade performance on out-of-domain text.\n\nOur study complements this line of work by introducing a consistency-regularized objective that reduces sensitivity to annotation artifacts while preserving in-domain accuracy.",
    "reason": "Introduces a specific dataset at first mention without citing it, and asserts a field-wide practice without evidence (violates a and b).",
    "start": 250,
    "end": 342,
    "label": "Unsupported_claim"
  },
  {
    "span": "Lopez et al. 2",
    "document": "Related Work\n\nOnline misinformation spreads rapidly through social networks, driven by engagement-optimizing algorithms and coordinated campaigns (Vosoughi et al., 2018; Shu et al., 2017). Detection methods range from content-based classifiers to propagation-aware graph models (Zhou and Zafarani, 2019; Monti et al., 2019). User-centric features, such as account age and posting frequency, provide complementary signals (Ruchansky et al., 2017).\n\nIntervention strategies include fact-checking, friction in sharing, and algorithmic downranking (Pennycook et al., 2020; Guess et al., 2020). However, labeling delays and limited coverage reduce impact, motivating proactive detection and early warning systems (Kwon et al., 2017). Lopez et al. 2 demonstrate that small structured delays in resharing can dramatically curb cascades without large utility loss, but their approach has not been validated across languages and platforms.\n\nOur work develops multilingual early-warning models that fuse content and propagation signals, enabling platform-agnostic interventions with minimal user friction.",
    "reason": "Improper footnote-like usage without year; should include a proper citation with year (e.g., 'Lopez et al. (2022)') or be formatted as a legitimate footnote.",
    "start": 729,
    "end": 743,
    "label": "Format"
  },
  {
    "span": "Prior defenses in federated learning span secure aggregation, differential privacy at the client or server, shuffling, gradient compression, and Byzantine-robust aggregation (Bonawitz et al., 2017; McMahan et al., 2018; Bittau et al., 2017; Kairouz et al., 2021; Blanchard et al., 2017).",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative model training without centralizing raw user data. However, gradient sharing introduces privacy and integrity risks, including reconstruction attacks, membership inference, data poisoning, and model poisoning. The literature proposes both cryptographic and statistical defenses to mitigate these threats.\n\nPrior defenses in federated learning span secure aggregation, differential privacy at the client or server, shuffling, gradient compression, and Byzantine-robust aggregation (Bonawitz et al., 2017; McMahan et al., 2018; Bittau et al., 2017; Kairouz et al., 2021; Blanchard et al., 2017). In this paper, we introduce AdaptiveClip, a per-round, distribution-aware clipping scheme that automatically calibrates sensitivity across heterogeneous clients to improve the privacy–utility–robustness trade-off.\n\nWe conduct experiments on image, text, and tabular tasks under heterogeneous participation and adversarial settings, demonstrating reduced attack surfaces and improved accuracy under tight privacy budgets.\n",
    "reason": "The span enumerates existing techniques without connecting them to the proposed method or articulating what specific limitation remains; the contribution follows immediately without an explicit gap.",
    "start": 366,
    "end": 653,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Streaming anomaly detection methods include sketch-based summarization, online autoencoders, and probabilistic forecasting with dynamic thresholds (Zhang and Lee, 2018; Patel et al., 2020; Nguyen et al., 2021; Costa et al., 2022). Lightweight detectors for edge deployment have been proposed using quantization and sparsity (Gao et al., 2021; Ikram et al., 2022).",
    "document": "Related Work\nTime-series anomaly detection in streaming settings. Streaming anomaly detection methods include sketch-based summarization, online autoencoders, and probabilistic forecasting with dynamic thresholds (Zhang and Lee, 2018; Patel et al., 2020; Nguyen et al., 2021; Costa et al., 2022). Lightweight detectors for edge deployment have been proposed using quantization and sparsity (Gao et al., 2021; Ikram et al., 2022). Drift adaptation has been explored via windowed retraining and test-time adaptation (Lopez et al., 2021; Ruan and He, 2022).\n\nExplainability and feedback. Post-hoc attribution for anomalies uses shapley approximations, counterfactuals, and subsequence highlighting (Yeh et al., 2020; Sun and Malik, 2021). Feedback-driven refinement integrates human labels to reduce false positives in monitoring (Chen et al., 2022).\n\nWe introduce a forecast-calibrated sequential test that bounds false alarm rate under nonstationary noise, coupled with an adaptive memory that compresses seasonality while preserving rare-event signatures, enabling stable operation on resource-constrained edge devices.",
    "reason": "The span provides a list of categories and citations but does not explain their comparative strengths, remaining challenges, or why a new method is warranted; it lacks explicit connection to the authors' argument.",
    "start": 66,
    "end": 429,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent works have demonstrated that small instruction-tuned models can match or exceed GPT-3.5 on safety benchmarks.",
    "document": "Introduction\n\nAligning large language models (LLMs) with human preferences has become a central objective in natural language processing. Techniques such as instruction tuning and reinforcement learning from human feedback (RLHF) are commonly used to increase helpfulness and reduce harmful behavior. However, the cost of training and serving very large models has prompted interest in smaller models that retain strong alignment properties. Recent works have demonstrated that small instruction-tuned models can match or exceed GPT-3.5 on safety benchmarks. This observation motivates our investigation into data-efficient alignment protocols that scale down compute while preserving safety and robustness.\n\nDespite promising progress, open questions remain about how alignment strategies interact with model size, data diversity, and evaluation protocols. In particular, it is unclear whether safety gains observed in controlled evaluations transfer to adversarial or out-of-domain contexts. To address these issues, we focus on systematic comparisons across model sizes and training recipes under a unified experimental framework.",
    "reason": "Mentions 'recent works' and a specific comparative result without providing any citations to those works (rule d and a).",
    "start": 442,
    "end": 558,
    "label": "Unsupported_claim"
  },
  {
    "span": "Previous work on Yoruba ASR used only 20 hours of labeled audio.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) for low-resource African languages remains underexplored relative to high-resource languages [1]. Transfer learning and self-supervised pretraining have narrowed the gap but still require labeled audio for fine-tuning [2]. Previous work on Yoruba ASR used only 20 hours of labeled audio. This scarcity complicates evaluation and hyperparameter selection.",
    "reason": "Mentions 'previous work' and a specific dataset size without providing a citation.",
    "start": 272,
    "end": 336,
    "label": "Unsupported_claim"
  },
  {
    "span": "The Kinetics-700 dataset is known to contain significant label noise.",
    "document": "Related Work\n\nLarge-scale video datasets such as Sports-1M, Kinetics, and Moments-in-Time have enabled end-to-end training of spatiotemporal architectures for action recognition (Karpathy et al., 2014; Kay et al., 2017; Monfort et al., 2019). Architectures based on 3D convolutions, factorized temporal kernels, and transformer-style attention have progressively improved top-1 accuracy while reducing computation (Tran et al., 2018; Feichtenhofer et al., 2019; Bertasius et al., 2021). Recent work explores weak supervision, multi-modal pretraining, and label-efficient finetuning to address data scarcity and domain shift (Patrick et al., 2021; Alayrac et al., 2020).\n\nThe Kinetics-700 dataset is known to contain significant label noise. We therefore study noise-robust training via consistency regularization and curriculum relabeling, and we evaluate on Kinetics and transfer to Something-Something V2.",
    "reason": "Asserts a qualitative property of a specific dataset without providing a citation; such claims about dataset quality should be supported.",
    "start": 671,
    "end": 740,
    "label": "Unsupported_claim"
  },
  {
    "span": "Vision Transformers pre-trained on ImageNet-21K are commonly used for few-shot medical image classification.",
    "document": "Introduction\n\nFew-shot medical image classification faces challenges due to limited labeled data, distribution shifts across scanners, and strict requirements on model reliability. Transfer learning has become a widely adopted solution, with models initialized from large-scale natural image corpora and adapted to medical domains via fine-tuning or adapters.\n\nVision Transformers pre-trained on ImageNet-21K are commonly used for few-shot medical image classification. However, the inductive biases and tokenization schemes of these models may not align with clinical semantics, potentially limiting sample efficiency and calibration.\n\nWe propose a prototype-regularized adaptation strategy that preserves local structural cues while constraining decision boundaries through class prototypes, yielding improved performance under class imbalance and label noise.\n",
    "reason": "Asserts a common practice and mentions a specific pretraining corpus without citing any supporting works.",
    "start": 361,
    "end": 469,
    "label": "Unsupported_claim"
  },
  {
    "span": "Kumar et. al. (2018)",
    "document": "Related Work\n\nGraph representation learning has leveraged self-supervision through context prediction and contrastive objectives (Hamilton et al., 2017; Velickovic et al., 2019). Kumar et. al. (2018) explore node-level pretext tasks to capture structural roles, while others utilize subgraph sampling strategies to improve scalability (You et al., 2020; Hassani and Khasahmadi, 2020).\n\nOur method extends contrastive pretraining with topology-aware augmentations tailored to heterogeneous graphs.\n",
    "reason": "Typographical error in 'et al.'; should be 'et al.' without the extra period after 'et'.",
    "start": 179,
    "end": 199,
    "label": "Format"
  },
  {
    "span": "Fader et al. (2018) used paraphrase generation for semantic augmentation. Sennrich et al. (2016) introduced back-translation in machine translation. Wei and Zou (2019) proposed EDA for simple lexical and syntactic perturbations. Kobayashi (2018) performed contextual augmentation with language model-based replacements.",
    "document": "Related Work\n\nData Augmentation for Text Classification\nAugmentation combats overfitting and label scarcity by generating diverse yet label-preserving variants (Shorten and Khoshgoftaar, 2019). Methods vary in how they preserve semantics and control distributional drift.\n\nLexical, Syntactic, and Semantic Transformations\nApproaches range from rule-based to model-based paraphrasing and translation pipelines. Fader et al. (2018) used paraphrase generation for semantic augmentation. Sennrich et al. (2016) introduced back-translation in machine translation. Wei and Zou (2019) proposed EDA for simple lexical and syntactic perturbations. Kobayashi (2018) performed contextual augmentation with language model-based replacements.\n\nConsistency Training and Pretraining Signals\nComplementary strategies enforce prediction stability under perturbations or leverage pretrained encoders to guide augmentation quality (Xie et al., 2020; Kumar et al., 2020). Our method conditions augmentation on class-specific salience.",
    "reason": "The span lists several augmentation techniques without clarifying their relationships or comparative properties (e.g., semantic vs. lexical, MT-based vs. LM-based). Lacking transitions, the connections among these works remain unclear.",
    "start": 410,
    "end": 729,
    "label": "Coherence"
  },
  {
    "span": "BERT was used in an AES task trained on essays from the TOEFL corpus.",
    "document": "Related Work\n\nAutomated essay scoring (AES) has a long history, with early systems relying on handcrafted features and regression models (Attali and Burstein, 2006). With the advent of pretrained language models, neural approaches have become competitive for holistic and trait-specific scoring. BERT was used in an AES task trained on essays from the TOEFL corpus. Other lines of work leverage hierarchical encoders to capture discourse structure, or combine surface features with contextual embeddings to improve robustness across prompts. Despite these advances, generalization to unseen prompts and domains remains challenging, motivating research on transfer and domain adaptation for AES.",
    "reason": "Makes a specific claim about a prior setup (BERT on TOEFL AES) without a citation to the study or dataset usage (rules a and e-iii).",
    "start": 296,
    "end": 365,
    "label": "Unsupported_claim"
  },
  {
    "span": "According to recent surveys, 60% of users disable personalized ads on mobile devices.",
    "document": "Introduction\n\nPersonalization in mobile advertising promises higher relevance but raises privacy concerns and regulatory compliance challenges (Shokri et al., 2011; Narayanan and Shmatikov, 2008). Platform-level privacy controls and OS policy changes have shifted targeting from device identifiers to on-device learning and aggregated measurement (Rogers et al., 2021; Bittau et al., 2017). This transition motivates lightweight models that respect user choices while maintaining utility.\n\nAccording to recent surveys, 60% of users disable personalized ads on mobile devices. As opt-out rates grow, advertisers face sparse conversion signals and increased uncertainty in attribution. We propose a federated, privacy-preserving uplift modeling approach that learns heterogeneous treatment effects using only consented, differentially private aggregates.\n\nExperiments on semi-synthetic data derived from real-world logs show that our approach improves incremental lift estimation under high opt-out rates.",
    "reason": "Presents a precise statistic attributed to 'recent surveys' but does not cite any specific survey or report.",
    "start": 490,
    "end": 575,
    "label": "Unsupported_claim"
  },
  {
    "span": "In chest X-ray classification, over 80% of prior studies rely on synthetic oversampling to mitigate imbalance.",
    "document": "Introduction\n\nMedical imaging classifiers often face severe class imbalance, where rare pathologies are underrepresented relative to normal findings. Imbalance biases training loss landscapes and can degrade sensitivity on clinically important minority classes (Johnson et al., 2019; Oakden-Rayner, 2020). Common remedies include cost-sensitive losses, calibration, and data augmentation strategies tailored to medical domains (Cui et al., 2019; He and Garcia, 2009).\n\nIn chest X-ray classification, over 80% of prior studies rely on synthetic oversampling to mitigate imbalance. Despite its simplicity, naive oversampling can exacerbate overfitting and distort conditional feature distributions, motivating approaches that respect image-level and patient-level correlations.\n\nWe introduce a structure-preserving resampling method that operates on patient timelines and radiology report semantics to rebalance training while maintaining cohort realism. We validate on two multi-label benchmarks and present subgroup analyses.",
    "reason": "The sentence presents a quantitative statistic ('over 80%') about prior studies without any citation or evidence to substantiate the figure.",
    "start": 469,
    "end": 579,
    "label": "Unsupported_claim"
  },
  {
    "span": "Neural program synthesis trains sequence-to-sequence models to map utterances to code, optionally constrained by type signatures or execution traces (Yin and Neubig, 2017; Rabinovich et al., 2017; Bunel et al., 2018; Chen et al., 2021).",
    "document": "Introduction\n\nProgram synthesis from natural language promises to democratize software development and automate repetitive tasks. However, the search space is vast, training data may be noisy, and generalization to unseen specifications remains difficult.\n\nHybrid approaches combine symbolic constraints with neural models to prune search and enforce semantic consistency, yet integration overhead and domain specificity can limit applicability.\n\nNeural program synthesis trains sequence-to-sequence models to map utterances to code, optionally constrained by type signatures or execution traces (Yin and Neubig, 2017; Rabinovich et al., 2017; Bunel et al., 2018; Chen et al., 2021).\n\nWe propose a retrieval-augmented decoder that conditions generation on analogous specifications and verified partial programs, improving both correctness and data efficiency without domain-specific rules.",
    "reason": "The span summarizes a line of work without explaining what remains challenging or how those methods inform the proposed retrieval-augmented approach; it does not synthesize or state a gap.",
    "start": 447,
    "end": 683,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Model compression for Transformers includes pruning, quantization, low-rank factorization, and knowledge distillation (See et al., 2016; Sanh et al., 2020; Michel et al., 2019; Junczys-Dowmunt et al., 2018; Hou et al., 2020). Layer dropping and early-exit strategies have been proposed to trade accuracy for reduced inference time (Fan et al., 2020; Liu et al., 2020).",
    "document": "Introduction\n\nTransformer-based language models offer strong performance across NLP tasks but can be prohibitively expensive at inference time. Reducing computational cost without degrading quality is critical for edge deployment and large-scale serving.\n\nModel compression for Transformers includes pruning, quantization, low-rank factorization, and knowledge distillation (See et al., 2016; Sanh et al., 2020; Michel et al., 2019; Junczys-Dowmunt et al., 2018; Hou et al., 2020). Layer dropping and early-exit strategies have been proposed to trade accuracy for reduced inference time (Fan et al., 2020; Liu et al., 2020).\n\nDynamic inference methods that adapt computation per example show promise, yet they can suffer from instability during training and calibration mismatches at test time (Kaya et al., 2019; Schwartz et al., 2020). Moreover, structured pruning often fails to align with hardware efficiency due to irregular sparsity patterns.\n\nWe introduce confidence-calibrated routing with latency-aware distillation, enabling robust early exits that preserve accuracy under strict latency budgets. Our approach achieves hardware-friendly speedups by coupling structured gate design with device-specific profiles.",
    "reason": "The span catalogs compression methods without connecting them to the paper’s specific challenge or indicating which shortcomings motivate the new approach.",
    "start": 256,
    "end": 624,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Wang et al.",
    "document": "Introduction\n\nContrastive learning has emerged as a powerful paradigm for representation learning in vision and language (Chen et al., 2020; He et al., 2020). As shown by Wang et al., contrastive objectives benefit from hard-negative mining and temperature scaling, but current approaches often overlook domain shift between pretraining and downstream tasks. We mitigate this by aligning instance and cluster-level signals (Caron et al., 2020) and incorporating domain-adaptive priors (Ganin et al., 2016).\n",
    "reason": "Narrative citation missing the publication year; should appear as “Wang et al. (YEAR)”.",
    "start": 171,
    "end": 182,
    "label": "Format"
  },
  {
    "span": "Kingma & Ba (2015)",
    "document": "Introduction\n\nOptimization is a critical component of deep learning efficacy. Stochastic gradient methods with adaptive learning rates are widely used due to their robustness to poorly conditioned objectives (Duchi et al., 2011; Tieleman and Hinton, 2012). Kingma & Ba (2015) introduced Adam, which combines momentum and per-parameter scaling to accelerate convergence. While effective in practice, recent work analyzes its generalization properties and proposes modifications for improved stability (Reddi et al., 2018; Loshchilov and Hutter, 2019). We revisit these analyses and derive a proximal variant with tighter regret bounds.",
    "reason": "Incorrect conjunction in a narrative citation for APA-style author–year: narrative form should use 'and' instead of '&' (i.e., 'Kingma and Ba (2015)').",
    "start": 257,
    "end": 275,
    "label": "Format"
  },
  {
    "span": "Furthermore, a detailed analysis of the proposed model shows that interpolation and stochastic perturbation positively contribute to the overall performance.",
    "document": "Introduction\n\nRetrieval systems aim at retrieving the documents most relevant to the input queries, and have received substantial spotlight since they work as core elements in diverse applications, especially for open-domain question answering (QA) (Voorhees, 1999). Open-domain QA is a task of answering the question from a massive amount of documents, often requiring two components, a retriever and a reader (Chen et al., 2017;Karpukhin et al., 2020). Specifically, a retriever ranks the most questionrelated documents, and a reader answers the question using the retrieved documents.\n\nTraditional sparse retrieval approaches such as BM25 (Robertson et al., 1994) and TF-IDF rely on term-based matching, hence suffering from the vocabulary mismatch problem: the failure of retrieving relevant documents due to the lexical difference from queries. To tackle such a problem, recent research focuses on dense retrieval models to generate learnable dense representations for queries and documents with a dual encoder structure (Karpukhin et al., 2020;. Despite their recent successes, some challenges still remain in the dense retrieval scheme for a couple of reasons. First, dense retrieval models need a large amount of labeled training data for a decent performance. However, as Figure 1 shows, the proportion of labeled query-document pairs is extremely small since it is almost impossible to rely on humans for the annotations of a large document corpus. Second, in order to adapt a retrieval model to the real world, where documents constantly emerge, handling unlabeled documents that are not seen during training should obviously be considered, but remains challenging.\n\nTo automatically expand the query-document pairs, recent work generates queries from generative models (Liang et al., 2020;Ma et al., 2021) or incorporates queries from other datasets , and then generates extra pairs of augmented queries and documents. However, these query augmentation schemes have serious obvious drawbacks. First, it is infeasible to augment queries for every document in the dataset (see the number of unlabeled documents in Figure 1), since generating and pairing queries are quite costly. Second, even after obtaining new pairs, we need extra training steps to reflect the generated pairs on the retrieval model. Third, this query augmentation method does not add variations to the documents but only to the queries, thus it may be suboptimal to handle enormous unlabeled documents. Since augmenting additional queries is costly, the question is then if it is feasible to only manipulate the given query-document pairing to handle numerous unlabeled documents. To answer this question, we first visualize the embeddings of labeled and unlabeled documents. Figure 1 shows that there is no distinct distributional shift between labeled and unlabeled documents. Thus it could be effective to manipulate only the labeled documents to handle the nearby unlabeled documents as well as the labeled documents. Using this observation, we propose a novel document augmentation method for a dense retriever, which not only interpolates two different document representations associated with the labeled query (Figure 2, center), but also stochastically perturbs the representations of labeled documents with a dropout mask (Figure 2,right). One notable advantage of our scheme is that, since it manipulates only the representations of documents, our model does not require explicit annotation steps of query-document pairs, which is efficient. We refer to our overall method as Document Augmentation for dense Retrieval (DAR).\n\nWe experimentally validate our method on standard open-domain QA datasets, namely Natural Question (NQ) (Kwiatkowski et al., 2019) and Triv-iaQA (Joshi et al., 2017) (TQA), against various evaluation metrics for retrieval models. The experimental results show that our method significantly improves the retrieval performances on both the unlabeled and labeled documents. Furthermore, a detailed analysis of the proposed model shows that interpolation and stochastic perturbation positively contribute to the overall performance.\n\nOur contributions in this work are threefold: • We propose to augment documents for dense retrieval models to tackle the problem of insufficient labels of query-document pairs. • We present two novel document augmentation schemes for dense retrievers: interpolation and perturbation of document representations. • We show that our method achieves outstanding retrieval performances on both labeled and unlabeled documents on open-domain QA tasks.\n\n ",
    "start": 3989,
    "end": 4146,
    "label": "Unsupported_claim"
  },
  {
    "span": "Self-supervised pretraining has recently become the de facto standard for ASR.",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) has benefited from advances in encoder architectures and training objectives. Limited labeled audio has motivated approaches that leverage large-scale unlabeled corpora to learn robust acoustic representations. Self-supervised pretraining has recently become the de facto standard for ASR. Methods based on contrastive or masked prediction objectives have led to improvements in low-resource and noisy conditions.\n\nHowever, questions remain about how pretraining data composition, fine-tuning schedules, and language mismatch affect downstream performance. We compare several pretraining strategies under controlled label budgets and report results across diverse noise profiles.",
    "reason": "Claims a field-wide trend ('de facto standard') without citing supporting works (rule d and b).",
    "start": 271,
    "end": 349,
    "label": "Unsupported_claim"
  },
  {
    "span": "Neural program repair approaches fine-tune sequence-to-sequence models on bug-fix pairs and evaluate on benchmarks such as Defects4J and QuixBugs (Tufano et al., 2019; Chen et al., 2019; Lutellier et al., 2020; Jiang et al., 2021).",
    "document": "Introduction\nAutomated program repair (APR) aims to reduce developer burden by generating plausible patches for failing programs. Recent neural methods learn from large corpora of historical fixes, but generalization to unseen projects and preserving semantic intent remain key challenges.\nNeural program repair approaches fine-tune sequence-to-sequence models on bug-fix pairs and evaluate on benchmarks such as Defects4J and QuixBugs (Tufano et al., 2019; Chen et al., 2019; Lutellier et al., 2020; Jiang et al., 2021). Transformer-based variants incorporate copy mechanisms and constrained decoding for syntactic validity (Yasunaga and Liang, 2020; Cambronero et al., 2019). Hybrid systems combine static analysis with learned models to prune search (Dinella et al., 2020; Jiang et al., 2021b).\nWe introduce a constrained repair framework that integrates unit-test-aware decoding with repository-level context retrieval. Our experiments investigate transfer across repositories and the impact of project-specific idioms on patch correctness.",
    "reason": "The span summarizes prior work and benchmarks without articulating how these methods relate to the paper’s proposed approach or what specific gap motivates the new method.",
    "start": 290,
    "end": 521,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Kumar et al. 3",
    "document": "Related Work\n\nPersonalized recommendation systems have evolved from matrix factorization to deep sequential models that track evolving user interests (Barton and Lee, 2018; Silva et al., 2020). Kumar et al. 3 argue that exposure bias leads to feedback loops, suggesting counterfactual correction to debias logs, while Wang and Patel (2021) incorporate causal regularizers to disentangle preference from position effects. Moreover, listwise training objectives have been shown to improve top-k metrics in practice (Owen and Sato, 2019; Chen and Roy, 2022).\n\nWe adopt a counterfactual propensity weighting scheme similar to doubly robust estimators (Hsu and Tang, 2020) and combine it with sequence-aware encoders to adapt to changes in context, extending prior click modeling frameworks (Liang and Zhou, 2019).",
    "reason": "Wrong use of footnote-style number in place of a year or proper citation. Should include the publication year (e.g., \"Kumar et al. (2020)\") or be formatted as a proper footnote.",
    "start": 194,
    "end": 208,
    "label": "Format"
  },
  {
    "span": "[Klein and Murphy, 2015]",
    "document": "Related Work\n\nLexical semantics research has documented systematic ambiguities in sense and usage. Recent surveys [Klein and Murphy, 2015] synthesize evidence on polysemy, while computational models leverage contextual embeddings for disambiguation (Peters et al., 2018; Huang et al., 2019). Our study examines domain-conditioned senses using contrastive supervision.",
    "reason": "Wrong bracket style for an author–year reference in a document using parentheses elsewhere. Should use parentheses: “(Klein and Murphy, 2015)”.",
    "start": 114,
    "end": 138,
    "label": "Format"
  },
  {
    "span": "Graph-based methods have consistently outperformed sequence models on cross-sentence relation extraction.",
    "document": "Introduction\n\nCross-sentence relation extraction requires aggregating evidence across multiple sentences, often mediated by discourse and coreference. While sequence encoders capture local patterns, they struggle to model long-range dependencies and document structure.\n\nGraph-based methods have consistently outperformed sequence models on cross-sentence relation extraction. Nevertheless, graph construction typically relies on brittle syntactic or coreference parsers, and inference can be costly for long documents.\n\nWe propose a lightweight hybrid that learns latent document graphs end-to-end, preserving long-range interactions while reducing dependency on external parsers. Our contributions include a scalable training recipe and a comprehensive ablation of graph induction choices.",
    "reason": "Asserts a consistent superiority of a prior approach over another without citing empirical studies (violates b and e).",
    "start": 271,
    "end": 376,
    "label": "Unsupported_claim"
  },
  {
    "span": "Continual learning methods mitigate catastrophic forgetting using regularization on important parameters, rehearsal of exemplars, or dynamic architectural expansion (Kirkpatrick et al., 2017; Rebuffi et al., 2017; Rusu et al., 2016). Test-time adaptation and online normalization also improve robustness under domain shift (Sun et al., 2020; Schneider et al., 2020).",
    "document": "Related Work\n\nIn continual learning, models must acquire new tasks without sacrificing performance on previously learned tasks. The key challenge is catastrophic forgetting due to parameter interference across tasks.\n\nContinual learning methods mitigate catastrophic forgetting using regularization on important parameters, rehearsal of exemplars, or dynamic architectural expansion (Kirkpatrick et al., 2017; Rebuffi et al., 2017; Rusu et al., 2016). Test-time adaptation and online normalization also improve robustness under domain shift (Sun et al., 2020; Schneider et al., 2020).\n\nWe explore a sparsity-driven plasticity mechanism that gates task-specific subnetworks while sharing a compact core. Our approach requires no exemplars and enables on-the-fly task discovery via lightweight probes, achieving strong results on class-incremental ImageNet subsets.",
    "reason": "This span catalogs categories and citations without articulating their shortcomings or how they motivate the proposed sparsity mechanism, failing to synthesize prior work with the paper’s argument (a, c).",
    "start": 218,
    "end": 584,
    "label": "Lacks_synthesis"
  },
  {
    "span": "there are many recent works that explore instruction tuning for multilingual models",
    "document": "Introduction\n\nInstruction tuning aligns large language models with user intents by finetuning on collections of task–response pairs. While most early work focused on English-only data, there is growing interest in extending alignment to multiple languages to improve global accessibility. Despite this trend, there are many recent works that explore instruction tuning for multilingual models, yet it remains unclear how to balance cross-lingual generalization and script-specific tokenization. We address this gap with a controlled study on language coverage and data mixture design.",
    "reason": "Uses the phrase “many recent works” without citing any representative studies.",
    "start": 309,
    "end": 392,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most prior approaches fine-tune transformer decoders with label smoothing for this task.",
    "document": "Related Work\n\nAbstractive summarization has benefited from advances in sequence-to-sequence pretraining, improved attention mechanisms, and copy/coverage controls. Recent methods incorporate factuality constraints and reinforcement learning to align model outputs with reference summaries and factual consistency metrics. Most prior approaches fine-tune transformer decoders with label smoothing for this task. However, the calibration of generation probabilities remains poorly understood, and overconfidence contributes to hallucinations and unsupported content.",
    "reason": "Generalizes about prior methods without citing representative works to support the claim (rule b/d).",
    "start": 322,
    "end": 410,
    "label": "Unsupported_claim"
  },
  {
    "span": "Garcia et al. 1",
    "document": "Related Work\n\nBilingual lexicon induction has progressed from mapping-based methods to joint training with shared subword vocabularies (Smith et al., 2017; Artetxe et al., 2018). Garcia et al. 1 demonstrated the benefits of orthogonal constraints for stable alignment across domains. Later work introduced iterative refinement (Lample et al., 2018) and dictionary expansion using contextualized embeddings (Conneau et al., 2020). Our approach leverages pseudo-parallel lexica extracted from comparable corpora.\n\nBeyond static embeddings, multilingual pretraining provides strong cross-lingual alignment signals (Conneau and Lample, 2019; XLM-Roberta, 2020), which we exploit to improve coverage for low-resource pairs.",
    "reason": "Wrong use of footnotes or numeric marker in place of a proper citation; should include the year or be formatted as a real footnote.",
    "start": 179,
    "end": 194,
    "label": "Format"
  },
  {
    "span": "Constrained MDP formulations introduce a separate cost signal to enforce safety thresholds during training (Altman, 1999; Achiam et al., 2017). Intrinsic curiosity drives agents to explore novel states by predicting dynamics errors (Pathak et al., 2017). Safety shields intercept unsafe actions using control-theoretic filters (Alshiekh et al., 2018). Risk-sensitive objectives optimize tail risk measures such as CVaR (Tamar et al., 2015).",
    "document": "Related Work\n\nSafe Exploration in Reinforcement Learning\nReal-world RL must negotiate the tension between performance and safety, preventing catastrophic actions especially during early training (García and Fernández, 2015). Methods vary in how they encode constraints, estimate risk, and intervene during control.\n\nFormulations and Mechanisms\nConstrained MDP formulations introduce a separate cost signal to enforce safety thresholds during training (Altman, 1999; Achiam et al., 2017). Intrinsic curiosity drives agents to explore novel states by predicting dynamics errors (Pathak et al., 2017). Safety shields intercept unsafe actions using control-theoretic filters (Alshiekh et al., 2018). Risk-sensitive objectives optimize tail risk measures such as CVaR (Tamar et al., 2015). Model-based approaches estimate dynamics to forecast constraint violations (Dalal et al., 2018). While complementary, these strands are rarely combined systematically under shared benchmarks.\n\nBenchmarks and Metrics\nSimulators such as Safety Gym and CARLA offer standardized tasks with measurable constraint violations (Ray et al., 2019; Dosovitskiy et al., 2017). We adopt these settings to unify evaluation of constrained and shielded learners under comparable risk budgets.",
    "reason": "The span abruptly shifts between constrained MDPs, curiosity-driven exploration, shielding, and risk objectives without transitions or stated relationships, making coherence across the citations unclear.",
    "start": 344,
    "end": 784,
    "label": "Coherence"
  },
  {
    "span": "BERT was used in an AES task trained on essays from multiple prompts",
    "document": "Related Work\n\nAutomatic essay scoring (AES) has progressed from handcrafted features and linear models to neural architectures that learn holistic representations of writing quality. Sequence encoders and hierarchical models capture local coherence and discourse structure, while attention mechanisms highlight salient evidence. More recently, BERT was used in an AES task trained on essays from multiple prompts to improve cross-prompt generalization and reduce calibration error. Complementary directions include domain adaptation to new rubrics and fairness-aware modeling to mitigate demographic bias.",
    "reason": "Claims a specific prior setup and result in AES using BERT without citing the corresponding study, which requires a citation (rule b/a).",
    "start": 344,
    "end": 412,
    "label": "Unsupported_claim"
  },
  {
    "span": "Cross-lingual biomedical named entity recognition has leveraged multilingual transformers fine-tuned on English corpora with zero-shot transfer. Annotation projection aligns source mentions to target languages via parallel texts and word alignments. Distant supervision uses dictionaries and ontologies to generate weak labels. Self-training and pseudo-labeling iteratively refine target-language predictions. We build on multilingual transformers with a lexicon-aware decoding layer.",
    "document": "Introduction\n\nBiomedical NER across languages is hindered by label scarcity and domain mismatch between general-purpose multilingual models and specialized biomedical terminology. Methods vary in the degree of supervision and reliance on external resources.\n\nCross-lingual biomedical named entity recognition has leveraged multilingual transformers fine-tuned on English corpora with zero-shot transfer. Annotation projection aligns source mentions to target languages via parallel texts and word alignments. Distant supervision uses dictionaries and ontologies to generate weak labels. Self-training and pseudo-labeling iteratively refine target-language predictions. We build on multilingual transformers with a lexicon-aware decoding layer.\n\nWe evaluate on multiple European and Asian languages with entity-level F1 and conduct ablations on lexicon coverage.",
    "reason": "The span summarizes techniques and then presents the authors' method without explaining how lexicon-aware decoding remedies a specific deficiency in prior cross-lingual approaches or stating the authors' perspective.",
    "start": 259,
    "end": 743,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recently, several studies have explored prompt-based learning and in-context adaptation for large language models (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022; Zhou et al., 2022).",
    "document": "Introduction\n\nLarge language models (LLMs) have demonstrated strong few-shot and zero-shot performance across a range of NLP tasks. Their emergent abilities have motivated research on methods that steer model behavior without task-specific fine-tuning. While parameter-efficient strategies reduce compute and data burden, the space of control mechanisms remains broad and rapidly evolving.\n\nRecently, several studies have explored prompt-based learning and in-context adaptation for large language models (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022; Zhou et al., 2022). Prompt design, calibration, and automatic prompt search methods have been proposed to improve downstream accuracy and stability. Parallel efforts introduce instruction tuning and chain-of-thought prompting to enhance reasoning and generalization.\n\nIn this paper, we investigate controllable text generation under weak supervision. We present a framework that uses automatically induced control codes to influence content attributes without full model fine-tuning, and we evaluate across summarization and style transfer benchmarks.",
    "reason": "The span lists prior work on prompts without connecting those studies to the paper’s goals, gap, or methodological choices; it describes citations but does not synthesize them into an argument or explain their relevance.",
    "start": 391,
    "end": 599,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[Garcia et al., 2018)",
    "document": "Related Work\n\nGraph neural networks (GNNs) generalize convolution and attention to non-Euclidean domains, enabling learning on graphs for tasks such as node classification and link prediction (Kipf and Welling, 2017; Veličković et al., 2018). Early message-passing methods focus on local neighborhood aggregation, while more recent architectures incorporate global attention and positional encodings (Dwivedi et al., 2021; Rampášek et al., 2022).\n\nRobustness to distribution shifts has been studied through subgraph training, invariance penalties, and test-time adaptation [Garcia et al., 2018). We complement these approaches by introducing a causal regularizer that prioritizes stable relational features under node attribute perturbations.\n\nOur benchmark spans citation, molecular, and social graphs, comparing to strong baselines with domain generalization objectives (Arjovsky et al., 2019; Sagawa et al., 2020).",
    "reason": "Mismatched brackets in the citation: opening square bracket with closing parenthesis. Should use matching parentheses.",
    "start": 573,
    "end": 594,
    "label": "Format"
  },
  {
    "span": "Data augmentation for low-resource ASR includes speed perturbation, noise mixing, and vocal tract length perturbation (Ko et al., 2015; Cui et al., 2015; Ragni et al., 2014). Text-to-speech based augmentation and neural codec resynthesis have also been explored (Ragni et al., 2019; Polyak et al., 2021).",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) performance in low-resource settings is constrained by limited transcribed audio and domain mismatch. Augmentation can reduce overfitting and improve robustness, but designing transformations that preserve phonetic content while enriching variability remains difficult.\n\nData augmentation for low-resource ASR includes speed perturbation, noise mixing, and vocal tract length perturbation (Ko et al., 2015; Cui et al., 2015; Ragni et al., 2014). Text-to-speech based augmentation and neural codec resynthesis have also been explored (Ragni et al., 2019; Polyak et al., 2021).\n\nWe propose phoneme-consistent spectral remixing that combines prosodic contours from unlabeled in-domain speech with labeled utterances via forced alignment, yielding gains on three low-resource languages with minimal computational overhead.",
    "reason": "The span only enumerates augmentation techniques from prior work without explaining how they relate to the authors’ approach or what gap they leave unaddressed.",
    "start": 320,
    "end": 624,
    "label": "Lacks_synthesis"
  },
  {
    "span": " (Xue et al., 2016;Lee and Goldwasser, 2019;, ",
    "document": "Introduction\n\nDistributed representations of events, are a common way to represent events in a machine-readable form and have shown to provide meaningful features for various tasks (Lee and Goldwasser, 2018;Rezaee and Ferraro, 2021;Deng et al., 2021;Martin et al., 2018;. Obtaining effective event representations is challenging, as it requires representations to capture various relations between events. Figure 1 presents four pairs of events with different relations. Two events may share the same event attributes (e.g. event types and sentiments), and there may also be a causal or temporal relation between two events.\n\nEarly works (Weber et al., 2018) exploit easily accessible co-occurrence relation of events to learn event representations. Although the use of cooccurrence relation works well, it is too coarse for deep understanding of events, which requires finegrained knowledge (Lee and Goldwasser, 2019). Recent works focus on fine-grained knowledge, such as discourse relations (Prasad et al., 2006;Lee and Goldwasser, 2019;Zheng et al., 2020) and commonsense knowledge (e.g. sentiments and intents) (Sap et al., 2019;Ding et al., 2019). Concretely, Lee and Goldwasser (2019) and Zheng et al. (2020) leverage 11 discourse relation types to model event script knowledge. Ding et al. (2019) incorporate manually labeled commonsense knowledge (intents and sentiments) into event representation learning. However, the types of fine-grained event knowledge are so diverse that we cannot enumerate all of them and currently adopted finegrained knowledge fall under a small set of event knowledge. In addition, some manually labeled knowledge (Sap et al., 2019;Hwang et al., 2021) is costly and difficult to apply on large datasets.\n\nIn our work, we observe that there is a rich amount of information in co-occurring events, but previous works did not make good use of such information. Based on existing works on event relation extraction (Xue et al., 2016;Lee and Goldwasser, 2019;, we find that the co-occurrence relation, which refers to two events appearing in the same document, can be seen as a superset of current defined explicit discourse relations, as most existing automatic methods extract event relations from documents or sentences. More than that, it also includes other implicit event knowledge, that is, events that occur in the same document may share the same topic and event type. Previous works (Granroth-Wilding and Clark, 2016;Weber et al., 2018) based on cooccurrence information usually exploit instancewise contrastive learning approaches related to the margin loss, which consists of an anchor, positive, and negative sample, where the anchor is more similar to the positive than the negative. However, they share two common limitations: (1) such marginbased approaches struggle to capture the essential differences between events with different semantics, as they only consider one positive and one negative per anchor. (2) Randomly sampled negative samples may contain samples semantically related to the anchor but are undesirably pushed apart in embedding space. This problem arises because these instance-wise contrastive learning approaches treat randomly selected events as negative samples, regardless of their semantic relevance.\n\nWe are motivated to address the above issues with the goal of making better use of cooccurrence information of events. To this end, we present SWCC: a Simultaneous Weakly supervised Contrastive learning and Clustering framework for event representation learning, where we exploit document-level co-occurrence information of events as weak supervision and learn event representations by simultaneously performing weakly supervised contrastive learning and prototype-based clustering. To address the first issue, we build our approach on the contrastive framework with the InfoNCE objective (van den Oord et al., 2019), which is a self-supervised contrastive learning method that uses one positive and multiple negatives. Further, we extend the InfoNCE to a weakly supervised contrastive learning setting, allowing us to consider multiple positives and multiple negatives per anchor (as opposed to the margin loss which uses only one positive and one negative). Co-occurring events are then incorporated as additional positives, weighted by a normalized co-occurrence frequency. To address the second issue, we introduce a prototype-based clustering method to avoid semantically related events being pulled apart. Specifically, we impose a prototype for each cluster, which is a representative embed-ding for a group of semantically related events. Then we cluster the data while enforce consistency between cluster assignments produced for different augmented representations of an event. Unlike the instance-wise contrastive learning, our clustering method focuses on the cluster-level semantic concepts by contrasting between representations of events and clusters. Overall, we make the following contributions:\n\n• We propose a simple and effective framework (SWCC) that learns event representations by making better use of co-occurrence information of events. Experimental results show that our approach outperforms previous approaches on several event related tasks. • We introduce a weakly supervised contrastive learning method that allows us to consider multiple positives and multiple negatives, and a prototype-based clustering method that avoids semantically related events being pulled apart. • We provide a thorough analysis of the prototypebased clustering method to demonstrate that the learned prototype vectors are able to implicitly capture various relations between events. The source code 1 of our SWCC has been uploaded to Anonymous Github for reproducing our results.\n\n ",
    "start": 1948,
    "end": 1994,
    "label": "Format"
  },
  {
    "span": "Nguyen et al.",
    "document": "Introduction\n\nSemi-supervised text classification leverages small labeled sets with large unlabeled corpora (Alvarez and Kim, 2017; Park et al., 2020). Following the approach of Nguyen et al., we design a consistency-regularized encoder that distills pseudo-labels from augmented views. Prior work has explored sharpening targets to avoid confirmation bias (Morgan, 2019) and curriculum schedules to stabilize training (Shen and Luo, 2021), but none study domain drift across time.\n\nRelated Work\n\nConsistency-based methods enforce prediction invariance to augmentations (Zhou et al., 2020; Dutta et al., 2021). Self-training with confidence thresholds is also common (Rahman and Lee, 2018). Our method unifies these streams with adaptive perturbations.",
    "reason": "Narrative citation missing year. In APA style, narrative mentions should include the year, e.g., 'Nguyen et al. (2019)'.",
    "start": 178,
    "end": 191,
    "label": "Format"
  },
  {
    "span": "[Mikolov et al., 2013]",
    "document": "Related Work\n\nDistributed word representations capture semantic similarity through co-occurrence statistics and have become standard features in NLP pipelines. Predictive models such as skip-gram with negative sampling learn embeddings that exhibit linear regularities (Pennington et al., 2014; Levy and Goldberg, 2014). Subword modeling further improves representations for rare and morphologically rich words (Bojanowski et al., 2017). Prior work [Mikolov et al., 2013] shows that simple context windows suffice to learn high-quality vectors. In contrast, we study curriculum schedules that prioritize informative contexts for faster convergence.",
    "reason": "Wrong citation style: author–year citation is placed inside square brackets as if using a numeric style; should be '(Mikolov et al., 2013)'.",
    "start": 449,
    "end": 471,
    "label": "Format"
  },
  {
    "span": "The widely used XQuAD+ dataset contains 200k question-answer pairs with document-level rationales.",
    "document": "Introduction\n\nQuestion answering benchmarks have evolved from sentence-level factoid tasks to document-level reasoning requiring multi-hop inference and explanation. Datasets that provide rationales are particularly valuable for training models that can justify predictions and for evaluating faithfulness.\n\nThe widely used XQuAD+ dataset contains 200k question-answer pairs with document-level rationales. Despite its popularity, reported metrics often obscure fine-grained error types, such as partial span overlaps and rationale hallucination. Furthermore, cross-lingual generalization on rationale-bearing QA remains understudied due to the scarcity of aligned annotations.\n\nWe address these gaps by introducing a diagnostic suite that decouples answer extraction from rationale selection and measures consistency under paraphrase and translation. Our experiments reveal that models trained with rationale supervision improve calibration but remain brittle to distractors.",
    "reason": "A specific dataset and its statistics are asserted without a citation to the dataset introduction or documentation (violates rule a/b).",
    "start": 308,
    "end": 406,
    "label": "Unsupported_claim"
  },
  {
    "span": "Human evaluation typically requires at least 5 raters per item to achieve acceptable reliability.",
    "document": "Related Work\n\nEvaluating generated text often relies on human judgments due to limitations of reference-based automatic metrics (Novikova et al., 2017; Callison-Burch et al., 2007). Best practices emphasize clear task design, assessor training, and reliability analysis through inter-rater agreement or generalizability theory (Artstein and Poesio, 2008).\n\nHuman evaluation typically requires at least 5 raters per item to achieve acceptable reliability. In our study, we follow this guidance and compute confidence intervals via bootstrap aggregation.",
    "reason": "Asserts a numeric rule-of-thumb for rater counts without evidence or citation (rule b: specific methodological prescription).",
    "start": 357,
    "end": 454,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior work has examined improving factual consistency in neural summarization via post-editing modules, constrained decoding, and auxiliary fact verification losses (Cao et al., 2018; Kryscinski et al., 2020; Zhao et al., 2020; Dong et al., 2021; Laban et al., 2022). Additional efforts leverage knowledge bases or entity linking to encourage faithful content selection (Nie et al., 2019; Chen et al., 2021; Goyal and Durrett, 2021).",
    "document": "Introduction\n\nAbstractive news summarization has reached a level of fluency that makes it widely usable, yet factual inconsistencies and unsupported claims (so-called hallucinations) remain a barrier to deployment in high-stakes settings. While stronger pretraining has improved surface quality, controlling faithfulness requires methods that reason about entities and relations grounded in the source document.\n\nEvaluation of factuality has evolved from n-gram overlap to entailment-style and information extraction-based metrics, but these diagnostics often disagree and can be gamed by trivial heuristics. Hence, modeling interventions remain essential to improve the causal determinants of factual faithfulness.\n\nPrior work has examined improving factual consistency in neural summarization via post-editing modules, constrained decoding, and auxiliary fact verification losses (Cao et al., 2018; Kryscinski et al., 2020; Zhao et al., 2020; Dong et al., 2021; Laban et al., 2022). Additional efforts leverage knowledge bases or entity linking to encourage faithful content selection (Nie et al., 2019; Chen et al., 2021; Goyal and Durrett, 2021).\n\nIn this paper we introduce a content-aligned editing objective that steers generation with document-level atomic facts extracted from the source. We couple this with a light-weight verifier trained on counterfactual perturbations to penalize unsupported statements without degrading informativeness.",
    "reason": "The span lists prior studies and techniques without explaining how they relate to the paper's approach, what limitations remain, or what specific gap motivates the new method; it provides no synthesis.",
    "start": 717,
    "end": 1150,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Teacher–student distillation transfers soft targets to smaller models (Hinton et al., 2015). Pruning removes redundant weights to shrink networks (Han et al., 2015). Offline reinforcement learning optimizes decision-making from logged data (Levine et al., 2020).",
    "document": "Related Work\n\nCompressing large language models aims to reduce memory and latency while preserving task performance. Common approaches include knowledge distillation, pruning, quantization, and low-rank adaptation, often combined with task-specific fine-tuning.\n\nTeacher–student distillation transfers soft targets to smaller models (Hinton et al., 2015). Pruning removes redundant weights to shrink networks (Han et al., 2015). Offline reinforcement learning optimizes decision-making from logged data (Levine et al., 2020). Recent advances in parameter-efficient tuning (Houlsby et al., 2019; Hu et al., 2021) attach small modules to frozen backbones, and quantization-aware training reduces precision with minimal degradation (Jacob et al., 2018).\n\nWe study instruction-tuned distillation with quantization-aware adapters, targeting edge deployment with tight latency budgets.",
    "reason": "The third sentence introduces offline reinforcement learning without any transition or explanation of its relevance to distillation and pruning, making the sequence incoherent.",
    "start": 263,
    "end": 525,
    "label": "Coherence"
  },
  {
    "span": "BPR optimizes pairwise ranking (Rendle et al., 2009). Contrastive learning uses augmentations to pull positive pairs (Xie et al., 2020). Propensity scoring adjusts for selection bias in implicit feedback (Schnabel et al., 2016). Causal inference frameworks formalize counterfactual recommendations (Bonner and Vasile, 2018).",
    "document": "Related Work\n\nModern recommender systems must learn robust preferences from implicit feedback while mitigating bias induced by historical exposure and user behavior. Recent approaches combine representation learning with techniques from causal inference to improve counterfactual generalization.\n\nBPR optimizes pairwise ranking (Rendle et al., 2009). Contrastive learning uses augmentations to pull positive pairs (Xie et al., 2020). Propensity scoring adjusts for selection bias in implicit feedback (Schnabel et al., 2016). Causal inference frameworks formalize counterfactual recommendations (Bonner and Vasile, 2018).\n\nDespite these contributions, a principled route to integrate debiasing with self-supervised signals remains open. We propose a risk decomposition that connects augmentation invariances to inverse-propensity-weighted objectives.",
    "reason": "The cited works are presented as isolated sentences without transitions or an explicit narrative linking ranking, contrastive learning, and causal debiasing, reducing coherence.",
    "start": 297,
    "end": 621,
    "label": "Coherence"
  },
  {
    "span": "Garcia et al.. (2015)",
    "document": "Related Work\n\nKnowledge graph completion models have evolved from tensor factorization to graph neural link predictors (Nguyen and Blake, 2018; Ortiz et al., 2020). As noted by Garcia et al.. (2015), relation patterns such as symmetry and transitivity can guide architecture design, and subsequent works encode these inductive biases explicitly (He and Zhou, 2019; Park and Lin, 2021). Recent benchmarks highlight the importance of calibrated scores for reliable ranking under incomplete graphs (Singh and Yao, 2022).\n\nWe contribute a modular scoring function that composes path-based features with neural embeddings, improving generalization across relation types and graph densities.",
    "reason": "Extraneous period before the year in a narrative citation. Should be \"Garcia et al. (2015)\" with a single period after \"et al.\".",
    "start": 177,
    "end": 198,
    "label": "Format"
  },
  {
    "span": "Smith et al., (2020)",
    "document": "Related Work\n\nGraph neural networks (GNNs) generalize convolution to non-Euclidean domains by aggregating information from local neighborhoods (Kipf and Welling, 2017; Hamilton et al., 2017). Variants differ in their message-passing schemes, expressivity, and computational trade-offs (Xu et al., 2019). Recent studies have focused on oversmoothing and heterophily, proposing architectures that mitigate representation collapse (Pei et al., 2020; Zhu et al., 2020). As demonstrated by Smith et al., (2020), careful normalization and residual connections can stabilize deep GNNs. Building on this, we propose a regularizer that adapts smoothing to graph topology.",
    "reason": "Comma incorrectly inserted before the parenthetical year in a narrative citation; should be 'Smith et al. (2020)'.",
    "start": 485,
    "end": 505,
    "label": "Format"
  },
  {
    "span": "there has been an explosion of recent works on self-supervised speech pretraining",
    "document": "Introduction\n\nSelf-supervised learning has reshaped representation learning by exploiting abundant unlabeled data. In speech, learned acoustic representations can reduce the labeling burden and improve performance across ASR, speaker recognition, and spoken language understanding.\n\nIn the last three years, there has been an explosion of recent works on self-supervised speech pretraining, leading to significant advances in ASR and spoken language understanding. However, most approaches focus on encoder-only architectures and under-explore pretraining strategies for sequence generation.\n\nWe introduce an encoder-decoder pretraining strategy that aligns acoustic and linguistic spaces through masked prediction and cross-modal contrastive objectives. We demonstrate consistent improvements on multilingual ASR and speech translation.",
    "reason": "Mentions 'recent works' without citing any examples, violating the requirement to back claims about prior literature with citations (rule d).",
    "start": 308,
    "end": 389,
    "label": "Unsupported_claim"
  },
  {
    "span": "Zhou et al.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have emerged as a powerful paradigm for learning over structured data, enabling state encoders that respect permutation invariance and exploit local connectivity (Kipf and Welling, 2017; Hamilton et al., 2017). Message-passing networks unify a wide family of architectures under a common framework (Gilmer et al., 2017), while spectral approaches provide theoretical grounding for convolution over graphs (Defferrard et al., 2016). Zhou et al. extend these models to dynamic graphs, demonstrating improvements on temporal link prediction and evolving community detection, but they do not evaluate under severe distribution shift. Subsequent work incorporates positional encodings to capture long-range dependencies (Dwivedi et al., 2021) and leverages subgraph extraction for scalable training (Zeng et al., 2021). We focus on robustness to structural sparsity, proposing regularizers that decouple topology noise from feature noise.\n\nIntroduction\n\nDespite their success, GNNs can suffer from oversmoothing and oversquashing, where repeated aggregation dilutes node-specific information (Li et al., 2018; Alon and Yahav, 2021). Recent remedies include residual connections and normalization tailored for graphs (Chen et al., 2020b) as well as expressivity gains via higher-order motifs (Morris et al., 2019).",
    "reason": "Narrative citation missing year; should include the publication year, e.g., \"Zhou et al. (2019)\".",
    "start": 475,
    "end": 486,
    "label": "Format"
  },
  {
    "span": "There are many recent works that explore contrastive pretraining for caption generation.",
    "document": "Introduction\n\nImage captioning aims to generate natural language descriptions for images by jointly modeling visual content and linguistic structure. Transformer-based architectures have become dominant, with encoder-decoder or prefix-tuning variants used to condition text on image features.\n\nThere are many recent works that explore contrastive pretraining for caption generation. These methods typically align images and texts in a shared embedding space and then fine-tune generative decoders. Despite strong zero-shot retrieval performance, their benefits for downstream caption quality remain unsettled, especially under limited supervision.\n\nWe investigate how contrastive pretraining objectives interact with sequence-to-sequence learning. Our contributions include: (1) a controlled study of negative sampling strategies, (2) an analysis of alignment-calibration trade-offs, and (3) a simple curriculum that improves fluency without sacrificing semantic fidelity.",
    "reason": "Vague reference to 'many recent works' without providing citations (violates rule d; mentions of recent works must be supported by references).",
    "start": 294,
    "end": 382,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Vatswani et al., 2019)",
    "document": "Related Work\n\nNeural retrievers have rapidly advanced open-domain question answering by enabling efficient dense indexing of large corpora (Johnson et al., 2020; Xie and Rao, 2021). While early approaches relied on sparse lexical signals (Robertson and Zaragoza, 2009), recent methods employ dual-encoders trained with contrastive objectives to align questions and passages (Karpal and Finn, 2020; Liu et al., 2021). In (Vatswani et al., 2019), the authors introduce a hybrid retriever that combines lexical and dense signals but train the components independently, limiting end-to-end gains. Subsequent work integrates retrieval and reading through joint training, improving answerability under domain shift (Kang et al., 2020; Ortega et al., 2022). Generative readers further mitigate error propagation by marginalizing over multiple passages (Rao and Jensen, 2020). Despite these improvements, retrieval remains brittle when queries contain sparse evidence or long-range dependencies (Miller and Huang, 2021). We explore query reformulation techniques that reweight evidence-bearing terms and smooth semantic drift (Chen and Park, 2022), and we evaluate their impact on both recall and downstream exact-match accuracy.\n",
    "reason": "Wrong citation style: a preposition directly precedes a parenthetical citation. It should be narrative (e.g., \"In Vatswani et al. (2019)\") or rephrased without the leading preposition.",
    "start": 417,
    "end": 443,
    "label": "Format"
  },
  {
    "span": "BPE with 32k merges is the de facto standard for low-resource NMT.",
    "document": "Introduction\n\nLow-resource neural machine translation (NMT) remains challenging due to data scarcity and domain mismatch. Transfer learning, multilingual training, and data augmentation are common strategies to improve performance. Tokenization choices, including character-level modeling, byte-pair encoding (BPE), and unigram segmentation, significantly affect downstream results.\n\nBPE with 32k merges is the de facto standard for low-resource NMT. However, segmentation granularity interacts with vocabulary coverage, morphology, and training stability, suggesting that a one-size-fits-all configuration may be suboptimal.\n\nWe propose an adaptive segmentation curriculum that adjusts merge operations over training, jointly optimizing translation quality and vocabulary efficiency.",
    "reason": "Asserts a field-wide standard practice with a specific configuration but provides no citation or survey to support it.",
    "start": 384,
    "end": 450,
    "label": "Unsupported_claim"
  },
  {
    "span": "Vastwani et al. 1",
    "document": "Introduction\n\nCross-lingual entity linking connects mentions in many languages to a shared knowledge base, reducing annotation costs for low-resource settings (Upadhyay et al., 2018; Botha et al., 2020). Building on multilingual representations, recent models incorporate transliteration and phonetic matching to handle script variance (Muller et al., 2020). As argued by Vastwani et al. 1, joint training with mention detection can further improve disambiguation, but prior setups often leak language identity and hurt zero-shot generalization (Pan et al., 2017; Wu and Dredze, 2019). We propose a language-agnostic span selector coupled with candidate reranking.",
    "reason": "Incorrect footnote-style marker used with an author name and missing year; should include the year as a narrative citation (e.g., 'Vastwani et al. (YEAR)') or be formatted as a proper footnote.",
    "start": 372,
    "end": 389,
    "label": "Format"
  },
  {
    "span": "We build on a widely used sarcasm dataset released on Reddit in 2018.",
    "document": "Datasets for Multimodal Sarcasm Detection\n\nSarcasm detection benefits from integrating textual cues with paralinguistic and contextual signals from images and user histories. Public datasets vary in annotation protocol, balance, and platform, complicating cross-study comparisons. We build on a widely used sarcasm dataset released on Reddit in 2018. To enrich multimodal coverage, we augment posts with linked images and temporally adjacent comments, curating a cleaner subset with stricter consensus labels.\n\nOur dataset card documents source platforms, preprocessing steps, annotator demographics, and known limitations. We also release strong baselines spanning late fusion transformers and cross-modal alignment objectives, along with standardized train/validation/test splits for reproducibility.",
    "reason": "Mentions a specific dataset and release year but fails to provide a citation to the dataset paper or repository at first mention.",
    "start": 281,
    "end": 350,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Chen et al., 2020;.)",
    "document": "Introduction\n\nPretrained sequence-to-sequence models have reshaped text generation by enabling task-agnostic finetuning (Raffel et al., 2020; Lewis et al., 2020). For controllable generation, attribute-specific prompts and discriminators guide outputs toward desired styles while maintaining fluency (Dathathri et al., 2020; Krause et al., 2021). Nevertheless, degeneration and exposure bias persist, particularly under long-form constraints (Holtzman et al., 2020). Recent surveys summarize mitigation tactics (Chen et al., 2020;.) but overlook evaluation protocols for factual consistency.\n\nWe contribute a unified benchmark and training recipe that jointly targets fluency, control, and factuality.",
    "reason": "Extraneous punctuation inside the citation: a semicolon followed by a period is incorrect formatting.",
    "start": 511,
    "end": 532,
    "label": "Format"
  },
  {
    "span": "Johnson et al., 2020)",
    "document": "Introduction\n\nRobustness to distribution shift has become a central criterion for model deployment (Sagawa et al., 2020; Koh et al., 2021). Techniques such as group DRO and invariant risk minimization seek predictors that remain stable across environments (Arjovsky et al., 2020; Sagawa et al., 2020). Recent encoder architectures improve worst-group accuracy by reweighting gradients based on estimated group membership (Liu et al., 2021; Creager et al., 2021). Johnson et al., 2020) show that model calibration is critical for selective prediction under shift, motivating training-time interventions that align confidence with accuracy (Guo et al., 2017; Ovadia et al., 2019).\n\nDespite these advances, model selection under shift remains challenging. Validation heuristics that approximate worst-group risk or rely on proxy shifts can be brittle (Rosenfeld et al., 2020; Taori et al., 2020). We propose a calibration-aware early stopping criterion that correlates better with robust test performance.",
    "reason": "Mismatched parenthesis: closing parenthesis appears without an opening one; citation should be narrative 'Johnson et al. (2020)'.",
    "start": 463,
    "end": 484,
    "label": "Format"
  },
  {
    "span": "(Lopez et al. 2021)",
    "document": "Related Work\n\nNeural retrieval methods have shifted from sparse lexical matching to dense vector encoders trained with contrastive losses (Karpukhin et al., 2020; Xiong et al., 2021). Hybrid retrieval combines the strengths of both paradigms, yielding robust performance across domains (Gao et al., 2021; Lin et al., 2021). Negative sampling strategies and hard-mining curricula have been shown to be critical for generalization (Zhan et al., 2021; Qu et al., 2021). Cross-encoder re-rankers further refine candidate sets at higher computational cost (Nogueira and Cho, 2019; Pradeep et al., 2021). Recent domain-adaptive pretraining enhances out-of-domain robustness (Izacard et al., 2022; Lopez et al. 2021).",
    "reason": "Missing comma in APA-style parenthetical citation; should be (Lopez et al., 2021).",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on essays from the ASAP competition",
    "document": "Related Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and linear models to neural architectures that capture semantics and discourse structure. Early neural AES approaches used LSTMs and CNNs to encode token sequences, while subsequent work incorporated attention and pretrained language models for richer representations.\n\nSeveral studies explore transfer learning for AES, domain adaptation across prompts, and robustness to adversarial edits. In particular, BERT was used in an AES task trained on essays from the ASAP competition, demonstrating the benefit of contextualized embeddings over static word vectors. Complementary lines of work investigate coherence modeling and argument mining to capture higher-level writing quality aspects.\n\nOur method extends this line by integrating prompt-aware adapters and rhetorical move supervision, aiming to improve cross-prompt generalization without extensive labeled data.",
    "reason": "Describes a specific experimental setup (BERT in AES on a named competition dataset) without citing the corresponding paper(s) (definition a/eiii).",
    "start": 489,
    "end": 561,
    "label": "Unsupported_claim"
  },
  {
    "span": "Hernandez et al. 1",
    "document": "Related Work\n\nExplainability for graph neural networks (GNNs) spans feature attribution, subgraph extraction, and counterfactual reasoning. Early perturbation-based methods quantify edge importance via performance degradation (Yuan et al., 2020), while mask-learning approaches optimize discrete subgraphs for fidelity (Ying et al., 2019). Following Hernandez et al. 1, we consider plausibility in addition to fidelity by aligning explanations with human-annotated rationales, but depart from their reliance on node labels during training.\n\nCausal framing has recently been introduced to separate spurious from invariant structures (Wu et al., 2022). Our method complements these works by regularizing explanation stability under graph augmentations.",
    "reason": "Improper footnote-like marker appended to an author citation; should include a year (e.g., Hernandez et al. (YYYY)) or be reformatted as a proper footnote.",
    "start": 350,
    "end": 368,
    "label": "Format"
  },
  {
    "span": "Smith et al, 2020",
    "document": "Related Work\n\nData Augmentation for Natural Language Processing\n\nData augmentation improves generalization by diversifying training examples (Fadaee et al., 2017; Kobayashi, 2018). Back-translation remains a strong baseline for sequence tasks (Sennrich et al., 2016), while token-level noising and span masking provide complementary gains (Xie et al., 2020; Gontijo-Lopes et al., 2020). Synonym replacement and paraphrastic augmentation were first tested at scale by Smith et al, 2020, demonstrating improvements under low-resource regimes, and later combined with task-specific consistency losses (Clark et al., 2020).\n",
    "reason": "Incorrect author–year formatting: missing parentheses around the year and missing period after 'al.'; should be 'Smith et al. (2020)'.",
    "start": 467,
    "end": 484,
    "label": "Format"
  },
  {
    "span": "(Santos et al., 2015; Zhou et al., 2016;.",
    "document": "Related Work\n\nNeural relation extraction has progressed from feature-based and kernel methods (Zelenko et al., 2003; Bunescu and Mooney, 2005) to convolutional and recurrent architectures that encode sentences with distant supervision (Mintz et al., 2009; Zeng et al., 2014). Piecewise CNNs and attention mechanisms improved instance selection and noise robustness (Lin et al., 2016; Ji et al., 2017).\n\nRecent advances include position-aware LSTMs and segment-level attention to mitigate overlapping relations (Zhang et al., 2017; Wang et al., 2018). Multi-instance learning and bag-level attention have become standard in distant supervision pipelines (Lin et al., 2016; Ye and Ling, 2019). Prior sequence encoders have been extended with dependency structures and coreference cues (Miwa and Bansal, 2016; Sahu et al., 2019).\n\nContextual embeddings from pretrained language models further raised the bar for sentence-level relation extraction (Peters et al., 2018; Baldini Soares et al., 2019), and span-based formulations unify entity and relation prediction (Luan et al., 2019). Early neural baselines include (Santos et al., 2015; Zhou et al., 2016;. Our work differs by addressing calibration under label noise and proposing a confidence-aware loss for bag aggregation.\n\nWe evaluate on standard benchmarks with manual and distantly supervised labels, and analyze failure modes under entity boundary errors.",
    "reason": "Extraneous punctuation and missing closing parenthesis in a multi-citation; the sequence ends with \";.\" and lacks the final \")\".",
    "start": 1113,
    "end": 1154,
    "label": "Format"
  },
  {
    "span": "MIMIC-III includes over 50,000 ICU stays and is widely used for clinical NLP benchmarking.",
    "document": "Related Work\n\nClinical NLP has seen rapid growth with the availability of de-identified electronic health records. MIMIC-III includes over 50,000 ICU stays and is widely used for clinical NLP benchmarking. Prior efforts in concept extraction, de-identification, and mortality prediction have leveraged this resource to assess progress and reproducibility. However, domain shifts across institutions and note types often degrade performance. We address these issues by proposing a domain-adaptive pretraining regimen tailored to clinical discourse structure.",
    "reason": "Mentions a specific dataset and quantitative detail, as well as its widespread use, without citing the dataset paper or supporting studies (criteria a and b).",
    "start": 115,
    "end": 205,
    "label": "Unsupported_claim"
  },
  {
    "span": "Exploration in reinforcement learning has been addressed through heuristic strategies (epsilon-greedy, Boltzmann), optimism (UCB, optimism in the face of uncertainty), count-based bonuses and pseudo-counts, intrinsic motivation via curiosity and prediction error, and randomized value functions (Strehl and Littman, 2008; Bellemare et al., 2016; Pathak et al., 2017; Osband et al., 2016). Recent additions include RND and information gain estimators that encourage visits to novel states.",
    "document": "Related Work: Exploration for Deep Reinforcement Learning\n\nEfficient exploration is crucial for sparse-reward and long-horizon tasks. In deep RL, balancing exploration with function approximation stability remains challenging due to non-stationarity and partial observability.\n\nExploration in reinforcement learning has been addressed through heuristic strategies (epsilon-greedy, Boltzmann), optimism (UCB, optimism in the face of uncertainty), count-based bonuses and pseudo-counts, intrinsic motivation via curiosity and prediction error, and randomized value functions (Strehl and Littman, 2008; Bellemare et al., 2016; Pathak et al., 2017; Osband et al., 2016). Recent additions include RND and information gain estimators that encourage visits to novel states.\n\nWe adapt these ideas to a continuous-control setting with an ensemble-driven bonus and stability constraints.",
    "reason": "The span lists techniques and references without relating them to the proposed approach or explaining which shortcomings motivate the adaptation; the following sentence only mentions an adaptation without an explicit gap (criteria a and b).",
    "start": 278,
    "end": 766,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Imitation learning, deep reinforcement learning with actor-critic methods, and model-based planning have all been explored for decision-making in autonomous vehicles (Codevilla et al., 2018; Kendall et al., 2019; Chitta et al., 2021; Sauer et al., 2018).",
    "document": "Introduction\n\nRobust decision-making under uncertainty is a critical component of autonomous driving. Agents must reason about intent, safety margins, and long-horizon objectives while reacting in real time to dynamic scenes.\n\nImitation learning, deep reinforcement learning with actor-critic methods, and model-based planning have all been explored for decision-making in autonomous vehicles (Codevilla et al., 2018; Kendall et al., 2019; Chitta et al., 2021; Sauer et al., 2018).\n\nWe propose a risk-aware hierarchical controller with uncertainty calibration that couples a scenario-level planner with a reactive local policy, enabling improved sample efficiency and safety guarantees.",
    "reason": "The span simply catalogs existing approaches and citations without explaining their limitations or how the present work builds on or differs from them.",
    "start": 227,
    "end": 481,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Vatswani et al., 2019)",
    "document": "Related Work\n\nBayesian treatments of uncertainty have been proposed for active learning in NLP. In (Vatswani et al., 2019), the authors evaluate Monte Carlo dropout for sequence labeling and report limited gains over simple confidence-based heuristics. Subsequent studies broadened the comparison to distillation-based selectors (Shelmanov et al., 2021) and reinforcement learning policies (Fang et al., 2017; Liu et al., 2018). Our study complements these findings by analyzing compute-aware budgets.",
    "reason": "Wrong citation style after the preposition 'In'; should be narrative 'In Vatswani et al. (2019)'.",
    "start": 99,
    "end": 122,
    "label": "Format"
  },
  {
    "span": "It is well known that BioBERT underperforms on long-span chemical entities.",
    "document": "Related Work\n\nBiomedical named entity recognition (BioNER) has benefited from domain-adaptive pretraining and span-level modeling. It is well known that BioBERT underperforms on long-span chemical entities. Subsequent work has explored character-level enhancements and span boundary refinement to alleviate this limitation. We build on span classification paradigms and introduce curriculum sampling to emphasize difficult boundary cases during training.",
    "reason": "Makes a niche, field-specific performance claim without citing supporting studies (definition b and e).",
    "start": 131,
    "end": 206,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from four U.S. states with rubric-aligned prompts.",
    "document": "Related Work\n\nAutomated essay scoring (AES) has progressed from handcrafted features and regression models to neural architectures that capture discourse and semantics (Attali and Burstein, 2006; Taghipour and Ng, 2016). Pre-trained language models such as BERT and RoBERTa have achieved strong results by fine-tuning on prompt-specific training sets (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from four U.S. states with rubric-aligned prompts. More recent work explores prompt-agnostic scoring and domain adaptation to unseen prompts using meta-learning and contrastive objectives (Uto et al., 2020; Ridley et al., 2021).\n\nWe extend this line by introducing a rubric-conditioned encoder that jointly models trait scores and holistic scores, improving robustness to prompt shifts.",
    "reason": "Describes a specific prior setup and dataset composition without citing the study or dataset (violates rule a and e-iii).",
    "start": 392,
    "end": 489,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior work showed that dark patterns significantly increase conversion rates.",
    "document": "Related Work\n\nDesign patterns that steer users toward particular choices, often termed dark patterns, have drawn increasing scrutiny from regulators and researchers. Prior work showed that dark patterns significantly increase conversion rates. Studies have documented prevalence in e-commerce and subscription flows, and proposed taxonomies covering obstruction, sneaking, and urgency cues (Mathur et al., 2019; Gray et al., 2018). Countermeasures range from disclosure designs to friction for irreversible actions (Bösch et al., 2016). Our study quantifies behavioral impact under realistic shopping scenarios and tests mitigations.",
    "reason": "Asserts findings from prior studies without citing any specific work to support the claim.",
    "start": 166,
    "end": 243,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Nguyen and Tran 2021)",
    "document": "Introduction\n\nVision Transformers (ViTs) have emerged as a powerful alternative to convolutional networks in image classification and beyond (Dosovitskiy et al., 2021; Touvron et al., 2021). However, their data-hungry nature and computational footprint motivate research on efficient training and transfer learning. Recent work examines token pruning and dynamic routing (Rao et al., 2021), as well as better positional encodings for dense prediction tasks (Chu et al., 2021; (Nguyen and Tran 2021). We study a complementary direction: regularization via stochastic token mixing.\n\nOur method improves sample efficiency while maintaining accuracy on standard benchmarks.",
    "reason": "Missing comma between authors and year in a parenthetical citation; APA-style requires \"(Nguyen and Tran, 2021)\".",
    "start": 476,
    "end": 498,
    "label": "Format"
  },
  {
    "span": "Fairness-aware recommenders seek to reduce exposure disparities or outcome inequities across user groups and providers (Yao and Huang, 2017; Burke, 2017; Mehrotra et al., 2018; Li et al., 2021; Diaz et al., 2020).",
    "document": "Related Work\n\nRecommender systems mediate access to information and opportunity. Beyond relevance, there is growing concern that ranking mechanisms unevenly allocate exposure, amplifying historical biases and disadvantaging certain users or item providers.\n\nMitigation strategies vary by fairness notion (e.g., demographic parity, equal opportunity) and by locus (pre-, in-, or post-processing), but they often trade off accuracy and stability and are difficult to audit in dynamic environments.\n\nFairness-aware recommenders seek to reduce exposure disparities or outcome inequities across user groups and providers (Yao and Huang, 2017; Burke, 2017; Mehrotra et al., 2018; Li et al., 2021; Diaz et al., 2020).\n\nWe introduce an exposure-constrained learning-to-rank objective with counterfactual propensity corrections and a simple auditor for online monitoring, enabling controllable fairness-utility trade-offs under position bias.",
    "reason": "The span lists fairness-aware approaches at a high level without relating them to the paper's objectives, clarifying unresolved gaps, or synthesizing common challenges; it lacks connection to the authors' argument.",
    "start": 497,
    "end": 710,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Shelmanov et al., 2021",
    "document": "Related Work\n\nRecent studies leverage imitation learning for query policy optimization (Fang et al., 2017; Liu et al., 2018; Vu et al., 2019). However, the transferability of learned policies remains limited according to (Shelmanov et al., 2021. Concurrently, budget-aware strategies use cost-sensitive acquisition functions to reduce annotation effort (Settles, 2009; Ash et al., 2020).",
    "reason": "Missing closing parenthesis in the citation.",
    "start": 221,
    "end": 244,
    "label": "Format"
  },
  {
    "span": "Transformer-based recommenders consistently outperform matrix factorization on long-tail items.",
    "document": "Related Work\n\nRecommender systems increasingly leverage sequence modeling to capture user intent and context. Transformer-based recommenders consistently outperform matrix factorization on long-tail items. Beyond accuracy, sequence models can better adapt to evolving preferences and cold-start scenarios through contextual signals.\n\nNevertheless, gains reported in the literature often depend on negative sampling strategies and evaluation protocols, highlighting the need for careful, apples-to-apples comparisons.",
    "reason": "Strong comparative performance claim about prior work is made without citations to empirical studies (rule b).",
    "start": 110,
    "end": 205,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Miller et. al., 2016)",
    "document": "Related Work\n\nActive learning reduces labeling cost by selecting informative instances (Settles, 2010). Classical strategies include uncertainty sampling and query-by-committee (Lewis and Gale, 1994; Seung et al., 1992), while modern methods integrate deep uncertainty estimates (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017).\n\nCore-set selection and representation learning approaches aim to choose diverse and representative batches (Sener and Savarese, 2018; Ash et al., 2020). Semi-supervised and self-supervised pretraining further improve sample efficiency (Tarvainen and Valpola, 2017; Chen et al., 2020).\n\nPool-based active learning for deep networks was revisited by (Miller et. al., 2016) and later extended to streaming and contextual bandit settings (Zhu and Bento, 2017; Zhan et al., 2022). Our work builds on these ideas by coupling selection with label quality estimation.\n\nWe evaluate across image classification tasks with human-in-the-loop simulations and report consistent gains over entropy-based baselines.",
    "reason": "Incorrect use of \"et. al.\" with a period after \"et\"; the correct form is \"et al.\" without a period after \"et\".",
    "start": 687,
    "end": 709,
    "label": "Format"
  },
  {
    "span": "Smith et al.",
    "document": "Introduction\n\nAbstractive summarization has advanced with transformers (Vaswani et al., 2017; Lewis et al., 2020). Early extractive methods relied on graph-based ranking (Mihalcea and Tarau, 2004), while neural approaches learn end-to-end (Rush et al., 2015; See et al., 2017). Following Smith et al., we adopt a pretraining-then-finetuning pipeline that leverages large scientific corpora. Other works incorporate discourse structure (Cohan et al., 2018) and citation-aware signals (Yasunaga et al., 2019). We focus on domain-adaptive pretraining for long documents (Beltagy et al., 2020) and efficient decoding (Fan et al., 2018), showing gains on PubMed and arXiv datasets.",
    "reason": "Narrative citation missing year; should be formatted as 'Smith et al. (YEAR)'.",
    "start": 288,
    "end": 300,
    "label": "Format"
  },
  {
    "span": "Jones et al. 1",
    "document": "Introduction\n\nConversational agents have progressed from rule-based systems (Weizenbaum, 1966) to neural dialog models (Vinyals and Le, 2015; Serban et al., 2016). Grounded conversation leverages external knowledge bases (Dinan et al., 2019) and retrieval augmentation (Lewis et al., 2020). Jones et al. 1 argue that safety requires fine-grained intent modeling, while others target toxicity control (Gehman et al., 2020) and personalization (Zhang et al., 2018). We study controllable response generation with sparse constraints (Keskar et al., 2019; Krause et al., 2021).",
    "reason": "Improper footnote marker used in place of a citation year; should include the year or be formatted as a proper footnote.",
    "start": 291,
    "end": 305,
    "label": "Format"
  },
  {
    "span": "Federated learning has considered privacy risks including membership inference (Shokri et al., 2017; Nasr et al., 2019), gradient inversion (Zhu et al., 2019; Geiping et al., 2020), and data poisoning (Bhagoji et al., 2019; Bagdasaryan et al., 2020). Mitigations include secure aggregation (Bonawitz et al., 2017), differential privacy (Abadi et al., 2016; McMahan et al., 2018), robust aggregation rules (Blanchard et al., 2017; Yin et al., 2018), and Byzantine-resilient optimization (Karimireddy et al., 2020). Personalization strategies span meta-learning (Fallah et al., 2020), multi-task learning (Smith et al., 2017), and interpolation-based fine-tuning (Arivazhagan et al., 2019).",
    "document": "Introduction\n\nFederated learning enables distributed model training over decentralized data while maintaining data locality. Heterogeneity in client data distributions and system constraints remains a primary challenge.\n\nFederated learning has considered privacy risks including membership inference (Shokri et al., 2017; Nasr et al., 2019), gradient inversion (Zhu et al., 2019; Geiping et al., 2020), and data poisoning (Bhagoji et al., 2019; Bagdasaryan et al., 2020). Mitigations include secure aggregation (Bonawitz et al., 2017), differential privacy (Abadi et al., 2016; McMahan et al., 2018), robust aggregation rules (Blanchard et al., 2017; Yin et al., 2018), and Byzantine-resilient optimization (Karimireddy et al., 2020). Personalization strategies span meta-learning (Fallah et al., 2020), multi-task learning (Smith et al., 2017), and interpolation-based fine-tuning (Arivazhagan et al., 2019).\n\nWe propose a calibration-driven personalization framework that targets stability under distribution shift by aligning client-specific uncertainty with global model priors.",
    "reason": "The span aggregates literature across risks, defenses, and personalization without articulating the paper’s stance or how these relate to the proposed approach, matching (a) and (c).",
    "start": 221,
    "end": 909,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Previous studies consistently report that spatial attention outperforms static adjacency in urban grids.",
    "document": "Related Work\n\nTraffic forecasting combines temporal modeling with spatial dependencies across sensor networks. Graph-based approaches, including DCRNN, STGCN, and Graph WaveNet, have achieved state-of-the-art accuracy by learning dynamic correlations (Li et al., 2018; Yu et al., 2018; Wu et al., 2019). Attention mechanisms further adapt connectivity over time to capture non-stationary traffic patterns (Zheng et al., 2020).\n\nPrevious studies consistently report that spatial attention outperforms static adjacency in urban grids. However, the gains may conflate architecture depth with adaptivity, and few works isolate these factors.\n\nOur method disentangles topology learning from temporal modeling via a modular design and controlled ablations.",
    "reason": "Generalizes about prior studies and their findings without providing citations to those studies (rule b and d).",
    "start": 428,
    "end": 532,
    "label": "Unsupported_claim"
  },
  {
    "span": "more than 30% of crowdworkers annotate from mobile devices.",
    "document": "Related Work\n\nThe quality of crowdsourced annotations depends on worker expertise, task design, and device constraints. Prior efforts have explored interface optimizations, dynamic pricing, and quality control mechanisms to mitigate noise and bias in labels used for supervised learning. However, device heterogeneity introduces additional variability in annotation speed and accuracy.\n\nRecent surveys suggest that task context and ergonomics play critical roles in annotation fidelity, particularly for fine-grained labeling tasks in vision and NLP. In practical deployments, more than 30% of crowdworkers annotate from mobile devices. This has implications for interface layout, instruction brevity, and input modalities such as touch versus keyboard.\n\nOur study quantifies the effect of mobile-oriented design on label quality across three tasks, proposing lightweight UI adaptations that improve both speed and agreement without increasing abandonment rates.\n",
    "reason": "Presents a concrete statistic about worker devices without any citation or evidence.",
    "start": 577,
    "end": 636,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Garcia, 2016,)",
    "document": "Related Work\n\nData-centric AI. A data-centric view (Garcia, 2016,) emphasizes iterative dataset refinement over model complexity. This perspective aligns with efforts to curate balanced and diverse corpora (Bender et al., 2021) and to document dataset characteristics for reproducibility (Gebru et al., 2021).\n\nOur work extends these ideas by introducing validation diagnostics that quantify coverage and label noise at scale.",
    "reason": "Extraneous trailing comma before the closing parenthesis in the citation.",
    "start": 51,
    "end": 66,
    "label": "Format"
  },
  {
    "span": "Template-based prompting achieved strong zero-shot transfer on sentiment and NLI (Schick and Schütze, 2021; Gao et al., 2021). Adapter-based finetuning reduces parameter count with competitive accuracy (Houlsby et al., 2019). Multilingual alignment can be improved with contrastive objectives (Chi et al., 2021).",
    "document": "Related Work\n\nPrompting and parameter-efficient finetuning. Prompt-based methods reformulate downstream tasks as masked or autoregressive completions to better exploit pre-trained language models. Early work used manual templates and verbalizers, later extended to automatic prompt search and calibration to reduce label bias. In parallel, parameter-efficient finetuning aims to adapt large models by training a small number of additional parameters, enabling multi-task and multi-domain deployment under tight memory budgets.\n\nCross-lingual transfer. Zero-shot cross-lingual transfer leverages multilingual pre-training and shared subword vocabularies to project representations across languages. Techniques include translation-based data augmentation, alignment losses, and adapters specialized for language families. These methods are crucial for low-resource settings where labeled supervision is scarce or costly.\n\nTemplate-based prompting achieved strong zero-shot transfer on sentiment and NLI (Schick and Schütze, 2021; Gao et al., 2021). Adapter-based finetuning reduces parameter count with competitive accuracy (Houlsby et al., 2019). Multilingual alignment can be improved with contrastive objectives (Chi et al., 2021).\n\nOur approach combines fixed discrete prompts with language-specific lightweight modules trained on source-language supervision. We focus on structure prediction tasks and control for translation artifacts by evaluating exclusively on native corpora. We further compare contrastive alignment with task-conditional adapters to study their interaction under prompt-based supervision.",
    "reason": "The three sentences enumerate disparate lines of work (prompting, adapters, multilingual alignment) without transitions or explicit relations between them, making the connection between cited works abrupt and unclear (a, b).",
    "start": 920,
    "end": 1232,
    "label": "Coherence"
  },
  {
    "span": "Differential privacy in FL has been studied in several works. Zhu et al. (2019) show gradient leakage attacks that recover training data. Bonawitz et al. (2019) propose secure aggregation at scale. Compression techniques also reduce uplink cost (Konečný et al., 2016).",
    "document": "Related Work\n\nFederated Learning Foundations\n\nFederated learning (FL) enables decentralized training without centralizing raw data (McMahan et al., 2017). Subsequent work examined convergence under non-iid data and partial participation (Li et al., 2020), as well as personalization strategies to adapt global models to client heterogeneity (Smith et al., 2017).\n\nPrivacy and Security in FL\n\nDifferential privacy in FL has been studied in several works. Zhu et al. (2019) show gradient leakage attacks that recover training data. Bonawitz et al. (2019) propose secure aggregation at scale. Compression techniques also reduce uplink cost (Konečný et al., 2016).\n\nCommunication Efficiency\n\nA parallel line of work reduces communication by sparsifying or quantizing updates (Aji and Heafield, 2017; Alistarh et al., 2017) and by client selection strategies that balance utility and bandwidth (Nishio and Yonetani, 2019). Our work focuses on the interaction between partial participation and robust aggregation under system constraints.",
    "reason": "The sentences list disparate works (privacy, attacks, secure aggregation, and finally compression) without transitions or explicit relationships, making the connection between cited works abrupt and unclear.",
    "start": 392,
    "end": 660,
    "label": "Coherence"
  },
  {
    "span": "(Chen et al., 2019 Li et al., 2020)",
    "document": "Related Work\n\nTime-series forecasting employs statistical models such as ARIMA and ETS (Hyndman and Athanasopoulos, 2018) and neural architectures like RNNs, TCNs, and Transformers (Bai et al., 2018; Lim et al., 2021). Decomposition-based pipelines separate trend and seasonality before learning residual dynamics (Zhang et al., 2017), while probabilistic models capture predictive uncertainty (Salinas et al., 2019).\n\nRecent advances leverage exogenous signals and graph structures to improve multivariate forecasting (Wu et al., 2020; Sen et al., 2019). Representation learning with contrastive objectives encourages invariant features under time warping and scaling (Franceschi et al., 2019). Cross-domain pretraining reduces data requirements, especially in sparse regimes (Chen et al., 2019 Li et al., 2020).\n\nWe propose a decomposition-aware Transformer with cross-series attention, achieving state-of-the-art accuracy and calibrated uncertainty on electricity, traffic, and retail datasets.",
    "reason": "Multiple citations are missing a separator; should use a semicolon between them: '(Chen et al., 2019; Li et al., 2020)'.",
    "start": 777,
    "end": 812,
    "label": "Format"
  },
  {
    "span": "(Hewitt and Manning, 2019]",
    "document": "Related Work\n\nProbing studies examine the linguistic knowledge stored in contextual encoders (Adi et al., 2017; Tenney et al., 2019). Structural probes aim to recover syntax from embeddings (Hewitt and Manning, 2019], while others target morphology and semantics (Peters et al., 2018; Rogers et al., 2020). Despite their insights, probes can conflate representation content with classifier capacity, motivating control tasks and causal analyses (Hewitt et al., 2021). Our work complements these approaches by evaluating how task supervision reshapes probing outcomes.",
    "reason": "Mismatched bracket in the citation; the closing bracket should be a parenthesis: '(Hewitt and Manning, 2019)'.",
    "start": 190,
    "end": 216,
    "label": "Format"
  },
  {
    "span": "Large language models pre-trained on code have been applied to program synthesis with prompting (Chen et al., 2021; Austin et al., 2021), few-shot learning (Brown et al., 2020), chain-of-thought and self-consistency (Wei et al., 2022; Wang et al., 2022), execution-guided decoding (Chen et al., 2018; Liang et al., 2022), and self-debugging or repair (Le et al., 2022; Chen et al., 2023).",
    "document": "Related Work\n\nProgram synthesis benchmarks test the ability of models to produce correct programs consistent with specifications such as input-output examples or natural language. While large language models (LLMs) achieve strong pass@k on competitive datasets, robustness to ambiguous specs and constraint satisfaction remains uneven. We explore constraint-aware planning that integrates symbolic solvers during decoding.\n\nLarge language models pre-trained on code have been applied to program synthesis with prompting (Chen et al., 2021; Austin et al., 2021), few-shot learning (Brown et al., 2020), chain-of-thought and self-consistency (Wei et al., 2022; Wang et al., 2022), execution-guided decoding (Chen et al., 2018; Liang et al., 2022), and self-debugging or repair (Le et al., 2022; Chen et al., 2023).\n\nOur approach coordinates a verifier with a symbolic constraint solver to prune inconsistent partial programs and guide search, improving both correctness and resource use.",
    "reason": "The span lists LLM-based strategies without connecting them to the paper’s constraint-aware planning or highlighting any unresolved gap, violating (a) and (c).",
    "start": 424,
    "end": 812,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Most previous works pretrain on ImageNet-21k before fine-tuning on aerial imagery.",
    "document": "Related Work\n\nRemote sensing scene understanding has benefited substantially from advances in vision transformers, which can better capture long-range spatial dependencies than convolutional baselines. However, differences in viewpoint, object scale, and spectral bands complicate direct transfer from natural images. Most previous works pretrain on ImageNet-21k before fine-tuning on aerial imagery. Alternative strategies explore self-supervised objectives on unlabeled satellite tiles, yet the optimal balance between generic and domain-specific pretraining remains unresolved. Our approach investigates hybrid curricula that mix multispectral contrastive learning with lightweight supervised warm-up.",
    "reason": "Generalizes about prior work practices without providing citations to those works.",
    "start": 318,
    "end": 400,
    "label": "Unsupported_claim"
  },
  {
    "span": "Wang et. al. (2018)",
    "document": "Related Work\n\nMultimodal Representation Learning\n\nIntegrating vision and language has been explored through joint embeddings and attention mechanisms (Karpathy and Fei-Fei, 2015; Lu et al., 2016). Early fusion concatenates features for unified processing (Ngiam et al., 2011), while late fusion aggregates modality-specific predictions (Baltrusaitis et al., 2019). Wang et. al. (2018) propose co-attention to couple image regions and words, and Sun et al. (2019) extend these ideas with transformer architectures for cross-modal alignment. Contrastive pretraining further improves retrieval and captioning (Radford et al., 2021).\n",
    "reason": "Misspelled Latin abbreviation: should be 'et al.' (no period after 'et' and a period after 'al').",
    "start": 365,
    "end": 384,
    "label": "Format"
  },
  {
    "span": "(Lee and Kim, 2020; Chen et al., 2021.",
    "document": "Introduction\n\nFederated learning (FL) enables on-device training without centralizing raw data, reducing privacy risks and communication costs (Konečný et al., 2016; McMahan et al., 2017). However, client heterogeneity induces representation drift and degrades global model performance (Zhao et al., 2018; Li et al., 2020).\n\nPersonalized FL addresses this by tailoring models to client-specific distributions via meta-learning, multi-task optimization, and mixture-of-experts strategies (Fallah et al., 2020; Dinh et al., 2020). Recent work explores representation disentanglement and adapter-based tuning to isolate shared from private components (Lee and Kim, 2020; Chen et al., 2021.\n\nWe propose a bi-level objective that regularizes global invariants while allowing client adapters to capture idiosyncratic features, achieving better Pareto trade-offs between global accuracy and personalization.",
    "reason": "Missing closing parenthesis in a parenthetical citation list.",
    "start": 648,
    "end": 686,
    "label": "Format"
  },
  {
    "span": "Shelmanov et al.",
    "document": "Related Work\n\nActive learning for natural language processing has progressed from classic uncertainty sampling to strategies that balance uncertainty with representativeness. Early approaches in sequence tagging used uncertainty-based heuristics with efficient scoring to limit training overhead (Shen et al., 2017). Shelmanov et al. propose leveraging distilled teacher–student models to decouple acquisition from training, while Siddhant and Lipton (2018) investigate Bayesian methods with Monte Carlo dropout for more calibrated uncertainty. Core-set selection reframes acquisition as coverage over learned embeddings and has shown benefits in vision and NLP (Sener and Savarese, 2018). Recent studies also use pretrained Transformers to improve both query scoring and downstream generalization (Ein-Dor et al., 2020; Vaswani et al., 2017). Despite these advances, the impact of acquisition bias on successor training remains underexplored, motivating our analysis of model mismatch between acquisition and final training.\n",
    "reason": "Narrative citation missing year; should be formatted as “Shelmanov et al. (YEAR)”.",
    "start": 317,
    "end": 333,
    "label": "Format"
  },
  {
    "span": "Zhou et al. (2021) propose informer with probabilistic sparsity. Oreshkin et al. (2019) decompose trends and seasonality in a hierarchical fashion. Sen et al. (2019) benchmark forecasting on the M4 dataset. Lim et al. (2021) introduce temporal fusion transformers for interpretable forecasting.",
    "document": "Related Work\n\nTime-series forecasting benefits from architectures that capture long-range dependencies, handle multiple covariates, and remain robust under distribution shift. Transformers and hybrid models have recently shown strong performance on diverse benchmarks.\n\nZhou et al. (2021) propose informer with probabilistic sparsity. Oreshkin et al. (2019) decompose trends and seasonality in a hierarchical fashion. Sen et al. (2019) benchmark forecasting on the M4 dataset. Lim et al. (2021) introduce temporal fusion transformers for interpretable forecasting.\n\nWe focus on multi-horizon forecasting under memory constraints and propose a lightweight attention kernel with decomposed trend-seasonality priors and uncertainty calibration.\n",
    "reason": "The span lists methods and a benchmark with no transitions or explanation of how they relate, creating an abrupt sequence of sentences with implied rather than explicit connections.",
    "start": 270,
    "end": 564,
    "label": "Coherence"
  },
  {
    "span": "Exploration strategies span count-based bonuses, intrinsic curiosity, posterior sampling, and optimism under uncertainty (Bellemare et al., 2016; Pathak et al., 2017; Osband et al., 2016; Agrawal and Goyal, 2012).",
    "document": "Related Work\n\nEfficient exploration remains a central challenge in reinforcement learning, particularly in sparse-reward environments. Numerous algorithmic ideas have been proposed to reduce sample complexity by guiding agents toward informative experiences.\n\nExploration strategies span count-based bonuses, intrinsic curiosity, posterior sampling, and optimism under uncertainty (Bellemare et al., 2016; Pathak et al., 2017; Osband et al., 2016; Agrawal and Goyal, 2012).\n\nRecent works combine model-based rollouts with uncertainty-aware planning, while others use representation learning to shape exploration in high-dimensional observation spaces. Safety-aware exploration introduces constraints but often at the cost of slower learning.\n\nWe propose a representation-driven exploration scheme that calibrates novelty with dynamics consistency, avoiding detours into epistemically misleading states.",
    "reason": "The span lists categories of exploration methods with citations but does not explain their limitations, relevance, or relation to the proposed approach, fulfilling (a) and (c).",
    "start": 260,
    "end": 473,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Nakamura et al. 2020)",
    "document": "Introduction\n\nProgram synthesis with neural-guided search combines symbolic constraints with learned heuristics (Balog et al., 2017; Ellis et al., 2019). In low-resource settings, semi-supervised objectives leverage partial specifications to expand training data (Alet et al., 2020). Prior work also highlights the importance of curriculum design for constraint hardness (Devlin et al., 2017). We investigate scaling laws for constraint-guided decoders building upon (Nakamura et al. 2020) and recent modular architectures (Cai and Shin, 2021).",
    "reason": "Missing comma between authors and year in a parenthetical citation; should be '(Nakamura et al., 2020)'.",
    "start": 467,
    "end": 489,
    "label": "Format"
  },
  {
    "span": "the recent CodeXGLUE competition established sequence-level metrics for program repair",
    "document": "Introduction\n\nProgram repair models seek to generate minimal edits that correct compilation or semantic errors. Evaluations have traditionally relied on test-suite pass rates and token-level accuracy, which may not reflect patch plausibility. In response to these limitations, the recent CodeXGLUE competition established sequence-level metrics for program repair, encouraging models that optimize for behavioral fidelity rather than surface similarity. Building on this shift, we propose a repair-aware decoder that integrates constraint checking into beam hypotheses.",
    "reason": "References a specific competition and its contributions without citation (rule a; first mention of a shared task/competition must be cited).",
    "start": 277,
    "end": 363,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Garcia and Chen, 2021)",
    "document": "Introduction\n\nNeural dialogue planning aims to balance controllability with naturalness. Prior work grounds planning in intents and slots (Wen et al., 2017; Budzianowski and Vulić, 2019) and in latent actions (Yarats and Lewis, 2018). In (Garcia and Chen, 2021) we examine the effect of constrained decoding on factual consistency, concluding that coverage penalties are necessary. Building on this line, we propose a training objective that directly penalizes hallucinations while preserving fluency.",
    "reason": "Wrong citation style: a preposition precedes a parenthetical citation. Should be narrative “In Garcia and Chen (2021), we …” or rephrased to avoid “In (… )”.",
    "start": 235,
    "end": 261,
    "label": "Format"
  },
  {
    "span": "According to industry reports, 70% of IoT devices ship with default credentials.",
    "document": "Introduction\n\nThe proliferation of Internet-of-Things (IoT) devices has expanded the attack surface across consumer and industrial networks. Weak authentication, limited update mechanisms, and heterogeneous stacks complicate security hardening.\n\nAccording to industry reports, 70% of IoT devices ship with default credentials. Motivated by the prevalence of credential reuse, we propose a proactive scanning and remediation framework that identifies default accounts and enforces credential rotation at scale.\n\nWe evaluate detection coverage on a mixed lab environment and measure remediation latency under varying network policies.",
    "reason": "This is a statistical claim attributed to generic \"industry reports\" without a concrete citation or source (guideline b).",
    "start": 246,
    "end": 326,
    "label": "Unsupported_claim"
  },
  {
    "span": "Question answering over knowledge graphs has been addressed by semantic parsing approaches, embedding-based ranking, and neural symbolic methods (Berant et al., 2013; Bordes et al., 2014; Das et al., 2021). Weak supervision via question-answer pairs and distant supervision are common training signals (Yih et al., 2015; Liang et al., 2017). Standard datasets include WebQuestionsSP, SimpleQuestions, and MetaQA (Yih et al., 2016; Bordes et al., 2015; Zhang et al., 2018).",
    "document": "Related Work\n\nKnowledge-graph question answering (KGQA) requires mapping natural language questions to structured queries or reasoning paths over entities and relations. The field includes symbolic, neural, and hybrid paradigms that vary in expressivity and data requirements.\n\nQuestion answering over knowledge graphs has been addressed by semantic parsing approaches, embedding-based ranking, and neural symbolic methods (Berant et al., 2013; Bordes et al., 2014; Das et al., 2021). Weak supervision via question-answer pairs and distant supervision are common training signals (Yih et al., 2015; Liang et al., 2017). Standard datasets include WebQuestionsSP, SimpleQuestions, and MetaQA (Yih et al., 2016; Bordes et al., 2015; Zhang et al., 2018).\n\nRecent work explores compositional generalization, multi-hop reasoning, and incomplete KGs using auxiliary text (Sun et al., 2019; Xiong et al., 2020). Evaluation practices remain heterogeneous across datasets and metrics.\n\nOur approach targets robustness to entity surface-form variation.",
    "reason": "The span lists methods, supervision strategies, and datasets without tying them to a specific gap or the authors' focus, demonstrating a lack of synthesis (criteria a and c).",
    "start": 278,
    "end": 750,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Garcia 2017)",
    "document": "Related Work\n\nActive learning strategies reduce annotation costs by prioritizing uncertain or informative samples (Settles, 2009). Previous surveys (Garcia 2017) categorize selection criteria into uncertainty-, diversity-, and representativeness-based families, highlighting trade-offs between coverage and redundancy. Subsequent empirical studies emphasize the role of calibration in improving query quality (Kumar and Sarawagi, 2019) and the benefits of batch-mode selection that accounts for mutual information (Ash et al., 2020). In natural language processing, pool-based methods have been complemented by task-aware pretraining that enhances sample efficiency (Lewis et al., 2020; Zhang et al., 2021).\n\nWe extend this line by proposing an adaptive curriculum that evolves the acquisition function with model competence, balancing exploration and exploitation during training.",
    "reason": "Missing comma in parenthetical citation. It should be formatted as \"(Garcia, 2017)\" rather than \"(Garcia 2017)\".",
    "start": 148,
    "end": 161,
    "label": "Format"
  },
  {
    "span": "In (Huang and Liu, 2018)",
    "document": "Related Work\n\nCross-lingual information retrieval has leveraged translation signals to bridge lexical gaps between source and target languages. In (Huang and Liu, 2018) a pivot-language method is proposed that first maps documents into a shared semantic space before retrieval. Subsequent work reduces reliance on dictionaries by learning multilingual encoders directly from parallel corpora (Kim and Park, 2019). Contextualized models further improve alignment by sharing subword vocabularies across languages (Santos et al., 2020). Recent benchmarks highlight the importance of domain adaptation and low-resource settings (Ibrahim et al., 2021).",
    "reason": "Wrong citation style; the preposition should be followed by a narrative citation, e.g., \"In Huang and Liu (2018)\" rather than a parenthetical.",
    "start": 144,
    "end": 168,
    "label": "Format"
  },
  {
    "span": "To the best of our knowledge, no previous study has evaluated cross-lingual summarization in low-resource Bantu languages.",
    "document": "Introduction\n\nCross-lingual summarization aims to generate concise summaries of source documents in a different target language. While substantial attention has been devoted to high-resource language pairs, research on low-resource settings remains limited. Data sparsity, orthographic variation, and lack of standardized benchmarks complicate progress in these communities. To the best of our knowledge, no previous study has evaluated cross-lingual summarization in low-resource Bantu languages. In this work, we introduce a corpus and baseline models that target this gap, emphasizing practical constraints such as limited supervision and domain shift across news and conversational text.",
    "reason": "This is a novelty claim about the absence of prior studies that requires evidence or citations to surveys or systematic searches.",
    "start": 375,
    "end": 497,
    "label": "Unsupported_claim"
  },
  {
    "span": "Previous instruction-tuning efforts rely primarily on English-only sources.",
    "document": "Introduction\n\nInstruction tuning has emerged as an effective strategy for aligning large language models with user intent by fine-tuning on collections of task instructions and demonstrations (Ouyang et al., 2022; Wei et al., 2022). Such models exhibit improved generalization to unseen tasks and more helpful behavior in interactive settings. Multilingual transfer remains an open challenge due to lexical, morphological, and cultural variation across languages (Conneau et al., 2020).\n\nPrevious instruction-tuning efforts rely primarily on English-only sources. This limits cross-lingual alignment and can produce inconsistent behavior in multilingual applications. We propose a multilingual instruction-tuning pipeline that leverages parallel and comparable corpora, with language-adaptive adapters to reduce interference across languages.",
    "reason": "This sentence asserts a broad characterization of prior work ('previous instruction-tuning efforts') without providing citations; it should be supported with references to representative studies.",
    "start": 488,
    "end": 563,
    "label": "Unsupported_claim"
  },
  {
    "span": "Data augmentation for low-resource machine translation includes back-translation, self-training, and multilingual transfer (Sennrich et al., 2016; He et al., 2020; Conneau et al., 2020). Variants leverage noise-robust objectives, sampling strategies, or iterative refinement to improve synthetic data quality (Edunov et al., 2018; Imamura et al., 2018; Hoang et al., 2018).",
    "document": "Introduction\n\nLow-resource machine translation (MT) suffers from limited parallel data and domain mismatch. Augmenting supervision with synthetic examples has become a dominant strategy to close the performance gap with high-resource pairs.\n\nData augmentation for low-resource machine translation includes back-translation, self-training, and multilingual transfer (Sennrich et al., 2016; He et al., 2020; Conneau et al., 2020). Variants leverage noise-robust objectives, sampling strategies, or iterative refinement to improve synthetic data quality (Edunov et al., 2018; Imamura et al., 2018; Hoang et al., 2018).\n\nWe introduce contrastive dual-noise back-translation that explicitly balances lexical diversity and semantic fidelity via paired noising policies. Our approach yields consistent gains in BLEU and COMET on four low-resource directions and improves robustness to domain shift.",
    "reason": "The span summarizes augmentation techniques and improvements without articulating the specific limitation the paper tackles or how prior methods relate to the proposed approach, hence lacking synthesis (a, b, c).",
    "start": 242,
    "end": 615,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Transformers have been applied to classification (Dosovitskiy et al., 2020), segmentation (Chen et al., 2021; Hatamizadeh et al., 2022), and detection (Carion et al., 2020) in medical images.",
    "document": "Related Work\n\nMedical image analysis has benefited from advances in deep learning, but domain-specific constraints like limited labels, volumetric data, and clinical reliability requirements impose unique modeling and evaluation challenges. Recent architectures have sought to leverage global context while preserving fine anatomical detail.\n\nTransformers have been applied to classification (Dosovitskiy et al., 2020), segmentation (Chen et al., 2021; Hatamizadeh et al., 2022), and detection (Carion et al., 2020) in medical images. Hybrid CNN-Transformer designs aim to combine inductive biases of convolutions with the flexibility of self-attention. Self-supervised pretraining has also shown promise for label-efficient learning on radiology and pathology datasets.\n\nThis work studies scale-aware tokenization for volumetric scans, introducing a multi-plane aggregation scheme that preserves inter-slice continuity. We benchmark on multi-organ CT segmentation and report robustness to resolution shifts and scanner variability.\n\nWe discuss deployment factors including memory footprint, throughput, and calibration for clinical decision support scenarios.",
    "reason": "The span summarizes prior applications of Transformers but does not articulate how those works connect to the paper's problem setting or reveal a gap, fulfilling (a) and (c).",
    "start": 343,
    "end": 534,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Classical motion planning relies on sampling-based planners such as PRM and RRT* (Kavraki et al., 1996; Karaman and Frazzoli, 2011), while recent work integrates reinforcement learning to handle uncertainty (Kahn et al., 2018; Long et al., 2018). Our method combines planning with learned value functions.",
    "document": "Related Work\n\nRobotic navigation demands both geometric feasibility and robustness to sensing and actuation noise. Traditional planning focuses on computing collision-free paths, whereas learning-based methods emphasize adaptability under uncertainty.\n\nClassical motion planning relies on sampling-based planners such as PRM and RRT* (Kavraki et al., 1996; Karaman and Frazzoli, 2011), while recent work integrates reinforcement learning to handle uncertainty (Kahn et al., 2018; Long et al., 2018). Our method combines planning with learned value functions.\n\nHybrid approaches explore model predictive control with learned dynamics models, but remain sensitive to compounding errors and model-bias in unstructured environments.",
    "reason": "Moves from a generic summary to stating the authors’ approach without specifying the shortcoming in prior work that the approach addresses (definition b).",
    "start": 253,
    "end": 558,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There are many recent works leveraging contrastive learning for code-search and summarization.",
    "document": "Related Work\n\nNeural code intelligence leverages large corpora of code and paired natural language to learn joint representations. Tasks include method naming, docstring generation, and code search (Allamanis et al., 2018; Ahmad et al., 2020). Benchmarks such as CodeSearchNet have standardized evaluation protocols across languages (Husain et al., 2019). Pretrained models tailored to code syntax and data flow have further improved results (Guo et al., 2021).\n\nThere are many recent works leveraging contrastive learning for code-search and summarization. However, negative sampling strategies and cross-language alignment remain underexplored.\n\nWe introduce a curriculum for hard-negative mining that improves retrieval and downstream summarization quality.",
    "reason": "Uses the phrase 'recent works' and suggests a trend without citing any specific papers (rule d).",
    "start": 463,
    "end": 557,
    "label": "Unsupported_claim"
  },
  {
    "span": "It is well known that negative sampling almost always improves recommendation accuracy.",
    "document": "Related Work\n\nImplicit-feedback recommendation models typically optimize ranking losses that compare observed interactions to sampled non-interactions. Negative sampling strategies vary from uniform draws to popularity- or model-informed choices, with trade-offs in bias and variance that affect both training stability and final ranking quality.\n\nIt is well known that negative sampling almost always improves recommendation accuracy. Despite its ubiquity, the choice of sampling distribution and schedule can markedly influence results, and inconsistent reporting obscures true gains. We study how sampler difficulty, refresh rates, and batch coupling impact convergence and generalization across matrix factorization and neural recommenders under standardized protocols.",
    "reason": "Uses a field-wide generalization ('well known' and 'almost always') without references to empirical studies or surveys.",
    "start": 348,
    "end": 435,
    "label": "Unsupported_claim"
  },
  {
    "span": "Mixture-of-Experts (MoE) architectures scale model capacity by routing tokens to a subset of experts. Top-k routing variants balance load with auxiliary losses, while dense-to-sparse distillation preserves quality under lower compute. Recent work explores hierarchical experts, token-dropping for efficiency, and expert specialization through task or domain conditioning.",
    "document": "Introduction\nScaling model capacity efficiently is crucial for pushing performance without prohibitive compute. Sparse architectures like Mixture-of-Experts offer a path to decouple parameter count from per-token FLOPs.\n\nRelated Work\nEarly sparse models combined gating with expert subnetworks to activate only a fraction of parameters per input.\nMixture-of-Experts (MoE) architectures scale model capacity by routing tokens to a subset of experts. Top-k routing variants balance load with auxiliary losses, while dense-to-sparse distillation preserves quality under lower compute. Recent work explores hierarchical experts, token-dropping for efficiency, and expert specialization through task or domain conditioning.\nStability work addresses expert collapse and routing churn via regularization, noise, and improved gate architectures, while deployment studies examine memory layout and overlapping communication.\n\nOur Contribution\nWe introduce an inference-time routing strategy that reduces tail latency under strict memory budgets.",
    "reason": "The span summarizes MoE techniques without stating how they motivate the new routing strategy or what gap persists, satisfying the lack of synthesis criteria (a) and (c).",
    "start": 347,
    "end": 718,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Vatswani et al., 2019)",
    "document": "Related Work\n\nDense passage retrieval has emerged as a strong alternative to sparse lexical methods by learning dual-encoder representations that can be efficiently indexed (Karpukhin et al., 2020; Xiong et al., 2021). In (Vatswani et al., 2019) a contrastive training objective is proposed to better separate hard negatives from positives, which significantly improves top-k recall under limited annotation budgets. Later, hybrid systems combine dense and sparse signals to capture both term-level and semantic similarity (Formal et al., 2021; Luan et al., 2021).\n\nOn the reader side, generative models finetuned for open-domain question answering leverage retrieved passages to produce fluent answers (Lewis et al., 2020; Izacard and Grave, 2021). Our work focuses on improving retriever calibration under domain shift by aligning index-time and query-time distributions.",
    "reason": "Wrong citation style; the preposition 'In' should not precede a parenthetical citation and should instead be written as narrative, e.g., 'In Vatswani et al. (2019)' or simply '(Vatswani et al., 2019)'.",
    "start": 219,
    "end": 245,
    "label": "Format"
  },
  {
    "span": " (Kumar et al., 2018])",
    "document": "Related Work\n\nDomain adaptation for text classification typically minimizes distribution shift via importance weighting or feature alignment (Ben-David et al., 2010; Ganin and Lempitsky, 2015). In sentiment analysis, adversarial approaches learn domain-invariant representations while preserving label information (Li et al., 2018; Tzeng et al., 2017). Meta-learning variants further adapt to novel domains with limited labeled examples (Vu et al., 2020). Several studies combine pivot features with contextualized embeddings to improve transfer (Gururangan et al., 2020) (Kumar et al., 2018]).",
    "reason": "Mismatched closing bracket in a parenthetical citation; the citation closes with ']' instead of ')'.",
    "start": 571,
    "end": 593,
    "label": "Format"
  },
  {
    "span": "The Cora and Citeseer splits with 20 labels per class are standard",
    "document": "Related Work\n\nSemi-supervised node classification commonly benchmarks on citation graphs. The Cora and Citeseer splits with 20 labels per class are standard, and Pubmed is frequently included to test scalability. GCN (Kipf and Welling, 2017), GraphSAGE (Hamilton et al., 2017), and GAT (Velickovic et al., 2018) popularized message passing with local neighborhood aggregation. More recent methods introduce heterophily-aware propagation and spectral filters (Zhu et al., 2020; Chen et al., 2020). We follow the transductive protocol and report mean accuracy over 10 random seeds.",
    "reason": "Asserts a specific experimental setup as standard practice without citing sources that established it.",
    "start": 90,
    "end": 156,
    "label": "Unsupported_claim"
  },
  {
    "span": "CTC models assume conditional independence and align inputs and outputs with a collapse function (Graves et al., 2006). Attention-based encoder–decoder models learn soft alignments between acoustic frames and tokens (Chan et al., 2016). RNN-T combines alignment and sequence modeling in a streaming architecture (Graves, 2012). External language model fusion improves decoding by incorporating text-only data (Toshniwal et al., 2018; Gulati et al., 2020).",
    "document": "Related Work\n\nEnd-to-End Automatic Speech Recognition\nEnd-to-end ASR replaces HMM-based pipelines with neural architectures that jointly model acoustic and language information (Graves et al., 2013; Amodei et al., 2016). Different formulations trade off streaming capability, alignment modeling, and sample efficiency.\n\nModels and Decoding Enhancements\nCTC models assume conditional independence and align inputs and outputs with a collapse function (Graves et al., 2006). Attention-based encoder–decoder models learn soft alignments between acoustic frames and tokens (Chan et al., 2016). RNN-T combines alignment and sequence modeling in a streaming architecture (Graves, 2012). External language model fusion improves decoding by incorporating text-only data (Toshniwal et al., 2018; Gulati et al., 2020). Self-supervised pretraining has further improved robustness in low-resource regimes (Baevski et al., 2020; Hsu et al., 2021).\n\nBenchmarks and Settings\nCommon benchmarks include LibriSpeech and TED-LIUM, with far-field and noisy speech evaluations highlighting generalization limits (Panayotov et al., 2015; Rousseau et al., 2014). We focus on streaming constraints with limited compute.",
    "reason": "The span lists multiple ASR model families and a decoding technique as disconnected sentences without transitions, leaving the relationships and comparative context between them unclear.",
    "start": 353,
    "end": 808,
    "label": "Coherence"
  },
  {
    "span": "Several recent competitions have benchmarked federated learning algorithms on medical imaging.",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative training across institutions without centralized data pooling, offering a compelling paradigm for sensitive domains such as healthcare. Medical imaging poses unique challenges, including extreme class imbalance, scanner heterogeneity, and privacy-preserving evaluation. Several recent competitions have benchmarked federated learning algorithms on medical imaging. At the same time, existing clinical datasets vary widely in labeling protocols and de-identification practices, complicating reproducible comparisons. We propose a unified FL evaluation suite with standardized preprocessing, client sampling, and privacy accounting tailored to radiology tasks.\n",
    "reason": "Mentions 'recent competitions' without citing any specific competition or proceedings (violates rule d and a).",
    "start": 330,
    "end": 424,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith, 2019",
    "document": "Introduction\n\nPersonalized recommender systems balance short-term click-through with long-term user satisfaction (Zhang and Chen, 2020; Jannach and Jugovac, 2019). Smith, 2019 demonstrate that counterfactual estimators can reduce bias when logging propensities are known, inspiring off-policy learning objectives for ranking. Subsequent work integrates uncertainty estimates to prevent popularity collapse (Liang et al., 2021; Saito, 2020). Our approach complements these efforts by calibrating exploration to user-level risk profiles.",
    "reason": "Narrative citation uses comma style; should be “Smith (2019)” for narrative or “(Smith, 2019)” for parenthetical.",
    "start": 164,
    "end": 175,
    "label": "Format"
  },
  {
    "span": "Graph neural networks for molecular property prediction include message passing architectures and variants that modify aggregation or attention (Duvenaud et al., 2015; Kearnes et al., 2016; Gilmer et al., 2017; Schlichtkrull et al., 2018; Xu et al., 2019; Hu et al., 2020; Wang et al., 2021).",
    "document": "Related Work\n\nMolecular representation learning has progressed rapidly with the availability of large-scale datasets and improvements in differentiable graph encoders. Early approaches summarized molecules using fixed fingerprints or substructure counts, which limited expressivity for downstream predictions on diverse endpoints such as solubility, toxicity, and reactivity. In contrast, neural models can learn task-specific features directly from molecular graphs, capturing both local and global interactions.\n\nGraph neural networks for molecular property prediction include message passing architectures and variants that modify aggregation or attention (Duvenaud et al., 2015; Kearnes et al., 2016; Gilmer et al., 2017; Schlichtkrull et al., 2018; Xu et al., 2019; Hu et al., 2020; Wang et al., 2021). Beyond graph encoders, pretraining strategies and multi-task learning have been explored to improve sample efficiency under sparse labels, while scaffold-aware splits and cross-dataset evaluation protocols aim to assess generalization more rigorously.\n\nIn this paper, we study label-scarce molecular property prediction under extreme cold-start conditions and present a method that leverages subgraph-level invariances with contrastive regularization. We also introduce a stratified benchmark that emphasizes scaffold novelty.",
    "reason": "This sentence lists prior GNN works without explaining how they relate to the paper’s goals or what gap remains, providing no synthesis or author perspective.",
    "start": 515,
    "end": 807,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Question answering (QA) are fundamental for supporting humans’ development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning.",
    "document": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans’ development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students’ performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions’ reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Koˇcisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models’ reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n ",
    "start": 99,
    "end": 293,
    "label": "Unsupported_claim"
  },
  {
    "span": "This kind of high-quality questions is also valuable for improving machine reading comprehension.",
    "document": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans’ development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students’ performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions’ reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Koˇcisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models’ reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n ",
    "start": 842,
    "end": 938,
    "label": "Unsupported_claim"
  },
  {
    "span": "This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way.",
    "document": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans’ development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students’ performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions’ reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Koˇcisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models’ reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n ",
    "start": 1786,
    "end": 1992,
    "label": "Unsupported_claim"
  },
  {
    "span": "state-of-the-art QA models",
    "document": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans’ development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students’ performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions’ reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Koˇcisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models’ reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n ",
    "start": 4029,
    "end": 4054,
    "label": "Unsupported_claim"
  },
  {
    "span": "NarrativeQA (Koˇcisk `y et al., 2018)",
    "document": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans’ development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students’ performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions’ reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Koˇcisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models’ reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n ",
    "start": 4305,
    "end": 4341,
    "label": "Format"
  },
  {
    "span": "McMahan et al. (2017) formalized federated averaging for device-local training. Kairouz et al. (2021) surveyed advances in federated optimization and systems. Suresh et al. (2019) proposed gradient compression techniques for communication efficiency. Dwork et al. (2014) established differential privacy foundations.",
    "document": "Introduction\n\nFederated learning enables collaborative model training without centralizing raw data, reducing privacy risks while leveraging distributed datasets. Key challenges include client heterogeneity, limited communication bandwidth, and statistical non-iid distributions across devices. Furthermore, privacy guarantees and robustness to adversarial behavior are paramount in real-world deployments, where participation is sporadic and hardware is constrained.\n\nMcMahan et al. (2017) formalized federated averaging for device-local training. Kairouz et al. (2021) surveyed advances in federated optimization and systems. Suresh et al. (2019) proposed gradient compression techniques for communication efficiency. Dwork et al. (2014) established differential privacy foundations.\n\nOur work studies the interplay of compression and privacy accounting under client drift, introducing a method that co-designs quantization with privacy budgets. We provide end-to-end bounds on utility and privacy loss under partial participation and present an implementation that scales to millions of clients.",
    "reason": "The span lists four references but does not explain how the survey, compression, and privacy theory relate to federated averaging or to each other. The relationships are implied and transitions are missing, reducing coherence.",
    "start": 469,
    "end": 785,
    "label": "Coherence"
  },
  {
    "span": "Garcia et al.",
    "document": "Introduction\n\nDetecting anomalies in multivariate time series has become central to monitoring complex cyber-physical systems (Bai and Nguyen, 2020; Luo et al., 2022). Models that combine sequence encoders with probabilistic decoders often achieve strong performance while preserving interpretability (Ramos and Patel, 2021). However, real-world deployments must handle nonstationarity and missing data, which complicate evaluation (Chen and Silva, 2019).\n\nFollowing Garcia et al. we estimate a time-varying baseline and compare residual patterns across sensors to detect system-level deviations (Huang and Zhou, 2021). We further introduce a robust objective to mitigate the impact of short-term bursts, extending prior robust statistics used in streaming settings (Khan et al., 2020).",
    "reason": "Narrative citation is missing the publication year; should be Garcia et al. (YEAR).",
    "start": 467,
    "end": 480,
    "label": "Format"
  },
  {
    "span": "Kang et al. (2018) use self-attentive sequential models. Wang et al. (2019) incorporate knowledge graphs into recommendation. Random walk-based graph embedding has also been employed (Perozzi et al., 2014). Evaluation metrics include NDCG and Recall (He et al., 2017).",
    "document": "Related Work\n\nRecommendation Models with Graph Structure\n\nGraph-based recommenders leverage user–item interactions and side information to learn representations that support personalized ranking. Recent methods enrich sequence signals and structural relations to address sparsity and cold-start issues.\n\nKang et al. (2018) use self-attentive sequential models. Wang et al. (2019) incorporate knowledge graphs into recommendation. Random walk-based graph embedding has also been employed (Perozzi et al., 2014). Evaluation metrics include NDCG and Recall (He et al., 2017).\n\nOur approach builds upon message passing over user–item bipartite graphs and contrasts neighborhoods to improve robustness under distribution shifts.",
    "reason": "The span enumerates works and even jumps to metrics without articulating how they relate to each other; there are no transitions explaining connections between sequence models, knowledge graphs, random walks, and evaluation.",
    "start": 304,
    "end": 572,
    "label": "Coherence"
  },
  {
    "span": "For traffic prediction, widely used spatio-temporal graph models include DCRNN (Li et al., 2018), STGCN (Yu et al., 2018), Graph WaveNet (Wu et al., 2019), and StemGNN (Cao et al., 2020), along with attention-based variants (Zheng et al., 2020; Guo et al., 2021).",
    "document": "Related Work\n\nGraph-based spatio-temporal forecasting. Accurate traffic forecasting relies on modeling both spatial dependencies among sensors and temporal dynamics. Early neural approaches combine recurrent networks with graph convolutions to capture these interactions. For traffic prediction, widely used spatio-temporal graph models include DCRNN (Li et al., 2018), STGCN (Yu et al., 2018), Graph WaveNet (Wu et al., 2019), and StemGNN (Cao et al., 2020), along with attention-based variants (Zheng et al., 2020; Guo et al., 2021). Recent work also explores dynamic graphs and adaptive adjacency to reflect time-varying connectivity patterns in road networks.\n\nData augmentation and robustness. Complementary efforts study data augmentation, imputation for missing sensors, and robust training to handle distribution shifts due to incidents or weather (Cini et al., 2022; Rodrigues et al., 2021). Some methods incorporate external covariates such as events and meteorology to improve generalization.\n\nProbabilistic forecasting. Beyond point estimates, probabilistic models and quantile regression provide uncertainty estimates that are crucial for downstream decision-making in intelligent transportation systems (Salinas et al., 2020; Gasthaus et al., 2019).\n",
    "reason": "The span catalogs models and citations without clarifying how they relate to the current approach, what limitations they share, or why new methods are needed (criteria a and b).",
    "start": 272,
    "end": 535,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent works have shown that Vision Transformers consistently outperform CNNs on medical image classification.",
    "document": "Related Work\n\nTransformers have been adapted from natural language processing to computer vision by operating on image patches with self-attention. Hybrid architectures and pure transformer models have been explored to balance inductive biases with modeling flexibility.\n\nRecent works have shown that Vision Transformers consistently outperform CNNs on medical image classification. However, data scarcity, strong regularization needs, and resolution constraints complicate their deployment in clinical settings.\n\nThis study investigates multi-instance learning with slide-level supervision and introduces a hierarchical token pooling mechanism that preserves fine-grained cues while controlling computational cost.",
    "reason": "Asserts a broad performance claim about 'recent works' without providing references per rule (d).",
    "start": 272,
    "end": 382,
    "label": "Unsupported_claim"
  },
  {
    "span": "It is widely known that back-translation is the de-facto standard for leveraging monolingual data.",
    "document": "Related Work\n\nDomain adaptation for neural machine translation (NMT) leverages out-of-domain corpora and in-domain monolingual text to improve performance under data scarcity. Techniques range from fine-tuning and multi-domain tags to data augmentation and curriculum strategies.\n\nIt is widely known that back-translation is the de-facto standard for leveraging monolingual data. Variants extend the idea with iterative refinement, noise modeling, and sampling distributions tailored to domain mismatch.\n\nOur approach revisits monolingual exploitation through pseudo-dual learning that couples back-translation with uncertainty-aware filtering for robust adaptation.\n",
    "reason": "The sentence asserts a field-wide consensus about a method being the de-facto standard without any citations to foundational or survey papers.",
    "start": 281,
    "end": 379,
    "label": "Unsupported_claim"
  },
  {
    "span": "Our work differs from prior self-supervised ASR models trained exclusively on LibriSpeech.",
    "document": "Related Work\n\nSelf-supervised learning has transformed automatic speech recognition by reducing dependence on labeled data through pretext objectives over raw audio. Our work differs from prior self-supervised ASR models trained exclusively on LibriSpeech. We investigate multilingual pretraining on diverse broadcast and conversational audio, followed by domain-specific finetuning. This setup aims to mitigate domain mismatch and improve recognition of spontaneous speech phenomena.",
    "reason": "References 'prior' models and names a specific dataset on first mention without providing citations (definition a and e).",
    "start": 166,
    "end": 256,
    "label": "Unsupported_claim"
  },
  {
    "span": "the commonly adopted split of the MS MARCO passage dataset",
    "document": "Related Work\n\nNeural passage retrieval underpins open-domain QA and search, with dense retrievers replacing sparse term-matching in many settings. Contrastive pretraining using in-batch negatives and hard negative mining has proven effective, yet generalization to unseen domains remains challenging. Many studies report results on the commonly adopted split of the MS MARCO passage dataset, comparing average MRR and top-k recall. Recent work augments training with distillation from cross-encoders to bridge the gap between bi-encoder speed and cross-encoder accuracy. Our approach focuses on curriculum mining of negatives to better align the retriever with downstream QA objectives.",
    "reason": "Invokes a specific dataset and its standard split without providing a citation to MS MARCO or the protocol that defines the split.",
    "start": 332,
    "end": 390,
    "label": "Unsupported_claim"
  },
  {
    "span": "Time-series anomaly detection methods include reconstruction-based autoencoders (Zong et al., 2018), sequence models such as LSTM/GRU and TCNs (Malhotra et al., 2015; Bai et al., 2018), probabilistic models like VAEs and normalizing flows (An and Cho, 2015; Dinh et al., 2017), and generative adversarial approaches (Li et al., 2019b).",
    "document": "Introduction\n\nDetecting anomalies in IoT sensor streams is crucial for reliability and safety, but labeled anomalies are rare and operating conditions drift. Methods must be data-efficient and adaptive while providing actionable alerts.\n\nTime-series anomaly detection methods include reconstruction-based autoencoders (Zong et al., 2018), sequence models such as LSTM/GRU and TCNs (Malhotra et al., 2015; Bai et al., 2018), probabilistic models like VAEs and normalizing flows (An and Cho, 2015; Dinh et al., 2017), and generative adversarial approaches (Li et al., 2019b).\n\nYet, most approaches assume stationary behavior or retraining access, which is impractical for on-device settings with limited compute and intermittent connectivity.\n\nWe introduce StreamLite, an on-device adaptive detector using calibration-aware conformal scores and memory-bounded updates to handle drift under tight resource budgets.",
    "reason": "The span enumerates families of approaches without linking them to the constraints of the paper’s setting or articulating a gap that the new method fills.",
    "start": 238,
    "end": 573,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Numerous recent works propose certified defenses for large transformers.",
    "document": "Introduction\n\nAdversarial robustness of large language and vision-language transformers has become a pressing concern as these models are deployed in safety-critical settings. While empirical defenses such as adversarial training and input preprocessing can raise attack costs, they often fail under adaptive threat models. Numerous recent works propose certified defenses for large transformers.\n\nDespite promising theory, scaling certified guarantees to real-world workloads remains challenging due to computational costs and loose bounds. In this paper, we revisit training-time and inference-time trade-offs for certification on transformer architectures, proposing a relaxed objective that tightens bounds while preserving accuracy.",
    "reason": "Asserts the existence of many recent certified defense works without providing citations to any of them.",
    "start": 324,
    "end": 396,
    "label": "Unsupported_claim"
  },
  {
    "span": "Bias mitigation has been studied through pre-processing, in-processing, and post-processing strategies in recommendation systems (Kamishima et al., 2012; Beutel et al., 2019; Zhu et al., 2020). Metrics such as demographic parity, equalized odds, and exposure fairness are widely used to quantify disparities (Hardt et al., 2016; Diaz et al., 2020; Singh and Joachims, 2018).",
    "document": "Related Work and Introduction\n\nRecommendation systems can inadvertently propagate and amplify societal biases, raising concerns about equitable exposure, user experience, and downstream impacts. Addressing these issues requires both principled metrics and effective mitigation strategies that operate under practical constraints of scale and feedback loops.\n\nBias mitigation has been studied through pre-processing, in-processing, and post-processing strategies in recommendation systems (Kamishima et al., 2012; Beutel et al., 2019; Zhu et al., 2020). Metrics such as demographic parity, equalized odds, and exposure fairness are widely used to quantify disparities (Hardt et al., 2016; Diaz et al., 2020; Singh and Joachims, 2018).\n\nWe present FairTune, a feedback-aware calibration method that adjusts ranking scores with counterfactual exposure estimates learned from historical logs. Our approach integrates a constrained optimizer that targets platform-level exposure goals while preserving per-user utility.\n\nWe validate FairTune on two large-scale datasets with simulated interventions and live-trial-inspired protocols, showing improved exposure parity at minimal utility loss, and provide a sensitivity analysis to logging policy bias.",
    "reason": "The span enumerates methods and metrics without explaining how they relate to the proposed approach or what specific gap remains, lacking synthesis and author perspective (criteria a and c).",
    "start": 359,
    "end": 733,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Khan et al., 2020]",
    "document": "Related Work\n\nLegged robot locomotion combines model-based control with learning to achieve robust performance over diverse terrains (Katz et al., 2019; Hwangbo et al., 2019). Sim-to-real transfer is facilitated by domain randomization and system identification (Tobin et al., 2017; Antonova et al., 2017).\n\nModel-free RL has demonstrated agile behaviors, but sample efficiency and safety remain challenges (Peng et al., 2018; Tan et al., 2018). Model-based variants learn dynamics and plan via MPC or trajectory optimization (Chua et al., 2018; Williams et al., 2017). Safety filters and recovery policies reduce falls during exploration (Thananjeyan et al., 2021; Dalal et al., 2018).\n\nCurriculum learning and privileged observations during simulation help bootstrap policies that generalize under partial observability (Akkaya et al., 2019; Yu et al., 2021). For terrain-aware foothold selection, cost maps derived from onboard perception are fused with proprioceptive feedback (Falanga et al., 2020; Lee et al., 2020; (Khan et al., 2020] propose a learning-based foothold predictor with uncertainty estimates.\n\nWe contribute a contact-aware transformer that predicts feasible gait transitions conditioned on terrain geometry and stability margins.",
    "reason": "Mismatched brackets in a parenthetical citation; opening parenthesis with a closing square bracket.",
    "start": 1022,
    "end": 1041,
    "label": "Format"
  },
  {
    "span": "State-of-the-art systems achieve over 95% F1 on sentiment classification with distant supervision.",
    "document": "Introduction\n\nSentiment classification has evolved from feature-engineered models to transformer-based architectures trained at scale. Label acquisition strategies, including distant supervision from emojis and hashtags, have enabled the use of massive unlabeled corpora. State-of-the-art systems achieve over 95% F1 on sentiment classification with distant supervision. However, these results often rely on genre-specific distributions and may not transfer to domains with subtle pragmatic cues. We revisit this setting with a focus on calibration, domain shift, and evaluation that reflects realistic cost-sensitive decisions.",
    "reason": "Makes a numerical performance claim about SOTA without citing any paper or leaderboard as evidence.",
    "start": 272,
    "end": 370,
    "label": "Unsupported_claim"
  },
  {
    "span": "Sequential recommenders model user behavior using RNNs, CNNs, and self-attention (Hidasi et al., 2016; Tang and Wang, 2018; Kang and McAuley, 2018). We propose a Transformer-based recommender.",
    "document": "Introduction\n\nRecommender systems increasingly rely on users' action sequences to capture evolving preferences. Modeling temporal dependencies can improve next-item prediction, cold-start behavior, and robustness to distribution shifts.\n\nSequential recommenders model user behavior using RNNs, CNNs, and self-attention (Hidasi et al., 2016; Tang and Wang, 2018; Kang and McAuley, 2018). We propose a Transformer-based recommender. Our experiments evaluate the approach on standard e-commerce datasets with various sequence lengths and sparsity levels.\n\nWe additionally analyze the effect of different positional encodings and regularization strategies on long-horizon prediction accuracy.",
    "reason": "The span lists prior families and then states the contribution but fails to articulate the gap or rationale that necessitates their Transformer approach (criterion b and c).",
    "start": 238,
    "end": 430,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Secure aggregation masks client updates so the server cannot inspect individual gradients (Bonawitz et al., 2017). Differential privacy perturbs either client-side updates or aggregated models to bound information leakage (Abadi et al., 2016; Kairouz et al., 2021). Gradient quantization and sparsification reduce uplink bandwidth (Sattler et al., 2019). Homomorphic encryption allows computation over ciphertexts without decryption (Acar et al., 2018).",
    "document": "Related Work\n\nFederated Learning Overview\nFederated learning enables on-device training while keeping raw data local, coordinating a global model across many clients (McMahan et al., 2017). Subsequent work has explored aggregation protocols, robustness to stragglers, and client selection to improve scalability and efficiency (Li et al., 2020a; Karimireddy et al., 2020).\n\nCommunication and Privacy Mechanisms\nA significant thread of research studies how to protect user privacy without sacrificing utility. Secure aggregation masks client updates so the server cannot inspect individual gradients (Bonawitz et al., 2017). Differential privacy perturbs either client-side updates or aggregated models to bound information leakage (Abadi et al., 2016; Kairouz et al., 2021). Gradient quantization and sparsification reduce uplink bandwidth (Sattler et al., 2019). Homomorphic encryption allows computation over ciphertexts without decryption (Acar et al., 2018). While these techniques address different concerns, combining them effectively remains challenging under heterogeneous and intermittent client participation (Li et al., 2020b).\n\nPersonalization and Robustness\nAnother strand investigates adapting global models to client-specific distributions using meta-learning, multi-task optimization, or mixture models (Fallah et al., 2020; Arivazhagan et al., 2019). Robust aggregation and anomaly detection aim to mitigate the impact of byzantine or poisoned updates (Blanchard et al., 2017; Yin et al., 2018). Our work focuses on balancing privacy guarantees with communication efficiency when clients contribute sparse, non-iid updates.",
    "reason": "The span abruptly lists secure aggregation, differential privacy, communication compression, and homomorphic encryption without transitions or explicit relationships among them, making it unclear how each cited work relates to the others.",
    "start": 509,
    "end": 962,
    "label": "Coherence"
  },
  {
    "span": "However, we argue that the attention mechanism is not an appropriate way to capture and leverage lexico-logical alignments. It mainly has the following two problems. First, the standard attention can only model alignments at the token level rather than the phrase level, while there are many multi-granular, non-continuous alignments in the text-to-SQL task. For the example in Figure 1, \"order by . . . limit 1\" is a SQL keyword pattern representing a superlative operation. However, the standard attention module can only align \"order\", \"by\", \"limit\", and \"1\" to \"the longest\" token by token, rather than regarding them as a whole. It may confuse the decoder and lead to the failure to generate this pattern correctly (Herzig and Berant, 2020). ",
    "document": "Introduction\n\nText-to-SQL parsing is the task of mapping natural language questions to executable SQL queries on relational databases (Zhong et al., 2017). It provides an easy way for common users unfamiliar with query languages to access large databases and has attracted great attention. Recently, lexicological alignments, which align question phrases to their corresponding SQL query fragments, have been proved to be very helpful in improving parsing performance (Shi et al., 2020). As shown in Figure 1, the token \"competitor\" should be aligned to \"c1\" in the SQL query. To capture such alignments, several attention-based models were proposed (Shi et al., 2020;Lei et al., 2020;Liu et al., 2021), which employ the attention weights among tokens to indicate the alignments. Specifically, they use an attention module to perform schema linking at the encoding stage (Lei et al., 2020;Liu et al., 2021), and may use another attention to align each output token to its corresponding input tokens at the decoding stage (Shi et al., 2020).  However, we argue that the attention mechanism is not an appropriate way to capture and leverage lexico-logical alignments. It mainly has the following two problems. First, the standard attention can only model alignments at the token level rather than the phrase level, while there are many multi-granular, non-continuous alignments in the text-to-SQL task. For the example in Figure 1, \"order by . . . limit 1\" is a SQL keyword pattern representing a superlative operation. However, the standard attention module can only align \"order\", \"by\", \"limit\", and \"1\" to \"the longest\" token by token, rather than regarding them as a whole. It may confuse the decoder and lead to the failure to generate this pattern correctly (Herzig and Berant, 2020). Second, traditional attentionbased approaches are prone to overfitting the training data, which is harmful to the model's generalization capability. It is not only the domain generalization (Dong et al., 2019) but also the compositional generalization (Herzig and Berant, 2020).\n\nTo solve the aforementioned problems, we propose a neural parsing framework to leverage explicit lexico-logical alignments. Dong et al. (2019) have pointed out that if we align question tokens to columns or values in databases before parsing, it will help to improve the model's generalization among different domains (databases). Motivated by this, our framework consists of two steps. Specifi-cally, we first implement a simple model to obtain possible lexico-logical alignments before parsing. While in the second step, we inject such alignments into a standard seq2seq parser by treating them as additional contexts, similar to \"prompt information\" or \"evidence\" in machine reading comprehension (Mihaylov and Frank, 2018;Tu et al., 2020;Niu et al., 2020). Moreover, to alleviate the negative effects on the parser caused by noise alignments, we propose a data augmentation method that adds noisy alignments during the training procedure. Experimental results on an open-released dataset, SQUALL (Shi et al., 2020), show that our framework achieves state-of-the-art performance and obtains an absolute improvement of 3.4% compared with existing attention-based models.\n\n ",
    "start": 1042,
    "end": 1789,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent approaches leverage graph neural networks to propagate user–item signals (Wang et al., 2019; He et al., 2020; Xu and Yin, 2021; Chen et al., 2022). These models typically stack neighborhood aggregations, incorporate higher-order connectivity, and sometimes add self-supervision for regularization (Wu et al., 2021; Sun et al., 2020; Jin et al., 2021). Contrastive objectives on graph-structured interactions have also been explored to enhance robustness (Velickovic et al., 2019; Qiu et al., 2020).",
    "document": "Related Work\n\nGraph-based recommender systems have evolved from matrix factorization to neural architectures that explicitly model the user–item interaction graph. Early methods propagated preferences along paths or via random walks to capture higher-order signal, while neural models learn to aggregate neighborhood information with learned weights.\n\nRecent approaches leverage graph neural networks to propagate user–item signals (Wang et al., 2019; He et al., 2020; Xu and Yin, 2021; Chen et al., 2022). These models typically stack neighborhood aggregations, incorporate higher-order connectivity, and sometimes add self-supervision for regularization (Wu et al., 2021; Sun et al., 2020; Jin et al., 2021). Contrastive objectives on graph-structured interactions have also been explored to enhance robustness (Velickovic et al., 2019; Qiu et al., 2020).\n\nBeyond static graphs, several works incorporate temporal dynamics and side information into GNN recommenders, capturing session effects, item attributes, or knowledge graphs. Parallel to these, re-ranking and calibration techniques re-balance exposure to address popularity bias.",
    "reason": "Lists prior work without connecting it to the paper’s argument or explaining how these methods relate to the authors’ approach (definition a/c).",
    "start": 352,
    "end": 857,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Lopez 2020)",
    "document": "Related Work\n\nSemi-supervised learning. Classic approaches combine labeled and unlabeled data through entropy minimization and virtual adversarial training (Miyato et al., 2018). Graph-based label propagation and consistency objectives have also been successful (Berthelot et al., 2019). Semi-supervised methods (Lopez 2020) attempt to unify these ideas under a common framework for NLP tasks.\n\nDespite progress, stability remains a concern when unlabeled data diverges from the labeled distribution.",
    "reason": "Missing comma between author and year in the parenthetical citation; it should be '(Lopez, 2020)'.",
    "start": 312,
    "end": 324,
    "label": "Format"
  },
  {
    "span": "Miller et al. [3]",
    "document": "Related Work\n\nMultimodal learning integrates complementary signals from text, vision, and audio to improve generalization (Ngiam et al., 2011; Baltrušaitis et al., 2019). Early fusion approaches project modalities into a shared space before joint modeling (Huang et al., 2020), while late fusion aggregates modality-specific predictions (Arevalo et al., 2017). Miller et al. [3] propose a late-fusion baseline with gating, demonstrating competitive performance under missing-modality scenarios. Contrastive pretraining aligns modalities at scale and has become a de facto paradigm (Radford et al., 2021; Jia et al., 2021).",
    "reason": "Mixed citation styles: numeric bracket [3] used with author-year; should be Miller et al. (YEAR) or switch to a consistent numeric style.",
    "start": 361,
    "end": 378,
    "label": "Format"
  },
  {
    "span": "Lee & Park (2020)",
    "document": "Related Work\n\nSelf-supervised pretraining has driven major advances in visual recognition, with contrastive and masked-image objectives reducing labeled data requirements (Arun and Malik, 2020; Chen and Duarte, 2021). For semantic segmentation, pyramid pooling and multi-scale feature aggregation remain key architectural ingredients (Zhou and Lin, 2019). Lee & Park (2020) introduced a dynamic decoder that adapts receptive fields based on uncertainty, while later work incorporated transformer backbones to capture long-range dependencies (Hsu and Wang, 2021).\n\nWe propose a lightweight attention head that conditions on class-wise prototypes learned from unlabeled data. The approach yields consistent improvements on urban scene datasets under limited supervision and exhibits strong robustness to distribution shift (Ramos and Wei, 2022).",
    "reason": "Incorrect use of ampersand in a narrative citation; narrative form should use 'and' as in 'Lee and Park (2020)'.",
    "start": 356,
    "end": 373,
    "label": "Format"
  },
  {
    "span": "BERT has been used in AES trained on ASAP essays with additional handcrafted features.",
    "document": "Related Work\n\nAutomatic Essay Scoring (AES)\n\nNeural approaches to AES have replaced traditional feature-engineered pipelines by leveraging pre-trained language models. Transformer-based encoders offer strong contextual representations that correlate with human scoring rubrics across multiple prompts and genres. Recent studies also explore hybrid designs that combine holistic text embeddings with discourse-aware or prompt-specific signals.\n\nBERT has been used in AES trained on ASAP essays with additional handcrafted features. Other lines of work examine domain adaptation and prompt-agnostic evaluation by aligning representation spaces across prompts.\n\nIn contrast to these methods, our approach emphasizes calibration and explanation: we explicitly disentangle content, organization, and mechanics, and we provide per-dimension confidence estimates to better support formative feedback.",
    "reason": "Mentions a specific dataset (ASAP) and a concrete modeling setup as prior work without citing any paper that introduced or evaluated this configuration.",
    "start": 444,
    "end": 530,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior clinical NER studies report F1 scores above 90% on discharge summaries.",
    "document": "Introduction\n\nClinical named entity recognition (NER) underpins downstream tasks such as phenotyping, cohort selection, and pharmacovigilance. Compared to general-domain NER, clinical text poses challenges including nonstandard abbreviations, domain-specific terminology, and privacy constraints.\n\nPrior clinical NER studies report F1 scores above 90% on discharge summaries. Performance, however, can vary substantially across institutions and document types, with discharge summaries typically easier than progress notes due to richer context and more standardized phrasing.\n\nWe examine cross-institution generalization by training on one hospital's corpus and evaluating on another, analyzing degradation sources and mitigation strategies via domain-adaptive pretraining.",
    "reason": "Makes a quantitative claim about prior results without citing any studies.",
    "start": 298,
    "end": 375,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior surveys have documented that dark patterns appear in 70% of top e-commerce sites.",
    "document": "Related Work\n\nDesign patterns that manipulate user choice—often termed dark patterns—have drawn increasing scrutiny from researchers and regulators. Studies examine taxonomy, prevalence, and user susceptibility across web and mobile interfaces. Measurement challenges include identifying deceptive intent and accounting for regional legal differences.\n\nPrior surveys have documented that dark patterns appear in 70% of top e-commerce sites. Despite increased awareness, standardized auditing protocols and disclosure requirements remain limited. Our study contributes an open-source crawler and annotation schema designed to improve reproducibility and cross-jurisdictional comparisons.",
    "reason": "Reports a specific prevalence statistic from prior surveys without citing any sources, violating rule (b).",
    "start": 353,
    "end": 440,
    "label": "Unsupported_claim"
  },
  {
    "span": "There are many recent works that explore constrained decoding for code synthesis in production environments.",
    "document": "Introduction\nLarge language models have rapidly advanced the state of automated code synthesis, enabling developers to generate functions, tests, and refactoring suggestions from natural language prompts. As these models transition from research prototypes to developer tools, reliability and safety constraints become paramount. In real-world settings, code must compile, adhere to style guides, and respect security and privacy policies.\nThere are many recent works that explore constrained decoding for code synthesis in production environments. These approaches aim to enforce structural, syntactic, or semantic constraints during generation, reducing post-hoc filtering costs and failure rates. However, the design space of constraints and their impact on developer experience is not yet well understood.\nThis paper investigates constraint interfaces that balance correctness guarantees with flexibility for interactive use. We evaluate constraint formulations across a spectrum of complexity and report their effects on latency, acceptance rates, and error profiles.",
    "reason": "Mentions 'recent works' without citing specific prior work or papers.",
    "start": 440,
    "end": 548,
    "label": "Unsupported_claim"
  },
  {
    "span": "a previous study demonstrated that graph-based collaborative filtering reduces cold-start error by 20% on average",
    "document": "Introduction\n\nPersonalized recommendation systems often struggle with cold-start users and items due to sparse interaction histories. Graph-based collaborative filtering extends matrix-based models by explicitly encoding higher-order connectivity, which can propagate signals from dense regions to sparse nodes. Notably, a previous study demonstrated that graph-based collaborative filtering reduces cold-start error by 20% on average, motivating hybrid designs that mix content and structure. However, most evaluations rely on random splits that inflate performance under exposure bias. We propose a counterfactual evaluation protocol coupled with a debiased sampler for graph augmentation.",
    "reason": "References a specific prior study and quantitative improvement without citing the source (rule b/e; statistical claim about prior work requires citation).",
    "start": 321,
    "end": 434,
    "label": "Unsupported_claim"
  },
  {
    "span": "According to industry reports, over 70% of defects on conveyor belts are missed without deep learning.",
    "document": "Introduction\n\nAutomated visual inspection is critical for maintaining product quality in high-throughput manufacturing. Traditional rule-based methods often fail to capture subtle variations in defects, leading to inconsistent detection rates. According to industry reports, over 70% of defects on conveyor belts are missed without deep learning. This motivates the use of modern detection and segmentation architectures that can learn robust representations directly from image data.\n\nWhile recent advances in real-time object detection have enabled deployment at line speed, challenges persist in handling motion blur, reflective surfaces, and class imbalance. Furthermore, the scarcity of labeled anomalies complicates supervised training. We investigate a semi-supervised pipeline that leverages synthetic negatives and uncertainty-aware fine-tuning to bridge the gap between lab performance and production constraints.",
    "reason": "Presents a quantitative statistic attributed to 'industry reports' without citing any source (rule b and e).",
    "start": 244,
    "end": 346,
    "label": "Unsupported_claim"
  },
  {
    "span": "As noted in prior work, ROUGE correlates poorly with factuality on scientific abstracts.",
    "document": "Introduction\n\nAutomatic summarization evaluation has traditionally relied on surface-overlap metrics such as ROUGE and BLEU. With the rise of neural abstractive systems, these metrics often fail to capture semantic adequacy and factual consistency. Recent semantic metrics incorporate pretrained language models and entailment checks.\n\nAs noted in prior work, ROUGE correlates poorly with factuality on scientific abstracts. This gap has motivated alternatives that assess entity-level consistency, citation grounding, and logical entailment. Yet, adoption remains limited due to compute cost and dataset specificity.\n\nWe present a lightweight factuality metric that combines faithfulness probes with terminology grounding. Our contributions include a new dataset of expert factuality judgments for scientific domains and an analysis of metric robustness under paraphrasing and negation.",
    "reason": "Refers to 'prior work' asserting a specific empirical observation without citing those works (violates rule b/d).",
    "start": 336,
    "end": 424,
    "label": "Unsupported_claim"
  },
  {
    "span": "the state-of-the-art graph neural network for link prediction",
    "document": "Related Work\n\nLink prediction in knowledge graphs has a long history, from translational distance models to bilinear and tensor factorization approaches. More recently, graph neural networks (GNNs) have been leveraged to capture multi-hop relational context and to incorporate structural biases.\n\nHybrid models that combine message passing with scoring functions have shown strong empirical results across benchmarks. We compare against the state-of-the-art graph neural network for link prediction and several strong baselines that use convolutional scoring. Additionally, we consider path-based reasoning methods that exploit rule induction for interpretability.\n\nOur approach differs by introducing an adaptive neighborhood sampler that conditions on the query relation, improving efficiency without sacrificing accuracy.",
    "reason": "Claims comparison to 'the state-of-the-art' method without naming or citing the specific model(s) (definition b/e).",
    "start": 437,
    "end": 498,
    "label": "Unsupported_claim"
  },
  {
    "span": "According to (Miller, 2020)",
    "document": "Introduction\n\nTrustworthy NLP encompasses calibration, robustness, and fairness, with methods ranging from temperature scaling to distributionally robust objectives (Guo et al., 2017; Oren et al., 2019). According to (Miller, 2020) post-hoc explanation techniques can improve user trust, yet recent analyses caution that faithfulness is not guaranteed (Jacovi and Goldberg, 2020; Pruthi et al., 2020). We focus on uncertainty estimation under domain shift, combining selective prediction with conformal risk control to provide actionable abstentions (Angelopoulos et al., 2021; Rogriguez et al., 2022).",
    "reason": "Wrong citation style: narrative construction should be \"According to Miller (2020)\" without parentheses around the author-year.",
    "start": 204,
    "end": 231,
    "label": "Format"
  },
  {
    "span": "there are many recent works that explore streaming transducer architectures",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) has increasingly favored transducer models for low-latency applications. These models jointly learn acoustic and alignment processes and can be deployed with tight streaming constraints.\n\nWhile non-streaming encoder-decoder models benefit from bidirectional context and large batch training, deploying them in real time requires architectural and algorithmic modifications. In particular, there are many recent works that explore streaming transducer architectures, focusing on chunk-wise attention, alignment regularization, and inference-time beam control.\n\nOur approach targets stability and timestamp accuracy by introducing a monotonicity-aware objective that reduces emission jitter without degrading word error rate. We compare across latency budgets and analyze error patterns under overlapped speech.",
    "reason": "The statement about 'many recent works' lacks any supporting citations, violating rule d requiring citations for mentions of recent works.",
    "start": 449,
    "end": 524,
    "label": "Unsupported_claim"
  },
  {
    "span": "SpecAugment variants modify time–frequency masks to improve robustness (Park et al., 2019; Li et al., 2020; Zheng et al., 2021).",
    "document": "Related Work\n\nData augmentation for low-resource ASR. Augmentation is a central tool for improving automatic speech recognition (ASR) in data-scarce settings. Early methods injected noise or performed speed and pitch perturbations to mimic acoustic variability (Ko et al., 2015; Cui et al., 2015). SpecAugment variants modify time–frequency masks to improve robustness (Park et al., 2019; Li et al., 2020; Zheng et al., 2021). Other strategies synthesize speech from text via TTS or leverage unlabeled audio with pseudo-labels (Rosenberg et al., 2019; Kahn et al., 2020).\n\nSelf-supervised learning. Pre-training with self-supervised objectives such as contrastive predictive coding and masked acoustic modeling has yielded strong gains, especially in low-resource regimes (Schneider et al., 2019; Baevski et al., 2020; Hsu et al., 2021). Fine-tuning protocols vary from full-model updates to lightweight adapters (Houlsby et al., 2019; Hu et al., 2022).\n\nCross-lingual transfer. Multilingual models transfer representations across languages and accents through shared subword vocabularies and joint training (Conneau et al., 2020; Pratap et al., 2020). Lexicon constraints and pronunciation modeling further assist adaptation (Chen et al., 2019; Sanabria et al., 2020).",
    "reason": "The span only catalogs a family of augmentation techniques and citations without relating them to the authors' aims, limitations they face, or how their method interacts with or improves upon these approaches.",
    "start": 298,
    "end": 426,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Curiosity-driven exploration uses prediction error as intrinsic reward (Pathak et al., 2017). Thompson sampling balances exploration and exploitation in bandits (Thompson, 1933). Count-based bonuses improve sample efficiency in sparse-reward MDPs (Bellemare et al., 2016). Off-policy algorithms enable reuse of past experience (Mnih et al., 2016).",
    "document": "Related Work\n\nExploration in Reinforcement Learning. Efficient exploration is essential for learning in sparse or deceptive reward environments. Approaches include intrinsic motivation, uncertainty-driven policies, and density estimation to encourage visiting novel states (Schmidhuber, 1991; Bellemare et al., 2016; Pathak et al., 2017).\n\nIntrinsic and Uncertainty-Based Methods. Curiosity-driven exploration uses prediction error as intrinsic reward (Pathak et al., 2017). Thompson sampling balances exploration and exploitation in bandits (Thompson, 1933). Count-based bonuses improve sample efficiency in sparse-reward MDPs (Bellemare et al., 2016). Off-policy algorithms enable reuse of past experience (Mnih et al., 2016). We investigate a unified framework that calibrates intrinsic rewards with posterior uncertainty to mitigate detachment and derailment in long-horizon tasks.",
    "reason": "The cited sentences cover disparate areas—intrinsic curiosity, bandit Thompson sampling, count-based MDP bonuses, and off-policy learning—without transitions or explicit ties, leaving their interrelations unstated and coherence weak.",
    "start": 381,
    "end": 728,
    "label": "Coherence"
  },
  {
    "span": "Textual adversarial attacks manipulate tokens via synonym substitution, paraphrasing, or gradient-based character edits, while defenses include adversarial training, certified robustness, and input smoothing (Jin et al., 2020; Ebrahimi et al., 2018; Jia et al., 2019; Huang et al., 2021). Robustness evaluations now extend to pre-trained transformer encoders and decoders across GLUE, SQuAD, and MNLI (Pruthi et al., 2019; Morris et al., 2020; Si et al., 2021).",
    "document": "Introduction\n\nPre-trained transformers power a wide spectrum of NLP systems but remain vulnerable to adversarial perturbations. Understanding and improving robustness is therefore a key research objective.\n\nTextual adversarial attacks manipulate tokens via synonym substitution, paraphrasing, or gradient-based character edits, while defenses include adversarial training, certified robustness, and input smoothing (Jin et al., 2020; Ebrahimi et al., 2018; Jia et al., 2019; Huang et al., 2021). Robustness evaluations now extend to pre-trained transformer encoders and decoders across GLUE, SQuAD, and MNLI (Pruthi et al., 2019; Morris et al., 2020; Si et al., 2021).\n\nWe study robustness-transfer across tasks and domains by aligning perturbation invariances learned from auxiliary corpora. Our method improves attack resistance in zero-shot evaluation without task-specific adversarial training.",
    "reason": "The span enumerates attacks, defenses, and benchmarks but provides no synthesis about their relation to the paper’s focus or remaining gap, covering (a) and (c).",
    "start": 207,
    "end": 668,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Brown and Taylor 2017)",
    "document": "Introduction\n\nCausal inference in observational studies requires careful design to approximate randomized experiments. Matching, weighting, and doubly robust estimators aim to balance covariates while controlling variance (Imbens and Rubin, 2015; Austin, 2019). Recent tutorials summarize identification strategies (Brown and Taylor 2017) for practitioners and emphasize sensitivity analysis to unmeasured confounding (Rosenbaum, 2019). We propose a representation learning approach that enforces balance across latent dimensions for improved generalization.",
    "reason": "Wrong citation style: missing the comma before the year and using \"and\" instead of an ampersand in a parenthetical citation. Should be \"(Brown & Taylor, 2017)\" in APA-like styles.",
    "start": 315,
    "end": 338,
    "label": "Format"
  },
  {
    "span": "Lee et al.",
    "document": "Introduction\n\nGraph contrastive learning has emerged as a powerful paradigm for representation learning on graphs (You et al., 2020; Hassani and Khasahmadi, 2020). Earlier methods focused on homogeneous graphs (Velickovic et al., 2019), while more recent works adapt contrastive objectives to heterogeneous settings (Zhang et al., 2021). Prior work by Lee et al. proposes to align node representations across meta-path views, but leaves open how to handle relation-specific noise and imbalance. In this paper, we introduce a relation-aware debiasing objective that complements heterogeneous contrast.\n\nOur approach differs from meta-path sampling schemes (Dong et al., 2017) by dynamically weighting relations during training. We also draw connections to mutual information maximization (Poole et al., 2019) and report consistent gains on standard benchmarks (Hu et al., 2020).",
    "reason": "Narrative citation is missing the publication year; should be formatted as a narrative citation with year, e.g., 'Lee et al. (2019)'.",
    "start": 352,
    "end": 362,
    "label": "Format"
  },
  {
    "span": "According to (Baker, 2015)",
    "document": "Introduction\n\nCross-lingual representation learning aims to align semantic spaces across languages for transfer to low-resource settings. According to (Baker, 2015), typological properties can inform alignment decisions, yet many neural methods ignore such structure and rely solely on distributional cues (Ruder et al., 2019). We revisit typology-informed constraints and show they complement multilingual pretraining.",
    "reason": "Wrong citation style: leading preposition plus parenthetical citation is incorrect in narrative form. It should be 'According to Baker (2015)'.",
    "start": 138,
    "end": 164,
    "label": "Format"
  },
  {
    "span": "In (Garcia et al., 2020)",
    "document": "Related Work\n\nNeural machine translation (NMT) has advanced rapidly with attention-based architectures (Bahdanau et al., 2015; Vaswani et al., 2017). Domain adaptation techniques for NMT include data selection (Axelrod et al., 2011) and synthetic data generation (Sennrich et al., 2016). In (Garcia et al., 2020) we observed severe degradation when adapting to conversational domains; their findings motivate robust training with noise-aware objectives. Subsequent work follows with contrastive finetuning to mitigate exposure bias (He et al., 2021).\n\nPretraining on multilingual corpora further improves transfer (Conneau and Lample, 2019), and curriculum schedules help stabilize adaptation (Wang et al., 2021).",
    "reason": "Wrong citation style: a parenthetical citation should not be preceded by the preposition 'In'. It should be 'In Garcia et al. (2020)' or rephrased to use a narrative citation.",
    "start": 288,
    "end": 312,
    "label": "Format"
  },
  {
    "span": "A prior study showed that ResNet-50 achieves 97% accuracy on the CheXpert dataset.",
    "document": "Related Work\nDeep convolutional neural networks have driven rapid progress in automated chest X-ray interpretation. Beyond architecture design, advances include learning from noisy labels, multi-label calibration, and uncertainty estimation. Large-scale benchmarks such as CheXpert and MIMIC-CXR have standardized evaluation protocols and improved comparability across systems.\n\nDespite these improvements, reported performance can vary substantially with label definitions, train/validation splits, and thresholding strategies. A prior study showed that ResNet-50 achieves 97% accuracy on the CheXpert dataset. However, without consistent labelers and reporting of per-pathology metrics, single-number summaries may obscure clinically relevant failure modes. In this work, we re-examine threshold selection and calibration under shift between hospital systems.",
    "reason": "This sentence states a specific prior result and names a dataset with a numeric performance figure but provides no citation (violates a and b).",
    "start": 529,
    "end": 611,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent shared tasks on hate speech and offensive language detection have shown that cross-lingual transfer is highly effective.",
    "document": "Related Work\n\nToxic content detection spans multiple subproblems, including hate speech, offensive language, and harassment identification. Multilingual scenarios are particularly challenging due to data scarcity and cultural variation in expression. Recent shared tasks on hate speech and offensive language detection have shown that cross-lingual transfer is highly effective. Methods based on multilingual pretrained models and adversarial alignment have demonstrated promise, but sensitivity to domain drift and code-switching remains an open problem.\n",
    "reason": "Refers to 'recent shared tasks' and a specific outcome without citing the shared tasks or accompanying studies.",
    "start": 251,
    "end": 378,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most existing datasets in music recommendation are heavily skewed toward Western artists.",
    "document": "Introduction\n\nMusic recommendation systems often inherit biases present in user behavior and catalog coverage. These biases can manifest along dimensions of geography, language, and genre, potentially marginalizing underrepresented creators and listeners. Most existing datasets in music recommendation are heavily skewed toward Western artists. Such imbalance not only impacts fairness but also limits model generalization in global markets.\n\nAddressing these issues requires measurement protocols that disentangle preference signals from exposure and catalog effects. We propose a dataset auditing framework that quantifies representation across regions and examines the downstream impact of reweighting strategies on both accuracy and equity.",
    "reason": "Makes a broad claim about dataset skew without any citation or evidence (rule b and e).",
    "start": 256,
    "end": 345,
    "label": "Unsupported_claim"
  },
  {
    "span": "Self-training has been shown to be especially effective on low-resource POS tagging for Uralic languages.",
    "document": "Related Work\n\nSemi-supervised learning methods such as self-training and co-training are widely used to leverage unlabeled data. Self-training has been shown to be especially effective on low-resource POS tagging for Uralic languages. Cross-lingual transfer through multilingual encoders further reduces annotation demands but may struggle with morphological richness and orthographic variation.\n\nWe examine bootstrapping strategies tailored to agglutinative morphology and evaluate gains across several small corpora with limited gold annotations.",
    "reason": "Makes a niche, language-family-specific effectiveness claim without any supporting citations (rule b).",
    "start": 129,
    "end": 234,
    "label": "Unsupported_claim"
  },
  {
    "span": "Huang et. al. (2015)",
    "document": "Related Work\n\nNeural machine translation (NMT) shifted from phrase-based systems to end-to-end encoder–decoder models with attention (Bahdanau et al., 2014; Sutskever et al., 2014; Vaswani et al., 2017). Subword modeling mitigates the out-of-vocabulary problem and improves morphological generalization (Sennrich et al., 2016; Kudo, 2018).\n\nDomain adaptation for NMT leverages fine-tuning, data selection, and multi-domain training to preserve in-domain quality while maintaining generalization (Chu and Wang, 2018; Britz et al., 2017). Quality estimation and uncertainty modeling support safer deployment in low-resource settings (Specia et al., 2018; Malinin and Gales, 2021).\n\nHuang et. al. (2015) introduce a coverage mechanism to reduce over-translation, while Tu et al. (2016) refine alignment with recurrent coverage vectors. We build upon these insights to design a lightweight coverage prior compatible with pretrained sequence-to-sequence models.",
    "reason": "Incorrect use of 'et al.': it is written as 'et. al.' with an extra period after 'et'; the correct form is 'et al.'",
    "start": 680,
    "end": 700,
    "label": "Format"
  },
  {
    "span": "A previous study demonstrated that pruning 80% of weights in ResNet-50 has negligible impact on ImageNet accuracy.",
    "document": "Related Work\n\nModel compression techniques such as pruning and quantization aim to reduce inference cost while preserving accuracy. Unstructured pruning achieves high sparsity but can be challenging to accelerate on commodity hardware, whereas structured pruning targets channels or blocks to align with efficient kernels.\n\nA previous study demonstrated that pruning 80% of weights in ResNet-50 has negligible impact on ImageNet accuracy. Subsequent work explored lottery ticket style re-initialization and dynamic sparsification schedules to improve stability during fine-tuning.\n\nWhile these approaches emphasize sparsity patterns, we focus on hardware-aware constraints and propose a co-design method that couples block pruning with kernel autotuning.",
    "reason": "This sentence references a specific prior study and result but provides no citation (guideline b and a).",
    "start": 324,
    "end": 438,
    "label": "Unsupported_claim"
  },
  {
    "span": "There are many recent works exploring multilingual pretraining for low-resource translation.",
    "document": "Related Work\n\nNeural machine translation (NMT) in low-resource settings has benefited from transfer learning, where knowledge from high-resource languages is shared with related low-resource pairs. Multilingual pretraining and parameter sharing are common strategies to reduce data requirements while maintaining translation quality.\n\nThere are many recent works exploring multilingual pretraining for low-resource translation. Approaches range from shared subword vocabularies and language-agnostic encoders to adapter-based fine-tuning and vocabulary expansion. Despite these advances, the optimal way to balance cross-lingual sharing with language-specific adaptation remains unclear.\n\nOur study compares language-agnostic encoders with lightweight adapters under strict data budgets and evaluates robustness to domain shift across a variety of typologically diverse languages.",
    "reason": "Uses a vague reference to 'many recent works' without providing citations to those works.",
    "start": 335,
    "end": 427,
    "label": "Unsupported_claim"
  },
  {
    "span": "For transfer to multiple languages, recent advances in building MPLMs (Devlin et al., 2019;Conneau et al., 2020;Liu et al., 2020;Xue et al., 2020) have been extremely effective, especially in zero-shot transfer (Pires et al., 2019;Liu et al., 2020). Ein-Dor et al. (2020) studied the dataeffectiveness of these models when used in conjunction with AL, but, as with other AL work, with a single language focus. ",
    "document": "Related Work\n\nEffective utilization of annotation budgets has been the area of focus for numerous active learning works, showing improvements for different tasks like POS tagging (Ringger et al., 2007), sentiment analysis (Karlos et al., 2012;Li et al., 2013;Brew et al., 2010;Ju and Li, 2012), syntactic parsing (Duong et al., 2018), and named entity recognition (Settles and Craven, 2008;Shen et al., 2018). The focus of most of these works, however, has been on learning for a single language (often English). Prior work on AL that uses a multilingual setup or cross-lingual information sharing and that goes beyond training a separate model for each language has thus been limited. The closest work where multiple languages influence each other's acquisition is that of Qian et al. (2014); however, they still train a separate model for each language.\n\nFor transfer to multiple languages, recent advances in building MPLMs (Devlin et al., 2019;Conneau et al., 2020;Liu et al., 2020;Xue et al., 2020) have been extremely effective, especially in zero-shot transfer (Pires et al., 2019;Liu et al., 2020). Ein-Dor et al. (2020) studied the dataeffectiveness of these models when used in conjunction with AL, but, as with other AL work, with a single language focus. Finally, Lauscher et al. (2020) studied the effectiveness of the zero-shot setup, showing that adding a few examples to a model trained on English improves performance over zero-shot transfer. However, this assumes the availability of a full English task-specific corpus.\n\n ",
    "start": 857,
    "end": 1267,
    "label": "Coherence"
  },
  {
    "span": "For transfer to multiple languages, recent advances in building MPLMs (Devlin et al., 2019;Conneau et al., 2020;Liu et al., 2020;Xue et al., 2020) have been extremely effective, especially in zero-shot transfer (Pires et al., 2019;Liu et al., 2020). Ein-Dor et al. (2020) studied the dataeffectiveness of these models when used in conjunction with AL, but, as with other AL work, with a single language focus. ",
    "document": "Related Work\n\nEffective utilization of annotation budgets has been the area of focus for numerous active learning works, showing improvements for different tasks like POS tagging (Ringger et al., 2007), sentiment analysis (Karlos et al., 2012;Li et al., 2013;Brew et al., 2010;Ju and Li, 2012), syntactic parsing (Duong et al., 2018), and named entity recognition (Settles and Craven, 2008;Shen et al., 2018). The focus of most of these works, however, has been on learning for a single language (often English). Prior work on AL that uses a multilingual setup or cross-lingual information sharing and that goes beyond training a separate model for each language has thus been limited. The closest work where multiple languages influence each other's acquisition is that of Qian et al. (2014); however, they still train a separate model for each language.\n\nFor transfer to multiple languages, recent advances in building MPLMs (Devlin et al., 2019;Conneau et al., 2020;Liu et al., 2020;Xue et al., 2020) have been extremely effective, especially in zero-shot transfer (Pires et al., 2019;Liu et al., 2020). Ein-Dor et al. (2020) studied the dataeffectiveness of these models when used in conjunction with AL, but, as with other AL work, with a single language focus. Finally, Lauscher et al. (2020) studied the effectiveness of the zero-shot setup, showing that adding a few examples to a model trained on English improves performance over zero-shot transfer. However, this assumes the availability of a full English task-specific corpus.\n\n ",
    "start": 857,
    "end": 1267,
    "label": "Coherence"
  },
  {
    "span": "Prompt-based learning has been investigated under several paradigms, including cloze-style prompts for masked language models, gradient-free prompt tuning, and instruction tuning for generalization (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022). Prior work explores hand-crafted templates, discrete prompt search, and continuous soft prompts, as well as task-specific calibrations and verbalizer design, across benchmarks such as SuperGLUE and various text classification suites.",
    "document": "Related Work: Prompt-based Learning for Text Classification\n\nPrompting reformulates downstream tasks as text completion or token prediction, allowing pretrained language models to leverage their latent knowledge with minimal task-specific adaptation. Early studies showed that even simple cloze templates can substantially reduce the need for supervised data by aligning the task with the model's pretraining objective.\n\nPrompt-based learning has been investigated under several paradigms, including cloze-style prompts for masked language models, gradient-free prompt tuning, and instruction tuning for generalization (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022). Prior work explores hand-crafted templates, discrete prompt search, and continuous soft prompts, as well as task-specific calibrations and verbalizer design, across benchmarks such as SuperGLUE and various text classification suites.\n\nBeyond templates, calibration and label-space engineering methods have been proposed to mitigate bias due to surface forms and class imbalance. Parameter-efficient fine-tuning further reduces compute and memory requirements by adapting a small set of task-specific parameters while keeping the backbone frozen.\n\nIn this paper, we examine few-shot text classification through lightweight prompt mechanisms paired with stability-oriented selection strategies.",
    "reason": "The span enumerates prior prompting approaches and benchmarks but does not connect them to the paper's goals, limitations being addressed, or how the present work differs (criteria a and c).",
    "start": 421,
    "end": 928,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Distillation has been extended to transformer architectures with teacher–student frameworks (Hinton et al., 2015; Touvron et al., 2021; Yuan et al., 2021).",
    "document": "Introduction\n\nVision transformers (ViTs) achieve strong recognition performance but often require large datasets and compute. Model compression aims to reduce inference cost while preserving accuracy through pruning, quantization, and knowledge distillation.\n\nDistillation has been extended to transformer architectures with teacher–student frameworks (Hinton et al., 2015; Touvron et al., 2021; Yuan et al., 2021). Other efforts transfer intermediate representations, attention maps, or token-level features for improved student supervision (Zhang et al., 2022; Tung and Mori, 2019; Park et al., 2019). Data-free approaches synthesize pseudo-data to facilitate distillation when training data cannot be shared (Micaelli and Storkey, 2019; Yin et al., 2020).\n\nHardware-aware optimization integrates latency and memory constraints into the training objective to target specific devices, complementing distillation-based compression (Cai et al., 2020; Bender et al., 2018).",
    "reason": "The span enumerates prior distillation efforts for transformers without clarifying their limitations, the authors' stance, or how their method fits into or advances beyond these works, thus lacking synthesis.",
    "start": 260,
    "end": 415,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Mnih et al. (2015) introduced DQN for value-based control from pixels. Lillicrap et al. (2016) proposed DDPG for continuous actions. Kahn et al. (2017) studied safe navigation with model-based uncertainty. Tai et al. (2017) investigated mapless navigation using deep reinforcement learning.",
    "document": "Related Work\n\nReinforcement learning for robot navigation addresses perception, control, and safety under partial observability and real-world disturbances. Methods span model-free value and policy learning as well as model-based planning with uncertainty estimation.\n\nMnih et al. (2015) introduced DQN for value-based control from pixels. Lillicrap et al. (2016) proposed DDPG for continuous actions. Kahn et al. (2017) studied safe navigation with model-based uncertainty. Tai et al. (2017) investigated mapless navigation using deep reinforcement learning.\n\nWe build on off-policy learning with constrained optimization to ensure collision avoidance while maintaining sample efficiency in mobile platforms.",
    "reason": "The span lists four papers in isolation without transitions or explicit relational framing, yielding abrupt connections and leaving relationships implicit (issues a and b).",
    "start": 269,
    "end": 559,
    "label": "Coherence"
  },
  {
    "span": "Earlier papers report that unsupervised domain adaptation yields over 10% improvement in Dice score on cross-site segmentation.",
    "document": "Related Work\n\nDomain shift across scanners and institutions hinders the deployment of medical image segmentation models (Ghafoorian et al., 2017; Kamnitsas et al., 2017). Unsupervised domain adaptation (UDA) aligns source and target distributions without target labels through adversarial learning, self-training, or style transfer. Earlier papers report that unsupervised domain adaptation yields over 10% improvement in Dice score on cross-site segmentation. However, these gains can be sensitive to target-domain idiosyncrasies and hyperparameters, complicating clinical translation. We study stability via consistency-regularized self-training with uncertainty-aware pseudo-label selection.\n\nWe further analyze failure modes in head-and-neck CT and prostate MRI to guide practical deployment.",
    "reason": "This numerical performance claim attributes results to 'earlier papers' but provides no citations to support the statistic.",
    "start": 333,
    "end": 460,
    "label": "Unsupported_claim"
  },
  {
    "span": "TransE embeds relations as translations in vector space (Bordes et al., 2013). R-GCN propagates information along typed edges (Schlichtkrull et al., 2018). Rule mining learns Horn clauses to infer facts (Galárraga et al., 2015). Pretrained language models encode textual descriptions (Yao et al., 2019).",
    "document": "Related Work\n\nKnowledge Graph Completion\n\nKGC methods fall into embedding-based approaches, graph neural methods, symbolic rule mining, and text-enhanced models. While each family addresses different challenges such as sparsity and interpretability, their integration is still evolving.\n\nMethod Families\n\nTransE embeds relations as translations in vector space (Bordes et al., 2013). R-GCN propagates information along typed edges (Schlichtkrull et al., 2018). Rule mining learns Horn clauses to infer facts (Galárraga et al., 2015). Pretrained language models encode textual descriptions (Yao et al., 2019).\n\nOur Contribution\n\nWe introduce a hybrid system that couples relational embeddings with rule-regularized message passing and text-conditioned priors, achieving improvements on long-tail relations.",
    "reason": "The span enumerates four method families with no transitions or explanations of their relationships. The reader must infer connections among embeddings, GNNs, rule mining, and text-enhanced models, leading to an abrupt, coherence-lacking sequence.",
    "start": 305,
    "end": 608,
    "label": "Coherence"
  },
  {
    "span": "Baker et al., (2015)",
    "document": "Introduction\n\nTopic modeling discovers latent structure in document collections and supports exploratory analysis, recommendation, and summarization (Blei et al., 2003; Griffiths and Steyvers, 2004). Neural variants replace conjugate priors with amortized inference and deep generative decoders (Miao et al., 2016; Srivastava and Sutton, 2017).\n\nAnchored and guided topic models incorporate weak supervision or constraints to steer semantics and improve coherence (Jagarlamudi et al., 2012; Hu et al., 2020). Baker et al., (2015) introduce interactive topic refinement with user feedback, showing gains in interpretability over static models.\n\nWe extend this paradigm with differentiable constraints learned from user actions, enabling continuous adaptation during analysis sessions.",
    "reason": "Comma placed before the parenthetical year in a narrative citation. Should be 'Baker et al. (2015)'.",
    "start": 509,
    "end": 529,
    "label": "Format"
  },
  {
    "span": "A prior paper introduced the dynamic memory routing mechanism for few-shot image classification.",
    "document": "Introduction\n\nFew-shot learning methods leverage metric learning, meta-learning, or data augmentation to generalize from sparse labels (Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017). Memory-augmented networks store class-level prototypes or examples to support rapid adaptation, often with attention-based retrieval (Santoro et al., 2016; Sun et al., 2019).\n\nA prior paper introduced the dynamic memory routing mechanism for few-shot image classification.\n\nWe build on memory routing with a stability-regularized objective that mitigates prototype drift across episodes, achieving improved calibration under distribution shift.",
    "reason": "Mentions a specific prior mechanism ('dynamic memory routing') but provides no citation to the paper; first mention of a study should be cited.",
    "start": 374,
    "end": 470,
    "label": "Unsupported_claim"
  },
  {
    "span": "Isolation Forest detects anomalies by random partitioning of feature space (Liu et al., 2008). LSTM-based predictors model temporal dependencies for anomaly scoring (Malhotra et al., 2015). Seasonal-trend decomposition separates periodicity from residuals (Cleveland et al., 1990). Variational autoencoders learn latent representations for reconstruction-based detection (Kingma and Welling, 2014).",
    "document": "Related Work\n\nTime-series Anomaly Detection\n\nDetecting anomalies in time-series is crucial for monitoring industrial systems and services. Methods include classical statistical modeling, distance-based detection, and modern deep generative approaches. Key challenges involve nonstationarity, seasonality, and limited labels.\n\nClassical, Predictive, and Generative Models\n\nIsolation Forest detects anomalies by random partitioning of feature space (Liu et al., 2008). LSTM-based predictors model temporal dependencies for anomaly scoring (Malhotra et al., 2015). Seasonal-trend decomposition separates periodicity from residuals (Cleveland et al., 1990). Variational autoencoders learn latent representations for reconstruction-based detection (Kingma and Welling, 2014).\n\nOur Contribution\n\nWe introduce a seasonality-aware probabilistic forecaster with conformalized residual scoring that adapts to regime shifts via online calibration.",
    "reason": "The span lists heterogeneous methods across isolation-based, predictive, decomposition, and generative approaches with no transitions or explicit comparisons; relationships are left implied, reducing coherence.",
    "start": 372,
    "end": 770,
    "label": "Coherence"
  },
  {
    "span": "In (Liu et al., 2019)",
    "document": "Related Work\n\nOpen-Domain Question Answering\n\nOpen-domain question answering (ODQA) has evolved from sparse retrieval plus extractive readers (Chen et al., 2017) to dense retrievers paired with generative readers (Karpukhin et al., 2020; Izacard and Grave, 2021). Recent pipelines leverage pre-trained sequence-to-sequence models to synthesize answers from multiple passages (Lewis et al., 2020; Petroni et al., 2020). In (Liu et al., 2019) a hybrid retriever was proposed to combine term-based and embedding-based signals, showing gains over BM25 across diverse domains. Concurrently, atlas-style models align retrieval and generation through joint training (Izacard et al., 2022), while iterative reranking further improves evidence aggregation (Mao et al., 2021). Despite these advances, calibration and faithfulness remain challenging (Min et al., 2021; Longpre et al., 2021), motivating our approach to integrate confidence-aware decoding with retrieval regularization.",
    "reason": "Wrong citation style: narrative use should be 'in Liu et al. (2019)' rather than 'In (Liu et al., 2019)'.",
    "start": 419,
    "end": 440,
    "label": "Format"
  },
  {
    "span": "Back-translation has become the de facto standard for data augmentation in NMT.",
    "document": "Related Work\n\nData augmentation is a cornerstone of improving NMT performance, especially for low-resource language pairs. Synthetic data generation from monolingual corpora can expand training coverage and reduce overfitting to limited parallel data. Back-translation has become the de facto standard for data augmentation in NMT. Alternatives such as iterative forward–backward translation and noise-based perturbations have also been explored, yet their effectiveness varies with language morphology and domain mismatch. Our work revisits augmentation under strict compute budgets.",
    "reason": "This is a field-level prior work claim stated without citation, violating rule (a) and (b).",
    "start": 252,
    "end": 331,
    "label": "Unsupported_claim"
  },
  {
    "span": "Turner and Singh 2",
    "document": "Introduction\n\nTopic modeling infers latent structure in document collections. Latent Dirichlet allocation formalized mixture-based semantics (Blei et al., 2003), while supervised variants incorporate labels (Mcauliffe and Blei, 2008). Neural topic models improve coherence using amortized inference (Miao et al., 2017; Dieng et al., 2020). Turner and Singh 2 introduce a hierarchical prior to couple topics across domains, but their evaluation omits short texts and conversational data. We revisit this setting with length-robust priors and rigorous held-out likelihood estimation.",
    "reason": "Improper footnote-like marker used with an author citation; should include a year or be formatted as a proper footnote/endnote.",
    "start": 340,
    "end": 358,
    "label": "Format"
  },
  {
    "span": "Eye-tracking studies show that users largely ignore sidebar explanations when making decisions under time pressure.",
    "document": "Related Work\n\nExplainable interfaces aim to help users calibrate trust in AI recommendations. Placement, modality, and timing of explanations can dramatically affect whether users engage with them.\n\nEye-tracking studies show that users largely ignore sidebar explanations when making decisions under time pressure. These findings motivate designs that interleave just-in-time explanations within the main interaction flow rather than relegating them to peripheral UI regions.",
    "reason": "Invokes prior empirical studies and a behavioral finding without providing any citation; per rule (b), such domain-specific claims must be cited.",
    "start": 199,
    "end": 314,
    "label": "Unsupported_claim"
  },
  {
    "span": "The majority of prior work treats code summarization as a pure sequence-to-sequence problem.",
    "document": "Related Work\n\nCode summarization aims to generate natural language descriptions of source code functionality. Early approaches relied on retrieving comments or using template-based methods, while neural models popularized sequence transduction from code tokens to text. The majority of prior work treats code summarization as a pure sequence-to-sequence problem. More recently, structured representations such as abstract syntax trees and control-flow graphs have been incorporated via graph encoders to better capture program semantics. Complementary efforts address copy mechanisms, identifier splitting, and subword modeling to handle vocabulary sparsity. Our approach integrates structural bias while maintaining scalability to large repositories.",
    "reason": "Claims a dominant trend in prior literature without supporting citations (rule b).",
    "start": 270,
    "end": 362,
    "label": "Unsupported_claim"
  },
  {
    "span": "Morris et al. 2",
    "document": "Related Work\n\nTopic modeling has evolved from probabilistic graphical models to neural variational formulations. Morris et al. 2 argued that interpretability can be improved by sparsity-inducing priors, while subsequent work used amortized inference with encoder networks to scale to large corpora (Srivastava and Sutton, 2017). Anchored topic models constrain word–topic assignments to enforce semantic coherence (Arora et al., 2013), and neural topic models incorporate contextual embeddings to capture polysemy (Bianchi et al., 2021). However, reconciling coherence with diversity remains challenging.\n\nOur method enforces diversity through determinantal point processes while preserving coherence via semantic anchors.",
    "reason": "Wrong use of a footnote-like numeral without proper citation formatting; it should include the year (e.g., 'Morris et al. (YEAR)') or be formatted as a proper footnote.",
    "start": 113,
    "end": 128,
    "label": "Format"
  },
  {
    "span": "(Li and Zhao, 2020",
    "document": "Related Work. Active learning (AL) strategies for sequence labeling often rely on uncertainty estimates such as maximum entropy or margin sampling (Settles, 2009; Culotta & McCallum, 2005). With modern transformers, token-level uncertainty can be aggregated at the span or sentence level to prioritize informative annotations (Shelmanov et al., 2021; Ein-Dor et al., 2020). Committee-based methods compare predictions of heterogeneous models to mitigate estimator bias (Seung et al., 1992; Beluch et al., 2018). Recent work explores density-aware selection to avoid redundant samples (Li and Zhao, 2020 while balancing exploration and exploitation through batch-mode optimization (Ash et al., 2020). Our approach integrates density and diversity while preserving computational efficiency.",
    "reason": "Missing closing parenthesis in a parenthetical citation: '(Li and Zhao, 2020' should be '(Li and Zhao, 2020)'.",
    "start": 584,
    "end": 602,
    "label": "Format"
  },
  {
    "span": "Recent studies apply graph neural networks with temporal models to traffic prediction (Li et al., 2018; Wu et al., 2019; Bai et al., 2020; Song et al., 2020).",
    "document": "Related Work\n\nTraffic forecasting. Accurate short-term traffic forecasting is essential for congestion mitigation and route planning. Traditional approaches rely on statistical and shallow learning models that struggle to capture complex spatiotemporal dependencies across urban road networks. Recent studies apply graph neural networks with temporal models to traffic prediction (Li et al., 2018; Wu et al., 2019; Bai et al., 2020; Song et al., 2020). While these directions collectively highlight the promise of graph-based modeling, a comprehensive treatment of uncertainty and event-driven disruptions remains underexplored in the literature.\n\nOur setting. We consider the urban road network as a directed graph with dynamic edge attributes (speed, volume, occupancy). We study 5–30 minute horizons and emphasize robustness under incident-induced distribution shifts.\n\nOur contribution. We introduce an incident-aware spatiotemporal graph forecaster that fuses graph diffusion dynamics with a learned event encoder, demonstrating improved accuracy and calibrated uncertainty during disruptions.",
    "reason": "The sentence lists prior GNN-based traffic forecasting works without explaining how they relate to the present study or what gap persists, failing to synthesize the cited literature with the paper's aims.",
    "start": 294,
    "end": 452,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[Devlin et al., 2019)",
    "document": "Introduction\n\nSelf-supervised pretraining has become the foundation for modern NLP systems. Cloze-style masked language modeling (MLM) and next-sentence prediction established strong baselines with bidirectional encoders such as BERT [Devlin et al., 2019) and subsequent variants emphasizing larger corpora and longer training (Liu et al., 2019; Clark et al., 2020). Sequence-to-sequence pretraining offers broader transfer to generation tasks (Lewis et al., 2020; Raffel et al., 2020).\n\nScaling compute and data improves downstream accuracy, but raises issues of efficiency, domain shift, and safety (Kaplan et al., 2020; Dodge et al., 2020). We address these issues by exploring adaptive pretraining curricula and parameter-efficient transfer.",
    "reason": "Mismatched brackets and wrong citation style: opening square bracket with closing parenthesis; should be parenthetical author-year, e.g., \"(Devlin et al., 2019)\".",
    "start": 234,
    "end": 255,
    "label": "Format"
  },
  {
    "span": "Rosenbaum and Rubin (1983) introduced propensity score methods. Angrist and Imbens (1995) analyzed instrumental variables. Shalit et al. (2017) proposed CFR for counterfactual prediction. Hassanpour and Greiner (2019) modeled hidden confounding with representation learning.",
    "document": "Related Work\n\nCausal inference in healthcare requires estimating treatment effects under selection bias and unobserved confounding, often from observational electronic health records. Methods span classical identification strategies and modern representation learning.\n\nRosenbaum and Rubin (1983) introduced propensity score methods. Angrist and Imbens (1995) analyzed instrumental variables. Shalit et al. (2017) proposed CFR for counterfactual prediction. Hassanpour and Greiner (2019) modeled hidden confounding with representation learning.\n\nWe integrate domain knowledge with representation constraints to improve identifiability in longitudinal cohorts where overlap and instrument validity are uncertain.",
    "reason": "The paragraph enumerates distinct causal tools without transitions or clarifying how they relate, resulting in abrupt shifts between identification strategies and modern deep methods.",
    "start": 270,
    "end": 544,
    "label": "Coherence"
  },
  {
    "span": "Graves et al. (2006) introduce CTC for sequence labeling. Hannun et al. (2014) demonstrate end-to-end ASR with deep speech. Chan et al. (2016) propose Listen, Attend and Spell. Gulati et al. (2020) present Conformer for speech recognition.",
    "document": "Related Work\n\nEnd-to-End Speech Recognition\n\nWe discuss models that map acoustics directly to text without lexicons. Graves et al. (2006) introduce CTC for sequence labeling. Hannun et al. (2014) demonstrate end-to-end ASR with deep speech. Chan et al. (2016) propose Listen, Attend and Spell. Gulati et al. (2020) present Conformer for speech recognition. Complementary advances in self-supervised pretraining have improved label efficiency (Baevski et al., 2020; Hsu et al., 2021).\n\nOur contribution studies streaming constraints with minimal latency degradation.",
    "reason": "A sequence of citations is provided without transitional language; the relationships and evolution among CTC, attention, and Conformer models are left implicit.",
    "start": 117,
    "end": 356,
    "label": "Coherence"
  },
  {
    "span": "(Kim and Park, 2017",
    "document": "Related Work\n\nSequence labeling. CRF-based methods dominated early NER systems due to their ability to capture label dependencies (Lafferty et al., 2001). With neural encoders, BiLSTM-CRF models improved both precision and recall by leveraging character and word features (Ma and Hovy, 2016). Later, subword-aware Transformers further advanced the state of the art, outperforming earlier encoding schemes (Kim and Park, 2017 on multilingual benchmarks by exploiting byte-level regularities.\n",
    "reason": "Unbalanced parentheses: missing a closing parenthesis. It should be \"(Kim and Park, 2017)\".",
    "start": 405,
    "end": 424,
    "label": "Format"
  },
  {
    "span": "Robotic grasping has been approached with analytic methods grounded in force-closure and wrench space analysis. Data-driven methods train convolutional networks on depth or RGB images to predict grasp affordances. Reinforcement learning formulations optimize success rates via trial-and-error with shaped or sparse rewards. Attention mechanisms and transformer-based policies have been explored for cluttered scenes and multi-object manipulation. We propose an attention-guided policy trained with off-policy data.",
    "document": "Related Work\n\nReliable robotic grasping in unstructured environments remains a central challenge due to sensing noise, object diversity, and contact uncertainties. Approaches span analytic modeling, supervised learning, and reinforcement learning.\n\nRobotic grasping has been approached with analytic methods grounded in force-closure and wrench space analysis. Data-driven methods train convolutional networks on depth or RGB images to predict grasp affordances. Reinforcement learning formulations optimize success rates via trial-and-error with shaped or sparse rewards. Attention mechanisms and transformer-based policies have been explored for cluttered scenes and multi-object manipulation. We propose an attention-guided policy trained with off-policy data.\n\nWe assess performance on both simulated and real bins, measuring grasp success, throughput, and generalization to novel objects.",
    "reason": "The span enumerates prior analytic, supervised, and RL approaches and introduces the authors' idea without integrating these strands or identifying a specific weakness that the attention-guided policy addresses.",
    "start": 249,
    "end": 763,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Nguyen et al., 2017",
    "document": "Related Work\n\nCross-lingual transfer leverages multilingual representations to improve performance in low-resource languages (Ruder et al., 2019; Conneau et al., 2020). Prior work shows substantial gains from shared subword vocabularies and aligned spaces, as shown by Nguyen et al., 2017 in low-resource tagging and parsing. Recent approaches integrate adapters and prompt tuning to further reduce task-specific parameters (Pfeiffer et al., 2020; Houlsby et al., 2019).\n",
    "reason": "Parenthetical citation missing parentheses: it appears as author-year with a comma in running text; it should be either \"Nguyen et al. (2017)\" for narrative or \"(Nguyen et al., 2017)\" for parenthetical style.",
    "start": 269,
    "end": 288,
    "label": "Format"
  },
  {
    "span": "Anomaly detection for multivariate time series employs autoencoders and RNNs to model normality (Malhotra et al., 2016; Audibert et al., 2020), probabilistic models such as VAR and state-space models (Tsay, 2005; Tank et al., 2021), and graph-based temporal models to capture dependencies (Deng and Hooi, 2021; Chen et al., 2021). Streaming scenarios consider sketching and online change detection (Bifet and Gavalda, 2007; Aminikhanghahi and Cook, 2017).",
    "document": "Related Work Detecting anomalies in IoT sensor networks is vital for early fault detection and security monitoring. Multivariate dependencies, non-stationarity, and limited labels complicate modeling. Anomaly detection for multivariate time series employs autoencoders and RNNs to model normality (Malhotra et al., 2016; Audibert et al., 2020), probabilistic models such as VAR and state-space models (Tsay, 2005; Tank et al., 2021), and graph-based temporal models to capture dependencies (Deng and Hooi, 2021; Chen et al., 2021). Streaming scenarios consider sketching and online change detection (Bifet and Gavalda, 2007; Aminikhanghahi and Cook, 2017). We target resource-constrained gateways and propose a lightweight graph-forecasting approach with uncertainty-based alarms that adapts neighbor structure online under a fixed memory budget. On three public IoT benchmarks, our method achieves higher F1 and lower latency than strong baselines.",
    "reason": "The span lists families of methods and streaming techniques but does not connect them to the constraints or motivations of the proposed lightweight approach. It lacks synthesis of prior work relative to the paper's goals (a, c).",
    "start": 201,
    "end": 656,
    "label": "Lacks_synthesis"
  },
  {
    "span": "A large body of work has shown that popularity bias is the dominant source of unfair exposure in music recommenders.",
    "document": "Introduction\n\nFairness in recommender systems concerns equitable exposure and utility across items and users, beyond pure accuracy optimization (Burke, 2017; Ekstrand et al., 2021). In music platforms, exposure disparities can disadvantage niche artists, raising concerns about cultural diversity and creator livelihoods (Biega et al., 2018; Abdollahpouri et al., 2019). Mitigation strategies include re-ranking, regularization, and counterfactual evaluation (Singh and Joachims, 2018; Steiner et al., 2020).\n\nA large body of work has shown that popularity bias is the dominant source of unfair exposure in music recommenders. However, measuring dominance depends on the choice of exposure metric and catalog slice, suggesting the need for standardized protocols.\n\nWe contribute a counterfactual exposure estimator and controlled simulations to disentangle catalog and user-side effects.",
    "reason": "Asserts a consensus in prior literature about the dominant cause of unfair exposure without citing any of that literature (rule b and d).",
    "start": 510,
    "end": 626,
    "label": "Unsupported_claim"
  },
  {
    "span": "Neelakantan et al. (2016) developed neural programmers for differentiable reasoning over tables. Pasupat and Liang (2015) introduced the WikiTableQuestions dataset. Zhang and Sun (2019) proposed a graph-based reasoning model over table schemas.",
    "document": "Related Work\n\nQuestion answering over structured tables requires parsing, reasoning, and execution over semi-structured data. Methods vary from semantic parsing with discrete operations to neural execution and hybrid approaches.\n\nNeelakantan et al. (2016) developed neural programmers for differentiable reasoning over tables. Pasupat and Liang (2015) introduced the WikiTableQuestions dataset. Zhang and Sun (2019) proposed a graph-based reasoning model over table schemas. Weak supervision has been used to scale training without logical forms (Dong et al., 2019; Chen et al., 2020).\n\nEvaluation often reports denotation accuracy and execution consistency across variants (Zhong et al., 2017; Herzig et al., 2020).",
    "reason": "A dataset citation is placed between two modeling papers without clarifying its role, creating an implied relationship that is never stated or transitioned.",
    "start": 230,
    "end": 474,
    "label": "Coherence"
  },
  {
    "span": "Common Crawl has become the de facto pretraining corpus for large language models.",
    "document": "Introduction\n\nText pretraining at scale has transformed natural language processing, enabling strong transfer across tasks. Common Crawl has become the de facto pretraining corpus for large language models. While web-scale corpora offer coverage, they also introduce duplication, toxicity, and regional biases. We investigate data curation strategies that improve downstream safety without sacrificing performance.",
    "reason": "Asserts dominance of a specific dataset without evidence or citations; per rules (a) and (b), this claim should be supported by references.",
    "start": 124,
    "end": 206,
    "label": "Unsupported_claim"
  },
  {
    "span": "Exposure-aware ranking redistributes recommendation opportunities across items and providers (Abdollahpouri et al., 2017). Calibration methods align user-level recommendation distributions with historical preferences (Steck, 2018). Algorithmic fairness frameworks formalize disparate impact and treatment (Narayanan, 2018). Reinforcement learning has been applied to dynamically adjust recommendations (Liu et al., 2020).",
    "document": "Related Work\n\nFairness and Accountability in Recommender Systems\n\nRecommender systems can inadvertently amplify popularity bias and reduce visibility for minority providers. Fairness-aware approaches seek to balance platform utility with stakeholder equity, considering users, items, and creators.\n\nBias Mitigation and Dynamic Control\n\nExposure-aware ranking redistributes recommendation opportunities across items and providers (Abdollahpouri et al., 2017). Calibration methods align user-level recommendation distributions with historical preferences (Steck, 2018). Algorithmic fairness frameworks formalize disparate impact and treatment (Narayanan, 2018). Reinforcement learning has been applied to dynamically adjust recommendations (Liu et al., 2020).\n\nOur Contribution\n\nWe propose a constrained policy optimization framework that enforces multi-stakeholder exposure targets with probabilistic guarantees, learning from logged bandit feedback under covariate shift.",
    "reason": "The span enumerates distinct strands (exposure, calibration, fairness definitions, RL control) without transitions or explanation of their interdependence, making the connections between cited works unclear.",
    "start": 336,
    "end": 757,
    "label": "Coherence"
  },
  {
    "span": "The CHiME-6 dataset provides more than 10,000 hours of noisy conversational speech.",
    "document": "Related Work\n\nSingle- and multi-channel speech enhancement has evolved from classical spectral subtraction and Wiener filtering to deep architectures that jointly estimate magnitude and phase. Progress has been closely tied to the availability of realistic corpora recorded in everyday environments. The CHiME-6 dataset provides more than 10,000 hours of noisy conversational speech. Beyond scale, in-the-wild reverberation and overlapping speakers remain major challenges for supervised training, prompting interest in self-supervised objectives and semi-synthetic mixtures. While dereverberation and target speaker extraction have improved front-end quality, downstream recognition remains sensitive to microphone placement and room geometry. Our work addresses this gap by integrating geometric priors into a causal enhancement model designed for far-field, multi-talker households.",
    "reason": "Claims a specific dataset size without citing the dataset or any source (rule a and b).",
    "start": 300,
    "end": 383,
    "label": "Unsupported_claim"
  },
  {
    "span": "(2017)",
    "document": "Introduction\n\nData augmentation has become a central technique for improving the generalization of neural models in low-resource scenarios (Wei and Zou, 2019; Fadaee et al., 2017). Back-translation became popular after (2017), enabling the exploitation of large monolingual corpora for machine translation. Subsequent approaches refined sampling strategies and noise models to avoid distributional artifacts (Edunov et al., 2018; Caswell et al., 2019). Our approach extends augmentation to structured prediction via constraint-aware paraphrasing.",
    "reason": "Year-only parenthetical citation without authors; should include an author–year reference like \"(Author, 2017)\" or a narrative form with author and year.",
    "start": 219,
    "end": 225,
    "label": "Format"
  },
  {
    "span": "(O'Neil et al., 2022; ; Zhao, 2023)",
    "document": "Introduction\n\nAuditing algorithms for disparate impact requires metrics and interventions that remain effective under shifting demographics and feedback loops (Barocas and Selbst, 2016; Chouldechova, 2017). Post-processing methods equalize error rates but may sacrifice calibration, whereas pre-processing aims to debias representations upstream (Hardt et al., 2016; Feldman et al., 2015).\n\nCausal approaches identify pathways of unfairness and propose counterfactual repairs, yet scalability and identifiability remain challenging in practice (Kusner et al., 2017; Kilbertus et al., 2017). Recent adaptive audits consider policy-induced shifts and measurement error (O'Neil et al., 2022; ; Zhao, 2023) but provide limited guidance for online deployment.\n\nWe introduce an online auditing framework with uncertainty-aware thresholds that provably control worst-case disparities under covariate shift.",
    "reason": "Double semicolon inside the parenthetical citation list. Should be a single separator between citations.",
    "start": 667,
    "end": 702,
    "label": "Format"
  },
  {
    "span": "Recurrent models capture temporal dependencies in EHR sequences (Lipton et al., 2016). Phenotyping frameworks map codes to clinically meaningful labels (Zhang et al., 2018). De-identification pipelines remove protected health information from clinical notes (Dernoncourt et al., 2017).",
    "document": "Related Work\n\nPredictive modeling with electronic health records (EHR) requires integrating structured codes, time series, and unstructured notes. Prior research has proposed architectures and preprocessing pipelines tailored to these modalities, but their interdependencies are often underexplored.\n\nRecurrent models capture temporal dependencies in EHR sequences (Lipton et al., 2016). Phenotyping frameworks map codes to clinically meaningful labels (Zhang et al., 2018). De-identification pipelines remove protected health information from clinical notes (Dernoncourt et al., 2017). Graph-based methods further capture patient similarity across cohorts (Choi et al., 2020). However, it remains unclear how upstream preprocessing choices influence downstream risk prediction under distribution shift.\n\nOur study quantifies sensitivity to code grouping, note redaction, and visit windowing, providing recommendations for reproducible EHR modeling.",
    "reason": "The span strings together three distinct EHR topics—sequence modeling, phenotyping, and de-identification—without transitions or explanation of how they relate. Coherence suffers because the relationships are implied but unstated.",
    "start": 301,
    "end": 586,
    "label": "Coherence"
  },
  {
    "span": "Veitch et al. (2020) discussed text as a proxy for latent confounders in causal studies. Roberts et al. (2016) proposed STM for estimating topic effects. Fong and Grimmer (2016) addressed the identification of treatment effects with text. Egami et al. (2018) introduced a framework for the measurement of concepts with supervised learning.",
    "document": "Introduction\n\nCausal inference with text seeks to draw valid conclusions from observational corpora where language may mediate, confound, or proxy variables of interest (Grimmer et al., 2022). Core challenges include identification, measurement validity, and post-selection inference with high-dimensional representations (Athey and Imbens, 2017; Wager and Athey, 2018).\n\nVeitch et al. (2020) discussed text as a proxy for latent confounders in causal studies. Roberts et al. (2016) proposed STM for estimating topic effects. Fong and Grimmer (2016) addressed the identification of treatment effects with text. Egami et al. (2018) introduced a framework for the measurement of concepts with supervised learning.\n\nWhile these works highlight key pitfalls and opportunities, practical pipelines still lack guarantees when using modern embeddings. We contribute identification-aware representation learning with post-hoc sensitivity analysis tailored to text-derived covariates.",
    "reason": "The span presents four separate contributions without clarifying the relationships among them (e.g., how STM ties to proxy adjustment or concept measurement), and there are no transitions linking the sentences.",
    "start": 372,
    "end": 711,
    "label": "Coherence"
  },
  {
    "span": "(Nguyen and Park, 2017)",
    "document": "Related Work\n\nModel-based reinforcement learning promises improved sample efficiency by leveraging learned dynamics models (Sutton and Barto, 2018; Chua et al., 2018). Planning with uncertainty-aware models can reduce compounding error (Deisenroth and Rasmussen, 2011; Janner et al., 2019). Meanwhile, model-free methods remain state of the art on high-dimensional benchmarks (Mnih et al., 2015; Lillicrap et al., 2016).\n\nHybrid approaches combine policy gradients with short-horizon model rollouts to stabilize training and improve data efficiency, as demonstrated by (Nguyen and Park, 2017) and extended in PETS-style ensembles (Chua et al., 2018). Our method augments this line with a learned planner that adapts rollout horizons based on epistemic uncertainty.\n",
    "reason": "Wrong citation style: narrative context uses a parenthetical citation; should be \"as demonstrated by Nguyen and Park (2017)\".",
    "start": 569,
    "end": 592,
    "label": "Format"
  },
  {
    "span": "Molecular property prediction has progressed with message passing neural networks, graph attention mechanisms, and spectral methods that encode local neighborhoods and edge attributes (Gilmer et al., 2017; Velickovic et al., 2018; Kearnes et al., 2016; Duvenaud et al., 2015; Xu et al., 2019). Pre-training strategies on large unlabeled chemical graphs have also been explored (Hu et al., 2020; Rong et al., 2020; You et al., 2020).",
    "document": "Introduction\n\nAccurate molecular property prediction accelerates discovery by reducing iterative synthesis and assay costs. While graph neural networks (GNNs) dominate current practice, challenges remain in low-data regimes and in transferring knowledge across related endpoints. We examine multi-task settings with imbalanced labels and scarce positives.\n\nMolecular property prediction has progressed with message passing neural networks, graph attention mechanisms, and spectral methods that encode local neighborhoods and edge attributes (Gilmer et al., 2017; Velickovic et al., 2018; Kearnes et al., 2016; Duvenaud et al., 2015; Xu et al., 2019). Pre-training strategies on large unlabeled chemical graphs have also been explored (Hu et al., 2020; Rong et al., 2020; You et al., 2020).\n\nIn contrast to existing single-task pre-training, we propose a label-balanced joint contrastive objective that leverages cross-endpoint co-occurrence, enabling transfer to endpoints with limited positives.",
    "reason": "The span summarizes prior GNNs and pre-training work without connecting them to the paper’s multi-task, imbalanced-label motivation or identifying why those methods fall short (criteria a and c).",
    "start": 357,
    "end": 789,
    "label": "Lacks_synthesis"
  },
  {
    "span": "RLHF has rapidly become the standard for aligning large language models.",
    "document": "Introduction\n\nLarge language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but alignment with human preferences and safety norms remains a key challenge. Instruction tuning and preference optimization are two complementary strategies to steer model behavior (Brown et al., 2020; Ouyang et al., 2022).\n\nRLHF has rapidly become the standard for aligning large language models. Despite its success, RL-based optimization can be unstable and sensitive to reward model misspecification. We present a direct preference optimization approach that avoids policy rollouts while achieving comparable alignment quality.",
    "reason": "Makes a broad, field-wide claim about the standard methodology without citing supporting surveys or sources; such assertions need evidence.",
    "start": 334,
    "end": 406,
    "label": "Unsupported_claim"
  },
  {
    "span": "[Johnson et al., 2020)",
    "document": "Related Work\n\nProgram synthesis has benefited from neural sequence models that learn from input-output examples (Devlin et al., 2017; Balog et al., 2017) and from structured search guided by learned heuristics (Neelakantan et al., 2016; Ellis et al., 2018). Neuro-symbolic methods combine differentiable components with symbolic execution to improve generalization (Parisotto et al., 2017; Bunel et al., 2018).\n\nGrammar-constrained decoders have been effective for generating well-typed programs (Yin and Neubig, 2017; Rabinovich et al., 2017), and execution-guided decoding can prune inconsistent candidates (Chen et al., 2019; Sun et al., 2019). Pretrained transformers further boost performance on code generation and repair (Chen et al., 2021; Wang et al., 2021).\n\nClosest to our setting are systems that infer transformation pipelines from examples and weak supervision [Johnson et al., 2020). We differ by introducing a verification loop that learns to critique and repair candidate programs using counterfactual inputs.\n\nBenchmarks include spreadsheet manipulation and data cleaning tasks, where correctness and robustness to distribution shift are critical.",
    "reason": "Mismatched brackets in a citation; it begins with \"[\" and ends with \")\", violating consistent parenthetical formatting.",
    "start": 875,
    "end": 897,
    "label": "Format"
  },
  {
    "span": "(Zhao, 2019,)",
    "document": "Related Work\n\nProbabilistic forecasting for time series combines calibrated uncertainty estimates with strong point predictions (Ibrahim and Chen, 2020; Singh et al., 2021). Mixture density networks and quantile regression are popular choices for multi-horizon settings (Quinn and Hart, 2022).\n\nTransformer-based forecasters introduce temporal attention and cross-variable interactions but risk overfitting without inductive biases (Zhao, 2019,). Regularization via frequency-domain priors and instance normalization has demonstrated consistent gains (Liu and Park, 2023). We propose a hierarchical prior that aligns seasonal components across related series.\n",
    "reason": "Extraneous comma before the closing parenthesis in the citation.",
    "start": 432,
    "end": 445,
    "label": "Format"
  },
  {
    "span": "Rubin (1974) formalized the potential outcomes framework. Angrist and Krueger (1991) popularized instrumental variables in applied settings. Difference-in-differences compares treated and control trends (Card and Krueger, 1994). Abadie et al. (2010) introduced synthetic control. Double machine learning enables flexible nuisance estimation (Chernozhukov et al., 2018).",
    "document": "Introduction\n\nCausal Inference with Observational Data\n\nEstimating causal effects when randomized experiments are infeasible requires assumptions and designs that address confounding and selection. We develop a sensitivity-analysis toolkit that integrates with flexible estimators.\n\nRubin (1974) formalized the potential outcomes framework. Angrist and Krueger (1991) popularized instrumental variables in applied settings. Difference-in-differences compares treated and control trends (Card and Krueger, 1994). Abadie et al. (2010) introduced synthetic control. Double machine learning enables flexible nuisance estimation (Chernozhukov et al., 2018).\n\nWe focus on quantifying violations of parallel trends and instrument exogeneity through interpretable stress tests.",
    "reason": "Multiple methods are listed in sequence with no transitions; how IV, DiD, synthetic control, and double ML relate to each other or to the paper’s focus is not made explicit.",
    "start": 283,
    "end": 652,
    "label": "Coherence"
  },
  {
    "span": "Recent graph contrastive learning methods define instance discrimination losses with view augmentations such as edge perturbation, node dropping, attribute masking, and subgraph sampling. Some approaches enforce alignment across local and global representations, while others regularize with agreement over multiple stochastic views. Negative-free objectives and bootstrapped predictors have also been proposed to stabilize training.",
    "document": "Related Work\n\nSelf-supervised learning on graphs has progressed rapidly through contrastive, generative, and predictive pretext tasks. Contrastive frameworks typically learn node or graph-level embeddings by maximizing agreement between augmented views of the same instance while repelling negatives.\n\nRecent graph contrastive learning methods define instance discrimination losses with view augmentations such as edge perturbation, node dropping, attribute masking, and subgraph sampling. Some approaches enforce alignment across local and global representations, while others regularize with agreement over multiple stochastic views. Negative-free objectives and bootstrapped predictors have also been proposed to stabilize training.\n\nBeyond contrastive setups, predictive tasks forecast masked attributes or next-hop neighborhoods, and generative approaches reconstruct adjacency or features. Downstream evaluations commonly include node classification, link prediction, and graph classification benchmarks.\n\nWe consider semi-supervised settings where label sparsity and topology shifts degrade alignment, and we report results across citation, molecule, and social benchmarks.",
    "reason": "The span lists prior methods and techniques without connecting them to the authors' goals or explaining the specific limitation that their work addresses.",
    "start": 302,
    "end": 735,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Bellemare et al. (2016) introduce pseudo-counts to encourage exploration in high-dimensional spaces. Pathak et al. (2017) propose curiosity-driven exploration with an intrinsic reward based on prediction error. Options can accelerate learning by providing temporal abstractions (Sutton et al., 1999).",
    "document": "Related Work\n\nExploration in Reinforcement Learning\n\nEffective exploration is critical for sparse-reward problems. Prior work spans intrinsic motivation, uncertainty estimation, and structured policies. Bellemare et al. (2016) introduce pseudo-counts to encourage exploration in high-dimensional spaces. Pathak et al. (2017) propose curiosity-driven exploration with an intrinsic reward based on prediction error. Options can accelerate learning by providing temporal abstractions (Sutton et al., 1999). Despite their promise, how intrinsic rewards interact with hierarchical control under tight sample budgets remains unclear.\n\nEvaluation Domains\n\nBenchmarks such as Atari and DM Control vary in reward sparsity and observability. We focus on tasks with long horizons and limited resets to stress-test exploration efficiency.",
    "reason": "The sentences enumerate methods from different subareas (counts, curiosity, options) without explicit connections or transitions, leaving their relationships implied rather than stated.",
    "start": 203,
    "end": 503,
    "label": "Coherence"
  },
  {
    "span": "ARIMA remains a strong classical baseline for univariate forecasting (Box and Jenkins, 1970). DeepAR uses autoregressive RNNs for probabilistic forecasting (Salinas et al., 2020). Transformer-based models introduce sparse attention to handle long contexts (Zhou et al., 2021).",
    "document": "Related Work\n\nTime Series Forecasting\n\nForecasting methods range from classical statistical models to deep neural architectures. Choices of objective, inductive bias, and context length affect accuracy and calibration under distribution shift. ARIMA remains a strong classical baseline for univariate forecasting (Box and Jenkins, 1970). DeepAR uses autoregressive RNNs for probabilistic forecasting (Salinas et al., 2020). Transformer-based models introduce sparse attention to handle long contexts (Zhou et al., 2021). Although these approaches differ substantially, guidance on selecting architectures under limited data and varying horizon lengths remains limited.\n\nEvaluation Criteria\n\nResearchers report MAE, MAPE, and CRPS across benchmarks with different seasonality profiles. We adopt a unified evaluation to compare calibration and accuracy.",
    "reason": "The span lists unrelated approaches across eras without explaining transitions or their relevance to one another, producing weak sentence-to-sentence coherence.",
    "start": 244,
    "end": 520,
    "label": "Coherence"
  },
  {
    "span": "Epsilon-greedy remains a strong baseline (Sutton and Barto, 2018). Count-based exploration generalizes via hashing (Tang et al., 2017). Intrinsic curiosity modules reward prediction error (Pathak et al., 2017). Random network distillation uses fixed target networks (Burda et al., 2019).",
    "document": "Related Work\n\nExploration in Reinforcement Learning\n\nEfficient exploration is central to sample-efficient reinforcement learning, especially in sparse-reward and long-horizon tasks. Methods provide bonuses or uncertainty that guide behavior toward informative states.\n\nExploration Bonuses and Heuristics\n\nEpsilon-greedy remains a strong baseline (Sutton and Barto, 2018). Count-based exploration generalizes via hashing (Tang et al., 2017). Intrinsic curiosity modules reward prediction error (Pathak et al., 2017). Random network distillation uses fixed target networks (Burda et al., 2019).\n\nUncertainty-Driven Methods\n\nPosterior sampling and ensemble-based approaches approximate epistemic uncertainty (Osband et al., 2016; Lakshminarayanan et al., 2017). We analyze how exploration signals interact with model-based planning.",
    "reason": "The works are merely enumerated without transitions or explicit comparisons, making it unclear how the methods relate or differ, thus exhibiting abrupt and implied connections.",
    "start": 305,
    "end": 592,
    "label": "Coherence"
  },
  {
    "span": "COMET has become the de facto choice for injecting commonsense into dialogue models.",
    "document": "Related Work Commonsense knowledge has been integrated into neural models via retrieval over knowledge graphs, generation of latent explanations, and constrained decoding (Speer et al., 2017; Bosselut et al., 2019; Sap et al., 2020). Such techniques help mitigate shortcut learning by introducing structured priors at training or inference time (Yasunaga et al., 2021). COMET has become the de facto choice for injecting commonsense into dialogue models. We instead explore lightweight retrieval from semi-structured narratives to guide response planning.",
    "reason": "Asserts a field-wide trend about prior work without providing any supporting citations, violating rule (b).",
    "start": 370,
    "end": 454,
    "label": "Unsupported_claim"
  },
  {
    "span": "It is widely recognized that CORA, Citeseer, and Pubmed are transductive benchmarks with limited scale and strong homophily biases.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become the de facto approach for semi-supervised node classification and link prediction by propagating and transforming neighborhood information. Early message-passing architectures emphasize homophily, while more recent methods address heterophily, over-smoothing, and scalability to large graphs. Evaluation practices in the field have evolved alongside benchmark curation, with attention to reproducibility and fair comparison across random splits and training budgets. It is widely recognized that CORA, Citeseer, and Pubmed are transductive benchmarks with limited scale and strong homophily biases. Consequently, results on these datasets may overestimate performance in inductive or heterophilous settings such as social networks or protein interactions. Newer benchmarks introduce larger graphs, controlled heterophily, and standardized splits to reduce variance across runs. In this paper, we study a family of decoupled architectures and spectral regularizers, reporting systematic results across homophilous and heterophilous regimes and analyzing sensitivity to label rates and degree distributions.",
    "reason": "Asserts specific characteristics about well-known datasets without providing citations; claims about dataset properties should be supported (rule b).",
    "start": 522,
    "end": 653,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Agrawal et al., 2017",
    "document": "Introduction\n\nVisual question answering (VQA) has exposed multimodal models' reliance on dataset biases, leading to architectures that better integrate vision and language (Antol et al., 2015; Anderson et al., 2018). Counterfactual data augmentation and balanced splits reduce shortcut learning but do not eliminate it (Goyal et al., 2017; Teney et al., 2020). The role of explicit program supervision remains debated, with some gains offset by annotation cost (Johnson et al., 2017; Hudson and Manning, 2019).\n\nAttention mechanisms aligned with object regions have been widely adopted, yet often attend diffusely, hindering explanation faithfulness (Doshi-Velez and Kim, 2017; Jain and Wallace, 2019). As noted by (Agrawal et al., 2017 models can exploit language priors; later work introduced causal probes to disentangle sources of evidence (Chen et al., 2020; Niu et al., 2021).\n\nWe revisit robustness through targeted perturbations to question semantics and spatial relations, measuring how evidential support shifts within the model. Our analysis includes stress tests across compositional templates and adversarial foils.",
    "reason": "Missing closing parenthesis in a parenthetical citation.",
    "start": 715,
    "end": 736,
    "label": "Format"
  },
  {
    "span": "Ein-Dor et al., 2020)",
    "document": "Introduction\n\nTransformer-based models dominate modern NLP benchmarks. Several works study their behavior under limited supervision, e.g., entropy-based selection for QA by Ein-Dor et al., 2020) and gradient-length heuristics (Ash et al., 2020). We build on these insights to design a calibration-aware sampler for low-resource scenarios.",
    "reason": "Missing opening parenthesis for the parenthetical citation.",
    "start": 173,
    "end": 194,
    "label": "Format"
  },
  {
    "span": "Optimization in federated learning commonly relies on local SGD aggregation and its momentum or adaptive variants, with client sampling and partial participation to address scale (McMahan et al., 2017; Karimireddy et al., 2020; Reddi et al., 2020). Techniques for handling heterogeneity include proximal terms, control variates, and personalization layers (Li et al., 2020; Sahu et al., 2018; Hanzely et al., 2020; Dinh et al., 2020).",
    "document": "Related Work\n\nFederated learning (FL) aims to train models across decentralized data while preserving privacy. A central challenge is heterogeneity in client data and systems, which leads to drift, instability, and fairness concerns. We study convergence-fairness trade-offs when participation is bursty and data are non-identically distributed.\n\nOptimization in federated learning commonly relies on local SGD aggregation and its momentum or adaptive variants, with client sampling and partial participation to address scale (McMahan et al., 2017; Karimireddy et al., 2020; Reddi et al., 2020). Techniques for handling heterogeneity include proximal terms, control variates, and personalization layers (Li et al., 2020; Sahu et al., 2018; Hanzely et al., 2020; Dinh et al., 2020).\n\nOur approach introduces a participation-aware dual averaging scheme with per-client normalization that provides stability under bursty participation while preserving convergence guarantees.",
    "reason": "The span itemizes optimization methods and heterogeneity techniques but does not articulate how these relate to the bursty participation setting or what specific limitation motivates the new method (criteria a and b).",
    "start": 347,
    "end": 781,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The FEVER dataset includes over 500K claims annotated for stance and evidence.",
    "document": "Introduction\n\nAutomated fact verification has emerged as a critical task for combating misinformation at scale. Systems typically retrieve candidate evidence from a large corpus and then infer whether the evidence supports, refutes, or is insufficient for a given claim. Benchmarks in this space vary in their annotation protocols and evidence granularity, and they drive architectural choices for retrieval and reasoning. The FEVER dataset includes over 500K claims annotated for stance and evidence. While contemporary models achieve strong performance on in-domain data, robustness to paraphrase, evidentiary gaps, and domain shift remains limited. We introduce a new benchmark focusing on compositional reasoning with multi-hop evidence chains and provide a unified evaluation for retrieval and verification.",
    "reason": "Presents a specific statistical property of a dataset without providing a citation (rule a, b).",
    "start": 423,
    "end": 501,
    "label": "Unsupported_claim"
  },
  {
    "span": "Reinforcement learning for traffic signal control has explored value-based methods, actor-critic architectures, and multi-agent coordination (Wei et al., 2018; Chen et al., 2020; Wu and Lin, 2021; Park et al., 2022). Sim-to-real transfer via domain randomization and offline datasets has been investigated to mitigate deployment risk (Kong and Zhou, 2021; Li et al., 2022).",
    "document": "Introduction\nOptimizing urban traffic signals with learning-based controllers promises reduced congestion and emissions, yet safe deployment hinges on sample efficiency, robustness, and coordination under partial observability. We study multi-intersection control with sparse sensing and limited exploration budgets, emphasizing transferability to real-world signal plans.\n\nRelated Work\nReinforcement learning for traffic signal control has explored value-based methods, actor-critic architectures, and multi-agent coordination (Wei et al., 2018; Chen et al., 2020; Wu and Lin, 2021; Park et al., 2022). Sim-to-real transfer via domain randomization and offline datasets has been investigated to mitigate deployment risk (Kong and Zhou, 2021; Li et al., 2022). Classical optimization methods based on max-pressure and dynamic programming provide strong baselines in certain regimes (Varaiya, 2013; Huang and Xie, 2017).\n\nWe propose an offline-to-online curriculum that constrains exploration using safety envelopes derived from feasible signal timing plans, and a graph-based critic that propagates uncertainty-aware value estimates across neighboring intersections.\n",
    "reason": "The span enumerates prior RL approaches and sim-to-real strategies without relating them to the paper's constraints (partial observability, safety) or stating unresolved gaps; it lacks synthesis and author perspective.",
    "start": 387,
    "end": 760,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(2019, Lee et al.)",
    "document": "Introduction\n\nPrivacy-preserving machine learning aims to protect sensitive information while maintaining utility, with differential privacy (DP) providing formal guarantees under bounded leakage (Dwork et al., 2016; Abadi et al., 2016). Federated learning complements DP by keeping raw data on device, though aggregation can still reveal patterns (Kairouz et al., 2021). For membership inference and attribute leakage, auditing tools quantify risk during model development (Shokri et al., 2017; Carlini et al., 2022). Recent surveys (2019, Lee et al.) emphasize the importance of privacy-utility trade-offs across modalities. We contribute a calibration-aware DP optimizer that adaptively clips gradients to match local curvature, improving accuracy at fixed privacy budgets.",
    "reason": "Author-year order is reversed inside the parenthetical citation; should be \"(Lee et al., 2019)\".",
    "start": 534,
    "end": 552,
    "label": "Format"
  },
  {
    "span": "(Nguyen et al. 2022)",
    "document": "Related Work\n\nGraph contrastive learning (GCL) methods differ in how they construct positive/negative pairs and how they avoid representation collapse. Early work focuses on node-level contrast with random augmentations (Velickovic et al., 2019), while more recent approaches explore subgraph and global objectives (You et al., 2020). Curriculum-based sampling has also been proposed to stabilize training (Nguyen et al. 2022), but these methods may still rely on brittle heuristics.",
    "reason": "Missing comma before the year in an author–date citation; should be '(Nguyen et al., 2022)'.",
    "start": 406,
    "end": 426,
    "label": "Format"
  },
  {
    "span": "((Nguyen and Patel, 2020))",
    "document": "Introduction\n\nProgram synthesis with neural-guided search has leveraged symbolic constraints to improve generalization (Devlin et al., 2017; Ellis et al., 2021). A line of work on neural deductive reasoning ((Nguyen and Patel, 2020)) argues that differentiable theorem proving can serve as an inductive bias for compositional tasks.\n\nNevertheless, scalability remains a challenge due to the combinatorics of constraint satisfaction. Recent hybrid approaches prioritize batched constraint evaluation and caching to accelerate search (Sun et al., 2022; Chen and Liang, 2023).",
    "reason": "Redundant nested parentheses around a parenthetical citation; should be '(Nguyen and Patel, 2020)'.",
    "start": 207,
    "end": 233,
    "label": "Format"
  },
  {
    "span": "Akoglu et al. (2015) survey graph outlier detection methods. Ding et al. (2019) detect anomalies with deep autoencoders on attributed graphs. Liu et al. (2022) use contrastive objectives for rare pattern discovery. Zhao and Akoglu (2021) generate synthetic anomalies for evaluation.",
    "document": "Introduction\n\nGraph anomaly detection aims to identify nodes, edges, or subgraphs that deviate from normal patterns, with applications in fraud detection and cybersecurity. Methods increasingly leverage graph neural networks (GNNs) to capture structure and attributes.\n\nAkoglu et al. (2015) survey graph outlier detection methods. Ding et al. (2019) detect anomalies with deep autoencoders on attributed graphs. Liu et al. (2022) use contrastive objectives for rare pattern discovery. Zhao and Akoglu (2021) generate synthetic anomalies for evaluation.\n\nWe build on self-supervised objectives tailored to imbalanced settings, introducing a calibration module that adapts anomaly scores to local neighborhood statistics.\n",
    "reason": "The span presents a sequence of works without transitions or explicit statements tying them together, making the relationships between a survey, autoencoders, contrastive learning, and synthetic data unclear.",
    "start": 270,
    "end": 552,
    "label": "Coherence"
  },
  {
    "span": "Vaswani et al (2017)",
    "document": "Related Work\n\nAttention mechanisms enabled deeper sequence modeling than recurrent architectures. Vaswani et al (2017) demonstrate the Transformer’s superiority on translation benchmarks, which sparked adaptations for summarization and question answering (Liu and Lapata, 2019; Raffel et al., 2020). We focus on scaling encoder-decoder pretraining under budget constraints.",
    "reason": "Missing period after 'al' in the narrative citation. Should be Vaswani et al. (2017).",
    "start": 98,
    "end": 118,
    "label": "Format"
  },
  {
    "span": "Chen and Gupta (2021",
    "document": "Related Work\n\nFew-shot learners leverage meta-learning and prompt-based tuning to generalize from limited labels (Finn et al., 2017; Brown et al., 2020). Chen and Gupta (2021 propose a prototype refinement mechanism that reduces variance in class centroids across episodes, complemented by transductive inference (Liu et al., 2019). Later studies explored distribution calibration to mitigate covariate shift (Yang et al., 2021; Tian et al., 2022). While these methods improve sample efficiency, their robustness to domain shift remains underexplored, motivating our cross-domain evaluations.",
    "reason": "Missing closing parenthesis in a narrative citation; should be 'Chen and Gupta (2021) propose ...'.",
    "start": 154,
    "end": 174,
    "label": "Format"
  },
  {
    "span": "There have been several shared tasks on code-mixed sentiment analysis across Indic languages in recent years.",
    "document": "Related Work\n\nCode-mixed sentiment analysis poses unique challenges due to transliteration, informal spelling, and frequent switches between languages within a single utterance. There have been several shared tasks on code-mixed sentiment analysis across Indic languages in recent years. Methods have ranged from character-level CNNs to multilingual transformers with phonetic features, but consistent evaluation remains difficult due to heterogeneous annotation schemes and label distributions. We aim to standardize evaluation by curating harmonized splits and reporting macro-averaged metrics across language pairs.",
    "reason": "Mentions 'several shared tasks' in recent years without citing the specific shared tasks or their reports (criteria a and d).",
    "start": 178,
    "end": 287,
    "label": "Unsupported_claim"
  },
  {
    "span": "GNNExplainer learns subgraph masks to highlight influential structures (Ying et al., 2019). PGExplainer parameterizes edge masks with a generative network (Luo et al., 2020). Counterfactual explanations remove edges to flip predictions (Lucic et al., 2022). Spectral GNNs design frequency-selective filters for message passing (Wu et al., 2019).",
    "document": "Related Work\n\nExplainability in Graph Neural Networks\n\nAs GNNs are increasingly applied to high-stakes domains, interpretability has become a central concern. Post-hoc methods typically produce instance-level rationales in the form of subgraphs, features, or prototypes. Some approaches enforce interpretability during training via constraints or sparsity.\n\nInstance-Level Explanations\n\nGNNExplainer learns subgraph masks to highlight influential structures (Ying et al., 2019). PGExplainer parameterizes edge masks with a generative network (Luo et al., 2020). Counterfactual explanations remove edges to flip predictions (Lucic et al., 2022). Spectral GNNs design frequency-selective filters for message passing (Wu et al., 2019).\n\nModel-Level Interpretability and Causality\n\nOther work targets model-level insights by learning global rules or causal abstractions. GraphMask induces sparse, context-specific pathways through the network (Schlichtkrull et al., 2020). Causal formulations define interventions on graph structure to test invariance (Zečević et al., 2021). Our method unifies instance- and model-level views through a constrained bilevel optimization that yields certifiable explanations.",
    "reason": "The first three sentences pertain to explanation methods; the fourth abruptly shifts to spectral GNN architectures without clarifying relevance. There are no transitions indicating why spectral filters are discussed in the context of explanation, leaving the relationship between sentences implicit and unclear.",
    "start": 387,
    "end": 732,
    "label": "Coherence"
  },
  {
    "span": "Prior studies have shown that users from rural areas are underrepresented in social media geotagging.",
    "document": "Introduction\n\nInferring user location from social media text supports applications in disaster response, public health, and marketing. Prior studies have shown that users from rural areas are underrepresented in social media geotagging. This imbalance leads to models that perform poorly outside major metropolitan regions. We propose a stratified sampling and debiasing pipeline to address these disparities.",
    "reason": "References unspecified prior studies without citation; per rule (a), prior work must be cited at first mention.",
    "start": 135,
    "end": 236,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Baker, 2014)",
    "document": "Related Work\n\nExplainable AI in finance has considered model transparency, documentation standards, and human factors (Doshi-Velez and Kim, 2017; Lage et al., 2019). According to (Baker, 2014), model transparency directly influences regulator trust and adoption. Subsequent studies analyze the trade-off between explanation fidelity and usability (Lakkaraju et al., 2019) and propose domain-specific reporting templates (Sokol and Flach, 2020).",
    "reason": "Wrong citation style for a narrative construction: after \"According to\", the citation should be narrative (\"According to Baker (2014)\") rather than parenthetical.",
    "start": 179,
    "end": 192,
    "label": "Format"
  },
  {
    "span": "[12]",
    "document": "Related Work\n\nGraph neural networks (GNNs) generalize convolution and message passing to non-Euclidean structures and have achieved strong results in chemistry, recommendation, and NLP (Kipf and Welling, 2017; Hamilton et al., 2017). Training stability and over-smoothing remain key challenges, motivating residual connections and normalization schemes (Li et al., 2019). Some methods report improvements using decoupled propagation and adaptive depth scheduling [12], whereas others incorporate label propagation to exploit graph homophily (Zhou et al., 2021). Our work adopts an author–year citation style throughout and examines curriculum-based depth selection for sparse graphs.\n",
    "reason": "Numeric bracket citation used in an author–year context; wrong citation style for the paper.",
    "start": 463,
    "end": 467,
    "label": "Format"
  },
  {
    "span": "Neural solvers learn to map text to equations for arithmetic word problems (Wang et al., 2017). Geometry problems require diagram understanding (Seo et al., 2015). Pretraining on math datasets benefits symbolic reasoning (Hendrycks et al., 2021).",
    "document": "Introduction\n\nAutomatic mathematical problem solving spans arithmetic, algebra, geometry, and competition-level reasoning. Techniques range from template filling and semantic parsing to end-to-end neural models and hybrid neuro-symbolic systems.\n\nNeural solvers learn to map text to equations for arithmetic word problems (Wang et al., 2017). Geometry problems require diagram understanding (Seo et al., 2015). Pretraining on math datasets benefits symbolic reasoning (Hendrycks et al., 2021).\n\nDespite progress, generalization across formats and problem types remains limited. We introduce a modular framework that separates linguistic parsing, diagram grounding, and solver selection, enabling cross-domain transfer.",
    "reason": "The three sentences jump across subareas without transitions or explicit statements of how they relate, creating abrupt shifts and unclear connections between the cited works.",
    "start": 247,
    "end": 493,
    "label": "Coherence"
  },
  {
    "span": "Personalized federated learning aims to adapt a global model to heterogeneous clients using techniques such as meta-learning, model interpolation, and clustered updates (Fallah et al., 2020; Arivazhagan et al., 2019; Sattler et al., 2020). Several algorithms balance personalization and collaboration via regularization and proximal objectives (Li et al., 2020; Collins et al., 2021).",
    "document": "Introduction\n\nFederated learning enables on-device training without centralizing raw data, but client heterogeneity often degrades the performance of a single global model. Personalization strategies seek to tailor models to local distributions while preserving cross-client knowledge sharing.\n\nPersonalized federated learning aims to adapt a global model to heterogeneous clients using techniques such as meta-learning, model interpolation, and clustered updates (Fallah et al., 2020; Arivazhagan et al., 2019; Sattler et al., 2020). Several algorithms balance personalization and collaboration via regularization and proximal objectives (Li et al., 2020; Collins et al., 2021).\n\nWe propose FedSparse-Head, a method that learns a shared backbone with client-specific sparse heads selected via communication-efficient l0 proxies. The approach reduces uplink costs and isolates client drift to small, interpretable modules.\n\nWe demonstrate improvements over strong baselines on heterogeneous vision and language benchmarks, analyze communication savings, and study stability under varying participation and data skew.",
    "reason": "The span catalogs existing personalized FL techniques without articulating how they relate to the proposed method or what exact gap remains, failing to synthesize prior work into the paper’s motivation (criteria a and c).",
    "start": 295,
    "end": 679,
    "label": "Lacks_synthesis"
  },
  {
    "span": "M4 competition",
    "document": "Introduction\n\nAccurate time series forecasting underpins planning in retail, energy, and finance. Classical statistical models remain competitive, while recent deep learning approaches aim to leverage cross-series information and hierarchical structure.\n\nInsights from the M4 competition have highlighted the importance of strong statistical baselines across heterogeneous series and careful evaluation protocols that avoid leakage. Nevertheless, complex seasonality, intermittent demand, and sparse covariates continue to challenge accuracy and reliability.\n\nWe introduce a hybrid architecture that combines decomposition-based preprocessing with a lightweight global model, yielding improvements in both accuracy and uncertainty calibration across diverse horizons.",
    "reason": "The 'M4 competition' is a specific benchmark/competition and is mentioned without citation at first mention, violating rule a.",
    "start": 273,
    "end": 287,
    "label": "Unsupported_claim"
  },
  {
    "span": "In a previous study, the authors claim that domain-adaptive pretraining eliminates the need for large labeled corpora.",
    "document": "Related Work Domain-adaptive pretraining (DAPT) continues to be a simple and effective way to specialize language models to new corpora by further training on unlabeled in-domain text (Gururangan et al., 2020). Variants such as task-adaptive pretraining (TAPT) and intermediate training objectives have been explored to bridge the domain gap with modest compute budgets (Phang et al., 2019; Pruksachatkun et al., 2020). In a previous study, the authors claim that domain-adaptive pretraining eliminates the need for large labeled corpora. Our work revisits these findings under stricter evaluation protocols with robust early stopping and cross-domain generalization tests.",
    "reason": "Refers to a specific 'previous study' and its claim without citing the source, violating rule (a).",
    "start": 420,
    "end": 538,
    "label": "Unsupported_claim"
  },
  {
    "span": "There has been growing interest in combining vision and language for tasks such as visual-guided machine translation (Sigurdsson et al., 2020;Surís et al., 2020;Huang et al., 2020), multi-lingual visual question answering (Gao et al., 2015;Gupta et al., 2020;Shimizu et al., 2018), multi-lingual image captioning (Gu et al., 2018;Lan et al., 2017), multilingual video captioning (Wang et al., 2019b), and multi-lingual image-sentence retrieval Burns et al., 2020). In this paper, we work on multi-lingual vision-and-language navigation. We use vision (i.e., navigation path) as a bridge between multi-lingual instructions and learn a crosslingual representation that captures visual concepts. Furthermore, our approach also use language as a bridge between different visual environments to learn an environment-agnostic visual representation.",
    "document": "Related Work\n\nVision-and-language navigation. Vision-and-Language Navigation (VLN) requires an agent to find the routes to the desired target based on instructions Thomason et al., 2020;Nguyen and Daumé III, 2019;Chen et al., 2019;Krantz et al., 2020). Specifically, there are two key challenges in VLN: grounding the natural language instruction to visual environments and generalizing to unseen environments. To address the first challenge, one line of research in VLN utilizes carefully designed cross-modal attention modules (Wang et al., , 2019aTan et al., 2019;Landi et al., 2019;Xia et al., 2020;Wang et al., 2020b,a;Zhu et al., 2020;Zhu et al., 2021;, progress monitor modules (Ma et al., 2019b,a;Ke et al., 2019), and object-action aware modules (Qi et al., 2020a). Another line of research improves vision and language co-grounding by improving vision and language representations with pre-training techniques (Li et al., 2019;Huang et al., 2019b;Hong et al., 2021). Li et al. (2019) directly adopts pre-trained BERT for encoding instructions,  and Hong et al. (2021) learn from a large amount of image-textaction triplets,  learns from large amount of text-image pairs from the web, and Huang et al. (2019b) transfers language and visual representation to in-domain representation with auxiliary tasks. Different from them, we utilize the visually-aligned multilingual instructions to learn a cross-lingual language representation that inherently captures visual semantics underlying the instruction.\n\nMultiple methods have been proposed to encourage generalization to unseen environments during training (Zhang et al., 2020;Tan et al., 2019;Wang et al., 2020c;Fu et al., 2020). Zhang et al. (2020) demonstrates that it is the low-level appearance information that causes the large performance gap between seen and unseen environments. Tan et al. (2019) proposes to use environment dropout on visual features to create new environments and Fu et al. (2020) utilizes adversarial path sampling to encourage generalization. However, both of these methods rely on a speaker module to generate synthetic training data and can be considered as data augmentation methods, which are complementary to our proposed environment-agnostic visual representation. The closest work to ours is Wang et al. (2020c), where they proposes to pair an environment classifier with gradient reversal layer to learn an environment-agnostic representation. However, they only consider one single environment when learning the visual representation for a given path (i.e., given one path and predict its environment). In our environment-agnostic representation learning, we explore the connections between multiple environments (i.e., maximize the similarity between paths from different environments). Vision-and-language with multilinguality. There has been growing interest in combining vision and language for tasks such as visual-guided machine translation (Sigurdsson et al., 2020;Surís et al., 2020;Huang et al., 2020), multi-lingual visual question answering (Gao et al., 2015;Gupta et al., 2020;Shimizu et al., 2018), multi-lingual image captioning (Gu et al., 2018;Lan et al., 2017), multilingual video captioning (Wang et al., 2019b), and multi-lingual image-sentence retrieval Burns et al., 2020). In this paper, we work on multi-lingual vision-and-language navigation. We use vision (i.e., navigation path) as a bridge between multi-lingual instructions and learn a crosslingual representation that captures visual concepts. Furthermore, our approach also use language as a bridge between different visual environments to learn an environment-agnostic visual representation.\n\n ",
    "start": 2828,
    "end": 3670,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Howard and Ruder 2018)",
    "document": "Introduction\n\nTransfer learning for NLP was popularized via language model pretraining followed by fine-tuning (Howard and Ruder 2018). Subsequent improvements leveraged larger corpora and masked objectives (Devlin et al., 2019; Peters et al., 2018). Our study investigates how adapter layers influence sample efficiency under distribution shift.",
    "reason": "Missing comma between authors and year in the parenthetical citation. Should be (Howard and Ruder, 2018).",
    "start": 111,
    "end": 134,
    "label": "Format"
  },
  {
    "span": "(Nguyen et al., 2018,)",
    "document": "Related Work\n\nAdversarial training has been explored to enhance robustness of text classifiers against token- and sentence-level perturbations. Virtual adversarial training imposes local smoothness constraints in representation space (Miyato et al., 2017), and free adversarial training reduces computational overhead (Shafahi et al., 2019). For domain shift, instance-level perturbations can simulate target-style noise (Nguyen et al., 2018,) while data augmentation with paraphrases improves invariance (Iyyer et al., 2018). We extend these approaches by aligning adversarial examples with acquisition probabilities estimated from unlabeled pools.\n",
    "reason": "Extraneous trailing comma before the closing parenthesis in a parenthetical citation.",
    "start": 421,
    "end": 443,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claim that subword units dramatically reduce WER on distant-talking speech.",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) systems have transitioned from hybrid HMM-DNN pipelines to fully neural end-to-end models, including CTC and attention-based encoders. Subword modeling with byte-pair encoding (BPE) and unigram language models has become a standard alternative to grapheme or wordpiece tokenization in many ASR setups. Far-field and distant-talking speech recognition settings introduce additional reverberation and noise, requiring robust acoustic modeling and data augmentation.\n\nIn a previous study, the authors claim that subword units dramatically reduce WER on distant-talking speech. While several works have anecdotally reported improvements with subword vocabularies, the relative gains are known to depend on the language, lexicon coverage, and decoder configuration. Existing literature also explores SpecAugment and multi-condition training to mitigate far-field degradation, as well as domain-adaptive language modeling.\n\nOur work systematically compares subword and character-level units under matched training budgets and augmentation recipes across three far-field corpora.",
    "reason": "Refers to a specific prior study and its claim but does not provide a citation to that study.",
    "start": 514,
    "end": 622,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Kim, 2019, Park, 2020)",
    "document": "Introduction\n\nExploration remains a central challenge in reinforcement learning (Sutton and Barto, 2018). Count-based bonuses and uncertainty estimates guide agents toward informative states (Bellemare et al., 2016; Pathak et al., 2017). In continuous control, ensembles and bootstrap methods reduce overestimation and stabilize training (Osband et al., 2016; Fujimoto et al., 2018). Empirically, performance varies widely across seeds and environments, motivating rigorous evaluation protocols (Henderson et al., 2018). Multiple studies (Kim, 2019, Park, 2020) report that state-action visitation coverage is critical when rewards are sparse. We propose a stratified exploration objective that balances novelty and value, improving sample efficiency on MuJoCo tasks (Todorov et al., 2012; Tassa et al., 2018).",
    "reason": "Incorrect separator for multiple citations; APA style uses semicolons between works in one parenthetical, e.g., '(Kim, 2019; Park, 2020)'.",
    "start": 538,
    "end": 561,
    "label": "Format"
  },
  {
    "span": "(Khan et al., 2020; Lee et al., 2021]",
    "document": "Introduction\n\nMulti-task learning seeks positive transfer across related objectives by sharing representations and mitigating task interference (Caruana, 1997; Ruder, 2017). Recent surveys (Khan et al., 2020; Lee et al., 2021] emphasize adapter-based parameter sharing and selective routing for heterogeneous tasks.\n\nDespite empirical gains, negative transfer persists when tasks exhibit conflicting gradients or divergent label spaces. We address this by learning a sparsity-inducing routing policy that conditions on input difficulty.",
    "reason": "Mismatched brackets in a parenthetical citation; closing ']' should be ')'.",
    "start": 189,
    "end": 226,
    "label": "Format"
  },
  {
    "span": "Recent competitions on embodied navigation have highlighted the centrality of learned map memory for long-horizon tasks.",
    "document": "Related Work\n\nEmbodied AI seeks agents that perceive, reason, and act within simulated or real environments. Navigation tasks combine perception, mapping, and planning under partial observability.\n\nRecent competitions on embodied navigation have highlighted the centrality of learned map memory for long-horizon tasks. Concurrently, benchmarking suites emphasize sample efficiency and generalization across unseen layouts, spurring interest in memory-augmented policies.",
    "reason": "References 'recent competitions' and their conclusions without citing the competitions or reports; per rules (a) and (d), such mentions require citations.",
    "start": 198,
    "end": 318,
    "label": "Unsupported_claim"
  },
  {
    "span": "According to (Smith, 2014)",
    "document": "Related Work\n\nAbstractive summarization systems increasingly rely on pretrained encoder–decoder architectures trained with maximum likelihood and refined with reinforcement learning (See et al., 2017; Paulus et al., 2018; Liu and Lapata, 2019). According to (Smith, 2014) coherence can be improved by modeling entity transitions, while subsequent neural work operationalized this via entity-aware attention (Barzilay and Lapata, 2008; Wiseman et al., 2017). Recent approaches incorporate controllable generation to steer faithfulness and coverage (Fan et al., 2018; He et al., 2020).",
    "reason": "Wrong narrative style: 'According to' should be followed by a narrative citation, e.g., \"According to Smith (2014)\" rather than a parenthetical citation.",
    "start": 245,
    "end": 271,
    "label": "Format"
  },
  {
    "span": "Various defenses against gradient inversion and membership inference include differential privacy, secure aggregation, gradient compression, and adversarial perturbation of updates (Abadi et al., 2016; Bonawitz et al., 2017; Zhu et al., 2019; Truex et al., 2019; Sun et al., 2021).",
    "document": "Related Work\n\nFederated learning enables collaborative training without sharing raw data, but gradients and model updates can leak sensitive information. Attacks such as gradient inversion and membership inference have spurred work on defenses and auditing.\n\nVarious defenses against gradient inversion and membership inference include differential privacy, secure aggregation, gradient compression, and adversarial perturbation of updates (Abadi et al., 2016; Bonawitz et al., 2017; Zhu et al., 2019; Truex et al., 2019; Sun et al., 2021).\n\nOur work investigates robustness-utility trade-offs under heterogeneous client distributions, proposing an adaptive clipping and noise allocation scheme that tunes protections to client-level drift.",
    "reason": "The span lists defenses without relating them to the paper’s aims or identifying a specific gap; it lacks the author’s perspective or synthesis.",
    "start": 259,
    "end": 540,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The WMT shared task has consistently driven progress in neural MT since 2014.",
    "document": "Introduction\n\nNeural machine translation (NMT) has rapidly advanced with encoder–decoder architectures, attention mechanisms, and large-scale pretraining. Community benchmarks and shared tasks provide standardized evaluation and spur methodological innovation. The WMT shared task has consistently driven progress in neural MT since 2014. Parallel efforts explore evaluation beyond BLEU, including human assessments and learned metrics. Nevertheless, domain and low-resource scenarios continue to expose brittleness in current systems.",
    "reason": "Mentions a specific shared task and its historical influence without citation at first mention.",
    "start": 261,
    "end": 338,
    "label": "Unsupported_claim"
  },
  {
    "span": "Classical approaches to time-series anomaly detection rely on statistical modeling and change-point detection, whereas recent methods employ LSTM autoencoders, variational models, GAN-based detectors, and Transformer architectures (Tsay, 2010; Malhotra et al., 2015; Xu et al., 2018; Schlegl et al., 2019; Wu et al., 2020).",
    "document": "Related Work\n\nIndustrial IoT deployments demand timely and reliable anomaly detection to prevent downtime and reduce maintenance costs. Data are multi-variate, irregular, and subject to concept drift from evolving operating conditions. Methods differ in their ability to capture temporal dependencies, calibrate uncertainty, and adapt online.\n\nClassical approaches to time-series anomaly detection rely on statistical modeling and change-point detection, whereas recent methods employ LSTM autoencoders, variational models, GAN-based detectors, and Transformer architectures (Tsay, 2010; Malhotra et al., 2015; Xu et al., 2018; Schlegl et al., 2019; Wu et al., 2020). Many benchmarks report high AUCs, yet production rollouts remain limited.\n\nOur work proposes DriftGuard, a detector that combines probabilistic forecasting with drift-aware thresholds learned from weak supervision to maintain stable precision at fixed alert budgets.\n",
    "reason": "The span enumerates methods and citations without clarifying how they relate to the authors’ goals or what gap they leave; it lacks synthesis connecting literature to the proposed solution.",
    "start": 344,
    "end": 667,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Levine et al. (2016) use deep visuomotor policies for grasping with reinforcement learning. Mahler et al. (2017) build Dex-Net for robust grasp planning via analytic metrics. Domain randomization helps sim-to-real transfer (Tobin et al., 2017). Contact-rich manipulation benefits from tactile sensing (Calandra et al., 2018).",
    "document": "Related Work\n\nLearning-based grasping leverages visual perception, simulation, and tactile feedback to cope with clutter and uncertainty. We highlight representative learning, analytic, and sim-to-real strategies.\n\nLevine et al. (2016) use deep visuomotor policies for grasping with reinforcement learning. Mahler et al. (2017) build Dex-Net for robust grasp planning via analytic metrics. Domain randomization helps sim-to-real transfer (Tobin et al., 2017). Contact-rich manipulation benefits from tactile sensing (Calandra et al., 2018).\n\nEven with these advances, generalization to novel objects under limited experience remains a key challenge that our method addresses via few-shot affordance adaptation.",
    "reason": "The cited works are juxtaposed without transitions or an explanation of how reinforcement learning, analytic metrics, simulation, and tactile sensing relate; coherence across the multiple sentences is weak.",
    "start": 215,
    "end": 540,
    "label": "Coherence"
  },
  {
    "span": "Smith et al., 2020",
    "document": "Introduction\n\nLarge-scale visual pretraining has reshaped transfer learning in computer vision. Supervised pretraining on ImageNet followed by fine-tuning remains a strong baseline (He et al., 2016), while self-supervised learning narrows the gap without labels (He et al., 2020; Chen et al., 2020). Vision Transformers benefit from large datasets and strong augmentation (Dosovitskiy et al., 2021; Touvron et al., 2021). Several works Smith et al., 2020 compare linear probing versus full fine-tuning across domains, highlighting the importance of representation stability. We extend this analysis by introducing protocol-invariant evaluation that controls for augmentation, batch size, and optimizer settings, revealing consistent ranking shifts across downstream tasks.",
    "reason": "Citation is missing parentheses in a parenthetical context; it should be '(Smith et al., 2020)' or used narratively as 'Smith et al. (2020)'.",
    "start": 436,
    "end": 454,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claim that pretrained encoders saturate by 100M tokens.",
    "document": "Related Work\n\nPretraining objectives and data scales are key drivers of transfer performance in NLP. Masked language modeling, permutation-based objectives, and contrastive variants have each been shown to shape downstream generalization in distinct ways. Recent analyses explore scaling laws, relating model size, data size, and compute to validation loss and downstream accuracy.\n\nThe role of pretraining dataset size, in particular, has received renewed attention due to the cost of collecting and curating high-quality corpora. In a previous study, the authors claim that pretrained encoders saturate by 100M tokens. Subsequent work investigates whether such saturation is objective-specific, architecture-dependent, or an artifact of evaluation protocol.\n\nOur contribution isolates the effect of pretraining data beyond 100M tokens using identical model capacity, optimizer settings, and domain composition. We find that additional high-quality data continues to yield non-trivial gains for low-resource tasks and for tasks requiring longer context integration.",
    "reason": "This sentence references 'a previous study' and attributes a specific claim without providing a citation (violates rule a and b).",
    "start": 532,
    "end": 620,
    "label": "Unsupported_claim"
  },
  {
    "span": "Swin Transformer has become the de facto standard backbone for object detection.",
    "document": "Related Work\n\nBackbone architectures have evolved from convolutional networks to vision transformers for dense prediction tasks. ResNet variants (He et al., 2016) long served as strong backbones in object detection and instance segmentation. Vision Transformers (Dosovitskiy et al., 2020) introduced global attention, but scaling them for dense tasks required architectural refinements. Swin Transformer has become the de facto standard backbone for object detection. Recent detectors further integrate multi-scale features and improved normalization to exploit these backbones more effectively.",
    "reason": "Claims a dominant status for a specific prior method without citing evidence or comparative studies.",
    "start": 387,
    "end": 467,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Kumar and Lee, 2020",
    "document": "Related Work\n\nThere has been growing interest in efficient transformer variants that trade representational fidelity for sub-quadratic attention (Kitaev et al., 2020; Choromanski et al., 2021). Earlier surveys (Kumar and Lee, 2020 discuss theoretical guarantees for kernelized attention and summarize empirical trade-offs across tasks. Beyond sparsity, retrieval-augmented architectures amortize long-context processing by caching key-value states (Borgeaud et al., 2022). Our framework complements these lines by introducing a task-adaptive routing strategy that selects attention patterns per layer.\n\nWe benchmark across long-range modeling tasks to quantify latency–accuracy trade-offs.",
    "reason": "Missing closing parenthesis in a parenthetical citation; should be '(Kumar and Lee, 2020)'.",
    "start": 210,
    "end": 230,
    "label": "Format"
  },
  {
    "span": "Recent competitions on automated essay scoring have standardized prompts",
    "document": "Related Work\n\nAutomated essay scoring (AES) assesses writing quality using machine learning models trained on labeled essays. Traditional approaches rely on hand-engineered features capturing grammar, coherence, and lexical diversity, whereas recent neural models learn holistic representations directly from text. Recent competitions on automated essay scoring have standardized prompts and grading rubrics, enabling more reliable cross-system comparisons. Despite this progress, domain transfer across prompts and genres remains difficult due to prompt-specific cues and annotation artifacts. Our work addresses this challenge by disentangling prompt content from writing quality signals via adversarial training.\n",
    "reason": "Claims the existence and impact of 'recent competitions' without citing any specific shared task or competition.",
    "start": 315,
    "end": 387,
    "label": "Unsupported_claim"
  },
  {
    "span": "Foundational approaches for estimating treatment effects from observational data include propensity score matching and weighting (Rosenbaum and Rubin, 1983; Imbens, 2000), doubly robust estimators (Bang and Robins, 2005), and instrumental variables (Angrist et al., 1996). Recent machine learning methods adopt representation learning and meta-learners (Shalit et al., 2017; Kunzel et al., 2019; Schwab et al., 2020).",
    "document": "Introduction Estimating heterogeneous treatment effects from observational data is critical for policy evaluation and precision medicine, yet confounding and selection bias complicate identification and estimation. Practical pipelines must navigate model misspecification, covariate shift, and limited overlap. Foundational approaches for estimating treatment effects from observational data include propensity score matching and weighting (Rosenbaum and Rubin, 1983; Imbens, 2000), doubly robust estimators (Bang and Robins, 2005), and instrumental variables (Angrist et al., 1996). Recent machine learning methods adopt representation learning and meta-learners (Shalit et al., 2017; Kunzel et al., 2019; Schwab et al., 2020). We propose an overlap-aware representation learning objective that penalizes counterfactual extrapolation via density ratio regularization, alongside a calibration procedure for uncertainty quantification. On semi-synthetic and clinical datasets, our method yields lower PEHE and better coverage than strong baselines.",
    "reason": "The span summarizes classical and ML-based causal inference approaches but does not connect them to the proposed overlap-aware method or articulate the specific gap being addressed. It lacks synthesis (a, b).",
    "start": 311,
    "end": 728,
    "label": "Lacks_synthesis"
  },
  {
    "span": "We adopt the data splits from the SemEval 2021 Toxic Spans Detection shared task.",
    "document": "Introduction\n\nIdentifying toxic spans within text is essential for fine-grained content moderation and explainable toxicity detection. Unlike document-level toxicity classification, the span detection setting requires models to localize offensive fragments while maintaining context sensitivity.\n\nWe adopt the data splits from the SemEval 2021 Toxic Spans Detection shared task. Building on a pretrained encoder, we formulate span detection as a token classification problem with a boundary-aware loss and incorporate character-level features to improve robustness to obfuscation. We compare our method against strong baselines and conduct ablations on boundary penalties and label smoothing.",
    "reason": "First mention of a shared task lacks a citation to the task (definition a).",
    "start": 297,
    "end": 378,
    "label": "Unsupported_claim"
  },
  {
    "span": "Continual learning has introduced rehearsal buffers, generative replay, parameter isolation, and regularization-based constraints to mitigate catastrophic forgetting (Lopez-Paz and Ranzato, 2017; Shin et al., 2017; Rusu et al., 2016; Kirkpatrick et al., 2017; Aljundi et al., 2018). Task inference methods and prompt-based adapters have also been explored (Mendez et al., 2021; Zhou et al., 2022).",
    "document": "Related Work\n\nContinual learning studies how models can learn from a non-stationary stream of tasks without catastrophic forgetting. Methods vary in their assumptions about task boundaries, memory budgets, and computational overhead.\n\nContinual learning has introduced rehearsal buffers, generative replay, parameter isolation, and regularization-based constraints to mitigate catastrophic forgetting (Lopez-Paz and Ranzato, 2017; Shin et al., 2017; Rusu et al., 2016; Kirkpatrick et al., 2017; Aljundi et al., 2018). Task inference methods and prompt-based adapters have also been explored (Mendez et al., 2021; Zhou et al., 2022).\n\nHowever, memory-efficient strategies that preserve performance under tight latency constraints remain underexplored. In resource-constrained settings, rehearsal may be infeasible and parameter isolation can inflate model size.\n\nWe introduce a selective replay scheme driven by gradient influence estimation with lightweight adapters, reducing memory while maintaining transfer across tasks.",
    "reason": "The span enumerates approaches and citations without relating them to the paper’s problem setting or articulating the insufficiency that motivates the new method.",
    "start": 235,
    "end": 632,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Knowledge distillation for speech recognition has explored frame-level and sequence-level teachers, CTC-to-attention transfer, and streaming teacher–student alignment (Hinton et al., 2015; Kim et al., 2016; Kurata and Audhkhasi, 2019; Kahn et al., 2020). Students distill logits, hidden states, or intermediate representations under latency constraints (Romero et al., 2015; Tjandra et al., 2017).",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) models deliver strong accuracy but often exceed on-device latency and memory limits. Knowledge distillation (KD) compresses models by transferring information from large teachers to compact students.\n\nKnowledge distillation for speech recognition has explored frame-level and sequence-level teachers, CTC-to-attention transfer, and streaming teacher–student alignment (Hinton et al., 2015; Kim et al., 2016; Kurata and Audhkhasi, 2019; Kahn et al., 2020). Students distill logits, hidden states, or intermediate representations under latency constraints (Romero et al., 2015; Tjandra et al., 2017).\n\nWe propose latency-aware sequence KD that reweights alignment paths to account for endpointing and beam search behavior in streaming ASR. Experiments on Librispeech and a far-field dataset show improved word error rates at fixed real-time factors.",
    "reason": "The span lists KD variants and signals without clarifying their limitations or how they inform the paper’s contribution, providing no explicit gap or perspective (a, c).",
    "start": 261,
    "end": 658,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Self-supervised pretraining on speech yields robust encoders (Baevski et al., 2020). End-to-end CTC models simplify the pipeline (Graves et al., 2006). Beam search heuristics affect decoding latency (Hori et al., 2017).",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) has advanced with end-to-end models and large-scale pretraining. Nevertheless, optimizing for latency and accuracy under deployment constraints requires understanding interactions between acoustic encoders and decoding strategies.\n\nSelf-supervised pretraining on speech yields robust encoders (Baevski et al., 2020). End-to-end CTC models simplify the pipeline (Graves et al., 2006). Beam search heuristics affect decoding latency (Hori et al., 2017).\n\nDespite these developments, there is limited guidance on balancing encoder capacity with search complexity under streaming constraints. We analyze this trade-off through a unified latency–accuracy budget.",
    "reason": "The cited works are presented without transitions or an explicit narrative connecting pretraining, CTC modeling, and decoding heuristics, making the relationships unclear.",
    "start": 281,
    "end": 500,
    "label": "Coherence"
  },
  {
    "span": "in (Larsen et al., 2018)",
    "document": "Related Work\n\nTask-oriented dialogue systems have evolved from pipeline architectures to end-to-end neural models. Following the taxonomy in (Larsen et al., 2018), we categorize recent methods into retrieval-based and generation-based approaches. Retrieval-based methods leverage dual-encoders and dense retrievers to select candidate responses from a fixed index, while generation-based models produce responses directly from the encoder-decoder backbone. Our survey emphasizes generalization across domains and user intents, a challenge widely discussed in prior literature (Gao et al., 2019; Mehri and Eskenazi, 2020).",
    "reason": "Wrong citation style: the narrative form places the preposition outside the parentheses. It should be 'Following the taxonomy in Larsen et al. (2018)' or rephrased to keep the citation parenthetical.",
    "start": 138,
    "end": 162,
    "label": "Format"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nNeural acoustic modeling has progressed rapidly with self-supervised pretraining on unlabeled speech. According to [12], incorporating large-scale contrastive pretraining greatly reduces word error rates on low-resource benchmarks. Subsequent studies (Baevski et al., 2020; Hsu et al., 2021) refine objective functions and data augmentation strategies. End-to-end architectures integrate language modeling with transducer losses to further improve recognition (Graves, 2012; Gulati et al., 2020).",
    "reason": "Numeric bracket citation used within an author–year styled section; should be replaced with an author–year citation such as \"According to Smith et al. (2020)\".",
    "start": 129,
    "end": 133,
    "label": "Format"
  },
  {
    "span": "There has been a surge of multimodal QA datasets in the last two years.",
    "document": "Related Work\n\nMultimodal question answering (QA) requires joint reasoning over language and visual inputs. Early efforts focused on synthetic scenes and templated questions, while subsequent datasets introduced real images and free-form questions to better capture open-world complexity. There has been a surge of multimodal QA datasets in the last two years. Methods leveraging cross-modal pretraining, region-based attention, and program induction have improved performance, yet models still struggle with compositional generalization and fine-grained counting.",
    "reason": "References 'a surge' of recent datasets without citing any of the works (rule d).",
    "start": 288,
    "end": 359,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT has been used in AES trained on essays collected from nationwide standardized tests.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) aims to predict holistic or trait-specific scores based on textual features. Deep learning approaches have shifted from handcrafted features to contextual encoders that capture discourse, coherence, and stylistic cues.\n\nBERT has been used in AES trained on essays collected from nationwide standardized tests. Other approaches explore hierarchical encoders that aggregate sentence-level representations, as well as prompt-specific adapters to reduce overfitting.\n\nOur work focuses on cross-prompt generalization and investigates how content leakage can inflate performance when prompts share topical vocabulary.",
    "reason": "Mentions a specific model application and dataset source without citing the studies or datasets that established this usage.",
    "start": 264,
    "end": 353,
    "label": "Unsupported_claim"
  },
  {
    "span": "Miller & Davis (2020)",
    "document": "Introduction\n\nCausal representation learning aims to discover disentangled factors consistent with underlying causal structures. While variational approaches learn latent variables, most do not capture interventions or counterfactuals (Locatello et al., 2019). Miller & Davis (2020) propose using invariances across environments to identify stable mechanisms, drawing on distributional robustness (Peters et al., 2016). However, assumptions on interventions are often unmet in practice.\n\nWe introduce a weakly supervised objective that leverages soft interventions derived from logged policies, relaxing identifiability requirements.",
    "reason": "In narrative citations, APA style uses 'and' rather than '&'; it should be \"Miller and Davis (2020)\".",
    "start": 261,
    "end": 282,
    "label": "Format"
  },
  {
    "span": "Graves et al. (2006) propose CTC for sequence labeling without alignment. Amodei et al. (2016) describe DeepSpeech for end-to-end ASR. Park et al. (2019) introduce SpecAugment for robust speech recognition. Vaswani et al. (2017) propose Transformers applied to ASR encoders. Panayotov et al. (2015) release LibriSpeech as a benchmark corpus.",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) integrates acoustic modeling and decoding, with advances in sequence criteria, architectures, and data augmentation. Benchmarks and training strategies have standardized evaluation and improved robustness.\n\nGraves et al. (2006) propose CTC for sequence labeling without alignment. Amodei et al. (2016) describe DeepSpeech for end-to-end ASR. Park et al. (2019) introduce SpecAugment for robust speech recognition. Vaswani et al. (2017) propose Transformers applied to ASR encoders. Panayotov et al. (2015) release LibriSpeech as a benchmark corpus.\n\nRecent studies combine attention-based transducers with stronger augmentation and self-supervised pretraining. We build on conformer encoders with curriculum masking to improve recognition in low-resource settings.",
    "reason": "The span lists heterogeneous contributions (loss function, system, augmentation, architecture, dataset) without transitions or explicit relationships, resulting in an abrupt, unconnected sequence.",
    "start": 267,
    "end": 608,
    "label": "Coherence"
  },
  {
    "span": "Recent benchmarks show that BLEU is uncorrelated with human judgments for dialogue.",
    "document": "Introduction\n\nAutomatic metrics are indispensable for rapid iteration in open-domain dialogue, yet their validity varies across tasks and data regimes. Over-reliance on n-gram overlap has been criticized for failing to capture relevance, coherence, and engagement.\n\nRecent benchmarks show that BLEU is uncorrelated with human judgments for dialogue. While embedding-based metrics and learned evaluators attempt to bridge this gap, their calibration and domain transfer remain open problems. Moreover, many studies report significance without accounting for annotator variance, obscuring the reliability of observed differences.\n\nWe present a comprehensive evaluation of automatic metrics across controllable dialog attributes, calibrate learned evaluators against rater consistency, and recommend reporting practices that reflect measurement uncertainty.",
    "reason": "The phrase 'Recent benchmarks' makes a prior-work claim without citations to the specific benchmarks (violates rule d).",
    "start": 266,
    "end": 349,
    "label": "Unsupported_claim"
  },
  {
    "span": "Nguyen et al. 1",
    "document": "Related Work\n\nHuman-in-the-loop learning provides partial or noisy feedback to reduce annotation costs in sequence labeling and QA. Preference learning has been used to align model outputs with user judgments (Stiennon et al., 2020), while bandit learning integrates binary or scalar feedback directly into optimization (Kreutzer et al., 2018). Nguyen et al. 1 report that binary accept/reject feedback can approximate supervised span labels, though careful policy regularization is required to avoid drift. Our approach builds on these insights by introducing a constrained objective that preserves coverage while incorporating implicit negative signals.\n",
    "reason": "Wrong use of footnote formatting; citation lacks year and is followed by an orphaned footnote marker.",
    "start": 345,
    "end": 360,
    "label": "Format"
  },
  {
    "span": "Chen et al.",
    "document": "Introduction\n\nGraph neural networks (GNNs) have emerged as a powerful paradigm for learning on relational data, enabling state-of-the-art performance on node classification, link prediction, and graph-level tasks (Hamilton et al., 2017; Kipf and Welling, 2017; Xu et al., 2019). Despite these advances, the design of message-passing mechanisms remains an active area of research, especially for handling heterophily and long-range dependencies (Pei et al., 2020; Zhao and Akoglu, 2020). Chen et al. propose to augment standard aggregation with adaptive attention over structural contexts to better capture informative neighborhoods. Subsequent work has explored positional encodings and spectral operators to enrich node representations (Dwivedi et al., 2021; Kreutz-Delgado, 2021). In this paper, we investigate whether sparsifying message flows with learned gating can improve both robustness and scalability compared with dense attention-based approaches (Klicpera et al., 2019; Rong et al., 2020).",
    "reason": "Narrative citation missing year; should be formatted as \"Chen et al. (YEAR)\".",
    "start": 487,
    "end": 498,
    "label": "Format"
  },
  {
    "span": "in (Garcia et al., 2018)",
    "document": "Related Work\n\nCross-lingual transfer. A large body of work studies transfer across languages using multilingual encoders and alignment objectives. Following the taxonomy in (Garcia et al., 2018), we categorize prior approaches into representation alignment, data augmentation, and task-specific adaptation. Representation alignment emphasizes shared subspaces induced by contrastive or adversarial objectives (Lee and Kim, 2020; Huang et al., 2021). Data augmentation focuses on synthetic parallel signals via back-translation or pivoting (Singh et al., 2019). Task-specific adaptation explores label-space mapping and structural constraints to bridge typological gaps (Wu and Zhao, 2022).\n\nPrompting for structured prediction. Prompt-based methods have recently been extended from classification to structured tasks such as NER and RE (Qin and Tu, 2021; Chen et al., 2022). However, most designs presuppose language-specific verbalizations, limiting zero-shot transfer.\n",
    "reason": "Wrong citation style: preposition placed before a parenthetical citation. Should be narrative style, e.g., \"Following the taxonomy in Garcia et al. (2018)\" or remove \"in\" and keep only the parenthetical citation.",
    "start": 170,
    "end": 194,
    "label": "Format"
  },
  {
    "span": "In (Vatswani et al., 2019)",
    "document": "Related Work\n\nTransfer learning alleviates data scarcity by pretraining models on source domains and adapting them to target tasks with limited labels. In (Vatswani et al., 2019), the authors introduced a multi-domain pretraining objective that aligns intermediate representations via moment matching. Subsequent approaches leverage adversarial alignment and contrastive objectives to improve cross-domain robustness (Garcia and Kumar, 2020; Lee et al., 2021). Our work complements these methods by explicitly modeling domain-specific features while preserving domain-general knowledge.",
    "reason": "Wrong citation style: the preposition should not be outside a parenthetical citation. Use a narrative form, e.g., \"In Vatswani et al. (2019), ...\" or drop \"In\" and keep only the parenthetical citation.",
    "start": 152,
    "end": 178,
    "label": "Format"
  },
  {
    "span": "It is well known that MF-based baselines underperform by 20–30% on cold-start users.",
    "document": "Introduction\n\nCold-start recommendation remains a central challenge for collaborative filtering systems. Matrix factorization (MF) performs well for warm users with ample history but struggles when only a handful of interactions are available. Item content, side information, and meta-learning approaches have been proposed to mitigate the sparsity problem.\n\nIt is well known that MF-based baselines underperform by 20–30% on cold-start users. However, reported deltas vary widely depending on the definition of cold-start, the held-out protocol, and whether popularity priors are included. We present a unified evaluation across three public datasets, harmonizing user-history truncation and item-frequency distributions, to disentangle the sources of variance.",
    "reason": "Claims a specific and widely known performance gap without any supporting citations or evidence, violating rule (b).",
    "start": 359,
    "end": 443,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Kulshreshtha et al., 2021,",
    "document": "Related Work\n\nBandit learning has been applied to a variety of NLP problems including neural machine translation (NMT; Sokolov et al., 2017;Kreutzer et al., 2018a,b;Mendoncca et al., 2021), structured prediction (Sokolov et al., 2016), semantic parsing (Lawrence and Riezler, 2018), intent recognition (Falke and Lehnen, 2021), and summarization (Gunasekara et al., 2021). Human feedback has been studied as a direct learning signal for NMT (Kreutzer et al., 2018b;Mendoncca et al., 2021), semantic parsing (Lawrence and Riezler, 2018), summarization (Stiennon et al., 2020), and dialogue (Jaques et al., 2020). Nguyen et al. (2017) simulates bandit feedback to improve an MT system fully trained on a large annotated dataset, including analyzing robustness to feedback perturbations. Our work shows that simulated bandit feedback is an effective learning signal for extractive question answering tasks. Our work differs in focus on reducing annotation costs by relying on few annotated examples only to train the initial model, or by eliminating the need for in-domain annotation completely by relying on data in other domains to train initial models. Alternative forms of supervision for QA have been explored in prior work, such as explicitly providing fine-grained information (Dua et al., 2020;Khashabi et al., 2020a). Kratzwald et al. (2020) resembles our setting in that it seeks binary feed-back to replace span annotation, but their goal is to create supervised data more economically. Domain adaptation for QA has been studied in prior work (Fisch et al., 2019;Khashabi et al., 2020b), including using data augmentation (Yue et al., 2021), adversarial training , contrastive method (Yue et al., 2021), back-training (Kulshreshtha et al., 2021, and exploiting small lottery subnetworks (Zhu et al., 2021).\n\n ",
    "start": 1726,
    "end": 1753,
    "label": "Format"
  },
  {
    "span": "Rao et al.",
    "document": "Related Work\n\nNeural retrieval has evolved rapidly with dense representations replacing sparse term matching in many settings (Johnson et al., 2019; Karpukhin et al., 2020). Early dual-encoder models emphasized scale but struggled with domain mismatch (Guo and Yates, 2021). Building on Rao et al., several approaches have explored domain-adaptive pretraining for retrieval, showing promising improvements on out-of-domain benchmarks (Singh and Lee, 2020; Xiong et al., 2021). Concurrently, re-ranking with cross-encoders continues to provide strong gains at higher computational cost (Nogueira and Cho, 2019; Pradeep et al., 2021). Recent work also studies hybrid systems that combine sparse and dense signals (Zhou et al., 2022), as well as retrieval-augmented generation for knowledge-intensive NLP tasks (Lewis et al., 2020).",
    "reason": "Narrative citation missing the publication year; should be formatted as Rao et al. (YEAR).",
    "start": 287,
    "end": 297,
    "label": "Format"
  },
  {
    "span": "(Lee et al. 2021)",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition leverages attention and transducer architectures to replace classical pipelines (Chan et al., 2016; Graves, 2012). Large-scale pretraining on unlabelled audio further improves performance (Baevski et al., 2020). Accuracy has improved steadily (Lee et al. 2021) with larger datasets and more expressive decoders (Prabhavalkar et al., 2018), though robustness to noise remains challenging (Ko et al., 2015).",
    "reason": "Missing comma before the year in a parenthetical citation; should be “(Lee et al., 2021)”.",
    "start": 297,
    "end": 314,
    "label": "Format"
  },
  {
    "span": "Schnabel et al. (2016) estimate propensities for unbiased learning-to-rank. Joachims et al. (2017) propose counterfactual evaluation for ranking policies. He et al. (2017) introduce neural collaborative filtering for implicit feedback. Wang et al. (2019) model user-item graphs with graph neural networks.",
    "document": "Introduction\n\nRecommender systems trained on implicit feedback suffer from selection bias, confounding, and exposure effects. Recent work spans causal inference for unbiased learning and deep architectures that leverage higher-order interactions, yet the interplay between these lines remains underexplored.\n\nSchnabel et al. (2016) estimate propensities for unbiased learning-to-rank. Joachims et al. (2017) propose counterfactual evaluation for ranking policies. He et al. (2017) introduce neural collaborative filtering for implicit feedback. Wang et al. (2019) model user-item graphs with graph neural networks.\n\nOur method unifies debiasing with graph-based encoders by learning exposure-adjusted embeddings on user-item graphs, enabling counterfactual risk minimization with structural priors.\n",
    "reason": "The span abruptly lists causal propensity methods alongside deep collaborative filtering and GNN recommenders without transitions or explanation of their relationships, leaving the relevance between sentences implicit.",
    "start": 309,
    "end": 614,
    "label": "Coherence"
  },
  {
    "span": "The largest publicly available dataset for NL-to-code is CoNaLa with roughly 2.1k annotated examples and 600k mined pairs.",
    "document": "Related Work\n\nProgram synthesis from natural language bridges human intent and executable code. Datasets for NL-to-code span small curated corpora with high-quality annotations and large mined collections with noisy alignments. Models range from sequence-to-sequence architectures to retrieval-augmented generators that leverage code repositories.\n\nThe largest publicly available dataset for NL-to-code is CoNaLa with roughly 2.1k annotated examples and 600k mined pairs.\n\nDespite dataset growth, generalization across libraries and idioms remains challenging. We propose a hybrid retriever-generator that conditions on API graphs to improve robustness on out-of-domain tasks.",
    "reason": "Asserts dataset primacy and provides specific statistics without any citation (violates rule a and b).",
    "start": 349,
    "end": 471,
    "label": "Unsupported_claim"
  },
  {
    "span": "In a previous study, the authors claim that self-attention completely obviates recurrence for morphological inflection.",
    "document": "Related Work\n\nNeural approaches to morphological inflection have evolved from encoder-decoder RNNs with attention to convolutional and transformer variants (Kann and Schütze, 2016; Aharoni and Goldberg, 2017; Wu et al., 2021). Character-level modeling benefits from copy mechanisms and biasing techniques that reflect paradigm structure (Makarov and Clematide, 2018; Peters and Martins, 2019). Shared tasks have catalyzed reproducible evaluation and parameter sharing across languages with varying typology (Cotterell et al., 2017, 2018).\n\nIn a previous study, the authors claim that self-attention completely obviates recurrence for morphological inflection.\n\nWe revisit this claim with a controlled suite of ablations isolating positional biases, subword segmentation, and training data size, showing that inductive bias, rather than architecture alone, drives the observed gains.",
    "reason": "Introduces a claim about prior work ('a previous study') without citing that study; first mention of a study must be referenced.",
    "start": 540,
    "end": 659,
    "label": "Unsupported_claim"
  },
  {
    "span": "We evaluate on the widely used WikiTableQuestions dataset.",
    "document": "Introduction\n\nQuestion answering over semi-structured tables requires models to map natural language questions to executable logical forms. Prior approaches struggle with compositional generalization and spurious programs, motivating methods that integrate stronger inductive biases and better supervision signals. We evaluate on the widely used WikiTableQuestions dataset. In addition, we consider cross-domain transfer from publicly available spreadsheet collections to assess robustness. Our contributions include a modular parser architecture and a constrained decoding procedure that enforces schema consistency during inference.",
    "reason": "First mention of a specific dataset lacks a citation, and the claim of being 'widely used' also requires evidence (definition a and b).",
    "start": 315,
    "end": 373,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Smith and Doe, 2021",
    "document": "Introduction\n\nPrivacy-preserving analytics seeks to enable utility while constraining information leakage about individuals (Dwork, 2006; Dwork and Roth, 2014). Differential privacy (DP) provides rigorous guarantees by perturbing statistics or learning procedures.\n\nApplications of DP to federated learning have examined the tension between client-level protection and model convergence (McMahan et al., 2018; Kairouz et al., 2021). Recent surveys summarize open challenges in composition, auditing, and deployment (Smith and Doe, 2021 and propose best-practice guidelines for practitioners operating under strict regulatory regimes.\n\nWe extend prior work by introducing an adaptive privacy budget allocator that accounts for non-IID participation patterns across rounds.",
    "reason": "Mismatched or missing bracket: the opening parenthesis is not closed; it should be '(Smith and Doe, 2021)'.",
    "start": 515,
    "end": 535,
    "label": "Format"
  },
  {
    "span": "BERT was used in an NER active learning setup with uncertainty sampling",
    "document": "Related Work\n\nActive learning for sequence labeling aims to reduce annotation costs by strategically selecting informative samples. Classic strategies include uncertainty sampling, query-by-committee, and expected model change. Recent transformer-based encoders have substantially improved named entity recognition (NER), motivating renewed interest in active learning policies tailored to contextual embeddings.\n\nIn early experiments, BERT was used in an NER active learning setup with uncertainty sampling, demonstrating that deep contextual features can amplify acquisition functions that were originally designed for linear models. Subsequent work explored diversity-aware batch selection and density-weighted criteria to mitigate redundancy in selected sentences. Our method complements these advances by introducing span-level utilities that directly target boundary errors.",
    "reason": "This sentence describes a specific prior setup and result but does not cite the corresponding study (rule a), and it matches the provided example of mentioning a task-specific setup without citation (rule e iii).",
    "start": 436,
    "end": 507,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prompt tuning has been explored for classification and generation tasks (Lester et al., 2021; Li and Liang, 2021). Multilingual adaptation remains challenging due to vocabulary mismatch (Conneau et al., 2020). Some studies introduce translation-based prompts (Winata et al., 2021). Template mining has also been applied to QA (Gao et al., 2021).",
    "document": "Related Work\n\nCross-lingual sequence labeling. Zero-shot transfer with multilingual encoders has enabled effective cross-lingual NER and POS tagging without target-language labels (Pires et al., 2019; Hu et al., 2020). However, label semantics and surface forms may not align across languages, leading to degradation in low-resource settings (Keung et al., 2020).\n\nPrompting for multilingual NLP. Prompt-based methods intend to align tasks with language model pretraining objectives, often improving few-shot data efficiency. Prompt tuning has been explored for classification and generation tasks (Lester et al., 2021; Li and Liang, 2021). Multilingual adaptation remains challenging due to vocabulary mismatch (Conneau et al., 2020). Some studies introduce translation-based prompts (Winata et al., 2021). Template mining has also been applied to QA (Gao et al., 2021). In contrast, our work targets prompt transfer without translation by leveraging a language-agnostic verbalizer that decouples label words from language-specific tokens.\n\nLabel verbalizers and calibration. Choosing label words is critical for stable prompt performance (Schick and Schütze, 2021). Calibrated scoring and debiased verbalizers reduce label prior shift and improve cross-domain robustness (Zhao et al., 2021). We build on this line by constructing a multilingual verbalizer space with alignment-aware calibration.",
    "reason": "The sentences list disparate works without transitions or explicit relationships. It is unclear how template mining in QA relates to translation-based prompts or vocabulary mismatch, causing abrupt shifts and weak coherence across citations.",
    "start": 526,
    "end": 871,
    "label": "Coherence"
  },
  {
    "span": "However, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks.",
    "document": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n• To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n• We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n• Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n ",
    "start": 716,
    "end": 1194,
    "label": "Unsupported_claim"
  },
  {
    "span": "ASR task",
    "document": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n• To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n• We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n• Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n ",
    "start": 3148,
    "end": 3155,
    "label": "Unsupported_claim"
  },
  {
    "span": "state-of-the-art voice Transformer network",
    "document": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n• To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n• We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n• Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n ",
    "start": 3191,
    "end": 3232,
    "label": "Unsupported_claim"
  },
  {
    "span": "VC task",
    "document": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n• To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n• We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n• Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n ",
    "start": 3242,
    "end": 3248,
    "label": "Unsupported_claim"
  },
  {
    "span": "SID task",
    "document": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n• To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n• We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n• Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n ",
    "start": 3451,
    "end": 3458,
    "label": "Unsupported_claim"
  },
  {
    "span": "Over 70% of production recommenders now incorporate GNN components.",
    "document": "Introduction\n\nRecommendation algorithms increasingly exploit graph structure to model relationships among users, items, and auxiliary entities. Graph neural networks (GNNs) provide a flexible framework to propagate signals across interaction graphs and side-information networks. These techniques are attractive for large platforms due to their ability to encode high-order connectivity and heterogeneous features at scale.\n\nOver 70% of production recommenders now incorporate GNN components. Nevertheless, deploying such models introduces engineering challenges, including dynamic graph updates, feature drift, and inference latency constraints. We address these challenges by proposing a decoupled architecture that amortizes neighborhood aggregation while preserving recommendation quality.",
    "reason": "Presents a precise adoption statistic with no source or evidence; quantitative claims require citation (rule b).",
    "start": 425,
    "end": 492,
    "label": "Unsupported_claim"
  },
  {
    "span": "Transformer-XL was first applied to protein sequence modeling in 2019",
    "document": "Introduction\n\nProtein language models treat amino acid sequences as tokens and learn representations that capture structure and function. Early architectures borrowed from NLP to model long-range dependencies in sequences. Transformer-XL was first applied to protein sequence modeling in 2019, demonstrating improvements on secondary structure prediction and remote homology classification.\n\nBuilding on this line of work, we introduce recurrence-aware adapters that preserve long-context reasoning while reducing memory footprint during finetuning on protein families.",
    "reason": "Claims a historical first application and a date but provides no citation to substantiate it (rule b).",
    "start": 223,
    "end": 292,
    "label": "Unsupported_claim"
  },
  {
    "span": "Nguyen et al. 2",
    "document": "Related Work\n\nOpen-domain question answering (QA) systems typically combine dense retrieval with extractive or generative readers (Karpukhin et al., 2020; Izacard and Grave, 2021). Calibration and answer verification have been studied to reduce hallucinations and improve trustworthiness (Kamath et al., 2020; Si et al., 2022). We follow Nguyen et al. 2 for negative sampling but replace uniform negatives with difficulty-aware selections mined via cross-encoders (Xiong et al., 2021). Complementary to re-ranking approaches (Nogueira and Cho, 2019), our method reduces reader overconfidence on near-miss contexts while maintaining high recall.",
    "reason": "Improper footnote-style marker without year; should include a year (e.g., \"Nguyen et al. (YEAR)\") or be reformatted as a proper footnote/endnote.",
    "start": 338,
    "end": 353,
    "label": "Format"
  },
  {
    "span": "(O'Neill et al 2022)",
    "document": "Introduction\n\nEnsuring fairness in machine learning models requires careful consideration of data imbalance, measurement bias, and deployment feedback loops (Barocas et al., 2019; Mitchell et al., 2021). Group fairness metrics such as demographic parity and equalized odds provide complementary perspectives (Hardt et al., 2016; Kleinberg et al., 2017), while individual fairness focuses on similar treatment for similar individuals (Dwork et al., 2012). Post-processing methods adjust decision thresholds, whereas in-processing adds constraints during training (Agarwal et al., 2018). Recent causal formulations aim to separate permissible and impermissible pathways (Kusner et al., 2017). A counterfactual data augmentation strategy was examined in (O'Neill et al 2022) for robust risk scoring.",
    "reason": "Punctuation and delimiter errors in the citation: missing period after 'al' and missing comma before the year. It should be '(O'Neill et al., 2022)'.",
    "start": 751,
    "end": 771,
    "label": "Format"
  },
  {
    "span": "Prior work on multilingual pretraining, CTC variants, and wav2vec have improved low-resource ASR (Baevski et al., 2020; Conneau et al., 2021; Graves et al., 2006). We adopt a similar setup.",
    "document": "Related Work\n\nLow-resource automatic speech recognition (ASR) approaches seek to compensate for limited labeled data via transfer learning, self-supervised objectives, and data augmentation. Research has explored multilingual joint training, phoneme-level supervision, and pseudo-labeling with iterative refinement.\n\nPrior work on multilingual pretraining, CTC variants, and wav2vec have improved low-resource ASR (Baevski et al., 2020; Conneau et al., 2021; Graves et al., 2006). We adopt a similar setup.\n\nOther lines of work consider lexicon-free decoding and unsupervised subword discovery to further reduce the reliance on expert-curated resources, often trading off stability for broader applicability.",
    "reason": "Describes others’ methods and then states that the paper follows them, without explaining the authors’ perspective, distinctive angle, or the gap being addressed (definition a/b/c).",
    "start": 317,
    "end": 506,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Kumar et al., 2021)",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a standard tool for learning over relational structures (Kipf and Welling, 2017; Hamilton et al., 2017; Veličković et al., 2018). Message passing variants capture local dependencies, while spectral methods emphasize global smoothness (Defferrard et al., 2016). In (Kumar et al., 2021) we find that pretraining node encoders on large heterogeneous graphs improves few-shot performance on downstream tasks. Complementary approaches leverage self-supervised objectives to reduce reliance on labeled data (You et al., 2020; Hu et al., 2020). Our work aligns with these trends but targets cross-graph transfer, where distribution shift and schema mismatch challenge standard GNN training routines.\n",
    "reason": "Incorrect style: a preposition followed by a parenthetical citation. It should read \"In Kumar et al. (2021), we find …\" or \"In prior work (Kumar et al., 2021), …\" without the preposition directly preceding a parenthetical.",
    "start": 316,
    "end": 339,
    "label": "Format"
  },
  {
    "span": "Kipf and Welling (2017) introduced graph convolutional networks via spectral approximations. Veličković et al. (2018) proposed attention-based message passing on graphs. Hamilton et al. (2017) presented neighborhood sampling for scalable training. Xu et al. (2019) analyzed the expressivity of GNNs relative to the Weisfeiler-Lehman test.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become central to learning over relational structures, enabling node classification, link prediction, and graph-level tasks. A key line of inquiry concerns scalability, expressivity, and inductive generalization across diverse topologies and sparsity patterns.\n\nKipf and Welling (2017) introduced graph convolutional networks via spectral approximations. Veličković et al. (2018) proposed attention-based message passing on graphs. Hamilton et al. (2017) presented neighborhood sampling for scalable training. Xu et al. (2019) analyzed the expressivity of GNNs relative to the Weisfeiler-Lehman test.\n\nBeyond message passing, hybrid and positional encoding methods inject structure beyond edges (Dwivedi et al., 2020; Ying et al., 2021). Pretraining over large graph corpora and self-supervision are active topics (Hu et al., 2020). Our contribution addresses stability under degree heterogeneity by combining normalization with adaptive aggregation.",
    "reason": "The span lists several GNN works in sequence without signaling how each builds upon or contrasts with the previous. The connection between scalability, attention, and expressivity is implied but not stated, making the flow abrupt.",
    "start": 310,
    "end": 648,
    "label": "Coherence"
  },
  {
    "span": "(Kim et al., 2015; Park and Lee, 2018;,",
    "document": "Introduction\n\nNeural text classification has benefited from advances in representation learning and architectural innovations such as convolutional and recurrent networks (Kim, 2014; Johnson & Zhang, 2017). However, many approaches still struggle to leverage document structure and long-range dependencies.\n\nPrior studies (Kim et al., 2015; Park and Lee, 2018;, have explored hierarchical encoders and graph-based models to capture context beyond sentence boundaries, but their methods often require extensive feature engineering. More recent work investigates pre-trained transformers fine-tuned for downstream tasks (Devlin et al., 2019; Liu et al., 2019), yet domain shift and data scarcity continue to limit performance. In this paper, we propose a lightweight adaptation module that improves robustness under limited supervision while maintaining competitive accuracy on standard benchmarks.\n",
    "reason": "Unbalanced citation punctuation: trailing comma and missing closing parenthesis in a multi-citation.",
    "start": 322,
    "end": 361,
    "label": "Format"
  },
  {
    "span": "Abadi et al. (2016) proposed DP-SGD for private learning. Li et al. (2021) studied privacy accounting under subsampling. Jayaraman and Evans (2019) evaluated utility-privacy trade-offs in deep models. Carlini et al. (2019) investigated memorization and extraction risks.",
    "document": "Introduction\n\nDifferential privacy (DP) provides rigorous guarantees against disclosure of individual examples while enabling learning from sensitive datasets. In natural language processing, DP faces acute challenges because token distributions are heavy-tailed and models are prone to memorizing rare sequences. Practical DP-NLP thus requires careful tuning of noise, clipping, and data curation to maintain utility.\n\nAbadi et al. (2016) proposed DP-SGD for private learning. Li et al. (2021) studied privacy accounting under subsampling. Jayaraman and Evans (2019) evaluated utility-privacy trade-offs in deep models. Carlini et al. (2019) investigated memorization and extraction risks.\n\nWe introduce a frequency-aware clipping mechanism with layer-wise noise allocation tailored to subword tokenization, reducing degradation on rare tokens while preserving end-to-end privacy budgets under the moments accountant.",
    "reason": "The span provides a series of citations without connecting their contributions or explaining how they relate to DP in NLP. The lack of transitions leads to an abrupt, incoherent list.",
    "start": 420,
    "end": 690,
    "label": "Coherence"
  },
  {
    "span": "Federated learning has proposed a variety of strategies for personalization, such as proximal regularization, model interpolation, and bi-level optimization (McMahan et al., 2017; Li et al., 2020; Fallah et al., 2020; Dinh et al., 2020).",
    "document": "Related Work\n\nFederated personalization. Heterogeneous client data distributions motivate personalization to avoid global model underfitting. Federated learning has proposed a variety of strategies for personalization, such as proximal regularization, model interpolation, and bi-level optimization (McMahan et al., 2017; Li et al., 2020; Fallah et al., 2020; Dinh et al., 2020). Additional streams consider lightweight adapters and representation sharing.\n\nPrivacy-aware adaptation. Differential privacy and secure aggregation constrain the information shared during personalization, often limiting per-client improvements.\n\nWe introduce a communication-efficient framework that learns a shared representation and client-specific linear heads under strict privacy budgets.\n",
    "reason": "The span lists approaches and citations without stating what limitation remains or how these works relate to the authors’ method; it immediately transitions to their contribution later without explicitly highlighting the gap.",
    "start": 142,
    "end": 379,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Numerous benchmarks now evaluate causal reasoning in QA.",
    "document": "Introduction\n\nQuestion answering has progressed rapidly with large pretrained models, yet systematic generalization and reasoning remain challenging. Beyond span extraction and multiple-choice formats, newer evaluations target compositionality, counterfactuals, and causal inference aspects.\n\nNumerous benchmarks now evaluate causal reasoning in QA. We contribute a diagnostic suite focusing on interventions and mediators, paired with templates for generating minimal counterfactuals. Our analysis reveals that models often conflate correlation with causation, especially under lexical overlap and spurious cues.",
    "reason": "References 'numerous benchmarks' without citing any specific datasets or papers.",
    "start": 293,
    "end": 349,
    "label": "Unsupported_claim"
  },
  {
    "span": "Anchor-free detectors localize keypoints without predefined boxes (Zhou et al., 2019). Non-maximum suppression remains a bottleneck in post-processing (Bodla et al., 2017). Larger training sets increase robustness to occlusion (Gupta et al., 2019).",
    "document": "Related Work\n\nObject detection has evolved from anchor-based frameworks to recent anchor-free approaches, with attention to both accuracy and computational efficiency. Post-processing and data scaling also influence performance.\n\nAnchor-free detectors localize keypoints without predefined boxes (Zhou et al., 2019). Non-maximum suppression remains a bottleneck in post-processing (Bodla et al., 2017). Larger training sets increase robustness to occlusion (Gupta et al., 2019). We propose a training-time debiasing objective for crowded scenes.\n\nOur debiasing term reduces suppression errors by aligning class-conditional overlaps with soft masks.",
    "reason": "The cited statements are juxtaposed without showing how anchor-free design, NMS, and dataset size are related; there are no transitions bridging these topics.",
    "start": 230,
    "end": 478,
    "label": "Coherence"
  },
  {
    "span": "Netflix Prize dataset",
    "document": "Introduction\n\nRecommender systems for implicit feedback must disentangle user preference from exposure bias and popularity effects. Matrix factorization and neural collaborative filtering methods have shown strong performance, but evaluation remains sensitive to dataset characteristics and negative sampling strategies. The Netflix Prize dataset has historically driven algorithmic innovation in collaborative filtering by providing a large-scale benchmark of user–item ratings. However, modern deployments increasingly rely on implicit logs, calling for methods that handle missing-not-at-random data. We introduce a counterfactual training objective that leverages inverse propensity weighting with exposure models learned from impression logs.\n",
    "reason": "First mention of a specific dataset lacks a supporting citation to the original source.",
    "start": 325,
    "end": 346,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Lopez et al. 2020)",
    "document": "Related Work\n\nDomain generalization studies exploit invariant predictors across environments (Arjovsky et al., 2019; Gulrajani and Lopez-Paz, 2021). For text classification, style-invariant encoders discourage spurious correlations by randomizing surface cues (Riva and Ng, 2020). Data augmentation with back-translation remains a strong baseline (Sennrich et al., 2016). A few works consider sample reweighting guided by causal criteria (Magliacane et al., 2018; Schölkopf, 2019), while others propose risk extrapolation penalties (Koyama and Yamaguchi, 2020). Closest to our setting is class-conditional alignment (Lopez et al. 2020), which assumes access to unlabeled target data during training.",
    "reason": "Missing comma between author and year inside a parenthetical citation; should be '(Lopez et al., 2020)'.",
    "start": 616,
    "end": 635,
    "label": "Format"
  },
  {
    "span": "Bias mitigation techniques span data augmentation, adversarial debiasing, and counterfactual data generation (Zhao et al., 2017; Zhang et al., 2018; Garg et al., 2019; Maudslay et al., 2019). Evaluation metrics include demographic parity, equalized odds, and WEAT-based measures (Hardt et al., 2016; Caliskan et al., 2017).",
    "document": "Related Work\n\nFairness in NLP examines how models encode and propagate social biases across tasks such as coreference, sentiment analysis, and language modeling. Methods vary in their intervention points—data, objectives, architectures, and post-processing.\n\nBias mitigation techniques span data augmentation, adversarial debiasing, and counterfactual data generation (Zhao et al., 2017; Zhang et al., 2018; Garg et al., 2019; Maudslay et al., 2019). Evaluation metrics include demographic parity, equalized odds, and WEAT-based measures (Hardt et al., 2016; Caliskan et al., 2017).\n\nRecent work also emphasizes domain-specific fairness definitions, auditing pipelines, and trade-offs with utility, yet operational guidance for deployment remains limited.",
    "reason": "Only enumerates prior techniques and metrics without connecting them to the authors’ stance, gap, or methodological choices (definition a/c).",
    "start": 259,
    "end": 582,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Debiasing methods can be grouped into pre-processing, in-processing, and post-processing, with applications in toxicity detection, coreference resolution, and sentiment analysis (Bolukbasi et al., 2016; Zhao et al., 2018; Park et al., 2018; Sun et al., 2019; Prost et al., 2019).",
    "document": "Introduction\n\nFairness in NLP\n\nAs language technologies are deployed at scale, concerns about representational and allocative harms have intensified. Bias may arise from imbalanced data, model architectures, or deployment contexts, necessitating mitigation strategies and rigorous evaluation.\n\nDebiasing methods can be grouped into pre-processing, in-processing, and post-processing, with applications in toxicity detection, coreference resolution, and sentiment analysis (Bolukbasi et al., 2016; Zhao et al., 2018; Park et al., 2018; Sun et al., 2019; Prost et al., 2019).\n\nMeasurement and trade-offs\n\nFairness metrics in NLP span demographic parity, equalized odds, and counterfactual token fairness; however, improvements in one objective may degrade others, and distribution shift can invalidate measured gains.\n\nScope of this work\n\nWe examine fairness under domain transfer, focusing on robustness of mitigation strategies across topic and demographic shifts.",
    "reason": "The span categorizes and cites prior work but does not relate these methods to the authors’ goals or identify shortcomings they aim to address, thus lacking synthesis (criteria a and c).",
    "start": 294,
    "end": 573,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Shelton, 2018",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a dominant paradigm for learning over relational structures. Message-passing architectures generalize convolution to irregular domains (Gilmer et al., 2017), and variants incorporate attention to capture long-range dependencies (Velickovic et al., 2018). Expressivity has been studied via the Weisfeiler–Lehman test, linking GNN capabilities to graph isomorphism heuristics (Xu et al., 2019). Spectral methods connect convolution to Laplacian eigenbases, offering complementary insights into stability and transfer (Bruna et al., 2014). For scalable training on large graphs, sampling and partitioning strategies are common (Hamilton et al., 2017). Recent surveys synthesize advances in architectures, pretraining, and applications to molecules and knowledge graphs (Bianchi and Rossi, 2021), see also (Shelton, 2018 for a discussion of spectral filter localization. Our work contributes by aligning pretraining objectives with downstream structural probes, improving cross-domain generalization without task-specific tuning (Miller and Zhou, 2022).",
    "reason": "Missing closing parenthesis in the parenthetical citation.",
    "start": 857,
    "end": 871,
    "label": "Format"
  },
  {
    "span": "the YOLO family dominates real-time detection on edge devices",
    "document": "Introduction\n\nReal-time object detection has seen rapid progress driven by one-stage architectures optimized for throughput. Advances in backbone design, quantization-aware training, and memory-efficient feature pyramids have enabled deployment on mobile and embedded platforms. In current practice, the YOLO family dominates real-time detection on edge devices, with numerous variants trading accuracy for speed under tight latency budgets. However, these models often require platform-specific tuning to maintain accuracy under aggressive quantization. We present a hardware-agnostic distillation framework that preserves calibration across device targets without per-target retraining.",
    "reason": "Makes a broad claim about the dominance of a specific method line in practice without citing comparative studies or benchmarks (rule b/d; prior work trend needs citations).",
    "start": 300,
    "end": 361,
    "label": "Unsupported_claim"
  },
  {
    "span": "We evaluate on the widely used SQuAD v1.1 and v2.0 datasets.",
    "document": "Introduction\n\nMachine reading comprehension (MRC) aims to answer questions about a given context paragraph. While neural architectures have made considerable progress, robustness to adversarial perturbations and unanswerable questions remains an open challenge. We evaluate on the widely used SQuAD v1.1 and v2.0 datasets. To assess out-of-domain generalization, we also report results on NewsQA and Natural Questions. Our contributions include a simple calibration mechanism that reduces overconfident predictions without sacrificing in-domain accuracy.",
    "reason": "First mention of datasets (SQuAD v1.1/v2.0) lacks citations to the original sources (rule a).",
    "start": 262,
    "end": 322,
    "label": "Unsupported_claim"
  },
  {
    "span": "Rashkin et al. (2019) studied empathetic response generation. Miller and Rollnick (2013) described motivational interviewing strategies. Coppersmith et al. (2018) examined suicide risk signals in social media.",
    "document": "Related Work\n\nConversational agents for mental health require balancing empathy, safety, and goal-directed support. Prior work spans generative dialogue modeling, clinical communication frameworks, and risk assessment from user content.\n\nRashkin et al. (2019) studied empathetic response generation. Miller and Rollnick (2013) described motivational interviewing strategies. Coppersmith et al. (2018) examined suicide risk signals in social media. Henderson et al. (2019) surveyed ethical considerations for conversational AI in healthcare.\n\nWe build a safety-aware dialogue policy that integrates empathy cues and risk detection signals with constrained decoding.",
    "reason": "The span abruptly moves from empathetic response generation to clinical counseling techniques and then to risk detection without explaining how these areas relate, lacking transitional statements or explicit connections.",
    "start": 238,
    "end": 447,
    "label": "Coherence"
  },
  {
    "span": "(Garcia and Kim, 2018",
    "document": "Related Work\n\nUnsupervised domain adaptation leverages unlabeled target data to mitigate distribution shift (Ben-David et al., 2010; Ganin et al., 2016). Discrepancy-based methods minimize distances between source and target feature distributions (Long et al., 2015; Tzeng et al., 2017), while adversarial approaches align domains via domain-confusion losses (Ganin et al., 2016; Shen et al., 2018). Self-training techniques further exploit target pseudo-labels to refine decision boundaries (Zou et al., 2019; Lee, 2013).\n\nSeveral studies highlight the sensitivity of alignment to class-conditional mismatch (Saito et al., 2018; Zhang et al., 2019) and propose class-aware alignment losses (Kang et al., 2019). Recent work introduces adaptive confidence thresholds to stabilize pseudo-labeling (French et al., 2018; Berthelot et al., 2019). However, prior analyses rarely isolate the role of batch composition on alignment stability (Garcia and Kim, 2018 and its interaction with entropy minimization remains underexplored.\n",
    "reason": "Missing closing parenthesis in a parenthetical citation.",
    "start": 934,
    "end": 955,
    "label": "Format"
  },
  {
    "span": "Unsupervised anomaly detection in time series employs reconstruction error with autoencoders, probabilistic forecasting, and graph-based dependencies (Zong et al., 2018; Lai et al., 2018; Audibert et al., 2020; Deng and Hooi, 2021). Benchmarks and evaluation metrics vary across datasets and domains (Wu and Keogh, 2021; Hundman et al., 2018).",
    "document": "Introduction\n\nDetecting anomalies in multivariate time series is critical for monitoring complex systems in finance, manufacturing, and cloud services. The lack of labels and nonstationary patterns complicate both modeling and evaluation.\n\nUnsupervised anomaly detection in time series employs reconstruction error with autoencoders, probabilistic forecasting, and graph-based dependencies (Zong et al., 2018; Lai et al., 2018; Audibert et al., 2020; Deng and Hooi, 2021). Benchmarks and evaluation metrics vary across datasets and domains (Wu and Keogh, 2021; Hundman et al., 2018).\n\nWe present a unified scoring framework that calibrates detection thresholds via conformal prediction across heterogeneous channels. Our analysis highlights when reconstruction-based and forecasting-based signals are complementary and demonstrates improved false-positive control.",
    "reason": "The span lists methods and benchmark variability but does not articulate how these points motivate the authors’ framework or what specific gap remains, demonstrating lack of synthesis.",
    "start": 240,
    "end": 583,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Classical ARIMA models capture linear dependencies (Box et al., 2015). Deep sequence models improve long-horizon accuracy (Lim et al., 2021). Probabilistic forecasting estimates full predictive distributions (Salinas et al., 2019). Transformers reduce reliance on recurrence (Zhou et al., 2021).",
    "document": "Related Work\n\nTime series forecasting methods span classical statistical models and modern deep architectures. Selecting an approach involves trade-offs among interpretability, capacity, and the ability to represent uncertainty.\n\nClassical ARIMA models capture linear dependencies (Box et al., 2015). Deep sequence models improve long-horizon accuracy (Lim et al., 2021). Probabilistic forecasting estimates full predictive distributions (Salinas et al., 2019). Transformers reduce reliance on recurrence (Zhou et al., 2021).\n\nThese strands suggest complementary strengths that are rarely integrated coherently. Our method formulates a modular forecaster that decouples dynamics, seasonality, and uncertainty estimation.",
    "reason": "The span is a sequence of unconnected statements covering disparate approaches with no transitions or explicit explanation of their relations, causing coherence issues.",
    "start": 230,
    "end": 525,
    "label": "Coherence"
  },
  {
    "span": "Kim et al. (2020",
    "document": "Related Work\n\nFew-shot classification has been widely studied using metric-based and optimization-based approaches (Snell et al., 2017; Finn et al., 2017). Kim et al. (2020 propose to adapt class prototypes with task-conditioned hypernetworks, reducing the reliance on large support sets. Other works inject unlabeled data via transductive inference (Liu et al., 2019; Dhillon et al., 2020). We build on these ideas by introducing a distributionally robust learner that accounts for prototype uncertainty.",
    "reason": "Missing closing parenthesis in the narrative citation; should be Kim et al. (2020).",
    "start": 156,
    "end": 172,
    "label": "Format"
  },
  {
    "span": "Open-source systems for end-to-end speech translation now support over 70 languages.",
    "document": "Introduction\n\nEnd-to-end speech translation (ST) directly maps audio to target-language text, reducing error propagation compared to cascaded ASR→MT pipelines (Weiss et al., 2017). Self-supervised pre-training on unlabeled audio and large-scale multilingual text has significantly improved ST robustness to noise and domain shift (Baevski et al., 2020; Tang et al., 2021). Open-source systems for end-to-end speech translation now support over 70 languages. Nevertheless, long-form discourse coherence and low-resource languages remain open challenges, calling for improved segmentation, context handling, and data augmentation.\n",
    "reason": "Quantitative claim about the capabilities of open-source systems lacks any citations or evidence.",
    "start": 373,
    "end": 457,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior studies report that over 60% of crowdworkers rely on heuristics rather than comprehension when answering reading comprehension tasks.",
    "document": "Introduction Crowdsourcing enables rapid collection of annotations for reading comprehension, but annotation quality varies widely and is sensitive to interface design and incentives (Kittur et al., 2008; Pavlick and Kwiatkowski, 2019). Prior studies report that over 60% of crowdworkers rely on heuristics rather than comprehension when answering reading comprehension tasks. This observation motivates our proposal of adaptive attention checks that target shallow strategies while preserving task flow.",
    "reason": "Claims a numerical finding ('over 60%') from prior studies without any citation, violating rule (b).",
    "start": 237,
    "end": 376,
    "label": "Unsupported_claim"
  },
  {
    "span": "We use the standard IEMOCAP and MELD train/dev/test splits commonly adopted in the literature.",
    "document": "Introduction\n\nMultimodal emotion recognition integrates acoustic, visual, and textual cues to identify affective states in conversation. Progress has been driven by transformer architectures that model cross-modal interactions and by datasets featuring dyadic and multi-party dialogues. We use the standard IEMOCAP and MELD train/dev/test splits commonly adopted in the literature. However, reported results are often not directly comparable due to differing preprocessing of transcripts, speaker normalization, and class aggregation.\n\nTo address these issues, we propose a cross-modal alignment module with speech-aware attention and release a standardized preprocessing pipeline. We further examine robustness to missing modalities and cross-corpus transfer between acted and natural conversations.",
    "reason": "Mentions specific datasets and standard splits at first mention without any citations to the datasets or protocols, violating rule (a).",
    "start": 287,
    "end": 381,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Klein et al.), 2018",
    "document": "Introduction\n\nVision Transformers (ViTs) adapt transformer architectures to images by treating patches as tokens (Dosovitskiy et al., 2021). Subsequent work improves data efficiency via distillation and strong augmentation (Touvron et al., 2021; Steiner et al., 2022) and studies scaling laws across compute regimes (Zhai et al., 2022).\n\nHybrid convolution-transformer models aim to capture local inductive biases while retaining global context (Xiao et al., 2021; Graham et al., 2021), and hierarchical variants introduce multi-scale processing (Liu et al., 2021; Wang et al., 2021). Regularization and token pruning improve latency (Bolya et al., 2022; Rao et al., 2021).\n\nFor efficient training, strong baselines have been established by (Klein et al.), 2018 and further refined by augmentation strategies that stabilize optimization (Yamada et al., 2020). Our contribution focuses on curriculum token masking to reduce redundancy during pretraining.\n\nWe provide analyses of learned attention maps and ablations on token selection strategies across CIFAR and ImageNet subsets.",
    "reason": "Year placed outside the parenthetical citation; the correct form would include the year inside the same parentheses as the authors.",
    "start": 741,
    "end": 761,
    "label": "Format"
  },
  {
    "span": "Self-supervised pretraining has dramatically improved ASR performance across languages (Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2022).",
    "document": "Introduction\n\nAutomatic Speech Recognition at Scale. End-to-end ASR models have benefited from larger datasets and compute, yet labeled speech remains scarce for many languages and domains. Self-supervised pretraining has dramatically improved ASR performance across languages (Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2022). Complementary work examines multilingual training, pronunciation lexicons, and language model fusion to reduce word error rates (Pratap et al., 2020; Gulati et al., 2020; Hannun et al., 2014).\n\nDeployment Constraints. On-device inference and streaming latency constraints motivate compact architectures and chunk-wise processing.\n\nWe introduce an encoder distillation approach tailored for streaming ASR on edge devices.",
    "reason": "The span makes a broad claim about self-supervised gains without linking it to deployment constraints or explaining why distillation is needed, failing to articulate the paper's motivation relative to prior work (definition c).",
    "start": 190,
    "end": 337,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Gilmer et al. (2017) propose message passing neural networks for quantum property prediction. Xu et al. (2019) analyze the expressivity limits of GNNs via the Weisfeiler–Lehman test. Molecular fingerprints have been improved with attention mechanisms (Kearnes et al., 2016). Wang et al. (2020) study contrastive learning for molecule-level representations.",
    "document": "Related Work\n\nLearning molecular representations has advanced rapidly with graph neural networks (GNNs) that encode atoms and bonds as nodes and edges. Beyond supervised property prediction, recent efforts emphasize self-supervision and transferability to low-data regimes, as well as uncertainty estimation for safer molecular screening.\n\nGilmer et al. (2017) propose message passing neural networks for quantum property prediction. Xu et al. (2019) analyze the expressivity limits of GNNs via the Weisfeiler–Lehman test. Molecular fingerprints have been improved with attention mechanisms (Kearnes et al., 2016). Wang et al. (2020) study contrastive learning for molecule-level representations.\n\nIn contrast, our approach integrates physical inductive biases into the message updates while coupling them with a pretraining objective tailored to reaction centers, aiming to bridge accuracy and data efficiency.",
    "reason": "This span lists four citations as isolated facts with no connective tissue or explicit relationships among them, resulting in abrupt topic shifts and unclear relevance from one sentence to the next across multiple sentences.",
    "start": 340,
    "end": 696,
    "label": "Coherence"
  },
  {
    "span": "Data augmentation for low-resource machine translation includes back-translation (Sennrich et al., 2016), noising and denoising objectives (Lample et al., 2018), switchout and token perturbations (Wang et al., 2018), paraphrasing via pivoting (Mallinson et al., 2017), synthetic corpora from multilingual models (Edunov et al., 2018), and code-mixing with bilingual lexicons (Niu and Carpuat, 2020).",
    "document": "Related Work\n\nImproving translation quality in low-resource scenarios often hinges on augmenting data or transferring knowledge from related languages. Numerous techniques have been proposed to synthesize or exploit auxiliary signals when parallel corpora are scarce.\n\nData augmentation for low-resource machine translation includes back-translation (Sennrich et al., 2016), noising and denoising objectives (Lample et al., 2018), switchout and token perturbations (Wang et al., 2018), paraphrasing via pivoting (Mallinson et al., 2017), synthetic corpora from multilingual models (Edunov et al., 2018), and code-mixing with bilingual lexicons (Niu and Carpuat, 2020).\n\nWe re-examine augmentation when source and target exhibit strong morphological divergence. Our method learns morphology-aware perturbations via inflection generators and constrains back-translation with lemma-preserving decoding. Results on Uralic and Semitic pairs show consistent gains over standard back-translation and switchout.",
    "reason": "Provides a list of augmentation techniques without explaining why they are insufficient for morphologically divergent languages or how the proposed approach interfaces with them; lacks synthesis (criterion a/b/c).",
    "start": 269,
    "end": 668,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The widely used QuAC-v2 dataset includes conversational unanswerable questions",
    "document": "Related Work\n\nConversational question answering (CQA) extends single-turn QA by modeling sequential dependencies and clarifications. The widely used QuAC-v2 dataset includes conversational unanswerable questions, which require systems to recognize when information is missing rather than hallucinate answers. Prior neural approaches incorporate dialog history via hierarchical encoders and leverage turn-level supervision to manage topic drift.\n\nWhile progress on leaderboards is steady, evaluation still overemphasizes span extraction and underweights pragmatic reasoning and calibration.",
    "reason": "Mentions a specific dataset and its characteristic at first mention without providing a citation (rule a).",
    "start": 133,
    "end": 211,
    "label": "Unsupported_claim"
  },
  {
    "span": "Over the last decade, demand forecasting errors have cost electric utilities billions of dollars annually in imbalance penalties and reserve procurement.",
    "document": "Introduction\n\nAccurate short-term load forecasting underpins reliable and cost-effective power system operation. Growing penetration of distributed energy resources and electrification of end uses introduce volatility and nonstationarity that strain classical models. Over the last decade, demand forecasting errors have cost electric utilities billions of dollars annually in imbalance penalties and reserve procurement. This motivates probabilistic forecasting methods that quantify uncertainty and support risk-aware dispatch. We present a hierarchical transformer with weather-aware embeddings and coherent quantile reconciliation for multi-horizon load forecasts.",
    "reason": "Claims a large economic impact with specific magnitude and timeframe without citing industry reports or studies; per rule (b) such statistics require evidence.",
    "start": 268,
    "end": 421,
    "label": "Unsupported_claim"
  },
  {
    "span": "Early fusion concatenates modality features before classification (Poria et al., 2017). Tensor fusion models multiplicative interactions (Zadeh et al., 2017). CTC acoustic models improve phoneme alignment (Graves et al., 2006). Pretrained vision transformers provide stronger video features (Dosovitskiy et al., 2020).",
    "document": "Related Work\n\nMultimodal Sentiment Analysis\n\nModels for multimodal sentiment and emotion recognition seek to integrate information from text, audio, and video. A primary design choice is the fusion strategy, which can be early, late, or hybrid, and must contend with temporal asynchrony and modality-specific noise.\n\nFusion Strategies and Representations\n\nEarly fusion concatenates modality features before classification (Poria et al., 2017). Tensor fusion models multiplicative interactions (Zadeh et al., 2017). CTC acoustic models improve phoneme alignment (Graves et al., 2006). Pretrained vision transformers provide stronger video features (Dosovitskiy et al., 2020).\n\nOur Approach\n\nWe propose a cross-modal adapter that aligns modalities through shared bottlenecks and temporal contrastive learning, achieving robustness to missing modalities and improved interpretability.",
    "reason": "The span interleaves fusion methods with modality-specific architectures (CTC for acoustics, ViTs for video) without transitions or explanation of how they connect to fusion strategies. The relationships among these works are implied but not articulated, reducing coherence across the sentences.",
    "start": 356,
    "end": 674,
    "label": "Coherence"
  },
  {
    "span": "Fairness in federated learning has been explored through metrics such as equalized odds, demographic parity, and client-level fairness (Hardt et al., 2016; Li et al., 2019; Mohri et al., 2019). Algorithmic approaches include reweighting gradients (Li et al., 2020), distributionally robust optimization (Sagawa et al., 2020), personalized models (Deng et al., 2020; Fallah et al., 2020), and fair aggregation rules (Qiu et al., 2022).",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative training without centralizing raw data, but significant heterogeneity in client data and participation can lead to disparate outcomes. Addressing fairness in this setting requires both appropriate measurements and algorithms that mitigate inequities across clients and subpopulations.\n\nFairness in federated learning has been explored through metrics such as equalized odds, demographic parity, and client-level fairness (Hardt et al., 2016; Li et al., 2019; Mohri et al., 2019). Algorithmic approaches include reweighting gradients (Li et al., 2020), distributionally robust optimization (Sagawa et al., 2020), personalized models (Deng et al., 2020; Fallah et al., 2020), and fair aggregation rules (Qiu et al., 2022).\n\nRecent work has also considered communication constraints and participation bias, proposing partial participation-aware schedulers and incentive mechanisms (Wang et al., 2021; Kang et al., 2022). In contrast, our work targets a complementary axis: we provide a calibration-theoretic objective that ensures equitable predictive uncertainty across clients while preserving global accuracy, and we introduce a plug-in aggregation strategy that provably enforces client-level calibration fairness under bounded heterogeneity.",
    "reason": "Summarizes prior metrics and algorithms but does not connect them to the paper’s problem or show what gap remains, leaving the author’s stance implicit (criterion a/c).",
    "start": 346,
    "end": 780,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Vision Transformers have been adopted for medical image segmentation in numerous architectures such as TransUNet (Chen et al., 2021), Swin-UNet (Cao et al., 2021), LeViT-UNet (Xu et al., 2021), and hybrid CNN-ViT decoders (Hatamizadeh et al., 2022). Recent studies compare windowed attention, shifted windows, and hierarchical tokenization, showing strong performance on multi-organ CT and cardiac MRI (Tang et al., 2022; Chen et al., 2022).",
    "document": "Related Work\n\nMedical image segmentation models must balance global context with fine-grained boundary precision. CNNs provide strong inductive biases for locality, while Transformers excel at modeling long-range dependencies. Hybrid designs attempt to leverage both.\n\nVision Transformers have been adopted for medical image segmentation in numerous architectures such as TransUNet (Chen et al., 2021), Swin-UNet (Cao et al., 2021), LeViT-UNet (Xu et al., 2021), and hybrid CNN-ViT decoders (Hatamizadeh et al., 2022). Recent studies compare windowed attention, shifted windows, and hierarchical tokenization, showing strong performance on multi-organ CT and cardiac MRI (Tang et al., 2022; Chen et al., 2022).\n\nWhile effective, these models can be data-hungry and brittle under domain shifts across scanners and protocols. We propose a token-sparse consistency training strategy with structure-aware pseudo-label refinement that reduces reliance on labeled data and improves cross-site generalization without architectural changes.",
    "reason": "Lists existing Transformer-based segmentation works and results without explaining their limitations relative to the proposed approach; no explicit gap is identified within the span (criterion a/b).",
    "start": 269,
    "end": 710,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Model-based RL improves sample efficiency with learned dynamics (Sutton, 1990; Janner et al., 2019; Hafner et al., 2019). Off-policy value-based methods such as DQN and SAC are strong baselines (Mnih et al., 2015; Haarnoja et al., 2018). Recent work integrates latent imagination with policy optimization (Hafner et al., 2020). Our method is also model-based.",
    "document": "Introduction\n\nReinforcement learning (RL) promises data-efficient decision making but often requires many interactions to learn effective policies. Research spans model-free and model-based paradigms, with growing interest in leveraging learned world models.\n\nModel-based RL improves sample efficiency with learned dynamics (Sutton, 1990; Janner et al., 2019; Hafner et al., 2019). Off-policy value-based methods such as DQN and SAC are strong baselines (Mnih et al., 2015; Haarnoja et al., 2018). Recent work integrates latent imagination with policy optimization (Hafner et al., 2020). Our method is also model-based.\n\nWe study performance across continuous control domains and analyze the sensitivity to model errors and planning horizons.",
    "reason": "The span lists prior lines of work and then states that the proposed method is model-based, but it does not specify the shortcoming addressed, the authors' perspective, or how their approach differs.",
    "start": 260,
    "end": 619,
    "label": "Lacks_synthesis"
  },
  {
    "span": "FedAvg averages client updates to train a global model (McMahan et al., 2017). FedProx stabilizes training with a proximal term (Li et al., 2020). pFedMe uses Moreau envelopes for personalization (Dinh et al., 2020). Ditto learns personalized models alongside a global one (Li et al., 2021). Meta-learning approaches adapt quickly to clients with few samples (Fallah et al., 2020).",
    "document": "Related Work\n\nPersonalization in Federated Learning\n\nFederated learning (FL) faces heterogeneity across clients in data distribution, capacity, and objectives. Personalization strategies aim to tailor models while preserving cross-client knowledge sharing and privacy. Methods vary in whether they personalize parameters, representations, or decision layers, and in how strongly they couple local and global objectives.\n\nFedAvg averages client updates to train a global model (McMahan et al., 2017). FedProx stabilizes training with a proximal term (Li et al., 2020). pFedMe uses Moreau envelopes for personalization (Dinh et al., 2020). Ditto learns personalized models alongside a global one (Li et al., 2021). Meta-learning approaches adapt quickly to clients with few samples (Fallah et al., 2020).\n\nRepresentation and Head Personalization\n\nOther approaches share a common backbone and personalize prediction heads (Arivazhagan et al., 2019), or learn feature extractors with client-specific adapters (Collins et al., 2021). Regularization-based methods encourage proximity to the global model while allowing local deviation (Acar et al., 2021).\n\nOur Focus\n\nWe propose a bilevel personalized objective that decouples shared representations from client heads and dynamically reweights clients based on estimated domain shift.",
    "reason": "The span is a sequence of independent sentences summarizing different FL methods without transitions or explicit comparative links, making the connections abrupt and the relationships merely implied.",
    "start": 421,
    "end": 802,
    "label": "Coherence"
  },
  {
    "span": "Dinan et al. (2019) curated datasets for safer conversational AI. Toxicity classifiers can be used to filter unsafe outputs (Gehman et al., 2020). Persona consistency improves engagement in open-domain chat (Zhang et al., 2018). Li et al. (2016) introduced MMI decoding to increase response diversity.",
    "document": "Introduction\n\nSafety in Open-Domain Dialogue. Building conversational agents that are both engaging and safe remains a central challenge. Prior work has characterized unsafe behaviors such as toxicity, harassment, misinformation, and unsafe advice, proposing datasets, safety taxonomies, and moderation strategies (Dinan et al., 2019; Xu et al., 2020; Ousidhoum et al., 2021).\n\nMethods and Controls. Dinan et al. (2019) curated datasets for safer conversational AI. Toxicity classifiers can be used to filter unsafe outputs (Gehman et al., 2020). Persona consistency improves engagement in open-domain chat (Zhang et al., 2018). Li et al. (2016) introduced MMI decoding to increase response diversity. Recent training-time interventions include reinforcement learning from human feedback and safety-specific instruction tuning (Stiennon et al., 2020; Bai et al., 2022). Our work focuses on aligning decoding-time constraints with safety detectors to reduce harmful generations while preserving utility.",
    "reason": "The sequence lists safety datasets, toxicity filtering, persona consistency, and MMI decoding without explaining how engagement or diversity relate to safety, creating abrupt topical shifts and lacking explicit connections.",
    "start": 400,
    "end": 701,
    "label": "Coherence"
  },
  {
    "span": "In a previous study, the authors showed that character-CNNs outperform byte-pair models on de-identification across three hospital systems.",
    "document": "Related Work\n\nClinical named entity recognition (NER) and de-identification are critical for enabling secondary use of electronic health records. Early work used conditional random fields with handcrafted lexical and orthographic features, while recent approaches favor contextual encoders with domain-adaptive pretraining. In a previous study, the authors showed that character-CNNs outperform byte-pair models on de-identification across three hospital systems. Concurrent efforts investigate leveraging synthetic notes and weak labeling to reduce annotation costs, and cross-domain adaptation to address dataset shift between institutions. Our approach integrates span-level contrastive learning with curriculum adaptation to improve cross-hospital robustness.",
    "reason": "References a specific prior study and its findings without citing it, which violates rule (a) regarding first mentions of studies.",
    "start": 324,
    "end": 463,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Garcia, 2017",
    "document": "Introduction\n\nModel compression techniques aim to preserve accuracy while reducing inference cost. Earlier analyses (Garcia, 2017 indicate that pruning strategies targeting structured components yield more predictable speedups than unstructured sparsity. Subsequent work refined low-rank factorization with data-aware initialization (Kim and Huang, 2019) and combined quantization with knowledge distillation for edge deployment (Rao et al., 2020).",
    "reason": "Missing closing parenthesis in a parenthetical citation: should be “(Garcia, 2017)”.",
    "start": 116,
    "end": 129,
    "label": "Format"
  },
  {
    "span": "Sato et. al. (2016)",
    "document": "Introduction\n\nData augmentation has become integral to improving generalization in both vision and text. In computer vision, geometric and photometric transformations underlie strong invariances (Shorten and Khoshgoftaar, 2019), while mixup and CutMix promote linear behavior between samples (Zhang et al., 2018; Yun et al., 2019). For NLP, paraphrasing, back-translation, and token-level perturbations help mitigate overfitting (Fadaee et al., 2017; Wei and Zou, 2019). Sato et. al. (2016) introduced task-aware augmentation policies that adapt to model confidence, but their approach can amplify biases without calibration. We extend this idea with uncertainty-aware scheduling and demonstrate consistent gains under label noise.\n\nRelated Work\n\nAutomatic augmentation search has been explored with reinforcement learning and population-based training (Cubuk et al., 2019; Lim et al., 2019). Our method complements these by incorporating risk-sensitive objectives.",
    "reason": "Misspelled citation component: \"et. al.\" should be \"et al.\" without a period after \"et\".",
    "start": 471,
    "end": 490,
    "label": "Format"
  },
  {
    "span": "Explainability in medical imaging has been pursued through saliency and gradient-based maps (Simonyan et al., 2013; Sundararajan et al., 2017), class activation and attribution methods such as CAM/Grad-CAM (Zhou et al., 2016; Selvaraju et al., 2017), perturbation-based occlusion tests (Zeiler and Fergus, 2014), concept-level explanations (Kim et al., 2018), and counterfactual generation (Goyal et al., 2019).",
    "document": "Introduction\n\nAI systems for medical imaging face adoption barriers due to lack of transparency and difficulty validating explanations against clinical reasoning. Reliability and faithfulness of explanations are critical for safety and accountability.\n\nExplainability in medical imaging has been pursued through saliency and gradient-based maps (Simonyan et al., 2013; Sundararajan et al., 2017), class activation and attribution methods such as CAM/Grad-CAM (Zhou et al., 2016; Selvaraju et al., 2017), perturbation-based occlusion tests (Zeiler and Fergus, 2014), concept-level explanations (Kim et al., 2018), and counterfactual generation (Goyal et al., 2019).\n\nHowever, most studies prioritize visual plausibility over quantitative clinical validity, and few incorporate reader studies with practicing radiologists.\n\nWe introduce ClinFaith, a protocol and toolkit that couples calibrated attribution with protocolized radiologist assessment, reporting statistically grounded measures of explanation faithfulness.",
    "reason": "The span lists explanation methods without clarifying their shortcomings in clinical contexts or positioning the paper’s approach relative to them.",
    "start": 253,
    "end": 664,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Chen and Rao",
    "document": "Introduction\n\nMultilingual entity normalization remains challenging due to morphological variation and sparse supervision (Upadhyay et al., 2018; Rijhwani et al., 2020). Prior work has leveraged weak lexicons and cross-lingual embeddings to align mentions to canonical forms (Artetxe and Schwenk, 2019). Following Chen and Rao, we adopt a type-constrained decoding strategy but adapt it to zero-shot scenarios where coverage is limited by domain shift. Our framework further integrates pronunciation features and cognate detection (Muller et al., 2021), which we hypothesize reduce search space ambiguity. We evaluate on biomedical and e-commerce datasets to study transfer robustness (Dogan et al., 2014; Pan et al., 2019).",
    "reason": "Narrative citation missing year: 'Chen and Rao' should include the year, e.g., 'Chen and Rao (2020)'.",
    "start": 314,
    "end": 326,
    "label": "Format"
  },
  {
    "span": "Prompting strategies for large language models include manual prompts, prompt tuning, and chain-of-thought to elicit multi-step reasoning (Lester et al., 2021; Brown et al., 2020; Wei et al., 2022). Tool-augmented models connect LMs to external APIs for retrieval, calculators, and code execution (Lewis et al., 2020; Schick et al., 2023; Gao et al., 2023).",
    "document": "Introduction\n\nLarge language models (LLMs) have demonstrated strong few-shot performance across tasks, yet they often struggle with factuality and complex reasoning without external context or structured interfaces. Recent research explores prompting, training, and tool integration to improve reliability.\n\nPrompting strategies for large language models include manual prompts, prompt tuning, and chain-of-thought to elicit multi-step reasoning (Lester et al., 2021; Brown et al., 2020; Wei et al., 2022). Tool-augmented models connect LMs to external APIs for retrieval, calculators, and code execution (Lewis et al., 2020; Schick et al., 2023; Gao et al., 2023).\n\nWe propose a planner–executor framework with verifiable subgoals that grounds decisions in retrieved evidence and symbolic checks. Our evaluation spans open-domain QA, math word problems, and multi-hop reasoning.\n",
    "reason": "The span lists prompting and tool-use approaches without stating how they compare to or motivate the planner–executor framework, leaving the gap and perspective unstated.",
    "start": 308,
    "end": 665,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Patel et al. 3",
    "document": "Introduction\n\nFederated learning (FL) aims to train models across decentralized data silos while preserving user privacy and minimizing communication costs (McMahan et al., 2017; Kairouz et al., 2021). Beyond basic FedAvg, advances in personalization address client heterogeneity via local adaptation and meta-learning (Fallah et al., 2020; Arivazhagan et al., 2019). Robust aggregation mitigates the impact of corrupted updates and adversarial clients (Blanchard et al., 2017; Yin et al., 2018). Privacy guarantees are commonly achieved with secure aggregation and differential privacy mechanisms (Bonawitz et al., 2017; Geyer et al., 2017). Early explorations of domain-specific FL for healthcare and finance demonstrate the potential of cross-institutional collaboration despite non-IID distributions, as shown by Patel et al. 3 in a multi-hospital study of imaging classifiers. In this work, we propose an adaptive regularization scheme that calibrates the strength of personalization based on client drift estimates, improving both stability and fairness across clients.\n",
    "reason": "Wrong use of footnotes/numbering in place of a year; should be a proper author–year citation (e.g., 'Patel et al. (2021)') or formatted as a proper footnote.",
    "start": 817,
    "end": 831,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors reported a 12% lift in CTR using session-based GNNs.",
    "document": "Introduction\n\nPersonalized recommendation has increasingly leveraged graph-based representations to capture high-order user–item interactions. Graph neural networks (GNNs) enable message passing over interaction graphs, which can improve ranking quality in sparse regimes. In industrial settings, session-based recommenders are attractive because they do not require persistent user profiles yet can adapt to rapidly changing preferences.\n\nIn a previous study, the authors reported a 12% lift in CTR using session-based GNNs. Building on that insight, we examine how lightweight graph encoders can be integrated into next-item prediction pipelines with minimal latency overhead. We further explore strategies to stabilize training when item catalogs evolve quickly and cold-start events are frequent.\n\nOur contributions are: (1) a modular encoder for session graphs with efficient neighbor sampling, (2) an analysis of scalability on billion-edge logs, and (3) an ablation on loss functions for balancing short- and long-term preferences.",
    "reason": "Claims findings from a prior study without citing the study. Mentions a quantitative result (12% CTR lift) that requires a reference.",
    "start": 440,
    "end": 525,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Alvarez et. al., 2016)",
    "document": "Related Work\n\nModel-based reinforcement learning (MBRL) promises data efficiency by learning dynamics models to support planning (Deisenroth and Rasmussen, 2011; Chua et al., 2018). Hybrid approaches interleave model-free policy updates with model-based rollouts to reduce bias while leveraging synthetic data (Janner et al., 2019; Kaiser et al., 2019).\n\nUncertainty-aware planning uses ensembles and trajectory sampling to mitigate compounding error, while representation learning improves generalization across tasks (Kurutach et al., 2018; Hafner et al., 2020). Latent dynamics with world models have scaled to complex visual control (Hafner et al., 2019; 2021; Schrittwieser et al., 2020).\n\nSafety-aware MBRL considers constraint satisfaction and risk-sensitive objectives (Thomas et al., 2015; Chow et al., 2018; Alvarez et. al., 2016) but often requires specialized optimization.",
    "reason": "Incorrect abbreviation 'et. al.' in the citation; should be 'et al.' without the extra period after 'et'.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "Recent works have shown dramatic gains on legal entailment and contract analysis.",
    "document": "Introduction\n\nLarge language models are increasingly being adapted to legal text processing tasks such as entailment, clause extraction, and contract risk assessment. While foundation models pretrained on general corpora can transfer to legal tasks, domain adaptation and instruction-tuning remain necessary to capture terminology and structure unique to statutes and contracts. Recent works have shown dramatic gains on legal entailment and contract analysis. However, benchmarking remains complicated by licensing constraints, uneven document formats, and the scarcity of shared evaluation suites with human judgments. In this paper, we present a curated corpus of contracts and judicial opinions with standardized prompts and multi-criteria evaluation to facilitate reproducible progress in legal NLP.\n\nWe focus on three representative tasks: (i) clause-level classification in procurement agreements, (ii) section-level entailment against policy templates, and (iii) opinion summarization with compliance-focused rubrics. Our contributions include a data release framework with document-level metadata, a set of task-specific prompting templates, and a strong baseline based on retrieval-augmented generation. We emphasize evaluation under distribution shift by constructing out-of-domain test sets from a later time period and different jurisdictions.",
    "reason": "Vague reference to 'recent works' reporting gains without any citations to specific studies violates the requirement to cite prior work at first mention.",
    "start": 379,
    "end": 460,
    "label": "Unsupported_claim"
  },
  {
    "span": "Zhang et al.",
    "document": "Introduction\n\nMultimodal sensor fusion has become essential for robust perception under real-world noise and occlusion. Prior work has explored cross-modal alignment with attention mechanisms (Lee and Park, 2021; Duarte et al., 2022) and late fusion with uncertainty weighting (Gonzalez et al., 2019). Following Zhang et al., we propose a lightweight fusion block that adapts to modality-specific reliability without retraining the entire backbone. Our approach differs from calibration-centric methods that require explicit uncertainty estimation (Kendall and Gal, 2017) by learning a reliability proxy from features directly. We also build on recent transformer-based fusion for robotics (Kim et al., 2020) but target streaming settings with strict latency budgets.\n\nRelated work in self-supervised alignment shows that contrastive pretraining can benefit cross-view consistency (He et al., 2020; Chen et al., 2020). In parallel, dynamic routing has been applied to multi-sensor setups to reduce cross-talk (Rahman et al., 2021). These advances motivate our design of a routing-aware fusion layer that is both parameter-efficient and end-to-end trainable.",
    "reason": "Narrative citation missing year; should be formatted as Zhang et al. (YYYY) in narrative style.",
    "start": 312,
    "end": 324,
    "label": "Format"
  },
  {
    "span": "(Chen, 2016,Li et al., 2019)",
    "document": "Related Work\n\nCross-lingual transfer commonly evaluates on multilingual slot-filling and intent datasets. Several corpora (Chen, 2016,Li et al., 2019) have been utilized to measure zero-shot transfer from high-resource to low-resource languages. Recent advances combine bilingual lexicons with adversarial alignment (Park and Luo, 2020) and leverage parallel data for consistency regularization (Gao et al., 2021).",
    "reason": "Incorrect separator in multiple citations: missing space and semicolon; should be “(Chen, 2016; Li et al., 2019)”.",
    "start": 122,
    "end": 150,
    "label": "Format"
  },
  {
    "span": "Think-aloud protocols are the standard in usability studies.",
    "document": "Introduction\n\nUsability evaluation methods (UEMs) seek to uncover breakdowns in user interaction and inform iterative design. Lab-based testing and remote unmoderated studies each offer distinct advantages in cost, scale, and ecological validity (Nielsen, 1994; Barnum, 2010). Advanced logging and video analysis tools have expanded the granularity at which behaviors can be captured and analyzed.\n\nThink-aloud protocols are the standard in usability studies. Despite their prevalence, they can alter task performance and cognitive load, raising concerns about external validity. Alternative techniques, such as retrospective probing and passive behavioral telemetry, aim to mitigate intrusiveness while retaining diagnostic power.\n\nWe present a mixed-methods framework that triangulates think-aloud with passive signals to improve problem discovery rates.",
    "reason": "Asserting a field-wide standard is a claim about prior practice that should be supported with citations (violates rule b).",
    "start": 399,
    "end": 459,
    "label": "Unsupported_claim"
  },
  {
    "span": "For navigation, deep reinforcement learning has leveraged value-based methods like DQN and distributional variants, as well as policy gradients and actor–critic frameworks (Mnih et al., 2015; Bellemare et al., 2017; Schulman et al., 2015; Lillicrap et al., 2016; Haarnoja et al., 2018). Curriculum learning and auxiliary tasks are also employed to stabilize training (Sukhbaatar et al., 2018; Mirowski et al., 2017; Savva et al., 2019).",
    "document": "Introduction\n\nEmbodied navigation in unknown environments requires agents to perceive, reason, and act under partial observability and sparse rewards. While classical methods rely on mapping and planning with hand-engineered features, deep reinforcement learning promises end-to-end optimization from pixels to actions.\n\nFor navigation, deep reinforcement learning has leveraged value-based methods like DQN and distributional variants, as well as policy gradients and actor–critic frameworks (Mnih et al., 2015; Bellemare et al., 2017; Schulman et al., 2015; Lillicrap et al., 2016; Haarnoja et al., 2018). Curriculum learning and auxiliary tasks are also employed to stabilize training (Sukhbaatar et al., 2018; Mirowski et al., 2017; Savva et al., 2019).\n\nWe propose a hierarchical model-based agent with uncertainty-aware planning and skill discovery, achieving strong generalization to novel layouts and dynamic obstacles without task-specific tuning.",
    "reason": "The span catalogs techniques and citations but does not articulate the authors' perspective, limitations of these approaches, or the specific gap the paper addresses.",
    "start": 321,
    "end": 757,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT has been extensively adopted for AES on the ASAP dataset with prompt-specific heads.",
    "document": "Related Work\n\nAutomated essay scoring (AES) evaluates writing quality along dimensions such as organization, coherence, and grammar, supporting scalable assessment and feedback (Shermis and Burstein, 2013). Traditional approaches rely on engineered features capturing syntax, discourse, and lexical richness (Attali and Burstein, 2006). Neural models have shifted toward end-to-end representations that better capture global structure and semantics (Taghipour and Ng, 2016; Dong et al., 2017).\n\nPretrained transformers improve AES by leveraging contextualized embeddings and task-specific adapters to model prompt sensitivity and content relevance (Devlin et al., 2019; Uto et al., 2020). BERT has been extensively adopted for AES on the ASAP dataset with prompt-specific heads. Yet, concerns remain about robustness to adversarially injected off-topic content and spurious length correlations.\n\nWe introduce a multi-view scoring framework that decouples topical relevance from writing quality, employing a contrastive content filter and a calibration layer to reduce overreliance on surface features.",
    "reason": "This refers to a specific dataset and a claimed prevalent modeling setup without providing citations to the works that adopted it.",
    "start": 689,
    "end": 778,
    "label": "Unsupported_claim"
  },
  {
    "span": "Control codes have been used to steer style and length in generation (Keskar et al., 2019). Extractive summarization models with sentence selection heuristics have also been explored for news (Nallapati et al., 2017). Graph-based text representations capture discourse-level structure for summarization (Yasunaga et al., 2017).",
    "document": "Related Work\n\nControllable summarization aims to produce concise, accurate summaries while allowing users to specify attributes such as length, aspect focus, and style. Early neural approaches combined attention with copying to better preserve salient facts (See et al., 2017), and planning modules improved content selection prior to decoding (Gehrmann et al., 2018). Recent work broadens control mechanisms across pre-trained encoder–decoder architectures and prompt-based interfaces.\n\nControl codes have been used to steer style and length in generation (Keskar et al., 2019). Extractive summarization models with sentence selection heuristics have also been explored for news (Nallapati et al., 2017). Graph-based text representations capture discourse-level structure for summarization (Yasunaga et al., 2017). Other efforts condition generation on discourse segments or rhetorical roles to align outputs with document structure (Xu et al., 2020), while constraint decoding enforces attribute satisfaction during beam search (Hokamp and Liu, 2017).\n\nDespite progress, controllability often trades off with faithfulness, motivating approaches that incorporate factuality signals and content verifiers during training and inference. Our work studies control under factuality constraints by augmenting the objective with entailment-based rewards.",
    "reason": "Each sentence introduces a separate thread (control codes, extractive models, graph representations) with no transitions or explanation of how they relate to one another or to controllability, creating abrupt shifts and unclear connections.",
    "start": 488,
    "end": 815,
    "label": "Coherence"
  },
  {
    "span": "(Garcia, 2021.)",
    "document": "Related Work\n\nEnd-to-end speech recognition has progressed rapidly with encoder–decoder and transducer models (Chan et al., 2016; Graves, 2012; Gulati et al., 2020). Self-supervised pretraining on large unlabeled audio corpora has improved performance in low-resource settings (Baevski et al., 2020; Hsu et al., 2021). Data augmentation methods such as SpecAugment continue to provide robust gains (Park et al., 2019). In conversational ASR, diarization and overlap handling remain open challenges, particularly in multi-speaker settings (Watanabe et al., 2020; Raj et al., 2021). A recent survey (Garcia, 2021.) outlines open problems in integrating speech and downstream NLP tasks.\n\nWe extend prior work by introducing a streaming-friendly align-and-predict architecture with monotonic attention and lightweight language model fusion. Our experiments on meeting corpora demonstrate improved latency-accuracy trade-offs over strong baselines.",
    "reason": "Period placed inside the parenthetical citation; the period should appear after the closing parenthesis: \"(Garcia, 2021).\"",
    "start": 597,
    "end": 612,
    "label": "Format"
  },
  {
    "span": "[Lopez et al., 2015]",
    "document": "Introduction\n\nNeural machine translation (NMT) supplanted phrase-based systems by enabling end-to-end sequence modeling with attention (Bahdanau et al., 2015; Vaswani et al., 2017). Early work established strong baselines for encoder-decoder architectures [Lopez et al., 2015] and highlighted the importance of subword units for handling rare words (Sennrich et al., 2016). Building on these insights, we investigate domain-adaptive fine-tuning and vocabulary interpolation for low-resource transfer (Chu and Wang, 2018; Zoph et al., 2016).\n",
    "reason": "Wrong bracket type for author–year citation: uses square brackets instead of parentheses; should be \"(Lopez et al., 2015)\".",
    "start": 256,
    "end": 276,
    "label": "Format"
  },
  {
    "span": "(O'Neil 2021)",
    "document": "Introduction\n\nConcerns about algorithmic fairness have prompted the development of metrics that capture group and individual harms (Hardt et al., 2016; Dwork et al., 2012). Practical deployments highlight the trade-offs between predictive performance and equity across subpopulations (Corbett-Davies and Goel, 2018). As discussed in (O'Neil 2021), the social context in which models operate can amplify disparities when feedback loops are ignored. To address these pitfalls, recent work proposes post-processing calibrations and constrained optimization during training (Pleiss et al., 2017; Zafar et al., 2017). We contribute a method that aligns loss weighting with distributionally robust objectives to stabilize fairness across shifts.",
    "reason": "Missing comma between author and year in parenthetical citation; should be “(O'Neil, 2021)”.",
    "start": 333,
    "end": 346,
    "label": "Format"
  },
  {
    "span": "There has been a surge of recent works on graph transformers for AMR parsing.",
    "document": "Related Work\n\nAbstract Meaning Representation (AMR) parsing has evolved from graph-based algorithms to neural sequence transduction approaches that serialize graphs into strings (e.g., linearized or PENMAN forms) (Flanigan et al., 2014; Konstas et al., 2017). Transition-based and factorized decoders further improved structural fidelity by explicitly modeling nodes and edges (Zhang and Barzilay, 2015; Dozat and Manning, 2018). Pre-trained language models have boosted AMR parsing through contextualized encoders that capture long-range dependencies (Bevilacqua et al., 2021). There has been a surge of recent works on graph transformers for AMR parsing. Despite these advances, most models still struggle with rare frames and reentrancies, motivating architectures that can better propagate information across distant nodes.\n",
    "reason": "Mentions 'recent works' without providing any citations to support the claim.",
    "start": 579,
    "end": 656,
    "label": "Unsupported_claim"
  },
  {
    "span": "It is well known that transformer decoders struggle with long-range dependencies.",
    "document": "Introduction\n\nAutoregressive transformers have become the de facto standard for sequence generation across modalities. Despite architectural advances, maintaining coherence over long sequences remains challenging.\n\nIt is well known that transformer decoders struggle with long-range dependencies. Memory-augmented variants partially mitigate this issue but at the cost of increased computation and complex training dynamics.\n\nWe introduce a recurrent memory adapter that preserves constant-time retrieval while enabling cross-segment credit assignment, improving long-context modeling without large latency overhead.",
    "reason": "Asserts a common-knowledge claim about model behavior in a specialized domain without evidence or citation (rule b).",
    "start": 215,
    "end": 296,
    "label": "Unsupported_claim"
  },
  {
    "span": "Knowledge distillation techniques comprise logit matching, feature mimicking, attention transfer, and progressive or multi-teacher schemes (Hinton et al., 2015; Romero et al., 2015; Zagoruyko and Komodakis, 2017; You et al., 2017). We introduce a parameter-efficient distillation recipe for decoder-only LMs.",
    "document": "Related Work\n\nScaling language models improves performance but raises deployment costs. Knowledge distillation aims to compress large models into smaller ones that retain most of the accuracy at reduced compute.\n\nKnowledge distillation techniques comprise logit matching, feature mimicking, attention transfer, and progressive or multi-teacher schemes (Hinton et al., 2015; Romero et al., 2015; Zagoruyko and Komodakis, 2017; You et al., 2017). We introduce a parameter-efficient distillation recipe for decoder-only LMs.\n\nOur approach exploits sparse intermediate supervision and adaptive temperature schedules to stabilize training while preserving calibration.",
    "reason": "The span shifts from listing prior distillation variants directly to announcing the contribution without identifying a specific deficiency in prior work, meeting criterion (b).",
    "start": 213,
    "end": 521,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Several recent works have demonstrated that reference-free MT metrics surpass BLEU across multiple language pairs.",
    "document": "Introduction\n\nAutomatic evaluation of machine translation (MT) systems remains an open problem. Classic reference-based metrics such as BLEU and METEOR are simple to compute but correlate imperfectly with human judgments (Callison-Burch et al., 2006). More recently, learned metrics and reference-free approaches aim to better reflect adequacy and fluency by leveraging pretrained language models. Several recent works have demonstrated that reference-free MT metrics surpass BLEU across multiple language pairs. Despite this trend, many practitioners still rely on BLEU due to its simplicity and historical adoption. In this paper, we revisit reference-free evaluation by proposing a promptable scoring model and a robust calibration procedure.\n\nRelated Work\n\nReference-based metrics have evolved from n-gram overlap to trained metrics that compare system outputs to human references (Banerjee and Lavie, 2005; Sellam et al., 2020). Reference-free metrics eliminate the need for gold references and instead assess quality directly from the source and hypothesis. While promising, their reliability varies across domains and language pairs, motivating our re-examination of their strengths and weaknesses.",
    "reason": "The sentence claims 'several recent works' and a performance comparison against BLEU without providing any citations to those works.",
    "start": 398,
    "end": 512,
    "label": "Unsupported_claim"
  },
  {
    "span": "We follow the widely adopted split introduced by the earlier EdNet study.",
    "document": "Related Work\n\nKnowledge tracing models seek to infer a student’s latent mastery from interaction logs. Neural approaches, including recurrent and attention-based models, have demonstrated improved predictive accuracy over classical Bayesian variants in large-scale educational platforms. Public datasets have enabled reproducible comparisons across diverse curricula and subject areas.\n\nWe follow the widely adopted split introduced by the earlier EdNet study. While this split is intended to reduce leakage across students and items, implementations vary and can induce optimistic estimates if session boundaries are not respected. We therefore re-define train/validation/test partitions that prevent temporal and user overlap and release code to facilitate exact replication.",
    "reason": "Refers to a specific dataset split attributed to a prior study without citing that study, violating rule (a).",
    "start": 387,
    "end": 460,
    "label": "Unsupported_claim"
  },
  {
    "span": "The Swahili Broadcast News dataset contains approximately 120 hours of transcribed audio.",
    "document": "Related Work\n\nLow-resource automatic speech recognition (ASR) has advanced rapidly with self-supervised pretraining and multilingual transfer. Despite these gains, data scarcity, domain mismatch, and orthographic variability remain major bottlenecks. Researchers often bootstrap models using related high-resource languages and fine-tune on small in-language corpora.\n\nMultiple public datasets have been used to benchmark progress, including broadcast news, conversational telephone speech, and radio archives collected over many years. The Swahili Broadcast News dataset contains approximately 120 hours of transcribed audio. While the corpus is widely adopted for cross-lingual transfer studies, its license, channel conditions, and speaker demographics vary substantially from typical deployment settings.\n\nBeyond supervised resources, pseudo-labeling pipelines have been applied to unlabeled audio, and pronunciation lexicons have been automatically induced to reduce annotation costs. Our work situates within this line of research by comparing transfer learning recipes under matched and mismatched domains for several Bantu languages and by proposing a simple domain-adaptive normalization module.",
    "reason": "Provides a specific dataset statistic without citing the source (violates rule a/b; dataset size requires a citation).",
    "start": 537,
    "end": 626,
    "label": "Unsupported_claim"
  },
  {
    "span": "There are many recent works that explore sim-to-real transfer for in-hand manipulation.",
    "document": "Related Work\n\nBridging the reality gap is a longstanding challenge in robot learning, with approaches spanning domain randomization, dynamics adaptation, and representation alignment (Tobin et al., 2017; Peng et al., 2018; Rusu et al., 2017). Policy learning in simulation offers scalability, while targeted adaptation improves deployment success on real hardware.\n\nThere are many recent works that explore sim-to-real transfer for in-hand manipulation. Despite promising results, reproducibility remains limited due to proprietary hardware and incomplete reporting of calibration procedures.",
    "reason": "Uses the vague phrase 'many recent works' without citing any of them (rule d).",
    "start": 366,
    "end": 453,
    "label": "Unsupported_claim"
  },
  {
    "span": "Zehlike et al. (2017) enforce group fairness constraints in ranked lists. Exposure-based metrics formalize attention allocation across positions (Singh and Joachims, 2018). Click models explain user behavior under position bias (Craswell et al., 2008). Calibration aligns predicted scores with outcome probabilities (Guo et al., 2017).",
    "document": "Introduction\n\nFairness in Ranking Systems. Ranking algorithms shape access to opportunities and information. Ensuring fairness requires accounting for position bias, exposure allocation, and the distribution of utility across protected groups (Biega et al., 2018; Singh and Joachims, 2018; Celis et al., 2018).\n\nMetrics and Mechanisms. Zehlike et al. (2017) enforce group fairness constraints in ranked lists. Exposure-based metrics formalize attention allocation across positions (Singh and Joachims, 2018). Click models explain user behavior under position bias (Craswell et al., 2008). Calibration aligns predicted scores with outcome probabilities (Guo et al., 2017). We propose a learning-to-rank objective that integrates exposure-aware fairness constraints with calibrated uncertainty estimates to balance relevance and equity.",
    "reason": "The sentences cover fairness constraints, exposure metrics, click models, and calibration without transitions or explanations tying them together, resulting in abrupt shifts and unclear relationships between cited works.",
    "start": 336,
    "end": 671,
    "label": "Coherence"
  },
  {
    "span": "Shelmanov et al. (2021",
    "document": "Related Work\n\nDeep active learning with pretrained Transformers has shown competitive performance across sequence labeling and classification (Shen et al., 2017; Ein-Dor et al., 2020; Yuan et al., 2020). Bayesian approximations such as Monte Carlo dropout offer uncertainty estimates without modifying model parameters (Gal and Ghahramani, 2016), but require multiple stochastic forward passes. Shelmanov et al. (2021 propose leveraging distilled student models for efficient acquisition while retaining teacher performance.\n\nAlternative strategies learn acquisition policies via reinforcement learning, but their transferability is limited and training is costly (Fang et al., 2017; Brantley et al., 2020). Our approach focuses on scalable, uncertainty-aware selection compatible with large language models.",
    "reason": "Missing closing parenthesis in the narrative citation; should be \"Shelmanov et al. (2021)\".",
    "start": 395,
    "end": 417,
    "label": "Format"
  },
  {
    "span": "Recent works apply deep reinforcement learning to navigation with a variety of algorithms, including DQN, DDPG, PPO, and SAC (Zhu et al., 2017; Tai et al., 2017; Chen et al., 2019; Kahn et al., 2018). Some approaches rely on global maps and path planners (Zhang et al., 2021; Li et al., 2020), while others perform end-to-end control directly from raw sensors (Mirowski et al., 2016; Gupta et al., 2017).",
    "document": "Introduction\n\nMobile robot navigation in cluttered and dynamic environments remains a long-standing challenge. Deep reinforcement learning (RL) has emerged as a promising paradigm to learn control policies directly from experience without hand-engineered features.\n\nRecent works apply deep reinforcement learning to navigation with a variety of algorithms, including DQN, DDPG, PPO, and SAC (Zhu et al., 2017; Tai et al., 2017; Chen et al., 2019; Kahn et al., 2018). Some approaches rely on global maps and path planners (Zhang et al., 2021; Li et al., 2020), while others perform end-to-end control directly from raw sensors (Mirowski et al., 2016; Gupta et al., 2017).\n\nWe develop a visual-navigation policy trained with curriculum learning in procedurally generated mazes and evaluate sim-to-real transfer on a wheeled platform.",
    "reason": "The span lists categories and algorithms with citations but does not connect them to the paper’s approach, limitations, or motivation, thus lacking synthesis.",
    "start": 266,
    "end": 670,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent user studies have found that dark mode reduces eye strain for programmers",
    "document": "Introduction\n\nIntegrated development environments (IDEs) offer multiple color themes, yet the usability impact of light versus dark modes remains debated. Programmers frequently cite comfort and readability when choosing themes.\n\nRecent user studies have found that dark mode reduces eye strain for programmers. However, the mechanisms by which color schemes interact with code comprehension and visual fatigue are not fully understood.\n\nWe present a controlled experiment measuring visual comfort, task performance, and physiological proxies under varying syntax highlighting and background luminance, offering design guidance for accessible coding environments.\n",
    "reason": "Mentions 'recent user studies' without providing any references; per rule (d), such statements must be backed by citations.",
    "start": 230,
    "end": 310,
    "label": "Unsupported_claim"
  },
  {
    "span": "Antol et al. (2015) introduced the VQA dataset and a baseline model. Fukui et al. (2016) proposed multimodal compact bilinear pooling for fusion. Anderson et al. (2018) incorporated bottom-up attention using region features. Kim et al. (2018) developed bilinear attention networks.",
    "document": "Related Work\n\nVision–Language Reasoning\nAnswering questions about images requires modeling both visual content and linguistic structure, as well as their interactions (Zitnick et al., 2016). Benchmarks and architectures have evolved to emphasize compositional reasoning and localized evidence.\n\nVisual Question Answering and Fusion\nMany methods differ in how they fuse modalities and attend to image regions conditioned on the question. Antol et al. (2015) introduced the VQA dataset and a baseline model. Fukui et al. (2016) proposed multimodal compact bilinear pooling for fusion. Anderson et al. (2018) incorporated bottom-up attention using region features. Kim et al. (2018) developed bilinear attention networks.\n\nPretraining and Transfer\nRecent approaches leverage large-scale contrastive pretraining and unified transformer models to improve data efficiency and generalization (Lu et al., 2019; Li et al., 2020). Our method revisits fusion under constrained compute budgets.",
    "reason": "The span lists works sequentially without explaining how each advances or contrasts with the others or how the fusion mechanisms relate. There are no transitions to guide the reader through the progression, leading to low coherence.",
    "start": 437,
    "end": 718,
    "label": "Coherence"
  },
  {
    "span": "Brown et al.",
    "document": "Introduction\n\nCurriculum learning has been shown to improve sample efficiency by ordering examples from easy to hard in vision and NLP (Bengio et al., 2009; Platanios et al., 2019). As Brown et al. argue, exposing the model to structured difficulty can reduce optimization barriers and improve generalization in low-data regimes. We incorporate a difficulty-aware scheduler that adapts to domain shifts without additional supervision (Zhang et al., 2020).",
    "reason": "Narrative citation missing the publication year (should be 'Brown et al. (YEAR)').",
    "start": 185,
    "end": 197,
    "label": "Format"
  },
  {
    "span": "Lee et al. 1",
    "document": "Introduction\n\nConversational interfaces must balance efficiency and user satisfaction through adaptive dialogue strategies (Jurafsky and Martin, 2023; Bohus and Rudnicky, 2009). Mixed-initiative systems can recover from errors by clarifying intent and leveraging context (Litman and Pan, 2002; Gervits et al., 2021). Recent approaches use reinforcement learning to optimize turn-taking and grounding behavior (Young et al., 2013; Su et al., 2016).\n\nPrior work has shown that multimodal cues, such as prosody and gaze, improve turn prediction and reduce repair rates (Skantze, 2017; Ishii et al., 2016). However, evaluations often lack ecological validity outside controlled lab settings. Lee et al. 1 report a field study of task-oriented agents deployed in customer support, revealing significant shifts in user behavior compared to simulated environments. Our study complements these findings with a large-scale A/B test across multiple domains.\n",
    "reason": "Improper footnote-style marker without a year; should include a year (e.g., \"Lee et al. (YEAR)\") or be formatted as a proper footnote.",
    "start": 688,
    "end": 700,
    "label": "Format"
  },
  {
    "span": "TransE embeds entities and relations into a metric space using translation vectors (Bordes et al., 2013). Rule mining discovers Horn clauses that capture regularities in graphs (Galárraga et al., 2015). Graph neural encoders aggregate neighborhood information for link prediction (Schlichtkrull et al., 2018). Foundation models pretrain on web-scale corpora with next-token objectives (Brown et al., 2020).",
    "document": "Related Work\n\nKnowledge graph reasoning techniques include embedding-based approaches, logical rule induction, and neural message passing. Recent trends also consider leveraging large pretrained language models as sources of background knowledge.\n\nTransE embeds entities and relations into a metric space using translation vectors (Bordes et al., 2013). Rule mining discovers Horn clauses that capture regularities in graphs (Galárraga et al., 2015). Graph neural encoders aggregate neighborhood information for link prediction (Schlichtkrull et al., 2018). Foundation models pretrain on web-scale corpora with next-token objectives (Brown et al., 2020).\n\nOur method aligns textual and structural signals by distilling from a language model into graph encoders.",
    "reason": "The paragraph lists multiple approaches with abrupt shifts, especially the jump from KG-specific methods to foundation models, without transitions or an explicit explanation of relevance.",
    "start": 248,
    "end": 654,
    "label": "Coherence"
  },
  {
    "span": "(2015)",
    "document": "Related Work\n\nFederated learning (FL) enables decentralized training under privacy constraints by keeping data on-device (McMahan et al., 2017). Personalization addresses client heterogeneity using meta-learning or mixture models (Fallah et al., 2020; Arivazhagan et al., 2019). Robust aggregation mitigates the effect of adversarial or corrupted clients (Blanchard et al., 2017; Yin et al., 2018). Compression and quantization reduce communication overhead (Konečný et al., 2016; Alistarh et al., 2017). Differential privacy protects individual updates at the cost of utility (Abadi et al., 2016). The idea of secure aggregation was popularized in (2015) and later adapted for practical deployments (Bonawitz et al., 2017). We build a bi-level scheduler that adapts client participation based on gradient diversity to accelerate convergence under stragglers.",
    "reason": "Year-only parenthetical citation lacks author information; in author–year style it should include the authors, e.g., '(Author, 2015)'.",
    "start": 649,
    "end": 655,
    "label": "Format"
  },
  {
    "span": "(Smith et al., 2020.)",
    "document": "Introduction\n\nEvaluation of text generation often relies on automatic overlap metrics. While BLEU and ROUGE are widely used (Papineni et al., 2002; Lin, 2004), they correlate imperfectly with human judgments. Recent meta-analyses (Smith et al., 2020.) suggest that task-specific learned metrics yield better agreement. Motivated by these findings, we propose a calibration procedure that aligns metric outputs with pairwise human preferences.",
    "reason": "Extraneous period inside the parentheses. The period should follow the closing parenthesis: “(Smith et al., 2020).”",
    "start": 230,
    "end": 251,
    "label": "Format"
  },
  {
    "span": "Recent advances in program synthesis from natural language leverage large language models, retrieval-augmented generation, and constraint-guided decoding (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Le et al., 2022; Li et al., 2023).",
    "document": "Related Work\n\nProgram synthesis from NL\n\nMapping natural language to executable code has seen rapid progress with pretrained language models and specialized decoding strategies. Benchmarks now cover diverse tasks, from competitive programming to API-centric code generation.\n\nRecent advances in program synthesis from natural language leverage large language models, retrieval-augmented generation, and constraint-guided decoding (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Le et al., 2022; Li et al., 2023).\n\nEvaluation and reliability\n\nDespite strong pass@k metrics, issues remain around test contamination, robustness to specification ambiguity, and execution safety. Methods for semantic checking and formal verification are promising but often expensive.\n\nOur aim\n\nWe investigate lightweight semantic constraints that improve faithfulness without heavy-weight verification.",
    "reason": "The span summarizes prior advances without explaining how they connect to the paper’s aims or what specific deficiencies motivate the new method, thus lacking synthesis (criteria a and c).",
    "start": 276,
    "end": 527,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The WNUT-2017 shared task defines emerging entities as those appearing in social media within the last 12 months.",
    "document": "Related Work\n\nNamed entity recognition (NER) on noisy, user-generated text differs from newswire NER due to non-standard orthography, drifting vocabularies, and sparsity of canonical mentions. Community evaluations have highlighted the difficulty of detecting emerging and rare entities in social media streams and microblogs. The WNUT-2017 shared task defines emerging entities as those appearing in social media within the last 12 months. Approaches exploiting subword modeling, gazetteer expansion, and multi-task learning with POS tagging and language modeling have reported improvements, but performance remains far below that observed on clean text.",
    "reason": "Provides a specific definition attributed to a shared task without citing the task description or proceedings (rule a/b).",
    "start": 327,
    "end": 440,
    "label": "Unsupported_claim"
  },
  {
    "span": "Finn et al. (2017) proposed MAML for fast adaptation. Snell et al. (2017) introduced Prototypical Networks for metric-based classification. Ravi and Larochelle (2017) optimized an LSTM meta-learner. Rusu et al. (2019) presented MetaGAN for few-shot synthesis.",
    "document": "Related Work\n\nFew-Shot Learning and Meta-Learning\nFew-shot learning aims to generalize from limited labeled data by leveraging prior experience across tasks (Lake et al., 2015). Meta-learning methods shape models to adapt quickly to new tasks with minimal supervision (Hospedales et al., 2021).\n\nOptimization- and Metric-Based Approaches\nApproaches differ in how they extract task-invariant structure and encode task-specific updates. Finn et al. (2017) proposed MAML for fast adaptation. Snell et al. (2017) introduced Prototypical Networks for metric-based classification. Ravi and Larochelle (2017) optimized an LSTM meta-learner. Rusu et al. (2019) presented MetaGAN for few-shot synthesis.\n\nData Augmentation and Pretraining\nRecent work augments scarce target data with synthetic examples or leverages large-scale pretraining for better transfer (Gidaris and Komodakis, 2018; Tian et al., 2020). Our method combines metric structure with adaptation under distribution shift.",
    "reason": "The span strings together several citations without articulating their relationships (e.g., optimization-based vs. metric-based vs. generative) or how they compare. The lack of transitions makes it unclear how one work follows from or contrasts with another.",
    "start": 435,
    "end": 694,
    "label": "Coherence"
  },
  {
    "span": "Representation learning for counterfactual estimation seeks balanced latent spaces that reduce covariate shift between treatment groups (Johansson et al., 2016; Shalit et al., 2017). Counterfactual fairness defines predictors invariant to sensitive attributes under structural assumptions (Kusner et al., 2017; Kilbertus et al., 2018). Do-calculus characterizes identifiability via graphical rules (Pearl, 2009; Bareinboim et al., 2020).",
    "document": "Related Work\n\nDeep Learning for Causal Inference\n\nData-driven causal estimation aims to infer treatment effects from observational data under confounding and selection biases (Imbens and Rubin, 2015). Recent work integrates representation learning with causal objectives to improve identifiability and robustness.\n\nRepresentation learning for counterfactual estimation seeks balanced latent spaces that reduce covariate shift between treatment groups (Johansson et al., 2016; Shalit et al., 2017). Counterfactual fairness defines predictors invariant to sensitive attributes under structural assumptions (Kusner et al., 2017; Kilbertus et al., 2018). Do-calculus characterizes identifiability via graphical rules (Pearl, 2009; Bareinboim et al., 2020).\n\nBeyond static settings, temporal causal modeling considers recurrent structures, continuous-time dynamics, and interventions over sequences (Bica et al., 2020; Oberst and Sontag, 2019). Despite advances, bridging theoretical identifiability with practical model design for high-dimensional data remains challenging.",
    "reason": "The paragraph lists three different subareas—balanced representations, fairness, and do-calculus—without transitions or explicit relationships, making the connection between the cited works unclear.",
    "start": 315,
    "end": 752,
    "label": "Coherence"
  },
  {
    "span": "Prior studies have shown that judges prefer extractive rationales over abstractive ones in legal search results.",
    "document": "Related Work\n\nLegal information retrieval (IR) has long focused on precision-oriented ranking due to the high cost of reviewing non-relevant authorities. Neural ranking models adapted from web search now incorporate domain expertise via legal-specific pretraining and citation-aware features. Explainability is a pressing requirement, as practitioners demand transparent evidence supporting retrieved items. Prior studies have shown that judges prefer extractive rationales over abstractive ones in legal search results. Recent approaches produce token-level importance scores, sentence summaries, or graph rationales over citation networks, but there is little agreement on which explanation format best supports legal reasoning.\n\nEvaluation remains challenging: relevance judgments are expensive, gold rationales are scarce, and case law evolves. Proxy tasks such as headnote prediction, section matching, and citation prediction have been proposed to approximate relevance signals. However, these proxies may not reflect practitioners' needs, underscoring the importance of user-centered evaluation and justification quality metrics.",
    "reason": "Claims findings from 'prior studies' about user preference without any citation, violating rule (b)/(d).",
    "start": 408,
    "end": 520,
    "label": "Unsupported_claim"
  },
  {
    "span": "Fairness in machine learning has been formalized through demographic parity, equalized odds, calibration, and counterfactual notions (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017; Kusner et al., 2017). Distribution shift arises from covariate, label, or concept drift and challenges the stability of fairness interventions (Quionero-Candela et al., 2009; Moreno-Torres et al., 2012; Quiñonero-Candela, 2022).",
    "document": "Introduction\n\nEnsuring equitable model behavior is complicated by nonstationary environments. Deployed systems encounter shifts that can invalidate assumptions made at training time and destabilize fairness guarantees.\n\nFairness in machine learning has been formalized through demographic parity, equalized odds, calibration, and counterfactual notions (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017; Kusner et al., 2017). Distribution shift arises from covariate, label, or concept drift and challenges the stability of fairness interventions (Quionero-Candela et al., 2009; Moreno-Torres et al., 2012; Quiñonero-Candela, 2022).\n\nWe introduce shift-aware fairness auditing that pairs importance-weighted risk estimates with conditional subgroup evaluation, yielding valid post-hoc assessments under covariate shift. We further propose a robust training objective that balances accuracy and fairness across a set of plausible target distributions.",
    "reason": "The span lists fairness definitions and types of shift without explaining how these relate to the paper’s proposed auditing and training approach or identifying a concrete gap, hence lacking synthesis (a, c).",
    "start": 220,
    "end": 646,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Chen et al. (2020a) presented SimCLR. He et al. (2020) introduced MoCo. Grill et al. (2020) proposed BYOL. Caron et al. (2021) developed DINO.",
    "document": "Related Work\n\nSelf-supervised learning in computer vision leverages pretext tasks and contrastive objectives to learn representations without labels. Recent methods adopt instance discrimination, momentum encoders, and teacher–student distillation.\n\nChen et al. (2020a) presented SimCLR. He et al. (2020) introduced MoCo. Grill et al. (2020) proposed BYOL. Caron et al. (2021) developed DINO.\n\nOur approach focuses on label-efficient fine-tuning by aligning features to a small set of prototypes learned under mixed augmentations.",
    "reason": "This span lists several methods with no transitions or explicit comparisons, making their relationships and progression unclear and disrupting coherence.",
    "start": 250,
    "end": 392,
    "label": "Coherence"
  },
  {
    "span": "(Kumar et al., 2017; Li et al., 2019;)",
    "document": "Introduction\n\nNeural machine translation (NMT) with attention and Transformers has become the de facto standard across language pairs (Bahdanau et al., 2015; Vaswani et al., 2017). Nevertheless, performance degrades under domain shift and low-resource settings due to overfitting to source domains (Chu and Wang, 2018; Koehn and Knowles, 2017).\n\nDomain adaptation strategies include fine-tuning on in-domain data, multi-domain training with instance weighting, and meta-learning for rapid adaptation (Britz et al., 2017; Zeng et al., 2018). Data augmentation via back-translation and noising has also proven effective (Sennrich et al., 2016; Edunov et al., 2018; Fadaee et al., 2017).\n\nRecent adapters and prompt-based conditioning enable parameter-efficient adaptation across many domains (Kumar et al., 2017; Li et al., 2019;) but remain underexplored for morphologically rich languages.",
    "reason": "Trailing semicolon before the closing parenthesis in a citation list.",
    "start": 790,
    "end": 828,
    "label": "Format"
  },
  {
    "span": "(Nguyen and Lee, 2020",
    "document": "Related Work\n\nImage Captioning and Vision-Language Modeling\n\nNeural image captioning began with CNN-RNN architectures (Vinyals et al., 2015; Xu et al., 2015) and progressed to Transformer-based decoders with object-level features (Anderson et al., 2018; Cornia et al., 2020). Cross-modal pretraining further improved grounding and fluency (Lu et al., 2019; Li et al., 2020). Some approaches rely on encoder-decoder Transformers (Nguyen and Lee, 2020 to enhance long-range dependencies, while contrastive learning aligns visual and textual embeddings for better retrieval and caption quality (Radford et al., 2021; Jia et al., 2021). We adopt a retrieval-augmented captioner that conditions decoding on similar exemplars to improve specificity in fine-grained domains.",
    "reason": "Missing closing parenthesis in a parenthetical citation.",
    "start": 428,
    "end": 449,
    "label": "Format"
  },
  {
    "span": "we annotate responses generated by several state-of-the-art models, including ones that are designed to alleviate hallucinations.",
    "document": "Introduction\n\nKnowledge-grounded conversational models, powered by large pre-trained language models (Radford et al., 2019;Brown et al., 2020;Raffel et al., 2020), are well-known to generate factually incorrect statements, a phenomenon commonly called hallucination (Dziri et al., 2021b;Rashkin et al., 2021b). A large commonality in the majority of prior work seeks to address hallucination by ameliorating the model (Shuster et al., 2021;Mielke et al., 2020;Dziri et al., 2021a;Rashkin et al., 2021b), but no attempt has been made so far to audit the conversational benchmarks to the best of our knowledge.\n\nOn one hand, knowledge-grounded conversational benchmarks may contain hallucinations due to error-prone collection protocols, or due to a design framework that encourages informativeness over faithfulness. Existing dialogue systems are typically trained on corpora crowd-sourced through online platforms (Dinan et al., 2018; Gopalakrishnan et al., 2019; Moon et al., 2019). With loose incentive to come up with faithfully-grounded utterances on the provided knowledge, crowdworkers may ignore knowledge-snippets altogether, use their personal knowledge or sometimes assume a fictional persona, resulting in conversations that are rife with subjective content and unverified factual knowledge. Figure 1 shows a hallucinated conversation from the WoW dataset (Dinan et al., 2018), On the other hand, neural conversational models are not necessarily designed to generate faithful outputs, but to mimic the distributional properties of the data. This kind of optimization will likely push the models to replicate and even amplify the hallucination behaviour at test time (Bender et al., 2021). The presence of even few hallucinated responses may skew the data distribution in a way that curbs the model's ability to generate faithful responses (Kang and Hashimoto, 2020).\n\nIn this work, drawing insights from the linguistic coding system for discourse phenomena (Stiles, 1992) and evaluation frameworks such as BEGIN (Dziri et al., 2021b) and AIS (Rashkin et al., 2021a), we annotate responses from the three widely-used knowledge-grounded conversational benchmarks: Wizard of Wikipedia (Dinan et al., 2018), CMU-DoG (Zhou et al., 2018) and Topi-calChat (Gopalakrishnan et al., 2019). Our analysis reveals surprisingly that more than 60% of the responses are hallucinated in the three datasets, with major hallucination modes that manifest principally through the expression of subjective information (e.g., thoughts, beliefs, feelings, intentions, personal experiences) and the expression of unsupported objective factual information. Further, to understand if neural conversational models make this hallucination more severe, we annotate responses generated by several state-of-the-art models, including ones that are designed to alleviate hallucinations. We find that the generated responses consist of an even larger portion of hallucinations, in comparison with the training data. Our findings question the quality of current conversational datasets, their appropriateness to train knowledgegrounded conversational systems, and the robustness of existing models.\n\n ",
    "start": 2734,
    "end": 2863,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recently, pre-trained language models have become the de facto encoders for sentiment analysis across domains, including adaptations of BERT, RoBERTa, and XLNet with task-specific heads (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Sun et al., 2020; Keung et al., 2020). Approaches further incorporate adversarial training, virtual adversarial training, and consistency regularization to stabilize fine-tuning (Miyato et al., 2017; Clark et al., 2020; Xie et al., 2020).",
    "document": "Related Work\n\nDomain adaptation for sentiment analysis has been approached through feature alignment, instance reweighting, and self-training. Classical methods align second-order statistics or subspaces between domains, while recent neural methods align latent representations via adversarial objectives or moment matching. We study cross-domain sentiment classification in the presence of severe label shift and sparse target supervision.\n\nRecently, pre-trained language models have become the de facto encoders for sentiment analysis across domains, including adaptations of BERT, RoBERTa, and XLNet with task-specific heads (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Sun et al., 2020; Keung et al., 2020). Approaches further incorporate adversarial training, virtual adversarial training, and consistency regularization to stabilize fine-tuning (Miyato et al., 2017; Clark et al., 2020; Xie et al., 2020).\n\nParallel to representation learning, several works study sample selection and pseudo-labeling curricula under domain shift, often combining confidence thresholds, agreement mechanisms, or teacher–student frameworks. In this paper, we explore label-shift-aware adaptation by jointly calibrating posteriors and aligning features using lightweight target statistics.",
    "reason": "The span lists families of models and techniques with citations but does not connect them to the paper’s aims, identify a gap, or articulate how these approaches relate to the proposed method (criteria a and c).",
    "start": 442,
    "end": 925,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There has been a surge of recent works on parameter-efficient tuning for transformers.",
    "document": "Introduction\n\nPretrained language models have transformed natural language processing by enabling strong performance across diverse tasks with minimal task-specific engineering (Devlin et al., 2019; Brown et al., 2020). However, fully fine-tuning all parameters for each downstream task is often computationally prohibitive and memory intensive in multi-task or multi-domain settings.\n\nThere has been a surge of recent works on parameter-efficient tuning for transformers. In this paper, we study lightweight adaptation strategies that modify only a small subset of parameters while preserving downstream performance. We analyze adapter-based methods, prompt tuning, and low-rank updates from a unified perspective focused on capacity allocation and representation reuse.\n\nOur contributions are threefold: we present a taxonomy of parameter-efficient methods, a controlled experimental suite across text classification, sequence labeling, and generation, and an ablation study identifying the role of layer placement and bottleneck size in transfer effectiveness. We also release reproducible code and configurations to facilitate fair comparison under consistent training budgets.\n\nRelated Work\n\nPrior work on transfer learning in NLP has emphasized the benefits of large-scale pretraining followed by task-specific adaptation (Ruder et al., 2019). Approaches differ primarily in how much of the pretrained network is updated, ranging from full fine-tuning to frozen-encoder methods with additional task heads (Howard and Ruder, 2018). Orthogonal directions explore multi-task supervision (Sanh et al., 2019) and continual learning strategies for avoiding catastrophic forgetting (Kirkpatrick et al., 2017).",
    "reason": "The sentence claims 'recent works' without providing any citations to those works, violating the requirement to cite recent literature when first mentioned.",
    "start": 386,
    "end": 472,
    "label": "Unsupported_claim"
  },
  {
    "span": "Laine and Aila (2017) propose temporal ensembling for semi-supervised learning. Tarvainen and Valpola (2017) introduce Mean Teacher to stabilize training with consistency. Lee (2013) explores pseudo-labeling by self-training. Hung et al. (2018) apply adversarial learning to segmentation with unlabeled data.",
    "document": "Related Work\n\nSemi-Supervised Semantic Segmentation\n\nLabel scarcity in segmentation has motivated consistency training, self-training, and adversarial approaches. Some methods enforce perturbation invariance, while others propagate labels from confident predictions. Laine and Aila (2017) propose temporal ensembling for semi-supervised learning. Tarvainen and Valpola (2017) introduce Mean Teacher to stabilize training with consistency. Lee (2013) explores pseudo-labeling by self-training. Hung et al. (2018) apply adversarial learning to segmentation with unlabeled data. Domain shifts and strong augmentations affect stability and calibration of these techniques.\n\nOur approach integrates uncertainty-aware consistency with class-balanced pseudo-labeling tailored to dense prediction, improving robustness under strong photometric and geometric perturbations.",
    "reason": "The span strings together four works with no transitions or explicit description of how consistency relates to self-training or adversarial learning, leaving the connection between methods implied rather than articulated.",
    "start": 267,
    "end": 575,
    "label": "Coherence"
  },
  {
    "span": "Audio-visual emotion recognition has benefited from spectral CNNs for audio (Trigeorgis et al., 2016), 3D CNNs and temporal models for video (Fan et al., 2016; Zhao et al., 2018), and transformer-based fusion (Tsai et al., 2019; Mai et al., 2020). Text modality has been incorporated via pretrained language encoders and cross-modal attention (Poria et al., 2017; Hazarika et al., 2020).",
    "document": "Introduction\n\nAutomatic emotion recognition seeks to infer affective states from behavioral signals, enabling applications in human-computer interaction, healthcare, and media analysis. Multimodal approaches combine complementary cues from audio, video, and text to improve robustness.\n\nAudio-visual emotion recognition has benefited from spectral CNNs for audio (Trigeorgis et al., 2016), 3D CNNs and temporal models for video (Fan et al., 2016; Zhao et al., 2018), and transformer-based fusion (Tsai et al., 2019; Mai et al., 2020). Text modality has been incorporated via pretrained language encoders and cross-modal attention (Poria et al., 2017; Hazarika et al., 2020).\n\nHowever, label noise and subject biases often hamper generalization across speakers and domains. We introduce BiasLite, a causal-inference-inspired training scheme that disentangles identity and emotion factors via weak supervision, improving cross-corpus transfer without access to demographic metadata.",
    "reason": "The span presents a list of prior techniques across modalities without explaining their limitations relative to the new method or articulating the authors' perspective. It lacks synthesis according to (a) and (c).",
    "start": 287,
    "end": 674,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Nguyen et al., 2018)",
    "document": "Related Work. Graph neural networks (GNNs) have advanced node classification and link prediction by exploiting message passing and neighborhood aggregation (Kipf & Welling, 2017; Hamilton et al., 2017). Subsequent work studied oversmoothing and oversquashing, proposing architectural and training remedies (Li et al., 2019; Alon & Yahav, 2020). In (Nguyen et al., 2018) the authors introduce a sampling-based approach for large-scale graphs, while Chen & Zhu (2020) explore curriculum strategies for stabilizing GNN training. Recent surveys synthesize these developments and highlight gaps in scalability and expressivity (Wu et al., 2021; Zhou et al., 2020). Our work builds on scalable training but targets dynamic graphs, where temporal consistency remains underexplored (Rossi et al., 2020).",
    "reason": "Wrong citation style: the preposition 'In' should not precede a parenthetical citation; it should be written as 'in Nguyen et al. (2018)'.",
    "start": 345,
    "end": 369,
    "label": "Format"
  },
  {
    "span": "Multilingual encoders enable zero-shot transfer for sequence labeling by sharing subword and sentence representations (Devlin et al., 2019; Conneau et al., 2020; Xue et al., 2021). Disagreement-based query strategies prioritize uncertain tokens or spans for annotation (Settles, 2009; Shen et al., 2018). Annotation projection uses word alignments in parallel corpora to induce labels (Yarowsky et al., 2001; Täckström et al., 2012).",
    "document": "Related Work\n\nCross-Lingual Named Entity Recognition and Annotation Strategies\n\nCross-lingual NER aims to leverage supervision in high-resource languages to bootstrap performance in low-resource settings (Rahimi et al., 2019). Approaches include multilingual pretraining, alignment-based transfer, and active learning to reduce annotation costs while maintaining label quality.\n\nMultilingual encoders enable zero-shot transfer for sequence labeling by sharing subword and sentence representations (Devlin et al., 2019; Conneau et al., 2020; Xue et al., 2021). Disagreement-based query strategies prioritize uncertain tokens or spans for annotation (Settles, 2009; Shen et al., 2018). Annotation projection uses word alignments in parallel corpora to induce labels (Yarowsky et al., 2001; Täckström et al., 2012).\n\nFurther improvements consider character-level modeling, constrained decoding, and lexicon integration to handle morphological richness and orthographic variation (Lample et al., 2016; Akbik et al., 2018; Wang et al., 2020). Despite progress, balancing annotation efficiency with cross-lingual generalization remains unresolved.",
    "reason": "Three consecutive sentences introduce multilingual encoders, active learning, and annotation projection without transitions or an explicit explanation of how they relate, leaving the connections between cited works implicit.",
    "start": 379,
    "end": 812,
    "label": "Coherence"
  },
  {
    "span": "Transformer-based forecasters address long-range dependencies via sparse attention, auto-correlation, and frequency decomposition (Zhou et al., 2021; Wu et al., 2021; Zhou et al., 2022; Zhang et al., 2022).",
    "document": "Related Work\n\nClassical time series forecasting approaches such as ARIMA and ETS provide strong baselines under stationarity assumptions but struggle with multivariate, nonstationary data. Deep learning methods model nonlinear temporal patterns and cross-variable dependencies.\n\nTransformer-based forecasters address long-range dependencies via sparse attention, auto-correlation, and frequency decomposition (Zhou et al., 2021; Wu et al., 2021; Zhou et al., 2022; Zhang et al., 2022).\n\nIn practical settings, data scarcity, concept drift, and resource limits require approaches that balance accuracy with stability and efficiency.",
    "reason": "Lists trends and citations without explaining how they relate to or motivate the authors’ contribution, leaving the gap unarticulated (definition a/c).",
    "start": 279,
    "end": 485,
    "label": "Lacks_synthesis"
  },
  {
    "span": "{Miller et al., 2022)",
    "document": "Related Work\n\nTime-series representation learning has adopted contrastive and predictive objectives to capture temporal structure (Franceschi et al., 2019; Eldele et al., 2021). Causal representation learning has seen renewed interest {Miller et al., 2022) focusing on disentangling exogenous drivers from observed sequences. Our framework unifies these views by regularizing invariances across interventions (Peters et al., 2016; Schölkopf et al., 2021).\n",
    "reason": "Mismatched delimiters in citation: starts with a curly brace and ends with a parenthesis; both should be parentheses.",
    "start": 235,
    "end": 256,
    "label": "Format"
  },
  {
    "span": "Davis et al (2018)",
    "document": "Introduction\n\nSafe reinforcement learning aims to optimize policies while satisfying safety constraints during both training and deployment (Garcıa and Fernández, 2015; Achiam et al., 2017). Constrained policy optimization and shielded exploration are two prominent approaches (Ray et al., 2019; Wabersich and Zeilinger, 2018).\n\nDavis et al (2018) highlight the risk of reward hacking and propose counterfactual evaluation metrics to detect specification gaming. Building on these insights, we incorporate constraint-aware critics with risk-sensitive objectives to reduce catastrophic failures in safety-critical environments.\n\nWe evaluate our approach on control suites with explicit safety limits, demonstrating improved constraint satisfaction without sacrificing task performance.",
    "reason": "Narrative citation is missing the period after “al.”; correct style is “Davis et al. (2018)”.",
    "start": 329,
    "end": 347,
    "label": "Format"
  },
  {
    "span": "Alvarez et al.",
    "document": "Related Work\n\nDomain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled or sparsely labeled target domain (Pan and Yang, 2010; Patel et al., 2015). Early approaches rely on feature alignment via distribution matching (Gretton et al., 2012; Tzeng et al., 2014), while more recent works exploit adversarial training to learn domain-invariant representations (Ganin and Lempitsky, 2015; Long et al., 2018).\n\nAs noted by Alvarez et al., cross-domain alignment remains challenging when label shift and conditional shift co-occur. Subsequent research investigates class-conditional alignment (Zhang et al., 2019) and sample reweighting (Lipton et al., 2018), yet robust performance under severe shift is still elusive.\n\nOur approach complements these lines by incorporating uncertainty-aware alignment with target-consistent calibration, bridging the gap between feature and label distributions.",
    "reason": "Narrative citation is missing the publication year; it should appear as a narrative citation with the year in parentheses (e.g., “Alvarez et al. (2017)”).",
    "start": 449,
    "end": 463,
    "label": "Format"
  },
  {
    "span": "Graph-based traffic forecasting has progressed from early spatial-temporal convolutions to diffusion and attention mechanisms on road graphs (Yu et al., 2018; Li et al., 2018; Wu et al., 2019; Bai et al., 2020). These models incorporate sensor connectivity, temporal dynamics, and multi-hop dependencies, and report steady gains on standard datasets.",
    "document": "Introduction: Graph-based Traffic Forecasting\n\nUrban mobility systems require accurate short-term forecasting of traffic speed and volume to enable routing, signal control, and infrastructure planning. The challenge lies in capturing complex spatial-temporal dependencies across heterogeneous road networks and under distribution shifts due to incidents and weather.\n\nGraph-based traffic forecasting has progressed from early spatial-temporal convolutions to diffusion and attention mechanisms on road graphs (Yu et al., 2018; Li et al., 2018; Wu et al., 2019; Bai et al., 2020). These models incorporate sensor connectivity, temporal dynamics, and multi-hop dependencies, and report steady gains on standard datasets.\n\nIn this paper, we propose T-GraphNet, a hybrid diffusion-attention architecture with dynamic edge weighting and multi-horizon supervision for traffic forecasting.",
    "reason": "The span summarizes prior models and improvements without articulating what limitation remains or how the proposed method addresses a specific gap; it is followed by a contribution statement that also lacks an explicit gap (criterion b).",
    "start": 368,
    "end": 718,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Brown et al. (2020) introduced GPT-3 with in-context learning. Lester et al. (2021) proposed prompt tuning with continuous vectors. Gao et al. (2021) explored retrieval-augmented prompt methods.",
    "document": "Related Work\n\nPrompt-based learning has emerged as a central paradigm for adapting large language models (LLMs) to downstream tasks. Early research highlights the capacity of LLMs to follow instructions and utilize context examples without explicit fine-tuning, prompting a wave of lightweight adaptation strategies.\n\nBrown et al. (2020) introduced GPT-3 with in-context learning. Lester et al. (2021) proposed prompt tuning with continuous vectors. Gao et al. (2021) explored retrieval-augmented prompt methods. Li and Liang (2021) presented prefix tuning for controlling transformer activations, and Wei et al. (2022) studied chain-of-thought prompting for reasoning.\n\nOur work builds on this literature by examining how prompt initialization and retrieval selection interact in low-resource settings, focusing on consistency under distribution shift.",
    "reason": "The cited works are listed in sequence without transitions or explanation of how each method relates to the others, making the connection between the sentences abrupt and implicit rather than explicit.",
    "start": 318,
    "end": 512,
    "label": "Coherence"
  },
  {
    "span": "(Garcia, 2021))",
    "document": "Introduction\n\nDomain adaptation aims to transfer knowledge across distribution shifts by aligning feature spaces or learning invariant predictors (Ben-David et al., 2010; Ganin and Lempitsky, 2015). Source-free adaptation removes reliance on labeled source data during deployment by distilling transferable priors into the target (Liang et al., 2020; Kundu et al., 2021). Self-training with confidence filtering has proven effective but can amplify spurious correlations when target labels are highly skewed (Zhang et al., 2022; Wang and Zhao, 2023). Recent work emphasizes class-balanced pseudo-labeling and uncertainty-aware consistency (Huang et al., 2021; Lee and Kim, 2022). A complementary perspective leverages clustering and centroid alignment to stabilize training (Garcia and Patel, 2020). Nevertheless, practical deployments face shifts in both features and label proportions (Tachet des Combes et al., 2020), which complicates calibration (Garcia, 2021)). We propose a bi-level objective that jointly reweights classes and aligns conditional distributions, leading to improved calibration and transfer.\n",
    "reason": "Extra closing parenthesis at the end of the citation; should be a single closing parenthesis: \"(Garcia, 2021)\".",
    "start": 951,
    "end": 966,
    "label": "Format"
  },
  {
    "span": "Large language models for code generation have been explored via in-context learning (Brown et al., 2020), prompt engineering (Liu et al., 2021; Reynolds and McDonell, 2021), few-shot exemplars (Chen et al., 2021), instruction tuning (Sanh et al., 2022), and chain-of-thought prompting (Wei et al., 2022). Retrieval-augmented generation and program repair have also been studied (Borgeaud et al., 2022; Le et al., 2016).",
    "document": "Introduction\n\nProgram synthesis with large language models holds promise for accelerating software development, education, and data science. However, practical deployment requires reliability under distribution shifts, correct use of external tools, and calibrated uncertainty estimates that support human oversight.\n\nLarge language models for code generation have been explored via in-context learning (Brown et al., 2020), prompt engineering (Liu et al., 2021; Reynolds and McDonell, 2021), few-shot exemplars (Chen et al., 2021), instruction tuning (Sanh et al., 2022), and chain-of-thought prompting (Wei et al., 2022). Retrieval-augmented generation and program repair have also been studied (Borgeaud et al., 2022; Le et al., 2016).\n\nIn this work, we target tool-integrated code synthesis and propose an execution-calibrated decoding strategy that harmonizes test-time retrieval, unit-test feedback, and verifier signals. We introduce a selective prediction wrapper that abstains when failure is likely, improving pass@k under a fixed runtime budget on HumanEval+ and MBPP-ET.",
    "reason": "The span enumerates prior techniques without relating them to the paper’s focus on tool integration or specifying what is missing, thus lacking synthesis with the authors’ motivation (criterion a/c).",
    "start": 318,
    "end": 738,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Closely related to claim verification is the problem of fake news detection. In this problem, the credibility of an entire news article is evaluated. The credibility of a news article can be estimated based on linguistic and textual features (Conroy et al., 2015;Reis et al., 2019;Li et al., 2019), discourse level structure (Karimi and Tang, 2019), network analysis (Conroy et al., 2015), knowledge graphs (Cui et al., 2020), inter-user behaviour dynamics (Gangireddy et al., 2020) or a combination of multiple modalities . Some techniques reorder the articles returned by a search engine based on their degree of credibility (Olteanu et al., 2013;Beylunioglu, 2020). An important task that can help the detection of fake news is the task of stance detection (Borges et al., 2019;Jwa et al., 2019), i.e., does the content of an article agree or disagree with the title of the article? The following surveys summarize existing work on fake news detection: (Kumar and Shah, 2018;Bondielli and Marcelloni, 2019).",
    "document": "Related Work\n\nThere is an important line of work that focuses on claim verification. This includes techniques that predict the veracity of a claim based on the text of the claim only (Rashkin et al., 2017), linguistic features (Popat et al., 2017), meta information about the claimant (e.g., name, job, party affiliation, veracity history) (Wang, 2017b), review articles (Augenstein et al., 2019;Shu et al., 2018;Nakov et al., 2021), as well as relevant articles returned by a search engine (Popat et al., 2018;Augenstein et al., 2019;Mishra and Setty, 2019). To the best of our knowledge, none of the existing work considers the problem of claim verification based on premise articles. There is an important distinction between articles returned by a search engine in previous work and the premise articles that we consider. The techniques that use a search engine to find articles related to a claim query the search engine after a fact checking website has published a review article and therefore end up retrieving articles that include the review article as well as other articles that summarize and/or discuss the verdict of the fact checking website. Hence they are tackling an entailment problem. In contrast, the premise articles that we consider are the source articles used by a fact checker before publishing a review article. Those articles contain relevant facts, but not a summary or discussion of the review article since they are published before the review article and in fact serve as premises for the review article.\n\nClosely related to claim verification is the problem of fake news detection. In this problem, the credibility of an entire news article is evaluated. The credibility of a news article can be estimated based on linguistic and textual features (Conroy et al., 2015;Reis et al., 2019;Li et al., 2019), discourse level structure (Karimi and Tang, 2019), network analysis (Conroy et al., 2015), knowledge graphs (Cui et al., 2020), inter-user behaviour dynamics (Gangireddy et al., 2020) or a combination of multiple modalities . Some techniques reorder the articles returned by a search engine based on their degree of credibility (Olteanu et al., 2013;Beylunioglu, 2020). An important task that can help the detection of fake news is the task of stance detection (Borges et al., 2019;Jwa et al., 2019), i.e., does the content of an article agree or disagree with the title of the article? The following surveys summarize existing work on fake news detection: (Kumar and Shah, 2018;Bondielli and Marcelloni, 2019).\n\n ",
    "start": 1538,
    "end": 2548,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Patel et al. 1",
    "document": "Related Work\n\nAlgorithmic fairness literature spans pre-, in-, and post-processing techniques to mitigate bias in predictive models (Kamiran and Calders, 2012; Hardt et al., 2016). Causality-based methods seek counterfactual fairness by modeling sensitive attributes and their effects (Kusner et al., 2017; Kilbertus et al., 2017). Distribution-shift robustness intersects with fairness when sensitive subpopulations are underrepresented (Sagawa et al., 2020; Liu et al., 2021). For a broad survey of group and individual fairness definitions, see Patel et al. 1 and the comprehensive taxonomy proposed in recent overviews (Mehrabi et al., 2021; Verma and Rubin, 2018). Our work focuses on calibration-aware post-processing that preserves utility while improving parity across subgroups (Pleiss et al., 2017; Guo et al., 2017).\n",
    "reason": "Wrong use of footnote/numbering in place of a proper citation; should include a year (e.g., \"Patel et al. (2020)\") or be formatted as a proper reference/footnote.",
    "start": 548,
    "end": 562,
    "label": "Format"
  },
  {
    "span": "there are many recent works on multilingual speech recognition for low-resource languages",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) has benefited substantially from end-to-end neural architectures and large-scale pretraining. However, building robust systems for under-represented languages remains challenging due to limited transcribed audio and domain mismatch. In multilingual and cross-lingual settings, model sharing and transfer have emerged as promising strategies for improving performance when labeled data are scarce. In particular, there are many recent works on multilingual speech recognition for low-resource languages that explore shared subword vocabularies, language adapters, and self-supervised pretraining. Nevertheless, issues of negative transfer, code-switching, and lexicon variability persist. This paper revisits multilingual transfer with a focus on parameter-efficient tuning and simple data balancing to mitigate overfitting to high-resource sources.\n",
    "reason": "Uses the phrase “recent works” to claim a body of literature without providing any citations to those works.",
    "start": 461,
    "end": 550,
    "label": "Unsupported_claim"
  },
  {
    "span": "Early multimodal sentiment studies fused audio and visual features with text encoders (Poria et al., 2017). Transformer-based models achieved new benchmarks on CMU-MOSI and CMU-MOSEI (Tsai et al., 2019; Rahman et al., 2020). Fusion-gating mechanisms were introduced for cross-modal alignment (Mai et al., 2021).",
    "document": "Related Work\n\nMultimodal sentiment analysis aims to infer user opinions by integrating language, acoustic, and visual signals. Prior work has examined alignment, fusion, and temporal modeling across modalities to improve robustness and generalization. While end-to-end neural architectures have become prevalent, their components vary widely in how they capture cross-modal interactions.\n\nEarly multimodal sentiment studies fused audio and visual features with text encoders (Poria et al., 2017). Transformer-based models achieved new benchmarks on CMU-MOSI and CMU-MOSEI (Tsai et al., 2019; Rahman et al., 2020). Fusion-gating mechanisms were introduced for cross-modal alignment (Mai et al., 2021).\n\nIn contrast to pure fusion strategies, some approaches prioritize alignment via co-attention or shared latent spaces, whereas others focus on robustness to missing modalities. Our work targets controllable fusion under sparse supervision, building on insights from prior benchmarks but emphasizing explainability.",
    "reason": "The three sentences list separate works without explicit transitions or statements clarifying how each relates to the others; the connection between cited works is abrupt and implied rather than explained.",
    "start": 389,
    "end": 700,
    "label": "Coherence"
  },
  {
    "span": "Marino et al. (2019) augment VQA with ConceptNet relations. Wang et al. (2018) use memory networks to store retrieved facts. Teney et al. (2017) emphasize bottom-up attention for object features. Garderes et al. (2020) integrate pre-trained knowledge graph embeddings.",
    "document": "Related Work\n\nKnowledge-Augmented Visual Question Answering\n\nAnswering questions that require external knowledge often combines visual features with retrieved facts from structured resources. Approaches differ in retrieval, representation, and fusion mechanisms between modalities. Marino et al. (2019) augment VQA with ConceptNet relations. Wang et al. (2018) use memory networks to store retrieved facts. Teney et al. (2017) emphasize bottom-up attention for object features. Garderes et al. (2020) integrate pre-trained knowledge graph embeddings. Evaluation varies by the degree of commonsense reasoning and the coverage of the knowledge base.\n\nWe propose a retrieval-calibrated fusion module that aligns object regions with subgraph neighborhoods, improving grounding for compositional queries.",
    "reason": "The span abruptly mixes knowledge-based methods with a general attention baseline and does not explain how memory networks or graph embeddings relate to ConceptNet augmentation, lacking transitions and explicit relationships.",
    "start": 282,
    "end": 550,
    "label": "Coherence"
  },
  {
    "span": "(Wang et al., '19)",
    "document": "Related Work\n\nTime-series forecasting with deep learning spans sequence models, temporal convolutions, and attention mechanisms (Salinas et al., 2020; Lim et al., 2021). Probabilistic formulations provide calibrated uncertainty estimates critical for decision-making (Wen et al., 2017). For long-horizon forecasting, hierarchical decoders and seasonal-trend decompositions help mitigate error accumulation (Makridakis et al., 2018; (Wang et al., '19). We focus on scalable transformers with structured time embeddings for industrial workloads.\n\nOur contributions include an efficient decoder, a new benchmark, and an analysis of distribution shift.",
    "reason": "Year is improperly abbreviated with an apostrophe; it should be the full year, e.g., \"(Wang et al., 2019)\".",
    "start": 432,
    "end": 450,
    "label": "Format"
  },
  {
    "span": "Smith & Jones (2019)",
    "document": "Related Work\n\nEnd-to-end speech recognition has evolved from hybrid HMM-DNN pipelines to attention-based encoder–decoder and transducer architectures (Graves, 2012; Chan et al., 2016; Graves, 2012). Self-supervised pretraining with masked predictive objectives has substantially improved performance in low-resource regimes (Baevski et al., 2020; Hsu et al., 2021).\n\nLanguage modeling and shallow fusion continue to provide gains by integrating external text corpora during decoding (Gulcehre et al., 2015; Kannan et al., 2018). Robustness to noise and domain mismatch is addressed via data augmentation and domain-adaptive finetuning (Ko et al., 2015; Panayotov et al., 2015; Park et al., 2019).\n\nIn the streaming setting, Smith & Jones (2019) propose a chunk-wise attention mechanism to limit latency while preserving accuracy. We compare against monotonic alignment strategies and efficient conformer variants (Chiu and Raffel, 2018; Gulati et al., 2020).",
    "reason": "Wrong conjunction in a narrative citation: in author–year styles, narrative citations use 'and' not '&'; it should be 'Smith and Jones (2019)'.",
    "start": 724,
    "end": 744,
    "label": "Format"
  },
  {
    "span": "Differential privacy has been widely adopted in industry-scale analytics and recommendation pipelines.",
    "document": "Introduction\n\nAs data-driven services grow, privacy-preserving learning techniques are increasingly important for compliance and user trust. Differential privacy (DP) offers a formal framework that bounds the leakage of individual-level information.\n\nDifferential privacy has been widely adopted in industry-scale analytics and recommendation pipelines. Nevertheless, practical deployments expose tensions between utility, privacy budgets, and operational constraints, motivating methods that optimize privacy-utility trade-offs under resource limits.",
    "reason": "Asserts broad real-world adoption without citing any deployments or reports; per rule (b), such claims about practice require supporting citations.",
    "start": 251,
    "end": 353,
    "label": "Unsupported_claim"
  },
  {
    "span": "The LibriSpeech corpus is still the only benchmark that truly reflects real-world audiobook speech.",
    "document": "Introduction\n\nRecent advances in end-to-end automatic speech recognition have been driven by larger models and more diverse training data (Graves et al., 2014; Chan et al., 2016). Public benchmarks have played a central role in enabling reproducible progress and standardized evaluation protocols.\n\nThe LibriSpeech corpus is still the only benchmark that truly reflects real-world audiobook speech. Meanwhile, semi-supervised methods leveraging untranscribed audio have narrowed the gap between supervised and weakly supervised training regimes (Park et al., 2020), and self-supervised pretraining has further improved low-resource performance (Baevski et al., 2020). Robustness to background noise, accents, and channel variability remains an open challenge, motivating research into domain adaptation and test-time augmentation (Ko et al., 2015; Sun et al., 2017).\n\nIn this work, we present a training recipe that combines masked acoustic modeling with consistency regularization to improve recognition in mismatched audiobook conditions. We evaluate on multiple English corpora and analyze error patterns by speaker and recording environment.",
    "reason": "Claims uniqueness and benchmark status for LibriSpeech without citing the dataset or supporting evidence.",
    "start": 299,
    "end": 398,
    "label": "Unsupported_claim"
  },
  {
    "span": "After the success of AlphaFold and RoseTTAFold, subsequent works explored template-free refinement, constrained docking, and protein complex prediction using graph neural networks and SE(3)-equivariant models (Jumper et al., 2021; Baek et al., 2021; Jing et al., 2021; Evans et al., 2022).",
    "document": "Introduction\n\nProtein structure prediction has undergone a step change due to accurate deep learning models that map sequences to 3D conformations. Nevertheless, modeling dynamics, complexes, and design objectives remains an active research frontier.\n\nAfter the success of AlphaFold and RoseTTAFold, subsequent works explored template-free refinement, constrained docking, and protein complex prediction using graph neural networks and SE(3)-equivariant models (Jumper et al., 2021; Baek et al., 2021; Jing et al., 2021; Evans et al., 2022).\n\nWe introduce a generative diffusion model over torsion angles that conditions on sparse experimental constraints, enabling uncertainty-aware sampling for complex assembly.",
    "reason": "The span summarizes prior advances without explaining their shortcomings relative to the present objective or how the current approach addresses any gap.",
    "start": 252,
    "end": 541,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In a previous study, the authors claim that pretraining on code-switching data eliminates domain mismatch.",
    "document": "Related Work\n\nCode-switching presents unique challenges for representation learning, including tokenization drift, morphological mixing, and contextual shifts. Prior efforts have examined data augmentation, specialized tokenizers, and mixed-language pretraining to improve performance. In a previous study, the authors claim that pretraining on code-switching data eliminates domain mismatch. Nevertheless, it remains unclear how these strategies interact with downstream tasks that vary in sequence length and discourse structure. Our work compares pretraining regimes under unified evaluation and quantifies improvements beyond lexical overlap.",
    "reason": "Mentions a specific 'previous study' and its claim without providing a citation to that study.",
    "start": 286,
    "end": 392,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT was used in an AES setup trained on argumentative essays",
    "document": "Related Work\n\nAutomatic essay scoring (AES) research has evolved from handcrafted features and regression to neural encoders that better capture coherence and discourse structure. BERT was used in an AES setup trained on argumentative essays to capture long-range dependencies and context-sensitive usage, reporting improvements over earlier LSTM baselines. Other strands considered prompt-specific encoders and rank-based losses to reduce score inflation. We extend this line with a hybrid objective that constrains rubric-aligned evidence while preserving holistic fluency cues.",
    "reason": "Specific prior setup and claim are mentioned without citing the originating study (rule e/iii).",
    "start": 180,
    "end": 241,
    "label": "Unsupported_claim"
  },
  {
    "span": "Gilmer et al. (2017) proposed neural message passing for quantum chemistry datasets. SMILES-based recurrent models have also been used (Segler et al., 2018). AttentiveFP applies attention to graph nodes for property prediction (Xiong et al., 2020). 3D-aware models incorporate geometric tensors (Schütt et al., 2018).",
    "document": "Related Work\n\nGraph Neural Networks for Molecular Property Prediction\n\nRecent advances in graph neural networks (GNNs) have driven progress on molecular property prediction by leveraging the relational structure of atoms and bonds. Approaches differ in how they encode local neighborhoods, handle long-range interactions, and incorporate 3D information.\n\nModeling Strategies\n\nGilmer et al. (2017) proposed neural message passing for quantum chemistry datasets. SMILES-based recurrent models have also been used (Segler et al., 2018). AttentiveFP applies attention to graph nodes for property prediction (Xiong et al., 2020). 3D-aware models incorporate geometric tensors (Schütt et al., 2018).\n\nData and Evaluation Protocols\n\nBenchmarks such as QM9, MoleculeNet, and Open Graph Benchmark have standardized evaluation (Wu et al., 2018; Hu et al., 2020). We build on these datasets but analyze calibration under covariate shift between assay conditions.",
    "reason": "The paragraph lists several methods (message passing, SMILES RNNs, attention, 3D tensors) without explaining how they relate to each other or transition between modalities, leaving the relationships implied and coherence weak.",
    "start": 376,
    "end": 693,
    "label": "Coherence"
  },
  {
    "span": "Classical methods detect anomalies via statistical thresholds (Schölkopf et al., 2001). Forecasting-based detectors flag large residuals (Hundman et al., 2018). Self-supervised pretext tasks have been applied to time series (Eldele et al., 2021).",
    "document": "Related Work\n\nTime series anomaly detection spans classical statistics, representation learning, and deep forecasting. Approaches differ in whether they model normality directly, reconstruct signals, or predict future trajectories.\n\nClassical methods detect anomalies via statistical thresholds (Schölkopf et al., 2001). Forecasting-based detectors flag large residuals (Hundman et al., 2018). Self-supervised pretext tasks have been applied to time series (Eldele et al., 2021).\n\nWhile effective in certain regimes, many methods degrade under distribution shift or limited labels. We explore contrastive forecasting that unifies prediction and invariance to improve robustness.",
    "reason": "The span lists methods across paradigms with no connective tissue; it does not state how these works relate to each other or to a common theme, causing poor coherence.",
    "start": 233,
    "end": 479,
    "label": "Coherence"
  },
  {
    "span": "Martinez et al.",
    "document": "Introduction\n\nAbstractive summarization has advanced due to attention and pretraining. The pointer-generator network (See et al., 2017) introduced copying. Later, pre-trained encoders such as BERT improved summarization (Liu and Lapata, 2019). Martinez et al. argue that dataset biases confound evaluations, prompting robustness benchmarks (Narayan et al., 2018; Fabbri et al., 2021). We build upon these insights to examine domain shift in news and scientific summarization with consistent automatic and human evaluation protocols.",
    "reason": "Narrative citation missing year; should be formatted as 'Martinez et al. (YEAR)'.",
    "start": 244,
    "end": 259,
    "label": "Format"
  },
  {
    "span": "Chen et al. (2021) leveraged large language models for program synthesis from docstrings. Austin et al. (2021) examined compositional generalization in code generation. Nijkamp et al. (2023) proposed code-specific pretraining corpora. Le et al. (2022) studied execution-guided decoding.",
    "document": "Related Work\n\nNeural program synthesis has benefited from scaling language models and integrating execution feedback. Pretraining on code corpora and carefully designed decoding strategies both contribute to improvements in functional correctness. However, evaluations often overestimate generalization due to template leakage and benchmark contamination, motivating stricter protocols.\n\nChen et al. (2021) leveraged large language models for program synthesis from docstrings. Austin et al. (2021) examined compositional generalization in code generation. Nijkamp et al. (2023) proposed code-specific pretraining corpora. Le et al. (2022) studied execution-guided decoding.\n\nWe propose a verifier-aware training loop that distills counterexamples into the reranker and calibrates confidence with pass@k-consistent scores, improving reliability under decontaminated splits.",
    "reason": "The span strings together four citations without specifying relationships among model scaling, compositionality, data curation, and decoding. The lack of connective explanations results in weak coherence.",
    "start": 388,
    "end": 674,
    "label": "Coherence"
  },
  {
    "span": "It is well known that simple fine-tuning on the client outperforms meta-learning based approaches.",
    "document": "Related Work\n\nFederated learning enables training models across decentralized data silos while preserving privacy (McMahan et al., 2017). Personalization methods aim to adapt global models to heterogeneous client distributions using techniques such as fine-tuning, multi-task learning, and meta-learning (Smith et al., 2017; Fallah et al., 2020). Evaluations typically consider performance under varying participation rates and non-IID splits.\n\nIt is well known that simple fine-tuning on the client outperforms meta-learning based approaches. Nevertheless, fine-tuning can overfit clients with very small datasets and destabilize aggregation when local steps are excessive. We propose an adaptive proximal schedule that balances generalization and personalization across clients.\n\nIntroduction\n\nOur contributions include a new benchmark protocol with cross-client early stopping and a sensitivity analysis over data heterogeneity parameters.",
    "reason": "Asserts a general consensus ('well known') comparing methods without citing empirical studies that demonstrate this result.",
    "start": 445,
    "end": 543,
    "label": "Unsupported_claim"
  },
  {
    "span": "Vatswani et al.",
    "document": "Introduction\n\nGraph neural networks (GNNs) generalize convolutional architectures to irregular domains, enabling learning on graphs (Hamilton et al., 2017; Kipf and Welling, 2017). Despite their success, over-smoothing and scalability remain open problems (Li et al., 2018; Chen et al., 2020).\n\nRecent studies explore sampling and hierarchical pooling to address these challenges (Gao and Ji, 2019; Ying et al., 2018). As shown by Vatswani et al., residual connections can alleviate gradient degradation in deep GNNs, but they may also propagate noise across distant nodes. In this work, we propose a depth-aware normalization scheme that stabilizes training without sacrificing expressivity.\n\nWe evaluate on transductive and inductive benchmarks, demonstrating improvements over prior baselines while maintaining efficiency on large graphs.",
    "reason": "Narrative citation missing year; should be formatted as “Vatswani et al. (YEAR)”.",
    "start": 431,
    "end": 446,
    "label": "Format"
  },
  {
    "span": "(Olsen, 2022;)",
    "document": "Related Work\n\nCalibration in classification is commonly improved by post-hoc methods such as temperature scaling (Guo et al., 2017) and Dirichlet calibration (Kull et al., 2019). Downstream applications in medical AI require well-calibrated decisions under shift (Ashukha et al., 2020; Ovadia et al., 2019). For structured prediction, sequence-level calibration remains challenging due to exposure bias (Kumar and Sarawagi, 2019). Recent studies propose distributionally robust objectives to align confidence with coverage (Olsen, 2022;). Our work unifies these objectives with conformal prediction for end-to-end calibration.",
    "reason": "Trailing semicolon inside the parenthetical citation; citation punctuation/formatting is incorrect.",
    "start": 523,
    "end": 537,
    "label": "Format"
  },
  {
    "span": "Recent works have explored controllable summarization by injecting aspect prompts and factual constraints, showing consistent gains on news benchmarks.",
    "document": "Introduction\n\nAbstractive summarization aims to produce concise narratives that retain salient information while avoiding redundancy. While sequence-to-sequence models have driven substantial progress, they often struggle to balance faithfulness and coverage. Recent works have explored controllable summarization by injecting aspect prompts and factual constraints, showing consistent gains on news benchmarks. However, the mechanisms by which control signals interact with pretrained decoders remain underexplored. In this paper, we introduce a lightweight control adapter that conditions decoding on user-specified intents without modifying backbone parameters. We evaluate our approach under varying degrees of control and noise to assess stability and generality.",
    "reason": "Mentions 'recent works' and their findings without providing any citations to the relevant studies (criterion d).",
    "start": 260,
    "end": 411,
    "label": "Unsupported_claim"
  },
  {
    "span": "According to widely reported statistics, low-resource languages account for 40% of global translation demand.",
    "document": "Introduction\n\nMachine translation (MT) has achieved remarkable progress for high-resource language pairs, yet quality remains uneven for low-resource communities. Data scarcity, domain drift, and orthographic variation hinder standard neural training pipelines. According to widely reported statistics, low-resource languages account for 40% of global translation demand. This gap motivates research into data augmentation, multilingual transfer, and active learning strategies tailored to community needs.\n\nWe investigate curriculum-based multilingual pretraining coupled with bilingual lexicon induction to reduce reliance on parallel corpora. We also analyze the impact of script normalization and subword segmentation for morphologically rich languages. Our evaluation includes domain-specific test sets curated with community partners to better reflect real-world usage scenarios such as public health communication and legal aid.",
    "reason": "Presents a precise statistic about global translation demand without citing a data source, report, or study.",
    "start": 262,
    "end": 371,
    "label": "Unsupported_claim"
  },
  {
    "span": "However, the motivation for weight normalisation is guided empirically. From the perspective of this work, weight normalisation provably prevents Stolen Probability from arising when a softmax layer has no bias term",
    "document": "Related Work\n\nOther works have observed limitations of the softmax layer when modelling infrequent classes for image classification (Kang et al., 2020) and rare words for MT (Nguyen and Chiang, 2018;Raunak et al., 2020). They show that normalising the magnitude of the softmax weight vectors improves predictions for infrequent classes. However, the motivation for weight normalisation is guided empirically. From the perspective of this work, weight normalisation provably prevents Stolen Probability from arising when a softmax layer has no bias term. For more details, see Section D in the Appendix.\n\n ",
    "start": 337,
    "end": 552,
    "label": "Coherence"
  },
  {
    "span": "Garcia et al. 2",
    "document": "Introduction\n\nInformation retrieval has evolved from lexical matching to representation learning. Early systems operationalized term weighting in the vector space model (Salton et al., 1975) and probabilistic approaches such as BM25 (Robertson and Zaragoza, 2009). Neural ranking models later replaced hand-crafted features with learned interactions (Guo et al., 2016; Xiong et al., 2017). Foundational studies of interactive retrieval were pioneered by Garcia et al. 2 and expanded by user-centric evaluations that incorporated satisfaction signals (Hassan et al., 2014).",
    "reason": "Improper footnote-style numeral used with a citation and missing year; should include a year or be formatted as a proper footnote/citation.",
    "start": 454,
    "end": 469,
    "label": "Format"
  },
  {
    "span": "Koren et al. (2009) studied matrix factorization for collaborative filtering. He et al. (2017) proposed Neural Collaborative Filtering. Rendle (2010) introduced Bayesian Personalized Ranking. Covington et al. (2016) described the YouTube recommendation system.",
    "document": "Related Work\n\nPersonalized recommendation has been driven by advances in representation learning and pairwise ranking objectives. As platforms scale to billions of interactions, challenges include modeling long-term preferences, handling cold-start users and items, and controlling exposure bias in logged data.\n\nCollaborative filtering and ranking\n\nKoren et al. (2009) studied matrix factorization for collaborative filtering. He et al. (2017) proposed Neural Collaborative Filtering. Rendle (2010) introduced Bayesian Personalized Ranking. Covington et al. (2016) described the YouTube recommendation system. Wang et al. (2019) combined knowledge graphs with CF to address sparsity.\n\nCounterfactual learning and debiasing\n\nRecent works apply inverse propensity weighting, doubly robust estimators, and slate-aware objectives to mitigate bias from historical policies. These approaches inform our design of a two-stage learner that separates deconfounded preference estimation from exposure modeling and re-ranking.",
    "reason": "This span lists multiple cited works in succession with no transitions or explicit connections, making it unclear how they relate to one another or to the narrative. This violates (a) and (b) and involves multiple sentences per (c).",
    "start": 350,
    "end": 610,
    "label": "Coherence"
  },
  {
    "span": "Brown & Clark (2015)",
    "document": "Introduction\n\nHuman–computer interaction research emphasizes usable, transparent systems that support user goals (Norman, 2013). Contrary to Brown & Clark (2015), our interface prioritizes exposing model rationales over hiding complexity, aligning with calls for explainability in interactive ML (Amershi et al., 2014; Ribeiro et al., 2016). We also adopt participatory design to iteratively refine workflows (Spinuzzi, 2005).",
    "reason": "Wrong author connector in narrative citation for the chosen style; should be “Brown and Clark (2015)”.",
    "start": 141,
    "end": 161,
    "label": "Format"
  },
  {
    "span": "[12]",
    "document": "Related Work\n\nPolicy gradient methods optimize stochastic policies directly using gradient estimates with variance reduction (Williams, 1992; Sutton et al., 2000). Trust region constraints and proximal objectives stabilize updates under function approximation (Schulman et al., 2015; Schulman et al., 2017). Actor-critic variants incorporate learned baselines to reduce variance while maintaining bias control (Konda and Tsitsiklis, 2000). Classic implementations demonstrate strong performance on continuous control [12], but these results often assume on-policy sampling.\n\nIn contrast, off-policy algorithms reuse data more efficiently via replay buffers (Lillicrap et al., 2016; Fujimoto et al., 2018). We build on these insights with a conservative update rule that improves sample efficiency under sparse rewards.",
    "reason": "Numeric bracket citation used in an author–year styled bibliography context; should cite by author and year instead of [12].",
    "start": 517,
    "end": 521,
    "label": "Format"
  },
  {
    "span": "In (Khan et al., 2019)",
    "document": "Introduction\n\nPretrained graph encoders have accelerated progress in molecule property prediction by enabling rapid fine-tuning on small datasets (Hu et al., 2020; Rong et al., 2020). In (Khan et al., 2019) a template-free approach is presented for reaction outcome prediction, while concurrent work integrates attention over atom pairs to capture long-range effects (Ying et al., 2021). However, these models often rely on extensive pretraining corpora and struggle when domain-specific functional groups are underrepresented (Stokes et al., 2020).\n\nData-centric methods address sparsity through active learning and synthetic augmentation (Zhu et al., 2021; Gao et al., 2020). Uncertainty estimation further supports risk-aware screening pipelines (Hirschfeld et al., 2020; Scalia et al., 2020). Yet, capturing stereochemistry and conformational ensembles remains a key obstacle, motivating 3D-aware message passing and equivariant networks (Schütt et al., 2017; Klicpera et al., 2020).\n\nWe propose a lightweight contrastive objective tailored to sparse reaction classes, reducing reliance on large-scale pretraining while preserving transferability. Our analysis emphasizes calibration, ablation on augmentation policies, and thorough evaluation on scaffold splits (Ramakrishnan et al., 2014; Wu et al., 2018).",
    "reason": "Wrong citation style placing a preposition inside the parentheses; should be 'In Khan et al. (2019)' or simply '(Khan et al., 2019)'.",
    "start": 184,
    "end": 206,
    "label": "Format"
  },
  {
    "span": "The retriever is trained with contrastive objectives (Karpukhin et al., 2020). Span-based extractors use start/end pointers over passages (Devlin et al., 2019). Dense passage indexing can be compressed with product quantization (Johnson et al., 2017).",
    "document": "Related Work\n\nOpen-Domain QA Pipelines. Modern ODQA systems typically comprise a retrieval component that surfaces potentially relevant passages and a reader that extracts or generates answers conditioned on the retrieved context. This modular design enables scaling to large corpora while maintaining reasoning capacity.\n\nRetrievers and Readers. Sparse bag-of-words and dense dual-encoders are widely used retrievers, whereas extractive and generative readers differ in whether they copy text or synthesize answers. Methods often co-train or iteratively distill retrievers using reader feedback.\n\nTraining and Efficiency. The retriever is trained with contrastive objectives (Karpukhin et al., 2020). Span-based extractors use start/end pointers over passages (Devlin et al., 2019). Dense passage indexing can be compressed with product quantization (Johnson et al., 2017).\n\nGenerative Readers. Sequence-to-sequence models take concatenated or fused evidence as input and can aggregate signals from multiple documents. Recent approaches focus on reranking, iterative retrieval, and answer calibration.",
    "reason": "Three sentences juxtapose retriever contrastive learning, reader span extraction, and indexing compression without clarifying their relationships or transitions, making the connection between topics abrupt and unclear.",
    "start": 623,
    "end": 874,
    "label": "Coherence"
  },
  {
    "span": "Neural program synthesis approaches include sequence-to-sequence models that map language to code (Ling et al., 2016; Yin and Neubig, 2018), grammar-constrained decoders that enforce syntactic validity (Rabinovich et al., 2017; Yin and Neubig, 2017), and retrieval-augmented generation that conditions on similar code snippets (Hayati et al., 2018; Hashimoto et al., 2018). Probabilistic symbolic methods search over program spaces using enumerative or constraint-based techniques (Gulwani, 2011; Solar-Lezama, 2008).",
    "document": "Related Work\n\nMapping natural language to executable programs has attracted interest due to applications in code assistance and end-user programming. Approaches vary in how they balance search, constraints, and learning from data.\n\nNeural program synthesis approaches include sequence-to-sequence models that map language to code (Ling et al., 2016; Yin and Neubig, 2018), grammar-constrained decoders that enforce syntactic validity (Rabinovich et al., 2017; Yin and Neubig, 2017), and retrieval-augmented generation that conditions on similar code snippets (Hayati et al., 2018; Hashimoto et al., 2018). Probabilistic symbolic methods search over program spaces using enumerative or constraint-based techniques (Gulwani, 2011; Solar-Lezama, 2008).\n\nRecent work also studies execution-guided decoding and dynamic verification to prune incorrect candidates (Chen et al., 2019; Austin et al., 2021). In contrast, we focus on grounding code generation to task-relevant APIs using structured constraints extracted from specifications, improving generalization to unseen libraries and tasks.",
    "reason": "The span catalogs neural and symbolic approaches with citations but does not synthesize them in relation to the paper's focus or highlight a specific gap. It lacks synthesis as defined in (a) and (c).",
    "start": 232,
    "end": 749,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Global mean surface temperature has increased by 1.3°C since pre-industrial levels.",
    "document": "Introduction\n\nData-driven climate modeling complements physics-based general circulation models by learning subgrid processes and emulators for long-term projections (Rasp et al., 2018; Schneider et al., 2017). Hybrid approaches constrain machine learning models with physical priors to improve stability and extrapolation.\n\nGlobal mean surface temperature has increased by 1.3°C since pre-industrial levels. Understanding how this aggregate warming propagates to regional extremes is essential for risk assessment and adaptation planning.",
    "reason": "Reports a quantitative statistic about global temperature change without a source (rule b: statistical claim requires evidence/citation).",
    "start": 325,
    "end": 408,
    "label": "Unsupported_claim"
  },
  {
    "span": "In a previous study, the authors claim that sequence-level KD outperforms frame-level KD for noisy speech.",
    "document": "Related Work\n\nKnowledge distillation (KD) trains a compact student model to mimic a stronger teacher (Hinton et al., 2015). In ASR, KD has been applied to acoustic models, pronunciation modeling, and end-to-end transducers, often yielding improvements in both WER and decoding efficiency (Kurata and Audhkhasi, 2018; Chebotar and Waters, 2016; Watanabe et al., 2017). Subsequent work explored data augmentation and temperature scaling to stabilize KD under domain shift (Park et al., 2019; Kim and Rush, 2016).\n\nIn a previous study, the authors claim that sequence-level KD outperforms frame-level KD for noisy speech. Other work has investigated teacher-free objectives and self-training for semi-supervised ASR, highlighting the importance of confidence estimation (Kahn et al., 2020; Xu et al., 2021). Our approach differs by aligning student and teacher distributions with a noise-robust contrastive loss, avoiding explicit frame alignments.",
    "reason": "Refers to a specific prior study and its claim but provides no citation.",
    "start": 512,
    "end": 618,
    "label": "Unsupported_claim"
  },
  {
    "span": "Static taint analyses trace information flows using IFDS/IDE (Reps et al., 1995; Sagiv et al., 2002), context sensitivity (Sridharan and Fink, 2009), and on-demand pointer analysis (Smaragdakis and Balatsouras, 2015). Dynamic tools leverage instrumentation and fuzzing (Zalewski, 2014; Böhme et al., 2017) to discover vulnerabilities at scale. We propose a hybrid approach combining static summaries with dynamic validation.",
    "document": "Related Work\n\nProgram analysis for security seeks precise yet scalable techniques to detect data leaks and exploitable paths. Trade-offs arise between over-approximation in static analysis and incomplete coverage in dynamic testing.\n\nStatic taint analyses trace information flows using IFDS/IDE (Reps et al., 1995; Sagiv et al., 2002), context sensitivity (Sridharan and Fink, 2009), and on-demand pointer analysis (Smaragdakis and Balatsouras, 2015). Dynamic tools leverage instrumentation and fuzzing (Zalewski, 2014; Böhme et al., 2017) to discover vulnerabilities at scale. We propose a hybrid approach combining static summaries with dynamic validation.\n\nOur evaluation examines precision-recall and triage cost on large Java and C++ codebases with seeded and real-world CVEs, and measures analysis time under varying budget constraints.\n",
    "reason": "The span lists prior static and dynamic techniques and then states the contribution, but it does not identify the specific gap that motivates a hybrid approach or how it improves on limitations, corresponding to (b).",
    "start": 234,
    "end": 658,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Most existing datasets contain on average 3.2 reference summaries per document.",
    "document": "Introduction\n\nAbstractive summarization benchmarks have enabled rapid progress in neural sequence modeling, with news datasets emphasizing factual compression and conversational datasets targeting dialogue-specific phenomena (See et al., 2017; Narayan et al., 2018; Zhong et al., 2021). Evaluation remains challenging due to lexical variance and faithfulness concerns, motivating reference-free metrics and human studies (Durmus et al., 2020; Laskar et al., 2022). Most existing datasets contain on average 3.2 reference summaries per document. However, the number and diversity of references affect both training dynamics and metric reliability, a factor that has received limited systematic study.\n\nWe introduce MultiRefSum, a corpus with variable numbers of references per source and controlled diversity, to analyze how reference multiplicity impacts model learning and evaluation.",
    "reason": "Provides a precise statistic about datasets without citing any source or showing evidence (violates rule b and e).",
    "start": 465,
    "end": 544,
    "label": "Unsupported_claim"
  },
  {
    "span": "Chen et al. 2",
    "document": "Related Work. Data augmentation has shown strong benefits in low-resource classification, from back-translation to mixup-style interpolations (Sennrich et al., 2016; Zhang et al., 2018). Task-specific transformations for text classification often combine lexical substitution and paraphrasing with semantic constraints (Wei & Zou, 2019; Kumar et al., 2020). Chen et al. 2 argue for example-dependent augmentation schedules that adapt to model uncertainty, while Fadaee et al. (2017) emphasize rare word substitution to improve lexical coverage. Despite these advances, overly aggressive augmentation can harm calibration and degrade minority class performance (Guo et al., 2017; Minderer et al., 2021).",
    "reason": "Wrong use of footnotes/markers: 'Chen et al. 2' uses a bare superscript-like number without proper citation formatting; it should include the year (e.g., 'Chen et al. (YEAR)') or be formatted as a proper footnote.",
    "start": 358,
    "end": 371,
    "label": "Format"
  },
  {
    "span": "Stereotype sensitivity metrics quantify bias in generated text (Nangia et al., 2020; Nadeem et al., 2021). Instruction-tuning improves general following ability (Ouyang et al., 2022; Wei et al., 2022).",
    "document": "Introduction\n\nLarge language models (LLMs) have made rapid progress on few-shot and zero-shot tasks, yet their behavior varies substantially across demographic groups and prompt formats. Recent work highlights the need for evaluation protocols that are robust to prompt phrasing while reflecting real-world harms.\n\nBias auditing frameworks propose targeted challenge sets to probe stereotypes and offensive content in model outputs (Sap et al., 2019; Kirk et al., 2021). Calibrated confidence and abstention can mitigate unsafe generations by deferring uncertain cases (Kumar and Sarawagi, 2019; Kamath et al., 2020). Stereotype sensitivity metrics quantify bias in generated text (Nangia et al., 2020; Nadeem et al., 2021). Instruction-tuning improves general following ability (Ouyang et al., 2022; Wei et al., 2022). In-context learning methods further demonstrate that formatting and exemplars strongly affect responses (Brown et al., 2020; Min et al., 2022).\n\nWe introduce a suite that jointly perturbs prompts and demographic attributes to quantify stability and fairness, connecting robustness to outcome disparities under controlled prompt variability.",
    "reason": "The two-sentence span juxtaposes bias metrics with instruction-tuning without explaining how tuning affects bias measurement or why it is relevant to the prior evaluation discussion, lacking connective tissue.",
    "start": 618,
    "end": 819,
    "label": "Coherence"
  },
  {
    "span": "We follow the annotation schema introduced by the 2014 i2b2 challenge.",
    "document": "Introduction\n\nDe-identification of clinical narratives aims to remove protected health information while preserving clinical utility. Approaches span rule-based systems, sequence labeling with CRFs, and modern neural architectures that integrate character and contextual embeddings. Evaluation in this domain requires careful attention to entity granularity and boundary consistency.\n\nCommunity benchmarks have catalyzed progress by providing shared corpora and task definitions for electronic health records. We follow the annotation schema introduced by the 2014 i2b2 challenge. However, differences in note types and de-identification policies across institutions can substantially affect both precision and recall, suggesting a need for domain-adaptive models.\n\nOur work examines cross-institution generalization in de-identification. We assemble a multi-institution corpus with harmonized labels, propose a lightweight adapter for clinical subdomains, and evaluate transfer under privacy-preserving constraints.",
    "reason": "Mentions a specific shared task and its schema without citing it (violates rule a; first mention of a shared task/dataset requires citation).",
    "start": 510,
    "end": 580,
    "label": "Unsupported_claim"
  },
  {
    "span": "(2020, Chen et al.)",
    "document": "Related Work\n\nFew-shot learning methods aim to generalize from a handful of labeled examples by leveraging inductive biases or meta-learned priors (Vinyals et al., 2016; Finn et al., 2017). Metric-based approaches learn embedding spaces amenable to nearest-neighbor classification (Snell et al., 2017; Sung et al., 2018). Optimization-based methods adapt model parameters rapidly to new tasks (Ravi & Larochelle, 2017; Nichol et al., 2018). For a broader discussion, see recent meta-learning surveys (2020, Chen et al.) and evaluations emphasizing realistic task distributions (Triantafillou et al., 2019; Gidaris & Komodakis, 2018). Our work combines transductive inference with distribution calibration to reduce bias under class imbalance (Zhang et al., 2020; Liu et al., 2020).",
    "reason": "Author–year order is reversed in the parenthetical citation; should be '(Chen et al., 2020)'.",
    "start": 500,
    "end": 519,
    "label": "Format"
  },
  {
    "span": "Teacher-student distillation has been explored for ASR with CTC (Hori et al., 2019), attention-based encoder-decoders (Wang et al., 2019), and transducer models (Li et al., 2020).",
    "document": "Related Work\n\nKnowledge Distillation for ASR\nModel compression and knowledge transfer are central to deploying automatic speech recognition (ASR) on edge devices. Teacher-student distillation has been explored for ASR with CTC (Hori et al., 2019), attention-based encoder-decoders (Wang et al., 2019), and transducer models (Li et al., 2020). These methods frequently distill frame-level posteriors or sequence-level distributions.\n\nSelf-Training and Pseudo-Labeling\nBeyond distillation from labeled corpora, self-training methods leverage large unlabeled audio to improve recognition accuracy, often with confidence filtering and consistency regularization.\n\nStreaming Constraints\nFor streaming ASR, latency and memory constraints restrict model size and receptive field, motivating architectures and training techniques that preserve accuracy under strict real-time budgets.",
    "reason": "This span cites categories of prior work but does not connect them to a motivating gap, limitation, or the authors' perspective on what is missing (criterion a and c).",
    "start": 163,
    "end": 342,
    "label": "Lacks_synthesis"
  },
  {
    "span": "the D4RL benchmark is widely adopted for off-policy evaluation",
    "document": "Related Work\n\nOffline reinforcement learning seeks to learn policies from fixed datasets without additional environment interaction. This setting is attractive for domains where exploration is expensive or unsafe, such as robotics and healthcare. A central challenge is distributional shift between the behavior policy that generated the data and the learned policy.\n\nA variety of methods address extrapolation error via conservative objectives, behavior regularization, or uncertainty-aware critics. Evaluating these methods requires standardized datasets and metrics. In this context, the D4RL benchmark is widely adopted for off-policy evaluation, providing tasks that span locomotion control, navigation, and manipulation with varying dataset qualities.\n\nRecent trends include action-conditional diffusion models for value-guided planning and sequence modeling approaches that frame decision making as supervised learning. Our contribution complements these lines by proposing a bootstrapped ensemble with pessimistic value clipping that scales to high-dimensional robotic manipulation while maintaining stability.\n\nWe report results across diverse offline datasets and analyze sensitivity to dataset coverage, stochasticity, and model size. Additional studies probe generalization to unseen initial states and robustness to reward misspecification.",
    "reason": "Asserts broad community adoption of a benchmark without citing supporting surveys, reports, or the benchmark paper.",
    "start": 587,
    "end": 649,
    "label": "Unsupported_claim"
  },
  {
    "span": "We evaluate on the COCO dataset, which was originally created to evaluate everyday objects in cluttered scenes.",
    "document": "Related Work\n\nObject detection has evolved from two-stage region-based methods to single-shot detectors with strong multi-scale features (Girshick et al., 2014; Ren et al., 2015; Redmon et al., 2016; Lin et al., 2017). Modern architectures leverage transformer backbones and advanced label assignment strategies to improve precision at high IoU thresholds (Carion et al., 2020; Zhang et al., 2020). We evaluate on the COCO dataset, which was originally created to evaluate everyday objects in cluttered scenes. We further report results on a long-tailed detection benchmark to assess robustness under class imbalance (Gupta et al., 2019).",
    "reason": "First mention of a specific dataset requires a citation; none is given for COCO or its description.",
    "start": 399,
    "end": 510,
    "label": "Unsupported_claim"
  },
  {
    "span": "to the best of our knowledge, this is the first work to unify speech and text in a single decoder",
    "document": "Introduction\n\nCross-modal learning has seen rapid advances, with shared encoders enabling representation transfer between speech and text. Existing approaches often pretrain separate encoders and align them via contrastive losses or distillation. However, generation tasks still rely on modality-specific decoders, limiting parameter sharing across modalities.\n\nIn contrast, we propose a unified sequence-to-sequence architecture that accepts both acoustic features and tokenized text as inputs and produces either text or speech outputs. To the best of our knowledge, this is the first work to unify speech and text in a single decoder, enabling parameter sharing for generation across modalities and simplifying deployment.\n\nWe demonstrate consistent improvements across ASR, TTS, and speech translation tasks and provide analyses on cross-modal transfer and decoder capacity.",
    "reason": "Makes a 'first work' novelty claim without supporting citations or survey evidence (definition b/e).",
    "start": -1,
    "end": -1,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works have demonstrated that prompting strategies dramatically improve code generation accuracy.",
    "document": "Introduction\n\nLarge language models (LLMs) have rapidly advanced code generation across a variety of programming languages. Recent works have demonstrated that prompting strategies dramatically improve code generation accuracy. Despite these gains, production settings still require reliability, testability, and security guarantees that current models do not consistently offer. In this paper, we systematically compare prompting-based techniques and lightweight fine-tuning for constrained code generation, focusing on robustness to specification changes and execution safety.\n\nWe evaluate across multiple programming tasks, including algorithmic challenges and API-centric scripting. We also study the interaction between decoding strategies and static analysis, aiming to understand when constraints help or hinder model performance.",
    "reason": "Mentions 'recent works' and a substantive claim about their findings without any supporting citations (rule d).",
    "start": 124,
    "end": 227,
    "label": "Unsupported_claim"
  },
  {
    "span": "Hamilton et al. (2017) sample neighborhood nodes to enable mini-batch training on large graphs. Chen et al. (2018) control variance of stochastic training via layer-wise sampling. Graph sparsification reduces memory with minimal loss in accuracy (Serrano and Gkantsidis, 2009).",
    "document": "Related Work\n\nScalable Graph Representation Learning\n\nScaling graph neural networks requires managing memory, computation, and communication overhead. Sampling, partitioning, and compression-based strategies are common approaches to tractability. Hamilton et al. (2017) sample neighborhood nodes to enable mini-batch training on large graphs. Chen et al. (2018) control variance of stochastic training via layer-wise sampling. Graph sparsification reduces memory with minimal loss in accuracy (Serrano and Gkantsidis, 2009). However, the interplay between sampling bias, receptive field coverage, and sparsification remains poorly characterized in theory and practice.\n\nDistributed and Out-of-Core Methods\n\nSystems work explores offloading to disk and distributed training. We study single-node scalability with precise control of memory budgets.",
    "reason": "Three works are presented in sequence without articulating how they relate or build upon each other, resulting in an abrupt, non-cohesive narrative.",
    "start": 247,
    "end": 524,
    "label": "Coherence"
  },
  {
    "span": "Sennrich et al. (2016) use subword units to handle open vocabulary. Edunov et al. (2018) show the effectiveness of back-translation. Conneau and Lample (2019) pretrain cross-lingual encoders with masked language modeling. Wu et al. (2016) scale NMT with GNMT.",
    "document": "Related Work\n\nNeural machine translation (NMT) has progressed through advances in architectures, data augmentation, and pretraining. Techniques for handling morphology, leveraging monolingual data, and scaling models have each contributed to the current state of the art.\n\nSennrich et al. (2016) use subword units to handle open vocabulary. Edunov et al. (2018) show the effectiveness of back-translation. Conneau and Lample (2019) pretrain cross-lingual encoders with masked language modeling. Wu et al. (2016) scale NMT with GNMT.\n\nDespite these successes, discrete bottlenecks can improve controllability and interpretability in translation systems. We explore a variational discretization layer that preserves adequacy while enabling targeted style control.\n",
    "reason": "The span enumerates four influential NMT techniques without transitions or clarifying how each work relates to the others or to the paper’s focus, resulting in abrupt shifts between topics.",
    "start": 273,
    "end": 532,
    "label": "Coherence"
  },
  {
    "span": "Neural program synthesis generates code from natural language descriptions (Yin and Neubig, 2017). Type constraints reduce search space during decoding (Maddison and Tarlow, 2014). Unit tests provide feedback for iterative refinement (Chen et al., 2021). Retrieval augments models with relevant code snippets (Guo et al., 2022).",
    "document": "Related Work\n\nProgram synthesis with large language models has advanced rapidly, with improvements in data scale, decoding strategies, and feedback mechanisms. Despite strong results on benchmarks, challenges persist in generalization to unseen libraries and adherence to type and security constraints.\n\nNeural program synthesis generates code from natural language descriptions (Yin and Neubig, 2017). Type constraints reduce search space during decoding (Maddison and Tarlow, 2014). Unit tests provide feedback for iterative refinement (Chen et al., 2021). Retrieval augments models with relevant code snippets (Guo et al., 2022).\n\nWe investigate a constrained decoding approach that integrates static analysis with retrieval-guided prompts to enforce type safety and library usage policies.",
    "reason": "The span lists separate techniques and citations without transitions or explicit connections, making the relationship among them unclear and reducing coherence.",
    "start": 304,
    "end": 632,
    "label": "Coherence"
  },
  {
    "span": "The ReDial dataset remains the most widely used benchmark for conversational recommendation.",
    "document": "Introduction\n\nConversational recommender systems aim to elicit user preferences through natural dialogues and provide timely, personalized suggestions. Unlike static recommenders, these systems must manage uncertainty, ask informative questions, and incorporate feedback signals. Public datasets for conversational recommendation are limited and vary widely in dialogue quality, length, and annotation granularity, complicating fair comparison across models. The ReDial dataset remains the most widely used benchmark for conversational recommendation. However, its reliance on annotated movie mentions and templated flows can bias models toward entity spotting rather than genuine preference modeling. We propose a simulation framework and a new evaluation protocol that decouples entity resolution from conversational policy learning.\n",
    "reason": "The statement makes a popularity/prevalence claim about a specific dataset without any supporting citation (rule a and b).",
    "start": 459,
    "end": 551,
    "label": "Unsupported_claim"
  },
  {
    "span": "((Li et al., 2018))",
    "document": "Related Work\n\nSemi-supervised learning combines small labeled sets with large unlabeled corpora to improve generalization. Consistency regularization and pseudo-labeling are two dominant paradigms that have been adapted to NLP tasks ((Li et al., 2018)); Xie et al. (2020) extend these ideas with iterative self-training and noise-aware losses. In-domain data selection further reduces distribution mismatch when unlabeled pools are heterogeneous (Moore and Lewis, 2010). We build upon these techniques by introducing acquisition-aware consistency constraints tailored to incremental annotation.\n",
    "reason": "Redundant double parentheses around an author–year citation.",
    "start": 233,
    "end": 252,
    "label": "Format"
  },
  {
    "span": "BioBERT has consistently outperformed general-domain models on NER.",
    "document": "Related Work\n\nPre-trained language models specialized for the biomedical domain improve representation quality by exploiting in-domain corpora such as PubMed abstracts and PMC articles. Variants include domain-adaptive pre-training and from-scratch biomedical pre-training (Beltagy et al., 2019; Gu et al., 2020), which have shown gains on entity recognition, relation extraction, and question answering.\n\nBioBERT has consistently outperformed general-domain models on NER. Beyond pre-training, task-specific architectures leverage span-level classification, CRF decoders, and gazetteers to capture domain-specific patterns (Lample et al., 2016; Ma and Hovy, 2016). Recent directions integrate ontology constraints and distant supervision to mitigate annotation scarcity.\n\nWe extend domain-adaptive pre-training with cross-corpus curriculum and evaluate robustness to shifting entity guidelines across benchmarks.",
    "reason": "Claims about comparative performance of a specific prior model should be supported with citations to empirical studies (violates rule a and b).",
    "start": 406,
    "end": 473,
    "label": "Unsupported_claim"
  },
  {
    "span": "Neural Programmer-Interpreter learned execution traces (Reed and de Freitas, 2016). RobustFill targeted string transformations from examples (Devlin et al., 2017). DeepCoder used neural-guided search over DSLs (Balog et al., 2017). Codex demonstrated few-shot program generation (Chen et al., 2021).",
    "document": "Related Work\n\nProgram Synthesis from Natural Language and Examples\n\nProgram synthesis approaches vary by supervision (NL, I/O examples, traces), search procedures, and the expressivity of the target DSL or language. Recent advances mix learned priors with symbolic constraints to improve reliability.\n\nNeural Programmer-Interpreter learned execution traces (Reed and de Freitas, 2016). RobustFill targeted string transformations from examples (Devlin et al., 2017). DeepCoder used neural-guided search over DSLs (Balog et al., 2017). Codex demonstrated few-shot program generation (Chen et al., 2021).\n\nDespite progress, compositional generalization and specification ambiguity persist. We address these by disentangling semantic parsing from constraint satisfaction using a verifier-guided refinement loop.",
    "reason": "The span cites four works from different supervision paradigms and target languages but provides no transitions or explicit relationships. The sequence is list-like and abrupt, reducing coherence between sentences (criteria a and b).",
    "start": 302,
    "end": 601,
    "label": "Coherence"
  },
  {
    "span": "Classical navigation policies were built with model-free reinforcement learning using value-based methods and actor-critic architectures. Subsequent work incorporated auxiliary tasks, curriculum learning, and map-based memory to accelerate convergence and improve long-horizon performance. Sim-to-real transfer has been tackled via domain randomization, privileged information at training time, and sensor noise modeling.",
    "document": "Related Work\n\nLearning-based robot navigation targets goal-reaching and obstacle avoidance under partial observability and noisy sensing. Reinforcement learning approaches dominate due to their capacity to optimize long-horizon rewards directly from interactions or simulation.\n\nClassical navigation policies were built with model-free reinforcement learning using value-based methods and actor-critic architectures. Subsequent work incorporated auxiliary tasks, curriculum learning, and map-based memory to accelerate convergence and improve long-horizon performance. Sim-to-real transfer has been tackled via domain randomization, privileged information at training time, and sensor noise modeling.\n\nRecent benchmarks emphasize crowd navigation, dynamic obstacles, and multi-goal tasks. Evaluation protocols vary in sensing modalities, map complexity, and collision penalties.\n\nWe evaluate policies in a photorealistic simulator with dynamic agents and real-world deployments on a differential-drive platform.",
    "reason": "The span is a literature list that does not relate these methods to the authors' aims or identify which shortcomings their work intends to resolve.",
    "start": 279,
    "end": 700,
    "label": "Lacks_synthesis"
  },
  {
    "span": "the BioCreative V CDR dataset remains the de facto benchmark",
    "document": "Related Work\n\nBiomedical named entity recognition (BioNER) and relation extraction benefit from domain-specific pretraining and carefully curated ontologies (Lee et al., 2020; Gu et al., 2021). Span-level models with adaptive wordpiece alignment and character-level augmentation have narrowed the gap between general-domain and biomedical performance (Beltagy et al., 2019; Peng et al., 2019). Despite the availability of multiple corpora for chemicals, diseases, and genes, the BioCreative V CDR dataset remains the de facto benchmark for chemical–disease relation extraction.\n\nRecent advances incorporate distant supervision and contrastive pretraining from full-text articles, yielding gains on cross-corpus generalization (Yuan et al., 2022; Luo et al., 2021). We extend these findings by introducing ontology-guided negative sampling and evaluating robustness under concept drift across publication years.",
    "reason": "Claims a specific dataset is the 'de facto benchmark' without citing evidence; first mention of the dataset should be accompanied by a citation per rule (a).",
    "start": 475,
    "end": 535,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Johnson et al., 2018",
    "document": "Introduction\n\nFederated learning (FL) enables training models across a federation of client devices while keeping data local. Early work on on-device training (McMahan et al., 2017) introduced the FedAvg algorithm and demonstrated the feasibility of cross-device optimization at scale. Subsequent surveys (Kairouz et al., 2021) have summarized systems, privacy, and statistical challenges. Personalization and heterogeneity remain central problems, with approaches that adapt global models to local distributions, address client drift, and balance resource constraints across participants. A line of work studies personalization (Johnson et al., 2018 and heterogeneity (Smith et al., 2018). Practical deployments also require robust aggregation and adversarial resilience under Byzantine clients (Blanchard et al., 2017). Communication compression, secure aggregation, and partial participation are active research areas.\n\nOur work focuses on scalable personalization via prototype sharing. We build on prior methods for representation learning across clients and analyze convergence under non-IID sampling. We compare against baselines that incorporate proximal regularization and adaptive client weighting. We also discuss interpretability and fairness implications in cross-silo settings (Li et al., 2020). For evaluation, we consider datasets with varying participation rates and label skew, and we study the effects of client subsampling on convergence speed and final accuracy.\n\nRelated Work\n\nMeta-learning for federated personalization has been proposed to adapt shared representations to client-specific tasks (Fallah et al., 2020). Multi-task formulations balance global and local objectives to mitigate client drift (Smith et al., 2018). In addition, robust aggregation under adversarial behavior has been explored with coordinate-wise median and trimmed mean defenses (Blanchard et al., 2017).",
    "reason": "Missing closing parenthesis in the parenthetical citation; it should be '(Johnson et al., 2018)'.",
    "start": 629,
    "end": 650,
    "label": "Format"
  },
  {
    "span": "[Miller et al.]",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become the de facto approach for learning over relational data (Kipf and Welling, 2017; Hamilton et al., 2017). Despite their success, oversmoothing and oversquashing hinder performance on long-range dependencies (Alon and Yahav, 2021; Topping et al., 2022). Prior sampling methods reduce computation but may bias training toward frequent structures [Miller et al.]. We revisit sampling with a coverage-aware objective that balances exploration and efficiency (Chen et al., 2018; Zou et al., 2019).\n\nOur contributions include a scalable sampler with provable coverage guarantees and a stochastic training regime that preserves global signals while limiting variance.",
    "reason": "Uses square brackets and omits the year; in author–year styles this should be a parenthetical citation with a year, e.g., “(Miller et al., YEAR)”.",
    "start": 398,
    "end": 413,
    "label": "Format"
  },
  {
    "span": "Instance discrimination learns view-invariant features via contrast (Wu et al., 2018; He et al., 2020). Clustering-based methods assign codes to produce consistency (Caron et al., 2018; Caron et al., 2020). Masked image modeling reconstructs missing patches (Bao et al., 2022; He et al., 2022). Pretext tasks include rotation prediction (Gidaris et al., 2018).",
    "document": "Related Work\n\nSelf-supervised visual representation learning. Modern approaches replace labels with surrogate objectives that encourage invariances aligned with downstream tasks (Jing and Tian, 2020). Contrastive and non-contrastive methods have shown strong transfer across detection and segmentation benchmarks (Chen et al., 2020; Grill et al., 2020).\n\nInstance discrimination learns view-invariant features via contrast (Wu et al., 2018; He et al., 2020). Clustering-based methods assign codes to produce consistency (Caron et al., 2018; Caron et al., 2020). Masked image modeling reconstructs missing patches (Bao et al., 2022; He et al., 2022). Pretext tasks include rotation prediction (Gidaris et al., 2018). Our approach bridges masked image modeling with instance-level discrimination by aligning token reconstructions with instance prototypes.",
    "reason": "The span lists categories of self-supervised methods with citations but provides no transitions or explicit connections among them. The relationships are implied and the flow between sentences is abrupt, reducing coherence.",
    "start": 355,
    "end": 715,
    "label": "Coherence"
  },
  {
    "span": "Zadeh et al. (2018) introduce a large-scale multimodal sentiment dataset. Audio embeddings from self-supervised pretraining yield strong speech features (Baevski et al., 2020). Face alignment improves visual cues for expression analysis (Bulat and Tzimiropoulos, 2017). Fusion transformers learn cross-modal interactions (Tsai et al., 2019).",
    "document": "Related Work\n\nMultimodal emotion recognition integrates linguistic, acoustic, and visual cues to capture complementary signals under noisy and asynchronous conditions. Key challenges include robust temporal alignment, missing modalities, and generalization across speakers and recording environments.\n\nZadeh et al. (2018) introduce a large-scale multimodal sentiment dataset. Audio embeddings from self-supervised pretraining yield strong speech features (Baevski et al., 2020). Face alignment improves visual cues for expression analysis (Bulat and Tzimiropoulos, 2017). Fusion transformers learn cross-modal interactions (Tsai et al., 2019).\n\nOur contribution is a missing-modality robust fusion module that aligns streams with uncertainty-aware gating and distills cross-modal priors during training.",
    "reason": "The paragraph strings together four citations across dataset, audio embeddings, face alignment, and fusion models without transitions or explicit linkage, making the relationships between sentences unclear over multiple sentences.",
    "start": 302,
    "end": 643,
    "label": "Coherence"
  },
  {
    "span": "Wang et al., 2020)",
    "document": "Related Work\n\nNeural machine translation (NMT) has evolved from recurrent encoder–decoders to fully attention-based transformers (Bahdanau et al., 2014; Vaswani et al., 2017). Pre-training on monolingual and multilingual corpora significantly improves low-resource performance (Lample and Conneau, 2019; Liu et al., 2020). Data augmentation via back-translation and noising further enhances robustness (Sennrich et al., 2016; Edunov et al., 2018).\n\nDomain adaptation remains a key challenge due to lexical and stylistic shifts (Chu and Wang, 2018; Koehn and Knowles, 2017). Instance weighting, fine-tuning, and meta-learning have been explored extensively (Britz et al., 2017; Gu et al., 2020). Test-time adaptation methods update model statistics or parameters using target-side information without access to source data (Li et al., 2021; Sun et al., 2020).\n\nWe study source-free adaptation with constrained self-training that preserves adequacy while reducing exposure bias. Our approach complements prefix-tuning and lightweight adapters for efficient deployment (Li and Liang, 2021; Pfeiffer et al., 2020). Prior adequacy metrics rely on reference-based estimates, which can be unreliable under paraphrase-rich domains (Kocmi et al., 2021). Similar to Wang et al., 2020), we decouple fluency and adequacy during optimization, but we introduce a dynamic thresholding mechanism guided by uncertainty estimates.",
    "reason": "Missing opening parenthesis for a parenthetical citation; it should be '(Wang et al., 2020)'.",
    "start": 1256,
    "end": 1274,
    "label": "Format"
  },
  {
    "span": "Post-hoc explanation methods such as saliency maps, SHAP, and counterfactuals are widely adopted in healthcare models (Ribeiro et al., 2016; Lundberg and Lee, 2017; Sundararajan et al., 2017; Wachter et al., 2018; Ghassemi et al., 2021).",
    "document": "Introduction\nAs machine learning is integrated into clinical decision support, there is growing emphasis on transparency, accountability, and safety. However, aligning model explanations with clinician reasoning and workflow remains challenging.\nPost-hoc explanation methods such as saliency maps, SHAP, and counterfactuals are widely adopted in healthcare models (Ribeiro et al., 2016; Lundberg and Lee, 2017; Sundararajan et al., 2017; Wachter et al., 2018; Ghassemi et al., 2021). Prototype- and concept-based methods aim to increase interpretability by reasoning over human-understandable factors (Kim et al., 2018; Chen et al., 2019b; Yeh et al., 2020). Evaluation practices include faithfulness tests and clinician studies (Adebayo et al., 2018; Doshi-Velez and Kim, 2017).\nWe present a prospective study of explanation utility in triage, emphasizing actionability and temporal stability. Our framework measures whether explanations change care decisions under uncertainty and whether they remain consistent across shifts in covariates.",
    "reason": "The span enumerates methods and citations but does not connect them to the specific problems the paper tackles or articulate the gap/motivation, thus lacking synthesis.",
    "start": 246,
    "end": 483,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Smith et al.",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become a standard tool for learning over relational data. Since the seminal work of Kipf and Welling (2017), numerous architectures have been proposed to improve expressivity and scalability (Hamilton et al., 2017; Xu et al., 2019). Prior studies Smith et al. demonstrate that deeper message passing can capture long-range dependencies but often suffers from over-smoothing, motivating regularization techniques (Li et al., 2018; Oono and Suzuki, 2020). In this paper, we revisit depth versus width trade-offs in GNNs and propose a topology-aware normalization scheme that mitigates feature homogenization without incurring significant computational overhead.",
    "reason": "Narrative citation missing year; should be formatted as 'Smith et al. (YEAR)' or converted to a parenthetical citation.",
    "start": 295,
    "end": 307,
    "label": "Format"
  },
  {
    "span": "in a previous study, the authors claim that adversarial training eliminates exposure bias completely",
    "document": "Related Work\n\nNeural abstractive summarization has benefited from sequence-to-sequence models with attention and pretraining. A central issue in training such models is exposure bias: the discrepancy between training with teacher forcing and inference with autoregressive decoding. Various techniques have been proposed to mitigate exposure bias, including scheduled sampling and reinforcement learning.\n\nMore recently, self-critical sequence training and minimum risk training have been adopted to optimize sequence-level metrics. However, in a previous study, the authors claim that adversarial training eliminates exposure bias completely, which remains controversial in the community. Several follow-up works suggest that while adversarial objectives can improve robustness, they do not fully close the gap between training and inference conditions.\n\nOur work departs from these approaches by decoupling the objective from decoding dynamics through a consistency regularizer that penalizes discrepancy between teacher-forced and free-running distributions.",
    "reason": "Refers to a 'previous study' and attributes a specific claim without citing the study (definition a/b/eii).",
    "start": 541,
    "end": 641,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prompt-based learning for language models has been widely studied. Brown et al. (2020) introduced in-context learning with few-shot prompts. Gao et al. (2021) proposed cloze-style prompts with a verbalizer. Wei et al. (2022) demonstrated chain-of-thought prompting improves multi-step reasoning. Instruction tuning aligns models to follow task descriptions (Ouyang et al., 2022). Retrieval augmentation conditions generation on external documents (Lewis et al., 2020).",
    "document": "Related Work\n\nLarge language models have catalyzed rapid progress in task generalization with minimal supervision. Early work explored conditioning on demonstrations and natural language task descriptions, while subsequent approaches altered training objectives to better follow instructions. Despite these advances, it remains unclear how different prompting paradigms relate to each other in terms of mechanisms and limitations.\n\nPrompt-based learning for language models has been widely studied. Brown et al. (2020) introduced in-context learning with few-shot prompts. Gao et al. (2021) proposed cloze-style prompts with a verbalizer. Wei et al. (2022) demonstrated chain-of-thought prompting improves multi-step reasoning. Instruction tuning aligns models to follow task descriptions (Ouyang et al., 2022). Retrieval augmentation conditions generation on external documents (Lewis et al., 2020).\n\nIn this work, we compare prompting strategies under a unified evaluation protocol and analyze their sensitivity to prompt design and retrieval quality.",
    "reason": "The span lists multiple cited works in sequence without transitions or explicit explanation of how each method relates to the previous ones, creating abrupt shifts and unclear connections between the studies.",
    "start": 432,
    "end": 900,
    "label": "Coherence"
  },
  {
    "span": "Vatswani et al.",
    "document": "Introduction\n\nPretraining on multilingual corpora has reshaped how we approach low-resource parsing and tagging by enabling effective transfer through shared subword vocabularies and representations. Earlier work emphasized bilingual transfer with alignment features, but recent studies explore purely representation-based transfer that avoids external resources. Vatswani et al. argue that consistent task formulations and language-agnostic objectives can close much of the gap between high- and low-resource settings. Building on this line, we hypothesize that robust parameter sharing at the encoder and task-specific adapters can further stabilize cross-lingual performance.\n\nWe evaluate our framework on standard multilingual benchmarks and ablations across language families. Our contributions are: (1) a simple but strong multilingual encoder with frozen sublayers, (2) task adapters that reduce interference across languages, and (3) an analysis of transfer asymmetries driven by script and morphology. We situate our approach relative to prior multilingual NER and parsing systems and discuss how tokenization choices interact with transferability.",
    "reason": "Narrative citation is missing the publication year. In APA-style narrative form it should be 'Vatswani et al. (YEAR)', e.g., 'Vatswani et al. (2020)'.",
    "start": 364,
    "end": 379,
    "label": "Format"
  },
  {
    "span": "Zhang et al.",
    "document": "Introduction\n\nFederated learning (FL) has emerged as a promising paradigm for privacy-preserving analytics in healthcare (McMahan et al., 2017; Li et al., 2020). Cross-silo deployments allow institutions to collaboratively train models without sharing raw data (Kairouz et al., 2021). Building on the cross-silo setting, Zhang et al. proposed a personalized aggregation strategy to accommodate heterogeneous patient populations across hospitals. Subsequent work explores secure aggregation and differential privacy (Bonawitz et al., 2017; Geyer et al., 2017), while others address statistical heterogeneity via clustered optimization (Sattler et al., 2020) and meta-learning (Fallah et al., 2020). Nevertheless, practical constraints such as limited communication budgets and stragglers persist in real-world deployments.",
    "reason": "Narrative citation missing year: 'Zhang et al.' should include the publication year, e.g., 'Zhang et al. (2019)'.",
    "start": 321,
    "end": 333,
    "label": "Format"
  },
  {
    "span": "Keogh et al. (2005) formalized discord discovery for time series. Malhotra et al. (2015) applied LSTM autoencoders for reconstruction-based detection. Zenati et al. (2018) used adversarial training for detecting anomalies. Audibert et al. (2020) proposed USAD to balance reconstruction and discrimination.",
    "document": "Related Work\n\nTime-Series Anomaly Detection\n\nAnomaly detection spans distance-based methods, reconstruction-based neural models, and probabilistic forecasting approaches. The choice of method is influenced by stationarity, noise, and labeling scarcity. Evaluations often mix point and range anomalies, obscuring algorithmic trade-offs.\n\nKeogh et al. (2005) formalized discord discovery for time series. Malhotra et al. (2015) applied LSTM autoencoders for reconstruction-based detection. Zenati et al. (2018) used adversarial training for detecting anomalies. Audibert et al. (2020) proposed USAD to balance reconstruction and discrimination.\n\nWe introduce a calibration protocol that harmonizes scores across detectors and a range-aware metric that better reflects operational constraints.",
    "reason": "The span lists disparate techniques without connecting them or explaining their relationships; there are no transitions clarifying how discord discovery contrasts with neural approaches.",
    "start": 337,
    "end": 642,
    "label": "Coherence"
  },
  {
    "span": "Commonsense reasoning resources such as ConceptNet provide background facts for open-domain dialogue (Speer et al., 2017). Knowledge distillation compresses response selection models into smaller students (Hinton et al., 2015). Safety datasets annotate harmful prompts to filter toxic outputs (Dinan et al., 2019).",
    "document": "Related Work\n\nGrounded dialogue systems aim to generate contextually appropriate responses that leverage external knowledge while avoiding unsafe content. Prior work has integrated structured knowledge bases, unstructured passages, and retrieved snippets into both retrieval- and generation-based architectures.\n\nCommonsense reasoning resources such as ConceptNet provide background facts for open-domain dialogue (Speer et al., 2017). Knowledge distillation compresses response selection models into smaller students (Hinton et al., 2015). Safety datasets annotate harmful prompts to filter toxic outputs (Dinan et al., 2019). Retrieval-augmented models couple a retriever with a generator to condition responses on relevant evidence (Lewis et al., 2020), and recent safety-aware architectures incorporate risk estimators to gate outputs (Sun et al., 2022).\n\nOur approach unifies grounding and safety by jointly optimizing retrieval quality and toxicity risk under a multi-task objective, improving factuality while maintaining safe behavior.",
    "reason": "The sentences jump between commonsense grounding, model compression, and safety datasets without explaining their relationships or transitions, leaving the reader unsure how the works connect.",
    "start": 313,
    "end": 627,
    "label": "Coherence"
  },
  {
    "span": "[12]",
    "document": "Related Work\n\nHierarchical encoders for document classification capture sentence- and document-level semantics with stacked attention (Yang et al., 2016; Li and Qiu, 2020). Some works adopt hierarchical encoders [12] to improve cross-domain generalization by separating topic and style signals (Garcia and Tan, 2019). Others integrate pretraining with hierarchical pooling for better long-context modeling (Zhou and He, 2021). Our contribution combines hierarchical attention with sparse gating to scale to longer inputs without quadratic cost.",
    "reason": "Inconsistent citation style: numeric bracketed citation '[12]' in an author–year context; it should be replaced with an author–year citation, e.g., '(Author et al., Year)'.",
    "start": 212,
    "end": 216,
    "label": "Format"
  },
  {
    "span": "Curiosity-driven exploration rewards novelty (Pathak et al., 2017). Behavior regularization stabilizes offline RL (Kumar et al., 2020). Conservative value estimation reduces extrapolation error (Fujimoto et al., 2019).",
    "document": "Introduction\n\nExploration and stability remain central challenges in reinforcement learning (RL), particularly when data are scarce or collected offline. While a rich set of techniques exists, their connections are often discussed within separate subcommunities, obscuring shared principles.\n\nCuriosity-driven exploration rewards novelty (Pathak et al., 2017). Behavior regularization stabilizes offline RL (Kumar et al., 2020). Conservative value estimation reduces extrapolation error (Fujimoto et al., 2019). Ensemble critics can further reduce overestimation bias (An et al., 2021), and uncertainty-aware policies modulate risk under limited coverage (Ciosek et al., 2019). Despite this progress, practical guidance on selecting and combining these methods remains limited.\n\nWe present a unifying view based on density coverage and value pessimism, and we empirically map regimes in which exploration bonuses can be safely integrated into conservative offline learners.",
    "reason": "The span juxtaposes exploration (curiosity) with offline RL stabilization and conservative value estimation with no connective tissue. The relationships among these cited methods are not articulated, leading to poor coherence across multiple sentences.",
    "start": 293,
    "end": 511,
    "label": "Coherence"
  },
  {
    "span": "[Johnson, 2017)",
    "document": "Related Work\n\nInteractive retrieval systems integrate user feedback to refine ranking models over multiple iterations. Early methods relied on relevance feedback and query expansion to adjust term weights based on clicked or judged documents (Rocchio, 1971; Salton and Buckley, 1990). More recent bandit-based frameworks treat user interactions as stochastic rewards to optimize exploration–exploitation trade-offs. For evaluation, several studies emphasize the need for counterfactual estimators under logged interactions [Johnson, 2017) and advocate for simulation testbeds that vary user patience and noise (Agarwal et al., 2019; Joachims et al., 2017).",
    "reason": "Mismatched brackets in the citation; it should use matching parentheses, e.g., “(Johnson, 2017)”, or matching square brackets if that is the house style.",
    "start": 523,
    "end": 538,
    "label": "Format"
  },
  {
    "span": "There are many recent works that explore factuality correction in summarization.",
    "document": "Related Work\n\nNeural abstractive summarization systems can generate fluent but factually inconsistent outputs, prompting research into factuality detection and correction. Post-editing models, constrained decoding, and retrieval augmentation are among the strategies proposed to improve faithfulness. There are many recent works that explore factuality correction in summarization. However, evaluation remains challenging due to limited correlation between automatic factuality metrics and human judgments. We present a calibration-aware correction framework that couples evidence attribution with targeted edits, and we benchmark it under a unified human evaluation protocol.",
    "reason": "Vague reference to 'many recent works' without citing any specific papers (rule d).",
    "start": 301,
    "end": 381,
    "label": "Unsupported_claim"
  },
  {
    "span": "Over 70% of industrial RPA deployments fail within the first year.",
    "document": "Introduction\n\nAutomation technologies promise efficiency gains by reducing repetitive manual work, yet real-world deployments often encounter organizational and technical barriers. Process discovery from logs, predictive monitoring, and exception handling have been proposed to improve reliability. Over 70% of industrial RPA deployments fail within the first year. Understanding the causes of failure and designing resilient automation pipelines that incorporate human-in-the-loop oversight is therefore critical to realizing long-term benefits.",
    "reason": "Presents a precise statistic without any supporting citation or evidence (rule b/e).",
    "start": 299,
    "end": 365,
    "label": "Unsupported_claim"
  },
  {
    "span": "Transformers have been applied to medical image classification and segmentation with impressive results, leveraging self-attention to capture long-range dependencies (Dosovitskiy et al., 2021; Chen et al., 2021; Hatamizadeh et al., 2022). In this work, we present MedViT++, a transformer-based framework for volumetric organ segmentation.",
    "document": "Introduction\n\nMedical image segmentation is foundational for computer-assisted diagnosis and therapy planning. While convolutional neural networks (CNNs) have dominated the field, transformer-based architectures have recently shown promise in modeling global context across images and volumes. Transformers have been applied to medical image classification and segmentation with impressive results, leveraging self-attention to capture long-range dependencies (Dosovitskiy et al., 2021; Chen et al., 2021; Hatamizadeh et al., 2022). In this work, we present MedViT++, a transformer-based framework for volumetric organ segmentation.\n\nWe evaluate the proposed model across multiple public datasets and compare against strong CNN and hybrid baselines, focusing on accuracy, robustness to resolution changes, and inference efficiency.",
    "reason": "The span moves from a generic summary of prior work directly to announcing the authors' contribution without identifying a specific shortcoming or gap in those prior transformer approaches.",
    "start": 294,
    "end": 632,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Automated program repair has included search-based techniques with genetic algorithms (Weimer et al., 2009; Le Goues et al., 2012), constraint-based repairs using SMT (Nguyen et al., 2013; Mechtaev et al., 2015), template- and pattern-based fixes (Long and Rinard, 2016; Rolim et al., 2018), and learning-based approaches with neural code models (Tufano et al., 2019; Chen et al., 2019). Datasets such as Defects4J and Bugs.jar are commonly used (Just et al., 2014; Saha et al., 2018).",
    "document": "Introduction\n\nAutomated program repair (APR) aims to generate patches that restore intended behavior while preserving functionality. The field spans classical heuristic search and modern learning-based approaches.\n\nAutomated program repair has included search-based techniques with genetic algorithms (Weimer et al., 2009; Le Goues et al., 2012), constraint-based repairs using SMT (Nguyen et al., 2013; Mechtaev et al., 2015), template- and pattern-based fixes (Long and Rinard, 2016; Rolim et al., 2018), and learning-based approaches with neural code models (Tufano et al., 2019; Chen et al., 2019). Datasets such as Defects4J and Bugs.jar are commonly used (Just et al., 2014; Saha et al., 2018).\n\nDespite promising results, patch overfitting and limited generalization across projects persist. We present a counterfactual-consistency objective that penalizes spurious fixes by enforcing behavioral equivalence across semantically similar inputs.",
    "reason": "The span catalogs prior techniques and datasets but does not articulate how they connect to the present work’s motivation or what gap is targeted, matching (a) and (b).",
    "start": 215,
    "end": 700,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Kim, 2020)",
    "document": "Related Work\n\nReinforcement learning (RL) in continuous control has advanced through off-policy algorithms with function approximation (Lillicrap et al., 2016; Haarnoja et al., 2018). Distributional critics and entropy regularization improve stability and exploration (Bellemare et al., 2017; Nachum et al., 2018). Model-based RL leverages learned dynamics for sample efficiency (Janner et al., 2019; Wang and Ba, 2019). In meta-RL, context encoders rapidly adapt to new tasks (Rakelly et al., 2019). We build on implicit Q-learning and advantage-weighted regression, which reduce bootstrapping error and overfitting to the behavior policy (IQL; Kostrikov et al., 2021; Nair et al., 2020). Prior work shows conservative objectives mitigate extrapolation error as shown in Kim, 2020) but tuning conservatism remains challenging under sparse rewards.",
    "reason": "Missing opening parenthesis in a parenthetical citation; it should be '(Kim, 2020)'.",
    "start": 772,
    "end": 782,
    "label": "Format"
  },
  {
    "span": "BM25 has remained the strong lexical baseline across the TREC Deep Learning tracks.",
    "document": "Related Work\n\nNeural retrieval has progressed from representation-focused dual encoders to interaction-heavy cross encoders, often outperforming classical baselines in reranking settings (Karpukhin et al., 2020; Nogueira and Cho, 2019). However, first-stage retrieval still faces a trade-off between efficiency and accuracy, and hybrid systems combine lexical and dense signals for robustness (Zhan et al., 2021). BM25 has remained the strong lexical baseline across the TREC Deep Learning tracks. Recent work explores distillation from cross encoders to improve dense retrievers and more faithful negative sampling strategies (Hofstätter et al., 2021). We propose a segment-aware retriever that exploits discourse structure to improve recall for multi-faceted queries.",
    "reason": "References a specific competition/benchmark series (TREC Deep Learning tracks) and makes a comparative claim without citations.",
    "start": 414,
    "end": 497,
    "label": "Unsupported_claim"
  },
  {
    "span": "Xu et al. (2019) introduced a tensor fusion network to capture unimodal and bimodal interactions. Tsai et al. (2020) proposed multimodal transformers with cross-modal attention. User-level aggregation has also been explored (Hazarika et al., 2018). Sarafianos et al. (2017) studied person re-identification using visual attention.",
    "document": "Related Work\n\nMultimodal sentiment analysis aims to infer user affect by integrating text, audio, and visual streams collected from conversational or social media settings. Early work focused on feature-level fusion, while recent approaches have emphasized end-to-end learning with attention and transformers to align modalities under noisy and asynchronous conditions.\n\nXu et al. (2019) introduced a tensor fusion network to capture unimodal and bimodal interactions. Tsai et al. (2020) proposed multimodal transformers with cross-modal attention. User-level aggregation has also been explored (Hazarika et al., 2018). Sarafianos et al. (2017) studied person re-identification using visual attention.\n\nOur work differs in that we study calibration under missing modalities and propose a regularizer that encourages consistent predictions across observed subsets, complementing prior alignment and fusion strategies.",
    "reason": "The sentences list disparate studies without transitions or an explicit relationship; the final sentence abruptly shifts to person re-identification, making the connection to multimodal sentiment analysis unclear.",
    "start": 371,
    "end": 701,
    "label": "Coherence"
  },
  {
    "span": "Only a handful of papers have considered fairness in ASR for low-resource languages.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) systems have achieved near-human performance on some high-resource benchmarks, yet disparities persist across demographics and languages. Biases can arise from imbalanced training data, pronunciation variation, and dialectal diversity. Only a handful of papers have considered fairness in ASR for low-resource languages. In this work, we examine error disparities across speaker groups in three under-represented languages and explore mitigation strategies based on data augmentation and domain-adaptive pretraining.\n",
    "reason": "Asserts the scarcity of prior work without citing any surveys or representative studies.",
    "start": 285,
    "end": 369,
    "label": "Unsupported_claim"
  },
  {
    "span": "A comprehensive survey last year concluded that ViTs consistently surpass CNNs on chest X-rays.",
    "document": "Related Work\n\nMedical image classification has traditionally relied on convolutional neural networks (CNNs), but transformer-based architectures have shown promise in modeling long-range dependencies. Vision transformers (ViTs) can capture global context that may be relevant for subtle radiographic findings.\n\nA comprehensive survey last year concluded that ViTs consistently surpass CNNs on chest X-rays. Subsequent studies have explored hybrid CNN–Transformer backbones and token pruning to reduce inference costs while preserving accuracy.\n\nIn this paper, we benchmark ViTs across multiple chest radiograph datasets with harmonized preprocessing and report uncertainty-aware metrics to assess clinical reliability.",
    "reason": "Claims conclusions from a specific survey without citing the survey, including a strong comparative statement that requires evidence.",
    "start": 311,
    "end": 406,
    "label": "Unsupported_claim"
  },
  {
    "span": "Classical time series forecasting approaches include ARIMA and exponential smoothing (Box and Jenkins, 1970; Hyndman et al., 2008), while machine learning methods such as gradient boosting and random forests have been applied to capture nonlinearities (Chen and Guestrin, 2016; Prokhorenkova et al., 2018). Deep learning models comprise RNNs and LSTMs (Hochreiter and Schmidhuber, 1997; Laptev et al., 2017), temporal convolutional networks (Bai et al., 2018), attention mechanisms (Lin et al., 2017), and Transformers for long-horizon dependencies (Li et al., 2019; Wu et al., 2021; Zhou et al., 2021).",
    "document": "Related Work\n\nMultivariate time series forecasting spans statistical, machine learning, and deep learning paradigms, with increasing emphasis on capturing long contexts and handling diverse seasonalities and covariates.\n\nClassical time series forecasting approaches include ARIMA and exponential smoothing (Box and Jenkins, 1970; Hyndman et al., 2008), while machine learning methods such as gradient boosting and random forests have been applied to capture nonlinearities (Chen and Guestrin, 2016; Prokhorenkova et al., 2018). Deep learning models comprise RNNs and LSTMs (Hochreiter and Schmidhuber, 1997; Laptev et al., 2017), temporal convolutional networks (Bai et al., 2018), attention mechanisms (Lin et al., 2017), and Transformers for long-horizon dependencies (Li et al., 2019; Wu et al., 2021; Zhou et al., 2021).\n\nRecent work has further investigated decomposition-inspired architectures and frequency-domain modeling to disentangle trends and seasonality. Our study examines how multi-resolution attention can unify these perspectives and reduce horizon-specific degradation.",
    "reason": "The span only enumerates existing methods and citations without connecting them to the paper’s argument or clarifying what remains unsolved, triggering (a) and (b).",
    "start": 221,
    "end": 824,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There are many recent works that achieve over 90% accuracy on standard fake news benchmarks.",
    "document": "Introduction\n\nThe rapid spread of misinformation on social platforms has motivated a surge of research on automated fake news detection. Early approaches emphasized surface-level linguistic cues and simple propagation patterns, while later models integrated richer social context signals and user interaction features. Recent neural methods combine text encoders with graph-based modules to capture stance, credibility, and temporal diffusion. There are many recent works that achieve over 90% accuracy on standard fake news benchmarks. However, these results often depend on specific dataset splits, limited topic diversity, and potential annotation artifacts. In this paper, we revisit generalization in fake news detection by proposing cross-topic and cross-platform evaluation protocols alongside a robust, domain-adaptive training strategy. We focus on quantifying robustness to topic shift and user demographic variation, and we release standardized splits to facilitate reproducible comparisons.",
    "reason": "Mentions 'recent works' and a performance statistic without any supporting citations (rule d and a).",
    "start": 444,
    "end": 536,
    "label": "Unsupported_claim"
  },
  {
    "span": "Earlier works used KDD'99 and UNSW-NB15 almost exclusively for network anomaly detection benchmarks.",
    "document": "Related Work\n\nNetwork anomaly detection benchmarks shape modeling choices and reported progress. Earlier works used KDD'99 and UNSW-NB15 almost exclusively for network anomaly detection benchmarks. While these datasets enabled reproducibility, they have known artifacts, outdated traffic patterns, and limited attack diversity.\n\nRecent corpora introduce modern traffic captures, richer labels, and realistic background noise, yet they are often smaller, proprietary, or lack standardized splits. Methodologically, representation learning with flow-level features and temporal context has gained traction over traditional signature-based systems.\n\nWe contribute a unified evaluation protocol spanning multiple public datasets with harmonized label taxonomies, and we release scripts for robust cross-dataset validation.",
    "reason": "Claims specific historical usage of datasets without any citations.",
    "start": 97,
    "end": 197,
    "label": "Unsupported_claim"
  },
  {
    "span": "Thus, recent works have also explored enhancing pretrained models with external knowledge.",
    "document": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Schütze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n ",
    "start": 468,
    "end": 557,
    "label": "Unsupported_claim"
  },
  {
    "span": "COMET",
    "document": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Schütze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n ",
    "start": 993,
    "end": 997,
    "label": "Unsupported_claim"
  },
  {
    "span": "ConceptNet",
    "document": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Schütze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n ",
    "start": 1091,
    "end": 1100,
    "label": "Unsupported_claim"
  },
  {
    "span": "For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets.",
    "document": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Schütze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n ",
    "start": 1373,
    "end": 1506,
    "label": "Unsupported_claim"
  },
  {
    "span": "OHAMA 1 .",
    "document": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Schütze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n ",
    "start": 1705,
    "end": 1713,
    "label": "Format"
  },
  {
    "span": "For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet.",
    "document": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Schütze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n ",
    "start": 742,
    "end": 1101,
    "label": "Coherence"
  },
  {
    "span": "The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 .",
    "document": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Schütze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n ",
    "start": 1508,
    "end": 1713,
    "label": "Coherence"
  },
  {
    "span": "McMahan et al. (2017) introduced FedAvg to aggregate local updates across heterogeneous devices. Karimireddy et al. (2020) analyze client drift and propose SCAFFOLD to reduce variance in updates. Adaptive optimizers have been explored in distributed training (Reddi et al., 2020). Li et al. (2021) study personalization layers to capture user-specific patterns.",
    "document": "Related Work\n\nFederated learning enables collaborative model training without centralizing raw data. Early systems focused on communication-efficient aggregation and robustness under heterogeneity while balancing privacy and utility. We summarize strands on optimization algorithms and personalization techniques relevant to our study.\n\nMcMahan et al. (2017) introduced FedAvg to aggregate local updates across heterogeneous devices. Karimireddy et al. (2020) analyze client drift and propose SCAFFOLD to reduce variance in updates. Adaptive optimizers have been explored in distributed training (Reddi et al., 2020). Li et al. (2021) study personalization layers to capture user-specific patterns.\n\nBeyond optimization, privacy amplification by subsampling and secure aggregation have been widely explored to protect client updates, and systems work addresses stragglers, partial participation, and deployment constraints in mobile environments.",
    "reason": "The sentences enumerate four papers with no transitions or explicit links, leaving unclear how adaptive optimizers relate to client drift or why personalization follows aggregation; relationships are only implied across multiple sentences.",
    "start": 337,
    "end": 698,
    "label": "Coherence"
  },
  {
    "span": "Knowledge-graph-enhanced recommenders integrate entities via path-based regularization, graph convolutions, attention over neighbors, and reinforcement learning over multi-hop paths (Wang et al., 2018; Wang et al., 2019; Zhang et al., 2016; Sun et al., 2019; Xie et al., 2020).",
    "document": "Related Work\n\nRecommendation models increasingly leverage side information to alleviate sparsity and cold-start issues. Knowledge graphs (KGs) are particularly attractive because they encode multi-relational structure that can reveal latent user-item compatibilities beyond co-consumption signals.\n\nKnowledge-graph-enhanced recommenders integrate entities via path-based regularization, graph convolutions, attention over neighbors, and reinforcement learning over multi-hop paths (Wang et al., 2018; Wang et al., 2019; Zhang et al., 2016; Sun et al., 2019; Xie et al., 2020). Beyond explicit KG signals, recent work blends textual and visual modalities with KG embeddings to improve explainability and coverage.\n\nWe introduce a calibration-aware KG recommender that explicitly balances specificity and diversity by controlling relation-level exposure, enabling stable performance under shifting item catalogs.",
    "reason": "The span lists categories of KG recommenders without clarifying how they inform or contrast with the paper’s method, offering no synthesis or clear gap.",
    "start": 299,
    "end": 576,
    "label": "Lacks_synthesis"
  },
  {
    "span": "A growing body of work proposes redaction, differential privacy training, and filtering pipelines to mitigate leakage of personal data from language models (Carlini et al., 2021; Brown et al., 2022; Li et al., 2022; Yu et al., 2023).",
    "document": "Introduction\n\nAs large language models are integrated into user-facing applications, concerns about memorization and personal data leakage intensify. Practical safeguards must address both training-time and inference-time risks.\n\nA growing body of work proposes redaction, differential privacy training, and filtering pipelines to mitigate leakage of personal data from language models (Carlini et al., 2021; Brown et al., 2022; Li et al., 2022; Yu et al., 2023).\n\nHowever, existing defenses often assume static prompts and do not account for adversarial elicitation across multi-turn dialogues. We introduce a conversation-aware guardrail that tracks entity mentions over turns.",
    "reason": "Lists prior defenses without clarifying how they relate to the paper’s setting or specifying the gap within that sentence (criteria a and b).",
    "start": 230,
    "end": 463,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Nguyen, 2018 and Patel, 2019)",
    "document": "Related Work\n\nCausal inference with observational data requires assumptions to overcome confounding, selection bias, and measurement error (Pearl, 2009; Hernán and Robins, 2020). Representation learning approaches aim to balance covariates across treatment groups via adversarial or kernel methods (Johansson et al., 2016; Shalit et al., 2017). Heterogeneous treatment effect estimation combines meta-learners with tree-based or neural estimators to capture non-linear effect modifiers (Künzel et al., 2019; Nie and Wager, 2021). Several studies consider policy learning under budget constraints (Dudík et al., 2011; Kitagawa and Tetenov, 2018). Recent critiques highlight the fragility of identification under small violations of ignorability (Nguyen, 2018 and Patel, 2019), motivating sensitivity analyses and partial identification bounds. We contribute a robust objective that integrates sensitivity parameters directly into training.\n\nIntroduction\n\nWe evaluate on semi-synthetic datasets with known ground truth and real-world marketing interventions, reporting PEHE and policy value metrics.",
    "reason": "Incorrect conjunction inside a parenthetical citation. Multiple citations should be separated by a semicolon: \"(Nguyen, 2018; Patel, 2019)\".",
    "start": 744,
    "end": 774,
    "label": "Format"
  },
  {
    "span": "Prior exploration approaches include intrinsic motivation based on curiosity or prediction error, count-based bonuses, pseudo-count estimators, and optimism under uncertainty (Bellemare et al., 2016; Pathak et al., 2017; Burda et al., 2019; Tang et al., 2017; Osband et al., 2016).",
    "document": "Introduction\n\nExploration in reinforcement learning\n\nEffective exploration is essential in sparse-reward environments, where naive strategies like epsilon-greedy can lead to prohibitively slow learning. Numerous lines of work have proposed mechanisms to guide agents toward informative states and reduce sample complexity.\n\nPrior exploration approaches include intrinsic motivation based on curiosity or prediction error, count-based bonuses, pseudo-count estimators, and optimism under uncertainty (Bellemare et al., 2016; Pathak et al., 2017; Burda et al., 2019; Tang et al., 2017; Osband et al., 2016).\n\nChallenges in long-horizon tasks\n\nDespite progress, exploration remains challenging in long-horizon and partially observable settings, where deceptive rewards and stochastic transitions can mislead bonus-based strategies. Recent studies suggest combining structural priors with uncertainty quantification, but existing methods often incur high computational cost or suffer from miscalibration.\n\nOur focus\n\nWe aim to improve exploration efficiency under sparse rewards by leveraging compact state abstractions and temporally consistent uncertainty estimates.",
    "reason": "The span summarizes prior methods without linking them to the authors’ focus or explaining the shortcomings that motivate the new approach, satisfying lack of synthesis (criteria a and c).",
    "start": 324,
    "end": 605,
    "label": "Lacks_synthesis"
  },
  {
    "span": "A variety of domain adaptation techniques for neural machine translation have been explored, including fine-tuning on in-domain bitext, iterative back-translation, instance weighting, curriculum learning, and adapter-based parameter-efficient updates (Luong and Manning, 2015; Sennrich et al., 2016; Wang et al., 2017; Chu and Wang, 2018; Bapna and Firat, 2019).",
    "document": "Introduction\n\nNeural machine translation (NMT) systems often exhibit performance degradation when deployed in domains with stylistic, terminological, or structural shifts relative to their training distributions. Addressing domain mismatch is critical for real-world deployments, where data availability, latency, and memory constraints vary across applications.\n\nA variety of domain adaptation techniques for neural machine translation have been explored, including fine-tuning on in-domain bitext, iterative back-translation, instance weighting, curriculum learning, and adapter-based parameter-efficient updates (Luong and Manning, 2015; Sennrich et al., 2016; Wang et al., 2017; Chu and Wang, 2018; Bapna and Firat, 2019). While these methods have improved adaptation quality, practical pipelines still struggle to balance generalization with domain specificity under limited compute.\n\nWe present SparseMix, a lightweight mixture-of-experts adapter strategy for rapid domain switching. Our method learns sparse routing signals that remain stable across domains while activating domain-specific subspaces only when needed.\n",
    "reason": "The span summarizes prior domain adaptation strategies but does not relate them to the authors’ approach or identify the unresolved issue their method addresses; it lacks synthesis with the paper’s argument.",
    "start": 364,
    "end": 726,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Neural-guided search prunes the combinatorial space via learned heuristics (Neelakantan et al., 2016; Ellis et al., 2019). Type systems ensure program correctness by construction (Pierce, 2002; Polikarpova et al., 2016).",
    "document": "Related Work\n\nProgram synthesis approaches span neural sequence models, symbolic solvers, and neuro-symbolic hybrids. A central challenge is balancing expressivity with search efficiency while ensuring correctness with respect to specifications.\n\nNeural decoders trained on code corpora can map from examples or natural language to programs but often struggle with compositional generalization (Yin and Neubig, 2017; Austin et al., 2021). Constraint-based solvers guarantee correctness but require effective pruning to scale (Solar-Lezama, 2008; Alur et al., 2013). Neural-guided search prunes the combinatorial space via learned heuristics (Neelakantan et al., 2016; Ellis et al., 2019). Type systems ensure program correctness by construction (Pierce, 2002; Polikarpova et al., 2016). Recent work explores grammar-constrained decoding and repair mechanisms to enforce syntactic validity (Yin and Neubig, 2018; Lee et al., 2021).\n\nOur method integrates type-aware constraints into beam search with uncertainty estimates that prioritize branches likely to satisfy semantic checks early in the search.",
    "reason": "The span places neural-guided search next to type systems without articulating how these areas connect in synthesis pipelines, leaving the relationship implied and the transition missing.",
    "start": 566,
    "end": 786,
    "label": "Coherence"
  },
  {
    "span": "WMT19 News Translation shared task",
    "document": "Introduction\n\nDomain adaptation in neural machine translation (NMT) aims to maintain high translation quality when test data deviates from the training domain. Approaches such as back-translation, fine-tuning, and data selection have been shown to be effective in controlled settings. However, evaluations are often fragmented across domains and metrics, making it difficult to compare methods fairly. For evaluation, we follow the WMT19 News Translation shared task to benchmark performance on out-of-domain news sources and enable comparability across systems.\n\nRelated Work\n\nData-centric methods include iterative back-translation and domain-tagging, which can bias the model toward target domains. Model-centric methods introduce adapters or mixture-of-experts to condition on domain signals. Recent benchmarks emphasize robustness under distribution shift, yet a consistent protocol across domains remains elusive. In this paper, we unify these perspectives with a contrastive fine-tuning objective and report results on multiple held-out domains.",
    "reason": "Mentions a specific shared task without providing a citation at first mention, violating rule (a).",
    "start": 432,
    "end": 466,
    "label": "Unsupported_claim"
  },
  {
    "span": "A large body of work investigates privacy-preserving federated optimization, personalization, and secure aggregation in medical settings (McMahan et al., 2017; Konecny et al., 2018; Li et al., 2020; Bonawitz et al., 2019).",
    "document": "Related Work\n\nFederated Learning in Healthcare. Federated learning (FL) enables collaborative model training across institutions without centralizing patient data, potentially unlocking value in fragmented healthcare ecosystems. A large body of work investigates privacy-preserving federated optimization, personalization, and secure aggregation in medical settings (McMahan et al., 2017; Konecny et al., 2018; Li et al., 2020; Bonawitz et al., 2019). Prior efforts have also considered challenges such as label scarcity, device/data heterogeneity, and regulatory constraints, with demonstrations on imaging, EHR prediction, and mobile health scenarios.\n\nDomain Shift and Heterogeneity. Clinical data distributions vary markedly across sites due to demographics, acquisition protocols, and coding practices. Recent research examines methods for client-specific personalization, robust aggregation, and drift-aware training to mitigate cross-site disparities (Arivazhagan et al., 2019; Fallah et al., 2020; Collins et al., 2021).",
    "reason": "The span merely catalogs areas of prior work with citations and provides no explanation of how these works connect to the authors' problem setting or reveal a gap.",
    "start": 229,
    "end": 451,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Math Problem Understanding Math problem understanding task focuses on understanding the text, formulas and symbols in math domain. A surge of works aim to understand the math formulas for problem solving or mathematical information retrieval. In this way, the formula is usually transformed as a tree or graph (e.g., Operator Tree (Zanibbi and Blostein, 2012)), then network embedding methods Mansouri et al. (2019) and graph neural networkSong and Chen (2021) are utilized to encode it. ",
    "document": "Related Work\n\nMath Problem Understanding Math problem understanding task focuses on understanding the text, formulas and symbols in math domain. A surge of works aim to understand the math formulas for problem solving or mathematical information retrieval. In this way, the formula is usually transformed as a tree or graph (e.g., Operator Tree (Zanibbi and Blostein, 2012)), then network embedding methods Mansouri et al. (2019) and graph neural networkSong and Chen (2021) are utilized to encode it. Besides, a number of works focus on understanding math problem based on the textual information. Among them, Math Word Problem (MWP) Solving is a popular task that generates answers of math word problems. Numerous deep learning based methods have been proposed to tackle MWP, ranging from Seq2Seq (Chiang and Chen, 2019;Li et al., 2019), Seq2Tree Qin et al., 2020), to Pre-trained Language Models Liang et al., 2021). More recently, several works attempt to modeling more complex math problems (Huang et al., 2020;Hendrycks et al., 2021) that require to understand both textual and formula information.\n\n ",
    "start": 14,
    "end": 502,
    "label": "Coherence"
  },
  {
    "span": "Johnson et al., 2018)",
    "document": "Related Work\n\nTransformers have revolutionized sequence modeling by replacing recurrence with multi-head self-attention (Vaswani et al., 2017). Large-scale pretraining with masked or autoregressive objectives has further pushed state of the art across NLP tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019). Evidence suggests that deeper and wider models yield emergent capabilities, as shown in Johnson et al., 2018) and later scaling studies (Brown et al., 2020). Parameter-efficient finetuning methods reduce adaptation costs by modifying a small subset of weights (Houlsby et al., 2019; Lester et al., 2021).",
    "reason": "Missing opening parenthesis before the citation; should be (Johnson et al., 2018).",
    "start": 413,
    "end": 434,
    "label": "Format"
  },
  {
    "span": "Fusion architectures range from early concatenation to tensor fusion and transformer-based cross-modal attention (Zadeh et al., 2017; Tsai et al., 2019; Mai et al., 2020).",
    "document": "Related Work\n\nMultimodal sentiment analysis. Integrating text, audio, and visual signals can improve robustness over unimodal models. Fusion architectures range from early concatenation to tensor fusion and transformer-based cross-modal attention (Zadeh et al., 2017; Tsai et al., 2019; Mai et al., 2020). Pretraining strategies learn aligned multimodal representations from large-scale video corpora (Sun et al., 2019; Lei et al., 2021).\n\nFocus. We consider label-scarce settings with noisy, asynchronous modalities.\n\nOur method. We introduce a noise-aware alignment objective with uncertainty-weighted fusion to handle modality dropouts and desynchronization.",
    "reason": "The sentence catalogs prior fusion approaches without explaining their trade-offs or how they motivate the authors' design, thus not synthesizing prior work with the paper’s aims.",
    "start": 134,
    "end": 305,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Rubric-aligned scoring improves transparency (Brookhart, 2013). Argument mining identifies claims and evidence (Stab and Gurevych, 2014). Peer assessment increases student agency (Topping, 1998). Active learning reduces annotation burden (Settles, 2009).",
    "document": "Related Work\n\nAutomated feedback for student writing spans rubric design, discourse analysis, and human-AI collaboration. Systems must balance formative guidance with reliability while minimizing instructor workload. We synthesize prior findings across assessment, NLP, and pedagogy.\n\nRubric-aligned scoring improves transparency (Brookhart, 2013). Argument mining identifies claims and evidence (Stab and Gurevych, 2014). Peer assessment increases student agency (Topping, 1998). Active learning reduces annotation burden (Settles, 2009).\n\nHowever, current tools seldom connect discourse-aware feedback directly to rubric criteria with uncertainty estimates. Our approach maps mined argument structure to criterion-specific suggestions with confidence calibration.",
    "reason": "The span presents four disconnected claims across assessment, NLP, pedagogy, and ML sampling without transitions or an explicit explanation of how they relate, causing coherence issues.",
    "start": 285,
    "end": 539,
    "label": "Coherence"
  },
  {
    "span": "Isolation Forest isolates anomalies in feature space (Liu et al., 2008). LSTM autoencoders reconstruct normal patterns (Malhotra et al., 2016). Matrix profile detects discord subsequences (Yeh et al., 2016). Variational methods handle probabilistic uncertainty (An and Cho, 2015).",
    "document": "Introduction\n\nTime Series Anomaly Detection\n\nDetecting anomalies in time series underpins applications in monitoring, finance, and manufacturing. Methods vary in supervision level, scalability, and interpretability.\n\nClassical and Deep Methods\n\nIsolation Forest isolates anomalies in feature space (Liu et al., 2008). LSTM autoencoders reconstruct normal patterns (Malhotra et al., 2016). Matrix profile detects discord subsequences (Yeh et al., 2016). Variational methods handle probabilistic uncertainty (An and Cho, 2015).\n\nOperational Considerations\n\nStreaming constraints and concept drift complicate deployment (Gama et al., 2014). We propose a calibration-aware detector with adaptive thresholds.",
    "reason": "The cited works are listed without stating how they relate or transition from one approach to another, leaving the relationships implied and the connections abrupt.",
    "start": 245,
    "end": 525,
    "label": "Coherence"
  },
  {
    "span": "The SemEval-2020 commonsense validation shared task established accuracy as the primary metric.",
    "document": "Introduction Commonsense validation tasks probe a model’s ability to distinguish plausible from implausible statements and require both linguistic and world knowledge (Wang et al., 2019). Benchmarks often use minimally contrasting sentences to reduce spurious cues and emphasize reasoning (Talmor et al., 2019). The SemEval-2020 commonsense validation shared task established accuracy as the primary metric. Building on these setups, we introduce a multilingual variant with typologically diverse negatives.",
    "reason": "References a specific shared task and its evaluation protocol without citation to the task description, violating rule (a).",
    "start": 312,
    "end": 407,
    "label": "Unsupported_claim"
  },
  {
    "span": "There are many recent works that explore prompt-tuning for low-resource speech recognition.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) has witnessed rapid progress with the advent of large pretrained acoustic and language models. Nevertheless, performance in truly low-resource settings remains far from satisfactory due to data scarcity and domain mismatch. Inspired by developments in NLP, prompt-based and instruction-driven adaptation methods have been proposed to reuse general representations for task-specific objectives with minimal labeled data.\n\nThere are many recent works that explore prompt-tuning for low-resource speech recognition. However, the extent to which prompts can reliably steer acoustic-linguistic encoders without catastrophic forgetting is not well understood. In this paper, we propose a lightweight prompting interface over a frozen encoder-decoder ASR model and study its behavior across multiple languages and domains. We evaluate on cross-lingual transfer from high-resource English to target languages with under 10 hours of labeled data, and analyze robustness to noise and speaker variability.",
    "reason": "Mentions 'recent works' without providing citations to those works (definition d).",
    "start": 470,
    "end": 561,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Peters et al., 2018)",
    "document": "Introduction\n\nContextualized representations improved many downstream tasks. According to (Peters et al., 2018), deep bidirectional language models capture syntactic and semantic information beneficial for sequence labeling. Subsequent transformer architectures further advanced state-of-the-art results (Vaswani et al., 2017). We explore how these advances interact with active learning under annotation budgets.",
    "reason": "Wrong citation style after 'According to'; should be narrative 'According to Peters et al. (2018)'.",
    "start": 90,
    "end": 111,
    "label": "Format"
  },
  {
    "span": "Recent works have shown that contrastive pretraining substantially boosts robustness to distribution shift in vision-language models.",
    "document": "Related Work\n\nVision-language pretraining. Recent works have shown that contrastive pretraining substantially boosts robustness to distribution shift in vision-language models. Beyond contrast, alignment objectives and masked modeling have been combined to improve sample efficiency, while lightweight adapters enable parameter-efficient finetuning across tasks. However, most approaches require large-scale paired data, which is expensive to curate and often reflects social and geographic biases.\n\nCompositional generalization. A parallel line of research studies compositional reasoning to improve out-of-domain performance. Methods in this area introduce synthetic perturbations or structured decoders to factorize object, attribute, and relation predictions. Despite promising results, these methods commonly trade off accuracy on in-distribution samples.\n\nOur method bridges these lines by integrating a contrastive curriculum with compositional prompts to reduce reliance on spurious correlations while maintaining in-distribution accuracy.",
    "reason": "Uses the phrase “Recent works have shown…” without citing any specific studies (violates d).",
    "start": 43,
    "end": 176,
    "label": "Unsupported_claim"
  },
  {
    "span": "recent shared tasks have standardized end-to-end ASR evaluation for code-switching",
    "document": "Related Work\n\nCode-switching Automatic Speech Recognition (ASR) poses challenges due to interleaved phonotactics, language model interference, and data imbalance. Prior approaches explore tokenization schemes, dual decoders, and language-aware rescoring to mitigate error spikes near switch points. In parallel, recent shared tasks have standardized end-to-end ASR evaluation for code-switching, enabling more consistent comparisons across corpora and language pairs. Despite this progress, open-set generalization to unseen speaker demographics remains limited. We study multilingual pretraining with adaptive lexicon constraints to improve robustness under distribution shift.",
    "reason": "Claims the existence and impact of 'recent shared tasks' without citing them (rule a/d; first mention of shared tasks requires citations).",
    "start": 312,
    "end": 394,
    "label": "Unsupported_claim"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nPersonalized news recommendation balances relevance with diversity to avoid filter bubbles (Nguyen et al., 2014; Kunaver and Požrl, 2017). Contextual bandits provide an online framework for exploration–exploitation trade-offs (Li et al., 2010). We incorporate source credibility signals and user fatigue modeling to improve long-term engagement, instead of relying solely on click-through rates as in [12].\n\nRelated Work\n\nNeural recommenders with attention (Okura et al., 2017) and graph encoders (Wu et al., 2019) have advanced personalization but often ignore publisher quality; our method integrates both content and source features.",
    "reason": "Inconsistent citation style: numeric bracket “[12]” used in a document otherwise using author–year citations. It should be replaced with an author–year citation, e.g., “(Author Name, 2018)”.",
    "start": 415,
    "end": 419,
    "label": "Format"
  },
  {
    "span": "Backdoor adjustment estimates interventional effects under observed confounding (Pearl, 2009). Contrastive learning learns invariant features from augmentations (Chen et al., 2020). Propensity weighting rebalances samples to approximate randomized data (Rosenbaum and Rubin, 1983).",
    "document": "Related Work\n\nCausal approaches to robust text classification seek representations and training objectives that mitigate spurious correlations and confounding. Techniques span explicit adjustment, invariance learning, and reweighting to approximate interventional distributions.\n\nBackdoor adjustment estimates interventional effects under observed confounding (Pearl, 2009). Contrastive learning learns invariant features from augmentations (Chen et al., 2020). Propensity weighting rebalances samples to approximate randomized data (Rosenbaum and Rubin, 1983). Recent NLP work combines causal graphs with counterfactual data augmentation to reduce reliance on artifacts (Kaushik et al., 2020) and employs invariant risk minimization to generalize across domains (Arjovsky et al., 2019).\n\nWe propose a propensity-informed contrastive objective that aligns representations across counterfactual views while correcting for observable selection bias.",
    "reason": "The sequence lists three distinct techniques with no connective tissue or explicit rationale tying them together, leaving their relationships and progression unclear.",
    "start": 280,
    "end": 561,
    "label": "Coherence"
  },
  {
    "span": "the SemEval 2018 Task 1",
    "document": "Related Work\n\nMultimodal emotion recognition investigates the fusion of textual, acoustic, and visual modalities to infer affective states. Early approaches concatenated features from each modality, whereas recent methods learn cross-modal attention to capture complementary cues. Datasets vary in annotation granularity, including categorical emotions and dimensional valence-arousal.\n\nCommunity benchmarks have accelerated progress, notably the SemEval 2018 Task 1, which standardized evaluation on social media posts with emotion intensity labels. Models targeting this benchmark adopted transfer learning from large language models and incorporated lexicon priors to improve data efficiency. Our work extends these ideas by aligning prosodic features with token-level attention to better capture subtle expressions of emotion.",
    "reason": "A shared task (\"the SemEval 2018 Task 1\") is named at first mention without a citation to the task description paper or website (rule a).",
    "start": 443,
    "end": 466,
    "label": "Unsupported_claim"
  },
  {
    "span": "WMT14 English–German remains the most widely adopted benchmark for evaluating general-domain machine translation.",
    "document": "Related Work\n\nNeural machine translation (NMT) benchmarks have guided model development and comparison for nearly a decade. WMT14 English–German remains the most widely adopted benchmark for evaluating general-domain machine translation. Despite its popularity, recent work has highlighted saturation effects and domain biases, motivating exploration of more diverse test suites. Our study complements these efforts by introducing an evaluation protocol that stresses robustness to morphological variation and rare word usage.",
    "reason": "Claims widespread adoption of a specific benchmark without citing supporting surveys or benchmark papers (criteria a and b).",
    "start": 124,
    "end": 237,
    "label": "Unsupported_claim"
  },
  {
    "span": "the CoNLL-03 dataset remains the de facto benchmark for multilingual NER",
    "document": "Related Work\n\nNamed Entity Recognition (NER) has evolved from feature-engineered CRFs to neural architectures that leverage contextual embeddings. While transfer learning and multilingual pretraining have expanded coverage for low-resource languages, evaluation practice has not kept pace with these modeling advances. In particular, the CoNLL-03 dataset remains the de facto benchmark for multilingual NER, despite growing interest in cross-lingual robustness and domain shift. Recent studies argue for broader, more diverse evaluation suites that better capture entity variability and annotation ambiguity. Our work contributes a new evaluation protocol focusing on entity boundary calibration and low-frequency entities.",
    "reason": "Introduces and characterizes a specific dataset as a field-wide benchmark without citation at first mention (rule a; dataset claim needs support).",
    "start": 334,
    "end": 406,
    "label": "Unsupported_claim"
  },
  {
    "span": "Garcia et al. 2",
    "document": "Related Work\n\nQuestion Generation in Education Technology\n\nAutomated question generation (QG) has been applied to formative assessment and practice (Heilman and Smith, 2010; Duan et al., 2017). Curriculum-aligned QG emphasizes skill coverage and cognitive depth rather than surface fluency (Lindberg et al., 2013; Kurdi et al., 2020). Garcia et al. 2 proposed a taxonomy for aligning questions with learning objectives, though later work integrated Bloom-level prediction into the generation loop (Sachan and Xing, 2018; Yuan et al., 2017). Recent neural approaches condition on passage highlights or rationales to improve answerability (Chen et al., 2021; Scialom et al., 2019). Our work extends rationale-conditioned QG with a multi-objective loss to balance difficulty calibration and content validity.",
    "reason": "Wrong use of footnote-style number without a year; should include year or be formatted as a proper footnote/citation (e.g., 'Garcia et al. (YEAR)')).",
    "start": 335,
    "end": 350,
    "label": "Format"
  },
  {
    "span": "Group fairness notions such as demographic parity and equalized odds formalize constraints on predictions (Dwork et al., 2012; Hardt et al., 2016). Calibration requires predicted probabilities to reflect empirical frequencies across strata (Guo et al., 2017). Post-processing can adjust thresholds per group to mitigate disparities (Pleiss et al., 2017).",
    "document": "Introduction\n\nFairness in machine learning encompasses statistical definitions, causal reasoning, and practical mitigation strategies. While group metrics provide actionable targets, they may conflict with one another and with overall utility.\n\nRecent work has explored pre-, in-, and post-processing interventions to reduce disparities under distribution shift. Understanding the trade-offs between interpretability, stability, and performance is essential for deployment.\n\nGroup fairness notions such as demographic parity and equalized odds formalize constraints on predictions (Dwork et al., 2012; Hardt et al., 2016). Calibration requires predicted probabilities to reflect empirical frequencies across strata (Guo et al., 2017). Post-processing can adjust thresholds per group to mitigate disparities (Pleiss et al., 2017).\n\nThis paper studies calibration-preserving mitigation by jointly optimizing a proper scoring rule and disparity penalties. We provide guarantees under label shift and assess practical impact on risk-sensitive applications.",
    "reason": "The three sentences enumerate different fairness concepts without transitions or explicit relational statements, leaving the reader to infer how calibration or post-processing connects to group fairness metrics (a, b).",
    "start": 475,
    "end": 829,
    "label": "Coherence"
  },
  {
    "span": "Huang 2015",
    "document": "Related Work\n\nNeural sequence labeling approaches typically frame tagging as sequence modeling with conditional dependencies. CRF-based encoders achieved early success using handcrafted features (Lafferty et al., 2001). Neural models improved performance by combining recurrent encoders with character-level representations (Ma and Hovy, 2016) and later transformers (Devlin et al., 2019). Span-based and semi-Markov formulations further reduced label bias (Kong et al., 2016; Sarawagi and Cohen, 2004). The dominant baseline remains BiLSTM-CRF (Huang 2015; Lample et al., 2016), though pretraining has narrowed the gap. We revisit this baseline under low-resource transfer and noisy labels.",
    "reason": "Author–year citation missing comma and et al.; should be (Huang et al., 2015).",
    "start": 546,
    "end": 556,
    "label": "Format"
  },
  {
    "span": "Garcia et al. 1",
    "document": "Related Work\n\nConversational agents have been deployed to support mental health, language learning, and customer service, with design choices ranging from fully scripted flows to mixed-initiative dialogue (Fitzpatrick et al., 2017; Park et al., 2019). Safety and transparency are central concerns, as users are sensitive to errors in intent recognition and tone (Amershi et al., 2019). Garcia et al. 1 reported that proactive clarification can reduce user frustration in field deployments, but the study lacked long-term retention analysis. Follow-up work integrated uncertainty estimates into response selection to manage expectations (Kıcıman et al., 2020; Stumpf et al., 2021).\n\nBuilding on this literature, we present an incremental learning framework that uses post-dialogue feedback to refine intents while preserving safety constraints.",
    "reason": "Wrong use of footnote marker in place of a proper citation; should include the year (e.g., 'Garcia et al. (2020)') or be formatted as a proper footnote per style guidelines.",
    "start": 386,
    "end": 401,
    "label": "Format"
  },
  {
    "span": "[24]",
    "document": "Related Work\n\nIn continual learning, catastrophic forgetting has motivated regularization-based methods such as EWC (Kirkpatrick et al., 2017) and memory replay (Rolnick et al., 2019). Architecture-based strategies isolate parameters for each task (Rusu et al., 2016). As shown in [24], rehearsal-free methods can still retain prior knowledge under sparse updates. Our work revisits rehearsal with adaptive sampling to prioritize diverse experiences.",
    "reason": "Inconsistent citation style: numeric bracketed citation used in a section that otherwise uses author–date citations.",
    "start": 281,
    "end": 285,
    "label": "Format"
  },
  {
    "span": "Perez et al., (2020)",
    "document": "Related Work\n\nAdversarial data augmentation has been proposed to improve robustness against spurious correlations. According to Perez et al., (2020), generating counterfactual examples can expose shortcut features and promote causal generalization. Follow-up work shows benefits from invariance penalties across generated views (He & Liu, 2021) and from curriculum schedules that gradually increase perturbation strength (Singh et al., 2022).\n\nOur method complements these strategies with targeted edits guided by attribution maps to maximize coverage of suspected shortcuts.",
    "reason": "Comma incorrectly inserted before the year in a narrative citation; should be formatted as \"Perez et al. (2020)\" without the comma.",
    "start": 128,
    "end": 148,
    "label": "Format"
  },
  {
    "span": "The CoNLL-03 dataset contains exactly 1.4M tokens across four languages.",
    "document": "Introduction\n\nNamed entity recognition (NER) serves as a foundational task for information extraction pipelines. Benchmark datasets such as newswire and web corpora have standardized evaluation and fostered progress on multilingual generalization. Token-level statistics and label distributions are often used to assess domain coverage and to design sampling strategies for transfer learning.\n\nThe CoNLL-03 dataset contains exactly 1.4M tokens across four languages. Building on these resources, we investigate scaling laws for cross-lingual NER and explore curriculum schedules based on entity frequency and span length. We further evaluate robustness under noisy tokenization and sentence segmentation to mimic real-world preprocessing conditions.",
    "reason": "Presents a precise dataset statistic without any citation or source.",
    "start": 394,
    "end": 466,
    "label": "Unsupported_claim"
  },
  {
    "span": "Statistical downscaling maps coarse outputs to local scales (Maraun et al., 2010). Generative adversarial networks synthesize high-resolution fields (Grover et al., 2021). Bias correction shifts distributions toward observations (Teutschbein and Seibert, 2012). Ensemble methods quantify uncertainty (Tebaldi and Knutti, 2007).",
    "document": "Related Work\n\nProjecting climate signals to actionable local information requires methods that refine global model outputs while preserving physical consistency. Prior work covers statistical models, learning-based super-resolution, and uncertainty quantification. Our study focuses on physically constrained downscaling.\n\nStatistical downscaling maps coarse outputs to local scales (Maraun et al., 2010). Generative adversarial networks synthesize high-resolution fields (Grover et al., 2021). Bias correction shifts distributions toward observations (Teutschbein and Seibert, 2012). Ensemble methods quantify uncertainty (Tebaldi and Knutti, 2007).\n\nYet, few approaches jointly enforce conservation laws and calibrate extremes under nonstationarity. We present a hybrid model that couples bias-aware generators with physics-informed constraints.",
    "reason": "The span lists four strands of work without transitions or explanation of their interplay, leaving the relationships between the cited approaches implicit and unclear.",
    "start": 323,
    "end": 650,
    "label": "Coherence"
  },
  {
    "span": "Ganin et al. (2016) present domain-adversarial training to learn invariant features. Gururangan et al. (2020) show the benefits of domain-adaptive pretraining for BERT. Source-target divergence can be quantified with Wasserstein distance (Shen et al., 2018). Ruder and Plank (2018) explore multi-source transfer strategies.",
    "document": "Related Work\n\nUnsupervised domain adaptation in NLP aims to leverage labeled source data and unlabeled target data to build models that generalize across domains. We highlight adversarial learning, continued pretraining, divergence minimization, and multi-source transfer.\n\nGanin et al. (2016) present domain-adversarial training to learn invariant features. Gururangan et al. (2020) show the benefits of domain-adaptive pretraining for BERT. Source-target divergence can be quantified with Wasserstein distance (Shen et al., 2018). Ruder and Plank (2018) explore multi-source transfer strategies.\n\nWhile these directions are promising, there remains a gap in tuning adaptation intensity automatically based on target drift and resource constraints, which motivates our approach.",
    "reason": "Each sentence introduces a different strand (adversarial, pretraining, divergence measures, multi-source) with no connective explanation of how they relate; the relationships between the cited works are only implied, reducing coherence.",
    "start": 274,
    "end": 597,
    "label": "Coherence"
  },
  {
    "span": "Our problem setup follows the SemEval Entity Linking shared task.",
    "document": "Introduction\n\nEntity Linking maps mentions to entries in a knowledge base (Shen et al., 2015; Kolitsas et al., 2018). Modern systems combine span detection with candidate ranking using neural encoders (Yamada et al., 2020). Our problem setup follows the SemEval Entity Linking shared task. We focus on zero-shot entities by leveraging descriptions and graph structure (Logeswaran et al., 2019).",
    "reason": "Mentions a specific shared task (SemEval Entity Linking) without citing it at first mention.",
    "start": 224,
    "end": 289,
    "label": "Unsupported_claim"
  },
  {
    "span": "in (Garcia and Lee, 2018)",
    "document": "Related Work\n\nTransfer learning for neural machine translation (NMT) has been studied extensively (Luong and Manning, 2015; Zoph et al., 2016; Neubig and Hu, 2018). Multilingual pretraining enables parameter sharing across languages to benefit low-resource pairs (Aharoni et al., 2019; Conneau et al., 2020). Domain adaptation techniques typically rely on fine-tuning or instance weighting, as shown in (Garcia and Lee, 2018), while others leverage adversarial objectives to align feature distributions (Chu and Wang, 2018; Britz et al., 2017). Recent approaches also investigate meta-learning to rapidly adapt to new domains (Gu et al., 2020) and data selection strategies to mitigate distribution shift (Wang et al., 2017; van der Wees et al., 2017). Despite these advances, robust adaptation under extreme scarcity of parallel data remains challenging.",
    "reason": "Wrong citation style: the preposition 'in' before a parenthetical citation is incorrect. It should be narrative style: 'in Garcia and Lee (2018)' rather than 'in (Garcia and Lee, 2018)'.",
    "start": 400,
    "end": 425,
    "label": "Format"
  },
  {
    "span": "Our approach follows the standard UNet setup for polyp segmentation adopted by most recent works.",
    "document": "Related Work Medical image segmentation of colorectal polyps is crucial for early detection and intervention, with datasets collected from colonoscopy videos exhibiting large appearance variation (Bernal et al., 2015; Fan et al., 2020). Hybrid encoder–decoder networks with attention and multi-scale aggregation remain the dominant architecture class (Ronneberger et al., 2015; Oktay et al., 2018). Our approach follows the standard UNet setup for polyp segmentation adopted by most recent works. We further incorporate boundary-aware losses to improve delineation of small, flat lesions.",
    "reason": "Uses the phrase 'most recent works' to justify a setup without citing any of those works, violating rule (d).",
    "start": 399,
    "end": 496,
    "label": "Unsupported_claim"
  },
  {
    "span": "[27]",
    "document": "Introduction\n\nReinforcement learning methods such as Q-learning and policy gradients have demonstrated strong performance in control and games (Mnih et al., 2015; Schulman et al., 2017; Haarnoja et al., 2018). While model-free methods are sample-inefficient, model-based approaches can reduce environment interactions (Schrittwieser et al., 2020; Janner et al., 2019). As shown in [27], curriculum learning can further accelerate training by staging tasks of increasing difficulty. Our approach integrates model-based rollouts with automatic curriculum scheduling to improve data efficiency without sacrificing asymptotic performance.\n",
    "reason": "Inconsistent citation style: numeric bracket citation used in an author–year style paper. Replace with an author–year format, e.g., \"as shown by Narvekar et al. (2020)\" or a parenthetical \"(Narvekar et al., 2020)\".",
    "start": 381,
    "end": 385,
    "label": "Format"
  },
  {
    "span": "Transformer-based fusion aligns image regions and words via cross-attention to improve question answering (Tan and Bansal, 2019; Chen et al., 2020). Image captioning models generate textual descriptions from visual features (Vinyals et al., 2015; Anderson et al., 2018). Object detectors localize candidate regions (Ren et al., 2015; Carion et al., 2020).",
    "document": "Related Work\n\nMultimodal Transformers for Visual Question Answering\n\nEarly VQA systems relied on joint embedding spaces of CNN and RNN features with simple fusion modules (Antol et al., 2015; Fukui et al., 2016). Recent approaches adopt transformer architectures with cross-attention to capture fine-grained interactions between vision and language (Tan and Bansal, 2019; Lu et al., 2019).\n\nTransformer-based fusion aligns image regions and words via cross-attention to improve question answering (Tan and Bansal, 2019; Chen et al., 2020). Image captioning models generate textual descriptions from visual features (Vinyals et al., 2015; Anderson et al., 2018). Object detectors localize candidate regions (Ren et al., 2015; Carion et al., 2020).\n\nPretraining on large-scale image–text pairs further enhances downstream performance through masked modeling and contrastive alignment (Radford et al., 2021; Li et al., 2021). Nevertheless, the gap between pretraining objectives and compositional reasoning in VQA remains a significant challenge.",
    "reason": "The move from cross-attentional VQA models to separate tasks like image captioning and object detection lacks an explicit rationale or connective explanation, resulting in abrupt topic shifts across consecutive sentences.",
    "start": 391,
    "end": 746,
    "label": "Coherence"
  },
  {
    "span": "It is well known that message passing suffers from over-squashing in graphs with long-range dependencies.",
    "document": "Related Work\n\nGraph neural networks (GNNs) based on message passing aggregate information over local neighborhoods. While effective on homophilous graphs, these models face challenges on tasks requiring long-range reasoning, where signals must traverse many hops. Expressivity limits and training instabilities further constrain performance on large, sparse graphs.\n\nIt is well known that message passing suffers from over-squashing in graphs with long-range dependencies. Various architectural remedies have been proposed, including positional encodings, graph rewiring, and global attention mechanisms. Complementary approaches leverage diffusion processes, subgraph extraction, or multi-scale pooling to preserve informative paths while controlling computational cost.\n\nIntroduction\n\nWe propose a scalable GNN that couples sparse global routing with local message passing to mitigate information bottlenecks. Our analysis formalizes conditions under which routing alleviates path congestion without incurring quadratic complexity.",
    "reason": "States a niche, field-specific concept as 'well known' without citation; per rule b, such specific claims require supporting references.",
    "start": 367,
    "end": 472,
    "label": "Unsupported_claim"
  },
  {
    "span": "Sequence-level distillation, CTC-to-attention transfer, and logit-matching have all been explored for ASR to compress large teachers into efficient student models (Kim and Rush, 2016; Watanabe et al., 2017; Hinton et al., 2015).",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) has benefited from increasingly large models trained on vast amounts of audio-text pairs. However, deploying such models on device or in low-latency settings requires careful compression with minimal loss in accuracy.\n\nSequence-level distillation, CTC-to-attention transfer, and logit-matching have all been explored for ASR to compress large teachers into efficient student models (Kim and Rush, 2016; Watanabe et al., 2017; Hinton et al., 2015). Data augmentation and noise-robust training further improve generalization in far-field and noisy conditions (Ko et al., 2015; Park et al., 2019). Recent works integrate self-supervised pretraining to reduce labeled data needs and boost performance across domains (Baevski et al., 2020; Hsu et al., 2021).\n\nMulti-task objectives, incorporating phonetic or linguistic supervision, can regularize the acoustic model and improve robustness under domain shift (Seltzer et al., 2013; Toshniwal et al., 2017). Evaluation typically centers on word error rate, with calibration and confidence estimation receiving growing attention for downstream user experience.\n\nThese advances provide a roadmap for making ASR models both accurate and efficient.",
    "reason": "The span enumerates distillation strategies and citations without explaining how they inform the present work, what gap persists, or the authors’ stance on their limitations.",
    "start": 268,
    "end": 496,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Early multimodal emotion recognition relied on unimodal classifiers for audio, video, or text (Eyben et al., 2010; Kossaifi et al., 2017). Fusion strategies include early feature concatenation, late decision fusion, and tensor fusion (Zadeh et al., 2017; Atrey et al., 2010). Attention mechanisms have been used to weight modalities dynamically (Hazarika et al., 2018; Tsai et al., 2019).",
    "document": "Related Work\n\nMultimodal emotion recognition aims to infer affective states from combinations of speech, facial expressions, and language. Progress depends on effective representations, alignment across modalities, and robustness to missing or noisy signals. Datasets vary in size, annotation granularity, and context (acted vs. in-the-wild).\n\nEarly multimodal emotion recognition relied on unimodal classifiers for audio, video, or text (Eyben et al., 2010; Kossaifi et al., 2017). Fusion strategies include early feature concatenation, late decision fusion, and tensor fusion (Zadeh et al., 2017; Atrey et al., 2010). Attention mechanisms have been used to weight modalities dynamically (Hazarika et al., 2018; Tsai et al., 2019).\n\nRecent work explores cross-modal transformers and self-supervised pretraining from large-scale audiovisual corpora. However, handling asynchronous, partially missing modalities remains challenging. We introduce an alignment-free, stochastic routing mechanism that marginalizes over missing streams while preserving discriminative power.",
    "reason": "The span lists prior approaches and techniques but does not connect them to a specific shortcoming or to the authors’ perspective, leaving the motivation and gap implicit.",
    "start": 344,
    "end": 732,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Ronneberger et al. (2015) introduced U-Net for biomedical image segmentation. Oktay et al. (2018) proposed attention gates to improve focus on relevant structures. Zhou et al. (2018) presented UNet++ with nested skip connections. Chen et al. (2017) developed atrous spatial pyramid pooling for semantic segmentation.",
    "document": "Related Work\n\nMedical image segmentation leverages architectural priors and multi-scale context to delineate fine anatomical structures across modalities. Key challenges stem from limited annotations, scanner variability, and domain shift across institutions.\n\nRonneberger et al. (2015) introduced U-Net for biomedical image segmentation. Oktay et al. (2018) proposed attention gates to improve focus on relevant structures. Zhou et al. (2018) presented UNet++ with nested skip connections. Chen et al. (2017) developed atrous spatial pyramid pooling for semantic segmentation.\n\nSelf-supervised pretraining and test-time adaptation have emerged to mitigate annotation scarcity and domain shift (Tajbakhsh et al., 2020; Karani et al., 2021). Our method combines structure-aware augmentations with boundary-sensitive losses for cross-domain generalization.",
    "reason": "The span strings together U-Net variants and a general semantic segmentation module without describing how they connect or differ in relevance to medical imaging. The lack of transitions leads to abrupt shifts among citations.",
    "start": 261,
    "end": 577,
    "label": "Coherence"
  },
  {
    "span": "Character-based models consistently outperform subword models for Vietnamese",
    "document": "Introduction\n\nVietnamese NLP presents unique challenges due to diacritics, compounding, and tokenization conventions that treat syllables as whitespace-separated units (Nguyen et al., 2018). Prior work has explored word segmentation and POS tagging with CRFs and neural encoders (Nguyen and Nguyen, 2020; Vũ et al., 2021). Character-based models consistently outperform subword models for Vietnamese, suggesting that finer granularity is beneficial. Building on this hypothesis, we propose a hybrid character-subword encoder that adaptively pools multi-granular representations for sequence labeling tasks.",
    "reason": "This is a comparative claim about prior empirical findings but lacks citations to studies demonstrating the stated performance trend.",
    "start": 323,
    "end": 399,
    "label": "Unsupported_claim"
  },
  {
    "span": "Random network distillation provides novelty bonuses (Burda et al., 2019). Conservative Q-learning stabilizes policy improvement from static datasets (Kumar et al., 2020). Curiosity-driven approaches use prediction error as reward (Pathak et al., 2017).",
    "document": "Introduction\n\nReinforcement learning (RL) methods often struggle under sparse rewards and limited environmental feedback. Two prominent lines of work attempt to mitigate these issues: exploration strategies that encourage informative behaviors, and offline RL algorithms that learn from fixed datasets without additional environment interactions.\n\nRandom network distillation provides novelty bonuses (Burda et al., 2019). Conservative Q-learning stabilizes policy improvement from static datasets (Kumar et al., 2020). Curiosity-driven approaches use prediction error as reward (Pathak et al., 2017).\n\nDespite notable progress in both areas, it is not obvious how exploration signals should be incorporated into dataset collection for offline pipelines. Our approach formalizes a data-value criterion that bridges exploration and conservatism during batch construction.",
    "reason": "The span jumps between exploration (RND, curiosity) and offline RL (CQL) without transitions or an explicit relational thread, making the connection between these cited works unclear.",
    "start": 348,
    "end": 601,
    "label": "Coherence"
  },
  {
    "span": "WMT14 En-De is saturated at over 30 BLEU on newstest2014",
    "document": "Introduction\n\nNeural machine translation (NMT) has advanced rapidly with the advent of attention and transformer architectures. Improvements have come from deeper encoders, larger vocabularies, better regularization, and noisy student training. While scaling continues to push accuracy, practical concerns such as latency and memory footprint drive research into efficient NMT.\n\nBenchmarks guide progress and enable meaningful comparisons across systems. WMT14 En-De is saturated at over 30 BLEU on newstest2014, leading some to argue that incremental gains come at disproportionate computational cost. This motivates more challenging evaluation settings, including low-resource directions, domain adaptation, and robustness to noise.\n\nWe propose an efficient decoder that leverages grouped self-attention with sparsity-inducing priors. The design reduces quadratic attention costs while preserving contextual coverage. We further incorporate a length-aware training objective to ameliorate brevity penalties common in beam search.\n\nExperiments on standard and out-of-domain test sets show favorable accuracy-latency trade-offs. Additional analyses examine calibration, length ratios, and degradation under synthetic noise, providing a holistic view of translation quality beyond BLEU.",
    "reason": "States a specific performance saturation level on a standard benchmark without citing sources or empirical studies.",
    "start": 455,
    "end": 511,
    "label": "Unsupported_claim"
  },
  {
    "span": "Hardt et al. (2016) define equalized odds for classification. Chen et al. (2019) address missing data with learned imputation. Zech et al. (2018) observe confounders in chest X-ray models. Zhang et al. (2020) study bias in clinical note representations.",
    "document": "Introduction\n\nEnsuring fairness in clinical predictive modeling requires balancing performance across demographic groups while accounting for systemic biases in data collection, labeling, and deployment. Healthcare datasets combine structured records with images and notes, introducing multiple sources of bias.\n\nHardt et al. (2016) define equalized odds for classification. Chen et al. (2019) address missing data with learned imputation. Zech et al. (2018) observe confounders in chest X-ray models. Zhang et al. (2020) study bias in clinical note representations.\n\nWe propose a unified framework that learns group-robust representations via counterfactual data augmentation and fairness-regularized risk, applicable across modalities.\n",
    "reason": "The span abruptly juxtaposes fairness metrics, imputation, imaging confounders, and NLP bias without transitions or clarifying their relationships, leaving connections between the cited works implicit.",
    "start": 313,
    "end": 566,
    "label": "Coherence"
  },
  {
    "span": "A large body of work has studied message passing neural networks for molecules (Duvenaud et al., 2015; Kearnes et al., 2016; Gilmer et al., 2017; Yang et al., 2019). Extensions include attention-based architectures (Velickovic et al., 2018), hierarchical pooling (Ying et al., 2018; Gao and Ji, 2019), and pretraining strategies tailored to chemical graphs (Hu et al., 2019; Rong et al., 2020).",
    "document": "Introduction\n\nMolecular property prediction underpins applications in drug discovery, materials design, and toxicity screening. Learned representations on molecular graphs have largely replaced hand-crafted fingerprints by capturing local and long-range chemical interactions directly on the graph structure. However, challenges such as data scarcity, label noise, and distribution shift across chemical spaces remain obstacles to robust deployment.\n\nA large body of work has studied message passing neural networks for molecules (Duvenaud et al., 2015; Kearnes et al., 2016; Gilmer et al., 2017; Yang et al., 2019). Extensions include attention-based architectures (Velickovic et al., 2018), hierarchical pooling (Ying et al., 2018; Gao and Ji, 2019), and pretraining strategies tailored to chemical graphs (Hu et al., 2019; Rong et al., 2020).\n\nIn this paper, we investigate scaling graph encoders with adaptive depth selection to better capture task-dependent receptive fields. We evaluate across benchmarks spanning quantum properties, bioactivity, and ADMET endpoints, and study generalization under scaffold splits and cross-dataset transfer.\n\nOur contributions are: (1) an adaptive-depth message passing mechanism, (2) a training objective that regularizes depth selection under distribution shift, and (3) a comprehensive empirical study across 16 datasets. We release code and pretrained models to facilitate reproducibility.",
    "reason": "The span lists prior work and variants without linking them to the paper's motivation, gap, or approach, satisfying (a) and (c) of the definition.",
    "start": 451,
    "end": 845,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There are many recent works that explore session-based recommendation with self-attention.",
    "document": "Related Work\n\nSession-based recommender systems infer short-term intent from sequences of user interactions without persistent profiles. Early Markov models were eclipsed by recurrent networks, which in turn gave way to attention mechanisms that capture non-local dependencies. There are many recent works that explore session-based recommendation with self-attention. Beyond accuracy, research has examined latency, cold-start robustness, and counterfactual evaluation protocols.\n\nOur approach introduces a causal masking scheme aligned with practical serving constraints and integrates uncertainty estimates to abstain under distribution shift. We report improvements on multiple public e-commerce logs and provide ablation studies on attention sparsity and sequence length.",
    "reason": "Uses the phrase 'many recent works' without citing any representative studies, violating the requirement to cite prior work at first mention.",
    "start": 278,
    "end": 368,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Ono, 2020;)",
    "document": "Introduction\n\nCalibration of probabilistic classifiers is critical for decision making (Gupta and Raghavan, 2020). Temperature scaling is a simple yet effective post-hoc method (Menon and Liu, 2021), while Bayesian approaches incorporate uncertainty during training (Akhtar et al., 2019).\n\nRelated Work\n\nIn sequential decision settings, under-confidence can accumulate across steps (Ono, 2020;). Alternatives include ensemble distillation (Huang and Seo, 2022) and isotonic regression (Carter and Zhou, 2018). We analyze calibration under distribution shift and abstention.",
    "reason": "Extraneous semicolon inside the parenthetical citation. Should be '(Ono, 2020)'.",
    "start": 382,
    "end": 394,
    "label": "Format"
  },
  {
    "span": "(Siddhant and Lipton,, 2018)",
    "document": "Introduction\n\nWe consider Bayesian active learning for text classification. Dropout-based approximations are frequently used to estimate predictive uncertainty (Gal and Ghahramani, 2016). Despite their popularity, reproductions are mixed (Siddhant and Lipton,, 2018), with some studies finding little benefit over softmax confidence. We instead emphasize calibration-aware selection.",
    "reason": "Punctuation error within citation (double comma) violates citation formatting conventions.",
    "start": 238,
    "end": 266,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on essays written by non-native speakers.",
    "document": "Introduction\n\nAutomated essay scoring (AES) estimates writing quality using linguistic features and machine learning (Shermis and Burstein, 2013). Neural models have improved AES by leveraging contextual representations and multi-aspect supervision (Taghipour and Ng, 2016; Dong and Zhang, 2016). Pretrained language models further boost performance by capturing rich syntax and semantics (Devlin et al., 2019; Uto et al., 2020).\n\nDatasets such as ASAP and TOEFL11 provide diverse prompts and proficiency levels (Kaggle, 2012; Blanchard et al., 2013). Transfer across prompts and domains remains challenging due to topic leakage and genre variation. BERT was used in an AES task trained on essays written by non-native speakers. However, few studies explicitly address cross-prompt generalization with distribution shift in both prompts and proficiency.\n\nWe propose PromptMix, a prompt-conditional encoder with distributionally robust training that improves generalization to unseen prompts and proficiency levels.",
    "reason": "Describes a specific prior setup ('BERT was used in an AES task...') without any citation to the study that did this.",
    "start": 650,
    "end": 728,
    "label": "Unsupported_claim"
  },
  {
    "span": "Adversarial domain adaptation has been proposed to learn invariant representations (Lopez and Kumar, 2018; Patel et al., 2019). Back-translation augments low-resource domains with synthetic data (Chen et al., 2020). Pseudo-labeling selects high-confidence target examples for iterative refinement (Rao and Singh, 2021).",
    "document": "Related Work\n\nCross-domain generalization in natural language processing has been widely studied under the lens of domain adaptation, transfer learning, and data augmentation. Early approaches focused on feature-space regularization and instance reweighting to bridge source–target divergences (Nguyen and Fong, 2016; Blei and Chen, 2017). More recent efforts prioritize representation learning that suppresses domain-specific signals while preserving task-relevant information.\n\nAdversarial domain adaptation has been proposed to learn invariant representations (Lopez and Kumar, 2018; Patel et al., 2019). Back-translation augments low-resource domains with synthetic data (Chen et al., 2020). Pseudo-labeling selects high-confidence target examples for iterative refinement (Rao and Singh, 2021). In parallel, multi-source adaptation aggregates signals from multiple related domains (Garcia et al., 2020), and meta-learning prepares models for rapid adaptation with few target examples (Huang and Lin, 2021). Despite progress, the mechanisms by which these techniques complement each other remain unclear in noisy, heterogeneous targets.\n\nOur work studies how small, targeted perturbations at the token level interact with self-training objectives across domain shifts in sentiment classification and slot filling, emphasizing label drift and coverage gaps.",
    "reason": "Three consecutive sentences cite different techniques without transitions or explicit links to each other or to the preceding context; the relationship among adversarial learning, back-translation, and pseudo-labeling is implied but not stated, causing abrupt topic shifts across multiple sentences.",
    "start": 480,
    "end": 799,
    "label": "Coherence"
  },
  {
    "span": "There are many recent works that explore multimodal sarcasm detection.",
    "document": "Introduction\n\nSarcasm detection benefits from modeling incongruity between literal and intended meanings, and visual cues can surface implicit sentiment reversals in social media (Ghosh and Veale, 2016; Felbo et al., 2017). Multimodal models fuse text with images or emojis to capture pragmatic signals beyond lexical cues (Cai et al., 2019; Schifanella et al., 2016).\n\nThere are many recent works that explore multimodal sarcasm detection.\n\nWe propose a cross-modal contradiction objective that explicitly aligns textual sentiment with visual affect, improving robustness to domain shifts across communities.",
    "reason": "Uses the phrase 'many recent works' without listing or citing any of them; per guideline (d), such mentions require citations.",
    "start": 370,
    "end": 440,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Smith et al., 2020",
    "document": "Introduction\n\nUnsupervised domain adaptation seeks to transfer models trained on labeled source domains to unlabeled target domains. Adversarial alignment (Ganin et al., 2016) and divergence minimization (Long et al., 2015; Saito et al., 2018) are two prominent strategies. Recent studies emphasize class-conditional alignment to avoid negative transfer (Chen et al., 2019; (Smith et al., 2020 and incorporate self-training with confidence thresholds (Zou et al., 2019). Nevertheless, sensitivity to pseudo-label noise remains a central challenge.\n\nWe introduce a curriculum over target samples driven by uncertainty estimates, improving robustness while maintaining efficiency.",
    "reason": "Missing closing parenthesis in a parenthetical citation; it should be \"(Smith et al., 2020)\".",
    "start": 374,
    "end": 393,
    "label": "Format"
  },
  {
    "span": "Federated optimization has been studied with FedAvg and its variants (McMahan et al., 2017; Reddi et al., 2021), client sampling strategies (Nishio and Yonetani, 2019; Cho et al., 2022), personalization techniques (Arivazhagan et al., 2019; Dinh et al., 2020), and privacy mechanisms such as secure aggregation and differential privacy (Bonawitz et al., 2017; Kairouz et al., 2021). In this paper, we present FedHealth++, a method for cross-silo healthcare settings.",
    "document": "Related Work\n\nFederated learning enables institutions to collaboratively train models without centralizing sensitive data. Healthcare applications emphasize robustness to heterogeneous patient populations and strict privacy guarantees due to regulatory constraints.\n\nFederated optimization has been studied with FedAvg and its variants (McMahan et al., 2017; Reddi et al., 2021), client sampling strategies (Nishio and Yonetani, 2019; Cho et al., 2022), personalization techniques (Arivazhagan et al., 2019; Dinh et al., 2020), and privacy mechanisms such as secure aggregation and differential privacy (Bonawitz et al., 2017; Kairouz et al., 2021). In this paper, we present FedHealth++, a method for cross-silo healthcare settings.\n\nPrior work in medical federated learning considers domain shift across hospitals and device variation (Li et al., 2021; Sheller et al., 2020). We later evaluate on multi-center cohorts with label scarcity and long-tailed disease distributions.\n",
    "reason": "Violates (b): follows a summary of previous work immediately with the authors' contribution but does not explicitly highlight the gap or limitation their method addresses.",
    "start": 267,
    "end": 733,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Vatswani et al., 2019)",
    "document": "Introduction\n\nNeural machine translation has benefited from large-scale pretraining and data augmentation (Sennrich et al., 2016; Ng et al., 2019). In (Vatswani et al., 2019) we explore multilingual parameter sharing strategies that balance capacity and cross-lingual transfer; however, uniform sharing can underfit high-resource languages while overfitting low-resource ones. Subsequent research proposed mixture-of-experts routing to address this imbalance (Lepikhin et al., 2020; Fedus et al., 2021). We extend this line by introducing target-aware adapters that modulate representations conditioned on morphological complexity.",
    "reason": "Wrong citation style: narrative lead-in followed by a parenthetical; should be 'In Vatswani et al. (2019) we ...' or rephrased.",
    "start": 148,
    "end": 174,
    "label": "Format"
  },
  {
    "span": "Unsupervised time-series anomaly detectors include isolation-based methods (Liu et al., 2008), density and distance-based models (Breunig et al., 2000; Zhao et al., 2019), matrix profile techniques (Yeh et al., 2016), and reconstruction-based autoencoders (Zong et al., 2018). Transformer architectures have recently been adapted to capture long-range dependencies for detection (Xu et al., 2021; Zhou et al., 2021).",
    "document": "Introduction\n\nAnomaly detection in time series underpins monitoring for critical systems in finance, manufacturing, and IT operations. The problem is challenging due to nonstationarity, seasonal structure, mixed anomalies, and scarce labels.\n\nUnsupervised time-series anomaly detectors include isolation-based methods (Liu et al., 2008), density and distance-based models (Breunig et al., 2000; Zhao et al., 2019), matrix profile techniques (Yeh et al., 2016), and reconstruction-based autoencoders (Zong et al., 2018). Transformer architectures have recently been adapted to capture long-range dependencies for detection (Xu et al., 2021; Zhou et al., 2021).\n\nDespite these advances, practitioners report brittle performance under regime shifts and limited interpretability. We propose SegReg, a segmentation-aware detector with causal attribution that improves robustness to structural changes while providing token-level explanations. We validate on diverse public benchmarks and a large-scale AIOps dataset.",
    "reason": "The span lists families of anomaly detection methods without tying them to the paper's objectives, offering critique, or identifying a gap. It lacks synthesis per (a) and (c).",
    "start": 243,
    "end": 659,
    "label": "Lacks_synthesis"
  },
  {
    "span": "the SemEval 2017 Task 4 setup uses macro-averaged F1 as the official metric",
    "document": "Introduction\n\nAspect-based sentiment analysis (ABSA) focuses on identifying sentiments toward specific aspects mentioned in text. Neural models have advanced ABSA by leveraging attention mechanisms, dependency structures, and pretrained encoders. Despite progress, domain transfer and low-resource settings continue to pose challenges.\n\nBenchmarking ABSA systems commonly involves shared tasks that define datasets and evaluation criteria. For Twitter sentiment classification, the SemEval 2017 Task 4 setup uses macro-averaged F1 as the official metric, and subsequent work often adopts the same configuration to allow comparability. However, reproducibility issues arise due to varying preprocessing and label mappings.\n\nWe revisit these issues and propose a standardized pipeline that normalizes mentions, URLs, and emojis, and we release scripts to ensure consistent evaluation across studies.",
    "reason": "Mentions a specific shared task and its official metric without citing the task description or overview paper (definition a).",
    "start": 478,
    "end": 553,
    "label": "Unsupported_claim"
  },
  {
    "span": "A rich body of research studies multimodal fake news detection using image-text consistency, cross-modal attention, and knowledge graphs (Jin et al., 2017; Khattar et al., 2019; Wang et al., 2020; Singhal et al., 2022). Datasets such as Fakeddit, Weibo, and Twitter rumor corpora are widely used benchmarks (Nakamura et al., 2020; Ma et al., 2016; Zubiaga et al., 2018).",
    "document": "Related Work\n\nDetecting misinformation that pairs deceptive text with misleading imagery is a central challenge in online safety. Multimodal models promise to capture inconsistencies and cues that are unavailable to unimodal classifiers.\n\nA rich body of research studies multimodal fake news detection using image-text consistency, cross-modal attention, and knowledge graphs (Jin et al., 2017; Khattar et al., 2019; Wang et al., 2020; Singhal et al., 2022). Datasets such as Fakeddit, Weibo, and Twitter rumor corpora are widely used benchmarks (Nakamura et al., 2020; Ma et al., 2016; Zubiaga et al., 2018).\n\nWe develop a framework that aligns visual evidence and claims via structured entailment over extracted scene graphs, and we report results across multiple domains with controlled label shift.",
    "reason": "The span lists prior work and datasets but does not connect them to the current study or articulate a gap or motivation, thus lacking synthesis.",
    "start": 239,
    "end": 609,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Self-supervised pretraining on raw audio improves ASR in low-resource settings (Costa et al., 2020). End-to-end transducers unify acoustic and language modeling (He and Raman, 2019). Code-switching introduces mixed-language dependencies (Dias and Kumar, 2021).",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) has advanced rapidly with end-to-end neural models and large-scale pretraining. Challenges persist for noisy conditions, code-switching, and limited supervision.\n\nSelf-supervised pretraining on raw audio improves ASR in low-resource settings (Costa et al., 2020). End-to-end transducers unify acoustic and language modeling (He and Raman, 2019). Code-switching introduces mixed-language dependencies (Dias and Kumar, 2021). Multilingual training pools data across languages (Nair et al., 2020), but the interaction with phonotactic variation and domain mismatch is not well understood.\n\nOur study examines how multilingual self-supervision and language-aware decoders interact under code-switching and noise.",
    "reason": "The span enumerates different strands (self-supervision, transducers, code-switching) with citations but lacks transitions or an explicit explanation of how they relate, resulting in an abrupt sequence across multiple sentences.",
    "start": 212,
    "end": 472,
    "label": "Coherence"
  },
  {
    "span": "Transformer-based summarizers have consistently outperformed RNN baselines on CNN/DailyMail since 2019.",
    "document": "Related Work\n\nAbstractive Summarization. Sequence-to-sequence RNNs with attention laid the groundwork for neural summarization (Rush et al., 2015; See et al., 2017). Pre-trained transformers brought large gains by leveraging self-supervised objectives (Liu and Lapata, 2019; Raffel et al., 2020). Transformer-based summarizers have consistently outperformed RNN baselines on CNN/DailyMail since 2019. Recent work also studies factuality constraints and faithfulness (Maynez et al., 2020; Goyal and Durrett, 2021).",
    "reason": "Makes a comparative performance claim about a specific benchmark and timeframe without citations.",
    "start": 297,
    "end": 400,
    "label": "Unsupported_claim"
  },
  {
    "span": "We participated in the Kaggle RSNA pneumonia detection competition and ranked in the top 1%.",
    "document": "Related Work\n\nDeep learning for medical imaging has benefited from standardized benchmarks and public competitions that spur methodological innovation and reproducibility. Detection of pulmonary opacities in chest radiographs is particularly well studied due to its clinical importance and data availability. We participated in the Kaggle RSNA pneumonia detection competition and ranked in the top 1%. Lessons from these large-scale efforts motivate our current design, which emphasizes robust uncertainty estimation and clinician-aligned evaluation metrics beyond mean average precision.",
    "reason": "Claims participation and a specific ranking in a public competition without citing the competition or providing verifiable evidence.",
    "start": 309,
    "end": 401,
    "label": "Unsupported_claim"
  },
  {
    "span": "According to clinical guidelines, 78% of discharge summaries omit medication changes.",
    "document": "Introduction\n\nAccurate clinical summarization is critical for care continuity, billing, and patient safety. Discharge summaries distill key events from a hospitalization, including diagnoses, procedures, and medication changes. Yet, documentation quality varies widely across institutions, contributing to medication errors and readmissions. According to clinical guidelines, 78% of discharge summaries omit medication changes. While neural summarizers have improved extractive fidelity, they still hallucinate contraindicated therapies and omit crucial follow-up instructions. We propose a constraint-aware abstractive model that incorporates structured medication reconciliation signals to reduce omissions and factual inconsistencies.",
    "reason": "Reports a precise statistic attributed to 'clinical guidelines' without citation or evidence (rule b).",
    "start": 342,
    "end": 427,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Smith and Jones, 2020)",
    "document": "Related Work\n\nAdversarial training. Robust optimization with adversarial examples can improve worst-case performance but often degrades average-case accuracy. Methods such as TRADES and MART trade off robustness and natural accuracy via margin-aware losses and calibrated confidence. In multi-task settings, shared encoders exacerbate robustness transfer issues across tasks (Smith and Jones, 2020).\n\nCertified defenses. Interval bound propagation and randomized smoothing offer certificates under specific threat models, though scalability and tightness remain open challenges. We relate these techniques to our approach that leverages feature denoising and task-specific regularizers.",
    "reason": "Incorrect coordinator inside a parenthetical citation for APA style: use '&' in parenthetical citations. Correct form is '(Smith & Jones, 2020)'.",
    "start": 375,
    "end": 398,
    "label": "Format"
  },
  {
    "span": "the sandhi rules are universally agreed to be context-free",
    "document": "Related Work\n\nSanskrit morphological segmentation is complicated by sandhi phenomena that merge morpheme boundaries through phonological processes. Prior computational approaches range from finite-state analyzers to neural transduction with constraint augmentation.\n\nIn Sanskrit morphological segmentation, the sandhi rules are universally agreed to be context-free, which suggests that lightweight parsers should suffice for boundary recovery.\n\nWe revisit this assumption by constructing adversarial minimal pairs and show that incorporating limited context-dependent constraints improves accuracy on compound-heavy texts.",
    "reason": "This niche, domain-specific theoretical claim needs a citation to linguistic literature; stating universal agreement without evidence is unsupported (guideline b).",
    "start": 307,
    "end": 365,
    "label": "Unsupported_claim"
  },
  {
    "span": "there are many recent works that explore this topic",
    "document": "Introduction\n\nSession-based recommendation focuses on predicting the next item a user will interact with given only the sequence of items within the current session. Early neural approaches such as gated recurrent models demonstrated strong performance by modeling sequential dependencies (e.g., GRU-based methods). Graph-based techniques further improved upon this by explicitly capturing complex transition structures between items within a session.\n\nDespite these advances, there are many recent works that explore this topic, yet a unified understanding of how architectural choices trade off accuracy and efficiency remains elusive. In this paper, we revisit the design space of session-based recommenders and present a lightweight graph-transformer hybrid that achieves competitive performance with reduced computational cost.\n\nOur contributions are threefold: (1) we analyze the impact of graph sparsification on recommendation quality; (2) we propose a simple positional encoding tailored for short sessions; and (3) we provide a reproducible benchmark across diverse datasets and metrics.",
    "reason": "Missing citations for the mention of 'recent works' per rule (d); the claim about many recent works requires references.",
    "start": 477,
    "end": 528,
    "label": "Unsupported_claim"
  },
  {
    "span": "Graph anomaly detection methods include community-based statistics (Akoglu et al., 2015; Peel and Clauset, 2015), spectral residual analyses (Miller et al., 2010; Pasquarella and Rubin-Delanchy, 2020), embedding-based detectors using random walks or GNNs (Perozzi et al., 2014; Ding et al., 2019; Ding et al., 2021), and temporal point process models for dynamic outliers (Ranshous et al., 2015; Xu et al., 2020).",
    "document": "Related Work\n\nDetecting anomalous nodes and substructures in graphs is critical for fraud detection, security monitoring, and system reliability. Approaches vary by the assumed data-generating process and the type of anomalies of interest.\n\nGraph anomaly detection methods include community-based statistics (Akoglu et al., 2015; Peel and Clauset, 2015), spectral residual analyses (Miller et al., 2010; Pasquarella and Rubin-Delanchy, 2020), embedding-based detectors using random walks or GNNs (Perozzi et al., 2014; Ding et al., 2019; Ding et al., 2021), and temporal point process models for dynamic outliers (Ranshous et al., 2015; Xu et al., 2020). A complementary line explores contrastive learning to distinguish normal from abnormal patterns in self-supervised ways (You et al., 2020; Sun et al., 2021).\n\nOur work studies detection under feature-label mismatch, where node attributes drift while topology remains stable. We benchmark detectors under controlled drift scenarios.",
    "reason": "The span catalogs categories and citations without explaining how they compare, where they fall short, or how the present work fits among them; it lacks synthesis and author perspective.",
    "start": 241,
    "end": 654,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Federated optimization has been widely studied through algorithms such as FedAvg, which averages client updates on a central server, and FedProx, which stabilizes training with a proximal term. Personalization-oriented methods include pFedMe that leverages Moreau envelopes, Per-FedAvg that adapts meta-learning, and Ditto that decouples global and local objectives. Clustered FL partitions clients into subpopulations based on similarity, while multi-task FL jointly learns task-related parameters across clients. We adopt a lightweight personalization head on top of a shared encoder.",
    "document": "Related Work\n\nFederated learning enables model training across decentralized datasets under privacy and communication constraints. Beyond vanilla aggregation, a central challenge is coping with client heterogeneity due to non-identically distributed data, device variability, and diverse task requirements.\n\nFederated optimization has been widely studied through algorithms such as FedAvg, which averages client updates on a central server, and FedProx, which stabilizes training with a proximal term. Personalization-oriented methods include pFedMe that leverages Moreau envelopes, Per-FedAvg that adapts meta-learning, and Ditto that decouples global and local objectives. Clustered FL partitions clients into subpopulations based on similarity, while multi-task FL jointly learns task-related parameters across clients. We adopt a lightweight personalization head on top of a shared encoder.\n\nOur experiments consider image classification and keyword spotting across synthetic and real-world non-IID partitions, reporting accuracy and calibration under varying participation rates.",
    "reason": "The span lists several federated and personalization methods and then states the authors' choice without explaining why prior approaches are insufficient or how the chosen design addresses a clearly stated need.",
    "start": 308,
    "end": 894,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Ohana et al. 1",
    "document": "Related Work\n\nBiomedical ontologies frequently rely on semi-automated curation pipelines that blend rule-based mapping with human validation (Bodenreider, 2004; Funk et al., 2017). Similar curation strategies were used by Ohana et al. 1 in their release of a cross-lingual disease lexicon that emphasizes clinical abbreviations.\n\nDownstream, such resources power entity normalization and relation extraction for clinical narratives and EHR notes (Wang et al., 2018; Peng et al., 2019). We focus instead on weakly supervised alignment to reduce manual effort while maintaining high precision.",
    "reason": "Improper use of a footnote number in place of a year or proper citation formatting.",
    "start": 222,
    "end": 236,
    "label": "Format"
  },
  {
    "span": "In (Zhang et al., 2020)",
    "document": "Introduction\n\nTask-oriented dialogue systems require robust policy learning and state tracking. Data-efficient reinforcement learning has been proposed to reduce interaction costs with users (Su et al., 2016; Peng et al., 2018). In (Zhang et al., 2020) a constrained policy optimization method is presented to stabilize learning under sparse rewards, but the approach assumes accurate belief states.\n\nWe instead explore model-based planning with uncertainty calibration, showing improvements under noisy state estimates while maintaining sample efficiency.\n",
    "reason": "Wrong citation style: preposition placed outside a parenthetical citation; should be 'In Zhang et al. (2020)' or rephrased to avoid 'In ( ... )'.",
    "start": 229,
    "end": 252,
    "label": "Format"
  },
  {
    "span": "There have been many recent works exploring instruction tuning for multilingual QA.",
    "document": "Related Work\n\nInstruction tuning for QA. Instruction tuning has improved alignment of LLMs to downstream tasks in English-only settings (Ouyang et al., 2022; Chung et al., 2022). There have been many recent works exploring instruction tuning for multilingual QA. Parallel prompting approaches translate instructions and answers across languages while leveraging shared subword vocabularies (Asai et al., 2021; Riabi et al., 2022). In contrast, few-shot exemplars have been studied to mitigate cross-lingual drift (Winata et al., 2021).",
    "reason": "Mentions \"recent works\" without providing any citations to those works.",
    "start": 179,
    "end": 262,
    "label": "Unsupported_claim"
  },
  {
    "span": "Neighborhood aggregation captures higher-order connectivity (Hamilton et al., 2017). Session-based recommenders model short-term interests with recurrent units (Hidasi et al., 2016). Graph contrastive learning improves robustness to sparsity (You et al., 2020). Knowledge graphs provide side information for cold-start users (Wang et al., 2019).",
    "document": "Related Work\n\nRecommender systems increasingly rely on graph neural networks (GNNs) to exploit the relational structure of users, items, and contexts. The graph perspective facilitates multi-hop reasoning, alleviates data sparsity, and integrates heterogeneous side information. Nevertheless, challenges persist around dynamic interests, cold-start scenarios, and limited supervision.\n\nNeighborhood aggregation captures higher-order connectivity (Hamilton et al., 2017). Session-based recommenders model short-term interests with recurrent units (Hidasi et al., 2016). Graph contrastive learning improves robustness to sparsity (You et al., 2020). Knowledge graphs provide side information for cold-start users (Wang et al., 2019).\n\nWe build on these insights by unifying short-term session dynamics with longer-term multi-hop dependencies through a contrastive pretraining objective that aligns session subgraphs with interest prototypes.",
    "reason": "The span juxtaposes four separate approaches without explaining their relationships or providing transitions, leaving unclear how each cited work connects to the others.",
    "start": 386,
    "end": 731,
    "label": "Coherence"
  },
  {
    "span": "Recent work has shown that Transformers struggle with compositional generalization even on synthetic benchmarks.",
    "document": "Related Work\n\nSystematic generalization assesses whether models can combine known primitives to handle novel combinations, a core aspect of human language competence. Neural sequence models often appear to learn surface regularities that do not extrapolate compositionally. Evaluating this property requires controlled datasets and protocols that separate training and test combinations.\n\nRecent work has shown that Transformers struggle with compositional generalization even on synthetic benchmarks. To address this limitation, several approaches incorporate explicit structure via parsing, modularity, or intermediate supervision. Our approach instead regularizes attention patterns to encourage disentangled representations of primitives and relations.",
    "reason": "Asserts findings of 'recent work' without providing citations, despite this being a specialized claim (rules b, d).",
    "start": 389,
    "end": 501,
    "label": "Unsupported_claim"
  },
  {
    "span": "In healthcare federated learning, privacy-preserving training has been studied via secure aggregation (Bonawitz et al., 2017), homomorphic encryption (Aono et al., 2017), differential privacy (McMahan et al., 2018; Geyer et al., 2017), client-level clipping (Thakkar et al., 2019), and audit mechanisms (Nasr et al., 2019). Communication efficiency has been addressed with update compression (Konecny et al., 2016), partial participation (Li et al., 2020), and asynchronous protocols (Xie et al., 2019), while robustness to heterogeneity has been approached through personalized FL and proximal objectives (Deng et al., 2020; Li et al., 2020b).",
    "document": "Introduction\n\nFederated learning promises collaborative modeling across hospitals without centralizing patient data. However, clinical data are highly non-IID, privacy regulations are stringent, and participation is intermittent, complicating both optimization and deployment.\n\nIn healthcare federated learning, privacy-preserving training has been studied via secure aggregation (Bonawitz et al., 2017), homomorphic encryption (Aono et al., 2017), differential privacy (McMahan et al., 2018; Geyer et al., 2017), client-level clipping (Thakkar et al., 2019), and audit mechanisms (Nasr et al., 2019). Communication efficiency has been addressed with update compression (Konecny et al., 2016), partial participation (Li et al., 2020), and asynchronous protocols (Xie et al., 2019), while robustness to heterogeneity has been approached through personalized FL and proximal objectives (Deng et al., 2020; Li et al., 2020b).\n\nDespite these advances, end-to-end evaluations on real-world hospital networks remain limited. Prior studies often rely on simplified simulators or proxy datasets.\n\nWe present MedFed-Robust, a heterogeneous-aware FL method with calibrated privacy accounting and adaptive participation scheduling, and evaluate it across four health systems spanning EHR and imaging tasks.",
    "reason": "The span catalogs methods and areas (privacy, communication, heterogeneity) without articulating how they inform the paper’s approach or what specific shortcoming motivates the new method.",
    "start": 278,
    "end": 922,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Nguyen et. al., 2021)",
    "document": "Introduction\n\nAbstractive summarization has advanced with attention-based encoder–decoder models, copy mechanisms, and coverage penalties to mitigate repetition (Rush et al., 2015; See et al., 2017). A neural approach (Nguyen et. al., 2021) augments the copy mechanism with discourse-aware constraints, while denoising pretraining improves factuality and coherence in few-shot settings (Raffel et al., 2020; Lewis et al., 2020). Despite these gains, automatic metrics correlate imperfectly with human judgments, motivating improved evaluation frameworks (Lin, 2004; Kryscinski et al., 2019).",
    "reason": "Incorrect punctuation in “et al.”; written as “et. al.” instead of the correct “et al.”.",
    "start": 218,
    "end": 240,
    "label": "Format"
  },
  {
    "span": "[Clark et al., 2020)",
    "document": "Related Work\n\nModel predictive control (MPC) provides strong guarantees for trajectory tracking but can be computationally demanding on embedded platforms (Mayne et al., 2000; Wang and Boyd, 2010). Learning-based controllers amortize computation by approximating value functions or policies (Levine and Koltun, 2013; Amos et al., 2018). Safety filters enforce constraints during execution using control barrier functions [Clark et al., 2020) and robust optimization (Dean et al., 2020; Berkenkamp et al., 2017).\n\nWe propose a differentiable safety layer that composes with learned policies to ensure constraint satisfaction while preserving sample efficiency.",
    "reason": "Mismatched brackets in citation: opens with square bracket and closes with parenthesis; should consistently use parentheses “(Clark et al., 2020)”.",
    "start": 421,
    "end": 441,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors showed that curriculum learning reduces hallucination in sequence-to-sequence models.",
    "document": "Related Work\n\nCurriculum learning proposes to organize training examples from easy to hard to facilitate optimization (Bengio et al., 2009). In neural machine translation, techniques such as scheduled sampling attempt to mitigate exposure bias by modifying the sampling process (Bengio et al., 2015). In a previous study, the authors showed that curriculum learning reduces hallucination in sequence-to-sequence models. Beyond translation, curriculum strategies have been explored in summarization and data-to-text generation to control the complexity of inputs over training epochs. However, it remains unclear how to construct effective curricula when the notion of example difficulty is latent and task dependent. Our work examines proxy difficulty signals derived from source-side uncertainty and length constraints to build adaptive curricula for abstractive summarization.",
    "reason": "References a 'previous study' and its finding without citing the study (rules a and e-ii).",
    "start": 301,
    "end": 419,
    "label": "Unsupported_claim"
  },
  {
    "span": "Kahn et al. (2020) explored pseudo-labeling for ASR. Baevski et al. (2020) introduced wav2vec 2.0 for self-supervised speech representation learning. Xie et al. (2020) demonstrated Noisy Student for semi-supervised learning. Park et al. (2020) combined self-training with language model rescoring.",
    "document": "Related Work\n\nSemi-supervised learning for speech recognition leverages unlabeled audio through self-training, representation learning, and data augmentation. These advances reduce transcription costs while improving robustness.\n\nKahn et al. (2020) explored pseudo-labeling for ASR. Baevski et al. (2020) introduced wav2vec 2.0 for self-supervised speech representation learning. Xie et al. (2020) demonstrated Noisy Student for semi-supervised learning. Park et al. (2020) combined self-training with language model rescoring.\n\nWe focus on confidence-aware label refinement that adapts to domain shifts, complementing representation learning approaches but targeting the pseudo-label noise model.",
    "reason": "The span provides a sequence of citations without making explicit how these works are connected or differ, resulting in abrupt, minimally contextualized statements.",
    "start": 230,
    "end": 527,
    "label": "Coherence"
  },
  {
    "span": "There has been a surge of recent works on controllable summarization.",
    "document": "Introduction\n\nAbstractive summarization has advanced rapidly with the advent of large pretrained sequence-to-sequence models, enabling systems to produce fluent, human-like summaries. Early neural models focused on improving factuality and reducing repetition (See et al., 2017), while later work explored training objectives and pretraining strategies to capture discourse-level information (Liu and Lapata, 2019). Recent efforts also consider user- or task-specific constraints such as target length, entity focus, and stylistic preferences.\n\nThere has been a surge of recent works on controllable summarization. Despite this activity, many approaches conflate control signals at training and inference time, making it difficult to isolate causality between control inputs and output behavior. Moreover, evaluation practices vary widely across datasets and control types, complicating head-to-head comparisons.\n\nIn this paper, we introduce a framework that decouples content planning from surface realization, allowing explicit control over length and entity salience. We evaluate on news and scientific domains, providing both automatic and human assessments.",
    "reason": "Claims the existence of many recent works without citing any examples; mentions 'recent works' requires citations.",
    "start": 545,
    "end": 614,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Rahman and Lee, 2017)",
    "document": "Introduction\n\nData valuation techniques attempt to quantify each example's contribution to model performance (Ghorbani and Zou, 2019; Wang and Jia, 2020). According to (Rahman and Lee, 2017), Shapley-value approximations enable tractable estimation with sublinear sampling, and later work adapts these ideas for active learning (Kwon and Zou, 2021). However, most estimators assume IID data and struggle under covariate shift. We propose an importance-weighted estimator that remains consistent under distribution drift.",
    "reason": "Parenthetical citation used after 'According to' in narrative context; should be 'According to Rahman and Lee (2017)'.",
    "start": 168,
    "end": 190,
    "label": "Format"
  },
  {
    "span": "Two-stage detectors were adapted to aerial imagery (Li et al., 2017). One-stage approaches focused on speed for dense scenes (Redmon and Farhadi, 2018). Anchor-free detectors reduce design complexity (Zhou et al., 2019).",
    "document": "Related Work\n\nObject detection in remote sensing faces challenges such as small object size, arbitrary orientations, and crowded layouts. Both two-stage and single-stage paradigms have been explored, with specialized anchors, rotations, and multi-scale features.\n\nTwo-stage detectors were adapted to aerial imagery (Li et al., 2017). One-stage approaches focused on speed for dense scenes (Redmon and Farhadi, 2018). Anchor-free detectors reduce design complexity (Zhou et al., 2019).\n\nDespite progress, detecting tiny and overlapping instances remains difficult. We introduce an orientation-aware feature pyramid with dynamic receptive fields tailored to sub-pixel targets.",
    "reason": "The three sentences enumerate different detector families without transitions or an explicit explanation of their interrelationships, making the connections abrupt and unclear.",
    "start": 264,
    "end": 484,
    "label": "Coherence"
  },
  {
    "span": "Brown (2019))",
    "document": "Related Work\n\nExploration in reinforcement learning has been approached via intrinsic motivation, count-based bonuses, and posterior sampling (Bellemare et al., 2016; Osband et al., 2016). Building on count-based exploration, Brown (2019)) introduced a density-model-based estimator for high-dimensional observations, combining pseudo-counts with policy gradients. Other lines incorporate curiosity-driven signals based on prediction error or information gain (Pathak et al., 2017; Still and Precup, 2012). Recent advances in model-based RL have also improved sample efficiency through uncertainty-aware planning (Janner et al., 2019; Hafner et al., 2020).\n\nOur method unifies episodic memory with latent novelty to stabilize exploration in sparse-reward domains.",
    "reason": "Extra closing parenthesis in narrative citation: 'Brown (2019))' should be 'Brown (2019)'.",
    "start": 226,
    "end": 239,
    "label": "Format"
  },
  {
    "span": "Ying et al. (2018) proposed PinSage for web-scale recommendations over graphs. He et al. (2020) introduce LightGCN by simplifying propagation layers. Session-based models use gated units to capture short-term interests (Hidasi et al., 2016). Contrastive pretraining improves embeddings in sparse regimes (Wu et al., 2021).",
    "document": "Related Work\n\nGraph-based recommendation methods model interactions as nodes and edges to capture higher-order connectivity. We review representative graph convolution approaches, session modeling, and contrastive learning techniques in recommender systems.\n\nYing et al. (2018) proposed PinSage for web-scale recommendations over graphs. He et al. (2020) introduce LightGCN by simplifying propagation layers. Session-based models use gated units to capture short-term interests (Hidasi et al., 2016). Contrastive pretraining improves embeddings in sparse regimes (Wu et al., 2021).\n\nRecent works also explore dynamic graphs, side-information integration, and scalability via sampling and efficient negative mining, which are orthogonal to our focus on cold-start robustness.",
    "reason": "The cited works are listed back-to-back without showing how graph convolution, session modeling, and contrastive pretraining connect; there are no transitions or explicit relationships across the multiple sentences.",
    "start": 259,
    "end": 581,
    "label": "Coherence"
  },
  {
    "span": "[Miller et al., 2017]",
    "document": "Related Work\n\nDomain adaptation for sentiment analysis spans feature alignment, instance reweighting, and adversarial training (Ganin et al., 2016; Li et al., 2018). While early pivot-based approaches identified transferable lexical cues, adversarial encoders improved cross-domain robustness by minimizing domain distinguishability (Zhang et al., 2019). Recent meta-learning methods adapt quickly with few labeled examples (Finn et al., 2017; Riemer et al., 2019). Some studies [Miller et al., 2017] also stress the need for balanced target-domain validation to avoid selection bias.\n\nWe synthesize these strands by proposing a bi-level objective that couples representation invariance with risk-aware model selection.",
    "reason": "Wrong bracket style for author–year citation; should use parentheses '(Miller et al., 2017)' rather than square brackets.",
    "start": 479,
    "end": 500,
    "label": "Format"
  },
  {
    "span": "[Wang, 2015]",
    "document": "Related Work\n\nTemporal knowledge graph reasoning addresses the evolution of facts over time. Several datasets [Wang, 2015] standardize evaluation by annotating time-interval validity and ensuring disjoint train–test timestamps. Methodologically, Hawkes-inspired intensity models (Xu and Chen, 2018) and recurrent event embeddings (Li et al., 2019) are common baselines, with recent graph transformers improving long-range temporal dependencies (Zhang and Patel, 2022).",
    "reason": "Wrong bracket style for APA-like citations: should use parentheses “(Wang, 2015)” instead of square brackets.",
    "start": 110,
    "end": 122,
    "label": "Format"
  },
  {
    "span": "Most prior graph pooling methods rely on spectral heuristics.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have achieved state-of-the-art performance on molecular property prediction, recommendation, and program analysis. Pooling mechanisms coarsen graphs to capture hierarchical structure, trading off resolution for global context. Most prior graph pooling methods rely on spectral heuristics. Recent approaches propose differentiable node selection and structure learning to retain task-relevant subgraphs, yet may induce instability across random initializations. We focus on robust pooling via consensus subgraph discovery that aggregates multiple stochastic views, reducing variance while preserving discriminative signals.",
    "reason": "Generalized claim about prior work trends without citing supporting papers (rule b and e).",
    "start": 270,
    "end": 331,
    "label": "Unsupported_claim"
  },
  {
    "span": "Hinton et al. (2015) introduce knowledge distillation to transfer soft targets. Sequence-level training improves ASR with sMBR and MMI objectives (Povey et al., 2016). CTC enables alignment-free training (Graves et al., 2006). SpecAugment is an effective regularizer for end-to-end models (Park et al., 2019).",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) has evolved with architectures such as RNN-T, CTC/attention hybrids, and conformer encoders. Student–teacher training and data augmentation are commonly used to stabilize optimization and boost robustness under noisy or low-resource conditions.\n\nHinton et al. (2015) introduce knowledge distillation to transfer soft targets. Sequence-level training improves ASR with sMBR and MMI objectives (Povey et al., 2016). CTC enables alignment-free training (Graves et al., 2006). SpecAugment is an effective regularizer for end-to-end models (Park et al., 2019).\n\nUnlike prior methods that treat distillation and augmentation separately, we formulate a joint objective aligning sequence-level distributions under perturbations, improving teacher–student consistency.",
    "reason": "The span enumerates disparate ASR techniques—distillation, sequence-level criteria, CTC, and augmentation—without transitions or explicit connections, making the relationship among sentences unclear and abrupt across multiple sentences.",
    "start": 306,
    "end": 615,
    "label": "Coherence"
  },
  {
    "span": "[Garcia et al., 2019)",
    "document": "Introduction\n\nDomain generalization aims to learn models robust to distribution shift without target-domain supervision (Muandet et al., 2013; Gulrajani and Lopez-Paz, 2021). Prior work explores invariant risk minimization and data augmentation to stabilize features across contexts (Arjovsky et al., 2020). However, strategies for causal feature selection remain underexplored [Garcia et al., 2019) despite promising preliminary evidence in structured prediction (Zhang et al., 2021).\n",
    "reason": "Mismatched brackets/parentheses and inconsistent citation delimiter; should use matching parentheses and a consistent style, e.g., “(Garcia et al., 2019)”.",
    "start": 378,
    "end": 399,
    "label": "Format"
  },
  {
    "span": "Spatio-temporal graph convolutional networks capture dynamic dependencies in sensor graphs (Yu et al., 2018). Attention mechanisms weight neighboring nodes for adaptive aggregation (Velickovic et al., 2018). Anomaly detection identifies unusual traffic patterns during events (Chen et al., 2021).",
    "document": "Related Work\n\nTraffic forecasting models must capture spatial correlations across road networks and temporal dynamics of flow, speed, and occupancy. Graph neural networks combined with temporal modules have become standard baselines, with extensions that handle non-stationarity and external signals.\n\nSpatio-temporal graph convolutional networks capture dynamic dependencies in sensor graphs (Yu et al., 2018). Attention mechanisms weight neighboring nodes for adaptive aggregation (Velickovic et al., 2018). Anomaly detection identifies unusual traffic patterns during events (Chen et al., 2021). Further studies integrate weather and event features via exogenous encoders (Li et al., 2018) and learn adaptive adjacency to reflect time-varying connectivity (Wu et al., 2019).\n\nWe develop a regime-aware graph forecaster that switches parameters across detected traffic regimes, improving robustness to distribution shifts caused by incidents and holidays.",
    "reason": "The cited items move from forecasting architectures to general attention and then to anomaly detection without transitions or clarifying how they connect, resulting in poor coherence.",
    "start": 302,
    "end": 598,
    "label": "Coherence"
  },
  {
    "span": "A previous study showed that freezing the shared subword vocabulary improves zero-shot transfer across typologically distant languages.",
    "document": "Introduction\n\nMassively multilingual sequence-to-sequence models, such as mBART and mT5, enable zero- and few-shot transfer across languages by sharing parameters and subword vocabularies (Liu et al., 2020; Xue et al., 2021). Adapters and prefix-tuning have been proposed to specialize models to tasks while retaining multilingual generality (Houlsby et al., 2019; Lester et al., 2021; Pfeiffer et al., 2020).\n\nA previous study showed that freezing the shared subword vocabulary improves zero-shot transfer across typologically distant languages. Nevertheless, the mechanism by which vocabulary sharing affects transfer remains poorly understood.\n\nWe investigate vocabulary interventions and morphology-aware tokenization for low-resource transfer to agglutinative and Semitic languages.",
    "reason": "References a specific prior study and its finding without providing any citation (rule a).",
    "start": 411,
    "end": 546,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT has been widely applied to AES on ASAP using pairwise ranking objectives.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) systems have evolved from handcrafted features and linear models to end-to-end neural architectures that learn holistic representations of writing quality (Shermis and Burstein, 2013; Taghipour and Ng, 2016; Dong and Zhang, 2016). Attention-based models and hierarchical encoders capture discourse-level coherence and argument structure, improving agreement with human raters (Yang et al., 2017; Dong et al., 2017).\n\nLarge pretrained language models further boost AES performance by transferring knowledge from massive text corpora (Radford et al., 2019; Devlin et al., 2019). BERT has been widely applied to AES on ASAP using pairwise ranking objectives. Yet, overfitting to prompt-specific artifacts and limited interpretability remain obstacles to deployment in high-stakes educational settings.\n\nOur approach introduces rubric-guided contrastive learning to emphasize criterion-aligned evidence, improving generalization across prompts while yielding more transparent rationales.",
    "reason": "This sentence asserts widespread use of BERT with a specific training objective on a particular dataset but provides no citations to support the claim.",
    "start": 621,
    "end": 699,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent shared tasks on low-resource machine translation have highlighted the importance of back-translation for these pairs.",
    "document": "Related Work\n\nNeural machine translation (NMT) for low-resource languages confronts data scarcity, domain mismatch, and morphological complexity. Transfer learning, multilingual training, and data augmentation are standard strategies to improve generalization when parallel corpora are limited. Among augmentation methods, back-translation synthesizes source sentences from target monolingual data and has been widely adopted.\n\nCommunity evaluations and benchmarks play a key role in consolidating best practices. Recent shared tasks on low-resource machine translation have highlighted the importance of back-translation for these pairs. Concurrently, self-training and dual learning variants attempt to combine monolingual signals on both sides to stabilize training.\n\nIn this paper, we propose noise-aware back-translation that calibrates synthetic data quality using teacher uncertainty and domain discriminators. We show consistent gains across four low-resource language pairs and provide ablations on teacher quality, sampling strategies, and domain mixing ratios.",
    "reason": "Mentions 'recent shared tasks' to support a claim about method importance without citing specific tasks or reports, and thus lacks required citations (a, d).",
    "start": 514,
    "end": 638,
    "label": "Unsupported_claim"
  },
  {
    "span": "WER fails to correlate with human judgment in low-resource languages",
    "document": "Introduction\n\nWord error rate (WER) is the predominant metric for automatic speech recognition (ASR) evaluation due to its simplicity and interpretability (Morris et al., 2004). However, recent work has raised concerns about its sensitivity to semantic errors and its suitability across diverse domains (Karita et al., 2021; Chan et al., 2021). In particular, WER fails to correlate with human judgment in low-resource languages, where phonological and orthographic conventions differ markedly from English. To address these limitations, we introduce a semantic-distance-aware metric that incorporates pronunciation similarity and lexical semantics derived from multilingual embeddings.",
    "reason": "The claim about correlation between WER and human judgment in a specific setting lacks any citation to empirical studies.",
    "start": 360,
    "end": 428,
    "label": "Unsupported_claim"
  },
  {
    "span": "Unsupervised domain adaptation has been tackled with adversarial alignment, moment matching, and self-training (Ganin et al., 2016; Long et al., 2015; Zhang et al., 2019; Zou et al., 2019). Recent studies introduce transformers and vision-language pretraining to reduce domain gaps (Dosovitskiy et al., 2021; Jia et al., 2021).",
    "document": "Introduction\n\nDistribution shift between training and deployment data degrades model performance and reliability. Domain adaptation aims to mitigate this gap by leveraging unlabeled or weakly labeled target data to adjust a model trained on a labeled source domain.\n\nUnsupervised domain adaptation has been tackled with adversarial alignment, moment matching, and self-training (Ganin et al., 2016; Long et al., 2015; Zhang et al., 2019; Zou et al., 2019). Recent studies introduce transformers and vision-language pretraining to reduce domain gaps (Dosovitskiy et al., 2021; Jia et al., 2021).\n\nPractical applications in retail, healthcare, and robotics demand methods that are sample-efficient and stable under deployment constraints such as limited target labels and hardware budgets.",
    "reason": "Lists prior approaches and trends without connecting them to the authors’ approach or clarifying what limitation remains and how their work responds (definition a/c).",
    "start": 267,
    "end": 594,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Brown et al., 2020)",
    "document": "Introduction\n\nLarge-scale generative language models have demonstrated emergent capabilities across few-shot tasks, motivating research on efficient adaptation and safety. Early encoder–decoder architectures required task-specific finetuning, whereas instruction-following models enable more generalization with carefully designed prompts. Our approach investigates constrained decoding to improve factual grounding without sacrificing zero-shot flexibility, extending prior generative frameworks introduced by Brown et al., 2020) with controllable retrieval signals.\n\nWe present experiments on open-domain QA and data-to-text generation, analyzing the trade-offs between calibration and coverage. We also examine robustness to prompt paraphrases and input perturbations, highlighting sensitivity to retrieval ordering and truncation effects.",
    "reason": "Missing opening parenthesis for a parenthetical citation. It should be '(Brown et al., 2020)'.",
    "start": 511,
    "end": 530,
    "label": "Format"
  },
  {
    "span": "Catastrophic forgetting has been addressed via replay buffers, regularization on parameter changes, and dynamic architectures (Lopez-Paz and Ranzato, 2017; Kirkpatrick et al., 2017; Rebuffi et al., 2017; Aljundi et al., 2019; Yoon et al., 2018).",
    "document": "Introduction\n\nContinual learning aims to acquire new tasks without compromising performance on previously learned ones. Visual recognition under non-stationary task streams is particularly susceptible to interference due to overlapping features and limited capacity.\n\nBenchmarks typically evaluate on class- and domain-incremental protocols, but strong performance often depends on rehearsal memory, which may be infeasible for privacy or storage reasons.\n\nCatastrophic forgetting has been addressed via replay buffers, regularization on parameter changes, and dynamic architectures (Lopez-Paz and Ranzato, 2017; Kirkpatrick et al., 2017; Rebuffi et al., 2017; Aljundi et al., 2019; Yoon et al., 2018).\n\nWe introduce task-agnostic orthogonal modulation that constrains representational drift in a parameter-efficient manner, enabling strong no-rehearsal results while remaining compatible with light-weight rehearsal when permitted.",
    "reason": "The span lists solution families and citations without indicating their limitations relative to the authors' goals or how they inform the proposed approach; it lacks synthesis or an explicit motivation.",
    "start": 457,
    "end": 702,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Industry reports estimate that over 60% of mobile apps now contain on-device models.",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative training across decentralized devices without centralizing raw data, improving privacy and reducing communication of sensitive information. Mobile and edge deployments introduce additional constraints such as limited compute, intermittent connectivity, and non-iid data partitions. Personalization and resource adaptivity have therefore become central to practical FL algorithms.\n\nThe rapid proliferation of on-device inference has further increased interest in privacy-preserving training. Industry reports estimate that over 60% of mobile apps now contain on-device models. However, training across heterogeneous devices at scale remains challenging due to stragglers, model drift, and fairness concerns.\n\nWe present FedLattice, a resource-adaptive FL framework that jointly schedules clients and configures subnetwork architectures under communication and energy budgets. Our approach learns a discrete distribution over subnet choices per client and optimizes a global objective with fairness-aware regularization. We evaluate on vision and speech tasks with real device traces and demonstrate improvements in accuracy–efficiency trade-offs and participation fairness.",
    "reason": "Introduces a quantitative statistic attributed to 'industry reports' without citing any specific source, violating the need to support such claims (b).",
    "start": 551,
    "end": 635,
    "label": "Unsupported_claim"
  },
  {
    "span": "We evaluate on the widely used BioASQ and PubMedQA datasets.",
    "document": "Introduction\n\nBiomedical question answering (BioQA) requires models that can handle domain-specific terminology and long, evidence-rich passages (Lee et al., 2020). Pretrained language models adapted to biomedical corpora have improved performance on extractive and yes/no QA tasks (Beltagy et al., 2019). Despite these advances, limited training data and annotation variability remain obstacles to robust generalization.\n\nWe evaluate on the widely used BioASQ and PubMedQA datasets. To assess domain shift, we also perform zero-shot transfer from general-domain QA to biomedical tasks and analyze error types related to negation and speculation.\n\nRelated Work\n\nDomain adaptation for BioQA has explored continual pretraining and task-specific adapters (Gururangan et al., 2020; Pfeiffer et al., 2020).",
    "reason": "Introduces specific datasets (BioASQ and PubMedQA) without providing citations at their first mention, which are required for dataset references.",
    "start": 423,
    "end": 483,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith and Rao (2016))",
    "document": "Introduction\n\nAbstractive summarization systems increasingly rely on pretrained encoder–decoder backbones augmented with coverage and length controls (See and Manning, 2017; Fan and Lewis, 2018). Following Smith and Rao (2016)) we incorporate discourse-aware constraints to better preserve entity coherence. Unlike prior methods that require domain-specific post-editing (Hsu and Wu, 2019), our approach integrates constraints directly in decoding via constrained beam search (Anderson and Ferraro, 2020).",
    "reason": "Extra closing parenthesis after the narrative citation; it should be 'Smith and Rao (2016)'.",
    "start": 206,
    "end": 227,
    "label": "Format"
  },
  {
    "span": "[Kim and Park, 2018)",
    "document": "Related Work\n\nTime-series forecasting has benefited from hybrid models that combine statistical priors with deep sequence architectures (Salinas et al., 2020; Sen et al., 2019). For multivariate signals, attention mechanisms capture cross-series dependencies and exogenous effects. Prior work [Kim and Park, 2018) proposes temporal convolutions with dilations to handle long-range patterns efficiently, while probabilistic decoders improve calibration (Wen et al., 2017). However, aligning seasonality across irregular series is still challenging.\n",
    "reason": "Mismatched brackets in the citation; uses '[' to open and ')' to close. Should consistently use parentheses '(Kim and Park, 2018)' or brackets '[Kim and Park, 2018]'.",
    "start": 293,
    "end": 313,
    "label": "Format"
  },
  {
    "span": "Robotic manipulation with reinforcement learning has explored off-policy algorithms, vision-based policies, and demonstration-augmented training (Levine et al., 2016; Kalashnikov et al., 2018; Radosavovic et al., 2020). Sim-to-real transfer leverages domain randomization and privileged state information (Tobin et al., 2017; Peng et al., 2018).",
    "document": "Related Work\n\nLearning general-purpose manipulation skills requires data-efficient policies that handle diverse objects, contact dynamics, and partial observability. Reinforcement learning (RL) has shown promise but suffers from sample inefficiency and brittleness when deployed on real hardware.\n\nRobotic manipulation with reinforcement learning has explored off-policy algorithms, vision-based policies, and demonstration-augmented training (Levine et al., 2016; Kalashnikov et al., 2018; Radosavovic et al., 2020). Sim-to-real transfer leverages domain randomization and privileged state information (Tobin et al., 2017; Peng et al., 2018).\n\nWe propose an uncertainty-aware action chunking framework that reduces decision frequency while preserving reactivity, combined with conservative value regularization, resulting in faster real-world learning on multi-task manipulation suites.",
    "reason": "The span lists areas of prior work without connecting them to a concrete limitation or explaining how they inform the proposed framework; it lacks synthesis and author perspective.",
    "start": 298,
    "end": 643,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The SemEval 2017 Task 10 defines the standard evaluation for keyphrase extraction.",
    "document": "Related Work\n\nKeyphrase extraction benchmarks. Automatic keyphrase extraction has been evaluated on a range of scientific and news corpora with differing domain coverage, document length, and annotation protocols. The SemEval 2017 Task 10 defines the standard evaluation for keyphrase extraction. Despite the prevalence of this benchmark, reported metrics vary depending on stemming, matching rules, and whether author- or reader-assigned keyphrases are used.\n\nUnsupervised and supervised approaches. Early unsupervised methods relied on graph centrality and positional heuristics, while supervised neural models introduced contextual encoders, sequence tagging formulations, and copy mechanisms. Recent advances focus on domain adaptation and low-resource transfer through distant supervision and constrained decoding.\n\nEvaluation challenges. Agreement between annotators remains limited, particularly for multi-word expressions and paraphrases. Moreover, strict matching can understate semantic overlap, motivating soft alignment and embedding-based evaluation protocols.",
    "reason": "The sentence mentions a specific shared task/dataset as a standard benchmark but does not provide a citation to the task description.",
    "start": 214,
    "end": 296,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most current self-supervised visual methods rely on instance discrimination as their pretext task.",
    "document": "Related Work\n\nSelf-supervised learning (SSL) in computer vision has progressed rapidly, producing representations competitive with supervised pretraining. Most current self-supervised visual methods rely on instance discrimination as their pretext task. Alternatives explore masked image modeling, cross-view prediction, and clustering-based targets to encourage invariances beyond instance identity.\n\nA key limitation of instance-level invariance is diminished sensitivity to fine-grained attributes necessary for downstream tasks like detection and segmentation. Recent approaches incorporate multi-granular objectives and region-level correspondences to mitigate this issue.\n\nOur method introduces a hierarchical pretext objective that unifies instance discrimination with part-aware matching, improving transfer to dense prediction tasks without additional supervision.",
    "reason": "Makes a broad claim about the majority of methods without providing citations to substantiate it.",
    "start": 155,
    "end": 253,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior work has established that RLHF consistently reduces harmful outputs across benchmarks.",
    "document": "Introduction\n\nLarge language models (LLMs) can generate unsafe or biased content, motivating research on alignment techniques that constrain model behaviors (Bai et al., 2022; Ouyang et al., 2022). Reinforcement learning from human feedback (RLHF) is a prevalent approach that couples a preference model with policy optimization to better reflect human values. Prior work has established that RLHF consistently reduces harmful outputs across benchmarks. Despite these advances, the mechanisms underlying safety gains remain unclear, and over-optimization on proxy metrics may degrade other capabilities. We propose a decomposed training objective that separates helpfulness and harmlessness signals to improve Pareto efficiency.\n\nRelated Work\n\nSafety interventions include supervised instruction tuning, adversarial data augmentation, and red-teaming (Ganguli et al., 2022), but comprehensive comparisons remain limited.",
    "reason": "The claim attributes consistent benchmark-wide safety gains to prior work without citing specific studies or benchmarks.",
    "start": 361,
    "end": 453,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Wang et. al., 2017)",
    "document": "Related Work\n\nEnd-to-end speech recognition has transitioned from hybrid HMM-DNN systems to encoder-decoder and transducer models (Graves, 2012; Chan et al., 2016; Graves, 2013). Attention-based models excel in offline settings but struggle with latency, while RNN-T variants balance streaming constraints and accuracy (He et al., 2019; Zhang et al., 2020).\n\nData augmentation and self-training improve robustness under domain shift (Park et al., 2019; Kahn et al., 2020). External language models further enhance recognition, especially for rare words (Gulati et al., 2020). Subword units reduce OOV rates but can fragment acoustically coherent units, prompting research on adaptive tokenization (Moritz et al., 2020). Concurrently, improved beam search and pruning strategies reduce computational overhead (Wang et. al., 2017).\n\nOur work unifies adaptive tokenization with uncertainty-aware rescoring to better handle rare and code-switched terms without sacrificing latency.",
    "reason": "Misuse of 'et al.' with an extra period after 'et' ('et. al.'); proper form is 'et al.' so the citation formatting is incorrect.",
    "start": 808,
    "end": 828,
    "label": "Format"
  },
  {
    "span": "Recent work on few-shot intent classification investigates metric-learning approaches, prototypical networks, and data augmentation (Lee et al., 2020; Chen and Li, 2019; Gupta et al., 2021; Zhang et al., 2022). Additional studies leverage meta-learning, contrastive learning, and prompt-based tuning to improve generalization across domains (Xiao et al., 2021; Park et al., 2022; Wang and Zhou, 2023).",
    "document": "Introduction\nBuilding robust conversational agents in low-resource settings requires models that can generalize from very few annotated examples of new intents. This need is especially acute in multilingual assistants, where coverage of emergent intents across languages lags behind monolingual English systems. In this paper, we investigate cross-lingual transfer for few-shot intent classification by leveraging meta-learning over multilingual corpora and aligning representations with lightweight adapters.\n\nRelated Work\nFew-shot intent classification. Recent work on few-shot intent classification investigates metric-learning approaches, prototypical networks, and data augmentation (Lee et al., 2020; Chen and Li, 2019; Gupta et al., 2021; Zhang et al., 2022). Additional studies leverage meta-learning, contrastive learning, and prompt-based tuning to improve generalization across domains (Xiao et al., 2021; Park et al., 2022; Wang and Zhou, 2023). Cross-lingual intent understanding has been studied via multilingual pretraining, translation-based augmentation, and shared subword vocabularies (Feng et al., 2020; Ranjan and Gupta, 2021; Iqbal et al., 2022).\n\nMultilingual representation learning. Prior work has explored aligning languages with parallel corpora, adversarial objectives, and unsupervised mapping (Artetxe et al., 2018; Rossi and Nguyen, 2020; Kim et al., 2021). Recent approaches learn language-agnostic representations via sentence-level contrastive objectives (Luo and Zhao, 2022) and adapter-based specialization (He and Tan, 2022).\n\nOur approach builds an episodic training scheme that mixes languages at the task level and introduces a decoupled alignment loss to stabilize transfer across high- and low-resource pairs. We present extensive experiments on three multilingual benchmarks and ablations on adapter capacity and alignment strength.",
    "reason": "The span only enumerates prior techniques and citations without explaining how they relate to the paper's focus, what limitations remain, or why the authors' approach is needed, thus lacking synthesis and author perspective.",
    "start": 556,
    "end": 957,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Smith et al. 1",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a dominant paradigm for learning on relational data, with message passing architectures achieving strong results on node classification, link prediction, and graph-level tasks (Kipf and Welling, 2017; Hamilton et al., 2017; Xu et al., 2019). Numerous extensions improve expressivity via higher-order neighborhoods and subgraph reasoning (Bouritsas et al., 2020; Ying et al., 2021). Smith et al. 1 introduced a hierarchical pooling mechanism that preserves long-range dependencies, but their evaluation lacked out-of-distribution assessments.\n\nOur work builds on spectral and spatial formulations to propose a calibration-aware training objective that discourages overconfident predictions on sparsely connected regions. We further include a suite of stress tests for structural sparsity and hub dominance, providing a more comprehensive analysis than prior benchmarks.",
    "reason": "Improper use of a footnote indicator without a proper citation year or footnote formatting; should include the year (e.g., \"Smith et al. (2020)\") or be formatted as an actual footnote.",
    "start": 437,
    "end": 451,
    "label": "Format"
  },
  {
    "span": "Garcia et. al. (2018)",
    "document": "Related Work\n\nAbstractive summarization. Neural encoder–decoder models have advanced summarization by learning to compress salient content into fluent text. Early sequence-to-sequence systems struggled with factuality and repetition; copy and coverage mechanisms alleviated some issues while pretrained language models further improved fluency. Garcia et. al. (2018) examined content selection with pointer networks and reported gains on long-form news, but noted brittleness under distribution shift.\n\nPretrained summarizers. Transformer-based models pretrained with denoising or span corruption objectives achieve strong results with minimal task-specific engineering. Domain adaptation strategies align summary style with target corpora via continued pretraining and instruction-tuned objectives.\n\nEvaluation. Automatic metrics (e.g., ROUGE, BERTScore) correlate imperfectly with human judgments and can incentivize surface overlap instead of factual consistency. Recent work proposes entailment- and question-answering-based measures to better capture faithfulness, yet calibration under domain shift remains challenging.",
    "reason": "Incorrect formatting of 'et al.': it should be 'et al.' (no period after 'et'). Correct form: 'Garcia et al. (2018)'.",
    "start": 345,
    "end": 366,
    "label": "Format"
  },
  {
    "span": "MovieLens-1M",
    "document": "Introduction\n\nExplainable recommendation seeks to make algorithmic decisions transparent by surfacing reasons that align with user preferences. While neural recommenders excel at accuracy, they often lack interpretable mechanisms to justify suggestions, which hinders user trust and system debugging.\n\nRelated Work\n\nEarly methods provided rule-based justifications derived from item attributes. Recent neural approaches align latent factors with textual rationales extracted from reviews. We evaluate on MovieLens-1M and Amazon Books with both accuracy and explanation faithfulness metrics, highlighting the trade-offs between performance and interpretability.",
    "reason": "Introduces a specific dataset by name without citation on first mention, violating rule (a).",
    "start": 504,
    "end": 516,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Martinez et. al., 2016)",
    "document": "Related Work\n\nGraph pooling. Hierarchical pooling enables multi-scale reasoning over graph-structured data. Early methods rely on node clustering via learned assignments, while later work adopts top-k selection guided by task-specific scores. Empirically, graph pooling strategies (Martinez et. al., 2016) improve classification on biochemical benchmarks but can oversimplify long-range dependencies.\n",
    "reason": "Incorrect punctuation in \"et al.\": written as \"et. al.\". The correct form is \"et al.\"; citation should be \"(Martinez et al., 2016)\".",
    "start": 281,
    "end": 305,
    "label": "Format"
  },
  {
    "span": "Transformer-based NMT surpassed RNN models on the WMT14 English–German task and set a new standard for sequence modeling.",
    "document": "Related Work\n\nNeural machine translation (NMT) has evolved from attention-based recurrent models to fully attention-driven architectures that scale training and inference (Bahdanau et al., 2015; Vaswani et al., 2017). Architectures leveraging self-attention enable better long-range context modeling and parallelism, facilitating larger datasets and vocabularies. Transformer-based NMT surpassed RNN models on the WMT14 English–German task and set a new standard for sequence modeling. Subsequent improvements target regularization, data augmentation, and multilingual transfer (Ng et al., 2019; Conneau et al., 2020). Our approach complements these advances by investigating curriculum schedules that adaptively adjust difficulty based on source-target uncertainty.",
    "reason": "Mentions a specific shared task/benchmark (WMT14 En–De) and a comparative performance claim without citing supporting studies.",
    "start": 364,
    "end": 485,
    "label": "Unsupported_claim"
  },
  {
    "span": "Martinez et al. 3",
    "document": "Related Work\n\nClinical entity linking maps mentions to controlled vocabularies and benefits from leveraging hierarchical structure and synonyms (Wang et al., 2018; Luo et al., 2019). Contextual encoders improve disambiguation by modeling long-range dependencies in clinical notes (Si et al., 2019; Sung et al., 2020).\n\nDistant supervision has been used to expand training pairs with weak labels, but noise can overwhelm small gold datasets. Martinez et al. 3 demonstrate that curriculum learning mitigates this issue by gradually increasing noise levels, though their method relies on confidence heuristics that are domain-specific.\n\nOur approach complements prior work by learning a noise-aware calibration layer that reweights supervision signals according to estimated mention ambiguity.",
    "reason": "Improper footnote-like numeral after authors without year. Should include the year (e.g., Martinez et al. (YEAR)) or a properly formatted footnote.",
    "start": 441,
    "end": 458,
    "label": "Format"
  },
  {
    "span": "Task offloading has been formulated via convex optimization, game-theoretic models, and deep reinforcement learning (Mao et al., 2017; Chen et al., 2016; Wang et al., 2019; Huang et al., 2020).",
    "document": "Related Work\nEdge computing aims to reduce latency and bandwidth by relocating computation closer to data sources. A central challenge is deciding when and where to offload tasks under dynamic wireless conditions and heterogeneous devices.\nTask offloading has been formulated via convex optimization, game-theoretic models, and deep reinforcement learning (Mao et al., 2017; Chen et al., 2016; Wang et al., 2019; Huang et al., 2020). Scheduling and resource allocation consider CPU cycles, energy budgets, and queueing delays (You et al., 2017; Bi and Zhang, 2018). Joint optimization with caching and migration further improves performance (Satyanarayanan, 2017; Li et al., 2018).\nWe study offloading under volatile interference and partial observability. Our approach leverages uncertainty-aware policies with lightweight context summarization for real-time decisions on resource-constrained devices.",
    "reason": "The span lists approaches and references without tying them to the present work or clarifying the gap the authors intend to fill, indicating a lack of synthesis.",
    "start": 240,
    "end": 433,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Propensity score matching, weighting, and doubly robust estimators are commonly used to mitigate confounding in observational studies (Rosenbaum and Rubin, 1983; Robins et al., 1994; Austin, 2011).",
    "document": "Introduction\n\nCausal Inference for Electronic Health Records. Estimating treatment effects from observational EHR data requires careful adjustment for confounding, missingness, and time-varying covariates. Propensity score matching, weighting, and doubly robust estimators are commonly used to mitigate confounding in observational studies (Rosenbaum and Rubin, 1983; Robins et al., 1994; Austin, 2011). Recent work explores representation learning for balanced covariates and nonparametric outcome modeling (Johansson et al., 2016; Shalit et al., 2017). Time-to-event formulations and longitudinal settings introduce additional complexities around censoring and dynamic treatment regimes (Robins, 2000; Tsiatis, 2006).\n\nEHR-Specific Challenges. Measurement error, irregular sampling, and informative missingness are prevalent in EHRs, and domain shift across hospitals complicates generalization (Beaulieu-Jones et al., 2018; Purushotham et al., 2017).\n\nWe develop a method for estimating heterogeneous treatment effects from irregularly sampled EHR sequences.",
    "reason": "The span summarizes standard methods but does not identify what limitations remain in EHR contexts or how the proposed method addresses them, lacking explicit gap articulation (definition b and c).",
    "start": 206,
    "end": 403,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Nguyen et. al. (2021)",
    "document": "Introduction\n\nEstimating causal effects from text requires modeling both treatment assignment and outcome mechanisms (Pearl, 2009; Imbens and Rubin, 2015). Nguyen et. al. (2021) propose text balancing via learned representations to reduce confounding in observational studies. Subsequent work uses invariant risk minimization for domain generalization (Arjovsky et al., 2020) and bounds with natural language rationales (DeYoung et al., 2020). We contribute a counterfactual data augmentation approach tailored to causal estimation with pre-trained encoders.\n\nWe validate across three datasets with semi-synthetic treatments and evaluate transportability under domain shift (Bareinboim and Pearl, 2016).",
    "reason": "Incorrect formatting of “et al.”; should be “et al.” (no period after “et”).",
    "start": 156,
    "end": 177,
    "label": "Format"
  },
  {
    "span": "Perez et al.",
    "document": "Introduction\n\nPretrained encoder-decoder models have reshaped the landscape of structured prediction. Following Perez et al. we frame information extraction as constrained generation and exploit task-specific templates to reduce search complexity. While prior work has focused on monolingual settings (Raffel et al., 2020; Lewis et al., 2020), cross-lingual extensions remain underexplored, particularly for low-resource languages.\n\nWe further investigate calibration strategies that mitigate exposure bias and label imbalance, extending recent advances in prompt-conditioned decoding.",
    "reason": "Narrative citation is missing the publication year; it should appear as 'Perez et al. (YEAR)'.",
    "start": 112,
    "end": 124,
    "label": "Format"
  },
  {
    "span": "A large body of literature has explored fairness in recommender systems over the past few years.",
    "document": "Introduction\n\nRecommender systems increasingly mediate access to information, products, and opportunities, raising concerns about equity for users and providers. A large body of literature has explored fairness in recommender systems over the past few years. Proposed definitions span individual and group fairness for exposure, calibration, and satisfaction, and mitigation strategies range from reweighting to constrained optimization and post-processing. However, offline evaluation often conflates relevance with historical bias, making it difficult to assess utility–fairness trade-offs. We introduce a counterfactual evaluation protocol using logged bandit data to more faithfully estimate exposure fairness under policy changes.",
    "reason": "Claims extensive prior literature without providing representative citations (rule b/d).",
    "start": 162,
    "end": 258,
    "label": "Unsupported_claim"
  },
  {
    "span": "the MulTiQA shared task",
    "document": "Introduction\n\nMultilingual question answering (QA) has emerged as a central challenge in cross-lingual NLP, where systems must reason over passages and questions that may appear in different languages. Prior work has explored zero-shot transfer via multilingual encoders and alignment methods to bridge representational gaps between languages. While most early benchmarks emphasized monolingual QA, the field has increasingly moved toward cross-lingual evaluation to better capture real-world use cases.\n\nDespite substantial progress, consistent evaluation protocols remain elusive. For instance, the MulTiQA shared task was designed to unify data sources and metrics across languages, but system comparisons remain difficult due to inconsistent preprocessing and training data usage across submissions. In this paper, we revisit multilingual QA evaluation with a focus on robust cross-lingual generalization and reproducible baselines.\n\nWe contribute: (1) a standardized data preprocessing pipeline across languages with transparent tokenization rules, (2) a unified training and evaluation script to enable apples-to-apples comparisons, and (3) an analysis of transfer patterns under varying degrees of language relatedness. We show strong zero-shot performance in medium-resource target languages and identify failure modes in low-resource settings.\n",
    "reason": "Mentions a specific shared task without providing a citation at its first mention.",
    "start": 597,
    "end": 620,
    "label": "Unsupported_claim"
  },
  {
    "span": "The MSR-VTT dataset contains around 10k videos paired with 200k captions.",
    "document": "Related Work\n\nVideo-text representation learning aligns visual and linguistic signals through contrastive and masked modeling objectives, enabling retrieval, captioning, and QA (Miech et al., 2019; Luo et al., 2020; Bain et al., 2021). Scaling to diverse web videos improves robustness to domain shift but introduces substantial noise that requires curriculum or filtering strategies (Abu-El-Haija et al., 2016; Sun et al., 2019).\n\nThe MSR-VTT dataset contains around 10k videos paired with 200k captions.\n\nWe investigate cross-modal alignment under varying clip granularity, showing that temporal cropping policies interact strongly with text tokenization to affect retrieval precision.",
    "reason": "Introduces a specific dataset with statistics but provides no citation to the dataset paper; first mention of a dataset and its stats should be cited.",
    "start": 432,
    "end": 505,
    "label": "Unsupported_claim"
  },
  {
    "span": "Data augmentation for low-resource automatic speech recognition includes speed perturbation, SpecAugment, noise mixing, vocal tract length perturbation, and generative approaches using TTS or VAEs (Ko et al., 2015; Park et al., 2019; Hannun et al., 2020). Semi-supervised learning with pseudo-labeling and consistency regularization further improves accuracy by leveraging unlabeled audio (Kahn et al., 2020; Xu et al., 2020).",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) in low-resource settings suffers from limited labeled data and domain mismatch. Augmentation and semi-supervised techniques have become critical to improving performance when annotation budgets are constrained.\n\nData augmentation for low-resource automatic speech recognition includes speed perturbation, SpecAugment, noise mixing, vocal tract length perturbation, and generative approaches using TTS or VAEs (Ko et al., 2015; Park et al., 2019; Hannun et al., 2020). Semi-supervised learning with pseudo-labeling and consistency regularization further improves accuracy by leveraging unlabeled audio (Kahn et al., 2020; Xu et al., 2020).\n\nAdditional work targets multilingual transfer and cross-lingual pretraining with wav2vec-style objectives to reduce the dependence on labeled data (Conneau et al., 2021; Baevski et al., 2020).\n\nWe investigate augmentation scheduling strategies under strict compute budgets.",
    "reason": "The span lists augmentation and semi-supervised methods without relating them to the paper's approach or identifying a specific shortcoming they aim to address, lacking synthesis (criteria a and c).",
    "start": 261,
    "end": 687,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Brown et al., 2020)",
    "document": "Related Work\n\nAbstractive summarization has been transformed by pretrained sequence-to-sequence models such as BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020). Large autoregressive language models have also been applied to summarization; Brown et al., 2020) report that few-shot prompting yields competitive results, but later studies emphasize instruction tuning for improved controllability (Ouyang et al., 2022). Retrieval-augmented generation further improves factuality (Lewis et al., 2020b; Izacard and Grave, 2021).",
    "reason": "Missing opening parenthesis for a parenthetical citation; it should be '(Brown et al., 2020)'.",
    "start": 249,
    "end": 268,
    "label": "Format"
  },
  {
    "span": "Smith et al., (2021)",
    "document": "Introduction\n\nFederated optimization addresses distributed learning under privacy constraints by keeping data local and communicating model updates (Kairouz et al., 2019). Smith et al., (2021) propose a proximal term to stabilize training across heterogeneous clients, while Karimireddy et al. (2020) analyze client-drift and variance reduction. Subsequent studies examine adaptive optimizers and partial participation strategies (Reddi et al., 2021; Liang et al., 2020). Personalization techniques tailor global models to local data distributions (Arivazhagan et al., 2019).",
    "reason": "Incorrect punctuation in narrative citation; the comma before the year is not used in author–year narrative style. Should be \"Smith et al. (2021)\".",
    "start": 172,
    "end": 192,
    "label": "Format"
  },
  {
    "span": "Neural program synthesis approaches span sequence-to-sequence models, grammar-constrained decoding, and retrieval-augmented generation (Yin and Neubig, 2017; Rabinovich et al., 2017; Ahmad et al., 2021). Benchmarks such as CONCODE, CoNaLa, and MBPP have catalyzed progress and evaluation (Iyer et al., 2018; Yin et al., 2018; Austin et al., 2021).",
    "document": "Introduction\n\nAutomatically generating code from natural language has the potential to increase developer productivity and broaden access to programming. However, models must reconcile ambiguous intent, compositional generalization, and the need for reliable execution.\n\nNeural program synthesis approaches span sequence-to-sequence models, grammar-constrained decoding, and retrieval-augmented generation (Yin and Neubig, 2017; Rabinovich et al., 2017; Ahmad et al., 2021). Benchmarks such as CONCODE, CoNaLa, and MBPP have catalyzed progress and evaluation (Iyer et al., 2018; Yin et al., 2018; Austin et al., 2021).\n\nWe propose SpecAlign, a specification-aware decoder that conditions on latent unit tests produced by a verifier model. The system jointly reasons over intent and candidate executions to filter spurious programs.\n\nExperiments on MBPP and HumanEval show that SpecAlign improves pass@k and reduces semantic errors. We provide analyses of verifier accuracy, test diversity, and failure modes on compositional tasks.",
    "reason": "The span catalogs approaches and datasets without explaining how they relate to the proposed SpecAlign method or which deficiency motivates it, demonstrating lack of synthesis (criteria a and c).",
    "start": 271,
    "end": 618,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT (Devlin et al 2019)",
    "document": "Introduction\n\nOpen-domain question answering (ODQA) systems typically combine a retriever with a reader to extract or generate answers from large text corpora (Chen et al., 2017; Karpukhin et al., 2020). Dense retrievers have improved recall by learning joint embeddings for queries and passages (Xiong et al., 2021; Izacard and Grave, 2021).\n\nOn the reader side, pretrained language models such as BERT (Devlin et al 2019) and T5 (Raffel et al., 2020) have become standard backbones, with recent work exploring fusion-in-decoder architectures to aggregate evidence from multiple passages (Izacard and Grave, 2021). We focus on calibration, analyzing how retrieval uncertainty propagates to answer confidence.",
    "reason": "Missing comma and period in the citation: should be 'BERT (Devlin et al., 2019)'.",
    "start": 399,
    "end": 423,
    "label": "Format"
  },
  {
    "span": "Forecasting methods range from statistical to neural approaches. ARIMA models capture linear autoregressive structure (Box et al., 2015). Prophet decomposes time series into trend and seasonality with changepoints (Taylor and Letham, 2018). LSTMs model nonlinear dependencies (Hochreiter and Schmidhuber, 1997). Transformers scale to long horizons with attention (Zhou et al., 2021). Post-hoc attribution explains feature contributions (Lundberg and Lee, 2017).",
    "document": "Related Work\n\nTime-series forecasting underpins planning in energy, retail, and healthcare. Traditional statistical models emphasize interpretability and uncertainty, while neural models have advanced accuracy on large and complex datasets. Hybrid approaches seek to combine strengths but face challenges in stability and calibration.\n\nForecasting methods range from statistical to neural approaches. ARIMA models capture linear autoregressive structure (Box et al., 2015). Prophet decomposes time series into trend and seasonality with changepoints (Taylor and Letham, 2018). LSTMs model nonlinear dependencies (Hochreiter and Schmidhuber, 1997). Transformers scale to long horizons with attention (Zhou et al., 2021). Post-hoc attribution explains feature contributions (Lundberg and Lee, 2017).\n\nWe study calibrated probabilistic forecasts with interpretable decompositions across both short and long horizons.",
    "reason": "The span lists heterogeneous works with no transitions or explicit statement of how attribution relates to forecasting models, making the connections between sentences and cited works unclear.",
    "start": 336,
    "end": 797,
    "label": "Coherence"
  },
  {
    "span": "Back-translation augments training data with synthetic sources (Sennrich et al., 2016). Quality estimation predicts the adequacy of translations without references (Specia et al., 2018). Multilingual NMT shares parameters across languages to transfer to low-resource pairs (Johnson et al., 2017).",
    "document": "Related Work\n\nNeural Machine Translation in Low-Resource Settings\n\nLow-resource NMT seeks to improve translation quality when parallel data is scarce by leveraging monolingual corpora, multilingual transfer, and data selection. Back-translation augments training data with synthetic sources (Sennrich et al., 2016). Quality estimation predicts the adequacy of translations without references (Specia et al., 2018). Multilingual NMT shares parameters across languages to transfer to low-resource pairs (Johnson et al., 2017). Unsupervised NMT relies on denoising autoencoders and iterative back-translation (Lample et al., 2018; Artetxe et al., 2018).",
    "reason": "The span switches among data augmentation, evaluation without references, and multilingual modeling without clarifying their relationships, creating an abrupt and incoherent flow.",
    "start": 228,
    "end": 524,
    "label": "Coherence"
  },
  {
    "span": "The i2b2 2010 shared task is the de facto benchmark for English clinical NER.",
    "document": "Related Work\n\nNamed entity recognition (NER) in the clinical domain has received substantial attention due to the importance of extracting problems, tests, and treatments from electronic health records. A range of datasets have been proposed to support clinical NER, covering varied annotation schemes and document types. The i2b2 2010 shared task is the de facto benchmark for English clinical NER. Beyond English, multilingual clinical resources remain limited, which motivates transfer and adaptation methods. Our study complements this literature by introducing a lightweight adaptation framework for low-resource clinical settings.\n",
    "reason": "Asserts the status of a specific shared task as 'the de facto benchmark' without citing the shared task or supporting evidence.",
    "start": 322,
    "end": 399,
    "label": "Unsupported_claim"
  },
  {
    "span": "The ImageNet Adversarial Challenge demonstrated that transfer attacks dominate white-box defenses.",
    "document": "Background on Adversarial Evaluation in Vision\nBenchmark competitions have played a central role in clarifying the strengths and weaknesses of adversarial defenses in image classification. Findings from public evaluations often differ from results reported under closed experimental conditions, emphasizing the need for standardized, reproducible testing.\nThe ImageNet Adversarial Challenge demonstrated that transfer attacks dominate white-box defenses. This motivates evaluation protocols that include multiple threat models, diverse source models, and unrestricted query budgets.\nIn this work, we adopt a stress-testing framework that aggregates results across attack families and perturbation budgets, providing robust comparisons of defenses under realistic constraints.",
    "reason": "References a specific competition and a key empirical finding without citing any report or paper from that challenge.",
    "start": 356,
    "end": 454,
    "label": "Unsupported_claim"
  },
  {
    "span": "2019 (Nguyen and Tran)",
    "document": "Related Work\n\nClassical time series forecasting relies on ARIMA and exponential smoothing (Box et al., 2015; Hyndman and Athanasopoulos, 2018). Deep learning approaches incorporate convolutional filters and temporal attention (Bai et al., 2018; Lim et al., 2021). 2019 (Nguyen and Tran) introduced hierarchical seasonality encoders for intermittent demand, while Sen et al. (2019) propose probabilistic autoregressive models. Recent benchmarks emphasize long-horizon accuracy and robustness to distribution shift (Zhou et al., 2021).",
    "reason": "Author–year order is reversed; should be a narrative citation like 'Nguyen and Tran (2019)'.",
    "start": 264,
    "end": 286,
    "label": "Format"
  },
  {
    "span": "Message passing networks, attention-based graph encoders, and spectral methods have been extensively used for molecular property prediction (Gilmer et al., 2017; Kearnes et al., 2016; Veličković et al., 2018; Tang et al., 2020; Ying et al., 2021).",
    "document": "Related Work\nPredicting molecular properties from structure is central to virtual screening, lead optimization, and toxicity assessment. Graph neural networks (GNNs) are a natural fit because molecules can be modeled as graphs of atoms and bonds.\nMessage passing networks, attention-based graph encoders, and spectral methods have been extensively used for molecular property prediction (Gilmer et al., 2017; Kearnes et al., 2016; Veličković et al., 2018; Tang et al., 2020; Ying et al., 2021). Pretraining strategies draw on masked atom/bond modeling and contrastive graph objectives (Hu et al., 2019; You et al., 2020; Rong et al., 2020). Data-centric advances include scaffold splits and augmentation of rare functional groups (Wu et al., 2018; Rogers and Hahn, 2010; Stokes et al., 2020).\nDespite strong performance on public benchmarks, domain shift across assays and distributional changes in scaffolds can degrade accuracy. Our study examines uncertainty-aware ensembling under scaffold splits and tests calibration-sensitive selection for hit discovery.",
    "reason": "The span lists categories of methods and citations without relating them to the authors’ research question or identifying what remains unresolved, hence lacking synthesis.",
    "start": 247,
    "end": 494,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The BioBERT corpus was released in 2019 with 18 billion tokens.",
    "document": "Related Work\n\nBiomedical named entity recognition (BioNER) and relation extraction benefit from domain-specific pretraining that captures terminology and stylistic conventions of biomedical text. Pretrained encoders have been adapted to biomedical corpora to improve coverage of rare entities and abbreviations. The BioBERT corpus was released in 2019 with 18 billion tokens. While such scaling has improved downstream accuracy, many benchmarks conflate generalization with lexicon overlap, making it difficult to assess true domain robustness. We introduce a split that controls for synonymy and abbreviation drift.",
    "reason": "Claims specific release date and corpus size for a dataset/model without citing a source.",
    "start": 312,
    "end": 375,
    "label": "Unsupported_claim"
  },
  {
    "span": "Counterfactual fairness ensures decisions do not change under interventions on sensitive attributes (Kusner et al., 2017). Calibration aims to match predicted probabilities to observed frequencies (Guo et al., 2017). Equalized odds enforces equal error rates across groups (Hardt et al., 2016).",
    "document": "Introduction\n\nFairness in machine learning encompasses multiple criteria that may be mutually incompatible, complicating deployment in high-stakes settings. Choosing an appropriate objective requires understanding trade-offs between group-level guarantees and individual-level notions.\n\nCounterfactual fairness ensures decisions do not change under interventions on sensitive attributes (Kusner et al., 2017). Calibration aims to match predicted probabilities to observed frequencies (Guo et al., 2017). Equalized odds enforces equal error rates across groups (Hardt et al., 2016). Post-processing methods can adjust thresholds to satisfy constraints (Pleiss et al., 2017), while representation learning seeks to remove sensitive information (Moyer et al., 2018). Yet, evaluating these methods under distribution shift is still an open challenge.\n\nWe propose a framework for stress-testing fairness criteria across shifts and provide guidelines for selecting compatible objectives in practice.",
    "reason": "The span presents three fairness criteria in sequence without linking phrases or clarifying their relationships or tensions. The lack of transitions makes the connection between cited works abrupt and unclear.",
    "start": 287,
    "end": 581,
    "label": "Coherence"
  },
  {
    "span": "Private empirical risk minimization is commonly achieved via output perturbation, objective perturbation, or differentially private SGD (Chaudhuri et al., 2011; Abadi et al., 2016; Bassily et al., 2014; Mironov, 2017).",
    "document": "Introduction\nDeploying machine learning under rigorous privacy constraints is increasingly important for sensitive domains such as healthcare and finance. Differential privacy (DP) provides formal guarantees against information leakage from individual training examples.\nPrivate empirical risk minimization is commonly achieved via output perturbation, objective perturbation, or differentially private SGD (Chaudhuri et al., 2011; Abadi et al., 2016; Bassily et al., 2014; Mironov, 2017). Accounting methods and privacy amplification by subsampling further refine guarantees (Balle et al., 2018; Mironov, 2019; Wang et al., 2019). Utility losses are often mitigated by architectural choices and public pretraining (Papernot et al., 2021; Tramer and Boneh, 2021).\nWe investigate label differential privacy under class imbalance, proposing a reweighting scheme compatible with privacy accounting. Our analysis characterizes privacy–utility trade-offs and reports improvements on imbalanced vision datasets.",
    "reason": "The span states standard techniques with citations but does not articulate how they relate to the paper’s focus or identify the unresolved issue motivating the study.",
    "start": 271,
    "end": 489,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Nguyen and Nguyen, 2021",
    "document": "Related Work\n\nMultilingual named entity recognition (NER). Early cross-lingual NER relied on projection from parallel corpora and bilingual lexicons. With multilingual encoders, parameter sharing became the dominant strategy, reducing dependence on external resources. Recent studies investigate language adapters and task-specific prompts to minimize negative transfer. For example, several works compare shared versus language-specific CRF layers and analyze label-set alignment across scripts (Nguyen and Nguyen, 2021 provide a comprehensive review).\n\nWeak supervision and distant signals. Gazetteers, dictionaries, and cross-lingual links have been used to bootstrap labels. Consistency training with back-translation further improves low-resource performance, though annotation noise remains a concern.",
    "reason": "Missing closing parenthesis in a parenthetical citation. It should be '(Nguyen and Nguyen, 2021)'.",
    "start": 496,
    "end": 520,
    "label": "Format"
  },
  {
    "span": "The widely used BC5CDR corpus includes 1.5 million entity mentions.",
    "document": "Introduction\n\nBiomedical named entity recognition (BioNER) underpins downstream tasks such as relation extraction and literature-based discovery. Domain-specific corpora and terminologies are essential to capture synonymy, ambiguity, and evolving nomenclature. The widely used BC5CDR corpus includes 1.5 million entity mentions. Pre-trained biomedical language models improve recognition of rare entities but can overfit to annotation artifacts. We propose a distant-supervision framework with curriculum reweighting that balances curated annotations and noisy weak labels, improving generalization to unseen journals and subdomains.",
    "reason": "States a precise dataset statistic without citing the dataset or a source (rule a and b).",
    "start": 261,
    "end": 328,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most prior work evaluates on Defects4J v2.0 but reports results only on the old test suite.",
    "document": "Related Work\n\nAutomated program repair (APR) seeks to synthesize patches that fix bugs without human intervention. Techniques range from generate-and-validate methods using test suites to semantics-based approaches that leverage symbolic constraints. Benchmarking has primarily relied on curated collections of real-world bugs to ensure comparability across methods.\n\nDefects4J is a widely used benchmark for Java projects, providing reproducible builds and test suites for historical bugs. Most prior work evaluates on Defects4J v2.0 but reports results only on the old test suite. This discrepancy can inflate perceived fix rates due to differences in test coverage and oracle strength, complicating fair comparison.\n\nIn this paper, we revisit evaluation practices for APR. We quantify the sensitivity of reported success rates to test suite versions and propose a standardized reporting checklist that distinguishes plausible from correct fixes. We also release tooling that automatically verifies environment consistency across studies.",
    "reason": "Claims a pattern in prior evaluations on a specific benchmark without citations to studies demonstrating it (violates rule b and a for dataset/benchmark references).",
    "start": 491,
    "end": 582,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Yu, 2018, Wang, 2019)",
    "document": "Introduction. Transfer learning has enabled rapid adaptation with limited labeled data by reusing features from large-scale pretraining (Yosinski et al., 2014; Pan & Yang, 2010). Fine-tuning strategies vary from full-model updates to adapter-based methods that reduce compute and storage (Houlsby et al., 2019; Pfeiffer et al., 2021). Regularization and early stopping mitigate overfitting when target datasets are small (Ruder, 2017). Prior work compares layerwise learning rates across vision and NLP benchmarks (Howard & Ruder, 2018). Related studies (Yu, 2018, Wang, 2019) evaluate negative transfer and propose domain discriminators to identify harmful sources. We extend these analyses with risk-sensitive selection under budget constraints.",
    "reason": "Multiple citations inside one parenthesis are separated by a comma instead of a semicolon; should be '(Yu, 2018; Wang, 2019)'.",
    "start": 554,
    "end": 576,
    "label": "Format"
  },
  {
    "span": "a previous study showed that annotators prefer contrastive explanations in 78% of cases",
    "document": "Introduction\n\nExplainable AI (XAI) for NLP has evolved from feature-importance visualizations to structured rationales and counterfactuals. While contrastive and counterfactual formats are argued to be more actionable for users, empirical evidence remains fragmented across tasks and populations. In particular, a previous study showed that annotators prefer contrastive explanations in 78% of cases, yet the robustness of this finding across domains has not been established. We revisit this question in the context of document classification, examining preference stability under different explanation granularities and time constraints.\n",
    "reason": "Reports a specific statistic from a prior study without providing a citation.",
    "start": 312,
    "end": 399,
    "label": "Unsupported_claim"
  },
  {
    "span": "[Zhang, 2020]",
    "document": "Related Work\n\nInteractive information retrieval (IIR) studies how systems can effectively support users through iterative query reformulation, result exploration, and feedback (Belkin et al., 2001; Kelly, 2009). Learning-to-rank methods have increasingly incorporated user interaction signals to adapt rankings online (Joachims, 2002; Radlinski and Joachims, 2007).\n\nSession-based recommendation in search leverages short-term behavioral context to refine relevance estimates, as shown in [Zhang, 2020] and extended by transformer-based models that capture multi-turn dependencies (Huang et al., 2020; Zeng et al., 2021). Our work unifies session context with explicit feedback uncertainty to improve robustness under sparse interactions.",
    "reason": "Wrong citation style: uses square brackets in an author–year context; should be '(Zhang, 2020)'.",
    "start": 489,
    "end": 502,
    "label": "Format"
  },
  {
    "span": "Post-hoc saliency methods like Grad-CAM, integrated gradients, and perturbation-based analyses are widely used to visualize model attention in medical imaging (Selvaraju et al., 2017; Sundararajan et al., 2017; Fong and Vedaldi, 2017).",
    "document": "Related Work\n\nExplainability in medical imaging seeks to increase clinician trust and support decision-making by exposing model rationale alongside predictions. Methods span from inherently interpretable models to post-hoc explanations applied to high-performing but opaque architectures.\n\nPost-hoc saliency methods like Grad-CAM, integrated gradients, and perturbation-based analyses are widely used to visualize model attention in medical imaging (Selvaraju et al., 2017; Sundararajan et al., 2017; Fong and Vedaldi, 2017). Attribution maps have been evaluated with deletion/insertion metrics, weak localization, and clinical reader studies (Petsiuk et al., 2018; Chattopadhay et al., 2018; Tjoa and Guan, 2020). Prototype learning and case-based reasoning provide alternative explanations by referencing similar examples in the training set (Chen et al., 2019; Barnett et al., 2021).\n\nCausality-inspired approaches attempt to decouple confounders from signal, while uncertainty quantification aims to flag unreliable predictions in distribution shifts (Kendall and Gal, 2017; DeGrave et al., 2021). Regulatory guidance emphasizes documentation, bias audits, and human factors considerations for clinical adoption.\n\nTogether, these strands reflect a diverse toolbox for communicating model behavior to clinical end users.",
    "reason": "The span lists common explainability methods without clarifying how they connect to the authors’ argument, what shortcomings remain, or why a new approach is needed.",
    "start": 290,
    "end": 525,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[Kumar et al., 2019]",
    "document": "Related Work\n\nTask-oriented dialogue systems traditionally rely on pipeline architectures with distinct modules for NLU, state tracking, and policy learning (Young et al., 2013; Henderson et al., 2014). End-to-end neural approaches learn these components jointly and benefit from large-scale pretraining on conversational corpora (Budzianowski and Vulić, 2019; Hosseini-Asl et al., 2020). Knowledge-grounded dialogue extends these models to incorporate external databases and documents for factual responses (Dinan et al., 2019; Zhao et al., 2020). Retrieval-augmented generation has shown strong gains when knowledge selection is accurate [Kumar et al., 2019], but brittle retrieval can propagate errors to the generator. We address this by integrating uncertainty-aware reranking with semantic coverage constraints.\n\nIntroduction\n\nEvaluation remains challenging due to the weak correlation between automatic metrics and human judgments (Mehri and Eskenazi, 2020). We propose a suite of calibrated metrics that better capture faithfulness and helpfulness in knowledge-grounded settings.",
    "reason": "Wrong citation style: uses square brackets for an author–year citation in a context where parentheses are used elsewhere. Should be \"(Kumar et al., 2019)\".",
    "start": 640,
    "end": 660,
    "label": "Format"
  },
  {
    "span": "(Adams et al. 2015)",
    "document": "Related Work\n\nObject detection evolved from sliding-window pipelines (Felzenszwalb et al., 2010) to region-based CNNs (Girshick et al., 2014; Ren et al., 2015) and one-stage detectors (Redmon et al., 2016; Lin et al., 2017). Focal loss addresses class imbalance (Lin et al., 2017), and feature pyramid networks enable multi-scale reasoning (Lin et al., 2017). Some works integrate non-local attention (Wang et al., 2018) to capture long-range context (Adams et al. 2015).",
    "reason": "Missing comma before the year in a parenthetical citation; it should be '(Adams et al., 2015)'.",
    "start": 451,
    "end": 470,
    "label": "Format"
  },
  {
    "span": "Knowledge graph link prediction has employed translational distance models, bilinear factorization, and neural encoders (Bordes et al., 2013; Yang et al., 2015; Dettmers et al., 2018). Recent work incorporates textual descriptions and pre-trained language models to enrich entity and relation representations (Yao et al., 2019; Sun et al., 2020).",
    "document": "Related Work\n\nLink prediction seeks to infer missing relations in knowledge graphs (KGs). Challenges include multi-relational structure, sparse entities, and long-tail relations. Models must capture semantic regularities while generalizing to unseen nodes and relations.\n\nKnowledge graph link prediction has employed translational distance models, bilinear factorization, and neural encoders (Bordes et al., 2013; Yang et al., 2015; Dettmers et al., 2018). Recent work incorporates textual descriptions and pre-trained language models to enrich entity and relation representations (Yao et al., 2019; Sun et al., 2020).\n\nWe present a retrieval-augmented KG scorer that conditions on neighborhood subgraphs retrieved via learned queries, enabling compositional generalization on few-shot relations with modest compute overhead.",
    "reason": "The span enumerates prior approaches and trends but does not articulate the gap or tie them to the paper’s proposed method, lacking synthesis and motivation.",
    "start": 272,
    "end": 618,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Kim et. al., 2018)",
    "document": "Introduction\n\nFederated learning enables collaborative model training without centralizing raw data, reducing privacy risks in regulated domains. Communication efficiency and client heterogeneity are key challenges that motivate partial participation and compression schemes (McMahan and Ramage, 2017; Karimireddy et al., 2020). Federated averaging reduces communication rounds (Kim et. al., 2018) but can struggle with non-IID data, prompting personalized and clustered variants (Dinh et al., 2021; Mansour et al., 2022). We introduce an adaptive optimizer that stabilizes updates under skewed client distributions.",
    "reason": "Incorrect abbreviation: uses \"et. al.\" instead of the correct \"et al.\" in the citation. Should be \"(Kim et al., 2018)\".",
    "start": 378,
    "end": 397,
    "label": "Format"
  },
  {
    "span": "Off-policy evaluation in recommenders has been addressed through inverse propensity scoring, self-normalization, doubly robust estimators, and logged bandit feedback corrections (Schnabel et al., 2016; Swaminathan and Joachims, 2015; Dudík et al., 2014; Wang et al., 2020).",
    "document": "Related Work\n\nEvaluating recommender policies offline is critical for safe deployment and rapid iteration. Logged user interactions collected under historical policies are biased, necessitating correction techniques for reliable estimation.\n\nOff-policy evaluation in recommenders has been addressed through inverse propensity scoring, self-normalization, doubly robust estimators, and logged bandit feedback corrections (Schnabel et al., 2016; Swaminathan and Joachims, 2015; Dudík et al., 2014; Wang et al., 2020).\n\nComplementary lines of work study counterfactual learning-to-rank and causal embeddings that attempt to disentangle user preference from exposure mechanisms. Practical systems combine multiple estimators for variance reduction.\n\nWe study how exposure dynamics and delayed feedback interact to distort risk-sensitive metrics and present an estimator tailored to heavy-tailed rewards.",
    "reason": "The span lists techniques and citations without articulating their assumptions, limitations, or how they motivate the current study, i.e., it lacks synthesis per (a) and (c).",
    "start": 242,
    "end": 515,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Han et al. (2016) study network pruning to reduce parameters in DNNs. Kang et al. (2017) propose Neurosurgeon to partition DNNs between mobile and cloud. Quantization reduces computation via low-precision arithmetic (Jacob et al., 2018). Li et al. (2018) present Edge AI accelerators for heterogeneous devices.",
    "document": "Related Work\n\nDeploying deep models at the edge requires balancing accuracy, latency, and energy. We review model compression and device-cloud partitioning.\n\nHan et al. (2016) study network pruning to reduce parameters in DNNs. Kang et al. (2017) propose Neurosurgeon to partition DNNs between mobile and cloud. Quantization reduces computation via low-precision arithmetic (Jacob et al., 2018). Li et al. (2018) present Edge AI accelerators for heterogeneous devices.\n\nOur method complements these techniques by jointly optimizing placement and bitrate for multi-camera analytics.",
    "reason": "Four different directions are presented in separate sentences without clarifying their interplay or comparative roles; transitions are absent, and the relationships are implied rather than stated.",
    "start": 158,
    "end": 468,
    "label": "Coherence"
  },
  {
    "span": "There are many recent works on multilingual NMT for low-resource languages that leverage shared subword vocabularies.",
    "document": "Introduction\n\nMultilingual neural machine translation (NMT) seeks to share parameters across languages to improve quality, especially for low-resource pairs. Joint training can transfer knowledge from high-resource auxiliaries through shared lexicons and representations, but negative interference and capacity limits pose challenges. Balancing language coverage and per-language performance requires careful design of tokenization, sampling, and model capacity. There are many recent works on multilingual NMT for low-resource languages that leverage shared subword vocabularies. In addition, adapters and language-specific layer norms have been proposed to modulate capacity allocation, and fine-tuning strategies mitigate catastrophic forgetting when adding new languages. Our work evaluates capacity scaling laws for encoder–decoder transformers under constrained training budgets, examining the trade-offs among vocabulary granularity, sampling temperature, and domain mismatch.",
    "reason": "Uses the vague phrase \"many recent works\" to describe prior literature without citing any of them; references to recent work should include citations (rule d).",
    "start": 463,
    "end": 580,
    "label": "Unsupported_claim"
  },
  {
    "span": "Self-supervised speech pretraining has achieved strong results in low-resource ASR, with wav2vec 2.0, HuBERT, and XLS-R among the most widely used backbones (Baevski et al., 2020; Hsu et al., 2021; Babu et al., 2021). We fine-tune wav2vec 2.0 on a small target-language dataset.",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) in low-resource settings is constrained by limited labeled audio. Self-supervised representation learning has emerged as a key enabler by leveraging large amounts of unlabeled speech.\n\nSelf-supervised speech pretraining has achieved strong results in low-resource ASR, with wav2vec 2.0, HuBERT, and XLS-R among the most widely used backbones (Baevski et al., 2020; Hsu et al., 2021; Babu et al., 2021). We fine-tune wav2vec 2.0 on a small target-language dataset.\n\nOur experiments cover three languages with less than 10 hours of labeled data each and compare tokenization schemes.",
    "reason": "The span jumps from listing prior models to stating the authors’ choice without articulating the gap or rationale, thus lacking synthesis.",
    "start": 234,
    "end": 512,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Several approaches aim to learn disentangled factors using variational objectives, inductive biases, or weak supervision (Higgins et al., 2017; Locatello et al., 2019; Khemakhem et al., 2020; Shu et al., 2020).",
    "document": "Introduction\n\nUnderstanding causal structure from high-dimensional observations is central to robust generalization and transfer. Representation learning literature has explored ways to separate underlying generative factors.\n\nSeveral approaches aim to learn disentangled factors using variational objectives, inductive biases, or weak supervision (Higgins et al., 2017; Locatello et al., 2019; Khemakhem et al., 2020; Shu et al., 2020).\n\nHowever, interventions at training time are rarely available, and identifiability often hinges on restrictive assumptions. We propose a weakly supervised objective that exploits naturally occurring counterfactual pairs.",
    "reason": "Lists prior disentanglement approaches without stating how they fall short for the present problem or how the paper builds on them in that sentence (criteria a and b).",
    "start": 227,
    "end": 437,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Prior studies report that multilingual tokenization harms morphological analysis in agglutinative languages.",
    "document": "Introduction\n\nMorphological analysis is a prerequisite for robust NLP in morphologically rich languages, enabling improved tagging, parsing, and translation (Cotterell et al., 2018). Neural approaches combine character-level encoders with subword tokenization to capture productive morphology, yet cross-lingual transfer remains challenging for typologically distant languages (Heigold et al., 2017; Mielke et al., 2019). Recent multilingual models offer broad coverage but may under-segment complex affixes.\n\nPrior studies report that multilingual tokenization harms morphological analysis in agglutinative languages. We investigate task-adaptive segmentation and affix-aware attention that mitigate under-segmentation, demonstrating gains on Turkish, Finnish, and Quechua benchmarks.",
    "reason": "Claims findings from unspecified prior studies without providing citations, which should be supported in related work or introduction.",
    "start": 510,
    "end": 618,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith & Jones (2019)",
    "document": "Introduction\n\nMultilingual dependency parsing has benefited from cross-lingual transfer via shared subword vocabularies and parameter tying (Ammar et al., 2016; Smith et al., 2018). Smith & Jones (2019) demonstrated that typology-aware adapters reduce negative transfer for low-resource languages, aligning with subsequent findings on language-specific layers (Pires et al., 2019; Pfeiffer et al., 2020). Nevertheless, structural divergences such as pro-drop and flexible word order still challenge universal parsers (Nivre et al., 2016; de Lhoneux et al., 2017).\n\nData augmentation through synthetic treebanks and projection has shown mixed results due to noise accumulation (Tiedemann and Agić, 2016; Rasooli and Collins, 2017). Recent work employs meta-learning to adapt quickly to new languages with few annotations (Nooralahzadeh et al., 2020; Ponti et al., 2021).\n\nWe propose a contrastively trained alignment module that conditions on typological features to improve head selection under transfer, with analyses on head–dependent distance and POS-specific accuracy.",
    "reason": "Ampersand used in a narrative citation; APA narrative style requires 'and' as in 'Smith and Jones (2019)'.",
    "start": 182,
    "end": 202,
    "label": "Format"
  },
  {
    "span": "The standard evaluation metric for dialog state tracking was established by the DSTC3 organizers.",
    "document": "Related Work\n\nTask-oriented dialogue systems require accurate tracking of user goals as conversations progress. The standard evaluation metric for dialog state tracking was established by the DSTC3 organizers. Subsequent benchmarks expanded domains and ontologies, introducing multi-domain and cross-lingual settings. Our work focuses on calibration and uncertainty quantification for state tracking models.",
    "reason": "Mentions a specific shared task and its contribution without citation; per rule (a), this requires a reference.",
    "start": 112,
    "end": 209,
    "label": "Unsupported_claim"
  },
  {
    "span": "GraphSAGE samples neighborhoods to scale to large graphs (Hamilton et al., 2017). BPR optimizes pairwise ranking loss (Rendle et al., 2009). Attention over metapaths captures heterogeneity (Wang et al., 2019).",
    "document": "Related Work\n\nGraph-based Recommendation. Graph neural networks (GNNs) have become a standard tool in recommender systems, enabling higher-order connectivity modeling and alleviating sparsity via message passing over user–item graphs (Ying et al., 2018; He et al., 2020).\n\nNeighborhood Aggregation and Heterogeneity. Inductive methods allow learning on unseen nodes by aggregating sampled neighborhoods, improving scalability and generalization (Hamilton et al., 2017). Heterogeneous information networks capture multi-typed relations, and metapath-based attention helps focus on informative connectivity patterns (Wang et al., 2019). GraphSAGE samples neighborhoods to scale to large graphs (Hamilton et al., 2017). BPR optimizes pairwise ranking loss (Rendle et al., 2009). Attention over metapaths captures heterogeneity (Wang et al., 2019). We consider cold-start scenarios with side information.\n\nContrastive Learning for Recommendation. Recent work introduces self-supervised signals to reinforce representations through augmentations on the interaction graph (Wu et al., 2021; Yu et al., 2022). Our method unifies contrastive objectives with metapath-aware propagation to improve robustness under sparsity.",
    "reason": "The span mixes GraphSAGE, BPR, and metapath attention without clarifying how a sampling method, a ranking loss, and a heterogeneity mechanism relate; no transitions connect the cited works.",
    "start": 635,
    "end": 844,
    "label": "Coherence"
  },
  {
    "span": "Recent works show that masked video modeling consistently outperforms contrastive learning on long-horizon tasks.",
    "document": "Related Work\n\nSelf-supervised learning for video has progressed rapidly, drawing inspiration from both image pretraining and language modeling. Contrastive methods align temporally augmented clips in a shared embedding space (Feichtenhofer et al., 2019; Qian et al., 2021), while reconstruction-based approaches mask frames or tokens and predict missing content (Tong et al., 2022; Wang et al., 2023). Hybrid objectives combine temporal ordering, masked prediction, and distillation to exploit complementary signals (Patrick et al., 2021; Arnab et al., 2021).\n\nTemporal reasoning over long horizons remains challenging due to compounding errors and computational constraints. Recent works show that masked video modeling consistently outperforms contrastive learning on long-horizon tasks. However, it is unclear whether these gains stem from better temporal credit assignment or simply from auxiliary supervision provided by dense reconstruction targets. We disentangle these factors by varying masking schedules and tokenization granularity under controlled compute budgets.",
    "reason": "Asserts a trend among recent works without providing specific citations (rule d).",
    "start": 676,
    "end": 789,
    "label": "Unsupported_claim"
  },
  {
    "span": "Garcia et al.",
    "document": "Related Work\n\nNeural text simplification has evolved from rule-based pipelines to end-to-end neural approaches, with early sequence-to-sequence baselines demonstrating the feasibility of learning simplification patterns from aligned corpora (Rao and Mittal, 2016). Later transformer-based architectures improved content retention and fluency by leveraging pretraining on large unlabeled text (Adams et al., 2019; Liu and Chen, 2020). Garcia et al. introduce a curriculum-learning strategy that prioritizes sentence pairs by lexical complexity, reporting gains over standard baselines. Subsequent research compares rule-based and neural approaches (Miller, 2018; Zhang and Xu, 2021) and explores controllable simplification via explicit difficulty parameters (Brown and Li, 2020). Finally, evaluation has shifted from surface metrics to meaning preservation and readability assessments (Sato and Nakamura, 2022).",
    "reason": "Narrative citation missing year; should appear as an author-year narrative such as \"Garcia et al. (2020)\".",
    "start": 434,
    "end": 447,
    "label": "Format"
  },
  {
    "span": "users click 'Accept All' 70% of the time",
    "document": "Introduction\n\nCookie consent dialogs are intended to provide users with transparency and control over tracking technologies on the web. However, interface design choices can strongly influence user behavior, raising concerns about whether consent is informed and freely given. Prior work in human–computer interaction has explored the role of dark patterns and nudges in shaping privacy decisions, yet empirical evidence across sites and regions remains fragmented. In typical banner configurations, users click 'Accept All' 70% of the time, suggesting that default-promoting designs may override expressed preferences. We investigate how layout, language, and timing affect acceptance rates, and propose design guidelines aligned with data protection principles.\n",
    "reason": "Presents a specific statistic about user behavior without citing a study or dataset supporting the figure.",
    "start": 500,
    "end": 540,
    "label": "Unsupported_claim"
  },
  {
    "span": "Privacy-preserving optimization has been explored through secure aggregation protocols, differential privacy mechanisms, and homomorphic encryption. Secure aggregation reduces the server’s visibility into individual updates, differential privacy perturbs gradients to bound information leakage, and homomorphic encryption enables computation over ciphertexts without decryption. Recent works also examine personalization layers atop federated objectives to mitigate client drift and data heterogeneity.",
    "document": "Related Work\n\nFederated learning enables collaborative model training across decentralized data silos without centralizing raw data. Foundational studies consider communication-efficient protocols, client selection strategies, and aggregation rules for non-IID data. In healthcare, federated learning has been adopted for imaging, electronic health records, and wearable sensor data due to stringent privacy regulations and cross-institutional deployment constraints.\n\nPrivacy-preserving optimization has been explored through secure aggregation protocols, differential privacy mechanisms, and homomorphic encryption. Secure aggregation reduces the server’s visibility into individual updates, differential privacy perturbs gradients to bound information leakage, and homomorphic encryption enables computation over ciphertexts without decryption. Recent works also examine personalization layers atop federated objectives to mitigate client drift and data heterogeneity.\n\nSystem-level studies investigate asynchronous aggregation, straggler mitigation, and client availability modeling, while communication-efficient approaches compress updates with sparsification and quantization. In clinical applications, benchmarking suites report variability across sites stemming from label noise, device differences, and demographic shifts.\n\nWe study federated learning for cross-hospital pathology classification under strict privacy budgets and intermittent connectivity, focusing on stability of training under non-stationary participation.",
    "reason": "This paragraph catalogs prior techniques without articulating how they relate to the authors' approach or what gap motivates the present work.",
    "start": 469,
    "end": 971,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Zhang et al. (2018) presented a framework for conversational recommendation using hierarchical RL. Christakopoulou et al. (2016) investigated preference elicitation through user interactions. Li et al. (2017) proposed persona-based dialog models to improve engagement. Chen et al. (2019) curated datasets for knowledge-grounded conversation.",
    "document": "Introduction\n\nConversational recommender systems seek to iteratively elicit user preferences and provide just-in-time item suggestions via natural language interactions (Sun and Zhang, 2018; Jannach et al., 2020). This paradigm combines techniques from recommendation, dialog management, and knowledge grounding to reduce cold-start latency and improve satisfaction.\n\nZhang et al. (2018) presented a framework for conversational recommendation using hierarchical RL. Christakopoulou et al. (2016) investigated preference elicitation through user interactions. Li et al. (2017) proposed persona-based dialog models to improve engagement. Chen et al. (2019) curated datasets for knowledge-grounded conversation.\n\nHowever, these lines of work leave open questions about the joint modeling of user preference, dialogue strategy, and knowledge access under explicit utility constraints. We address this by unifying slate-aware bandits with retrieval-augmented generation and constrained policy optimization for multi-turn recommendation.",
    "reason": "The span juxtaposes recommendation RL, elicitation, persona dialog, and datasets without explaining how each connects or builds upon the others; the relations are implied but not made explicit, causing coherence issues.",
    "start": 368,
    "end": 709,
    "label": "Coherence"
  },
  {
    "span": "(Alvarez et al., 2018; Kim and Park, 2020;;",
    "document": "Related Work\n\nDomain adaptation for neural machine translation (NMT) has been studied through data selection, fine-tuning, and multi-domain modeling (Chu and Wang, 2018; Koehn and Knowles, 2017). Instance reweighting and regularization can mitigate catastrophic forgetting during adaptation (Thompson et al., 2019; Saunders et al., 2022). Several studies investigate curriculum strategies to order adaptation steps for stability (Wang et al., 2020). In contrastive pretraining, parallel and comparable corpora have been used to align latent spaces across domains (Hassan et al., 2018). Prior work (Alvarez et al., 2018; Kim and Park, 2020;; Lee, 2021) also explores mixture-of-experts decoders to specialize domain-specific parameters while sharing a common encoder.\n\nOur approach differs by jointly optimizing domain routing with uncertainty-aware gating, improving both in-domain quality and out-of-domain retention.",
    "reason": "Extra punctuation in multi-citation: double semicolon before 'Lee, 2021'; should be a single semicolon.",
    "start": 597,
    "end": 640,
    "label": "Format"
  },
  {
    "span": "There are numerous benchmarks for moral reasoning, and most of them rely on crowdworkers from the U.S.",
    "document": "Introduction\n\nMoral reasoning datasets aim to evaluate how models navigate social norms, fairness, and harm. However, the cultural specificity of moral judgments can bias model behavior, raising questions about construct validity and generalizability.\n\nThere are numerous benchmarks for moral reasoning, and most of them rely on crowdworkers from the U.S. This reliance may limit cross-cultural applicability and can embed demographic skews in the data. We propose a multilingual, cross-cultural collection protocol and a validation framework that quantifies agreement across demographic strata.\n\nWe release the dataset and code, along with a set of culturally-aware baselines that highlight differences in moral salience across languages.",
    "reason": "Makes a broad claim about many benchmarks and data collection practices without citing any supporting studies or datasets (violates b and d).",
    "start": 253,
    "end": 355,
    "label": "Unsupported_claim"
  },
  {
    "span": "Transformer-based video captioning models typically fuse visual and textual features with a co-attention mechanism.",
    "document": "Related Work\n\nVideo captioning systems translate visual streams into coherent natural language descriptions. Transformer-based video captioning models typically fuse visual and textual features with a co-attention mechanism. Alternative approaches employ graph reasoning over detected objects and events. We build upon these insights by introducing temporally adaptive attention to handle long-range dependencies.",
    "reason": "Makes a specific methodological claim about typical practice in a niche area without citations; per rule (b), such claims need references.",
    "start": 109,
    "end": 224,
    "label": "Unsupported_claim"
  },
  {
    "span": "Gulwani (2011) presented FlashFill for string transformations from examples. Solar-Lezama (2008) introduced Sketch for synthesizing programs with holes. Ellis et al. (2018) used neural-guided search for program induction. Chen et al. (2021) trained a large language model to generate code from natural language.",
    "document": "Related Work\n\nProgram Synthesis from Examples and Natural Language\nProgram synthesis systems aim to produce correct programs from specifications such as I/O examples or textual intent (Gulwani et al., 2017). Techniques range from constraint solving to stochastic search guided by learned priors.\n\nSymbolic, Neuro-Symbolic, and Neural LMs\nThese approaches differ in the balance between symbolic guarantees and statistical generalization. Gulwani (2011) presented FlashFill for string transformations from examples. Solar-Lezama (2008) introduced Sketch for synthesizing programs with holes. Ellis et al. (2018) used neural-guided search for program induction. Chen et al. (2021) trained a large language model to generate code from natural language.\n\nConstraints and Verification\nRecent efforts integrate type and shape constraints with probabilistic models to prune search and verify candidates (Bunel et al., 2018; Shi et al., 2020). Our work adds executable constraints to steer neural generation at decode time.",
    "reason": "The span is a bare list of citations that are not linked by explicit relationships or transitions (e.g., how Sketch relates to FlashFill or how neural-guided search bridges to LLMs). The reader must infer connections, reducing coherence.",
    "start": 437,
    "end": 748,
    "label": "Coherence"
  },
  {
    "span": "Morris et al. 1",
    "document": "Introduction\n\nCalibrated uncertainty estimates are essential for deploying classifiers in safety-critical applications. While temperature scaling offers a simple post-hoc fix, it may fail under dataset shift. As noted by Morris et al. 1, combining ensembling with data augmentation improves calibration but increases computational cost. We propose a lightweight alternative based on stochastic feature masking.",
    "reason": "Wrong use of footnotes in place of a proper citation; should include the year in author–date style (e.g., 'Morris et al. (YEAR)') or be formatted as a proper footnote.",
    "start": 221,
    "end": 236,
    "label": "Format"
  },
  {
    "span": "(Alvarez et al., 2021;;)",
    "document": "Introduction\n\nUnsupervised pretraining has advanced neural machine translation by enabling better cross-lingual representations (Artetxe et al., 2018; Lample and Conneau, 2019). We adopt multilingual objectives for initialization (Sennrich et al., 2016). For continual learning, we refer to (Alvarez et al., 2021;;) and analyze interference across tasks (Wang and Cho, 2019). Our method reduces forgetting through adaptive rehearsal stratified by language family.",
    "reason": "Extraneous punctuation inside the parenthetical citation: duplicate semicolon before the closing parenthesis. Should be “(Alvarez et al., 2021)”.",
    "start": 291,
    "end": 315,
    "label": "Format"
  },
  {
    "span": "User studies commonly report a 30% increase in task completion with dark mode.",
    "document": "Related Work\nInterface theming has emerged as an important dimension of accessibility and usability. Dark mode, high-contrast palettes, and dynamic theming respond to environmental lighting and user preferences, potentially reducing visual fatigue and improving focus.\n\nUser studies commonly report a 30% increase in task completion with dark mode. Nevertheless, the impact of theme choice can depend on ambient conditions, device luminance, and content type. We analyze the interaction between theme and reading distance on mobile devices, controlling for luminance and font weight.",
    "reason": "A quantitative claim about user studies requires evidence and citations; none are provided (violates b and d).",
    "start": 270,
    "end": 348,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Smith et al. 2020)",
    "document": "Introduction\n\nText-to-SQL mapping seeks to translate natural language questions into executable database queries. Early systems relied on grammars and templates (Zelle and Mooney, 1996; Popescu et al., 2003). Neural sequence-to-sequence models with schema encoding and constrained decoding have since achieved strong results (Dong and Lapata, 2016; Yu et al., 2018; Wang et al., 2020). Coverage of compositional generalization remains limited, with performance degrading on unseen query patterns (Finegan-Dollak et al., 2018; Shaw et al., 2021). Regularization via intermediate representations and structural priors improves robustness (Bogin et al., 2019; Rubin and Berant, 2021; Smith et al. 2020).\n\nSchema linking and context modeling are crucial for multi-turn interaction and real-world databases (Zhang et al., 2019; Suhr et al., 2020). We build on constrained decoding and dynamic slot filling to enhance fidelity and correctness.",
    "reason": "Missing comma between author and year in a parenthetical citation; should be '(Smith et al., 2020)'.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "Gulwani (2011) introduces programming by example for end-users. Rabinovich et al. (2017) propose abstract syntax networks for code generation. Chen et al. (2021) present CodeT5 for code understanding and generation. Austin et al. (2021) study scaling laws for code generation with language models.",
    "document": "Introduction\n\nProgram synthesis from natural language spans inductive, deductive, and neural approaches. Research explores grammars and type systems, neural decoders aligned to abstract syntax trees, and large pretrained models of code.\n\nGulwani (2011) introduces programming by example for end-users. Rabinovich et al. (2017) propose abstract syntax networks for code generation. Chen et al. (2021) present CodeT5 for code understanding and generation. Austin et al. (2021) study scaling laws for code generation with language models.\n\nContemporary work aligns specifications and execution traces for reliability, and benchmarks compositional generalization. Our method integrates execution-guided decoding with constraint-aware prompting for improved correctness.",
    "reason": "The sentences enumerate disparate strands (PBE, AST decoders, pretrained models, scaling laws) with no transitions or explanation of how they connect, weakening coherence.",
    "start": 238,
    "end": 535,
    "label": "Coherence"
  },
  {
    "span": "Predicting user interruptibility has leveraged desktop activity logs, mobile sensor signals, and calendar context to reduce disruption (Fogarty et al., 2005; Pejovic and Musolesi, 2014; Mehrotra et al., 2016). Classifiers trained on notifications, app usage, and motion data show improvements in deferral and dismissal rates across field studies.",
    "document": "Introduction: Timing Notifications for Reduced Disruption\n\nPoorly timed notifications impose cognitive costs and undermine task performance. Understanding when users can accept interruptions can improve both satisfaction and downstream engagement.\n\nPredicting user interruptibility has leveraged desktop activity logs, mobile sensor signals, and calendar context to reduce disruption (Fogarty et al., 2005; Pejovic and Musolesi, 2014; Mehrotra et al., 2016). Classifiers trained on notifications, app usage, and motion data show improvements in deferral and dismissal rates across field studies.\n\nWe design a context-aware scheduler that triggers notifications based on inferred availability using multimodal smartphone signals.",
    "reason": "The span summarizes prior work and results but does not state what remains unresolved or how the new scheduler addresses a specific gap; it lacks synthesis and motivation (criteria b and c).",
    "start": 249,
    "end": 595,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent works demonstrate that reinforcement learning from human feedback significantly improves factuality in abstractive summarization.",
    "document": "Related Work\n\nAbstractive summarization has advanced with pretrained sequence-to-sequence models that learn to compress and paraphrase content. However, hallucinations and factual inconsistencies remain major obstacles for deployment in high-stakes domains. Several approaches target factuality with constrained decoding, faithfulness-oriented losses, and post-hoc correction models.\n\nRecent works demonstrate that reinforcement learning from human feedback significantly improves factuality in abstractive summarization. Parallel efforts explore factuality metrics as training rewards and leverage retrieval to anchor generations in source documents. Despite these advances, aligning rewards with human preferences for faithfulness and readability is still an open challenge.",
    "reason": "Uses the phrase 'recent works' to assert a result without providing citations (definition d).",
    "start": 385,
    "end": 521,
    "label": "Unsupported_claim"
  },
  {
    "span": "To the best of our knowledge, no prior work has evaluated long-form summarization on community-driven encyclopedic articles.",
    "document": "Introduction\n\nNeural text summarization has advanced rapidly with sequence-to-sequence architectures and large-scale pretraining (Rush et al., 2015; See et al., 2017; Nallapati et al., 2016; Liu and Lapata, 2019). Recent methods scale transformers to longer contexts and exploit sparse attention patterns to handle multi-thousand token inputs (Beltagy et al., 2020; Zaheer et al., 2020). In parallel, dataset development has emphasized news and scientific domains where summaries follow conventional styles (Hermann et al., 2015; Narayan et al., 2018; Cohan et al., 2018).\n\nWhile prior work examines summarization of news, legal, and scientific texts, less attention has been given to encyclopedic narratives that exhibit heterogeneous styles, multi-topic structure, and extensive cross-references. To the best of our knowledge, no prior work has evaluated long-form summarization on community-driven encyclopedic articles.\n\nWe propose a benchmark and modeling framework tailored to multi-section encyclopedic documents, focusing on structure-aware content selection and hierarchical decoding. Our contributions complement advances in long-context modeling while foregrounding a distinct and understudied genre.",
    "reason": "Claims novelty relative to prior work without providing any citation or evidence; a sweeping prior-work claim should be supported or scoped with references.",
    "start": 799,
    "end": 923,
    "label": "Unsupported_claim"
  },
  {
    "span": "in (Garcia et al., 2019)",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative model training without centralizing raw data, which is attractive for privacy-sensitive health records (Kairouz et al., 2021). However, medical institutions differ in population demographics and acquisition protocols, creating severe non-IID data that degrades convergence (Li et al., 2020). Prior personalization approaches adjust local heads or re-weight clients (Fallah et al., 2020), but their clinical utility remains unclear. For example, some hospitals maintain on-premise constraints that limit communication rounds, and improvements reported in (Smith and Zhang, 2022) may not hold under these constraints. Moreover, biases introduced during cohort selection can amplify health disparities, as discussed in (Suresh and Guttag, 2019). To bridge this gap, we study parameter-efficient adaptation for FL in (Garcia et al., 2019) oncology imaging cohorts and evaluate across multiple cancer sites.",
    "reason": "Wrong citation style; the preposition should not be inside parentheses. Use in Garcia et al. (2019) or remove the preposition before a parenthetical citation.",
    "start": 870,
    "end": 894,
    "label": "Format"
  },
  {
    "span": "The BioCreative V CDR task defines cross-sentence chemical–disease relation extraction as the standard benchmark for biocuration-oriented IE.",
    "document": "Introduction\n\nBiomedical information extraction (IE) supports curation, hypothesis generation, and knowledge base construction by identifying entities, events, and relations in the literature. Chemical–disease relation (CDR) extraction, in particular, is central to pharmacovigilance and toxicology applications. Models must capture long-range dependencies, resolve coreference, and operate under significant lexical variability across domains and journals. The BioCreative V CDR task defines cross-sentence chemical–disease relation extraction as the standard benchmark for biocuration-oriented IE. Beyond CDR, other shared tasks have focused on gene–disease and chemical–protein interactions, offering complementary evaluation settings. Recent neural approaches combine pretrained biomedical language models with graph reasoning over document-level entity co-mentions, while distant supervision and weak labeling aim to reduce annotation costs. In this paper, we present a document-level encoder that integrates discourse cues and section-specific priors, demonstrating gains on cross-sentence relation extraction with limited supervision.",
    "reason": "Introduces a specific shared task as a standard benchmark without providing a citation; datasets and competitions require citation at first mention (rule a).",
    "start": 458,
    "end": 599,
    "label": "Unsupported_claim"
  },
  {
    "span": "Brown et al.^4",
    "document": "Related Work\n\nDomain adaptation mitigates performance drops when training and test distributions differ (Ben-David et al., 2010; Csurka, 2017). Representation alignment methods minimize discrepancies across domains via adversarial learning or moment matching (Ganin et al., 2016; Long et al., 2015; Tzeng et al., 2017). As in Brown et al.^4, adapter modules can specialize subsets of parameters to new domains with minimal overhead, complementing full fine-tuning (Houlsby et al., 2019; Pfeiffer et al., 2020). Source-free adaptation explores using only the trained model and target data to avoid privacy concerns (Liang et al., 2020; Kundu et al., 2020). Our method integrates test-time entropy minimization with sparse adapters to achieve stable gains under distribution shift (Grandvalet & Bengio, 2005; Sun et al., 2020).",
    "reason": "Wrong use of footnote/superscript notation in an author–year citation; should provide a year (e.g., 'Brown et al. (YEAR)') or use a proper footnote format.",
    "start": 326,
    "end": 340,
    "label": "Format"
  },
  {
    "span": "(Chen 2020 Li et al., 2021)",
    "document": "Related Work\n\nOffline reinforcement learning aims to learn policies from fixed datasets without further environment interaction (Levine et al., 2020; Lange et al., 2012). Conservative objectives and uncertainty penalization prevent exploitation of out-of-distribution actions (Kumar et al., 2020; Fujimoto et al., 2019). Behavior-regularized methods constrain policies toward the dataset support (Wu et al., 2019; Kostrikov et al., 2021). Multiple studies (Chen 2020 Li et al., 2021) show that sequence models over trajectories can yield strong performance with minimal tuning. We extend these insights with a stratified behavior prior that dynamically adjusts to task difficulty.\n",
    "reason": "Missing delimiter between multiple citations inside the same parentheses; a semicolon is required, e.g., “(Chen, 2020; Li et al., 2021)”.",
    "start": 456,
    "end": 483,
    "label": "Format"
  },
  {
    "span": "Zhang et al.",
    "document": "Related Work\n\nAbstractive summarization has advanced rapidly with encoder-decoder architectures and pretrained language models (Tan and Li, 2019; Gao et al., 2020). Earlier neural systems relied on attention mechanisms to copy salient content (See et al., 2017), while more recent approaches leverage large-scale pretraining to improve factual consistency (Kryscinski et al., 2020; Durmus et al., 2020). Despite these gains, domain shift remains a major obstacle when moving from news to scientific or legal documents (Huang and Ren, 2021).\n\nFollowing Zhang et al., we adopt a content planning stage that structures key points before surface realization, thereby reducing redundancy and off-topic generations. Concurrently, controllable summarization explores user-specified constraints such as style or length (Fan et al., 2018; He et al., 2020), yet robustly enforcing constraints without degrading summary quality remains challenging.\n\nOur approach integrates domain-adaptive pretraining (Gururangan et al., 2020) with planning-aware decoding to better preserve salient entities and relations across domains. We evaluate on news, scientific papers, and court opinions, comparing against strong pretrained baselines (Lewis et al., 2020) and planning-free decoders (Rothe et al., 2020).",
    "reason": "Narrative citation missing year; should appear as 'Zhang et al. (YEAR)'.",
    "start": 552,
    "end": 564,
    "label": "Format"
  },
  {
    "span": "Empathetic response generation leverages emotion labels (Rashkin et al., 2019). Safety filters reduce toxic outputs (Gehman et al., 2020). Grounding on knowledge bases improves factual correctness (Dinan et al., 2019).",
    "document": "Introduction\n\nOpen-domain conversational agents must balance helpfulness, safety, and empathy. Prior work explores specialized objectives that target each of these facets, but practical systems require principled integration across them.\n\nEmpathetic response generation leverages emotion labels (Rashkin et al., 2019). Safety filters reduce toxic outputs (Gehman et al., 2020). Grounding on knowledge bases improves factual correctness (Dinan et al., 2019).\n\nOur framework introduces a multi-objective controller that coordinates these signals during decoding, enabling consistent behavior without task-specific fine-tuning.",
    "reason": "The span lists empathy, safety, and grounding literature without transitions or explicit relational statements, resulting in abrupt topic shifts and unclear connections.",
    "start": 239,
    "end": 457,
    "label": "Coherence"
  },
  {
    "span": "the ImageNet challenge drove top-5 error rates below 5% by 2015",
    "document": "Introduction\n\nImage classification progress has been tightly coupled with the availability of large annotated datasets and standardized evaluations. Landmark competitions spurred advances in architectures, training recipes, and data augmentation. In particular, the ImageNet challenge drove top-5 error rates below 5% by 2015, highlighting the effectiveness of deep convolutional networks and large-scale supervision. Subsequent work focused on scaling depth and width, improving optimization stability, and leveraging weakly supervised data. We revisit these trends through the lens of training efficiency and parameter sharing for resource-constrained deployments.\n",
    "reason": "States historical statistics about a well-known benchmark and timeframe without providing supporting citations.",
    "start": 262,
    "end": 325,
    "label": "Unsupported_claim"
  },
  {
    "span": "Miller et al.",
    "document": "Introduction\n\nGraph Neural Networks for Structured Data\n\nGraph neural networks (GNNs) have become the de facto choice for learning on relational data by propagating and aggregating messages across edges (Kipf and Welling, 2017; Hamilton et al., 2017). While message passing achieves strong performance on node classification (Velickovic et al., 2018), capturing hierarchical structure remains challenging. Miller et al. propose differentiable coarsening to induce multiscale representations, whereas Li et al. (2019) introduce hierarchical readouts with gated pooling. Recent advances also explore positional encodings to break over-smoothing and improve expressivity (Dwivedi et al., 2021; Ying et al., 2021).\n",
    "reason": "Narrative citation missing year: 'Miller et al.' should include the publication year, e.g., 'Miller et al. (2020)'.",
    "start": 406,
    "end": 419,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claim that uncertainty sampling quickly plateaus after just 500 labeled sentences.",
    "document": "Introduction\n\nActive learning for machine translation seeks to minimize annotation costs by selecting the most informative sentences for human translation. Common selection strategies include uncertainty sampling, diversity-based selection, and expected model change.\n\nWhile pool-based and streaming settings have both been studied, results can vary widely depending on domain, model capacity, and annotation granularity. In a previous study, the authors claim that uncertainty sampling quickly plateaus after just 500 labeled sentences. This assertion, if generally true, would have major implications for budget allocation in production translation workflows, but the evidence across domains remains mixed.\n\nWe revisit active learning for neural machine translation under realistic labeling constraints, comparing token- and segment-level supervision. We also investigate how calibration and selection frequency affect translation quality and stability. Finally, we introduce a cost-aware evaluation protocol that jointly measures BLEU improvements and annotation time.",
    "reason": "References a prior study and its claim without citing it (violates rule b and a for first mention of prior work).",
    "start": 422,
    "end": 537,
    "label": "Unsupported_claim"
  },
  {
    "span": "Self-supervised visual representation learning has advanced with contrastive methods, clustering-based assignments, and distillation without negatives. Approaches such as momentum contrast, instance discrimination, and teacher-student distillation learn invariances across augmented views. Vision Transformers have been adapted by incorporating patch-level augmentations and token-level objectives.",
    "document": "Related Work\n\nLabel-efficient learning in computer vision relies on pretraining strategies that can transfer to downstream tasks with limited supervision. Recent efforts focus on self-supervised objectives that improve linear probe accuracy and fine-tuning stability.\n\nSelf-supervised visual representation learning has advanced with contrastive methods, clustering-based assignments, and distillation without negatives. Approaches such as momentum contrast, instance discrimination, and teacher-student distillation learn invariances across augmented views. Vision Transformers have been adapted by incorporating patch-level augmentations and token-level objectives.\n\nDownstream applications include detection and segmentation, where pretraining yields improved sample efficiency and robustness to distribution shifts.\n\nWe investigate pretraining regimes with varied compute budgets and data scales.",
    "reason": "The paragraph describes prior techniques but does not connect them to the authors' goals, limitations they observe, or the motivation for their proposed approach.",
    "start": 269,
    "end": 667,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Several recent studies demonstrate that multilingual transformers consistently outperform monolingual models on code-switching tasks.",
    "document": "Related Work\n\nPretrained language models have transformed multilingual NLP by sharing parameters across languages and enabling zero-shot transfer. Models such as BERT and XLM-R have shown strong cross-lingual capabilities on benchmarks spanning POS tagging, NER, and QA. Despite these advances, code-switching remains challenging due to irregular syntax and mixed-language context that violate monolingual assumptions.\n\nSeveral recent studies demonstrate that multilingual transformers consistently outperform monolingual models on code-switching tasks.\n\nBeyond architectural choices, data augmentation and synthetic mixing strategies have been explored to increase robustness to code-switching. Other lines of work examine subword segmentations and tokenization strategies to better capture mixed-language morphology and vocabulary.",
    "reason": "Mentions 'recent studies' and a comparative performance claim without any citations to those studies (rule d and b).",
    "start": 420,
    "end": 553,
    "label": "Unsupported_claim"
  },
  {
    "span": "Chen et al.",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become a standard tool for learning over relational data, enabling state-of-the-art results in node classification and link prediction (Hamilton et al., 2017; Kipf and Welling, 2017; Xu et al., 2019). As shown by Chen et al., deeper architectures suffer from oversmoothing, which hampers node classification in large graphs (Li et al., 2018; Oono and Suzuki, 2020). Recent strategies, such as residual connections and normalization, have been explored to mitigate these issues (Li et al., 2019; Zhao and Akoglu, 2021). In this work, we study the role of neighborhood selection in improving depth scalability.",
    "reason": "Narrative citation missing year; should be formatted as Chen et al. (YEAR), e.g., “Chen et al. (2019)” instead of “Chen et al.”",
    "start": 261,
    "end": 272,
    "label": "Format"
  },
  {
    "span": "Back-translation synthesizes source sentences from monolingual target data (Sennrich et al., 2016). Multilingual pretraining exposes the encoder-decoder to shared subword distributions (Liu et al., 2020; Conneau et al., 2020).",
    "document": "Introduction\n\nLow-resource machine translation (MT) faces data scarcity and domain mismatch, motivating techniques that exploit monolingual corpora and cross-lingual transfer. Recent systems combine synthetic data generation with multilingual encoders to improve generalization.\n\nDenoising autoencoding and dual learning leverage monolingual text to regularize translation models via reconstruction tasks (Lample et al., 2018; He et al., 2016). Iterative back-translation has become a standard approach for bootstrapping from weak seeds (Hoang et al., 2018; Edunov et al., 2018). Back-translation synthesizes source sentences from monolingual target data (Sennrich et al., 2016). Multilingual pretraining exposes the encoder-decoder to shared subword distributions (Liu et al., 2020; Conneau et al., 2020). Domain adaptation further benefits from data selection and fine-tuning on in-domain examples (Chu and Wang, 2018; Aharoni and Goldberg, 2020).\n\nWe present a curriculum that interleaves synthetic data with mined parallel segments guided by a noise-aware scheduler that emphasizes hard examples over training.",
    "reason": "Two consecutive sentences introduce back-translation and multilingual pretraining with no connective explanation of how these strategies interact or relate to the preceding thread, resulting in an abrupt shift.",
    "start": 580,
    "end": 806,
    "label": "Coherence"
  },
  {
    "span": "Ebesu and Fang (2017) proposed neural attentive item-based recommendation. Ying et al. (2018) introduced GraphSAGE-based Pinterest PinSage for web-scale recommender. Wang et al. (2019) presented KGAT to leverage knowledge graphs for top-N recommendation. He et al. (2020) studied LightGCN with simplified propagation.",
    "document": "Related Work\n\nGraph-based Recommendation\n\nGraph neural networks (GNNs) have become a central paradigm for modeling user–item interactions because they propagate information over high-order connectivity. Early matrix factorization approaches capture latent factors but struggle to exploit graph structure at scale. Recent work leverages message passing to encode collaborative signals and structural bias.\n\nEbesu and Fang (2017) proposed neural attentive item-based recommendation. Ying et al. (2018) introduced GraphSAGE-based Pinterest PinSage for web-scale recommender. Wang et al. (2019) presented KGAT to leverage knowledge graphs for top-N recommendation. He et al. (2020) studied LightGCN with simplified propagation.\n\nSelf-Supervision and Regularization\n\nBeyond supervised training on observed interactions, researchers explore contrastive and self-supervised signals to mitigate sparsity and noise. Approaches incorporate dropout on edges, random walks, or auxiliary pretext tasks to stabilize embeddings under perturbations while controlling over-smoothing. Our work builds on these intuitions but focuses on adaptive propagation depth under item cold-start constraints.",
    "reason": "The span lists four cited works in sequence without transitions or explicit relationships. It is unclear how each work relates to the previous one or to a shared theme beyond being GNN recommenders, creating abrupt shifts between sentences (criteria a and b).",
    "start": 406,
    "end": 723,
    "label": "Coherence"
  },
  {
    "span": "Recent architectures fuse audio, visual, and textual signals using early fusion (Zadeh et al., 2018), tensor fusion networks (Zadeh et al., 2017), cross-modal self-attention (Tsai et al., 2019), and graph-based interaction modules (Hazarika et al., 2020). Standard datasets include CMU-MOSI, CMU-MOSEI, and CH-SIMS, which evaluate utterance-level and video-level sentiment and emotion recognition (Zadeh et al., 2016; Yu et al., 2020).",
    "document": "Related Work\n\nMultimodal sentiment analysis leverages complementary cues from speech prosody, facial expressions, and linguistic content to improve robustness over text-only models. The primary challenges involve modeling temporal alignment, modality asynchrony, and cross-modal dynamics.\n\nRecent architectures fuse audio, visual, and textual signals using early fusion (Zadeh et al., 2018), tensor fusion networks (Zadeh et al., 2017), cross-modal self-attention (Tsai et al., 2019), and graph-based interaction modules (Hazarika et al., 2020). Standard datasets include CMU-MOSI, CMU-MOSEI, and CH-SIMS, which evaluate utterance-level and video-level sentiment and emotion recognition (Zadeh et al., 2016; Yu et al., 2020).\n\nEfforts to handle missing modalities explore gating and imputation strategies (Mai et al., 2021), while domain adaptation methods align modality-specific distributions across speakers and recording conditions (Wang et al., 2019).\n\nOur work focuses on learning robust cross-modal representations under modality dropouts at inference time, but we defer methodological details to later sections.",
    "reason": "The span lists models and datasets without linking them to the paper's problem setting or indicating what remains unresolved, showing no synthesis or author stance (criteria a and c).",
    "start": 290,
    "end": 725,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Martinez et al., 2020",
    "document": "Related Work\n\nPolicy optimization in reinforcement learning has been improved by variance-reduced gradients (Schulman et al., 2017; Kakade and Langford, 2002) and trust-region updates. Off-policy corrections with importance sampling often introduce instability that must be controlled with clipping or truncation (Espeholt et al., 2018). Recent attempts to stabilize long-horizon credit assignment include return decomposition and auxiliary value tasks (Martinez et al., 2020 propose a hierarchical critic that shapes the reward at intermediate steps, reducing variance while preserving unbiasedness). Our work complements these approaches by introducing a model-based prior for bootstrapping sparse rewards.\n",
    "reason": "Missing closing parenthesis in the parenthetical citation; should be '(Martinez et al., 2020)'.",
    "start": 453,
    "end": 475,
    "label": "Format"
  },
  {
    "span": "Unsupervised anomaly detection in time series leverages reconstruction errors, probabilistic forecasting, deep generative models, and self-attention (Zong et al., 2018; Malhotra et al., 2016; Lai et al., 2018; Xu et al., 2018; Audibert et al., 2020; Tuli et al., 2022).",
    "document": "Related Work\n\nDetecting anomalies in multivariate time series is crucial for monitoring complex systems such as data centers, manufacturing lines, and sensor networks. Key challenges include nonstationarity, sparse labels, and timely detection with low false alarms.\n\nUnsupervised anomaly detection in time series leverages reconstruction errors, probabilistic forecasting, deep generative models, and self-attention (Zong et al., 2018; Malhotra et al., 2016; Lai et al., 2018; Xu et al., 2018; Audibert et al., 2020; Tuli et al., 2022). Semi-supervised and weakly supervised techniques further exploit limited anomaly tags or event windows when available.\n\nWe present a change-point-calibrated detector that reconciles short-term deviations with longer-horizon regime shifts by combining online segmentation with conformalized residual scoring.",
    "reason": "This span lists categories of methods and citations without relating them to the paper’s approach or identifying what remains unaddressed, so it lacks synthesis.",
    "start": 268,
    "end": 537,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Large-scale vision-language pretraining includes dual-encoder contrastive models and unified encoder-decoder architectures trained on web-scale image-text pairs (Radford et al., 2021; Jia et al., 2021; Li et al., 2022; Alayrac et al., 2022; Yu et al., 2022).",
    "document": "Related Work\n\nMultimodal pretraining\n\nLearning aligned representations across vision and language has led to strong zero-shot and few-shot transfer. Two dominant families are contrastive dual encoders and autoregressive encoder-decoder models, often trained on noisy web-scale corpora.\n\nLarge-scale vision-language pretraining includes dual-encoder contrastive models and unified encoder-decoder architectures trained on web-scale image-text pairs (Radford et al., 2021; Jia et al., 2021; Li et al., 2022; Alayrac et al., 2022; Yu et al., 2022).\n\nData quality and alignment\n\nDespite impressive results, recent work notes that dataset noise and long-tail concepts can impair grounding, and that scaling alone may not resolve fine-grained localization or compositional generalization.\n\nOur perspective\n\nWe focus on improving compositional grounding with targeted curricula and structured negatives that encourage disentanglement of attributes and objects.",
    "reason": "The span provides a list of models and training regimes without clarifying how they inform the proposed approach or what exact limitations motivate it, hence lacking synthesis (criteria a and c).",
    "start": 287,
    "end": 545,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Lopez and Kim, 2018)",
    "document": "Related Work\n\nNeural machine translation (NMT) has evolved from attention-based recurrent architectures to fully transformer-based systems that scale effectively with data and parameters (Singh et al., 2019; Ortega and Xu, 2020). In (Lopez and Kim, 2018), the authors introduced a context-aware decoder that conditions on document-level cues, but subsequent work argued that shallow discourse signals suffice for many domains (Baker and Chen, 2020). Pretraining on multilingual corpora further improves low-resource transfer (Patra et al., 2021), and adapter-based finetuning reduces the cost of domain specialization (Mendes and Roy, 2022).\n\nOur approach differs by leveraging a contrastive retrieval stage that injects sentence-aligned exemplars during decoding (Garcia and Vogel, 2021). We show that retrieval-augmented decoding is complementary to parameter-efficient finetuning, yielding gains in both adequacy and fluency on long-form translation benchmarks (Huang et al., 2022).",
    "reason": "Wrong citation style using a preposition with a parenthetical citation; should be 'In Lopez and Kim (2018)'.",
    "start": 230,
    "end": 254,
    "label": "Format"
  },
  {
    "span": "Plug and Play Language Models (PPLM) modifies hidden activations of the generator during decoding without any fine-tuning.",
    "document": "Related Work\n\nControllable text generation aims to steer language models toward desired attributes without sacrificing fluency. Early approaches focused on training class-conditional generators or adding external control codes. Subsequent methods combined a pretrained generator with an auxiliary signal that biases decoding toward target properties such as sentiment, topic, or toxicity avoidance. Plug and Play Language Models (PPLM) modifies hidden activations of the generator during decoding without any fine-tuning. Other approaches use smaller guiding models or discriminators that score partial continuations and adjust token selection on the fly. Some methods incorporate constraints directly into beam search or sampling mechanisms, while others re-rank candidate sequences with attribute-specific scorers after generation. Despite these advances, many techniques remain sensitive to the balance between attribute strength and semantic coherence, motivating strategies that stabilize control signals during long-horizon generation.\n",
    "reason": "Mentions a specific prior method (PPLM) and makes a claim about how it works without providing a citation at first mention.",
    "start": 399,
    "end": 521,
    "label": "Unsupported_claim"
  },
  {
    "span": "The 2021 RoboNav Challenge standardized the evaluation protocol for long-horizon navigation.",
    "document": "Introduction\n\nEmbodied navigation research aims to endow agents with the ability to reach distant goals using raw sensory inputs. Progress has been catalyzed by photorealistic simulators, large-scale datasets, and standardized tasks such as point-goal and object-goal navigation. However, differences in map fidelity, actuation noise, and success metrics make it difficult to compare methods fairly.\n\nThe 2021 RoboNav Challenge standardized the evaluation protocol for long-horizon navigation. Building on this notion of comparability, we focus on reproducible training curricula and report performance under fixed budgets and identical exploration priors. We additionally analyze failure modes related to partial observability and compounding pose error.",
    "reason": "Cites a specific competition and its impact without providing a reference, which must be cited at first mention per rule (a).",
    "start": 401,
    "end": 493,
    "label": "Unsupported_claim"
  },
  {
    "span": "A large body of work studies graph neural networks for molecular property prediction, including message passing networks, attention-based variants, and higher-order architectures (Kearnes et al., 2016; Duvenaud et al., 2015; Gilmer et al., 2017; Hu et al., 2020; Xu et al., 2019; Ying et al., 2017; Yang et al., 2019).",
    "document": "Related Work\n\nMolecular representation learning\n\nLearning effective molecular representations is central to property prediction, virtual screening, and drug discovery pipelines. Traditional cheminformatics approaches rely on handcrafted descriptors and fingerprints, while recent advances leverage deep learning on molecular graphs to learn task-specific features from structure directly. In this context, graph neural networks (GNNs) have emerged as a dominant paradigm due to their inductive bias for message passing over atoms and bonds.\n\nA large body of work studies graph neural networks for molecular property prediction, including message passing networks, attention-based variants, and higher-order architectures (Kearnes et al., 2016; Duvenaud et al., 2015; Gilmer et al., 2017; Hu et al., 2020; Xu et al., 2019; Ying et al., 2017; Yang et al., 2019).\n\nPretraining strategies\n\nBeyond supervised learning, self-supervised pretraining on large unlabeled molecular corpora has been shown to improve data efficiency. Contrastive objectives, masked node/edge prediction, and graph-level denoising are commonly adopted (Sun et al., 2020; Hu et al., 2020). These strategies aim to imbue encoders with transferable inductive biases before fine-tuning on downstream endpoints.\n\nLimitations in robustness and calibration\n\nDespite accuracy gains, recent analyses highlight robustness and calibration concerns in GNN-based predictors, especially under distribution shift induced by scaffold split, synthetic accessibility constraints, or assay variations (Grinsztajn et al., 2022; Packalen et al., 2021). Addressing these gaps requires training objectives and architectures that explicitly consider uncertainty and out-of-domain generalization.",
    "reason": "The span lists prior GNN approaches with citations but does not connect them to the paper’s aims, articulate a gap, or explain how they relate to the authors’ method (criteria a and c).",
    "start": 542,
    "end": 860,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Time-series anomaly detection methods span classical statistical tests, density and isolation-based models, and deep generative approaches such as VAEs and normalizing flows (Chandola et al., 2009; Liu et al., 2008; Malhotra et al., 2016; Xu et al., 2018; Zhang et al., 2019).",
    "document": "Related Work\n\nAnomaly detection in time series. Application domains include industrial monitoring, finance, and healthcare, where anomalies are rare and labels scarce. Time-series anomaly detection methods span classical statistical tests, density and isolation-based models, and deep generative approaches such as VAEs and normalizing flows (Chandola et al., 2009; Liu et al., 2008; Malhotra et al., 2016; Xu et al., 2018; Zhang et al., 2019). Reconstruction- and prediction-based neural methods have also been explored for multivariate data.\n\nLabel efficiency. Semi-supervised and weakly supervised strategies aim to leverage abundant unlabeled logs while minimizing annotation cost.\n\nWe propose a contrastive pretraining objective aligned with seasonal-cyclical structure, followed by calibrated one-class fine-tuning.\n",
    "reason": "The span lists categories and citations without integrating them or explaining their relation to label efficiency or the proposed approach; it does not articulate the authors’ motivation or argument.",
    "start": 168,
    "end": 444,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There are many recent works on prompt tuning for low-resource NER, but they mostly evaluate on English-only settings.",
    "document": "Introduction\n\nNamed entity recognition (NER) has benefited substantially from pretrained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). Prompt-based learning adapts pretrained models using task-specific templates or verbalizers instead of full fine-tuning, aiming to better exploit pretraining priors. There are many recent works on prompt tuning for low-resource NER, but they mostly evaluate on English-only settings. Despite advances in multilingual encoders (Conneau and Lample, 2019), cross-lingual NER remains challenging when annotations are scarce and label distributions shift across domains. In this paper, we investigate prompt selection and calibration strategies that transfer across languages with only a handful of labeled examples per class. We evaluate on typologically diverse languages and compare against standard fine-tuning baselines under strict budget constraints.",
    "reason": "Mentions 'recent works' without providing citations to those works (rule d).",
    "start": 338,
    "end": 455,
    "label": "Unsupported_claim"
  },
  {
    "span": "For traffic forecasting with road networks, graph-based models have been widely explored, including diffusion convolutional architectures, spatio-temporal graph convolutions, and attention-based graph transformers (Li et al., 2018; Yu et al., 2018; Guo et al., 2019; Wu et al., 2020).",
    "document": "Related Work\n\nTraffic forecasting. Accurate traffic forecasting requires modeling complex spatio-temporal dependencies and exogenous factors. Early models rely on ARIMA and Kalman filtering, which struggle with nonlinearity and network effects.\n\nGraph neural networks. For traffic forecasting with road networks, graph-based models have been widely explored, including diffusion convolutional architectures, spatio-temporal graph convolutions, and attention-based graph transformers (Li et al., 2018; Yu et al., 2018; Guo et al., 2019; Wu et al., 2020). External-variable fusion and adaptive adjacency learning have been proposed to improve flexibility across cities.\n\nDespite progress, transferring models across heterogeneous urban layouts and sensor coverages is difficult. We propose a meta-learned adapter that aligns latent node representations across cities with minimal calibration data.\n",
    "reason": "The span enumerates prior graph-based models with citations but does not connect them to the specific transfer challenge or clarify how they motivate the proposed meta-learned adapter, thus lacking synthesis.",
    "start": 269,
    "end": 553,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Reconstruction-based detectors use autoencoders, variational autoencoders, and generative adversarial networks to model normal patterns and flag deviations (An and Cho, 2015; Zenati et al., 2018; Park et al., 2018).",
    "document": "Related Work\n\nTime-series anomaly detection is central to monitoring industrial systems, finance, and IT operations. Methods are commonly categorized into reconstruction-based models, predictive models, and statistical tests, each with distinct strengths and weaknesses.\n\nReconstruction-based detectors use autoencoders, variational autoencoders, and generative adversarial networks to model normal patterns and flag deviations (An and Cho, 2015; Zenati et al., 2018; Park et al., 2018). Predictive models instead forecast the next step or segment and treat large residuals as anomalies (Hundman et al., 2018; Malhotra et al., 2015). Hybrid approaches incorporate seasonality decomposition and attention mechanisms to capture long- and short-term dynamics (Wu et al., 2020; Senin and Malinchik, 2013).\n\nRecent work on multivariate settings explores graph structures and causal discovery to disentangle inter-series dependencies (Liu et al., 2019; Deng and Hooi, 2021). Evaluation remains challenging due to label sparsity and varying tolerance for detection delays across applications.\n\nOverall, the literature reflects a wide spectrum of modeling choices tailored to different anomaly characteristics and operational constraints.",
    "reason": "The span lists categories and citations but does not synthesize how these relate to the paper’s objectives, failing to state the gap or the authors’ motivation.",
    "start": 272,
    "end": 487,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Bias mitigation in NLP encompasses data balancing, adversarial learning, and calibration-based post-processing across tasks such as sentiment analysis and coreference (Zhao et al., 2017; Zhang et al., 2018; Webster et al., 2018; Ravfogel et al., 2020).",
    "document": "Introduction\n\nEnsuring fairness in NLP is critical for responsible deployment, as models can amplify societal biases present in data. Bias can manifest across demographic attributes, propagate through pretraining, and surface in downstream applications. Bias mitigation in NLP encompasses data balancing, adversarial learning, and calibration-based post-processing across tasks such as sentiment analysis and coreference (Zhao et al., 2017; Zhang et al., 2018; Webster et al., 2018; Ravfogel et al., 2020). Prior work also explores counterfactual data augmentation and debiasing objectives during representation learning (Maudslay et al., 2019; Zmigrod et al., 2019; Lauscher et al., 2021).\n\nIn this paper, we study fairness-aware evaluation under distribution shift and propose a protocol for measuring disparate impact across domains.",
    "reason": "The span summarizes mitigation strategies with citations but does not articulate how they connect to the authors' perspective or what gap motivates their study.",
    "start": 254,
    "end": 506,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Model compression techniques for efficient NLP include pruning (Han et al., 2015), quantization (Jacob et al., 2018), knowledge distillation (Hinton et al., 2015; Sanh et al., 2019), parameter sharing (Lan et al., 2020), and neural architecture search (So et al., 2019). Based on these directions, we design a lightweight Transformer for edge devices.",
    "document": "Introduction\n\nDeploying NLP models on edge devices requires balancing accuracy with latency, memory, and energy constraints. Recent research has produced a wide array of compression and efficiency techniques.\n\nModel compression techniques for efficient NLP include pruning (Han et al., 2015), quantization (Jacob et al., 2018), knowledge distillation (Hinton et al., 2015; Sanh et al., 2019), parameter sharing (Lan et al., 2020), and neural architecture search (So et al., 2019). Based on these directions, we design a lightweight Transformer for edge devices.\n\nWe evaluate throughput, latency, and accuracy on on-device benchmarks and compare to baseline compact models.",
    "reason": "The span lists prior approaches and immediately states the authors’ contribution without explaining what gap remains or how the design addresses it, demonstrating a lack of synthesis.",
    "start": 210,
    "end": 561,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Chen, 20)",
    "document": "Related Work\n\nSelf-supervised learning for speech has progressed rapidly with predictive masking and contrastive objectives. CPC learns representations by predicting future latents (Oord et al., 2018), and wav2vec 2.0 improves data efficiency with masked acoustic modeling (Baevski et al., 2020). HuBERT further stabilizes learning using iterative target label discovery (Hsu et al., 2021).\n\nRecent work explores multi-task pretraining that unifies ASR and spoken language understanding (Qian et al., 2021), as well as cross-lingual transfer via shared acoustic units (Conneau et al., 2021). We differ by studying curriculum masking schedules inspired by findings in (Chen, 20) on stage-wise difficulty for visual pretraining.",
    "reason": "Year is truncated in the parenthetical citation. Should be a complete year, e.g., '(Chen, 2020)'.",
    "start": 667,
    "end": 677,
    "label": "Format"
  },
  {
    "span": "As of 2024, more than 50 benchmarks evaluate fairness in vision-language models.",
    "document": "Introduction\n\nFairness in vision-language models (VLMs) has emerged as a critical topic as these systems are increasingly deployed in high-stakes applications. Prior work documents biases along demographic attributes, cultural contexts, and geographic distributions in both pretraining data and downstream tasks (Buolamwini and Gebru, 2018; Zhao et al., 2017; Bender et al., 2021). To mitigate these harms, researchers have proposed data curation pipelines, debiasing objectives, and post-hoc calibration methods (Zhang et al., 2018; Wang et al., 2020; Ravfogel et al., 2020).\n\nAs of 2024, more than 50 benchmarks evaluate fairness in vision-language models. Despite this breadth, evaluation coverage remains uneven, with limited attention to multilingual and low-resource settings. We present WorldFair-VL, a benchmark spanning 30 languages and 12 demographic axes with carefully curated counterfactual pairs. Our analysis reveals systematic disparities that persist even in models trained with balanced sampling.",
    "reason": "Presents a precise statistic about the number of benchmarks without any supporting citation.",
    "start": 578,
    "end": 658,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Khan et al., 2018)",
    "document": "Related Work\n\nFederated learning enables on-device training without centralizing data (McMahan et al., 2017; Kairouz et al., 2021). Personalization strategies adapt global models to heterogeneous clients (Arivazhagan et al., 2019; Collins et al., 2021). In (Khan et al., 2018) a taxonomy of attacks on FL is introduced, motivating defenses based on robust aggregation (Blanchard et al., 2017; Pillutla et al., 2022). However, existing defenses often assume benign clients substantially outnumber adversaries, which may not hold in open federations.\n\nOur work complements robust aggregation by incorporating client-level uncertainty into the optimization objective, reducing susceptibility to targeted poisoning.",
    "reason": "Wrong citation style with parenthetical citation after a preposition; should be “in Khan et al. (2018)”.",
    "start": 254,
    "end": 276,
    "label": "Format"
  },
  {
    "span": "wav2vec 2.0 learns representations from raw audio via contrastive learning (Baevski et al., 2020). CPC captures predictive signals in speech (van den Oord et al., 2018). RNN-T unifies acoustic and language modeling for streaming ASR (Graves, 2012). SpecAugment improves training robustness (Park et al., 2019).",
    "document": "Related Work\n\nSelf-Supervised and End-to-End Speech Recognition\n\nSelf-supervised pretraining has advanced automatic speech recognition (ASR) by leveraging large unlabeled corpora. End-to-end architectures reduce hand-engineered components and enable joint optimization.\n\nPretraining and Architectures\n\nwav2vec 2.0 learns representations from raw audio via contrastive learning (Baevski et al., 2020). CPC captures predictive signals in speech (van den Oord et al., 2018). RNN-T unifies acoustic and language modeling for streaming ASR (Graves, 2012). SpecAugment improves training robustness (Park et al., 2019).\n\nDomain Adaptation\n\nAdapting to accents and noisy channels benefits from pseudo-labeling and test-time augmentation (Kahn et al., 2020; Li et al., 2021). Our method studies low-resource pretraining under compute budgets.",
    "reason": "The paragraph lists diverse methods (self-supervised pretraining, transducer architecture, data augmentation) without transitions or explicit connections, making the relevance between consecutive sentences unclear.",
    "start": 302,
    "end": 612,
    "label": "Coherence"
  },
  {
    "span": "Autoregressive models capture temporal dependencies in stationary series (Box and Jenkins, 1970). Isolation Forest detects anomalies by random partitioning (Liu et al., 2008). Prophet provides scalable forecasting with additive components (Taylor and Letham, 2018). GAN-based reconstruction approaches flag deviations as anomalies (Zenati et al., 2019).",
    "document": "Introduction\n\nTime Series Anomaly Detection. Detecting anomalous behavior in univariate and multivariate time series is critical for reliability in industrial systems, finance, and healthcare. Approaches span forecasting residual analysis, density estimation, and reconstruction-based methods, each with trade-offs in sample efficiency and robustness (Blázquez-García et al., 2021).\n\nClassical and Deep Methods. Autoregressive models capture temporal dependencies in stationary series (Box and Jenkins, 1970). Isolation Forest detects anomalies by random partitioning (Liu et al., 2008). Prophet provides scalable forecasting with additive components (Taylor and Letham, 2018). GAN-based reconstruction approaches flag deviations as anomalies (Zenati et al., 2019). We focus on probabilistic sequence models that unify forecasting and uncertainty quantification to yield calibrated anomaly scores under regime shifts.",
    "reason": "The paragraph lists unrelated methods (AR models, Isolation Forest, Prophet, GANs) with no transitions or explanations of how they relate to each other within anomaly detection, resulting in abrupt, unconnected sentences.",
    "start": 412,
    "end": 765,
    "label": "Coherence"
  },
  {
    "span": "BERT was used in an AES task trained on essays from the ASAP dataset",
    "document": "Introduction\n\nAutomatic essay scoring (AES) seeks to predict human-assigned scores for student writing. Neural approaches have recently replaced hand-crafted features by leveraging contextual encoders and prompt-aware architectures. Despite these advances, the community still struggles with cross-prompt generalization and robustness to adversarial edits.\n\nRelated Work\n\nEarlier systems relied on surface and discourse features, while neural models incorporate pre-trained language models to capture semantics and coherence. BERT was used in an AES task trained on essays from the ASAP dataset, demonstrating strong performance under in-domain evaluation but limited transfer across prompts. Subsequent work explored contrastive objectives to align representations of high- and low-scoring essays, and calibration techniques to mitigate variance across prompts.",
    "reason": "Claims a specific prior setup and result without citing the corresponding study, as per rule (b) and example (iii).",
    "start": 526,
    "end": 594,
    "label": "Unsupported_claim"
  },
  {
    "span": "Ganin and Lempitsky (2015) proposed domain-adversarial training to learn invariant features. Zou et al. (2019) introduced confidence regularization for self-training. Huang et al. (2018) explored image-to-image translation for domain alignment. Sun et al. (2016) studied correlation alignment to reduce domain shift.",
    "document": "Related Work\n\nUnsupervised domain adaptation aims to transfer models from a labeled source domain to an unlabeled target domain by mitigating the covariate shift between distributions (Pan and Yang, 2010; Patel et al., 2015). Methods include discrepancy minimization, adversarial alignment, and generative translation approaches (Tzeng et al., 2017; Long et al., 2015).\n\nGanin and Lempitsky (2015) proposed domain-adversarial training to learn invariant features. Zou et al. (2019) introduced confidence regularization for self-training. Huang et al. (2018) explored image-to-image translation for domain alignment. Sun et al. (2016) studied correlation alignment to reduce domain shift.\n\nRecent work considers semi-supervised targets and test-time adaptation with entropy minimization (Liang et al., 2020; Wang et al., 2021). Our contribution combines pseudo-label calibration with feature-level invariance and uncertainty-aware selection to stabilize training under severe class imbalance.",
    "reason": "The cited works are listed without articulating how adversarial training relates to self-training, translation, or correlation alignment, and no transitions clarify their interdependence or comparative roles.",
    "start": 371,
    "end": 687,
    "label": "Coherence"
  },
  {
    "span": "According to industry reports, more than 70% of call centers now deploy end-to-end ASR.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end models that directly map audio to text. These end-to-end architectures simplify training and enable rapid adaptation to new domains. According to industry reports, more than 70% of call centers now deploy end-to-end ASR. Despite widespread adoption, performance degradation persists in accented speech and noisy environments, prompting interest in self-supervised pretraining and domain-specific adaptation. We study a noise-robust finetuning pipeline with uncertainty-aware decoding tailored for contact-center data.",
    "reason": "Presents a quantitative statistic attributed to 'industry reports' without citing any source.",
    "start": 241,
    "end": 328,
    "label": "Unsupported_claim"
  },
  {
    "span": "[Smith et al., 2020)",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative training without centralizing data (Kairouz et al., 2021). Personalization approaches adapt global models to client-specific distributions (Arivazhagan et al., 2019), while compression reduces communication overhead [Smith et al., 2020) and preserves accuracy under bandwidth constraints (Sattler et al., 2019). Robust aggregation remains crucial to defend against adversaries and noise (Blanchard et al., 2017).",
    "reason": "Mismatched brackets in the citation: opening square bracket with a closing parenthesis. It should consistently use parentheses, e.g., “(Smith et al., 2020)”, or consistently use square brackets if the style requires it.",
    "start": 276,
    "end": 296,
    "label": "Format"
  },
  {
    "span": "In (Smith et al., 2019)",
    "document": "Related Work\n\nDomain adaptation. In (Smith et al., 2019) a benchmark was introduced for cross-domain sequence labeling with a unified evaluation protocol. Subsequent efforts improved feature alignment using adversarial training (Ganin and Lempitsky, 2015) and optimal transport-based objectives (Courty et al., 2017), but these approaches often require careful tuning and can suffer from mode collapse.\n\nMeta-learning. Gradient-based meta-learners (Finn et al., 2017) have been adapted to few-shot NLP tasks, but their sensitivity to task heterogeneity remains a challenge. Our study complements this line by focusing on robust pretraining priors that reduce task-specific overfitting.",
    "reason": "Wrong citation style: narrative citation is incorrectly placed inside parentheses after 'In'; it should be 'In Smith et al. (2019)'.",
    "start": 33,
    "end": 56,
    "label": "Format"
  },
  {
    "span": "(Vaswani et al., 2017)",
    "document": "Related Work\n\nNeural sequence modeling has progressed rapidly with the advent of attention-based architectures. Early encoder-decoder models relied on recurrent networks with attention mechanisms to align source and target tokens (Bahdanau et al., 2014). The introduction of large-scale pretraining further boosted performance across tasks such as translation and summarization (Devlin et al., 2019; Lewis et al., 2020). In (Vaswani et al., 2017), the Transformer architecture dispenses with recurrence entirely and relies on multi-head self-attention, enabling efficient parallelization and strong empirical results. Subsequent work explores scaling laws and training dynamics for ever-larger models (Kaplan et al., 2020; Hoffmann et al., 2022). We build on these insights to study efficient finetuning under resource constraints.",
    "reason": "Wrong citation style: the citation is used after the preposition 'In' as a parenthetical. It should be a narrative citation like 'In Vaswani et al. (2017)'.",
    "start": 424,
    "end": 446,
    "label": "Format"
  },
  {
    "span": "Hinton et al.",
    "document": "Introduction\n\nRepresentation learning seeks to acquire general-purpose features that transfer across tasks and domains. Self-supervised methods have become especially prominent in vision and language due to their scalability (Chen et al., 2020; He et al., 2020). A central concept is distillation, where a student mimics a stronger teacher model to improve sample efficiency. As shown by Hinton et al., knowledge distillation can compress large networks while preserving accuracy, influencing subsequent advances in model deployment (Gou et al., 2021). In this work, we revisit distillation from a calibration perspective and propose a temperature-scheduled objective for robust transfer.",
    "reason": "Narrative citation missing the year; should be formatted as 'Hinton et al. (2015)' or similar.",
    "start": 388,
    "end": 401,
    "label": "Format"
  },
  {
    "span": "Datasets such as IEMOCAP, RAVDESS, and EMODB provide acted speech with categorical labels, while features like MFCCs, prosodic statistics, and spectrogram-based embeddings have been widely explored (Busso et al., 2008; Livingstone and Russo, 2018).",
    "document": "Introduction\n\nSpeech emotion recognition (SER) aims to infer affective states from acoustic signals and has applications in assistive technologies and conversational agents. Performance in-the-wild remains limited by domain shift, speaker variability, and label subjectivity.\n\nDatasets such as IEMOCAP, RAVDESS, and EMODB provide acted speech with categorical labels, while features like MFCCs, prosodic statistics, and spectrogram-based embeddings have been widely explored (Busso et al., 2008; Livingstone and Russo, 2018).\n\nWe propose a contrastive learning framework that aligns acoustic cues with text-derived emotion prototypes and introduces uncertainty-aware calibration to handle ambiguous utterances, improving robustness across speakers and domains.",
    "reason": "Lists datasets and feature types without explaining implications for the current problem or connecting them to the proposed contrastive framework.",
    "start": 277,
    "end": 525,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Khan et al., 2018)",
    "document": "Related Work\n\nActive domain adaptation has been explored through uncertainty sampling and representation alignment. In (Khan et al., 2018), the authors propose a curriculum that progressively shifts sampling from high- to low-uncertainty regions to stabilize training. Subsequent research combines alignment losses with pseudo-label refinement (Rodriguez et al., 2019) and demonstrates improved robustness across multiple domains (Shen & Park, 2020). More recent methods integrate self-training with confidence calibration to reduce confirmation bias (Taylor et al., 2021).\n\nComplementary directions investigate semi-supervised techniques that leverage consistency regularization (Miyato et al., 2018; Xie et al., 2020) and task-specific priors (Liu & Chen, 2019), enabling competitive performance with fewer labeled samples.",
    "reason": "Wrong citation style: a parenthetical citation is incorrectly used after the preposition \"In\"; it should be a narrative form such as \"In Khan et al. (2018)\".",
    "start": 119,
    "end": 138,
    "label": "Format"
  },
  {
    "span": "In recent years, numerous studies have shown that prompt-tuning consistently outperforms full fine-tuning on low-resource classification.",
    "document": "Introduction\n\nPrompt-based learning has reoriented how practitioners adapt large language models to downstream tasks by aligning inputs with pretraining objectives. Rather than updating all model parameters, prompt-tuning introduces small task-specific tokens or lightweight adapters, aiming to preserve generalization while reducing computational cost. In recent years, numerous studies have shown that prompt-tuning consistently outperforms full fine-tuning on low-resource classification. Despite the popularity of these methods, there remain open questions about stability, data efficiency, and transfer across domains and label spaces.\n\nIn this paper, we examine the sensitivity of prompt-based methods to distributional shift and annotation sparsity. We analyze a suite of tasks that vary in label granularity and domain specificity and compare prompt-tuning, prefix-tuning, and full fine-tuning under a unified experimental protocol. We further study how prompt initialization and verbalizer selection affect performance and whether improvements hold across model scales and backbone architectures.",
    "reason": "Claims support from 'numerous studies' and a consistent performance advantage without providing any citations to prior work.",
    "start": 354,
    "end": 491,
    "label": "Unsupported_claim"
  },
  {
    "span": "Reinforcement learning for task-oriented dialogue has employed value-based, policy-gradient, and actor–critic methods to optimize success and user satisfaction (Li et al., 2016; Dhingra et al., 2017; Su et al., 2017; Peng et al., 2018).",
    "document": "Related Work\n\nDialogue policy learning. Task-oriented dialogue systems must balance exploration with user experience. Reinforcement learning for task-oriented dialogue has employed value-based, policy-gradient, and actor–critic methods to optimize success and user satisfaction (Li et al., 2016; Dhingra et al., 2017; Su et al., 2017; Peng et al., 2018). Simulation-based training with user simulators is also common.\n\nSample efficiency and safety. Off-policy learning and constrained optimization have been proposed to reduce risky exploration and data demands.\n\nWe propose a conservative Q-learning framework with uncertainty-aware action masking for safer online adaptation.\n",
    "reason": "The span lists families of RL methods and citations but does not connect them to sample efficiency or safety concerns, nor does it identify what remains unsolved; thus it lacks synthesis and author perspective.",
    "start": 118,
    "end": 354,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Nguyen et al., 2020",
    "document": "Introduction\n\nTask-oriented dialogue systems have steadily advanced with neural architectures that model dialogue state, policy, and generation jointly (Henderson, 2014; Young et al., 2013; Wen et al., 2017). Large pre-trained language models have further improved few-shot adaptation and robustness to domain shift (Zhang et al., 2020; Peng et al., 2021). Nevertheless, grounding utterances in external knowledge remains challenging, particularly when knowledge sources are dynamic or partially observed (Eric and Manning, 2017; Dinan et al., 2018).\n\nA prominent line of work integrates retrieval with generation to support knowledge-grounded responses (Lewis et al., 2020; Karpukhin et al., 2020). While end-to-end training helps align retrieved passages with response tokens, selection errors propagate and degrade fluency and factuality (Roller et al., 2021). Recent modular approaches decouple retrieval and planning to mitigate such errors (Gao et al., 2021; Shuster et al., 2021). In contrast, schema-guided systems rely on explicit ontologies to constrain interactions and reduce ambiguity (Rastogi et al., 2020; Ouyang et al., 2020), but suffer from limited coverage and costly maintenance.\n\nOur work examines retrieval-planning decoupling with adaptive reasoning over multi-hop evidence in goal-oriented settings. We position our method relative to memory-augmented models (Sukhbaatar et al., 2015), knowledge infusion via adapters (Pfeiffer et al., 2021), and contrastive filtering of distractors (Izacard and Grave, 2021). Prior evaluations often conflate retrieval accuracy with dialogue success, obscuring where failures occur (Zamani et al., 2020). To address this, we introduce a suite of diagnostics targeting retrieval, planning, and generation. Unlike prior multi-hop dialogue datasets (Nguyen et al., 2020, we annotate explicit evidence chains and failure modes at each stage, enabling clearer attribution and ablations.",
    "reason": "Missing closing parenthesis in a parenthetical citation; it should be '(Nguyen et al., 2020)'.",
    "start": 1805,
    "end": 1825,
    "label": "Format"
  },
  {
    "span": "Recent work has shown that BERT can be directly fine-tuned for automated essay scoring on high school essays.",
    "document": "Related Work\n\nAutomated essay scoring has evolved from handcrafted feature engineering toward neural architectures that learn holistic representations of writing quality. Early systems relied on surface features such as length, syntactic variety, and error counts (Shermis and Burstein, 2013). Subsequent neural approaches modeled essays with recurrent encoders and attention mechanisms to capture discourse-level signals (Taghipour and Ng, 2016; Dong and Zhang, 2016).\n\nRecent work has shown that BERT can be directly fine-tuned for automated essay scoring on high school essays. In parallel, domain adaptation techniques have been explored to transfer scoring models across prompts and genres (Jin et al., 2018), while calibration strategies aim to align neural predictions with human scoring rubrics (Mayfield and Black, 2019). Beyond predictive accuracy, fairness and interpretability are increasingly emphasized, with efforts to identify bias with respect to demographics and writing topics (Hussein et al., 2020).\n\nOur contribution builds on these trends by investigating prompt-agnostic fine-tuning under strict data constraints. We compare transformer backbones and study the impact of rubric-aware pretraining on model robustness and generalization across prompts.",
    "reason": "Mentions recent work and a specific setup of BERT for AES without providing any citation at first mention.",
    "start": 471,
    "end": 580,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Zhang et al., 2018)",
    "document": "Related Work\n\nNeural machine translation (NMT) shifted from phrase-based pipelines to end-to-end sequence models that learn continuous representations jointly (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). In (Zhang et al., 2018) we find that shared subword vocabularies can mitigate data sparsity in multilingual settings, complementing parameter sharing strategies (Johnson et al., 2017; Firat et al., 2016). Domain adaptation for NMT often relies on shallow fusion and fine-tuning (Chu & Wang, 2018; Freitag & Al-Onaizan, 2016), while recent work explores meta-learning to quickly adapt to new domains (Gu et al., 2018; Dou et al., 2019). Our approach integrates uncertainty-aware adapters with test-time inference strategies to improve robustness under domain shift (Kahn et al., 2020; Michel & Neubig, 2018).",
    "reason": "Wrong citation style: preposition placed before a parenthetical citation; should be 'In Zhang et al. (2018)' or rephrased to remove 'In' before '(Zhang et al., 2018)'.",
    "start": 230,
    "end": 253,
    "label": "Format"
  },
  {
    "span": "(Lee and Kim, 2021",
    "document": "Introduction\n\nFew-shot event extraction seeks to recognize event types and arguments with only a handful of labeled examples. Metric learning and prototypical representations have proven effective when annotated data is scarce (Snell et al., 2017;Sun et al., 2019), and pretraining further improves transferability (Devlin et al., 2019;Raffel et al., 2020).\n\nWe build upon a prototype-based classifier similar to (Lee and Kim, 2021 to encode event schemas and enable rapid adaptation. Unlike prior prompt-based tuning (Liu et al., 2021), we incorporate structure-aware encoders that capture trigger-argument dependencies.",
    "reason": "Missing closing parenthesis in the parenthetical citation; should be '(Lee and Kim, 2021)'.",
    "start": 413,
    "end": 431,
    "label": "Format"
  },
  {
    "span": "The Electricity dataset contains 321 customers and two years of hourly data",
    "document": "Introduction\n\nMultivariate time-series forecasting underpins resource allocation in energy systems and retail demand planning. The Electricity dataset contains 321 customers and two years of hourly data, making it a common benchmark for autoregressive and transformer-based forecasters. Seq2seq models with attention have improved long-horizon accuracy (Lai et al., 2018; Lim et al., 2021), yet capturing calendar effects and regime shifts remains challenging. We propose a probabilistic forecaster that adapts seasonality via learned time embeddings.",
    "reason": "Presents specific statistics about a benchmark dataset without providing a citation.",
    "start": 127,
    "end": 202,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Chen et al.) 2018",
    "document": "Related Work\n\nFairness in recommender systems has expanded from user-level parity to provider and item exposure equity. Early work measured disparate impact in ranking outcomes (Zehlike et al., 2017). Re-ranking methods adjust exposure while preserving utility (Singh and Joachims, 2018). Counterfactual evaluation techniques estimate fairness metrics without full-information logs (Wang et al., 2020). Representation learning approaches mitigate bias in user and item embeddings (Beutel et al., 2019). As argued by (Chen et al.) 2018, fairness constraints can be integrated directly into learning-to-rank objectives. We extend these ideas with a calibration-aware exposure regularizer that maintains relevance while equalizing opportunity across providers (Liu and Park, 2021).",
    "reason": "Year is placed outside the parentheses and separated from the authors; should be either 'Chen et al. (2018)' for narrative or '(Chen et al., 2018)' for parenthetical style.",
    "start": 516,
    "end": 534,
    "label": "Format"
  },
  {
    "span": "((Klein, 2020)",
    "document": "Introduction\n\nEvent extraction systems must disambiguate triggers and link arguments under limited supervision (Ahn, 2006; Ji and Grishman, 2008). Pretrained encoders with span-based decoders have recently improved cross-domain robustness (Wadden et al., 2019; Lin et al., 2020). We study a low-resource regime with distant supervision and propose a consistency-regularized training loop. Prior work on robust decoding proposes constrained Viterbi and calibration layers ((Klein, 2020) to mitigate overconfident predictions in ambiguous contexts.",
    "reason": "Redundant opening parenthesis before a citation.",
    "start": 471,
    "end": 485,
    "label": "Format"
  },
  {
    "span": "The LibriSpeech dataset contains around 1,000 hours of read English speech and is widely used for training and evaluation.",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) systems have benefited from larger models and self-supervised pretraining, reducing word error rates on standard benchmarks (Baevski et al., 2020; Gulati et al., 2020). Resource availability and domain coverage of training corpora strongly influence downstream performance and robustness. The LibriSpeech dataset contains around 1,000 hours of read English speech and is widely used for training and evaluation. However, read audio differs from conversational or spontaneous settings, prompting interest in domain adaptation and multi-domain training (Park et al., 2019). We investigate how a small amount of conversational data can be leveraged to improve robustness without sacrificing accuracy on read speech.",
    "reason": "States a specific statistic and references a well-known dataset at first mention without citation or evidence.",
    "start": 349,
    "end": 471,
    "label": "Unsupported_claim"
  },
  {
    "span": "The widely used XQA dataset contains over 2 million question–answer pairs.",
    "document": "Introduction\n\nOpen-domain question answering relies heavily on large-scale datasets that capture diverse knowledge and query styles. Benchmark selection affects not only accuracy but also the generalization profile of models across domains and answer types. The widely used XQA dataset contains over 2 million question–answer pairs. Despite its size, XQA exhibits long-tail phenomena and compositional questions that challenge retrieval and reasoning. We analyze these properties and propose training strategies that improve robustness without sacrificing efficiency.",
    "reason": "Claims a specific dataset statistic without citing the dataset or a source supporting the number.",
    "start": 258,
    "end": 332,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Li et al., 2018)",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become a standard tool for representation learning on relational data. In (Li et al., 2018) the authors interpret recurrent GNN updates as approximate inference, while later work proposes residual propagation schemes (Klicpera et al., 2019). Despite these advances, oversmoothing remains a persistent challenge (Oono and Suzuki, 2020). We revisit the problem from a spectral perspective and propose a simple normalization that mitigates feature homogenization across layers.",
    "reason": "Wrong citation style; the preposition should not be inside the parentheses (e.g., In Li et al. (2018)).",
    "start": 119,
    "end": 139,
    "label": "Format"
  },
  {
    "span": "Recent works have shown that back-translation consistently improves code summarization across languages.",
    "document": "Related Work\n\nCode summarization aims to generate natural language descriptions of source code, enabling faster comprehension and improved maintainability. Early approaches relied on information retrieval and template-based methods, while neural sequence-to-sequence models later achieved substantial gains by leveraging large code corpora. With the rise of multilingual repositories and cross-language projects, transfer learning and augmentation strategies have become central to improving robustness and coverage. Recent works have shown that back-translation consistently improves code summarization across languages. However, the applicability of these techniques to low-resource programming languages and highly domain-specific codebases remains unclear. In addition, there is ongoing debate about the trade-offs between syntactic signal (e.g., ASTs) and surface tokens when training large encoder-decoder architectures for summarization. We situate our work at this intersection by exploring simple augmentation pipelines that balance structural and lexical cues without expensive pretraining.",
    "reason": "Mentions 'recent works' and a performance claim without providing any citations.",
    "start": 517,
    "end": 621,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Garcia et al., 2021)",
    "document": "Related Work\n\nCross-lingual transfer leverages shared subword vocabularies and parameter sharing to bridge resource gaps (Conneau et al., 2020; Artetxe et al., 2020). In (Garcia et al., 2021) we find a comprehensive survey of multilingual prompt tuning, but few works consider zero-shot alignment with constrained decoders. Our approach complements adapters (Houlsby et al., 2019) and parallel projection methods (Schuster et al., 2019) by introducing bilingual consistency losses.",
    "reason": "Wrong citation style; 'In (Garcia et al., 2021)' should be 'in Garcia et al. (2021)'.",
    "start": 167,
    "end": 191,
    "label": "Format"
  },
  {
    "span": "[(Zhang et al., 2020)",
    "document": "Related Work\n\nMulti-agent reinforcement learning (MARL) tackles coordination and competition among agents under partial observability (Lowe et al., 2017; Foerster et al., 2016). Centralized training with decentralized execution is a prevalent paradigm that stabilizes learning while preserving scalability (Rashid et al., 2018; Oliehoek et al., 2008).\n\nValue factorization methods decompose global value functions into agent-wise utilities (Sunehag et al., 2018; Son et al., 2019). Policy-gradient approaches leverage shared critics or communication channels to handle non-stationarity (Iqbal and Sha, 2019; Foerster et al., 2018). Recent surveys [(Zhang et al., 2020) categorize algorithmic families and open challenges, including credit assignment, exploration, and sample efficiency. We complement these by introducing contrastive objectives for coordination signals and a curriculum over interaction graphs (Mahajan et al., 2019; Wang et al., 2020).\n\nOur framework unifies value-based and policy-based updates via a shared latent mixer and demonstrates improvements on cooperative navigation and StarCraft micromanagement tasks (Samvelyan et al., 2019; Vinyals et al., 2019).",
    "reason": "Extraneous opening square bracket before a parenthetical citation; should be '(Zhang et al., 2020)' without the '['.",
    "start": 647,
    "end": 668,
    "label": "Format"
  },
  {
    "span": "(Nguyen et al., 2021",
    "document": "Introduction\n\nPretrained sequence-to-sequence models have transformed abstractive summarization by enabling stronger content selection and paraphrasing (Lewis et al., 2020; Raffel et al., 2020). Despite these gains, factual consistency remains problematic, motivating constrained decoding and knowledge-grounded generation (Kryscinski et al., 2020; Cao et al., 2022). Earlier surveys (Nguyen et al., 2021 provide a taxonomy of faithfulness errors and evaluate mitigation strategies across news and scientific domains. We build on this literature by proposing an entailment-aware training objective coupled with calibration for uncertainty estimation (Desai and Durrett, 2020; Dong et al., 2022).\n\nWe conduct experiments on multi-document benchmarks and analyze error types through human annotations.",
    "reason": "Missing closing parenthesis in parenthetical citation: '(Nguyen et al., 2021' should be '(Nguyen et al., 2021)'.",
    "start": 384,
    "end": 404,
    "label": "Format"
  },
  {
    "span": "According to industry reports, transformer-based recommenders now serve over 80% of impressions on major e-commerce platforms.",
    "document": "Introduction\n\nPersonalized recommendation has shifted from matrix factorization (Koren et al., 2009) to neural collaborative filtering (He et al., 2017) and attention-based sequence models (Kang and McAuley, 2018; Sun et al., 2019). Sequential architectures model user intent through next-item prediction and benefit from self-attention to capture long-range dependencies. According to industry reports, transformer-based recommenders now serve over 80% of impressions on major e-commerce platforms. Despite these advances, data sparsity and cold-start remain central challenges, prompting work on meta-learning (Vartak et al., 2017), graph-based recommenders (Wang et al., 2019), and contrastive regularization (Xie et al., 2020).",
    "reason": "Presents a specific statistic ('over 80%') referenced to unspecified 'industry reports' without citation or evidence, violating rule (a)/(b).",
    "start": 373,
    "end": 499,
    "label": "Unsupported_claim"
  },
  {
    "span": "the MS MARCO v2 corpus contains over 12 million passages",
    "document": "Related Work\n\nDense retrieval has advanced rapidly with dual-encoder architectures and large-scale passage corpora. Benchmarks such as MS MARCO and BEIR have catalyzed progress by providing standardized training data and evaluation suites across domains.\n\nIn large-scale settings, pretraining on millions of passages is standard practice; for example, the MS MARCO v2 corpus contains over 12 million passages. However, scaling alone does not ensure domain transfer, motivating methods that blend corpus-specific adaptation with generalizable objectives.\n\nOur approach integrates domain-adaptive pretraining with contrastive distillation from cross-encoders to improve zero-shot retrieval on specialized collections.",
    "reason": "States a specific dataset statistic without providing a supporting citation (rule a and b).",
    "start": 352,
    "end": 408,
    "label": "Unsupported_claim"
  },
  {
    "span": "Ying et al. (2019) proposed GNNExplainer for instance-level rationales. Pope et al. (2019) examined gradient-based saliency on graph classifiers. Yuan et al. (2020) introduced subgraph masking for post-hoc explanations. Luo et al. (2020) studied counterfactuals on graphs.",
    "document": "Related Work\n\nExplainability for graph neural networks (GNNs) has emerged as a critical area to build trust in predictions over molecules, social networks, and knowledge graphs. Existing work ranges from post-hoc rationalization to intrinsically interpretable architectures. However, the field lacks standardized metrics and ground-truth benchmarks that reflect real-world explanatory needs, such as causal substructures or functional groups in chemistry.\n\nYing et al. (2019) proposed GNNExplainer for instance-level rationales. Pope et al. (2019) examined gradient-based saliency on graph classifiers. Yuan et al. (2020) introduced subgraph masking for post-hoc explanations. Luo et al. (2020) studied counterfactuals on graphs.\n\nIn contrast to these post-hoc approaches, our method injects sparsity and stability constraints during training, aiming to align explanations with invariant predictive features. We evaluate across synthetic and molecular datasets with human-interpretable ground truth and propose a perturbation-based metric that correlates with expert judgments.",
    "reason": "The span enumerates works with no connective phrasing, leaving readers to infer how saliency, masking, and counterfactuals relate to instance-level explanations. The abrupt listing lacks transitions and explicit relationships.",
    "start": 457,
    "end": 729,
    "label": "Coherence"
  },
  {
    "span": "Liu et al. (2008) proposed Isolation Forest for anomaly detection. Malhotra et al. (2016) used LSTM autoencoders for sequence reconstruction. Yeh et al. (2016) popularized the matrix profile for motif discovery. Adams and MacKay (2007) developed Bayesian online changepoint detection.",
    "document": "Related Work\n\nTime series anomaly detection spans model-based prediction errors, density estimation, and segmentation. Applications include sensor monitoring, finance, and industrial operations.\n\nLiu et al. (2008) proposed Isolation Forest for anomaly detection. Malhotra et al. (2016) used LSTM autoencoders for sequence reconstruction. Yeh et al. (2016) popularized the matrix profile for motif discovery. Adams and MacKay (2007) developed Bayesian online changepoint detection.\n\nOur contribution unifies reconstruction and segmentation via a probabilistic state-space model with learned emissions.",
    "reason": "The span presents a list of methods without transitions or explicit statements about their relationships, causing abrupt shifts that weaken coherence.",
    "start": 196,
    "end": 480,
    "label": "Coherence"
  },
  {
    "span": "FB15k-237",
    "document": "Introduction\n\nLink prediction in knowledge graphs (KGs) aims to infer missing edges from observed triples. Embedding-based models capture relational semantics with translational or bilinear operators, while recent message-passing architectures propagate neighborhood information.\n\nRelated Work\n\nEvaluation typically reports mean reciprocal rank and Hits@K on standard benchmarks such as FB15k-237 and WN18RR, but concerns remain about test leakage and inverse relation shortcuts. We revisit these benchmarks with stronger negative sampling and a calibration protocol to assess robustness to spurious correlations.",
    "reason": "Introduces a specific dataset name without citation at first mention, violating rule (a).",
    "start": 387,
    "end": 396,
    "label": "Unsupported_claim"
  },
  {
    "span": "Message passing neural networks model molecular graphs (Gilmer et al., 2017). Equivariant networks encode 3D geometry (Schütt et al., 2017; Batzner et al., 2022). Protein structure prediction leverages attention (Jumper et al., 2021). Docking scores approximate binding affinity (Trott and Olson, 2010).",
    "document": "Related Work\n\nLearning molecular properties requires capturing both topological and geometric information. Approaches vary by representation (SMILES, graphs, 3D point clouds) and supervision (quantum targets, bioassay labels).\n\nMessage passing neural networks model molecular graphs (Gilmer et al., 2017). Equivariant networks encode 3D geometry (Schütt et al., 2017; Batzner et al., 2022). Protein structure prediction leverages attention (Jumper et al., 2021). Docking scores approximate binding affinity (Trott and Olson, 2010).\n\nWe pursue a hybrid energy-based model with distance-aware message passing and multi-task training on quantum and activity datasets to improve out-of-distribution generalization.",
    "reason": "The span abruptly moves from small-molecule graph learning to protein structure prediction and docking without clarifying how these areas relate, and lacks transitions connecting the cited works.",
    "start": 228,
    "end": 531,
    "label": "Coherence"
  },
  {
    "span": "In a previous study, the authors claim that byte-level models eliminate the need for tokenization in all languages.",
    "document": "Related Work\n\nNeural machine translation (NMT) for low-resource languages often struggles with data sparsity and morphological richness. Subword segmentation methods have been adopted to mitigate vocabulary explosion, yet their optimal configuration remains task- and language-dependent. In a previous study, the authors claim that byte-level models eliminate the need for tokenization in all languages. This assertion, if generally valid, would simplify the multilingual NMT pipeline and reduce engineering overhead.\n\nAlternative approaches leverage character-level encoders or hybrid subword-character models to better capture rare word forms, but the trade-offs in efficiency and accuracy are still debated. Parallel corpus augmentation via back-translation and multilingual transfer have also shown potential, although their benefits can vary widely across language pairs and domains. Our work examines these modeling choices in the context of truly scarce data, focusing on the balance between robustness and computational cost.",
    "reason": "References a 'previous study' and attributes a specific claim without citing the study (rule a and e).",
    "start": 288,
    "end": 403,
    "label": "Unsupported_claim"
  },
  {
    "span": "McMahan et al. (2017) present FedAvg for decentralized optimization. Konečný et al. (2016) study communication-efficient updates with compression. Geyer et al. (2017) add differential privacy guarantees to shared gradients. Smith et al. (2018) address personalization via multi-task learning.",
    "document": "Related Work\n\nFederated Learning on Edge Devices\n\nFederated optimization trains models across client devices without centralizing raw data. This setting raises challenges in communication, privacy, and personalization due to heterogeneity and intermittency. McMahan et al. (2017) present FedAvg for decentralized optimization. Konečný et al. (2016) study communication-efficient updates with compression. Geyer et al. (2017) add differential privacy guarantees to shared gradients. Smith et al. (2018) address personalization via multi-task learning. Deployment considerations include stragglers, client sampling, and secure aggregation.\n\nOur method couples adaptive client weighting with on-device distillation to improve both personalization and communication efficiency under non-IID participation.",
    "reason": "The span enumerates methods across different subtopics (optimization, compression, privacy, personalization) without transitions or an explicit narrative linking them, making the connections between the cited works unclear.",
    "start": 258,
    "end": 550,
    "label": "Coherence"
  },
  {
    "span": "Rubric induction learns grading criteria from exemplars (Zhang and van der Schaar, 2020). Knowledge tracing models student mastery over time (Piech et al., 2015). Automated essay scoring uses handcrafted and neural features (Attali and Burstein, 2006; Taghipour and Ng, 2016). Peer assessment improves reliability with aggregation (Piech et al., 2013).",
    "document": "Related Work\n\nAutomated assessment in education includes short-answer grading, essay scoring, and program evaluation. Research explores alignment with pedagogical goals and fairness across student subgroups.\n\nRubric induction learns grading criteria from exemplars (Zhang and van der Schaar, 2020). Knowledge tracing models student mastery over time (Piech et al., 2015). Automated essay scoring uses handcrafted and neural features (Attali and Burstein, 2006; Taghipour and Ng, 2016). Peer assessment improves reliability with aggregation (Piech et al., 2013).\n\nOur method focuses on criterion-referenced short-answer grading with uncertainty estimation to support formative feedback.",
    "reason": "The four sentences reference different subfields (rubrics, knowledge tracing, essay scoring, peer assessment) with no transitions or explicit connections, making the relationships unclear.",
    "start": 209,
    "end": 561,
    "label": "Coherence"
  },
  {
    "span": "Deng et al (2015)",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a standard tool for modeling relational data (Kipf and Welling, 2017; Hamilton et al., 2017). Attention-based message passing improves expressivity by weighting neighbors according to learned relevance scores (Velickovic et al., 2018). Deng et al (2015) introduced early ideas on hierarchical pooling for large graphs, later extended with differentiable coarsening (Ying et al., 2018). Our architecture unifies attention and pooling through a shared scoring function that preserves cut sparsity.\n\nWe compare against strong GNN baselines on molecular property prediction and social network benchmarks.",
    "reason": "Missing period after 'al' in 'et al.'; correct narrative citation is 'Deng et al. (2015)'.",
    "start": 291,
    "end": 308,
    "label": "Format"
  },
  {
    "span": "there are many recent works on controllable text simplification that condition on style, length, and readability",
    "document": "Introduction\n\nText simplification aims to reduce linguistic complexity while preserving meaning, thereby improving accessibility for readers with lower literacy or for second-language learners. Early approaches relied on hand-crafted rules and parallel corpora, while more recent neural methods leverage large pretrained sequence-to-sequence models to generate fluent simplifications.\n\nA key limitation of vanilla simplification is the lack of control over attributes such as output length, reading grade level, and stylistic tone. In response, there are many recent works on controllable text simplification that condition on style, length, and readability. Yet, existing systems often rely on proxy control signals that do not generalize across domains, motivating our proposal of an explicit control interface aligned with standard readability scales.\n\nWe evaluate our approach on multiple corpora and show improved controllability with minimal degradation of adequacy and fluency compared to strong neural baselines.",
    "reason": "Mentions 'many recent works' and specific conditioning factors without providing citations to those works (rule d and a).",
    "start": 545,
    "end": 657,
    "label": "Unsupported_claim"
  },
  {
    "span": "Domain randomization and adaptation have been explored extensively for sim-to-real transfer in robotics (Tobin et al., 2017; Peng et al., 2018; James et al., 2019; Chen et al., 2020).",
    "document": "Related Work\n\nSim-to-Real Transfer. Bridging the gap between simulation and the real world remains a key challenge in robot learning. Domain randomization and adaptation have been explored extensively for sim-to-real transfer in robotics (Tobin et al., 2017; Peng et al., 2018; James et al., 2019; Chen et al., 2020). Additional strategies use privileged information, system identification, and visual representation learning to reduce the reality gap (Huang et al., 2019; OpenAI et al., 2019; Zhou et al., 2020).\n\nEvaluation. Transfer is typically assessed on manipulation and locomotion tasks with varying textures, lighting, and dynamics, often under limited real-world budget.\n\nWe present a policy optimization scheme that alternates between simulation curricula and constrained real-world rollouts.",
    "reason": "The span lists major approaches without clarifying what remains insufficient about them or how the proposed scheme differs, thus not synthesizing prior work into a motivating gap (definition a and b).",
    "start": 134,
    "end": 317,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Li and Zhao, 2016,)",
    "document": "Related Work\n\nSemi-supervised learning has seen a resurgence with consistency objectives and pseudo-labeling (Sohn et al., 2020; Xie et al., 2020). Classic self-training remains competitive (Li and Zhao, 2016,) when combined with strong augmentations and teacher-student ensembling (Tarvainen and Valpola, 2017). Recent graph-based methods further exploit neighborhood agreement to propagate labels (Iscen et al., 2019) while mitigating confirmation bias through confidence tempering (Arazo et al., 2020).\n",
    "reason": "Extraneous comma before the closing parenthesis in the citation.",
    "start": 190,
    "end": 210,
    "label": "Format"
  },
  {
    "span": "in a previous study, the authors claim that domain randomization alone can bridge the sim-to-real gap",
    "document": "Related Work\n\nTransferring control policies from simulation to real-world robots remains challenging due to modeling errors, sensing noise, and unmodeled dynamics. Various techniques have been proposed, including system identification, adaptive control, and domain adaptation via feature alignment. A prominent line of work explores domain randomization, where visual and physical parameters are randomized during training to improve robustness. Notably, in a previous study, the authors claim that domain randomization alone can bridge the sim-to-real gap, motivating efforts to increase the diversity of simulated experiences. While effective in some settings, domain randomization may not suffice for tasks with contact-rich dynamics, where residual learning and online adaptation are often required. We propose a hybrid approach that integrates dynamics randomization with uncertainty-aware policy optimization.\n",
    "reason": "Refers to a specific prior study and its claim without providing a citation.",
    "start": 455,
    "end": 556,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith (2018; Johnson, 2019)",
    "document": "Related Work\n\nNeural machine translation has progressed rapidly with attention and transformer architectures. Early sequence-to-sequence models with attention enabled effective context modeling across long sentences (Bahdanau et al., 2015). The transformer architecture further improved parallelism and quality by dispensing with recurrence (Vaswani et al., 2017). Neural MT matured after Smith (2018; Johnson, 2019) showed the gains from multilingual pre-training and shared subword vocabularies, while later work introduced domain adaptation and terminology constraints for specialized corpora (Chu and Wang, 2018; Dinu et al., 2019).\n\nWe propose a constrained decoding method that integrates lexicon priors without sacrificing fluency.",
    "reason": "Mixed narrative and parenthetical styles inside a single narrative citation; use either '(Smith, 2018; Johnson, 2019)' or separate narrative citations like 'Smith (2018) and Johnson (2019)'.",
    "start": 389,
    "end": 416,
    "label": "Format"
  },
  {
    "span": "Brown et al., (2020)",
    "document": "Introduction\n\nKnowledge graph completion benefits from combining structured reasoning with neural scoring functions (Bordes et al., 2013; Dettmers et al., 2018). Recent work integrates logical constraints to improve consistency and interpretability (Rocktäschel and Riedel, 2017; Minervini et al., 2020). Brown et al., (2020) present a pretraining strategy that aligns textual entity descriptions with graph embeddings to mitigate sparsity. However, most approaches still struggle with long-tail relations and domain transfer, motivating semi-parametric retrieval and calibration techniques (Sun et al., 2021; Yao et al., 2022).",
    "reason": "Extraneous comma in a narrative citation. It should be \"Brown et al. (2020)\" without the comma before the year.",
    "start": 305,
    "end": 325,
    "label": "Format"
  },
  {
    "span": "Recent works have demonstrated large improvements in factuality when adding entity-aware constraints.",
    "document": "Introduction\n\nNeural abstractive summarization has advanced rapidly with pretraining and large-scale datasets (See et al., 2017; Liu and Lapata, 2019). However, models often generate fluent yet factually inconsistent content (Kryscinski et al., 2020; Maynez et al., 2020), motivating research on factuality metrics and controllable generation. Entity representations and copy mechanisms have been explored to better anchor generations to the source (Gu et al., 2016; Wiseman et al., 2017), while constrained decoding and post-editing aim to reduce hallucinations (Miao et al., 2019; Dong et al., 2020).\n\nRecent works have demonstrated large improvements in factuality when adding entity-aware constraints. Building on this intuition, we introduce Entail-Sum, a training and decoding framework that integrates entity-level consistency checks with an entailment-guided reranker. Our method uses a lightweight entity linker to extract source-target entity alignments and penalizes deviations via a contrastive loss.\n\nWe evaluate on CNN/DailyMail, XSum, and SAMSum, reporting improvements in both automatic factuality metrics and human judgments. Ablations highlight the importance of entity alignment during fine-tuning and constrained decoding at inference.",
    "reason": "Mentions 'recent works' and a specific claim of improvements but provides no citations to those works.",
    "start": 604,
    "end": 705,
    "label": "Unsupported_claim"
  },
  {
    "span": "In industrial systems, over 70% of recommendations are made for users with sparse histories.",
    "document": "Introduction\n\nPersonalized recommendation has been dominated by deep architectures that fuse content, context, and interaction signals (Rendle, 2012; He et al., 2017; Zhou et al., 2018). Yet, performance often degrades in the cold-start regime, where user or item histories are limited (Schein et al., 2002; Steck, 2011). Approaches that leverage side information, meta-learning, or self-supervised objectives have shown promise in mitigating sparsity (Vartak et al., 2017; Pan et al., 2019; Yao et al., 2021).\n\nIn industrial systems, over 70% of recommendations are made for users with sparse histories. This observation motivates our focus on pre-ranking models that can operate reliably under minimal behavioral context. We introduce a representation learning framework that transfers knowledge from head users/items to tail entities via calibrated distillation and uncertainty-aware priors.",
    "reason": "Presents a specific statistic about industry systems without any citation or evidence (rule b).",
    "start": 512,
    "end": 604,
    "label": "Unsupported_claim"
  },
  {
    "span": "When instruments are weak or many, estimators such as LIML, Fuller, jackknife IV, and debiased machine learning have been proposed (Anderson and Rubin, 1949; Fuller, 1977; Angrist and Krueger, 1991; Chernozhukov et al., 2018; Hansen et al., 2008).",
    "document": "Introduction\n\nInstrumental variables (IV) provide identification in the presence of unobserved confounding, but practical validity hinges on instrument relevance and exclusion. Modern applications often face weak instruments or high-dimensional covariates, creating instability and bias in conventional two-stage least squares procedures.\n\nWhen instruments are weak or many, estimators such as LIML, Fuller, jackknife IV, and debiased machine learning have been proposed (Anderson and Rubin, 1949; Fuller, 1977; Angrist and Krueger, 1991; Chernozhukov et al., 2018; Hansen et al., 2008). Overidentification tests and weak-instrument robust confidence sets complement these estimators, but computational and finite-sample considerations remain.\n\nWe develop a curvature-regularized IV approach that stabilizes first-stage projections and delivers uniform coverage guarantees under local-to-zero instrument strength.",
    "reason": "The span lists methods and citations without explaining their limitations relative to the paper’s goals or how they motivate the proposed approach, showing a lack of synthesis.",
    "start": 340,
    "end": 587,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Enron email corpus",
    "document": "Introduction\n\nEmail understanding tasks such as thread disentanglement, intent detection, and response suggestion require models to capture long-range structure and pragmatic cues. Public corpora enable reproducible research but often differ in privacy handling and annotation quality.\n\nWe pretrain on the Enron email corpus to capture organizational communication patterns, then fine-tune on supervised labels for intent and urgency.\n\nOur analysis examines discourse markers, quoting styles, and temporal dynamics that influence prediction robustness.",
    "reason": "The dataset should be cited at first mention to acknowledge its origin and characteristics (guideline a).",
    "start": 306,
    "end": 324,
    "label": "Unsupported_claim"
  },
  {
    "span": "Unsupervised domain adaptation for visual recognition has explored moment-matching losses, adversarial alignment, self-training with pseudo-labels, and contrastive objectives (Long et al., 2015; Ganin et al., 2016; Zou et al., 2019; Kim et al., 2020).",
    "document": "Related Work\n\nDeploying image classifiers across changing environments requires adaptation to shifts in texture, illumination, and background statistics.\n\nUnsupervised domain adaptation for visual recognition has explored moment-matching losses, adversarial alignment, self-training with pseudo-labels, and contrastive objectives (Long et al., 2015; Ganin et al., 2016; Zou et al., 2019; Kim et al., 2020). Source-free methods further remove source data dependencies by relying on target-only adaptation.\n\nOur approach leverages target consistency under photometric perturbations to stabilize pseudo-labels, and we incorporate a style-augmented teacher for improved robustness.",
    "reason": "This sentence lists prior methods without explaining their relation to the authors’ consistency-based strategy or identifying shortcomings that the paper addresses.",
    "start": 155,
    "end": 406,
    "label": "Lacks_synthesis"
  },
  {
    "span": "For robust ASR, domain adaptation techniques include feature normalization (Viemeister et al., 2015), adversarial domain confusion (Ganin et al., 2016; Shinohara, 2016), multi-condition training (Lippmann et al., 1987), self-training with pseudo-labels (Kahn et al., 2020), and data augmentation with noise and reverberation (Ko et al., 2015; Ravanelli et al., 2018).",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) models suffer significant accuracy drops when evaluated on accents, channels, or environments unseen during training. Practical systems must adapt on the fly with minimal labeled data. We consider on-device adaptation with strict compute and latency constraints.\n\nFor robust ASR, domain adaptation techniques include feature normalization (Viemeister et al., 2015), adversarial domain confusion (Ganin et al., 2016; Shinohara, 2016), multi-condition training (Lippmann et al., 1987), self-training with pseudo-labels (Kahn et al., 2020), and data augmentation with noise and reverberation (Ko et al., 2015; Ravanelli et al., 2018).\n\nWe introduce a lightweight adapter with test-time entropy regularization and confidence-calibrated pseudo-labeling designed for mobile CPUs.",
    "reason": "The span provides a list of prior adaptation strategies without connecting them to on-device constraints or identifying shortcomings, thus lacking synthesis per (a) and (c).",
    "start": 313,
    "end": 680,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Antol et al. (2015) introduced the VQA benchmark. Anderson et al. (2018) proposed bottom-up attention for vision–language tasks. Lu et al. (2019) presented ViLBERT. Radford et al. (2021) developed CLIP for contrastive pretraining.",
    "document": "Related Work\n\nMultimodal learning integrates vision and language to enable grounded reasoning and open-vocabulary understanding. Progress has been driven by improved datasets, attention mechanisms, and large-scale contrastive pretraining across image–text pairs.\n\nVision–language benchmarks and models\n\nAntol et al. (2015) introduced the VQA benchmark. Anderson et al. (2018) proposed bottom-up attention for vision–language tasks. Lu et al. (2019) presented ViLBERT. Radford et al. (2021) developed CLIP for contrastive pretraining. Li et al. (2020) introduced UNITER with unified pretraining objectives.\n\nGrounding and compositionality\n\nRecent research probes compositional generalization, bias, and grounding failures, proposing counterfactual data augmentation and causal analyses to reduce shortcuts.\n\nOur approach unifies region-level grounding with text-conditioned contrastive alignment under a curriculum that prioritizes rare attribute–relation compositions, improving zero-shot performance and reducing spurious correlations.",
    "reason": "This span enumerates multiple cited works without transitions or explicit connections among them, leaving their relationships implied rather than stated. It constitutes a multi-sentence coherence problem under (a) and (b).",
    "start": 303,
    "end": 533,
    "label": "Coherence"
  },
  {
    "span": "In (Smith et al., 2019)",
    "document": "Introduction\n\nDomain adaptation seeks to transfer knowledge from a labeled source domain to an unlabeled or sparsely labeled target domain. In (Smith et al., 2019) we demonstrate that aligning conditional distributions can be as important as marginal alignment when class imbalance varies across domains. Subsequent work explores adversarial objectives and discrepancy measures to reduce distributional gaps, but stability and mode collapse remain practical concerns (Ganin et al., 2016; Zhao et al., 2019). Recent trends favor self-supervised pretext tasks and pseudo-label refinement to bootstrap target performance without strong assumptions about covariate shift (Chen et al., 2020; French et al., 2018).",
    "reason": "Wrong citation style: a parenthetical citation is used as the grammatical subject. It should be narrative, e.g., “In Smith et al. (2019), we demonstrate …”.",
    "start": 140,
    "end": 163,
    "label": "Format"
  },
  {
    "span": "Most existing graph-based recommenders assume homophily in user-item interactions.",
    "document": "Related Work\n\nGraph-based Recommendation. Graph neural networks propagate signals over user–item bipartite graphs to learn representations for ranking (Ying et al., 2018; He et al., 2020). Contrastive learning has been adopted to improve robustness under sparse feedback (Wu et al., 2021; Zhou et al., 2021). Most existing graph-based recommenders assume homophily in user-item interactions. However, real platforms exhibit both homophily and heterophily due to diverse tastes and exposure bias, motivating debiasing methods (Sinha et al., 2016; Zhu et al., 2021).",
    "reason": "Broad claim about the prevailing assumption in prior work without supporting citations.",
    "start": 309,
    "end": 391,
    "label": "Unsupported_claim"
  },
  {
    "span": "in a previous study, the authors showed that deformable convolutions consistently outperform standard ones on urban driving datasets",
    "document": "Related Work\n\nConvolutional backbones remain central to modern semantic segmentation pipelines for autonomous driving. Numerous architectural refinements target receptive field size, multi-scale fusion, and spatial precision. Deformable convolutions have been proposed to adapt sampling locations dynamically, ostensibly capturing object boundaries and perspective distortions more faithfully.\n\nHowever, in a previous study, the authors showed that deformable convolutions consistently outperform standard ones on urban driving datasets. Building on that observation, subsequent works introduced hybrid backbones mixing regular and deformable layers to reduce compute while maintaining accuracy.\n\nDespite these advances, a comprehensive account of when and where spatial adaptivity helps remains lacking. We therefore analyze layer-wise substitution strategies and training dynamics across multiple backbones, providing guidance on principled deployment of deformable modules.\n",
    "reason": "Claims specific findings of a prior study ('consistently outperform') without citing the study; per rule (a) and (b), first mentions of prior work and domain-specific claims require a citation.",
    "start": 404,
    "end": 536,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Prabhu et al., 2019, Ein-Dor et al., 2020, Yuan et al., 2020)",
    "document": "Related Work\n\nPretraining has reshaped approaches to data-efficient learning. Studies compare active learning with random sampling using BERT variants (Prabhu et al., 2019, Ein-Dor et al., 2020, Yuan et al., 2020) and examine diversity-promoting objectives (Sener and Savarese, 2018). We complement these results with an analysis of acquisition shift in multilingual settings.",
    "reason": "Multiple works inside one parenthetical citation are separated by commas instead of semicolons. Should be (Prabhu et al., 2019; Ein-Dor et al., 2020; Yuan et al., 2020).",
    "start": 151,
    "end": 213,
    "label": "Format"
  },
  {
    "span": "Bordes et al. (2013) introduced translational embeddings for knowledge graph completion. Dettmers et al. (2018) applied convolutional operators in complex vector spaces. Das et al. (2017) built memory networks for open-domain QA over KGs. Sun et al. (2019) leveraged pretraining with entity descriptions.",
    "document": "Related Work\n\nKnowledge Graphs for Question Answering\n\nKG-based QA involves representation learning for entities and relations, reasoning over multi-hop paths, and integration with text. Architectures vary widely in how they model compositionality and incorporate external descriptions or passages.\n\nBordes et al. (2013) introduced translational embeddings for knowledge graph completion. Dettmers et al. (2018) applied convolutional operators in complex vector spaces. Das et al. (2017) built memory networks for open-domain QA over KGs. Sun et al. (2019) leveraged pretraining with entity descriptions.\n\nOur approach unifies path-based reasoning with text-enhanced entity encoders via a shared retrieval and scoring interface, improving generalization to long-tail entities.",
    "reason": "The span juxtaposes embedding models, convolutional approaches, memory networks, and description pretraining without transitions or clarifying how these methods relate to one another.",
    "start": 300,
    "end": 604,
    "label": "Coherence"
  },
  {
    "span": "Domain randomization (Tobin et al., 2017; Sadeghi and Levine, 2017), privileged information (Rusu et al., 2016), and adaptation via self-supervision (Zhang et al., 2019; James et al., 2019) have been studied for sim-to-real transfer.",
    "document": "Related Work\n\nBridging the sim-to-real gap is essential for deploying learned control policies on physical robots. Differences in rendering, physics, and sensor characteristics often degrade performance when policies trained in simulation are transferred to hardware.\n\nDomain randomization (Tobin et al., 2017; Sadeghi and Levine, 2017), privileged information (Rusu et al., 2016), and adaptation via self-supervision (Zhang et al., 2019; James et al., 2019) have been studied for sim-to-real transfer.\n\nWe focus on dynamics mismatch and propose a policy update that conditions on a latent dynamics code estimated online via lightweight system identification. The approach interleaves identification and control updates in a single trajectory buffer for sample efficiency.\n\nResults on manipulation and locomotion tasks show improved transfer success rates without additional real-world pretraining. Ablations quantify the contribution of identification accuracy and code dimensionality.",
    "reason": "The span lists categories of prior sim-to-real techniques without explaining their limitations relative to the present work, thus lacking synthesis per (a) and (c).",
    "start": 269,
    "end": 502,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There is broad consensus that demographic parity is unsuitable for high-stakes hiring.",
    "document": "Related Work\nFairness metrics formalize different desiderata under competing constraints, including independence-based criteria like demographic parity and error-rate-based criteria like equalized odds. In practice, trade-offs arise due to base-rate differences and the impossibility of satisfying all metrics simultaneously.\n\nThere is broad consensus that demographic parity is unsuitable for high-stakes hiring. Yet organizations and regulators differ in their interpretations of adverse impact and acceptable disparity thresholds. We analyze metric selection through simulations on realistic hiring funnels and discuss implications for compliance and utility.",
    "reason": "Asserting a field-wide consensus on a nuanced topic requires citations to surveys, standards, or position papers; none are provided (violates b and d).",
    "start": 327,
    "end": 413,
    "label": "Unsupported_claim"
  },
  {
    "span": "We evaluate on SQuAD and Natural Questions, two widely used datasets for machine reading comprehension and open-domain QA.",
    "document": "Introduction\n\nQuestion answering (QA) systems increasingly rely on pretrained language models that can read, retrieve, and reason over large text collections. Closed-book variants compress factual knowledge into parameters, while open-domain systems combine retrieval with reading comprehension to answer diverse queries. To assess the generality of our method, we consider settings that stress both passage selection and answer extraction. We evaluate on SQuAD and Natural Questions, two widely used datasets for machine reading comprehension and open-domain QA. In addition, we probe cross-domain robustness on out-of-distribution question sets drawn from community forums. Our contributions include a calibration-aware reader and a retrieval adaptor that jointly optimize end-to-end exact match under a fixed compute budget.",
    "reason": "First mention of specific datasets lacks citations to the original dataset papers.",
    "start": 441,
    "end": 563,
    "label": "Unsupported_claim"
  },
  {
    "span": "MoCo maintains a momentum encoder for contrastive learning (He et al., 2020). Data augmentation policies can be learned automatically (Cubuk et al., 2019). BYOL removes negative pairs using online and target networks (Grill et al., 2020).",
    "document": "Related Work\n\nSelf-Supervised Visual Representation Learning. Contrastive and non-contrastive methods have achieved state-of-the-art performance by leveraging instance discrimination and invariances induced by augmentations (Chen et al., 2020; Caron et al., 2021).\n\nContrastive vs. Non-Contrastive Paradigms. Memory banks and momentum encoders help maintain large and consistent dictionaries of negatives, improving stability and throughput (He et al., 2020; Wu et al., 2018). Predictor asymmetry and stop-gradient designs enable collapse-free training without negatives (Grill et al., 2020; Chen and He, 2021). MoCo maintains a momentum encoder for contrastive learning (He et al., 2020). Data augmentation policies can be learned automatically (Cubuk et al., 2019). BYOL removes negative pairs using online and target networks (Grill et al., 2020). We study long-tail categories under limited compute budgets.\n\nAugmentation and Robustness. Strong augmentations such as color jitter, multi-crop, and blur are crucial to performance but can overspecialize features (Caron et al., 2020; Touvron et al., 2021). We propose curriculum augmentations aligned with dataset difficulty to balance invariance and discrimination.",
    "reason": "The span abruptly moves from MoCo to learned augmentation policies and then to BYOL without explaining their connections; there are no transitions clarifying why these works are discussed together.",
    "start": 612,
    "end": 850,
    "label": "Coherence"
  },
  {
    "span": "(Johnson 2020)",
    "document": "Introduction\n\nKnowledge graphs (KGs) encode entities and relations for structured reasoning, supporting applications in search, recommendation, and QA (Bordes et al., 2013; Nickel et al., 2016). Earlier surveys (Johnson 2020) discuss schema design and embedding trends, while more recent overviews emphasize neuro-symbolic integration (Sosa et al., 2021) and temporal reasoning (Goel et al., 2020). We build on these efforts by studying continual KG completion with streaming updates (Boschee et al., 2015; Trivedi et al., 2017), focusing on catastrophic forgetting and evaluation protocols.",
    "reason": "Missing comma between author and year in a parenthetical citation; it should be '(Johnson, 2020)'.",
    "start": 211,
    "end": 225,
    "label": "Format"
  },
  {
    "span": "[Bishop, 2006)",
    "document": "Related Work\n\nOptimization methods for deep learning rely on stochastic gradients with momentum and adaptive preconditioning (Nesterov, 2004; Kingma and Ba, 2015; Loshchilov and Hutter, 2019). Second-order approximations can accelerate convergence but are costly at scale (Martens, 2010; Botev et al., 2017). For a tutorial on conjugate gradients and quasi-Newton updates, see [Bishop, 2006) and references therein. Our contribution introduces a low-rank preconditioner learned via meta-gradients that preserves curvature information while maintaining linear-time updates.",
    "reason": "Mismatched brackets in the citation; opening square bracket with closing parenthesis is incorrect. It should use matching parentheses '(Bishop, 2006)'.",
    "start": 377,
    "end": 391,
    "label": "Format"
  },
  {
    "span": "Gray et al. (2018) categorized dark patterns in user interfaces. Mathur et al. (2019) measured dark patterns at scale on the web. The GDPR establishes requirements for consent and transparency (Voigt and Von dem Bussche, 2017). Fitness applications use persuasive design to increase adherence (Fogg, 2009). IoT devices raise privacy concerns in domestic settings (Zeng et al., 2017).",
    "document": "Introduction\n\nDesign Patterns, Persuasion, and Privacy in Consumer Technologies\n\nPrior work has examined how interface choices nudge users toward outcomes that may undermine autonomy, while regulators have codified constraints on consent and data processing. We investigate how disclosure framing affects opt-in rates for data sharing.\n\nGray et al. (2018) categorized dark patterns in user interfaces. Mathur et al. (2019) measured dark patterns at scale on the web. The GDPR establishes requirements for consent and transparency (Voigt and Von dem Bussche, 2017). Fitness applications use persuasive design to increase adherence (Fogg, 2009). IoT devices raise privacy concerns in domestic settings (Zeng et al., 2017).\n\nWe conduct preregistered experiments comparing short, layered, and just-in-time disclosures in a simulated app store.",
    "reason": "There is no explicit linkage between dark patterns, GDPR, persuasive design in fitness apps, and IoT privacy; sentences are juxtaposed without transitions clarifying their relationships.",
    "start": 337,
    "end": 720,
    "label": "Coherence"
  },
  {
    "span": "He et al. (2017) introduced neural collaborative filtering. Rendle et al. (2009) proposed Bayesian personalized ranking for implicit feedback. Covington et al. (2016) described a deep ranking architecture for large-scale recommendations. Wang et al. (2019) combined knowledge graphs with recommenders.",
    "document": "Related Work\n\nRecommendation Systems\n\nModern recommender models integrate user-item interaction signals with side information such as text, images, and knowledge graphs. While training objectives vary, the common goal is to balance personalization with generalization under sparse feedback.\n\nHe et al. (2017) introduced neural collaborative filtering. Rendle et al. (2009) proposed Bayesian personalized ranking for implicit feedback. Covington et al. (2016) described a deep ranking architecture for large-scale recommendations. Wang et al. (2019) combined knowledge graphs with recommenders.\n\nCold-Start and Multimodality\n\nRecent efforts tackle cold-start issues via meta-learning and representation transfer from auxiliary modalities. However, the alignment between modalities and user preferences is often heuristic and dataset-specific.\n\nOur Approach\n\nWe propose a modality-adaptive ranking objective that reweights side-information contributions per user cohort, yielding consistent gains in cold-start regimes.",
    "reason": "The span lists four recommender papers as isolated statements with no transitions or explicit relational framing, making the flow abrupt and the connections unclear.",
    "start": 292,
    "end": 593,
    "label": "Coherence"
  },
  {
    "span": "(Zhao et al., 2020.",
    "document": "Related Work\n\nDomain shift has been studied extensively in vision and NLP, with benchmarks emphasizing the fragility of models to distributional changes (Torralba and Efros, 2011; Koh et al., 2021). Several recent studies examine how robustness degrades under covariate shift (Zhao et al., 2020. We extend this analysis to sequence labeling by introducing controllable perturbations that target specific features, complementing findings on stability and calibration (Hendrycks and Dietterich, 2019; Ovadia et al., 2019).\n",
    "reason": "Missing closing parenthesis: the parenthetical citation lacks a closing \")\".",
    "start": 276,
    "end": 295,
    "label": "Format"
  },
  {
    "span": "Miller et al. 3",
    "document": "Related Work\n\nModern recommender systems rely on representation learning to model user–item interactions, incorporating side information such as text and images to mitigate cold-start issues (Zhang et al., 2019; Sun et al., 2019). Contrastive learning has emerged as an effective paradigm for learning robust user and item embeddings from implicit feedback (Xin et al., 2020; Zhou et al., 2020). Miller et al. 3 introduce a hybrid objective that balances alignment and uniformity while accounting for temporal dynamics in user behavior.\n\nContext-aware methods further personalize recommendations by modeling session context and intent switching (Quadrana et al., 2018; Ludewig and Jannach, 2018). We build upon these insights by proposing a context-conditioned contrastive loss that adapts to evolving user intents without sacrificing long-term preference modeling.",
    "reason": "Wrong use of footnote-like numbering appended to an author name; should include the year (e.g., \"Miller et al. (2021)\") or be formatted as a proper footnote.",
    "start": 396,
    "end": 411,
    "label": "Format"
  },
  {
    "span": "(Chen, 2015; Wang, 2016;.",
    "document": "Introduction\n\nGraph neural networks (GNNs) generalize convolution to non-Euclidean domains by aggregating information over neighborhoods. Spectral methods grounded in graph signal processing offer theoretical insights but face scalability limits (Bruna et al., 2014; Defferrard et al., 2016). Spatial methods emphasize localized message passing and have become dominant in practice (Hamilton et al., 2017; Xu et al., 2019). Early work on graph regularization and semi-supervised learning established the importance of smoothness priors (Chen, 2015; Wang, 2016;. However, deeper architectures can oversmooth representations, degrading class separability (Li et al., 2018; Oono and Suzuki, 2020).\n\nWe introduce a decoupled aggregation-activation block with spectral dropout to alleviate oversmoothing while maintaining stability on large graphs.",
    "reason": "Malformed citation list: trailing semicolon and period inside the parentheses; missing a closing parenthesis and incorrect punctuation.",
    "start": 536,
    "end": 561,
    "label": "Format"
  },
  {
    "span": "Back-translation has been widely adopted as the standard data augmentation method for low-resource NMT.",
    "document": "Related Work\n\nData augmentation is critical for improving NMT in low-resource settings where parallel corpora are scarce. Approaches include synthetic parallel data generation, noise injection, and multilingual transfer. Back-translation has been widely adopted as the standard data augmentation method for low-resource NMT. Subsequent refinements propose iterative and tagged back-translation, as well as quality filtering of synthetic sources. Despite its popularity, the interaction between synthetic data quality and model scaling remains underexplored.",
    "reason": "Claims widespread adoption and 'standard' status for a method without citing supporting literature.",
    "start": 221,
    "end": 324,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Lopez et al., 2020)",
    "document": "Related Work\n\nContrastive learning for structured representations has been widely adopted in NLP and vision (Sun and Wang, 2020; Rivera et al., 2021). In (Lopez et al., 2020) a memory bank was introduced to stabilize negatives across batches, while MoCo-style queues adaptively replace entries over time (Huang and Patel, 2021). However, most approaches ignore label noise, which is prevalent in web-scale collections (Qian et al., 2022).\n\nIntroduction\n\nWe address this by coupling instance discrimination with cluster-level debiasing (Ito and Chen, 2023). Our analysis complements prior work on temperature scaling and queue size (Barros et al., 2020).",
    "reason": "Wrong citation style. The preposition 'In' should be followed by a narrative citation, e.g., 'In Lopez et al. (2020) ...', not a parenthetical '(Lopez et al., 2020)'.",
    "start": 151,
    "end": 174,
    "label": "Format"
  },
  {
    "span": "In (Liang et al., 2020)",
    "document": "Related Work\n\nMultimodal pretraining. Early multimodal encoders focus on aligning vision and language through contrastive losses and masked modeling objectives (Kumar et al., 2019; Peters and Wang, 2020). In (Liang et al., 2020), the authors propose a fusion module that conditions textual representations on regional visual features to improve grounding. Subsequent methods refine cross-attention patterns and scale training corpora (Salim et al., 2021; Ortega et al., 2022). While these approaches show strong performance on retrieval, they often underperform on reasoning-intensive benchmarks (Gao et al., 2021).\n\nCompositional reasoning. A parallel line of work emphasizes structure-aware objectives to improve compositionality (Miller and Cho, 2020; Huang et al., 2022). These methods inject program-like supervision or scene graph constraints during pretraining, yielding better generalization to out-of-domain tasks (Park et al., 2023).",
    "reason": "Wrong citation style: narrative use of a parenthetical citation. It should be written as \"In Liang et al. (2020)\" rather than \"In (Liang et al., 2020)\".",
    "start": 205,
    "end": 228,
    "label": "Format"
  },
  {
    "span": "The ABCNewsQA dataset contains 200k question–answer pairs curated from broadcast transcripts.",
    "document": "Introduction\n\nQuestion answering over real-world news requires handling temporal drift, evolving entities, and shifting phrasing. Benchmarks that focus on broadcast content introduce additional challenges such as disfluencies and colloquial speech. To systematically evaluate models in this setting, prior work has assembled corpora from news programs and associated metadata.\n\nThe ABCNewsQA dataset contains 200k question–answer pairs curated from broadcast transcripts. While the dataset spans multiple years and topics, it remains under-explored compared to web-based QA resources. We target this gap by proposing a retrieval-augmented model tailored to spoken-language idiosyncrasies present in broadcast transcripts.",
    "reason": "Introduces a specific dataset with detailed statistics but provides no citation to its source (rule a).",
    "start": 378,
    "end": 471,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Ortega, 2017",
    "document": "Related Work\n\nMulti-agent reinforcement learning (MARL) methods address non-stationarity and credit assignment through centralized training with decentralized execution (Foerster et al., 2018; Lowe et al., 2017). Communication learning improves coordination in partially observable settings, for example (Ortega, 2017 introduces channel capacity constraints to regularize message passing. Recent approaches add differentiable communication protocols with redundancy penalties (Zhang and Tian, 2020) and value decomposition tailored to sparse rewards (Rashid and Samvelyan, 2018). Our work complements these by learning communication masks guided by task-specific saliency.",
    "reason": "Missing closing parenthesis in the parenthetical citation; it should be '(Ortega, 2017)'.",
    "start": 304,
    "end": 317,
    "label": "Format"
  },
  {
    "span": "Brown et al (2016)",
    "document": "Related Work\n\nContinual and Lifelong Learning\n\nCatastrophic forgetting motivates regularization-based methods such as EWC and SI (Kirkpatrick et al., 2017; Zenke et al., 2017), while rehearsal buffers preserve past knowledge through replay (Rolnick et al., 2019; Chaudhry et al., 2019). Brown et al (2016) discuss task-free scenarios with non-stationary streams, and meta-learning has been adapted to accelerate adaptation across tasks (Finn et al., 2017; Javed and White, 2019). Orthogonal gradient updates further mitigate interference (Farajtabar et al., 2020).\n",
    "reason": "Punctuation error in narrative citation: missing period after 'al'; should be 'Brown et al. (2016)'.",
    "start": 287,
    "end": 305,
    "label": "Format"
  },
  {
    "span": "Federated averaging aggregates client updates on a central server (McMahan et al., 2017). Differential privacy has been combined with federated training (Geyer et al., 2018). Personalization layers mitigate client drift (Arivazhagan et al., 2019).",
    "document": "Related Work\n\nFederated learning enables collaborative training without centralizing raw data. Key challenges include statistical heterogeneity, privacy guarantees, and communication constraints.\n\nFederated averaging aggregates client updates on a central server (McMahan et al., 2017). Differential privacy has been combined with federated training (Geyer et al., 2018). Personalization layers mitigate client drift (Arivazhagan et al., 2019).\n\nCommunication-efficient protocols and robust aggregation have further expanded applicability. Our method integrates client-adaptive regularization with privacy-preserving optimization to reduce performance variance across cohorts.",
    "reason": "The span presents three separate ideas in isolated sentences with no transitions or explicit connections, leaving the relationship among the cited works implicit and incoherent.",
    "start": 197,
    "end": 444,
    "label": "Coherence"
  },
  {
    "span": "Carion et al. (2020) proposed a transformer-based detector that frames detection as set prediction. He et al. (2017) improved multi-scale features for detectors with a pyramid hierarchy. Kuo et al. (2021) examined domain-adaptive detection techniques under synthetic-to-real shifts. Zhou et al. (2022) explored knowledge distillation for lightweight detection models.",
    "document": "Related Work\n\nObject detection has evolved from anchor-based region proposal methods to end-to-end architectures. Early systems emphasized proposal generation and classification, while later work integrated feature hierarchies and stronger backbones to boost performance.\n\nCarion et al. (2020) proposed a transformer-based detector that frames detection as set prediction. He et al. (2017) improved multi-scale features for detectors with a pyramid hierarchy. Kuo et al. (2021) examined domain-adaptive detection techniques under synthetic-to-real shifts. Zhou et al. (2022) explored knowledge distillation for lightweight detection models.\n\nBeyond architecture design, semi-supervised and active learning approaches have targeted label efficiency in detection (Sohn et al., 2020; Zhou et al., 2021). Self-supervised pretraining further reduces reliance on large annotated corpora and enhances transfer to low-data regimes (He et al., 2020; Grill et al., 2020). Our approach focuses on label-efficient adaptation under distribution shift, complementing methods that modify detection heads or backbones.",
    "reason": "The sentences list disparate works without transitions or an explicit thread linking set prediction, feature pyramids, domain adaptation, and distillation. The relationship of each cited work to the prior one is not explained, resulting in abrupt connections.",
    "start": 273,
    "end": 640,
    "label": "Coherence"
  },
  {
    "span": "A wide range of summarization evaluation metrics have been proposed, including n-gram overlap metrics (ROUGE, BLEU), alignment-aware variants (METEOR), and embedding-based measures (BERTScore, MoverScore, BLEURT) (Lin, 2004; Papineni et al., 2002; Banerjee and Lavie, 2005; Zhang et al., 2020; Zhao et al., 2019; Sellam et al., 2020). Recent work also explores learned reward models and entailment-based checks to approximate human judgments.",
    "document": "Related Work: Evaluation for Text Summarization\n\nReliable evaluation remains a central problem in summarization research. Human assessments are expensive and difficult to standardize, while automatic metrics vary in correlation with human judgments across domains and styles.\n\nA wide range of summarization evaluation metrics have been proposed, including n-gram overlap metrics (ROUGE, BLEU), alignment-aware variants (METEOR), and embedding-based measures (BERTScore, MoverScore, BLEURT) (Lin, 2004; Papineni et al., 2002; Banerjee and Lavie, 2005; Zhang et al., 2020; Zhao et al., 2019; Sellam et al., 2020). Recent work also explores learned reward models and entailment-based checks to approximate human judgments.\n\nOur study investigates evaluation robustness across domains with an emphasis on noisy, multi-document inputs.",
    "reason": "The span enumerates metrics and trends but does not link them to the study's aims or identify where current metrics fall short relative to the present work (criteria a and c).",
    "start": 277,
    "end": 719,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent works have demonstrated that graph neural networks outperform CNNs for molecular property prediction.",
    "document": "Related Work\n\nDeep learning for molecular property prediction has progressed from fixed fingerprints to learned representations. Convolutional neural networks (CNNs) over molecular images and sequences have been explored, as have message-passing methods that operate directly on molecular graphs. Recent works have demonstrated that graph neural networks outperform CNNs for molecular property prediction. Beyond architecture choices, self-supervision and multi-task learning have also been investigated to leverage unlabeled compounds and related endpoints. Our study revisits these trends under strict scaffold splits and calibration-aware evaluation.",
    "reason": "Mentions 'recent works' without providing citations to those works, violating the requirement to cite prior studies at first mention.",
    "start": 297,
    "end": 405,
    "label": "Unsupported_claim"
  },
  {
    "span": "There have been many recent works on counterfactual fairness in graph neural networks.",
    "document": "Related Work\n\nFairness in machine learning has been studied under criteria such as demographic parity and equalized odds (Hardt et al., 2016; Verma and Rubin, 2018). In graph-structured domains, biases can be amplified through message passing and neighborhood selection (Dai and Wang, 2021; Kang et al., 2020). There have been many recent works on counterfactual fairness in graph neural networks. Complementary lines of research focus on data debiasing via reweighting (Kamiran and Calders, 2012), adversarial representation learning (Zhang et al., 2018), and post-processing calibration (Pleiss et al., 2017) adapted to graphs.",
    "reason": "Mentions 'many recent works' without citing any of them, which requires references under rule (d).",
    "start": 311,
    "end": 397,
    "label": "Unsupported_claim"
  },
  {
    "span": "Anchor-based detectors use priors over scales and aspect ratios (Ren et al., 2015). Self-supervised pretraining on video improves representations (Bao et al., 2021). Anchor-free designs predict keypoints to remove priors (Zhou et al., 2019).",
    "document": "Related Work\n\nObject detection has advanced through architectural innovations and training strategies. Two-stage detectors pioneered region proposal mechanisms that remain competitive in accuracy (Ren et al., 2015), while one-stage designs improved speed via dense predictions (Redmon et al., 2016; Liu et al., 2016). Label assignment, loss design, and multi-scale features further boosted performance (Zhang et al., 2020; Lin et al., 2017).\n\nAnchor-based detectors use priors over scales and aspect ratios (Ren et al., 2015). Self-supervised pretraining on video improves representations (Bao et al., 2021). Anchor-free designs predict keypoints to remove priors (Zhou et al., 2019). Recently, transformer decoders unified detection with set prediction and bipartite matching (Carion et al., 2020; Zhu et al., 2021). Our approach studies assignment-free heads with masked pretraining on unlabeled frames.",
    "reason": "The span juxtaposes anchor-based detection, video self-supervised pretraining, and anchor-free designs without clarifying their relationships or providing transitions, making the linkage between the cited works abrupt and implicit.",
    "start": 443,
    "end": 684,
    "label": "Coherence"
  },
  {
    "span": "Prior work fuses modalities through early concatenation (Poria et al., 2017), tensor fusion (Zadeh et al., 2017), and cross-modal attention (Tsai et al., 2019; Hazarika et al., 2020). We introduce a lightweight gating module.",
    "document": "Related Work\n\nMultimodal emotion recognition integrates acoustic, visual, and textual signals to capture nuanced affective states. A central question is how to fuse heterogeneous modalities under limited labeled data.\n\nPrior work fuses modalities through early concatenation (Poria et al., 2017), tensor fusion (Zadeh et al., 2017), and cross-modal attention (Tsai et al., 2019; Hazarika et al., 2020). We introduce a lightweight gating module. Co-learning approaches align modalities via shared latent spaces or co-training (Baltrušaitis et al., 2019; Pham et al., 2019), and robustness has been studied under missing or noisy modalities (Neverova et al., 2015; Chen and Jin, 2022).\n\nOur method adaptively suppresses spurious modality interactions by estimating instance-wise fusion confidence from self-supervised consistency signals.",
    "reason": "The span jumps from an undigested list of prior fusion methods to announcing the new module without articulating a gap or perspective, thus lacking synthesis.",
    "start": 219,
    "end": 444,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Chen et al., 2020",
    "document": "Related Work\n\nDomain adaptation mitigates distribution shift between labeled source and unlabeled target data. Adversarial feature alignment encourages domain-invariant representations by minimizing a domain classifier’s accuracy (Goodman and Wei, 2019; Silva et al., 2021). Moment-matching alternatives minimize discrepancies in higher-order statistics (Rao and Kim, 2020).\n\nSelf-training with pseudo-labels has shown strong performance in vision and NLP (Ibrahim et al., 2019; (Chen et al., 2020 while curriculum schedules reduce confirmation bias (Tariq and Liu, 2022). Recent perspectives emphasize class-conditional alignment and target-specific normalization (Mendez and Clarke, 2023), which we build upon with calibrated entropy minimization.\n",
    "reason": "Missing closing parenthesis in parenthetical citation.",
    "start": 479,
    "end": 497,
    "label": "Format"
  },
  {
    "span": "Finn et al. (2017) introduced MAML for meta-learning. He et al. (2016) designed ResNet to ease optimization of deep nets. Chen et al. (2019) benchmarked metric-based few-shot learners.",
    "document": "Related Work\n\nFew-shot image classification addresses generalization from a handful of labeled examples. Approaches include metric learning, meta-learning, and data augmentation, often evaluated on standardized episodic benchmarks.\n\nFinn et al. (2017) introduced MAML for meta-learning. He et al. (2016) designed ResNet to ease optimization of deep nets. Chen et al. (2019) benchmarked metric-based few-shot learners. Self-supervised pretraining combined with shallow adaptation has also shown strong results (Gidaris et al., 2019; Tian et al., 2020).\n\nTransductive inference and task-specific adaptation further improve performance by leveraging query-set statistics (Dhillon et al., 2020; Sun et al., 2019).",
    "reason": "The span jumps from a meta-learning algorithm to a backbone architecture and then to metric-based few-shot methods without explaining how the backbone discussion connects to the few-shot methods.",
    "start": 233,
    "end": 417,
    "label": "Coherence"
  },
  {
    "span": "Early fusion combines heterogeneous features into a unified vector (Nguyen et al., 2016). Late fusion aggregates modality-specific predictions (Poria et al., 2017). Attention-based methods align audio and text representations (Tsai et al., 2019). Visual transformers improve cross-modal reasoning (Dosovitskiy et al., 2020).",
    "document": "Related Work\n\nMultimodal sentiment analysis (MSA) seeks to leverage complementary cues from text, audio, and vision to better capture users' affective states. Early studies highlighted the difficulty of aligning asynchronous streams and mitigating modality noise, especially in conversational settings where cues are sparse and context-dependent. Recent benchmarks emphasize robustness to missing modalities and domain shift across speakers and platforms.\n\nEarly fusion combines heterogeneous features into a unified vector (Nguyen et al., 2016). Late fusion aggregates modality-specific predictions (Poria et al., 2017). Attention-based methods align audio and text representations (Tsai et al., 2019). Visual transformers improve cross-modal reasoning (Dosovitskiy et al., 2020).\n\nDespite advances, prior work often evaluates under closed-world assumptions. We focus on open-vocabulary settings where lexical drift and speaker-specific prosody challenge rigid fusion strategies. Our approach targets adaptive cross-modal calibration with uncertainty-aware alignment.",
    "reason": "The span lists several approaches and citations without transitions or explicit relationships between them or to the surrounding discussion, creating abrupt shifts and unclear connections.",
    "start": 457,
    "end": 781,
    "label": "Coherence"
  },
  {
    "span": "Prior work on multimodal sarcasm detection explores textual cues (Rajadesingan et al., 2015; Bamman and Smith, 2015), visual features from CNNs (Hu et al., 2017), audio prosody (Chen and Hasegawa-Johnson, 2019), multimodal transformers (Zadeh et al., 2018; Tsai et al., 2019), and large-scale social media datasets (Cai et al., 2019; Xiong et al., 2020).",
    "document": "Introduction\n\nSarcasm detection is challenging due to context dependence and the need to reconcile signals across modalities. Social media posts often contain short text, images, and occasionally audio/video, with subtle incongruities hinting at sarcastic intent. We study robustness to spurious correlations by leveraging cross-modal counterfactual augmentations.\n\nPrior work on multimodal sarcasm detection explores textual cues (Rajadesingan et al., 2015; Bamman and Smith, 2015), visual features from CNNs (Hu et al., 2017), audio prosody (Chen and Hasegawa-Johnson, 2019), multimodal transformers (Zadeh et al., 2018; Tsai et al., 2019), and large-scale social media datasets (Cai et al., 2019; Xiong et al., 2020).\n\nWe propose counterfactual pairing that systematically perturbs one modality while holding others fixed, measuring reliance on artifacts and improving generalization to out-of-domain platforms.",
    "reason": "The span simply catalogs prior techniques and datasets with no explanation of how they compare to or motivate the proposed counterfactual approach, fitting (a) and (c).",
    "start": 366,
    "end": 720,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Most prior audits rely on post-hoc metrics rather than causal evaluation",
    "document": "Related Work\n\nFairness Auditing in Machine Learning\n\nFairness auditing aims to uncover disparities in model performance across protected groups. Classical approaches report group-wise error rates, calibration gaps, or threshold-based parity metrics. More recent work investigates counterfactual analyses and path-specific decompositions to separate allocation from behavioral effects.\n\nMost prior audits rely on post-hoc metrics rather than causal evaluation, emphasizing observational disparities without establishing mechanisms. This leaves open questions about intervention design and the stability of conclusions under dataset shift.\n\nWe bridge this gap by introducing a modular causal auditing framework that decomposes disparities into data, model, and deployment components, enabling targeted mitigation strategies.",
    "reason": "Makes a literature-wide generalization about prior work (“Most prior audits…”) without citing representative studies to substantiate the claim.",
    "start": 386,
    "end": 458,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Transformer Vaswani et al. (2017))",
    "document": "Introduction\n\nNeural machine translation (NMT) has been transformed by attention-based architectures, most notably the Transformer (Vaswani et al., 2017), which replaces recurrence with multi-head self-attention (Bahdanau et al., 2015; Gehring et al., 2017). Pretraining and multilingual transfer further boost performance in low-resource settings (Lample and Conneau, 2019; Aharoni et al., 2019).\n\nHowever, real-world deployments must balance latency and accuracy, particularly in simultaneous translation (Ma et al., 2020). Knowledge distillation and pruning reduce computational cost but often degrade translation quality if applied aggressively (Kim and Rush, 2016; Sanh et al., 2020). Recent work studies adaptive computation and early-exit strategies to maintain quality under time constraints (Schwartz et al., 2020).\n\nWe revisit layer-sharing and partial decoding with a latency-aware objective, showing consistent gains across En↔De and En↔Zh tasks. Our implementation integrates with standard baselines (Transformer Vaswani et al. (2017)) and supports streaming evaluation.",
    "reason": "Nested/combined citation improperly mixes a model name with a narrative citation inside parentheses; should be 'Transformer (Vaswani et al., 2017)' or 'Transformer by Vaswani et al. (2017)'.",
    "start": 1013,
    "end": 1048,
    "label": "Format"
  },
  {
    "span": "In (Lopez et al., 2021)",
    "document": "Introduction\n\nMultimodal fusion methods aim to integrate signals from text, audio, and vision for robust affect recognition. Prior work has explored late fusion with independent unimodal encoders (Baltrušaitis et al., 2019) and early fusion with shared transformers (Tsai et al., 2019). In (Lopez et al., 2021) a hierarchical cross-attention mechanism is introduced, but it struggles when modalities are missing at inference time. We address this by learning modality-conditioned gates trained with stochastic modality dropout (Neverova et al., 2016), improving resilience under partial observations.\n\nRelated Work\n\nRecent advances in masked modeling for multimodal data (Shi et al., 2022) demonstrate strong transfer across domains. Our approach differs by explicitly estimating uncertainty per modality, complementing reliability-weighted fusion (Wang and Wang, 2020).",
    "reason": "Wrong citation style for narrative use. The construction should be “In Lopez et al. (2021) ...” rather than placing the authors and year in parentheses after “In”.",
    "start": 287,
    "end": 310,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claim that spectral methods are less scalable than spatial methods for large graphs.",
    "document": "Related Work\n\nGraph neural networks (GNNs) can be broadly grouped into spectral and spatial architectures. Spectral methods define convolutions via graph Laplacian eigenbases, while spatial methods operate through message passing over local neighborhoods. In a previous study, the authors claim that spectral methods are less scalable than spatial methods for large graphs. Subsequent work has attempted to bridge this gap with polynomial filters and sampling strategies, yet questions persist around stability and over-smoothing. We revisit these issues by analyzing neighborhood growth and normalization in high-diameter graphs.",
    "reason": "Refers to a specific prior study and its claim without providing a citation to that study (criterion a).",
    "start": 256,
    "end": 373,
    "label": "Unsupported_claim"
  },
  {
    "span": "Explanations can calibrate user trust in AI systems (Barker and Evans, 2018). Mental models influence how users interpret system feedback (Singh et al., 2019). Task framing affects perceived reliability (Carter and Holm, 2020).",
    "document": "Introduction\n\nUnderstanding when and why users trust model outputs is central to the deployment of decision support tools. Prior research emphasizes alignment between model transparency and user needs, as well as the dynamics of iterative human–AI interaction.\n\nExplanations can calibrate user trust in AI systems (Barker and Evans, 2018). Mental models influence how users interpret system feedback (Singh et al., 2019). Task framing affects perceived reliability (Carter and Holm, 2020). While these findings are broadly consistent with calls for context-sensitive explanations (Johnson et al., 2021), existing evidence is fragmented across domains.\n\nThis paper unifies these perspectives by examining how explanation granularity, timing, and modality jointly shape trust trajectories in a multi-session study.",
    "reason": "The sentences enumerate separate findings with different citations but do not state how they connect to each other; there are no transitions clarifying relationships, leading to an abrupt sequence across multiple sentences.",
    "start": 262,
    "end": 489,
    "label": "Coherence"
  },
  {
    "span": "Two-stage detectors leverage region proposals for accuracy (Ren et al., 2015). One-stage methods emphasize speed with dense predictions (Redmon et al., 2016). Transformers replaced convolutions for global reasoning (Carion et al., 2020). Anchor-free designs avoid predefined boxes (Tian et al., 2019).",
    "document": "Related Work\n\nObject detection has evolved from handcrafted features to deep neural architectures that jointly reason about appearance and localization. Contemporary systems vary in how they trade off speed and accuracy, and in the architectural primitives they employ. We review representative families and identify the gap addressed by our method.\n\nTwo-stage detectors leverage region proposals for accuracy (Ren et al., 2015). One-stage methods emphasize speed with dense predictions (Redmon et al., 2016). Transformers replaced convolutions for global reasoning (Carion et al., 2020). Anchor-free designs avoid predefined boxes (Tian et al., 2019).\n\nWhile these approaches advance performance, cross-domain generalization to adverse conditions remains limited. We propose a detector that unifies uncertainty-aware localization with self-training to close this gap under distribution shift.",
    "reason": "The span enumerates multiple detection paradigms without signaling how each relates to the previous one or to a shared theme; transitions and explicit connections are missing.",
    "start": 351,
    "end": 652,
    "label": "Coherence"
  },
  {
    "span": "Long et al. (2015) presented fully convolutional networks for semantic segmentation. Ronneberger et al. (2015) developed U-Net for biomedical images. Chen et al. (2017) proposed DeepLab with atrous convolutions.",
    "document": "Related Work\n\nSemantic segmentation methods have evolved from patch-based classifiers to fully end-to-end dense predictors. The field now leverages multi-scale context, encoder–decoder topologies, and advanced training regimes to improve boundary fidelity and robustness under limited supervision.\n\nArchitectural progress\n\nLong et al. (2015) presented fully convolutional networks for semantic segmentation. Ronneberger et al. (2015) developed U-Net for biomedical images. Chen et al. (2017) proposed DeepLab with atrous convolutions. Zhao et al. (2017) introduced pyramid pooling to aggregate context. Badrinarayanan et al. (2017) emphasized efficiency with SegNet.\n\nSelf-training and weak supervision\n\nTo reduce annotation costs, researchers explored pseudo-labeling, scribble supervision, and image-level supervision, often combining class activation maps with CRF-based or region-growing refinements.\n\nOur work complements these directions by focusing on boundary-aware consistency under mixed supervision, introducing a calibration mechanism that aligns low-level edge cues with high-level semantic priors without incurring prohibitive memory costs.",
    "reason": "The three sentences list different models in sequence without transitions or explanation of their relationships or comparative contributions, causing abrupt shifts. This satisfies the multi-sentence coherence issue in (a) and (b).",
    "start": 323,
    "end": 534,
    "label": "Coherence"
  },
  {
    "span": "Bolukbasi et al. (2016) quantify gender bias in word embeddings and propose debiasing. Counterfactual data augmentation improves robustness to spurious correlations (Kaushik et al., 2020). Differential privacy adds noise to training to protect individuals (Dwork et al., 2014).",
    "document": "Related Work\n\nFairness and Robustness in Language Technologies\n\nBias measurement, mitigation, and privacy are central to deploying NLP systems responsibly. However, methods vary in assumptions, targets, and guarantees. Bolukbasi et al. (2016) quantify gender bias in word embeddings and propose debiasing. Counterfactual data augmentation improves robustness to spurious correlations (Kaushik et al., 2020). Differential privacy adds noise to training to protect individuals (Dwork et al., 2014). Despite progress, aligning group fairness, individual privacy, and task utility remains an open challenge.\n\nEvaluation Benchmarks\n\nDatasets such as WinoBias and StereoSet capture different bias types and contexts. We evaluate across multiple axes to probe trade-offs introduced by our method.",
    "reason": "The cited works address different problem facets (debiasing, robustness, privacy) with no explicit connections or transitions, making the relationships unclear.",
    "start": 219,
    "end": 496,
    "label": "Coherence"
  },
  {
    "span": "Prior work at the intersection of causal inference and text falls into three broad categories. One estimates propensity scores or nuisance functions using text encoders within doubly robust estimators. Another uses structural causal models with textual proxies or instruments to adjust for confounding. A third line develops counterfactual generation to probe model reliance on spurious lexical features. In contrast, we introduce a contrastive learning objective for treatment effect estimation on text.",
    "document": "Introduction\nEstimating treatment effects from observational data with free-form text is challenging due to high-dimensional confounding and selection bias. While text encoders provide rich representations, aligning them with causal objectives remains non-trivial.\n\nRelated Work\nText in causal pipelines has been used as high-dimensional covariates, surrogates, or proxies to address unobserved confounding.\nPrior work at the intersection of causal inference and text falls into three broad categories. One estimates propensity scores or nuisance functions using text encoders within doubly robust estimators. Another uses structural causal models with textual proxies or instruments to adjust for confounding. A third line develops counterfactual generation to probe model reliance on spurious lexical features. In contrast, we introduce a contrastive learning objective for treatment effect estimation on text.\nEvaluation commonly involves semi-synthetic datasets and limited real-world case studies, with metrics such as PEHE and ATE error.\n\nOverview\nWe propose a training scheme that aligns text representations with treatment balancing through contrastive objectives.",
    "reason": "The span summarizes prior categories and immediately states the paper's contribution without articulating the specific gap or why contrastive learning is needed, exemplifying (b) and (c).",
    "start": 408,
    "end": 912,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Rahul et al. (2021) demonstrate adversarial prompt attacks against dialog systems. Brown et al. (2020) present GPT-3 and show scaling laws for few-shot learning. Perez et al. (2022) study chain-of-thought prompting for reasoning. Wallace et al. (2019) reveal universal adversarial triggers for NLP models.",
    "document": "Introduction\n\nLarge language models (LLMs) have rapidly advanced capabilities in generation, reasoning, and task transfer, but their safety and robustness remain active areas of inquiry. Researchers examine vulnerabilities, mitigations, and evaluation frameworks to better understand model behavior under distribution shifts and adversarial conditions.\n\nRahul et al. (2021) demonstrate adversarial prompt attacks against dialog systems. Brown et al. (2020) present GPT-3 and show scaling laws for few-shot learning. Perez et al. (2022) study chain-of-thought prompting for reasoning. Wallace et al. (2019) reveal universal adversarial triggers for NLP models.\n\nSeveral lines of work explore red-teaming methods, automated attack generation, and robust training. Concurrently, evaluation protocols for safety have emerged, including stress tests for harmful content and alignment with human preferences. Our work focuses on robustness to adaptive prompt-based attacks and proposes a standardized benchmark to compare defenses across model families.",
    "reason": "Consecutive sentences enumerate disparate works without transitions or explicit links, making it unclear how scaling laws, chain-of-thought prompting, and adversarial triggers relate to each other in the context of safety.",
    "start": 354,
    "end": 659,
    "label": "Coherence"
  },
  {
    "span": "Garcia et al. 2",
    "document": "Related Work\n\nMulti-task learning (MTL) aims to exploit shared structure among related tasks (Caruana, 1997). Parameter-efficient MTL introduces shared adapters that reduce interference across tasks (Pfeiffer et al., 2021; Mahabadi et al., 2021). Garcia et al. 2 report that sparse routing between tasks improves both stability and transfer, but later analyses question the generality of those gains (Ruder, 2017; Standley et al., 2020). Our work complements these findings by evaluating task affinity measures for dynamic routing.",
    "reason": "Improper footnote-like notation in place of a year; should include a publication year or be formatted as a proper footnote.",
    "start": 247,
    "end": 262,
    "label": "Format"
  },
  {
    "span": "A previous study demonstrated that self-supervised pretraining halves the sample complexity for object detection in low-data regimes.",
    "document": "Introduction\n\nLabel scarcity remains a major obstacle for robust object detection in specialized domains. Self-supervised pretraining has emerged as a promising way to leverage large unlabeled corpora and transfer useful visual representations to downstream detectors.\n\nA previous study demonstrated that self-supervised pretraining halves the sample complexity for object detection in low-data regimes. Building on this intuition, we investigate whether task-adaptive pretext objectives further improve transfer by aligning representation learning with detection-specific invariances.",
    "reason": "Claims a specific result attributed to a prior study but provides no citation; per rule (b), niche, specific claims about prior work must be cited.",
    "start": 270,
    "end": 403,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Smith et al., 2020)",
    "document": "Introduction\n\nDomain adaptation seeks to transfer knowledge from a labeled source domain to an unlabeled or sparsely labeled target domain under distribution shift (Pan and Yang, 2010; Ganin et al., 2016). In (Smith et al., 2020) the authors highlight that representation alignment benefits from class-conditional structure rather than purely marginal matching, motivating conditional adversarial training. Subsequent approaches integrate self-training and consistency regularization to stabilize the alignment process (Zou et al., 2019; Xu et al., 2020), while semi-supervised variants exploit a handful of target labels for calibration (Berthelot et al., 2019; Sohn et al., 2020).",
    "reason": "Wrong citation style: a parenthetical citation should not be preceded by 'In'; use a narrative form like Smith et al. (2020) or remove the preposition.",
    "start": 206,
    "end": 229,
    "label": "Format"
  },
  {
    "span": "(Artetxe et al., 2018",
    "document": "Introduction\n\nUnsupervised and low-resource machine translation (MT) aim to reduce dependence on parallel corpora by leveraging monolingual data (Lample et al., 2018a; Conneau and Lample, 2019). Denoising objectives and back-translation have proven effective across language pairs (Sennrich et al., 2016; Edunov et al., 2018), particularly when combined with shared subword units and initialization from multilingual encoders (Devlin et al., 2019; Conneau et al., 2020).\n\nEarly work demonstrated that bilingual dictionaries and iterative refinement can bootstrap translation quality (Mikolov et al., 2013; Smith et al., 2017). The introduction of cross-lingual language models further improved alignment quality and robustness to domain shift (Lample and Conneau, 2019; Liu et al., 2020). Strong baselines for unsupervised MT were established by (Artetxe et al., 2018 and Lample et al. (2018b), motivating research on data selection and noise control.\n\nIn this paper we revisit data curriculum and iterative back-translation schedules, proposing a temperature-annealed sampling scheme that prioritizes lexically simpler sentences early in training. We show improvements on low-resource pairs with limited comparable corpora, and provide analyses of lexical coverage and domain mismatch.\n\nRelated work includes domain adaptation for MT (Chu and Wang, 2018), multilingual transfer (Johnson et al., 2017), and evaluation under noisy inputs (Belinkov and Bisk, 2018).",
    "reason": "Missing closing parenthesis in a parenthetical citation; the citation \"(Artetxe et al., 2018\" lacks a closing \")\".",
    "start": 846,
    "end": 867,
    "label": "Format"
  },
  {
    "span": "(Nguyen et al., 2022",
    "document": "Introduction\n\nGraph neural networks (GNNs) have been successfully applied to molecular property prediction, recommendation, and reasoning tasks (Gilmer et al., 2017; Hamilton et al., 2017). A key challenge is capturing long-range dependencies without oversmoothing node representations (Li et al., 2018; Oono and Suzuki, 2020). Hierarchical pooling and spectral filtering have been proposed to address this limitation (Ying et al., 2018; Wu et al., 2019). Recent attempts incorporate positional encodings to better capture structural signals (Dwivedi et al., 2021; Nguyen et al., 2022). Our work combines structural encodings with adaptive message passing to balance expressivity and stability (Sun et al., 2023).",
    "reason": "Mismatched parenthesis: missing closing parenthesis in the parenthetical citation.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "Recent works have shown dramatic gains from self-supervised pretraining on code-switched speech.",
    "document": "Related Work\n\nSelf-supervised pretraining has transformed automatic speech recognition (ASR), particularly in low-resource settings. Methods such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) learn powerful acoustic representations that can be fine-tuned with limited labeled data. Code-switching, where speakers alternate between languages within an utterance, presents additional challenges due to mixed phonotactics and orthographies. Recent works have shown dramatic gains from self-supervised pretraining on code-switched speech. However, most evaluations remain restricted to a handful of language pairs and rely on proprietary data, limiting reproducibility. Our study investigates open benchmarks and provides a standardized evaluation protocol across multiple code-switching scenarios.",
    "reason": "Claims the existence and effect of 'recent works' without citing any specific studies (rule d).",
    "start": 460,
    "end": 556,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from ASAP and exceeded human agreement.",
    "document": "Introduction\n\nAutomatic Essay Scoring (AES) aims to approximate human grading to support large-scale assessment. Early systems relied on handcrafted features and regression models (Shermis and Burstein, 2013). With the advent of pretrained language models, neural AES has improved robustness to spurious lexical cues (Devlin et al., 2019; Yang et al., 2020).\n\nThe ASAP dataset provides a widely used benchmark with prompts spanning narrative and persuasive writing. BERT was used in an AES task trained on essays from ASAP and exceeded human agreement. While encouraging, such results may reflect prompt-specific artifacts rather than general writing quality.\n\nWe propose a prompt-agnostic calibration approach that reduces overfitting and improves transfer across prompts.",
    "reason": "Claims a prior study, specific model–dataset setup, and an outcome relative to human agreement without any supporting citation (rule a and b).",
    "start": 466,
    "end": 552,
    "label": "Unsupported_claim"
  },
  {
    "span": "There has been a surge of recent works on controlled paraphrase generation for factual style transfer.",
    "document": "Introduction\n\nParaphrase generation aims to produce alternative phrasings that preserve meaning while varying surface form. Controlled paraphrasing further allows attributes such as formality, complexity, or lexical diversity to be targeted (Hu et al., 2017; Madaan et al., 2020). Techniques based on conditional generation and constrained decoding have shown steady improvements in fluency and semantic faithfulness (Krishna et al., 2023). There has been a surge of recent works on controlled paraphrase generation for factual style transfer. Despite progress, controlling factuality without hallucination remains challenging (Maynez et al., 2020). In this paper, we propose a contrastively guided decoding method that encourages paraphrases to remain both faithful and attribute-aligned, and we evaluate it across multiple corpora.\n\nOur contributions are threefold: (1) a training-free controller that integrates factual constraints during beam search, (2) a diagnostic benchmark for factual style transfer with human and automatic metrics, and (3) an analysis of brittleness under entity perturbations. We release code and data to facilitate reproducibility.",
    "reason": "Mentions 'recent works' without providing any citations to support the claim (violates rule d).",
    "start": 441,
    "end": 543,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Klein 2017)",
    "document": "Introduction. Fairness in machine learning has been examined through group, individual, and counterfactual lenses (Dwork et al., 2012; Kusner et al., 2017). Auditing methods estimate disparities across protected groups and propose corrections via post-processing or constrained optimization (Hardt et al., 2016; Zafar et al., 2017). However, data shift undermines static fairness guarantees, necessitating adaptive monitoring (Liu et al., 2018; Corbett-Davies & Goel, 2018). Early critiques argued for domain-specific metrics that align with legal standards (Barocas & Selbst, 2016; Klein 2017). We contribute a framework that unifies monitoring with interpretable, policy-aware constraints.",
    "reason": "Missing comma between author and year in parenthetical citation per author–year style: '(Klein 2017)' should be '(Klein, 2017)'.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "Elastic Weight Consolidation reduces interference by penalizing updates on important parameters (Kirkpatrick et al., 2017). Off-policy replay can improve sample efficiency in value-based RL (Mnih et al., 2015). Meta-learning formulations adapt to new tasks rapidly (Finn et al., 2017).",
    "document": "Related Work\n\nContinual Reinforcement Learning\n\nContinual reinforcement learning (CRL) studies agents that learn from non-stationary streams without catastrophic forgetting and with bounded memory. Approaches broadly include replay-based rehearsal, regularization of important parameters, modularization, and meta-learning. Replay-based methods maintain a memory of trajectories to rehearse prior tasks (Rolnick et al., 2019; Chaudhry et al., 2019), sometimes selecting exemplars by gradient impact (Aljundi et al., 2019). Regularization methods estimate parameter importance and constrain updates to retain prior competence (Kirkpatrick et al., 2017; Zenke et al., 2017). Task-identity-free approaches leverage context inference and gating to separate representations (Mendez et al., 2020; Xu and Zhu, 2018). Elastic Weight Consolidation reduces interference by penalizing updates on important parameters (Kirkpatrick et al., 2017). Off-policy replay can improve sample efficiency in value-based RL (Mnih et al., 2015). Meta-learning formulations adapt to new tasks rapidly (Finn et al., 2017). Benchmarks increasingly emphasize non-stationary control with sparse rewards and distractors (Nichol et al., 2018; Wadhwa et al., 2021), while evaluation protocols consider both forward and backward transfer (Lopez-Paz and Ranzato, 2017).",
    "reason": "The span rapidly shifts from parameter regularization to off-policy replay to meta-learning without transitions or an explicit explanation of how these works relate to each other in the context of continual RL.",
    "start": 810,
    "end": 1095,
    "label": "Coherence"
  },
  {
    "span": "Raffel et al., 2019)",
    "document": "Related Work\n\nPretrained sequence-to-sequence models have transformed text generation, with gains in summarization, translation, and data-to-text tasks (Lee and Carter, 2019; Wang and Zhou, 2020). Among these, BART (Lewis and Peters, 2019) and T5 Raffel et al., 2019) are widely used as base encoders and decoders for downstream tasks. Recent adaptations introduce retrieval-augmented generation to incorporate external evidence (Huang and Kim, 2020) and multi-source encoders to fuse structured inputs (Chen and Rao, 2021). We build on this line by adding a constrained decoding layer that enforces schema validity without retraining the backbone.",
    "reason": "Missing opening parenthesis in a parenthetical citation; it should be '(Raffel et al., 2019)'.",
    "start": 247,
    "end": 267,
    "label": "Format"
  },
  {
    "span": "[Ein-Dor et al., 2020)",
    "document": "Related Work\n\nPretrained transformers have improved data efficiency across tasks (Vaswani et al., 2017; Peters et al., 2018). Recent evaluations [Ein-Dor et al., 2020) suggest that simple uncertainty heuristics remain competitive with Bayesian methods on NER. However, inconsistencies in evaluation setups complicate comparisons (Prabhu et al., 2019). We standardize splits and budgets to mitigate these issues.",
    "reason": "Mismatched brackets; opening square bracket with closing parenthesis.",
    "start": 145,
    "end": 167,
    "label": "Format"
  },
  {
    "span": "Neural code summarization has employed sequence-to-sequence encoders, graph-based representations of ASTs, and pretrained models like CodeBERT and PLBART (Iyer et al., 2016; Alon et al., 2019; Ahmad et al., 2020; Feng et al., 2020; Ahmad et al., 2021).",
    "document": "Related Work\n\nCode Summarization. Automatically generating natural language descriptions of source code assists comprehension and maintenance. Neural code summarization has employed sequence-to-sequence encoders, graph-based representations of ASTs, and pretrained models like CodeBERT and PLBART (Iyer et al., 2016; Alon et al., 2019; Ahmad et al., 2020; Feng et al., 2020; Ahmad et al., 2021). Beyond architecture design, recent efforts investigate copy mechanisms, contrastive learning, and retrieval augmentation to better ground summaries in code semantics (Zhong et al., 2020; Liu et al., 2021; Parvez et al., 2021).\n\nBenchmarks and Metrics. Datasets such as CodeSearchNet and Java-large are standard, but metric reliability and semantic faithfulness remain debated (Henderson et al., 2020; Fomicheva and Specia, 2016).",
    "reason": "The sentence enumerates prior work without relating it to the authors' approach or clarifying what specific limitation the paper addresses.",
    "start": 143,
    "end": 395,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Early medical ViT efforts fine-tune pretrained transformers for classification and segmentation (Dosovitskiy et al., 2021; Carion et al., 2020; Chen et al., 2021), and hybrid CNN-Transformer backbones have been explored across CT, MRI, and fundus images (Xie et al., 2021; Wang et al., 2022; Hatamizadeh et al., 2022).",
    "document": "Introduction\n\nTransformers in medical imaging. Vision Transformers (ViTs) have recently achieved strong performance in natural image benchmarks, prompting their adaptation to medical applications where data and annotations are limited. Early medical ViT efforts fine-tune pretrained transformers for classification and segmentation (Dosovitskiy et al., 2021; Carion et al., 2020; Chen et al., 2021), and hybrid CNN-Transformer backbones have been explored across CT, MRI, and fundus images (Xie et al., 2021; Wang et al., 2022; Hatamizadeh et al., 2022). Semi-supervised and self-supervised strategies have also been examined to leverage unlabeled scans (Tajbakhsh et al., 2020; Tang et al., 2022).\n\nOur focus. We study data-efficient segmentation under sparse annotations with strong domain shifts across sites.\n\nSummary of contributions. We propose a patch-wise distribution alignment module paired with contrastive pretraining that improves cross-site generalization with minimal labels.",
    "reason": "This span summarizes prior ViT adaptations without connecting them to the authors’ goals or identifying what is missing, providing no synthesis or explicit gap.",
    "start": 236,
    "end": 554,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Inverse propensity scoring estimators are known to exhibit high variance in real-world recommender systems.",
    "document": "Related Work\n\nOff-policy evaluation (OPE) estimates the performance of new recommenders from logged bandit data. Importance sampling and its variants provide unbiased but potentially high-variance estimators (Li et al., 2011; Swaminathan and Joachims, 2015; Dudík et al., 2014). Doubly robust and self-normalized methods trade bias and variance under model misspecification (Dudík et al., 2011; Thomas and Brunskill, 2016; Jiang and Li, 2016).\n\nInverse propensity scoring estimators are known to exhibit high variance in real-world recommender systems. As a consequence, practitioners resort to heavy clipping and heuristics that introduce bias but stabilize evaluation.\n\nWe analyze the variance-bias frontier of clipped estimators and propose a data-dependent clipping scheme with finite-sample guarantees.",
    "reason": "Asserts a widely known property in a specific application context without citing empirical studies or reports (rule b).",
    "start": 445,
    "end": 552,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most public benchmarks use interaction logs truncated to the last 100 days.",
    "document": "Related Work\n\nSequential recommendation leverages users' recent actions to predict next-item interactions, often with transformer architectures that model long-range dependencies (Kang and McAuley, 2018; Sun et al., 2019). Regularization strategies such as item masking and time-aware sampling aim to reduce popularity bias and temporal drift (Zhang et al., 2021; Chen et al., 2022).\n\nEvaluation protocols vary widely across datasets and domains, affecting comparability of reported results (Krichene and Rendle, 2020). Most public benchmarks use interaction logs truncated to the last 100 days. This practice simplifies temporal alignment across users but can misrepresent seasonality and cold-start effects.\n\nWe advocate for rolling-window evaluation with non-overlapping test periods and introduce a drift-aware metric that discounts repeated exposures while maintaining calibration sensitivity.",
    "reason": "The sentence makes a broad claim about benchmark construction practices without referencing specific datasets or papers that use such truncation.",
    "start": 520,
    "end": 595,
    "label": "Unsupported_claim"
  },
  {
    "span": "Previous shared tasks have reported that back-translation yields up to 5 BLEU improvements in this regime.",
    "document": "Introduction\n\nNeural machine translation (NMT) in low-resource settings faces data scarcity that limits generalization (Koehn and Knowles, 2017). Semi-supervised techniques such as back-translation and self-training can leverage monolingual corpora to offset data limitations (Sennrich et al., 2016; He et al., 2020). Transfer learning from related high-resource pairs further improves performance (Zoph et al., 2016).\n\nPrevious shared tasks have reported that back-translation yields up to 5 BLEU improvements in this regime. Despite these gains, sensitivity to domain mismatch and noise in synthetic data remains a major challenge. We propose a risk-aware filtering and annealing strategy that stabilizes training under noisy synthetic targets.\n\nRelated Work\n\nRecent efforts explore multilingual pretraining for low-resource MT (Conneau and Lample, 2019) and iterative refinement of synthetic data (Edunov et al., 2018). Data selection techniques based on cross-entropy difference have also proved beneficial (Moore and Lewis, 2010).",
    "reason": "Claims quantitative results from 'shared tasks' without citing the specific tasks or reports that substantiate the 5 BLEU improvement.",
    "start": 420,
    "end": 526,
    "label": "Unsupported_claim"
  },
  {
    "span": "Gradient inversion attacks reconstruct user data from updates (Zhu et al., 2019). Secure aggregation prevents the server from viewing individual gradients (Bonawitz et al., 2017). Label leakage can occur through logits (Song and Raghunathan, 2020). Personalization layers adapt models to local domains (Arivazhagan et al., 2019).",
    "document": "Related Work\n\nPrivacy attacks in federated learning. Client updates can inadvertently leak sensitive information about training data via gradients, statistics, or shared representations (Zhu et al., 2019; Geiping et al., 2020). Defenses include perturbation, cryptographic protocols, and aggregation strategies that conceal individual contributions (Bonawitz et al., 2017; Truex et al., 2019).\n\nGradient inversion attacks reconstruct user data from updates (Zhu et al., 2019). Secure aggregation prevents the server from viewing individual gradients (Bonawitz et al., 2017). Label leakage can occur through logits (Song and Raghunathan, 2020). Personalization layers adapt models to local domains (Arivazhagan et al., 2019). In this work, we analyze how personalization interacts with secure aggregation under gradient inversion threats, proposing a defense that bounds mutual information between personalized heads and shared trunks.",
    "reason": "The cited works are presented as isolated statements without transitions or explicit relations. The leap from gradient inversion to secure aggregation to label leakage and then to personalization lacks connective explanation, reducing coherence.",
    "start": 395,
    "end": 724,
    "label": "Coherence"
  },
  {
    "span": "See et al. (2017) proposed pointer-generator networks for abstractive summarization. Narayan et al. (2018) introduced extreme summarization with news headlines. Liu and Lapata (2019) fine-tuned BERTSUM for extractive and abstractive variants. Zhong et al. (2020) studied factual consistency constraints.",
    "document": "Related Work\n\nText Summarization\n\nSummarization approaches range from extractive selection to fully abstractive generation, with increasing attention to faithfulness and controllability. Evaluation metrics lag behind model capabilities, creating a growing emphasis on factuality and calibration.\n\nSee et al. (2017) proposed pointer-generator networks for abstractive summarization. Narayan et al. (2018) introduced extreme summarization with news headlines. Liu and Lapata (2019) fine-tuned BERTSUM for extractive and abstractive variants. Zhong et al. (2020) studied factual consistency constraints.\n\nFaithfulness and Evaluation\n\nA parallel literature proposes automatic factuality metrics and constrained decoding to mitigate hallucinations, but reported gains often depend on dataset-specific artifacts.\n\nOur Contribution\n\nWe present a constraint-aware planner-decoder that disentangles content planning from surface realization, improving factuality under distribution shift.",
    "reason": "The sentences cite works one after another without transitions or explanations of how they connect, leaving the relationships among methods and contributions implicit rather than explicit.",
    "start": 297,
    "end": 600,
    "label": "Coherence"
  },
  {
    "span": "Prior competitions have shown that gradient obfuscation remains prevalent.",
    "document": "Related Work\n\nAdversarial robustness has been benchmarked through standardized attacks and leaderboards to prevent evaluation pitfalls (Carlini and Wagner, 2017; Athalye et al., 2018). Strong baselines emphasize transparent threat models and reproducible conditions (Croce and Hein, 2020). Despite guidance, defenses that mask gradients or rely on stochasticity continue to appear in the literature.\n\nPrior competitions have shown that gradient obfuscation remains prevalent. This motivates our focus on certifiable defenses and adaptive white-box evaluations that minimize the risk of false security. We present an attack suite that explicitly diagnoses masking through transferability and loss landscape analyses.\n\nIntroduction\n\nWe further contribute a robustness card that documents attack settings, compute budgets, and failure cases to encourage comparable reporting.",
    "reason": "References findings from 'prior competitions' without citing the specific competitions or reports that substantiate the claim.",
    "start": 401,
    "end": 475,
    "label": "Unsupported_claim"
  },
  {
    "span": "McMahan et al. (2017) introduced federated averaging to train models across devices. Bonawitz et al. (2017) presented secure aggregation for privacy-preserving training. Kairouz et al. (2021) surveyed advances and open problems in federated learning. Bagdasaryan et al. (2020) analyzed backdoor attacks in federated settings.",
    "document": "Related Work\n\nFederated learning enables training over decentralized data while constraining raw data movement, raising questions about privacy, robustness, and statistical heterogeneity. Methods vary in communication strategies, personalization, and security guarantees.\n\nMcMahan et al. (2017) introduced federated averaging to train models across devices. Bonawitz et al. (2017) presented secure aggregation for privacy-preserving training. Kairouz et al. (2021) surveyed advances and open problems in federated learning. Bagdasaryan et al. (2020) analyzed backdoor attacks in federated settings.\n\nSubsequent work addresses client drift and personalization via proximal objectives and meta-learning (Li et al., 2020; Fallah et al., 2020). Our approach focuses on robust aggregation under targeted poisoning with limited client participation.",
    "reason": "These sentences are juxtaposed without indicating how secure aggregation, surveys, and backdoor attacks relate to or build on federated averaging. The lack of transitions makes the connections between works unclear.",
    "start": 273,
    "end": 598,
    "label": "Coherence"
  },
  {
    "span": "Garcia et al. 3",
    "document": "Introduction\n\nInteractive program synthesis systems rely on user feedback to refine candidate programs (Gulwani et al., 2015). While ranking-based synthesis improves sample efficiency (Devlin et al., 2017), ambiguity remains a key bottleneck. We propose clarifying questions that maximize information gain under a probabilistic user model, extending the pragmatic reasoning of Garcia et al. 3 to the code domain.\n\nRelated Work\n\nQuestion selection has been studied in active learning (Settles, 2009) and dialog systems (Zhang et al., 2018). Our information-theoretic objective aligns with recent work on uncertainty-guided interaction (Kulkarni et al., 2020).",
    "reason": "Improper use of a superscript-style footnote/marker without a year. It should be a proper citation with year, e.g., “Garcia et al. (2019)”, or a properly formatted footnote if intended.",
    "start": 377,
    "end": 392,
    "label": "Format"
  },
  {
    "span": "According to (Davis, 2016)",
    "document": "Related Work\n\nHuman-computer interaction research highlights the importance of adaptive interfaces that calibrate assistance to user expertise (Lin and Cooper, 2018; Patel and Huang, 2020). According to (Davis, 2016), perceived usefulness and ease of use are primary determinants of technology adoption, a finding extended by subsequent studies that consider trust and transparency (Miller and Zhou, 2019). In parallel, explainable AI emphasizes actionable feedback that supports user goals rather than generic model introspection (Garcia and Noor, 2021).\n\nOur work operationalizes task-level intent recognition to modulate interface hints, combining implicit behavioral signals with explicit user preferences. We evaluate on a controlled lab study and a field deployment, measuring learning gains and sustained engagement (Khan and Rivera, 2022).",
    "reason": "Wrong citation style using a parenthetical citation after 'According to'; should be 'According to Davis (2016)'.",
    "start": 190,
    "end": 216,
    "label": "Format"
  },
  {
    "span": "Smith et al., 2020)",
    "document": "Introduction\n\nCounterfactual data augmentation has been proposed to improve model fairness by perturbing sensitive attributes while preserving semantics (Garg et al., 2019; Maudslay et al., 2019). However, naive perturbations can introduce label noise or grammatical errors. Building on stylistic control methods, we construct counterfactuals with a constrained decoder and semantic validation similar to Smith et al., 2020) to maintain label fidelity.\n\nRelated Work\n\nFairness interventions at training time (Zhao et al., 2017) and post-processing calibration (Pleiss et al., 2017) are complementary to data augmentation. Our approach targets representation bias by expanding minority subspaces.",
    "reason": "Missing opening parenthesis for a parenthetical citation. It should be “(Smith et al., 2020)” or, if narrative, “Smith et al. (2020)”.",
    "start": 405,
    "end": 424,
    "label": "Format"
  },
  {
    "span": "Patel et al. 3",
    "document": "Related Work\n\nFederated learning reduces data centralization by training models locally and aggregating updates on a server (McMahan et al., 2017). Communication efficiency is improved with sparsification and quantization methods (Konečný et al., 2016; Lin et al., 2018). Privacy is commonly enforced with secure aggregation and differential privacy (Bonawitz et al., 2017; Abadi et al., 2016). Patel et al. 3 propose a hybrid protocol that combines secure aggregation with client-side clipping to limit influence of outliers, while subsequent work explores Byzantine-robust aggregation rules (Blanchard et al., 2017; Yin et al., 2018). Our approach focuses on adaptive participation to counter client drift under non-IID data (Li et al., 2020).",
    "reason": "Wrong use of footnote marker: narrative citation includes a superscript-like number but no year; should include the year or be formatted as a proper footnote.",
    "start": 395,
    "end": 409,
    "label": "Format"
  },
  {
    "span": "GLUE is effectively saturated, with most leading models achieving near-human or superhuman scores.",
    "document": "Related Work\n\nGeneral-purpose language understanding benchmarks have catalyzed rapid progress in pretrained models. As architectures and training regimes improved, leaderboard performance climbed steadily across tasks. GLUE is effectively saturated, with most leading models achieving near-human or superhuman scores. In response, researchers have proposed more challenging evaluations that emphasize robustness, compositionality, and data efficiency. Our work contributes by introducing stress tests that target specific reasoning failures.\n",
    "reason": "Asserts saturation and near-/superhuman performance on a benchmark without citing leaderboard data or studies.",
    "start": 219,
    "end": 317,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior competitions have standardized the evaluation protocol for cross-lingual part-of-speech tagging.",
    "document": "Related Work\n\nCross-lingual sequence labeling transfers supervision via multilingual representations, annotation projection, and pseudo-labeling (Täckström et al., 2013; Plank and Agić, 2018; Conneau et al., 2020). Universal dependencies provide a shared inventory that facilitates cross-treebank evaluation but raises questions about genre and domain mismatch (Nivre et al., 2016; Zeman et al., 2020).\n\nPrior competitions have standardized the evaluation protocol for cross-lingual part-of-speech tagging.\n\nWe revisit cross-lingual POS with an analysis of source-target typological distance, delineating label confusability driven by morphological richness and tokenization.",
    "reason": "Claims the role of unspecified competitions in standardizing evaluation without citing the competitions or reports; first mention of competitions requires references.",
    "start": 404,
    "end": 506,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent audio-visual speech recognition systems fuse acoustic and visual streams using temporal convolution, attention-based encoders, and transformer architectures, with datasets such as LRS2, LRS3, and GRID enabling progress (Afouras et al., 2018; Ma et al., 2021; Shillingford et al., 2019; Chung and Zisserman, 2017).",
    "document": "Introduction\n\nAudio-visual speech recognition (AVSR) exploits complementary cues from lips and acoustics to improve robustness. Real-world settings, however, exhibit asynchrony between modalities due to transmission delays and sensor drift. We target AVSR that remains stable under unknown, time-varying lags.\n\nRecent audio-visual speech recognition systems fuse acoustic and visual streams using temporal convolution, attention-based encoders, and transformer architectures, with datasets such as LRS2, LRS3, and GRID enabling progress (Afouras et al., 2018; Ma et al., 2021; Shillingford et al., 2019; Chung and Zisserman, 2017).\n\nWe introduce a lag-adaptive alignment module that jointly estimates offsets and performs fusion, improving recognition when lags vary across utterances.",
    "reason": "The span summarizes prior AVSR architectures and datasets but does not connect them to the challenge of unknown, time-varying lags or explain how prior works handle asynchrony (criteria a and b).",
    "start": 311,
    "end": 631,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT was used in an instruction-following agent trained with behavior cloning on the ALFRED dataset.",
    "document": "Related Work\n\nLanguage-conditioned embodied agents connect natural language understanding to perception and control. Early approaches coupled template parsers with symbolic planners, while recent methods learn from demonstrations to map instructions directly to actions. Vision-and-language navigation benchmarks highlight the need to ground linguistic references in visual context and to reason about object affordances. BERT was used in an instruction-following agent trained with behavior cloning on the ALFRED dataset. Other work explored hierarchical policies that decompose long-horizon tasks into subgoals, leveraging object-centric representations and spatial memory. Data augmentation via paraphrasing and environmental perturbations further improves robustness to instruction variability and visual noise. Despite these advances, agents often overfit to dataset biases and struggle with compositional generalization. We study a modular architecture with a language planner and a visuomotor controller, providing a systematic comparison of pretrained language encoders and multi-view perception under distribution shifts across homes and object layouts.",
    "reason": "Mentions a specific model setup (BERT used on ALFRED with behavior cloning) without a citation to the work that did this; references to specific setups require citations (rule a and example iii).",
    "start": 422,
    "end": 522,
    "label": "Unsupported_claim"
  },
  {
    "span": "Program synthesis from natural language has explored domain-specific languages (DSLs) with neural encoders (Dong and Lapata, 2016; Devlin et al., 2017), constraint-based solvers (Gulwani, 2011; Solar-Lezama, 2008), neural-guided search (Balog et al., 2017; Ellis et al., 2019), and intermediate representation learning (Yin and Neubig, 2017; Rabinovich et al., 2017). Generalization has been studied via compositionality benchmarks and curriculum learning (Lake and Baroni, 2018; Nye et al., 2020).",
    "document": "Introduction\n\nMapping natural language specifications to executable programs demands models that reason compositionally while respecting execution semantics and constraints.\n\nProgram synthesis from natural language has explored domain-specific languages (DSLs) with neural encoders (Dong and Lapata, 2016; Devlin et al., 2017), constraint-based solvers (Gulwani, 2011; Solar-Lezama, 2008), neural-guided search (Balog et al., 2017; Ellis et al., 2019), and intermediate representation learning (Yin and Neubig, 2017; Rabinovich et al., 2017). Generalization has been studied via compositionality benchmarks and curriculum learning (Lake and Baroni, 2018; Nye et al., 2020).\n\nWe introduce an execution-grounded latent planner that enforces interface contracts between language understanding and symbolic search, improving data efficiency and out-of-distribution robustness.",
    "reason": "The span lists prior directions without explaining how they tie to the proposed latent planner or what gap persists, thus meeting (a) and (b).",
    "start": 175,
    "end": 673,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT was used in an AES task trained on essays with holistic scores and achieved state-of-the-art performance.",
    "document": "Related Work\n\nAutomated Essay Scoring (AES) research spans feature-based regressors, neural readers, and pretraining-based models. Traditional systems rely on surface and discourse features, while neural methods learn representations end to end from tokens to score predictions.\n\nBERT was used in an AES task trained on essays with holistic scores and achieved state-of-the-art performance. Recent directions also consider prompt-specific modeling and cross-prompt transfer, as well as multi-trait scoring for organization, style, and conventions. Despite progress, robustness to adversarial perturbations, off-topic responses, and distribution shift remains a critical challenge.\n\nWe build on this line by introducing calibration-aware scoring with uncertainty estimation, improving reliability under prompt drift.",
    "reason": "Claims use of BERT in a specific AES setup without citing the corresponding study.",
    "start": 280,
    "end": 390,
    "label": "Unsupported_claim"
  },
  {
    "span": "Only 12% of bug-fixing commits include a regression test.",
    "document": "Introduction\n\nAutomated program repair (APR) aims to generate patches that fix defects while preserving existing functionality. A major obstacle for learning-based APR is the scarcity of high-quality supervision and the noisiness of available commit metadata. Although prior datasets of bugs and fixes exist, they vary widely in code coverage, language ecosystems, and test quality.\n\nOnly 12% of bug-fixing commits include a regression test. This severely constrains the reliability of test-based fitness functions used by many APR systems and hinders robust evaluation of patch correctness. To address this, we propose a retrieval-augmented repair framework that leverages historical test evolution and cross-project code similarities to synthesize candidate tests alongside patches.\n\nWe evaluate on diverse Java and Python benchmarks and conduct ablations to quantify the impact of test synthesis on patch plausibility and correctness.\n",
    "reason": "Presents a specific statistic without any citation or evidence.",
    "start": 384,
    "end": 441,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT has been widely used for automated essay scoring with domain adaptation to different prompts.",
    "document": "Related Work\n\nAutomated essay scoring (AES) seeks to predict human-assigned grades for essays with reliability and fairness across prompts. BERT has been widely used for automated essay scoring with domain adaptation to different prompts. Recent neural approaches incorporate prompt-aware encoders and coherence features to reduce overfitting. Nevertheless, concerns remain regarding bias and robustness when scoring out-of-domain essays.",
    "reason": "Claims a specific prior-use setup (BERT for AES with domain adaptation) without citations; per rule (e)(iii) and (a), such mentions require references.",
    "start": 140,
    "end": 238,
    "label": "Unsupported_claim"
  },
  {
    "span": "The BraTS dataset is the de facto benchmark for brain tumor segmentation.",
    "document": "Related Work\n\nMedical image segmentation has benefited greatly from convolutional neural networks, with U-Net and its variants establishing strong baselines in clinical tasks (Ronneberger et al., 2015; Isensee et al., 2021). Advances include attention mechanisms, multi-scale context aggregation, and self-supervised pretraining to combat limited labeled data.\n\nThe BraTS dataset is the de facto benchmark for brain tumor segmentation. Numerous approaches evaluate on multi-institutional MRI collections with standardized preprocessing and evaluation metrics, yet challenges remain in domain shift and small-lesion detection. Our work proposes a domain-adaptive hybrid 2.5D architecture with test-time adaptation to better handle cross-scanner variability.",
    "reason": "First mention of a specific dataset lacks a citation; claims about benchmark status also require references to the dataset and supporting sources.",
    "start": 362,
    "end": 435,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Rao et al., 2017)",
    "document": "Related Work\n\nNeural recommender systems typically learn user and item representations from implicit feedback, often via matrix factorization or sequence modeling (Koren et al., 2009; Rendle et al., 2010; He et al., 2017). Attention-based architectures capture higher-order dependencies in clickstreams (Kang and McAuley, 2018; Sun et al., 2019), while contrastive learning improves robustness to popularity bias (Chen et al., 2020; Zhou et al., 2022). Context-aware methods incorporate temporal signals and side information to personalize ranking (Wu et al., 2017; Hidasi et al., 2016).\n\nIn (Rao et al., 2017) proposed to model user intents through hierarchical recurrent networks, showing gains on next-item prediction under cold-start conditions. Later work explored graph-based propagation to encode co-consumption patterns (Ying et al., 2018; Wang et al., 2019) and disentangled factors for controllable recommendations (Ma et al., 2019; Wang et al., 2020). Session-based recommenders benefit from self-supervised objectives that recover masked transitions (Xie et al., 2020) and from curriculum learning that gradually increases sequence complexity (Pi et al., 2020).\n\nOur approach differs by unifying intent discovery with exposure modeling in a single pre-training task, reducing reliance on extensive manual feature engineering (Covington et al., 2016) and mitigating exposure confounding captured by causal recommenders (Schnabel et al., 2016; Bonner and Vasile, 2018). We also introduce an evaluation protocol that isolates exposure effects via counterfactual reweighting (Joachims et al., 2017), facilitating comparisons across datasets with varying item catalogs.",
    "reason": "Wrong citation style: narrative text uses a parenthetical lead-in 'In (Rao et al., 2017)'; it should be narrative form 'In Rao et al. (2017)'.",
    "start": 589,
    "end": 610,
    "label": "Format"
  },
  {
    "span": "According to industry statistics, 73% of companies rely on A/B testing for product decisions",
    "document": "Introduction\n\nOnline controlled experiments (A/B tests) are a cornerstone of data-driven product development. They enable causal assessment of changes to user interfaces, ranking algorithms, and pricing strategies. However, high experimentation velocity can introduce issues such as metric peeking, interference, and selection bias.\n\nAccording to industry statistics, 73% of companies rely on A/B testing for product decisions, underscoring the need for reliable sequential monitoring and guardrail metrics. Despite its ubiquity, many organizations still struggle with power analysis and multiple testing corrections at scale.\n\nIn this work, we present a sequential testing framework with guaranteed type-I error control under staggered rollouts and propose variance reduction techniques tailored to heavy-tailed engagement metrics.",
    "reason": "Reports a specific statistic without citing its source (definition b).",
    "start": 334,
    "end": 426,
    "label": "Unsupported_claim"
  },
  {
    "span": "The majority of graph neural network explainability methods rely on edge masking.",
    "document": "Related Work\n\nExplainability for graph neural networks (GNNs) has gained attention as models are applied in high-stakes domains like chemistry and recommendation (Gilmer et al., 2017; Ying et al., 2018). Methods typically aim to attribute predictions to subgraphs, node features, or message-passing steps, often through perturbation or surrogate modeling (Yuan et al., 2020). The majority of graph neural network explainability methods rely on edge masking. However, these approaches may struggle to capture higher-order patterns or distributional shifts, motivating structure-aware attribution that accounts for motif-level reasoning. We propose a causal substructure selection framework that integrates counterfactual training with stability constraints.",
    "reason": "Broad quantitative claim about the landscape of prior methods without any citations to surveys or representative works.",
    "start": 376,
    "end": 457,
    "label": "Unsupported_claim"
  },
  {
    "span": "Hundman et al. (2018) developed LSTM-based autoencoders for spacecraft telemetry anomalies. Ren et al. (2019) proposed SR-CNN for point anomaly detection. Forecasting models such as Prophet can flag deviations from expected trends (Taylor and Letham, 2018). Isolation Forest detects outliers via random partitioning (Liu et al., 2008).",
    "document": "Related Work\n\nAnomaly detection in time series spans reconstruction-based, prediction-based, and density-based paradigms, each trading off detection latency and false positives. We briefly review representative techniques used in industrial monitoring.\n\nHundman et al. (2018) developed LSTM-based autoencoders for spacecraft telemetry anomalies. Ren et al. (2019) proposed SR-CNN for point anomaly detection. Forecasting models such as Prophet can flag deviations from expected trends (Taylor and Letham, 2018). Isolation Forest detects outliers via random partitioning (Liu et al., 2008).\n\nRecent works incorporate seasonality decomposition, probabilistic forecasting, and multivariate dependence modeling, but evaluation remains challenging due to label scarcity and delayed feedback.",
    "reason": "The paragraph enumerates heterogeneous methods without transitions or a clear narrative connecting reconstruction, detection, forecasting, and isolation approaches; the lack of explicit relationships across multiple sentences causes coherence issues.",
    "start": 254,
    "end": 589,
    "label": "Coherence"
  },
  {
    "span": "Lee et al. 1",
    "document": "Related Work\n\nGraph-enhanced recommenders. Modeling user–item interactions as bipartite graphs enables message passing that captures high-order connectivity. Early graph convolutional approaches propagate embeddings along observed edges; subsequent work introduces debiasing for exposure effects and disentangles user preferences from popularity signals. Lee et al. 1 discuss the pitfalls of naive neighbor aggregation under skewed degree distributions and motivate normalization schemes tailored to recommendation.\n\nContrastive learning. Self-supervised objectives on augmented interaction graphs improve cold-start robustness. Negative sampling strategies and temperature schedules substantially affect representation quality, especially under sparse regimes.\n\nTemporal dynamics. Session-based models incorporate sequential patterns, while continuous-time variants capture inter-event intervals for better recency modeling.",
    "reason": "Wrong use of footnotes: numeric superscript-style marker appears without a proper footnote or year. Should be a standard citation with year (e.g., 'Lee et al. (2020)') or a properly formatted footnote.",
    "start": 355,
    "end": 367,
    "label": "Format"
  },
  {
    "span": "Cora and Citeseer",
    "document": "Introduction\n\nGraph neural networks (GNNs) propagate and transform features over edges to capture relational structure. Semi-supervised node classification on citation graphs remains a canonical setting for evaluating GNNs due to the sparsity of labels and homophily patterns. Following prior practice, we assess performance on Cora and Citeseer as well as PubMed, and we analyze the sensitivity of training to degree distribution shifts. We further consider inductive evaluation on temporal citation splits to quantify robustness.",
    "reason": "Cites specific datasets by name without providing references to their source descriptions, breaching the requirement to cite datasets at first mention (rule a).",
    "start": 328,
    "end": 345,
    "label": "Unsupported_claim"
  },
  {
    "span": "[15]",
    "document": "Related Work\n\nTime-series forecasting blends classical statistical models with modern neural architectures to balance inductive bias and expressivity (Hyndman and Athanasopoulos, 2018; Salinas et al., 2020). Global models trained across many series reduce overfitting and enable zero-shot transfer (Montero-Manso and Hyndman, 2020; Oreshkin et al., 2020). According to [15], hybrid models that fuse decomposition with attention outperform purely end-to-end baselines on intermittent demand. Probabilistic forecasting further benefits from calibrated likelihoods and quantile objectives (Wen et al., 2017; Rangapuram et al., 2018). Our work introduces a decomposition-aware encoder that enforces seasonal-consistency constraints, complementing recent advances in (Lim et al., 2021).",
    "reason": "Numeric bracket citation conflicts with author–year style used elsewhere; should be replaced with an author–year citation such as Author (YEAR) or (Author, YEAR).",
    "start": 369,
    "end": 373,
    "label": "Format"
  },
  {
    "span": "Multimodal grounding has been studied through vision-language navigation, affordance learning, and tactile-visual fusion (Anderson et al., 2018; Nair et al., 2019; Lee et al., 2020; Gao et al., 2021).",
    "document": "Introduction\n\nGrounding language in robotic perception and action enables agents to follow natural instructions in unstructured environments. Challenges include data sparsity for long-horizon tasks, ambiguity in referring expressions, and partial observability with noisy sensors.\n\nPrior multimodal approaches. Multimodal grounding has been studied through vision-language navigation, affordance learning, and tactile-visual fusion (Anderson et al., 2018; Nair et al., 2019; Lee et al., 2020; Gao et al., 2021). Parallel efforts explore contrastive pretraining and temporal alignment for cross-modal understanding in egocentric scenarios (Miech et al., 2020; Damen et al., 2022).\n\nWe target object-centric grounding under occlusions, proposing a memory-augmented policy that links textual entities to persistent scene graphs built from partial views, evaluated on simulated kitchens and real tabletop manipulation.",
    "reason": "The span aggregates topical areas and citations without articulating what is missing in these approaches or how they inform the authors’ object-centric focus, thus lacking synthesis.",
    "start": 311,
    "end": 511,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Garcia et al. 2016)",
    "document": "Related Work\n\nNeural language models predict word sequences using distributed representations, enabling improved perplexity and downstream transfer (Bengio et al., 2003). Subword segmentation reduces out-of-vocabulary issues while preserving morphology (Sennrich et al., 2016). Recent trends (Garcia et al. 2016) indicate that regularization and data augmentation substantially affect calibration and robustness. Meanwhile, scaling laws show predictable gains from increased model and data size (Kaplan et al., 2020), though efficiency remains a key concern (Dao et al., 2022).",
    "reason": "Missing comma before the year in the parenthetical citation; should be “(Garcia et al., 2016)”.",
    "start": 292,
    "end": 312,
    "label": "Format"
  },
  {
    "span": "Data augmentation for low-resource ASR has leveraged speed perturbation, noise injection, and SpecAugment (Ko et al., 2015; Park et al., 2019; Hannun et al., 2014). Cross-lingual transfer via multilingual pretraining and phoneme mapping has been studied extensively (Schultz and Waibel, 2001; Conneau et al., 2021; Li et al., 2021). Synthetic speech with TTS and voice conversion further enhances coverage (Rosenberg et al., 2019; Jia et al., 2018; Nachmani et al., 2019).",
    "document": "Related Work Low-resource automatic speech recognition (ASR) must contend with limited labeled audio and significant domain mismatch between training and deployment conditions. A considerable body of work targets label efficiency by leveraging augmentation, cross-lingual transfer, and self-supervision. Data augmentation for low-resource ASR has leveraged speed perturbation, noise injection, and SpecAugment (Ko et al., 2015; Park et al., 2019; Hannun et al., 2014). Cross-lingual transfer via multilingual pretraining and phoneme mapping has been studied extensively (Schultz and Waibel, 2001; Conneau et al., 2021; Li et al., 2021). Synthetic speech with TTS and voice conversion further enhances coverage (Rosenberg et al., 2019; Jia et al., 2018; Nachmani et al., 2019). Recent work also explores SSL encoders such as wav2vec 2.0 and HuBERT for improved representations (Baevski et al., 2020; Hsu et al., 2021). In contrast, we explore unsupervised style mixing across languages to bridge phonotactic gaps without additional text resources, proposing a curriculum that alternates between masked acoustic modeling and consistency training. We evaluate on four Bantu languages and show consistent WER reductions under 10 hours of labeled data.",
    "reason": "The span summarizes augmentation, transfer, and synthesis methods but does not connect them to the authors' proposed approach or identify what gap persists in low-resource ASR. It lacks explicit synthesis or motivation (a, b, c).",
    "start": 304,
    "end": 776,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The TREC-COVID challenge demonstrated that dense retrievers outperform BM25 by more than 10 nDCG points on average.",
    "document": "Related Work\n\nNeural information retrieval (IR) has shifted from term-matching to learned dense representations that capture semantic similarity (Guo et al., 2016; Mitra and Craswell, 2018). Dual-encoder architectures trained with in-batch negatives scale to web-scale corpora, while cross-encoders provide higher accuracy at increased computational cost (Nogueira and Cho, 2019; Karpukhin et al., 2020). The TREC-COVID challenge demonstrated that dense retrievers outperform BM25 by more than 10 nDCG points on average. Subsequent work combines sparse and dense signals through hybrid scoring and learned late interaction to close remaining gaps on out-of-domain queries.\n",
    "reason": "Reports a specific result from a named challenge without citing the source or supporting evidence.",
    "start": 405,
    "end": 520,
    "label": "Unsupported_claim"
  },
  {
    "span": "Cross-lingual transfer with multilingual transformers supports zero-shot evaluation (Conneau et al., 2020). Domain adaptation techniques reduce distribution shift via self-training (He et al., 2020). Benchmark suites for low-resource languages report strong baselines (Hu et al., 2020).",
    "document": "Related Work\n\nGeneralization across languages and domains is central to building robust NLP systems. Prior efforts pursue both model-centric and data-centric solutions.\n\nCross-lingual transfer with multilingual transformers supports zero-shot evaluation (Conneau et al., 2020). Domain adaptation techniques reduce distribution shift via self-training (He et al., 2020). Benchmark suites for low-resource languages report strong baselines (Hu et al., 2020). We target cross-domain, cross-lingual settings where labeled data is scarce.\n\nOur approach combines pseudo-labeling with language-adaptive adapters to bridge mismatches.",
    "reason": "The sentences jump between multilingual transfer, domain adaptation, and benchmarks without linking how these areas relate; no transitions or rationale connect them.",
    "start": 170,
    "end": 456,
    "label": "Coherence"
  },
  {
    "span": "Previous systems for open-domain QA rely heavily on dense retrievers built with dual-encoders.",
    "document": "Related Work\n\nOpen-domain question answering (ODQA) typically decomposes into retrieval and reading. Neural retrievers learn dense representations to identify relevant passages from large corpora, which are then fed to extractive or generative readers. Previous systems for open-domain QA rely heavily on dense retrievers built with dual-encoders. Recent efforts also explore re-ranking, multi-hop reasoning, and knowledge-augmented generation, yet performance remains brittle under distribution shift and noisy queries.",
    "reason": "Attributes a design pattern to prior systems without citing representative or seminal works (rule a/d).",
    "start": 253,
    "end": 347,
    "label": "Unsupported_claim"
  },
  {
    "span": "The RealToxicityPrompts benchmark contains 1.2M prompts curated from web forums.",
    "document": "Related Work\n\nSafety evaluation for open-ended text generation relies on datasets that stress-test models under realistic prompting conditions. Toxicity-oriented resources assess propensity to generate offensive or harmful content and the effectiveness of mitigation strategies. The RealToxicityPrompts benchmark contains 1.2M prompts curated from web forums. Subsequent work probes biases along demographic axes and examines trade-offs between toxicity reduction and utility. Our contribution complements these efforts with an intervention-agnostic evaluation suite that measures toxicity spillover under multi-turn conversation.",
    "reason": "States a specific dataset composition statistic without any supporting citation (rule a, b).",
    "start": 279,
    "end": 359,
    "label": "Unsupported_claim"
  },
  {
    "span": "Fairness in toxicity detection has been studied through bias metrics such as demographic parity, equalized odds, and subgroup AUC (Hardt et al., 2016; Borkan et al., 2019; Dixon et al., 2018). Debiasing strategies include data reweighting, counterfactual augmentation, and adversarial training (Zhao et al., 2018; Garg et al., 2019; Zhang et al., 2018).",
    "document": "Related Work\n\nToxicity detection systems deployed at scale face the risk of uneven performance across demographic groups and lexical variants. Recent work highlights the social and legal implications of such disparities, motivating measures that go beyond aggregate accuracy.\n\nFairness in toxicity detection has been studied through bias metrics such as demographic parity, equalized odds, and subgroup AUC (Hardt et al., 2016; Borkan et al., 2019; Dixon et al., 2018). Debiasing strategies include data reweighting, counterfactual augmentation, and adversarial training (Zhao et al., 2018; Garg et al., 2019; Zhang et al., 2018).\n\nOur study examines distribution shift across platforms and proposes a calibration-aware training recipe that stabilizes group-wise error rates under domain transfer. We report results on multiple public bias-sensitive benchmarks.\n",
    "reason": "The span lists metrics and methods but does not link them to the study’s focus on calibration under shift or specify the unresolved issue motivating the new recipe.",
    "start": 277,
    "end": 630,
    "label": "Lacks_synthesis"
  },
  {
    "span": "SemEval-2016 Task 6 stance detection dataset",
    "document": "Introduction\n\nStance detection seeks to infer an author's position toward a target topic from text. Early datasets focus on social media, with labels capturing favor, against, or neutral stances. We evaluate on the SemEval-2016 Task 6 stance detection dataset and extend it with domain-specific targets in healthcare forums. Prior approaches include feature-rich linear models and attention-based neural architectures (Mohammad et al., 2016; Augenstein et al., 2016). Our contribution is a contrastive objective that aligns stance-bearing spans with target representations.",
    "reason": "First mention of a specific shared task dataset without providing a citation to the task description.",
    "start": 215,
    "end": 259,
    "label": "Unsupported_claim"
  },
  {
    "span": "Amodei et al. (2016) outlined concrete problems in AI safety. Thomas et al. (2015) studied high-confidence off-policy evaluation. Achiam et al. (2017) introduced constrained policy optimization. Leike et al. (2018) explored reward modeling with human feedback.",
    "document": "Related Work\n\nSafety in reinforcement learning (RL) involves aligning objectives, ensuring safe exploration, and providing guarantees about performance under uncertainty. Research spans theoretical constraints, evaluation protocols, and human-in-the-loop methods.\n\nAmodei et al. (2016) outlined concrete problems in AI safety. Thomas et al. (2015) studied high-confidence off-policy evaluation. Achiam et al. (2017) introduced constrained policy optimization. Leike et al. (2018) explored reward modeling with human feedback.\n\nOur work builds a unified benchmark that decomposes safety failures by source—exploration, generalization, and specification—and measures interventions across them.",
    "reason": "The sequence lists works from different safety subareas without transitions or explicit linkage, making the relationships between safety taxonomies, OPE, constraints, and reward modeling unclear.",
    "start": 265,
    "end": 525,
    "label": "Coherence"
  },
  {
    "span": "there are many recent works that explore this topic",
    "document": "Introduction\n\nMultimodal question answering (MMQA) seeks to answer natural language questions grounded in both visual and textual contexts. Recent advances in transformer-based architectures have made it possible to jointly encode image regions and passages while attending to the question. Despite progress, data scarcity and cross-modal alignment remain open challenges. In this paper we present a unified encoder that learns cross-modal representations via masked modeling and contrastive alignment.\n\nWhile MMQA has attracted increasing attention in both vision and language communities, there are many recent works that explore this topic but a consistent evaluation protocol is still lacking. Prior efforts vary in their choice of datasets, pretraining corpora, and negative sampling strategies, which complicates comparison across models. To address this, we adopt a standardized preprocessing pipeline and release our code to encourage reproducibility.\n\nOur contributions are three-fold: we (1) introduce a lightweight fusion module that improves cross-modal grounding, (2) establish a consistent evaluation protocol across four benchmarks, and (3) provide an ablation study on pretraining objectives that impact MMQA performance.",
    "reason": "Mentions 'recent works' without providing any citations to those works (definition d).",
    "start": 591,
    "end": 642,
    "label": "Unsupported_claim"
  },
  {
    "span": "in (Mora et al., 2020)",
    "document": "Related Work\n\nSemi-supervised text classification has progressed rapidly with advances in representation learning and consistency training (Allen and Brooks, 2018; Zhao et al., 2019). Following the taxonomy in (Mora et al., 2020), we categorize prior methods into pseudo-labeling, agreement regularization, and data augmentation. Recent methods emphasize stronger augmentations paired with contrastive objectives (Park and Nguyen, 2021) and adaptive confidence thresholds (Hsu et al., 2022). Our approach differs by jointly calibrating the classifier and the augmentation policy during training (Chen and Li, 2023).\n\nIn addition, several works investigate task-specific pretraining for label-efficient regimes (Rao and Patel, 2020), while others explore curriculum schedules that align with uncertainty estimates (Davis et al., 2021).",
    "reason": "Wrong citation style: the narrative construction should be 'following the taxonomy in Mora et al. (2020)' rather than 'in (Mora et al., 2020)'.",
    "start": 207,
    "end": 229,
    "label": "Format"
  },
  {
    "span": "Brown et al. (2020) introduce in-context learning in large language models. Gao et al. (2021) explore prompt-based tuning for few-shot classification. Liu et al. (2021) survey prompt methods across tasks. Wei et al. (2022) study chain-of-thought prompting. Min et al. (2022) question whether demonstrations are necessary.",
    "document": "Related Work\n\nPrompting and In-Context Learning\n\nRecent research explores how to steer pretrained language models with lightweight signals. Brown et al. (2020) introduce in-context learning in large language models. Gao et al. (2021) explore prompt-based tuning for few-shot classification. Liu et al. (2021) survey prompt methods across tasks. Wei et al. (2022) study chain-of-thought prompting. Min et al. (2022) question whether demonstrations are necessary. Prior work on control codes and instruction tuning examines supervision formats and alignment (Sanh et al., 2022; Ouyang et al., 2022).\n\nOur work focuses on robustness of discrete prompts under domain shift and label imbalance, complementing schema-driven approaches.",
    "reason": "Multiple consecutive sentences cite different works without transitions or explicit relationships, creating abrupt shifts and leaving unclear how each study relates to the others.",
    "start": 140,
    "end": 461,
    "label": "Coherence"
  },
  {
    "span": "Vatswani et al.",
    "document": "Related Work\n\nActive learning for text classification and sequence tagging has been studied for decades (Settles, 2009). Neural approaches typically rely on uncertainty sampling, diversity sampling, or a mix of both (Lewis and Gale, 1994; Roy and McCallum, 2001). Following Vatswani et al., we adopt an uncertainty-sampling baseline for transformer encoders and compare against core-set selection. Pretrained language models have reshaped the evaluation of acquisition strategies (Prabhu et al., 2019; Ein-Dor et al., 2020), but their interaction with pool bias remains underexplored. We also consider training-time cost, which prior work largely ignores (Sener and Savarese, 2018).",
    "reason": "Narrative citation missing year; should be formatted as 'Vatswani et al. (YEAR)'.",
    "start": 274,
    "end": 289,
    "label": "Format"
  },
  {
    "span": "Nguyen et al., (2016)",
    "document": "Introduction\n\nPretraining on large unlabeled corpora has transformed sequence modeling, enabling effective transfer to low-resource tasks (Peters et al., 2018; Devlin et al., 2019). According to Nguyen et al., (2016), domain-adaptive objectives further improve performance by aligning representations with target distributions. Building on these insights, we explore curriculum-based adaptation schedules for biomedical IE.",
    "reason": "Extraneous comma in a narrative citation; should be 'Nguyen et al. (2016)' without the comma.",
    "start": 195,
    "end": 216,
    "label": "Format"
  },
  {
    "span": "over 70% of developers already use LLM-based assistants",
    "document": "Introduction\n\nLarge language models (LLMs) have transformed code generation and developer tooling by enabling natural language to code translation, inline refactoring, and automated documentation. As these models integrate into IDEs and CI pipelines, questions arise regarding productivity impacts, learning effects, and software quality assurance. Understanding how assistance changes developer behavior is crucial for designing responsible tooling and evaluation metrics.\n\nAnecdotal reports suggest widespread adoption in industry, and over 70% of developers already use LLM-based assistants for at least some coding tasks. However, rigorous measurement of efficacy remains limited, with few controlled studies on real-world repositories and longitudinal outcomes. In this work, we introduce a randomized field experiment to quantify effects on task completion time, defect rates, and follow-up maintenance costs.",
    "reason": "This numerical adoption statistic lacks a supporting citation or evidence (rule b: specific statistics require citation).",
    "start": 538,
    "end": 593,
    "label": "Unsupported_claim"
  },
  {
    "span": "End-to-end detectors based on transformers dispense with hand-crafted anchors and non-maximum suppression, learning object queries directly (Carion et al., 2020; Zhu et al., 2021; Sun et al., 2021).",
    "document": "Introduction\n\nObject detection has progressed from hand-engineered features to deep convolutional architectures that dominate standard benchmarks. While two-stage detectors achieve strong accuracy through region proposals and per-region refinement, single-stage designs prioritize speed by predicting dense anchors and applying non-maximum suppression (NMS) heuristics.\n\nEnd-to-end detectors based on transformers dispense with hand-crafted anchors and non-maximum suppression, learning object queries directly (Carion et al., 2020; Zhu et al., 2021; Sun et al., 2021). Convolutional backbones remain prevalent due to their efficiency, but attention modules are increasingly used to capture long-range dependencies for complex scenes (Wang et al., 2018; Ramachandran et al., 2019). Additionally, training recipes with stronger augmentations and label assignment strategies continue to improve performance across scales (Zhang et al., 2020; Ge et al., 2021).\n\nMulti-scale features, deformable operators, and dynamic heads further help detectors localize small and overlapping objects (Lin et al., 2017; Zhu et al., 2020; Dai et al., 2017). Despite these advances, evaluation practices often emphasize mAP while overlooking calibration and uncertainty aspects relevant to safety-critical deployment.\n\nThis backdrop situates current detection research within a landscape of architectural simplification and stronger training strategies.",
    "reason": "The span summarizes a class of transformer-based detectors but does not synthesize how this thread relates to the paper’s goals, limitations it leaves, or the authors’ perspective.",
    "start": 371,
    "end": 569,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Lee and Kim, 2017; Park, 2018;; Chen, 2019)",
    "document": "Related Work\n\nText summarization methods span extractive algorithms that select salient sentences and abstractive models that generate paraphrased content. Prior extractive systems (Lee and Kim, 2017; Park, 2018;; Chen, 2019) rely on sentence scoring with graph centrality or neural sequence encoders, while more recent approaches integrate pre-trained language models to capture long-range dependencies (Liu and Lapata, 2019). Abstractive models leverage encoder–decoder architectures with copy and coverage mechanisms to enhance factuality and reduce repetition (See et al., 2017; Paulus et al., 2018). Hybrid methods combine selection with constrained generation to balance faithfulness and fluency.\n\nOur work introduces a contrastive objective for salience calibration that complements pre-trained encoders in low-resource domains.",
    "reason": "There is a double semicolon inside the parenthetical citation; it should be a single semicolon: '(Lee and Kim, 2017; Park, 2018; Chen, 2019)'.",
    "start": 181,
    "end": 225,
    "label": "Format"
  },
  {
    "span": "In (Lopez and Mei, 2019)",
    "document": "Related Work\n\nDomain adaptation for text classification has leveraged adversarial training to align feature distributions across domains (Ganin et al., 2016; Tzeng et al., 2017), as well as self-training with confidence regularization (Yarowsky, 1995; Zou et al., 2019). In (Lopez and Mei, 2019) a curriculum over source-to-target difficulty was proposed to stabilize training, while other work introduced pseudo-label filtering to mitigate confirmation bias (Arazo et al., 2020). Recent methods combine contrastive objectives with consistency constraints to encourage domain-invariant semantics (Kim et al., 2020; Sohn et al., 2020). Our approach differs by explicitly modeling domain-specific lexical priors and decoupling them from domain-agnostic compositional features, bridging gaps observed in prior adversarial-only methods (Ben-David et al., 2010; Saito et al., 2018).",
    "reason": "Wrong citation style using a preposition before a parenthetical citation; should be narrative: \"In Lopez and Mei (2019)\" or remove \"In\" before the parenthetical.",
    "start": 271,
    "end": 295,
    "label": "Format"
  },
  {
    "span": "Graph-based models for molecular property prediction include message passing networks (Gilmer et al., 2017), attention-based graph transformers (Ying et al., 2021; Rong et al., 2020), spectral approaches (Defferrard et al., 2016), and hybrid 3D-geometry methods (Schutt et al., 2018; Klicpera et al., 2020). Benchmarks such as MoleculeNet (Wu et al., 2018) and the Open Graph Benchmark (Hu et al., 2020) have standardized evaluation, while pretraining with self-supervision has been explored through context prediction, edge masking, and contrastive learning (Hu et al., 2019; You et al., 2020; Sun et al., 2020).",
    "document": "Related Work\n\nGraph learning for molecules has surged due to improved architectures and widely adopted benchmarks. Predicting toxicity remains challenging because endpoints are heterogeneous, assays are noisy, and many compounds are structurally similar yet functionally distinct.\n\nGraph-based models for molecular property prediction include message passing networks (Gilmer et al., 2017), attention-based graph transformers (Ying et al., 2021; Rong et al., 2020), spectral approaches (Defferrard et al., 2016), and hybrid 3D-geometry methods (Schutt et al., 2018; Klicpera et al., 2020). Benchmarks such as MoleculeNet (Wu et al., 2018) and the Open Graph Benchmark (Hu et al., 2020) have standardized evaluation, while pretraining with self-supervision has been explored through context prediction, edge masking, and contrastive learning (Hu et al., 2019; You et al., 2020; Sun et al., 2020).\n\nDatasets with multiple toxicity endpoints introduce distribution shift across assays, motivating models that are robust to label noise and multi-task conflicts. Some works consider uncertainty calibration for molecular predictions and task-weighting strategies to cope with imbalance.\n\nIn this paper, we develop MoTox-GNN, a multi-task graph model with calibrated label noise modeling and task-adaptive routing. We show improvements across five toxicity collections and analyze calibration under varying assay reliabilities.",
    "reason": "The paragraph lists prior models, benchmarks, and pretraining strategies without explaining how they relate to the present work, what specific limitations remain, or what gap the paper addresses.",
    "start": 282,
    "end": 895,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Quantization-aware training reduces model size (Jacob et al., 2018). NAS discovers compact detectors for mobile CPUs (Tan et al., 2019). Knowledge distillation further improves accuracy (Hinton et al., 2015). Pruning can remove redundant channels in backbones (He et al., 2017).",
    "document": "Related Work\n\nEfficient Object Detection on Edge Devices\n\nDeploying detectors on embedded hardware requires balancing accuracy, latency, and memory footprint. Techniques span architecture design, training-time regularization, and compression.\n\nCompression and Architecture Search\n\nQuantization-aware training reduces model size (Jacob et al., 2018). NAS discovers compact detectors for mobile CPUs (Tan et al., 2019). Knowledge distillation further improves accuracy (Hinton et al., 2015). Pruning can remove redundant channels in backbones (He et al., 2017).\n\nHardware-Aware Optimization\n\nLatency-aware loss terms and compiler-assisted kernels yield additional speedups (Cai et al., 2020; Chen et al., 2018). We target low-power ARM devices with dynamic resolution scheduling.",
    "reason": "The works are juxtaposed without clarifying how quantization, NAS, distillation, and pruning interrelate or build on one another, resulting in abrupt transitions and implied relationships only.",
    "start": 281,
    "end": 559,
    "label": "Coherence"
  },
  {
    "span": "Approaches include propensity score matching, inverse propensity weighting, and doubly robust estimation (Rosenbaum and Rubin, 1983; Imbens and Rubin, 2015; Bang and Robins, 2005). Deep causal models such as TARNet, CFRNet, and CEVAE have been proposed to learn representations for treatment effect estimation (Shalit et al., 2017; Louizos et al., 2017; Johansson et al., 2016).",
    "document": "Related Work\n\nEstimating causal effects from observational healthcare data is complicated by confounding, selection bias, and irregular measurement. A rich literature spans classical statistical methods and more recent machine learning approaches.\n\nApproaches include propensity score matching, inverse propensity weighting, and doubly robust estimation (Rosenbaum and Rubin, 1983; Imbens and Rubin, 2015; Bang and Robins, 2005). Deep causal models such as TARNet, CFRNet, and CEVAE have been proposed to learn representations for treatment effect estimation (Shalit et al., 2017; Louizos et al., 2017; Johansson et al., 2016).\n\nClinical translation further requires uncertainty quantification and interpretability, especially when recommendations inform high-stakes decisions under distribution shift.",
    "reason": "Summarizes existing methods without articulating how they relate to the authors’ objectives or what gap motivates the present study (definition a/c).",
    "start": 249,
    "end": 627,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Zhang et al., 2018)",
    "document": "Related Work\n\nFew-shot classification. Metric-based meta-learning approaches learn embedding spaces where simple classifiers perform well with few labels (Vinyals et al., 2016; Snell et al., 2017). Prototypical networks and subsequent variants improve robustness to distribution shift through episodic training (Allen et al., 2019; Ortega and Silva, 2020). In addition, transductive inference leverages unlabeled query sets to refine predictions, as shown by Zhang et al., 2018) in the context of graph propagation. Data augmentation and self-supervision further enhance generalization (Tian et al., 2020; Chen et al., 2020).",
    "reason": "Missing opening parenthesis in the parenthetical citation. It should be \"(Zhang et al., 2018)\".",
    "start": 459,
    "end": 478,
    "label": "Format"
  },
  {
    "span": "Early approaches rely on semantic parsing, while more recent systems adopt neural retrieval and multi-hop reasoning over KGs (Berant et al., 2013; Das et al., 2018; Sun et al., 2019; Zhang et al., 2020). We follow the retrieval-then-reasoning paradigm.",
    "document": "Introduction\n\nQuestion answering over knowledge graphs (KGQA) transforms natural language queries into structured reasoning steps grounded in entities and relations. The task requires handling ambiguity, compositionality, and incomplete knowledge.\n\nEarly approaches rely on semantic parsing, while more recent systems adopt neural retrieval and multi-hop reasoning over KGs (Berant et al., 2013; Das et al., 2018; Sun et al., 2019; Zhang et al., 2020). We follow the retrieval-then-reasoning paradigm.\n\nReal-world applications impose latency constraints and demand robustness to noisy linking, motivating lightweight models and uncertainty-aware inference.",
    "reason": "Summarizes existing lines of work and asserts the paper ‘follows’ a paradigm without clarifying what is missing in prior work or the authors’ specific perspective (definition b/c).",
    "start": 249,
    "end": 501,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Distant supervision was first introduced to address the lack of labeled data in relation extraction.",
    "document": "Related Work\n\nRelation extraction (RE) aims to identify semantic relations between entities mentioned in text. Supervised RE requires labor-intensive annotation, motivating weak and semi-supervised methods. Distant supervision was first introduced to address the lack of labeled data in relation extraction. Subsequent work mitigated label noise via multi-instance learning, denoising objectives, and posterior regularization. Recent neural approaches leverage pretrained language models and joint entity–relation modeling to improve performance under limited supervision. Our contribution is a contrastive denoising framework that better separates informative from spurious evidence at sentence and bag levels.\n",
    "reason": "Attributes an origin to a specific technique without citing the original paper (violates rule a).",
    "start": 207,
    "end": 307,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Ahmed and Rossi, 2019",
    "document": "Related Work\n\nData valuation methods estimate each sample’s contribution to model performance to enable informed curation and auditing (Ghorbani and Zou, 2019; Jia et al., 2019). Influence-function approximations enable scalable estimates but often rely on strong convexity assumptions (Basumallik and Wu, 2021).\n\nCounterfactual data valuation frames the problem as a cooperative game and computes marginal contributions via approximations (Ahmed and Rossi, 2019 across training subsets, though estimates can be noisy in nonconvex regimes (Wang and Patel, 2020). We propose a smoothed objective that reduces variance while preserving additivity (Lin and Ortega, 2022).",
    "reason": "Missing closing parenthesis in the parenthetical citation; should be (Ahmed and Rossi, 2019).",
    "start": 440,
    "end": 462,
    "label": "Format"
  },
  {
    "span": "It is well known that data augmentation alone can close half of the domain gap.",
    "document": "Introduction\n\nUnsupervised domain adaptation (UDA) aims to transfer knowledge from labeled source to unlabeled target domains by aligning distributions and learning invariant features (Pan and Yang, 2010; Ganin et al., 2016; Long et al., 2018). Augmentation and self-training are complementary strategies that improve robustness under covariate shift (French et al., 2018; Xu et al., 2020). It is well known that data augmentation alone can close half of the domain gap. Nevertheless, augmentation policies are often task- and domain-specific, underscoring the need for adaptive methods that tailor transformations to target characteristics.",
    "reason": "Presents a quantitative, general claim ('close half of the domain gap') without any supporting citation or evidence, violating rule (b).",
    "start": 391,
    "end": 470,
    "label": "Unsupported_claim"
  },
  {
    "span": "For medical image segmentation, U-Net and its derivatives have been dominant, including Attention U-Net (Oktay et al., 2018), U-Net++ (Zhou et al., 2018), nnU-Net (Isensee et al., 2021), and transformer-augmented variants (Chen et al., 2021; Hatamizadeh et al., 2022).",
    "document": "Introduction\n\nMedical image segmentation. Accurate delineation of anatomical structures and lesions is fundamental to diagnosis and treatment planning. Convolutional encoder-decoder architectures have achieved strong performance across modalities. For medical image segmentation, U-Net and its derivatives have been dominant, including Attention U-Net (Oktay et al., 2018), U-Net++ (Zhou et al., 2018), nnU-Net (Isensee et al., 2021), and transformer-augmented variants (Chen et al., 2021; Hatamizadeh et al., 2022). Additional work explores self-supervised pretraining and semi-supervised learning to reduce annotation costs (Tajbakhsh et al., 2020; Bortsova et al., 2019).\n\nDomain shifts and robustness. Performance can degrade under changes in scanners, protocols, and populations. Techniques for domain adaptation, test-time adaptation, and uncertainty estimation attempt to improve robustness in out-of-distribution settings (Guan and Liu, 2021; Karani et al., 2018).\n\nEvaluation and benchmarks. Multi-center datasets and standardized leaderboards such as MSD and BraTS enable fairer comparisons and highlight generalization gaps across institutions (Simpson et al., 2019; Bakas et al., 2018).\n",
    "reason": "The sentence lists representative methods without explaining their shortcomings or how the present work addresses them, offering no explicit author perspective or gap (criteria a and c).",
    "start": 248,
    "end": 516,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Nguyen et al., 2016;Brown, 2017;Li, 2018)",
    "document": "Introduction\n\nTransfer learning leverages knowledge from related tasks or domains to improve performance with limited supervision. Prior work (Nguyen et al., 2016;Brown, 2017;Li, 2018) assumed that source and target label spaces are aligned, which simplifies adaptation but limits generality. Recent advances explore partial transfer, open-set recognition, and label shift to relax these assumptions (Panareda Busto and Gall, 2017; You et al., 2019). In parallel, pre-trained language and vision models provide strong initializations that can be efficiently adapted via parameter-efficient fine-tuning.\n\nWe propose a source-free adaptation framework that estimates target prototypes without access to labeled source data.",
    "reason": "Missing spaces after semicolons in a multi-citation; it should be '(Nguyen et al., 2016; Brown, 2017; Li, 2018)'.",
    "start": 142,
    "end": 184,
    "label": "Format"
  },
  {
    "span": "Several hospitals have already deployed FL systems in production for radiology diagnosis.",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative training without centralizing raw data, which is particularly appealing in healthcare due to privacy and regulatory constraints (Kairouz et al., 2021). Medical imaging studies report promising results for cross-institution segmentation and classification with FL variants that address non-IID data and client drift (Sheller et al., 2020; Li et al., 2021). Several hospitals have already deployed FL systems in production for radiology diagnosis. Nevertheless, reproducibility remains challenging due to heterogeneous infrastructure, security considerations, and annotation protocols (Kaissis et al., 2021).\n\nOur work introduces a privacy-audited, containerized FL toolkit with standardized evaluation across institutions, lowering the barrier for clinical validation.",
    "reason": "Claims real-world production deployments in hospitals without providing case studies or citations (violates rule b and e).",
    "start": 416,
    "end": 505,
    "label": "Unsupported_claim"
  },
  {
    "span": "Miller et al. 3",
    "document": "Related Work\n\nInteractive visualization systems aim to balance expressivity and learnability. Prior studies have examined grammar-based interfaces for chart specification (Wilkinson, 2005; Satyanarayan et al., 2017) and natural language querying of visualizations (Setlur and Tory, 2020). As argued by Miller et al. 3, discoverability of advanced features often hinges on progressive disclosure and guided interactions. More recent work integrates mixed-initiative recommendations to surface relevant views (Dibia and Demiralp, 2019).\n\nOur system extends mixed-initiative guidance with task-aware cues that adapt to user intent while preserving direct manipulation.",
    "reason": "Improper footnote-like usage without a year; should include a year or be formatted as a proper footnote/endnote rather than “Author et al. NUMBER”.",
    "start": 302,
    "end": 317,
    "label": "Format"
  },
  {
    "span": "Neural approaches to combinatorial optimization learn heuristics for routing, matching, and scheduling, including pointer networks, attention-based decoders, and reinforcement learning with rollout baselines (Vinyals et al., 2015; Bello et al., 2017; Kool et al., 2019; Ma et al., 2021). Extensions tackle constraints via masking or penalty shaping and leverage offline datasets for policy improvement (Nazari et al., 2018; Chen and Tian, 2019; Lu et al., 2020). We consider the traveling salesman problem with additional time window constraints using a policy-gradient method.",
    "document": "Introduction\n\nCombinatorial optimization underpins logistics, transportation, and network design. Hand-crafted heuristics remain dominant but may be brittle under distribution shifts, motivating learned solvers that adapt to data.\n\nNeural approaches to combinatorial optimization learn heuristics for routing, matching, and scheduling, including pointer networks, attention-based decoders, and reinforcement learning with rollout baselines (Vinyals et al., 2015; Bello et al., 2017; Kool et al., 2019; Ma et al., 2021). Extensions tackle constraints via masking or penalty shaping and leverage offline datasets for policy improvement (Nazari et al., 2018; Chen and Tian, 2019; Lu et al., 2020). We consider the traveling salesman problem with additional time window constraints using a policy-gradient method.\n\nWe evaluate on synthetic and real instances and report improvements in feasibility rates.",
    "reason": "The span lists prior work and immediately states the paper’s focus without explaining what is missing in those methods or why a new policy-gradient approach is necessary; it lacks articulation of a gap and synthesis.",
    "start": 232,
    "end": 809,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Temporal fusion across modalities integrates audio, visual, and text streams (Tsai et al., 2019). Prosodic cues strongly affect perceived sentiment (Scherer, 2003). Visual attention mechanisms focus on salient facial regions (Mollahosseini et al., 2016).",
    "document": "Related Work\n\nMultimodal sentiment analysis seeks to combine complementary cues from language, acoustics, and vision to improve prediction accuracy and robustness. Early benchmarks revealed substantial gains from simple feature concatenation over unimodal baselines (Zadeh et al., 2017). Subsequent work developed cross-modal attention and alignment modules to mitigate temporal asynchrony between streams (Tsai et al., 2019; Pham et al., 2019).\n\nTemporal fusion across modalities integrates audio, visual, and text streams (Tsai et al., 2019). Prosodic cues strongly affect perceived sentiment (Scherer, 2003). Visual attention mechanisms focus on salient facial regions (Mollahosseini et al., 2016). Recent studies consider domain shift and noise robustness with adversarial training and modality dropout (Sanchez et al., 2021; Han et al., 2021). We investigate calibration-aware late fusion under missing-modality conditions.",
    "reason": "The span moves from multimodal fusion to standalone findings on prosody and then to visual attention without transitions or an explicit link among the cited works, weakening coherence across sentences.",
    "start": 447,
    "end": 701,
    "label": "Coherence"
  },
  {
    "span": "(Chen et al. 2019)",
    "document": "Introduction\n\nOpen-domain document retrieval underpins modern QA systems by narrowing the evidence set for downstream readers. The dual-encoder architecture has been widely adopted (Chen et al. 2019) to enable efficient dense retrieval over large corpora, while hybrid sparse-dense schemes (Luan et al., 2021; Gao et al., 2021) further improve recall. Recent work explores iterative reranking (Pradeep et al., 2021) and late interaction models (Khattab and Zaharia, 2020) to better capture term dependencies. However, efficiency–effectiveness trade-offs remain central.",
    "reason": "Missing comma between author and year in a parenthetical citation; APA-style requires '(Chen et al., 2019)'.",
    "start": 181,
    "end": 199,
    "label": "Format"
  },
  {
    "span": "(Nguyen et al., 2018",
    "document": "Introduction\n\nZero-shot cross-lingual QA typically relies on multilingual encoders and parallel data for alignment (Artetxe and Schwenk, 2019; Conneau et al., 2020). Retrieval-augmented approaches complement encoders by grounding answers in multilingual corpora (Asai et al., 2021). However, constructing aligned supervision remains costly (Nguyen et al., 2018 and subsequent heuristics based on distant supervision introduce noise that harms generalization. We address this with a confidence-aware distillation scheme.\n\nOur method combines language-adaptive pretraining (Liu et al., 2020) with selective filtering of noisy pairs similar to techniques in machine translation (Khayrallah and Koehn, 2018).",
    "reason": "Missing closing parenthesis in the parenthetical citation; it should be '(Nguyen et al., 2018)'.",
    "start": 340,
    "end": 360,
    "label": "Format"
  },
  {
    "span": "Prompting strategies such as chain-of-thought, self-consistency, and program-of-thought have been shown to improve few-shot code generation performance on HumanEval and MBPP (Wei et al., 2022; Wang et al., 2022; Chen et al., 2021; Nye et al., 2021). Retrieval-augmented prompting and tool use further enhance execution accuracy by incorporating libraries and documentation (Lewis et al., 2020; Schick et al., 2023).",
    "document": "Introduction\n\nLarge language models have rapidly advanced the state of code generation and program synthesis. Prompt design plays a crucial role in eliciting reasoning steps and leveraging external resources.\n\nPrompting strategies such as chain-of-thought, self-consistency, and program-of-thought have been shown to improve few-shot code generation performance on HumanEval and MBPP (Wei et al., 2022; Wang et al., 2022; Chen et al., 2021; Nye et al., 2021). Retrieval-augmented prompting and tool use further enhance execution accuracy by incorporating libraries and documentation (Lewis et al., 2020; Schick et al., 2023).\n\nWe investigate structure-aware prompting that decomposes problems into typed subgoals aligned with function signatures and unit tests. Our method achieves improvements in pass@k while reducing hallucinated API calls.",
    "reason": "The span summarizes prior prompting techniques without clarifying how they relate to or motivate the proposed structure-aware prompting, indicating (a) and (c).",
    "start": 210,
    "end": 625,
    "label": "Lacks_synthesis"
  },
  {
    "span": "2012; Gardner et al., 2020)",
    "document": "Introduction\n\nGeneral-purpose pretrained vision and language (V&L) models have gained notable performance on many V&L tasks (Lu et al., 2019;Tan and Bansal, 2019;Li et al., 2019;Li et al., 2020a;Su et al., 2020). As a result, V&L research has changed its focus from task-specific architectures to fine-tuning large V&L models.\n\nCurrent benchmarks give a good perspective on model performance on a wide range of V&L tasks Lourie et al., 2021;Li et al., 2021), but the field is only starting to assess why models perform so well and whether models learn specific capabilities that span multiple V&L tasks. Specifically, we lack detailed understanding of the extent to which such models are able to ground linguistic phenomena-from morphosyntax to semantics-in the visual modality (Bernardi and Pezzelle, 2021). For example, recent evidence suggests that models are insensitive to linguistic distinctions of verb-argument structure (Hendricks and Nematzadeh, 2021) and word order (Cirik et al., 2018;Akula et al., 2020).\n\nOur work addresses this gap with VALSE (Vision And Language Structured Evaluation), a benchmark for V&L model evaluation comprising six tasks, or 'pieces', where each piece has the same structure: given a visual input, a model is asked to distinguish real captions from foils, where a foil is constructed from a caption by altering a word or phrase that realizes a specific linguistic phenomenon, e.g., semantic number of nouns, verb argument structure, or coreference. VALSE uses a resource-lean diagnostic setup that dispenses with large-scale annotation (e.g., of bounding boxes), and builds on existing high-quality image captioning and VQA data. VALSE is designed to leverage the existing prediction heads in pretrained (or finetuned) V&L models; for that reason, our benchmark does not include any re-training and can be interpreted as a zero-shot evaluation. We build test data for each piece so as to safeguard against the possibility of models exploiting artefacts or statistical biases in the data, a well-known issue with highly parameterised neural models pretrained on large amounts of data (Goyal et al., 2017;Madhyastha et al., 2018;Kafle et al., 2019). With this in view, we propose novel methods to guard against the emergence of artefacts during foiling.\n\nOur main contributions are: i) We introduce VALSE, a novel benchmark aimed at gauging the sensitivity of pre-trained V&L models to foiled instances. ii) We cover a wide spectrum of basic linguistic phenomena affecting the linguistic and visual modalities: existence, plurality, counting, spatial relations, actions, and entity coreference. iii) We investigate novel strategies to build valid foils that include automatic and human validation. We balance word frequency distributions between captions and foils, and test against pretrained models solving the benchmark unimodally. We employ masked language modeling (MLM) in foil creation and semantic inference for validating foils, and finally collect human annotations for the entire benchmark. iv) We establish initial experimental results for pretrained V&L models of diverse architectures on VALSE. These models' overall weak performance indicates that the time is ripe for a novel, reliable foiling dataset targeting the visual grounding capabilities of V&L models through the lens of linguistic constructs. 1\n\n2 Background and Related work\n\nPretrained V&L models learn to combine vision and language through self-supervised multitask learning. Tasks include multimodal masked modeling-where words in the text and object labels or regions in the image are masked out, then predictedand image-sentence alignment, whereby a model learns to predict whether an image and a text correspond. Major architectures are single-and dualstream multimodal transformers: single-stream models concatenate word and image features, and encode the resulting sequence with a single transformer stack; dual-stream models use distinct transformer stacks to handle visual and textual inputs, and additional layers (e.g. co-attention) to fuse these into multimodal features.\n\nBenchmarking V&L models V&L models (Li et al., 2019;Lu et al., 2019;Tan and Bansal, 2019;Lu et al., 2020;Li et al., 2020b;Kim et al., 2021) are commonly evaluated on V&L tasks such as VQA (Goyal et al., 2017), visual reasoning (Suhr et al., 2019), or image retrieval (Lin et al., 2014;Plummer et al., 2015). Given how well transformer-based models perform across unimodal and multimodal tasks, research efforts have recently started to address what makes them so effective, and to what extent they learn generalisable representations. Techniques to address these questions in unimodal and multimodal V&L contexts include: adversarial examples (Jia and Liang, 2017;Jia et al., 2019); investigation of the impact of bias, be it linguistic (Gururangan et al., 2018), visual semantic (Agarwal et al., 2020), or socio-economic (Garg et al., 2019); and the use of linguistically-informed counterfactual and minimally-edited examples (Levesque et al., 1 We release our dataset containing all annotators' votes (Prabhakaran et al., 2021) and code upon acceptance. 2012; Gardner et al., 2020). A trend within the latter research line that is specific to V&L models is vision-and-language foiling (Shekhar et al., 2017b;Gokhale et al., 2020;Bitton et al., 2021;Parcalabescu et al., 2021;Rosenberg et al., 2021), where the idea is to create counterfactual (i.e., foiled) and/or minimally edited examples by performing data augmentation on captions (Shekhar et al., 2017b,a) or images (Rosenberg et al., 2021).\n\nSince most V&L models are pretrained on some version of the image-text alignment task, it is possible to test their ability to distinguish correct from foiled captions (in relation to an image) in a zeroshot setting. The construction of foils can serve many investigation purposes. With VALSE, we target the linguistic grounding capabilities of V&L models, focusing on pervasive linguistic phenomena that span multiple tokens, described in §3.1- §3.6. At the same time, we ensure that our data is robust to perturbations and artefacts by i) controlling for word frequency biases between captions and foils, and ii) testing against unimodal collapse, a known issue of V&L models (Goyal et al., 2017;Madhyastha et al., 2018), thereby preventing models from solving the task using a single input modality. The issue of neural models exploiting data artefacts is well-known (Gururangan et al., 2018;Jia et al., 2019;Wang et al., 2020b;He et al., 2021) and methods have been proposed to uncover such effects, including gradient-based, adversarial perturbations or input reduction techniques (cf. Wallace et al., 2020). Yet, these methods are still not fully understood (He et al., 2021) and can be unreliable (Wang et al., 2020b).\n\nOur work is related to Gardner et al. (2020), who construct task-specific contrast sets for NLU. However, our focus is on modelling linguistic phenomena instead of tasks, and we construct carefully curated, balanced, single foils from valid instances that we select from multiple multimodal datasets.\n\n ",
    "start": 5158,
    "end": 5185,
    "label": "Format"
  },
  {
    "span": "(Levesque et al., 1",
    "document": "Introduction\n\nGeneral-purpose pretrained vision and language (V&L) models have gained notable performance on many V&L tasks (Lu et al., 2019;Tan and Bansal, 2019;Li et al., 2019;Li et al., 2020a;Su et al., 2020). As a result, V&L research has changed its focus from task-specific architectures to fine-tuning large V&L models.\n\nCurrent benchmarks give a good perspective on model performance on a wide range of V&L tasks Lourie et al., 2021;Li et al., 2021), but the field is only starting to assess why models perform so well and whether models learn specific capabilities that span multiple V&L tasks. Specifically, we lack detailed understanding of the extent to which such models are able to ground linguistic phenomena-from morphosyntax to semantics-in the visual modality (Bernardi and Pezzelle, 2021). For example, recent evidence suggests that models are insensitive to linguistic distinctions of verb-argument structure (Hendricks and Nematzadeh, 2021) and word order (Cirik et al., 2018;Akula et al., 2020).\n\nOur work addresses this gap with VALSE (Vision And Language Structured Evaluation), a benchmark for V&L model evaluation comprising six tasks, or 'pieces', where each piece has the same structure: given a visual input, a model is asked to distinguish real captions from foils, where a foil is constructed from a caption by altering a word or phrase that realizes a specific linguistic phenomenon, e.g., semantic number of nouns, verb argument structure, or coreference. VALSE uses a resource-lean diagnostic setup that dispenses with large-scale annotation (e.g., of bounding boxes), and builds on existing high-quality image captioning and VQA data. VALSE is designed to leverage the existing prediction heads in pretrained (or finetuned) V&L models; for that reason, our benchmark does not include any re-training and can be interpreted as a zero-shot evaluation. We build test data for each piece so as to safeguard against the possibility of models exploiting artefacts or statistical biases in the data, a well-known issue with highly parameterised neural models pretrained on large amounts of data (Goyal et al., 2017;Madhyastha et al., 2018;Kafle et al., 2019). With this in view, we propose novel methods to guard against the emergence of artefacts during foiling.\n\nOur main contributions are: i) We introduce VALSE, a novel benchmark aimed at gauging the sensitivity of pre-trained V&L models to foiled instances. ii) We cover a wide spectrum of basic linguistic phenomena affecting the linguistic and visual modalities: existence, plurality, counting, spatial relations, actions, and entity coreference. iii) We investigate novel strategies to build valid foils that include automatic and human validation. We balance word frequency distributions between captions and foils, and test against pretrained models solving the benchmark unimodally. We employ masked language modeling (MLM) in foil creation and semantic inference for validating foils, and finally collect human annotations for the entire benchmark. iv) We establish initial experimental results for pretrained V&L models of diverse architectures on VALSE. These models' overall weak performance indicates that the time is ripe for a novel, reliable foiling dataset targeting the visual grounding capabilities of V&L models through the lens of linguistic constructs. 1\n\n2 Background and Related work\n\nPretrained V&L models learn to combine vision and language through self-supervised multitask learning. Tasks include multimodal masked modeling-where words in the text and object labels or regions in the image are masked out, then predictedand image-sentence alignment, whereby a model learns to predict whether an image and a text correspond. Major architectures are single-and dualstream multimodal transformers: single-stream models concatenate word and image features, and encode the resulting sequence with a single transformer stack; dual-stream models use distinct transformer stacks to handle visual and textual inputs, and additional layers (e.g. co-attention) to fuse these into multimodal features.\n\nBenchmarking V&L models V&L models (Li et al., 2019;Lu et al., 2019;Tan and Bansal, 2019;Lu et al., 2020;Li et al., 2020b;Kim et al., 2021) are commonly evaluated on V&L tasks such as VQA (Goyal et al., 2017), visual reasoning (Suhr et al., 2019), or image retrieval (Lin et al., 2014;Plummer et al., 2015). Given how well transformer-based models perform across unimodal and multimodal tasks, research efforts have recently started to address what makes them so effective, and to what extent they learn generalisable representations. Techniques to address these questions in unimodal and multimodal V&L contexts include: adversarial examples (Jia and Liang, 2017;Jia et al., 2019); investigation of the impact of bias, be it linguistic (Gururangan et al., 2018), visual semantic (Agarwal et al., 2020), or socio-economic (Garg et al., 2019); and the use of linguistically-informed counterfactual and minimally-edited examples (Levesque et al., 1 We release our dataset containing all annotators' votes (Prabhakaran et al., 2021) and code upon acceptance. 2012; Gardner et al., 2020). A trend within the latter research line that is specific to V&L models is vision-and-language foiling (Shekhar et al., 2017b;Gokhale et al., 2020;Bitton et al., 2021;Parcalabescu et al., 2021;Rosenberg et al., 2021), where the idea is to create counterfactual (i.e., foiled) and/or minimally edited examples by performing data augmentation on captions (Shekhar et al., 2017b,a) or images (Rosenberg et al., 2021).\n\nSince most V&L models are pretrained on some version of the image-text alignment task, it is possible to test their ability to distinguish correct from foiled captions (in relation to an image) in a zeroshot setting. The construction of foils can serve many investigation purposes. With VALSE, we target the linguistic grounding capabilities of V&L models, focusing on pervasive linguistic phenomena that span multiple tokens, described in §3.1- §3.6. At the same time, we ensure that our data is robust to perturbations and artefacts by i) controlling for word frequency biases between captions and foils, and ii) testing against unimodal collapse, a known issue of V&L models (Goyal et al., 2017;Madhyastha et al., 2018), thereby preventing models from solving the task using a single input modality. The issue of neural models exploiting data artefacts is well-known (Gururangan et al., 2018;Jia et al., 2019;Wang et al., 2020b;He et al., 2021) and methods have been proposed to uncover such effects, including gradient-based, adversarial perturbations or input reduction techniques (cf. Wallace et al., 2020). Yet, these methods are still not fully understood (He et al., 2021) and can be unreliable (Wang et al., 2020b).\n\nOur work is related to Gardner et al. (2020), who construct task-specific contrast sets for NLU. However, our focus is on modelling linguistic phenomena instead of tasks, and we construct carefully curated, balanced, single foils from valid instances that we select from multiple multimodal datasets.\n\n ",
    "start": 5029,
    "end": 5048,
    "label": "Format"
  },
  {
    "span": "the SQuAD v3 dataset introduces adversarial questions and unanswerable cases.",
    "document": "Introduction\n\nMachine reading comprehension (MRC) benchmarks have catalyzed progress in question answering by providing standardized evaluation settings. Early datasets emphasized extractive answers, while later iterations added more realistic and challenging conditions. Among these benchmarks, the SQuAD v3 dataset introduces adversarial questions and unanswerable cases. Despite advancements in modeling, systems often exploit annotation artifacts and spurious correlations, calling for stricter evaluation and diagnostics. We address these limitations by proposing a perturbation-aware training recipe and a robustness-focused evaluation suite.",
    "reason": "First mention of a specific dataset and its properties is made without citation (rule a).",
    "start": 296,
    "end": 373,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior shared tasks have standardized accent classification protocols for evaluation.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) performance degrades under accent and dialect variation, prompting research on accent-robust acoustic modeling and domain adaptation (Jain et al., 2020; Biadsy et al., 2011). Techniques such as adversarial training, feature disentanglement, and multi-accent lexicons have shown mixed results depending on speaker diversity and phonetic shifts (Shinohara, 2016; Sun et al., 2018).\n\nBenchmarking accent robustness requires consistent label taxonomies, balanced sampling, and protocol alignment across corpora (Koenecke et al., 2020). Prior shared tasks have standardized accent classification protocols for evaluation. However, inconsistency persists in train/validation splits and reporting of per-accent error rates, complicating meta-analyses across studies.\n\nWe present an accent-conditional conformal adaptation method that calibrates posterior distributions per accent group, improving fairness metrics while preserving WER on in-domain speech.",
    "reason": "It references 'shared tasks' and claims standardization without citing any specific shared task or competition.",
    "start": 581,
    "end": 665,
    "label": "Unsupported_claim"
  },
  {
    "span": "Johnson et al.",
    "document": "Introduction\n\nNeural ranking models have achieved strong performance across ad hoc retrieval tasks by leveraging deep semantic representations of query and document text (Chen and Li, 2019; Rao et al., 2020). While early approaches focused on pairwise hinge-loss objectives, more recent models adopt listwise training and distillation signals to improve calibration (Gao and Callan, 2021). Johnson et al. argue that interpretability should be viewed as a constraint during optimization rather than an afterthought, motivating attention regularizers and sparse interaction features. Building on these ideas, we propose a calibration-aware retriever that jointly optimizes ranking quality and explanation fidelity (Kim and Park, 2022). We evaluate our model on MS MARCO and BEIR, following standard experimental protocols (Thakur et al., 2021).\n\nRelated Work\n\nRepresentation learning for retrieval dates back to DSSM-style encoders (Huang et al., 2013), but modern architectures often rely on pre-trained transformers and late interaction operators (Khattab and Zaharia, 2020). Our approach complements work on post-hoc explanation by designing training-time constraints to preserve human-understandable signals (Pruthi et al., 2020).",
    "reason": "Narrative citation missing year; should be formatted as 'Johnson et al. (YEAR)'.",
    "start": 390,
    "end": 404,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claim that cross-lingual phoneme inventories reduce data requirements by 40%.",
    "document": "Related Work\n\nMultilingual automatic speech recognition (ASR) aims to share representations across languages to improve recognition quality and data efficiency. Early methods relied on universal phoneme sets and grapheme-based modeling to avoid language-specific components. In a previous study, the authors claim that cross-lingual phoneme inventories reduce data requirements by 40%. More recent neural approaches leverage shared subword vocabularies and adapter-based finetuning to allow rapid transfer to new languages. However, the degree to which cross-lingual units help under extreme low-resource settings remains unclear.",
    "reason": "Refers to a 'previous study' and a specific quantitative claim without citing the source.",
    "start": 275,
    "end": 385,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most previous papers pre-train on the Open Graph Benchmark and finetune on node classification.",
    "document": "Related Work\n\nGraph representation learning has progressed from shallow embeddings to message-passing neural networks and scalable sampling methods. Pretraining strategies seek to capture structural motifs and attribute semantics for transfer across tasks. Most previous papers pre-train on the Open Graph Benchmark and finetune on node classification. However, transfer to link prediction and graph-level tasks remains inconsistent, suggesting a mismatch between pretraining objectives and downstream demands.\n\nWe propose a contrastive pretraining scheme that aligns subgraph views with task-specific readouts, improving transfer without architecture changes.",
    "reason": "Claims a dominant experimental setup in prior work without providing citations to those papers (definition a and d).",
    "start": 257,
    "end": 352,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Nguyen 2020)",
    "document": "Related Work\n\nPrivacy-preserving analytics seek to enable utility while protecting sensitive attributes. Differential privacy (Dwork et al., 2006) provides formal guarantees by injecting calibrated noise, with applications in training machine learning models (Abadi et al., 2016; Papernot et al., 2017). In federated settings, secure aggregation and partial participation complicate threat models (Kairouz et al., 2021). Several works examine privacy leakage via gradients and model updates (Melis et al., 2019; Nguyen 2020), motivating stronger protections in cross-device scenarios.\n\nOur approach combines record-level differential privacy with compression-aware noise shaping to retain accuracy under tight communication budgets.",
    "reason": "Missing comma between author and year in a parenthetical citation; should be “(Nguyen, 2020)”.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "BERT has been successfully used in automated essay scoring with prompts collected from the TOEFL exam.",
    "document": "Introduction\n\nAutomated essay scoring (AES) seeks to predict human-assigned holistic and trait scores for student writing, reducing turnaround time and ensuring consistent feedback. Modern AES systems leverage pretrained encoders to capture syntactic fluency, discourse structure, and prompt adherence. Despite improvements, challenges remain in domain shift, prompt-specific bias, and robustness to adversarial inputs.\n\nBERT has been successfully used in automated essay scoring with prompts collected from the TOEFL exam. However, the community lacks a clear understanding of how prompt characteristics interact with encoder representations. We analyze prompt sensitivity and propose a contrastive prompt-anchored objective to mitigate overfitting to prompt artifacts.",
    "reason": "Describes a specific prior setup (model and dataset/provenance) without providing a citation to any prior study.",
    "start": 421,
    "end": 523,
    "label": "Unsupported_claim"
  },
  {
    "span": "Zhang et al.",
    "document": "Introduction\n\nRecent progress in semi-supervised learning has largely been driven by consistency regularization and pseudo-labeling strategies (Sohn et al., 2020; Xie et al., 2020). These methods leverage unlabeled data by enforcing invariances across perturbations or by generating labels with a teacher model. However, their effectiveness can degrade under severe domain shift, where the distribution of unlabeled samples diverges from the labeled source (Ben-David et al., 2010; Tachet des Combes et al., 2020). To address this challenge, researchers have explored combining domain-adversarial training with modern SSL objectives (Ganin et al., 2016; Long et al., 2018), as well as designing alignment losses that encourage class-conditional matching (Saito et al., 2019). Building upon the intuition of curriculum learning, Zhang et al. propose to gradually incorporate unlabeled target data according to model confidence, thereby mitigating error amplification. While such curricula are appealing, they can be brittle when confidence is not well-calibrated (Guo et al., 2017) and may overfit to spurious correlations in the target domain. In contrast, our approach introduces a risk-aware regularizer that penalizes uncertain predictions and a target-aware data selection scheme that balances exploration and exploitation across training.\n",
    "reason": "Narrative citation missing year; APA-style narrative citations should include the year immediately after the authors, e.g., 'Zhang et al. (2019)'.",
    "start": 828,
    "end": 840,
    "label": "Format"
  },
  {
    "span": "Previous shared tasks have shown that entity linking errors account for over 40% of failures.",
    "document": "Introduction\n\nQuestion answering over knowledge graphs (KGQA) translates user questions into structured queries executable over a graph of entities and relations. State-of-the-art systems decompose the problem into entity linking, relation inference, and query composition, often guided by neural semantic parsers. While these models excel on templated or single-hop queries, they struggle with ambiguity, compositionality, and incomplete knowledge.\n\nA persistent bottleneck in KGQA pipelines is error propagation from early stages. Previous shared tasks have shown that entity linking errors account for over 40% of failures. This suggests that improving mention disambiguation and candidate pruning could yield disproportionately large gains downstream.\n\nWe present an end-to-end KGQA model with uncertainty-aware entity linking. Our approach jointly optimizes mention detection, candidate scoring, and relation path selection under a unified objective, and it leverages graph embeddings regularized by link prediction priors. We introduce a calibration module that estimates confidence intervals for entity choices and defers uncertain cases to a reranker with broader graph context. Evaluations on multiple KGQA benchmarks demonstrate improvements in exact match and robustness to adversarial entity substitutions.",
    "reason": "Asserts a quantitative statistic from 'previous shared tasks' without citing any specific competition or report, violating the requirement to cite at first mention and to support statistics (a, b).",
    "start": 533,
    "end": 626,
    "label": "Unsupported_claim"
  },
  {
    "span": "The CoQA dataset contains roughly 127k questions covering eight conversational domains.",
    "document": "Introduction\n\nConversational question answering (CQA) extends single-turn QA by requiring multi-turn reasoning, coreference resolution, and conversational context modeling. Recent encoder-decoder architectures with attention over dialog history have improved performance but still struggle with long-range dependencies and grounding.\n\nThe CoQA dataset contains roughly 127k questions covering eight conversational domains. In this work, we introduce a history-aware retrieval-augmented reader that conditions on compacted dialog states and external evidence, aiming to reduce error propagation across turns while maintaining latency suitable for interactive systems.",
    "reason": "Presents specific statistics about a dataset without a citation or evidence (definition b).",
    "start": 335,
    "end": 422,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith et al., (2020)",
    "document": "Related Work\n\nUnsupervised domain adaptation reduces distribution shift between labeled source and unlabeled target domains. Feature alignment by adversarial training learns domain-invariant representations (Ganin and Lempitsky, 2015), while moment matching minimizes statistical discrepancies (Long et al., 2015). As shown by Smith et al., (2020) combining class-conditional alignment with entropy minimization further improves target performance. Semi-supervised variants leverage a small set of target labels to stabilize training (French et al., 2018).\n\nOur framework focuses on curriculum-based target sampling with confidence calibration. We analyze when entropy regularization harms under severe class imbalance and propose a reweighting scheme to address it.",
    "reason": "Incorrect comma before the year in a narrative citation; it should be 'Smith et al. (2020)' without the comma before the parenthetical year.",
    "start": 327,
    "end": 347,
    "label": "Format"
  },
  {
    "span": "Fairness in recommender systems has been formulated through individual and group criteria (Dwork et al., 2012; Hardt et al., 2016; Yao and Huang, 2017), exposure and calibration metrics (Singh and Joachims, 2018; Biega et al., 2018; Steck, 2018), and producer-consumer trade-offs (Burke, 2017; Mehrotra et al., 2018). Debiasing techniques include reweighting and propensity correction (Schnabel et al., 2016; Wang et al., 2019), adversarial representation learning (Beutel et al., 2017; Bose and Hamilton, 2019), and constrained optimization for exposure (Celis et al., 2018; Diaz et al., 2020).",
    "document": "Related Work\n\nRecommender systems can propagate and amplify historical biases in both relevance estimation and allocation of user attention, necessitating fairness-aware objectives and training procedures.\n\nFairness in recommender systems has been formulated through individual and group criteria (Dwork et al., 2012; Hardt et al., 2016; Yao and Huang, 2017), exposure and calibration metrics (Singh and Joachims, 2018; Biega et al., 2018; Steck, 2018), and producer-consumer trade-offs (Burke, 2017; Mehrotra et al., 2018). Debiasing techniques include reweighting and propensity correction (Schnabel et al., 2016; Wang et al., 2019), adversarial representation learning (Beutel et al., 2017; Bose and Hamilton, 2019), and constrained optimization for exposure (Celis et al., 2018; Diaz et al., 2020).\n\nWe focus on dynamic exposure fairness under shifting demand, introducing a control-theoretic objective that stabilizes disparity while preserving online relevance adaptation.",
    "reason": "The span summarizes metrics and methods but does not connect them to the paper’s dynamic exposure objective or highlight a concrete gap, aligning with (a) and (b).",
    "start": 207,
    "end": 802,
    "label": "Lacks_synthesis"
  },
  {
    "span": "the widely used FND-1.0 dataset",
    "document": "Related Work\n\nAutomated fake news detection has advanced rapidly with the advent of large-scale social media data and neural text encoders. Early methods relied on lexical features and handcrafted metadata signals, whereas recent models leverage pretrained transformers and propagation patterns to capture both content and context. Beyond text, multimodal systems incorporate images and social graphs to further improve robustness.\n\nBenchmark datasets play a pivotal role in standardizing evaluation. In particular, the widely used FND-1.0 dataset has enabled comparisons across methods on headline-level and article-level detection settings, with both binary and multi-class labels. However, label noise and topic leakage remain open challenges, motivating techniques for domain generalization and debiasing. Our work examines these issues by introducing stratified splits and calibration-aware training that reduce reliance on spurious correlations.",
    "reason": "The first mention of a specific dataset (\"the widely used FND-1.0 dataset\") requires a citation to the dataset or its accompanying paper (rule a).",
    "start": 516,
    "end": 547,
    "label": "Unsupported_claim"
  },
  {
    "span": "In automatic speech recognition, knowledge distillation has been applied to compress acoustic models, align CTC posteriors, and transfer sequence-level knowledge (Hernandez et al., 2019; Kim and Park, 2020; Zhao et al., 2021). Teacher-student training for streaming transducers and conformers has shown accuracy-latency trade-offs (Liu et al., 2022; Singh et al., 2023).",
    "document": "Related Work\nModel compression for ASR. In automatic speech recognition, knowledge distillation has been applied to compress acoustic models, align CTC posteriors, and transfer sequence-level knowledge (Hernandez et al., 2019; Kim and Park, 2020; Zhao et al., 2021). Teacher-student training for streaming transducers and conformers has shown accuracy-latency trade-offs (Liu et al., 2022; Singh et al., 2023). Quantization-aware and pruning-based compression have also been explored for on-device inference (Bai and Zhou, 2020; Ortega et al., 2021).\n\nGeneralization and domain shift. Robust ASR under noise and accent shift has been studied through data augmentation, multi-condition training, and unsupervised adaptation (Ferreira et al., 2020; Gupta and Ren, 2021; Costa et al., 2022).\n\nWe address on-device streaming ASR by distilling streaming teachers into compact students with latency-aware losses and entropy-regularized alignments, improving tail latency without sacrificing robustness under domain shift.",
    "reason": "The span lists distillation applications and models but does not connect them to the specific objectives or shortcomings the paper targets; it fails to articulate a gap or the author's viewpoint.",
    "start": 40,
    "end": 410,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Radiology report generation systems translate images into descriptive sentences (Jing et al., 2018). Lesion segmentation improves detection sensitivity on CT scans (Ronneberger et al., 2015). Chest X-ray datasets provide paired image–text supervision (Irvin et al., 2019).",
    "document": "Related Work\n\nMultimodal learning in medical imaging and clinical NLP targets tasks such as report generation, image-text retrieval, and decision support. Methods leverage paired image–text corpora, structured labels, and domain ontologies to improve both clinical accuracy and linguistic quality.\n\nRadiology report generation systems translate images into descriptive sentences (Jing et al., 2018). Lesion segmentation improves detection sensitivity on CT scans (Ronneberger et al., 2015). Chest X-ray datasets provide paired image–text supervision (Irvin et al., 2019). Further work grounds generation in structured findings and uncertainty cues to better reflect diagnostic nuance (Liu et al., 2019), while retrieval-augmented captioning reuses similar past reports to enhance factuality (Kougia et al., 2021).\n\nWe introduce a retrieval- and ontology-guided generator that enforces alignment between predicted findings and textual evidence through a contrastive consistency loss.",
    "reason": "The sentences jump from report generation to segmentation to datasets without transitions or explanation of their relationships, resulting in an abrupt, incoherent progression.",
    "start": 299,
    "end": 571,
    "label": "Coherence"
  },
  {
    "span": "Collaborative filtering learns user–item interactions from historical logs (Patil and Shah, 2017). Graph neural networks propagate signals along user–item relations (Zhang et al., 2019). Causal objectives aim to debias exposure and selection effects (Wei and Morton, 2021).",
    "document": "Introduction\n\nRecommendation systems must balance relevance, fairness, and robustness to distribution shifts induced by platform policies and user behavior. A growing body of work addresses representation learning and bias mitigation.\n\nCollaborative filtering learns user–item interactions from historical logs (Patil and Shah, 2017). Graph neural networks propagate signals along user–item relations (Zhang et al., 2019). Causal objectives aim to debias exposure and selection effects (Wei and Morton, 2021). Context-aware methods incorporate temporal dynamics and content features (Iyer et al., 2020), yet consistent evaluation across interventions is scarce.\n\nWe propose a counterfactual training protocol that aligns graph encoders with logged propensities to improve out-of-distribution ranking.",
    "reason": "The span lists three areas with citations but provides no transitions or explicit statements relating them; the jump from collaborative filtering to GNNs to causality is abrupt across multiple sentences, reducing coherence.",
    "start": 236,
    "end": 509,
    "label": "Coherence"
  },
  {
    "span": "In federated learning, privacy is commonly enforced with client-side or server-side differential privacy mechanisms (Geyer et al., 2017; McMahan et al., 2018; Kairouz et al., 2021). Various clipping strategies, noise schedules, and aggregation protocols have been proposed to trade off accuracy and privacy (Truex et al., 2019; Andrew et al., 2021; Bu et al., 2020).",
    "document": "Related Work\n\nCommunication-efficient training. A central line of research in federated learning focuses on reducing uplink and downlink communication through compression, sparsification, and partial participation schedules. These methods can significantly lower the bandwidth costs while maintaining competitive accuracy on standard benchmarks.\n\nPrivacy in federated learning. In federated learning, privacy is commonly enforced with client-side or server-side differential privacy mechanisms (Geyer et al., 2017; McMahan et al., 2018; Kairouz et al., 2021). Various clipping strategies, noise schedules, and aggregation protocols have been proposed to trade off accuracy and privacy (Truex et al., 2019; Andrew et al., 2021; Bu et al., 2020).\n\nRobustness and heterogeneity. Another thread addresses robustness to Byzantine clients and statistical heterogeneity across devices, introducing aggregation rules and personalization techniques that adapt models to local data without harming global convergence.\n\nWe build on these themes by introducing an adaptive per-layer privacy accounting scheme that aligns noise allocation with gradient sensitivity profiles, improving accuracy at a fixed privacy budget in heterogeneous settings.",
    "reason": "The span enumerates prior privacy mechanisms and techniques without explaining their limitations relative to the present work or articulating the specific gap to be addressed.",
    "start": 378,
    "end": 744,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Group fairness criteria such as demographic parity of exposure, equality of opportunity, and disparate impact have been adapted to ranking, with algorithms that re-order items to satisfy constraints (Singh and Joachims, 2018; Biega et al., 2018; Celis et al., 2020). Individual fairness notions based on similarity metrics have also been explored (Dwork et al., 2012; Zehlike et al., 2020).",
    "document": "Introduction\n\nLearning-to-rank systems can amplify societal biases by skewing exposure and opportunity. The fairness literature proposes both group-level and individual-level guarantees tailored to ranked outputs.\n\nGroup fairness criteria such as demographic parity of exposure, equality of opportunity, and disparate impact have been adapted to ranking, with algorithms that re-order items to satisfy constraints (Singh and Joachims, 2018; Biega et al., 2018; Celis et al., 2020). Individual fairness notions based on similarity metrics have also been explored (Dwork et al., 2012; Zehlike et al., 2020).\n\nWe consider fairness under user heterogeneity where relevance distributions vary across audiences. Our contribution is a user-conditional exposure allocation that balances relevance and equity without retraining base rankers.",
    "reason": "The span states prior criteria and methods but fails to tie them to user heterogeneity or explain what remains unaddressed, satisfying (a) and (b).",
    "start": 215,
    "end": 605,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Conversational recommenders have combined reinforcement learning for dialog policy with neural ranking or bandit-based item selection (Zhang et al., 2018; Chen et al., 2019; Lei et al., 2020). Knowledge-graph augmented approaches leverage entity relations to guide conversation and improve recommendation quality (Zhou et al., 2020; Sun et al., 2021).",
    "document": "Related Work\n\nConversational recommender systems (CRS) aim to elicit user preferences through natural dialog and deliver personalized suggestions. Methods vary in dialog management, user modeling, and knowledge utilization.\n\nConversational recommenders have combined reinforcement learning for dialog policy with neural ranking or bandit-based item selection (Zhang et al., 2018; Chen et al., 2019; Lei et al., 2020). Knowledge-graph augmented approaches leverage entity relations to guide conversation and improve recommendation quality (Zhou et al., 2020; Sun et al., 2021).\n\nOur work focuses on cold-start users who provide sparse signals in early turns. We propose intent-aware slot acquisition with uncertainty-driven question selection, improving early-stage satisfaction and success rate.",
    "reason": "The span lists categories of approaches and citations but does not connect them to the cold-start focus or delineate the gap, hence lacking synthesis per (a) and (b).",
    "start": 225,
    "end": 576,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent studies have examined data augmentation strategies for improving generalization in image classification (Cubuk et al., 2019; Zhang et al., 2018; Lim et al., 2019; Yun et al., 2019).",
    "document": "Related Work\n\nRobustness in Vision Transformers. Vision Transformers (ViTs) have demonstrated strong performance on a range of image classification benchmarks, but their robustness under distribution shifts and corruptions remains an active area of inquiry. Early works study the role of pretraining scale and tokenization strategies in shaping inductive biases for robustness (Dosovitskiy et al., 2021; Touvron et al., 2021). Subsequent analyses examine how patch size and positional encoding choices affect feature locality and sensitivity to noise (Raghu et al., 2021; Naseer et al., 2021). Recent studies have examined data augmentation strategies for improving generalization in image classification (Cubuk et al., 2019; Zhang et al., 2018; Lim et al., 2019; Yun et al., 2019). Parallel lines of work consider adversarial training and certified defenses tailored to transformer backbones (Bhojanapalli et al., 2021; Paul and Chen, 2022).\n\nTraining Dynamics and Scaling Laws. Scaling analyses suggest that model size, dataset size, and compute jointly determine the attainable error for ViTs (Nakkiran et al., 2021; Kaplan et al., 2020). Other studies investigate the optimization landscape and implicit regularization under different training protocols (Goyal et al., 2017; Hoffer et al., 2017).\n\nIn this paper, we study robustness trade-offs by evaluating a suite of pretraining recipes across multiple corruptions and shifts using standardized testbeds.",
    "reason": "The sentence lists prior augmentation works without explaining their relevance to ViT robustness or how the present study engages with or departs from them (definition a and c).",
    "start": 594,
    "end": 782,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[Karpukhin et al., 2020]",
    "document": "Related Work\n\nDense passage retrieval. Dual-encoder models map queries and passages into a shared embedding space to enable efficient nearest-neighbor search. Training with in-batch negatives and hard negative mining substantially boosts recall. While dense retrieval scales well, it can suffer from lexical mismatch and domain shift. Recent improvements integrate multi-vector representations and late interaction to capture fine-grained relevance signals [Karpukhin et al., 2020].\n\nReader integration. Fusion-in-decoder and reranking readers improve end-to-end QA by aggregating evidence across retrieved passages. However, joint optimization of retriever and reader can overfit to spurious correlations, motivating curriculum schemes and uncertainty-aware training.",
    "reason": "Wrong citation style: square brackets are used in an APA-style context. It should be '(Karpukhin et al., 2020)'.",
    "start": 457,
    "end": 481,
    "label": "Format"
  },
  {
    "span": "(BERT Devlin et al. (2019))",
    "document": "Related Work\n\nPretrained language models have transformed NLP by enabling efficient transfer to downstream tasks. We follow common practice and initialize our encoder with a bidirectional Transformer. In particular, we fine-tune (BERT Devlin et al. (2019)) and compare with domain-adapted variants that continue pretraining on in-domain corpora (Gururangan et al., 2020; Lewis and Fan, 2021). Our results highlight the importance of task-specific adapters over full-parameter fine-tuning when labeled data is scarce.",
    "reason": "Mismatched/doubled parentheses: nested parentheses around a narrative-citation hybrid. Correct forms include \"BERT (Devlin et al., 2019)\" or \"Devlin et al. (2019) introduced BERT\".",
    "start": 229,
    "end": 256,
    "label": "Format"
  },
  {
    "span": "(Lample and Conneau, 2019; Artetxe et al., 2020; Lee and Park, 2020; Chen, 2019",
    "document": "Related Work\n\nCross-lingual question answering has benefited from multilingual pretraining (Devlin et al., 2019; Conneau et al., 2020) and translation-based pipelines (Asai et al., 2018). Distant supervision for low-resource languages uses weakly labeled passages (Clark et al., 2020). Recent advances in dense retrieval enable language-agnostic indexing (Xiong et al., 2021; Asai et al., 2021). However, alignment noise remains a bottleneck (Lample and Conneau, 2019; Artetxe et al., 2020; Lee and Park, 2020; Chen, 2019. We evaluate transfer with adversarial training (Goodfellow et al., 2015) and consistency regularization across languages.",
    "reason": "Missing closing parenthesis in a parenthetical citation list.",
    "start": 442,
    "end": 521,
    "label": "Format"
  },
  {
    "span": "Li, et al. (2018)",
    "document": "Related Work\n\nContinual learning aims to mitigate catastrophic forgetting when models learn tasks sequentially. Regularization-based methods penalize updates to parameters deemed important for past tasks, while replay-based methods interleave examples or gradients from a memory buffer. Architecture-based techniques allocate new capacity per task to reduce interference. Li, et al. (2018) introduce a rehearsal-free approach using knowledge distillation between tasks, whereas recent work refines buffer sampling and constraint enforcement to balance plasticity and stability (Chaudhry et al., 2019; Aljundi et al., 2019).",
    "reason": "Incorrect punctuation in the narrative citation: the comma before “et al.” is not used in common styles like APA. It should be “Li et al. (2018)”.",
    "start": 372,
    "end": 389,
    "label": "Format"
  },
  {
    "span": "BALD selects informative samples via mutual information (Houlsby et al., 2011). Core-set sampling targets coverage in feature space (Sener and Savarese, 2018). Loss-based sampling prioritizes high-loss detections (Yoo and Kweon, 2019). Augmentation policies improve data efficiency (Cubuk et al., 2019). Training-time sampling integrates policies into the optimization loop (Ash et al., 2019).",
    "document": "Introduction\n\nActive Learning for Object Detection\n\nLabeling bounding boxes is expensive, motivating active learning (AL) strategies that maximize utility per annotation. Unlike classification, detection involves selecting images and regions, handling class imbalance, and accounting for localization uncertainty.\n\nBALD selects informative samples via mutual information (Houlsby et al., 2011). Core-set sampling targets coverage in feature space (Sener and Savarese, 2018). Loss-based sampling prioritizes high-loss detections (Yoo and Kweon, 2019). Augmentation policies improve data efficiency (Cubuk et al., 2019). Training-time sampling integrates policies into the optimization loop (Ash et al., 2019).\n\nRegion-Level Selection\n\nObjectness and localization uncertainty guide region proposals (Roy et al., 2018). Query-by-Committee variants exploit ensemble disagreement (Beluch et al., 2018), while diversity promotes non-redundant selections (Sinha et al., 2019).\n\nThis Work\n\nWe propose a joint image–region querying scheme that decouples classification and localization uncertainty, with a budget-aware optimizer that balances exploration and exploitation.",
    "reason": "The span presents a list of disparate AL strategies and even augmentation work with no transitions or explicit connections; the relationships are implied, resulting in abrupt, uncoordinated sentences.",
    "start": 315,
    "end": 708,
    "label": "Coherence"
  },
  {
    "span": "It is well known that MovieLens contains popularity bias that inflates NDCG.",
    "document": "Introduction\n\nRecommender systems are increasingly evaluated with offline metrics that may not reflect user welfare or fairness. Biases in exposure and feedback loops can distort both training and evaluation, particularly in implicit-feedback settings. It is well known that MovieLens contains popularity bias that inflates NDCG. While reweighting and counterfactual methods attempt to correct these issues, robust conclusions require careful treatment of missing-not-at-random data.\n\nIn this paper, we introduce a stratified evaluation that conditions on item popularity and calibration gaps, enabling a more faithful assessment of ranking quality across user segments.",
    "reason": "Claims a widely known property of a specific dataset that affects evaluation without citing supporting studies (definition b and e).",
    "start": 253,
    "end": 329,
    "label": "Unsupported_claim"
  },
  {
    "span": "[Chen et al., 2020)",
    "document": "Related Work\n\nSample efficiency remains a central challenge in reinforcement learning (RL). Model-based methods leverage learned dynamics to reduce environment interaction (Deisenroth and Rasmussen, 2011; Janner et al., 2019). Off-policy algorithms such as SAC and TD3 improve stability via better exploration and critic regularization (Haarnoja et al., 2018; Fujimoto et al., 2018). Some works [Chen et al., 2020) investigate distributional critics with auxiliary tasks to accelerate learning. In contrast, meta-RL approaches aim to rapidly adapt across tasks (Finn et al., 2017; Rakelly et al., 2019). Our method combines conservative value estimation with representation learning for sparse-reward domains.",
    "reason": "Mismatched brackets in a parenthetical citation; should use matching parentheses or square brackets consistently.",
    "start": 395,
    "end": 414,
    "label": "Format"
  },
  {
    "span": "Self-supervised speech models learn from raw audio at scale, including wav2vec 2.0 (Baevski et al., 2020), HuBERT (Hsu et al., 2021), WavLM (Chen et al., 2022), and data2vec (Baevski et al., 2022). Improvements arise from better objectives and larger corpora.",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) has benefited from self-supervised pretraining, which reduces reliance on labeled data by learning general acoustic representations. Subsequent fine-tuning yields strong performance on multiple tasks.\n\nSelf-supervised speech models learn from raw audio at scale, including wav2vec 2.0 (Baevski et al., 2020), HuBERT (Hsu et al., 2021), WavLM (Chen et al., 2022), and data2vec (Baevski et al., 2022). Improvements arise from better objectives and larger corpora.\n\nFine-tuning strategies vary from CTC to sequence-to-sequence decoders, with domain adaptation techniques addressing accent and noise. We evaluate on standard ASR benchmarks and assess robustness.",
    "reason": "The span summarizes notable self-supervised models but does not explain how they inform the authors' method, what problem remains unsolved, or why another approach is necessary.",
    "start": 251,
    "end": 510,
    "label": "Lacks_synthesis"
  },
  {
    "span": "WMT shared tasks have repeatedly shown that back-translation is the single most effective technique for low-resource MT.",
    "document": "Related Work\n\nLow-resource machine translation (MT) faces severe data scarcity, domain mismatch, and noisy supervision. Data augmentation strategies such as back-translation, self-training, and bilingual lexicon induction have been proposed to mitigate these challenges. WMT shared tasks have repeatedly shown that back-translation is the single most effective technique for low-resource MT. Recent work further explores synthetic data filtering, iterative refinement, and multilingual transfer to amplify gains. Despite strong results, the interaction between augmentation and evaluation artifacts remains underexamined.\n",
    "reason": "The sentence cites findings from shared tasks without referencing specific WMT years, reports, or papers, which is required for mentions of shared tasks and prior results (rule a).",
    "start": 271,
    "end": 391,
    "label": "Unsupported_claim"
  },
  {
    "span": "Tox21 benchmark",
    "document": "Related Work\n\nMolecular property prediction has increasingly relied on graph neural networks that operate directly on molecular graphs. Architectures such as message passing networks and attention-based variants have reported competitive performance across toxicity, solubility, and activity prediction tasks.\n\nEvaluation commonly uses public benchmarks such as the Tox21 benchmark, QM9, and other collections that span classification and regression endpoints. Beyond accuracy, recent literature emphasizes scaffold splits to better measure generalization to novel chemotypes and recommends uncertainty quantification for safety-critical decisions.\n\nOur work builds on these insights by proposing a calibration-aware training objective tailored to low-data regimes, and we compare against strong baselines under scaffold and temporal splits on multiple datasets.",
    "reason": "The dataset 'Tox21 benchmark' is mentioned without a citation at first mention, which should be referenced per rule a.",
    "start": 366,
    "end": 381,
    "label": "Unsupported_claim"
  },
  {
    "span": "The HumanEval benchmark remains the de facto standard for evaluating code generation systems.",
    "document": "Related Work\n\nRecent advances in large language models have enabled competitive code generation across multiple programming languages. Benchmark suites assess functional correctness through unit tests and measure generalization to unseen problems.\n\nThe HumanEval benchmark remains the de facto standard for evaluating code generation systems. Complementary resources introduce multi-language tasks, execution-based metrics, and security-focused assessments, yet comparability across studies can be limited by sampling temperature, prompt formatting, and test-time augmentation choices.\n\nWe propose a standardized evaluation protocol with controlled decoding and seed management to improve reproducibility and comparability across code generation models.",
    "reason": "First mention of a specific benchmark and its status lacks a supporting citation.",
    "start": 249,
    "end": 342,
    "label": "Unsupported_claim"
  },
  {
    "span": "Alvarez et al.",
    "document": "Introduction\n\nFederated learning enables mobile and edge devices to collaboratively train models without centralizing raw data, thereby reducing privacy and communication risks (McMahan et al., 2017; Kairouz et al., 2021). To guard against information leakage, researchers have proposed differential privacy and secure aggregation mechanisms (Dwork et al., 2014; Bonawitz et al., 2017). Alvarez et al. propose a personalization approach that adapts global models to local heterogeneity while preserving cross-client knowledge transfer. Building on this line, we study how aggregation frequency interacts with local optimization and client drift, extending insights from prior analyses of non-IID data (Li et al., 2020; Karimireddy et al., 2020). We also consider communication-efficient schemes via sparsification and quantization (Aji and Heafield, 2017; Alistarh et al., 2017) and measure their impact on fairness across clients with skewed data distributions (Li et al., 2021). Our evaluation spans vision and language benchmarks under realistic participation patterns and intermittent connectivity.\n",
    "reason": "Narrative citation missing year; in author–year style, narrative mentions should include the year as in “Alvarez et al. (2019)” rather than just the authors.",
    "start": 387,
    "end": 401,
    "label": "Format"
  },
  {
    "span": "Transformer-based forecasters model long-range dependencies with sparse attention patterns (Zhou et al., 2021; Lim et al., 2021). Decomposition methods separate trend and seasonality for robustness (Wang et al., 2022). Probabilistic forecasting captures uncertainty via quantiles and likelihood-based objectives (Wen et al., 2017).",
    "document": "Introduction\n\nTime series forecasting under long horizons requires models that balance inductive bias, capacity, and uncertainty. Recent neural approaches extend sequence models to handle multiple seasonalities and scale to multivariate settings.\n\nEvaluation protocols must disentangle accuracy from calibration, and benchmarks should reflect realistic non-stationarities, missing data, and covariate shifts common in practice.\n\nTransformer-based forecasters model long-range dependencies with sparse attention patterns (Zhou et al., 2021; Lim et al., 2021). Decomposition methods separate trend and seasonality for robustness (Wang et al., 2022). Probabilistic forecasting captures uncertainty via quantiles and likelihood-based objectives (Wen et al., 2017).\n\nWe introduce a unified architecture that couples seasonal-trend decomposition with attention and a coherent probabilistic head. Our experiments emphasize distribution shift and recalibration under regime changes.",
    "reason": "The span lists three distinct lines of work without transitions or explicit relationships, leaving unclear how transformers, decomposition, and probabilistic methods connect to each other (a, b).",
    "start": 429,
    "end": 760,
    "label": "Coherence"
  },
  {
    "span": "Bolukbasi et al. (2016) documented gender biases in word embeddings. Caliskan et al. (2017) measured implicit biases captured by distributional semantics. Zhao et al. (2018) studied occupation bias amplification in coreference systems. Sun et al. (2019) explored debiasing strategies for contextualized representations.",
    "document": "Related Work\n\nFairness in natural language processing examines how models encode, propagate, and amplify social biases across tasks and datasets. Recent studies consider definitional choices of fairness, measurement protocols, and mitigation techniques under distribution shift.\n\nBolukbasi et al. (2016) documented gender biases in word embeddings. Caliskan et al. (2017) measured implicit biases captured by distributional semantics. Zhao et al. (2018) studied occupation bias amplification in coreference systems. Sun et al. (2019) explored debiasing strategies for contextualized representations.\n\nOther works investigate causal perspectives on bias and counterfactual evaluation (Kusner et al., 2017; Garg et al., 2019). Dataset curation and annotation practices are increasingly recognized as critical for fairness outcomes (Bender and Friedman, 2018). We focus on task-agnostic mitigation that maintains downstream utility across domains.",
    "reason": "Although all sentences are about bias, they are presented as isolated statements without explicit transitions or explanation of how each cited work relates to the previous one or to the stated themes. The result is an abrupt, unconnected sequence.",
    "start": 280,
    "end": 599,
    "label": "Coherence"
  },
  {
    "span": "Human experts achieve 96% F1 on the MIMIC-CXR report classification task.",
    "document": "Introduction\n\nAutomated clinical text classification seeks to label radiology reports with findings that support downstream decision-making. While transformer encoders pretrained on biomedical corpora have advanced state of the art, performance can be sensitive to annotation quality and label imbalance. Human–AI collaboration further complicates evaluation, as thresholds can be tuned for different operating points.\n\nHuman experts achieve 96% F1 on the MIMIC-CXR report classification task. Such a figure, if accurate, would anchor the human upper bound, but consensus-based expert performance varies by label granularity and adjudication protocol. This work benchmarks models under unified label schemas and reports calibration metrics alongside F1 to support clinical deployment.",
    "reason": "Asserts a precise human performance statistic on a specific dataset without any citation or supporting evidence, violating rule (b).",
    "start": 420,
    "end": 493,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior efforts in federated optimization include asynchronous aggregation (Kairouz et al., 2019; Xie et al., 2020), variance reduction and adaptive optimizers (Reddi et al., 2021; Karimireddy et al., 2020), personalization layers and meta-learning (Smith et al., 2017; Arivazhagan et al., 2019), and communication compression (Suresh et al., 2017; Reisizadeh et al., 2020). In healthcare, studies have applied FL to medical imaging (Sheller et al., 2020; Li et al., 2020), ICU time series (Dayan et al., 2021), and genomics (Rieke et al., 2020). Benchmark suites and privacy accounting have also been explored (Caldas et al., 2018; Geyer et al., 2017).",
    "document": "Introduction Electronic health records (EHR) offer rich signals for clinical risk prediction, yet training models on pooled data is impeded by privacy, governance, and interoperability barriers. Federated learning (FL) has emerged as a promising paradigm to enable collaborative modeling without centralizing raw data. Nonetheless, heterogeneity across institutions, noisy documentation practices, and strict privacy budgets complicate optimization and evaluation in practice. Prior efforts in federated optimization include asynchronous aggregation (Kairouz et al., 2019; Xie et al., 2020), variance reduction and adaptive optimizers (Reddi et al., 2021; Karimireddy et al., 2020), personalization layers and meta-learning (Smith et al., 2017; Arivazhagan et al., 2019), and communication compression (Suresh et al., 2017; Reisizadeh et al., 2020). In healthcare, studies have applied FL to medical imaging (Sheller et al., 2020; Li et al., 2020), ICU time series (Dayan et al., 2021), and genomics (Rieke et al., 2020). Benchmark suites and privacy accounting have also been explored (Caldas et al., 2018; Geyer et al., 2017). In this work, we present a federated evaluation protocol for heterogeneous EHR risk modeling that standardizes cohort definitions, event time anchoring, and privacy accounting across institutions. We also propose a robust aggregation objective that adapts to skewed label prevalence and irregular sampling. Our experiments across four health systems demonstrate improved calibration under tight privacy budgets.",
    "reason": "The span lists prior federated learning methods and applications without explaining how they relate to the present study, what gap remains in healthcare FL, or why existing approaches are insufficient. It lacks articulation of the authors' perspective or motivation (a, c).",
    "start": 477,
    "end": 1128,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Klein et al., 2021)",
    "document": "Related Work\n\nContrastive representation learning has improved sample efficiency across language and vision tasks (He et al., 2020; Gao et al., 2021). For text classification, instance discrimination and supervised contrastive objectives provide robust sentence embeddings under domain shift (Zhou and Li, 2022).\n\nIn (Klein et al., 2021) a domain-adaptive contrastive loss is proposed to align clusters across corpora, while maintaining class separation (Martin and Xu, 2022). Our approach differs by explicitly modeling the curriculum of domains to avoid negative transfer (Ortega et al., 2023).",
    "reason": "Uses a parenthetical citation after the preposition 'In'; should be narrative style: In Klein et al. (2021) ...",
    "start": 314,
    "end": 337,
    "label": "Format"
  },
  {
    "span": "A broad literature explores mechanisms achieving differential privacy, including Laplace and Gaussian mechanisms (Dwork et al., 2006; Dwork and Roth, 2014), randomized response and local models (Erlingsson et al., 2014; Wang et al., 2017), and privacy accounting via advanced composition and Rényi DP (Kairouz et al., 2017; Mironov, 2017). There are also numerous applications to empirical risk minimization and deep learning (Abadi et al., 2016; Bassily et al., 2019).",
    "document": "Related Work\n\nDifferential privacy (DP) provides a rigorous framework for limiting the influence of any single record on the outcome of a computation. It has seen wide adoption in both centralized and local settings, with growing interest in its use for machine learning and analytics.\n\nA broad literature explores mechanisms achieving differential privacy, including Laplace and Gaussian mechanisms (Dwork et al., 2006; Dwork and Roth, 2014), randomized response and local models (Erlingsson et al., 2014; Wang et al., 2017), and privacy accounting via advanced composition and Rényi DP (Kairouz et al., 2017; Mironov, 2017). There are also numerous applications to empirical risk minimization and deep learning (Abadi et al., 2016; Bassily et al., 2019).\n\nBeyond pure mechanisms, practical deployments must account for utility trade-offs, hyperparameter selection, and auditing. Our work targets adaptive privacy budgeting for iterative learners, providing a controller that balances accuracy and privacy loss in real time with formal guarantees and black-box compatibility.",
    "reason": "The span merely catalogs mechanisms and applications with citations but does not articulate how they relate to the present work, what gap remains, or the authors' stance. This is a lack of synthesis under (a) and (c).",
    "start": 287,
    "end": 756,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In a previous study, the authors claim that spatial attention is sufficient without temporal modeling.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become the dominant paradigm for traffic forecasting by capturing road network topology alongside dynamic flow patterns. Early spatiotemporal models fused graph convolutions with recurrent units, while later architectures introduced attention to emphasize salient sensors and time intervals. In a previous study, the authors claim that spatial attention is sufficient without temporal modeling. This position contrasts with evidence that rush-hour periodicities and event-driven anomalies demand explicit temporal components. Our work revisits this debate by decoupling spatial and temporal operators and analyzing their contributions under varying data regimes and sensor sparsity.",
    "reason": "Mentions a prior study and its claim but provides no citation to identify the study.",
    "start": 340,
    "end": 442,
    "label": "Unsupported_claim"
  },
  {
    "span": "in (Garcia and Lee, 2020)",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become the de facto standard for learning over relational data (Kipf and Welling, 2017; Hamilton et al., 2017). Subsequent work has focused on improving expressivity via higher-order message passing and subgraph reasoning (Ying et al., 2018; Zhao et al., 2021), as well as enhancing scalability with sampling and clustering techniques (Chiang et al., 2019). As shown in (Garcia and Lee, 2020), attention mechanisms can adaptively weight neighborhood information to reduce over-smoothing in deep GNNs. Meanwhile, positional encodings and spectral features offer complementary structural signals that help distinguish nodes in regular graphs (Dwivedi et al., 2021; Kreuzer et al., 2021). More recently, pretraining on large graph corpora has demonstrated promising transfer to downstream tasks (Hu et al., 2020), though negative transfer remains a challenge under distribution shift (You et al., 2020). Our work builds upon these advances by combining attention-based aggregation with structure-aware augmentations designed to preserve class-discriminative motifs during training.\n",
    "reason": "Wrong citation style; the preposition 'in' should not precede a parenthetical citation. Use narrative form 'as shown by Garcia and Lee (2020)' or drop 'in' before a purely parenthetical citation.",
    "start": 415,
    "end": 440,
    "label": "Format"
  },
  {
    "span": "Smith et al. (2019.)",
    "document": "Related Work\n\nRecommendation systems have embraced representation learning to model user-item interactions, with matrix factorization and neural collaborative filtering as prominent approaches (Koren et al., 2009; He et al., 2017). Context-aware recommenders incorporate side information such as temporal signals and social graphs. Smith et al. (2019.) demonstrate that meta-learning can rapidly adapt to new users from few interactions, while maintaining global generalization. However, most meta-recommenders assume stationary user preferences, which rarely hold in dynamic environments.\n",
    "reason": "Punctuation error inside the year in a narrative citation; should be 'Smith et al. (2019)' without the trailing period inside the parentheses.",
    "start": 332,
    "end": 352,
    "label": "Format"
  },
  {
    "span": "Neural approaches formulate acyclicity with continuous constraints, including NOTEARS (Zheng et al., 2018), DAG-GNN (Yu et al., 2019), and GOLEM (Ng et al., 2020).",
    "document": "Introduction\n\nCausal discovery from observational data aims to recover directed acyclic graphs (DAGs) that capture cause-effect relationships. Recent progress leverages continuous relaxations and deep generative models to scale to high-dimensional settings.\n\nRelated Work\n\nNeural approaches formulate acyclicity with continuous constraints, including NOTEARS (Zheng et al., 2018), DAG-GNN (Yu et al., 2019), and GOLEM (Ng et al., 2020). Score-based methods optimize likelihood or independence criteria under DAG constraints (Hyttinen et al., 2013; Lachapelle et al., 2019), while constraint-based algorithms use conditional independence tests (Spirtes et al., 2000). Interventional extensions incorporate do-calculus when experiments are available (Eberhardt, 2008; Hauser and Bühlmann, 2012).\n\nWe introduce a sparsity-adaptive penalty that balances structure fidelity and sample efficiency in small-n, large-p regimes.",
    "reason": "The span enumerates prior techniques without connecting them to the authors’ aims or explaining what shortcomings motivate their proposed method.",
    "start": 273,
    "end": 436,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Dense retrievers encode queries and passages into a shared vector space for efficient nearest-neighbor search (Karpukhin et al., 2020). Sparse methods re-weight lexical matches to improve recall (Zhao et al., 2021). Generative models directly produce answers conditioned on documents (Lewis et al., 2020).",
    "document": "Introduction\n\nOpen-domain question answering (ODQA) systems must identify and integrate evidence from large corpora to produce accurate, grounded answers. Typical architectures decompose the task into retrieval and reading, but recent work explores end-to-end generation over retrieved evidence.\n\nRetrieval-augmented QA typically employs a first-stage retriever that returns candidate passages, followed by a reader that extracts or generates an answer. Training signals can be provided to either or both stages, and negative sampling strategies greatly affect retrieval quality and downstream accuracy.\n\nDense retrievers encode queries and passages into a shared vector space for efficient nearest-neighbor search (Karpukhin et al., 2020). Sparse methods re-weight lexical matches to improve recall (Zhao et al., 2021). Generative models directly produce answers conditioned on documents (Lewis et al., 2020).\n\nIn this work, we revisit the interaction between retriever noise and reader robustness. We introduce a contrastive reranker that jointly learns to score passages and calibrate the reader’s confidence, improving answer grounding while preserving efficiency.",
    "reason": "The span lists dense retrieval, sparse retrieval, and generative readers as isolated statements without transitions or an explicit explanation of how they relate to each other or to the discussion, causing abrupt topic shifts (a, b).",
    "start": 605,
    "end": 910,
    "label": "Coherence"
  },
  {
    "span": "Hybrid DNN-HMM systems dominate noisy conditions (Dahl et al., 2012). End-to-end CTC simplifies decoding (Graves et al., 2006). Speaker adaptation personalizes acoustic models (Yu et al., 2013). MOS evaluates perceived quality (Lopez et al., 2018).",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) approaches include hybrid DNN-HMM pipelines and end-to-end models such as CTC and attention-based encoder-decoders. Self-supervised pretraining has improved acoustic representations and label efficiency across domains (Baevski et al., 2020; Chan et al., 2016).\n\nHybrid DNN-HMM systems dominate noisy conditions (Dahl et al., 2012). End-to-end CTC simplifies decoding (Graves et al., 2006). Speaker adaptation personalizes acoustic models (Yu et al., 2013). MOS evaluates perceived quality (Lopez et al., 2018).\n\nWe target low-resource ASR by combining wav2vec pretraining with a noise-robust CTC loss and lightweight test-time adaptation.",
    "reason": "The span places a TTS-style evaluation metric (MOS) alongside ASR modeling works without transitions or a clear relationship, and each sentence shifts topics abruptly, reducing coherence.",
    "start": 311,
    "end": 559,
    "label": "Coherence"
  },
  {
    "span": "A previous study shows that adding calendar features consistently improves M4 competition performance.",
    "document": "Introduction\n\nAccurate time series forecasting under calendar effects is crucial in retail and energy applications [1]. External regressors such as holidays and promotions can complement autoregressive signals [2]. A previous study shows that adding calendar features consistently improves M4 competition performance. Nevertheless, the magnitude of gains varies by domain and forecast horizon, motivating a more granular analysis.",
    "reason": "References a specific 'previous study' and a well-known competition without citing the source.",
    "start": 215,
    "end": 317,
    "label": "Unsupported_claim"
  },
  {
    "span": "Self-supervised objectives discover structure from large unlabeled corpora (Ishikawa et al., 2019). Vision Transformers scale effectively with data and compute (Park and Zhou, 2021). Multi-scale feature pyramids have been shown to improve detection of small objects (Khandelwal and Ortiz, 2018).",
    "document": "Related Work\n\nObject detection has evolved from handcrafted pipelines to end-to-end deep architectures that localize and classify instances under varied conditions. Modern detectors differ in backbone design, supervision strategy, and multi-scale reasoning.\n\nSelf-supervised objectives discover structure from large unlabeled corpora (Ishikawa et al., 2019). Vision Transformers scale effectively with data and compute (Park and Zhou, 2021). Multi-scale feature pyramids have been shown to improve detection of small objects (Khandelwal and Ortiz, 2018). Recent works further consider efficient training via distillation (Liang et al., 2020), or task-agnostic pretraining for transfer (Chauhan and Lee, 2022). However, it remains unclear how these choices interact when annotation budgets are limited.\n\nWe investigate how pretraining depth and feature pyramid capacity co-vary to affect sample efficiency in open-vocabulary detection settings.",
    "reason": "The three sentences list distinct areas (self-supervision, transformers, feature pyramids) with no connective cues; the relationships among these cited works are not explained, reducing coherence over multiple sentences.",
    "start": 259,
    "end": 554,
    "label": "Coherence"
  },
  {
    "span": "There is a lack of publicly available fairness-aware recommendation datasets.",
    "document": "Related Work\n\nFairness in recommender systems has been conceptualized across multiple stakeholder perspectives, including user fairness, provider fairness, and overall platform equity (Burke, 2017; Ekstrand et al., 2019). Algorithmic approaches range from pre-processing reweighting and debiasing to in-processing constraints and post-processing re-ranking (Zhu et al., 2018; Abdollahpouri et al., 2019; Yao and Huang, 2017). Evaluation frameworks increasingly consider both accuracy and fairness trade-offs, along with exposure and calibration metrics.\n\nThere is a lack of publicly available fairness-aware recommendation datasets. This scarcity complicates reproducible research and hinders meaningful comparisons between methods. To address this gap, we release a curated dataset with demographic proxies and controlled exposure annotations, along with standardized splits and evaluation scripts.",
    "reason": "This is a claim about dataset availability in the field without citing surveys or repositories; it needs evidence or references to support the statement.",
    "start": 555,
    "end": 632,
    "label": "Unsupported_claim"
  },
  {
    "span": "TAC-KBP 2010 Entity Linking",
    "document": "Introduction\n\nEntity linking (EL) connects mentions in text to entries in a knowledge base, enabling downstream reasoning and retrieval. Core challenges include mention ambiguity, acronym expansion, and domain shift. We benchmark on TAC-KBP 2010 Entity Linking to assess performance under noisy, heterogeneous newswire and web text. Our approach combines dense retrieval with span-sensitive disambiguation and a calibration layer to trade off precision and recall across mention types.",
    "reason": "Mentions a specific shared task/dataset without providing a citation at first mention (rule a).",
    "start": 233,
    "end": 260,
    "label": "Unsupported_claim"
  },
  {
    "span": "The CoNLL-2005 shared task on SRL established the standard evaluation setup used today.",
    "document": "Related Work\n\nSemantic role labeling (SRL) assigns predicate-argument structures that capture who did what to whom, when, and where. Early systems combined syntactic parsing with feature-based classifiers, while neural encoders later reduced feature engineering and improved generalization. The CoNLL-2005 shared task on SRL established the standard evaluation setup used today. More recent research explores end-to-end, syntax-agnostic models and multilingual transfer, yet agreement on argument boundaries and role inventories remains a persistent issue.",
    "reason": "Mentions a specific shared task and its impact without any citation at first mention (rule a).",
    "start": 291,
    "end": 378,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior competitions have highlighted the gap between sim and real for manipulation tasks.",
    "document": "Related Work\n\nReinforcement learning for robotic manipulation has advanced rapidly due to improved perception, better exploration strategies, and scalable simulation environments (Levine et al., 2016; Kalashnikov et al., 2018). Despite progress in simulation, deploying learned policies on real robots remains difficult due to modeling errors, sensor noise, and actuation delays.\n\nPrior competitions have highlighted the gap between sim and real for manipulation tasks. These events often emphasize standard task suites and fixed interfaces, yet the benchmarks may still fail to capture the full range of dynamics variations encountered in practice.\n\nOur work contributes a protocol for domain randomization tuned to contact-rich manipulation, alongside a hardware-in-the-loop evaluation harness. We demonstrate improved transfer on peg-in-hole and drawer opening tasks under varying friction coefficients and sensor quantization.",
    "reason": "The sentence cites 'competitions' and a specific conclusion without providing references to those competitions or their reports.",
    "start": 381,
    "end": 469,
    "label": "Unsupported_claim"
  },
  {
    "span": "Brown et al.",
    "document": "Introduction\n\nSentiment Analysis for Noisy User-Generated Text\n\nSentiment analysis has progressed from lexicon-based heuristics (Taboada et al., 2011) to neural models that capture context and compositionality (Socher et al., 2013; Devlin et al., 2019). However, performance often drops on noisy, informal platforms due to misspellings, slang, and domain shift (Go et al., 2009; Eisenstein, 2013). Brown et al. propose a hierarchical encoder that models discourse-level cues to mitigate sentence-level ambiguity, complementing prior work on context windows and user priors (Tang et al., 2015; Hovy et al., 2015). We build on these insights by introducing a multi-view architecture that fuses character, subword, and conversation context signals, while adapting to domain shifts with lightweight finetuning (Gururangan et al., 2020).",
    "reason": "Narrative citation missing year: should be 'Brown et al. (YEAR)'.",
    "start": 398,
    "end": 410,
    "label": "Format"
  },
  {
    "span": "SHAP assigns additive feature attributions under game-theoretic axioms (Lundberg and Lee, 2017). Counterfactual explanations perturb inputs to flip model decisions (Wachter et al., 2018). Clinical decision support requires calibrated uncertainty estimates (Lakshminarayanan et al., 2017). Saliency methods derive gradients to highlight important pixels (Simonyan et al., 2013).",
    "document": "Related Work\n\nExplainable AI (XAI) in healthcare seeks to make model predictions transparent and actionable for clinicians while preserving safety and reliability. Methods span post-hoc attribution, inherently interpretable models, uncertainty quantification, and causal reasoning.\n\nSHAP assigns additive feature attributions under game-theoretic axioms (Lundberg and Lee, 2017). Counterfactual explanations perturb inputs to flip model decisions (Wachter et al., 2018). Clinical decision support requires calibrated uncertainty estimates (Lakshminarayanan et al., 2017). Saliency methods derive gradients to highlight important pixels (Simonyan et al., 2013).\n\nRecent work integrates causal structure to avoid spurious shortcuts (Subbaswamy and Saria, 2020) and develops task-specific interfaces for clinician trust (Tonekaboni et al., 2019). Our framework combines uncertainty-aware counterfactuals with attribution consistency checks tailored to electronic health records, aligning explanations with clinically meaningful variables.",
    "reason": "The span presents four sentences on disparate XAI techniques and uncertainty without transitions or explicit relationships, leaving unclear how each cited work connects to the others.",
    "start": 283,
    "end": 660,
    "label": "Coherence"
  },
  {
    "span": "Nguyen et al. 1",
    "document": "Related Work\n\nPrompt-based learning adapts pretrained language models to downstream tasks with task-specific templates and verbalizers (Schick and Schütze, 2021; Gao et al., 2021). Soft prompts optimize continuous tokens to steer model behavior (Lester et al., 2021), while prefix-tuning conditions the attention mechanism with task vectors (Li and Liang, 2021). Calibration aims to reduce label bias under discrete prompts (Zhao et al., 2021). Nguyen et al. 1 introduce a retrieval-augmented prompting scheme for few-shot relation extraction, whereas concurrent efforts integrate demonstrations to improve stability (Liu et al., 2022; Min et al., 2022). Despite progress, sensitivity to template wording and verbalizer choices remains a key limitation.",
    "reason": "Wrong use of footnotes: 'Nguyen et al. 1' uses a numeric footnote marker instead of including the year or a proper footnote format. It should be 'Nguyen et al. (2021)' or a correctly formatted footnote.",
    "start": 445,
    "end": 460,
    "label": "Format"
  },
  {
    "span": "SQuAD v2.0",
    "document": "Introduction\n\nExtractive question answering (QA) systems have advanced rapidly due to large-scale datasets and pretrained language models. However, real-world QA requires abstention when no answer is present and robustness to domain shift.\n\nUnlike SQuAD v2.0, many domain datasets lack unanswerable questions and provide limited negative examples, leading to overconfident predictions. Transfer learning partially mitigates this issue, but calibration and selective prediction remain open challenges.\n\nWe propose a simple, training-time abstention head that uses margin-based signals to improve selective QA performance without sacrificing accuracy on answerable queries. Experiments cover multiple domains and evaluate both accuracy and risk–coverage trade-offs.",
    "reason": "The dataset 'SQuAD v2.0' is introduced without a citation at first mention, which should be provided per rule a.",
    "start": 248,
    "end": 258,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works demonstrate that entropy minimization consistently outperforms pseudo-labeling on long-tailed text classification.",
    "document": "Introduction\n\nSemi-supervised text classification seeks to leverage abundant unlabeled data to improve performance when labeled data are scarce. Classical approaches rely on consistency regularization and entropy minimization, while more recent methods integrate distribution alignment and class-aware objectives. Recent works demonstrate that entropy minimization consistently outperforms pseudo-labeling on long-tailed text classification. Despite these advances, handling extreme class imbalance remains challenging under domain shift. In this paper, we revisit calibration-aware objectives and propose a reweighting scheme that explicitly accounts for head–tail disparities in the unlabeled pool.",
    "reason": "Claims results from 'recent works' without providing any citations.",
    "start": 314,
    "end": 441,
    "label": "Unsupported_claim"
  },
  {
    "span": "Instrumental variables identify causal effects under exclusion assumptions (Angrist and Imbens, 1995). Equalized odds requires equal false positive and false negative rates across groups (Hardt et al., 2016). Counterfactual fairness defines decisions invariant to latent interventions on protected attributes (Kusner et al., 2017).",
    "document": "Related Work\n\nCausality and Fairness in Machine Learning\n\nFair machine learning leverages causal reasoning to distinguish correlation from discrimination and to design corrective interventions. Post-processing, in-processing, and pre-processing methods impose statistical constraints or reweight training data (Kamiran and Calders, 2012; Hardt et al., 2016; Zafar et al., 2017). Causal approaches define fairness in terms of interventions on protected attributes and mediators (Pearl, 2009; Kusner et al., 2017). Instrumental variables identify causal effects under exclusion assumptions (Angrist and Imbens, 1995). Equalized odds requires equal false positive and false negative rates across groups (Hardt et al., 2016). Counterfactual fairness defines decisions invariant to latent interventions on protected attributes (Kusner et al., 2017). Recent work studies trade-offs between calibration, utility, and fairness under varying base rates (Pleiss et al., 2017; Menon and Williamson, 2018).",
    "reason": "The span juxtaposes instrumental variables, a statistical fairness criterion, and counterfactual fairness without transitions or an explicit statement of how these concepts relate, causing incoherence.",
    "start": 513,
    "end": 844,
    "label": "Coherence"
  },
  {
    "span": "MuST-C and IWSLT are known to contain alignment errors in the provided transcriptions.",
    "document": "Introduction\n\nEnd-to-end speech translation (ST) directly maps source speech to target text, bypassing intermediate ASR and MT components. While this reduces error propagation and simplifies deployment, it typically demands large paired speech–translation corpora.\n\nBenchmark datasets have facilitated progress by enabling common training and evaluation protocols across languages. MuST-C and IWSLT are known to contain alignment errors in the provided transcriptions. Such imperfections can hinder training stability and degrade translation quality, particularly in low-resource settings.\n\nWe propose a noise-robust training framework that combines alignment-aware augmentation with confidence-weighted loss scaling. Our experiments demonstrate improvements on multiple language pairs under both in-domain and out-of-domain conditions.",
    "reason": "Claims dataset quality issues without citing evidence per rule (a)/(b).",
    "start": 382,
    "end": 468,
    "label": "Unsupported_claim"
  },
  {
    "span": "Khan, 2019)",
    "document": "Introduction\n\nData augmentation is a simple yet powerful technique to improve model generalization. In NLP, token-level substitutions and back-translation are common strategies (Sennrich et al., 2016; Kobayashi, 2018). More recent work optimizes augmentations using policy gradients (Ho et al., 2019) or gradient matching (Wu et al., 2021). As shown by Khan, 2019) aggressive augmentations can harm calibration despite improving accuracy. We address this by introducing a calibration-aware selection mechanism.",
    "reason": "Missing opening parenthesis in a parenthetical-style reference; should be (Khan, 2019).",
    "start": 353,
    "end": 364,
    "label": "Format"
  },
  {
    "span": "(Kim et al., 2021;,",
    "document": "Introduction\n\nNeural machine translation (NMT) has evolved from attention-based encoder–decoder models to pre-trained sequence-to-sequence transformers (Bahdanau et al., 2015; Vaswani et al., 2017; Raffel et al., 2020). Handling morphology and rare words has been addressed with subword segmentation and character-aware models (Sennrich et al., 2016; Cherry et al., 2018). Prior work on domain adaptation includes instance weighting and back-translation (Bahdanau et al., 2015; (Kim et al., 2021;, Sennrich et al., 2016), yet stability under rapid domain drift remains underexplored. We propose a curriculum that incrementally exposes the model to domain-shifted data.",
    "reason": "Extraneous semicolon and comma inside the citation; punctuation is malformed.",
    "start": 478,
    "end": 497,
    "label": "Format"
  },
  {
    "span": "Chain-of-thought prompting has been shown to reduce harmful outputs in large language models.",
    "document": "Introduction\n\nReasoning-focused prompting strategies, including few-shot and chain-of-thought variants, have improved performance on multi-step arithmetic and commonsense tasks. At the same time, safety concerns around hallucination, harmful content, and unfair biases have prompted the exploration of alignment techniques that influence a model's intermediate reasoning processes.\n\nChain-of-thought prompting has been shown to reduce harmful outputs in large language models. However, it is unclear whether the observed reductions stem from explicit rationalization, self-consistency sampling, or the longer context provided by intermediate reasoning steps. We investigate these factors via controlled interventions that isolate the effect of rationale exposure and sampling temperature on safety-related metrics.\n\nOur study proposes a rationale filtering mechanism that preserves reasoning benefits while mitigating leakage of sensitive patterns.",
    "reason": "This claim attributes safety improvements to a specific prompting method without any supporting citations to prior empirical studies.",
    "start": 383,
    "end": 476,
    "label": "Unsupported_claim"
  },
  {
    "span": "Graph-based recommenders have recently dominated offline competitions.",
    "document": "Related Work\n\nRecommender systems increasingly leverage graph structures to capture high-order relationships among users, items, and contexts (Wu et al., 2020; He et al., 2020). Contrastive learning and self-supervised objectives further enhance representations under sparse feedback (Chen et al., 2020; Yu et al., 2022). Graph-based recommenders have recently dominated offline competitions. At the same time, questions remain about robustness to popularity bias and cold-start scenarios, motivating debiasing and causal approaches (Zheng et al., 2021; Wang et al., 2021).\n\nOur approach integrates counterfactual augmentation into a graph encoder, improving long-tail coverage without degrading overall precision.",
    "reason": "References 'recently dominated' and 'competitions' without citing specific competitions or results (violates rule d and e-i).",
    "start": 322,
    "end": 392,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Park et. al., 2020)",
    "document": "Related Work\n\nData augmentation has proven essential for improving generalization in low-resource settings (Shorten and Khoshgoftaar, 2019). In NLP, back-translation and noising strategies expand training coverage (Sennrich et al., 2016; Xie et al., 2020). Mixup-style interpolation in embedding space further regularizes classifiers (Zhang et al., 2018; Guo et al., 2019).\n\nParaphrase generation with pretrained sequence-to-sequence models yields diverse semantic variants (Witteveen and Andrews, 2019; Kumar et al., 2020). For intent classification, semantic consistency constraints maintain label fidelity during augmentation (Jia and Liang, 2017; Lee et al., 2021). We adopt a syntax-guided paraphraser similar to prior work (Park et. al., 2020) but incorporate a confidence-aware filter to prune low-quality variants.\n\nEmpirically, our approach yields larger gains when combined with curriculum learning that schedules augmented examples by difficulty.",
    "reason": "Incorrect abbreviation “et. al.”; the correct form is “et al.” without a period after “et”.",
    "start": 729,
    "end": 749,
    "label": "Format"
  },
  {
    "span": "Neural program synthesis leverages large language models to generate code from natural language or I/O examples (Chen et al., 2021; Austin et al., 2021). Constraint-based synthesis derives programs from logical specifications using SMT and CEGIS (Solar-Lezama, 2008). Type-directed repair proposes edits to satisfy typing judgments (Guha et al., 2010).",
    "document": "Related Work\n\nProgram synthesis spans data-driven generation, formal methods, and hybrid techniques that trade off expressivity and verifiability. Recent advances in code large language models (LLMs) have broadened applicability, while formal guarantees remain challenging at scale.\n\nVerification and repair complement synthesis by checking properties and proposing minimal edits when constraints are violated. Combining these paradigms can improve reliability without sacrificing coverage.\n\nNeural program synthesis leverages large language models to generate code from natural language or I/O examples (Chen et al., 2021; Austin et al., 2021). Constraint-based synthesis derives programs from logical specifications using SMT and CEGIS (Solar-Lezama, 2008). Type-directed repair proposes edits to satisfy typing judgments (Guha et al., 2010).\n\nOur work integrates test-driven prompting with lightweight constraint checks to prune erroneous generations online. We evaluate on competitive programming tasks with hidden tests and adversarial edge cases.",
    "reason": "The three sentences introduce separate areas (neural synthesis, constraint-based synthesis, type-directed repair) without transitions or explicit connections, producing an abrupt sequence lacking coherence (a, b).",
    "start": 492,
    "end": 844,
    "label": "Coherence"
  },
  {
    "span": "Prompting techniques include zero- and few-shot in-context learning (Brown et al., 2020), prompt tuning and prefix tuning (Lester et al., 2021; Li and Liang, 2021), chain-of-thought prompting (Wei et al., 2022), and retrieval-augmented prompting (Lewis et al., 2020; Borgeaud et al., 2022). Instruction tuning and alignment further shape model responses (Sanh et al., 2022; Ouyang et al., 2022).",
    "document": "Introduction Large language models (LLMs) demonstrate strong generalization through appropriate prompts, yet their behavior remains sensitive to phrasing and context. Robust prompt design is especially challenging in domains with scarce domain-specific exemplars and dynamic knowledge. Prompting techniques include zero- and few-shot in-context learning (Brown et al., 2020), prompt tuning and prefix tuning (Lester et al., 2021; Li and Liang, 2021), chain-of-thought prompting (Wei et al., 2022), and retrieval-augmented prompting (Lewis et al., 2020; Borgeaud et al., 2022). Instruction tuning and alignment further shape model responses (Sanh et al., 2022; Ouyang et al., 2022). We introduce a constraint-aware prompt compiler that uses a small validation set to synthesize prompts satisfying domain constraints via discrete search guided by a verifier. Our method improves factuality and calibration on biomedical QA and compliance tasks.",
    "reason": "The span enumerates prompting and alignment methods without linking them to the paper's constraint-aware prompt compiler or stating what gap persists. No author perspective or synthesis is provided (a, c).",
    "start": 286,
    "end": 681,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Matrix factorization captures user-item interactions from implicit feedback (Hu et al., 2008). Graph-based recommenders propagate signals over user-item graphs (Wang et al., 2019). Session-based models use RNNs for short-term preferences (Hidasi et al., 2016). Differential privacy protects user data (McMahan et al., 2018).",
    "document": "Related Work\n\nPersonalized recommendation has progressed through representation learning and graph modeling, with growing emphasis on privacy and robustness. Methods vary widely in supervision, temporal modeling, and auxiliary information usage.\n\nMatrix factorization captures user-item interactions from implicit feedback (Hu et al., 2008). Graph-based recommenders propagate signals over user-item graphs (Wang et al., 2019). Session-based models use RNNs for short-term preferences (Hidasi et al., 2016). Differential privacy protects user data (McMahan et al., 2018).\n\nOur contribution integrates long- and short-term preference modeling under exposure bias correction, and we analyze privacy-utility trade-offs separately from ranking accuracy.",
    "reason": "The sequence lists four areas without articulating how they relate or build on each other; transitions are missing and the connections between the cited domains are implicit rather than explicit.",
    "start": 247,
    "end": 571,
    "label": "Coherence"
  },
  {
    "span": "Explanation generation for recommenders has been studied using templates, attention, and graph-based rationales (Zhang et al., 2014; Chen et al., 2019; Wang et al., 2020).",
    "document": "Related Work\n\nExplainable recommendation seeks to increase transparency, user trust, and calibration by providing reasons for item suggestions. Early systems relied on rule-based justifications tied to user-item attributes or knowledge graphs.\n\nModel-driven explanations. Explanation generation for recommenders has been studied using templates, attention, and graph-based rationales (Zhang et al., 2014; Chen et al., 2019; Wang et al., 2020). Recent neural approaches condition natural language rationales on user histories and item content, while counterfactual explanations identify minimal changes to alter recommendations (Li et al., 2021; Mothilal et al., 2020).\n\nWe consider explanations that are consistent with ranking behavior under distribution shift, emphasizing faithfulness metrics that penalize post-hoc rationales which do not causally impact the recommender’s decisions.",
    "reason": "The span lists categories and citations but does not connect them to the authors’ problem setting or highlight limitations motivating the proposed focus on faithfulness under shift, demonstrating a lack of synthesis.",
    "start": 272,
    "end": 443,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent advances in self-supervised speech representation learning include contrastive predictive coding for audio, masked prediction objectives over latent units, and iterative clustering to refine targets (van den Oord et al., 2018; Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2021).",
    "document": "Related Work\n\nSelf-supervised speech learning. Self-supervised learning (SSL) for speech has enabled strong performance without extensive transcriptions by leveraging large amounts of unlabeled audio. Techniques span contrastive learning, masked prediction, and teacher-student distillation. Recent advances in self-supervised speech representation learning include contrastive predictive coding for audio, masked prediction objectives over latent units, and iterative clustering to refine targets (van den Oord et al., 2018; Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2021). Prior work demonstrates that pre-trained speech encoders transfer to downstream tasks such as ASR, speaker ID, and keyword spotting.\n\nDomain adaptation for speech SSL. Cross-domain transfer remains challenging due to channel shifts and phonetic mismatches. Methods for domain adaptation include feature normalization, adversarial alignment, and continued pretraining on target-domain audio. However, these strategies may still degrade when labeled target data is scarce.\n\nIn this paper, we introduce a lightweight adaptation layer trained with a small amount of paired and abundant unpaired target-domain audio to stabilize transfer without full model finetuning.\n",
    "reason": "The span lists prior methods and citations without explaining how they relate to the paper’s focus or identifying a gap; it does not articulate the authors’ perspective or connect these works to the proposed adaptation approach.",
    "start": 292,
    "end": 585,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Personalization in federated learning has been pursued via meta-learning (Fallah et al., 2020; Jiang et al., 2019), clustered aggregation (Sattler et al., 2020; Briggs et al., 2020), personalized layers or adapters (Arivazhagan et al., 2019; Collins et al., 2021), and data interpolation or proximal terms to handle heterogeneity (Li et al., 2020; Karimireddy et al., 2020). In this paper, we propose a personalization strategy that augments local objectives with a regularizer and a small cache of cross-client prototypes.",
    "document": "Introduction\n\nFederated learning (FL) enables on-device training without centralizing raw data, but client heterogeneity often leads to suboptimal global models. Personalization techniques aim to tailor models to client-specific distributions while retaining the communication and privacy benefits of FL.\n\nPersonalization in federated learning has been pursued via meta-learning (Fallah et al., 2020; Jiang et al., 2019), clustered aggregation (Sattler et al., 2020; Briggs et al., 2020), personalized layers or adapters (Arivazhagan et al., 2019; Collins et al., 2021), and data interpolation or proximal terms to handle heterogeneity (Li et al., 2020; Karimireddy et al., 2020). In this paper, we propose a personalization strategy that augments local objectives with a regularizer and a small cache of cross-client prototypes.\n\nWe evaluate our approach on vision and text benchmarks with significant label shift, reporting improvements in accuracy and calibration compared to strong baselines.",
    "reason": "The span jumps from listing prior methods to stating the paper’s contribution without articulating the specific gap or limitation in existing approaches that the contribution addresses; the connection is implicit and unsynthesized.",
    "start": 306,
    "end": 829,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Ying et al. (2019) proposed GNNExplainer to identify predictive subgraphs and features. Ribeiro et al. (2016) introduced LIME for post-hoc instance-level explanations. Yuan et al. (2020) leveraged information-theoretic constraints for faithful graph explanations.",
    "document": "Related Work\n\nExplainability for graph neural networks (GNNs) seeks to attribute predictions to substructures, features, or training signals in a way that is faithful and stable. Recent efforts span perturbation-based attribution, optimization of masks over nodes and edges, and counterfactual reasoning.\n\nYing et al. (2019) proposed GNNExplainer to identify predictive subgraphs and features. Ribeiro et al. (2016) introduced LIME for post-hoc instance-level explanations. Yuan et al. (2020) leveraged information-theoretic constraints for faithful graph explanations.\n\nParallel lines of work analyze explanation robustness, evaluation metrics, and human factors in interpretability studies for graphs (Agarwal et al., 2022; Lucic et al., 2022). We focus on faithfulness under distribution shifts and propose standardized stress tests for GNN explanation methods.",
    "reason": "The cited works are presented in sequence without establishing their relationships; the jump from GNN-specific methods to a general tabular/text method (LIME) lacks an explanation of relevance or linkage.",
    "start": 306,
    "end": 569,
    "label": "Coherence"
  },
  {
    "span": "In (Marquez et al., 2018)",
    "document": "Introduction\n\nDomain adaptation in natural language processing aims to transfer knowledge from a labeled source domain to an unlabeled or sparsely labeled target domain (Ben-David et al., 2010; Ganin and Lempitsky, 2015; Tzeng et al., 2017). Recent studies have emphasized representation learning techniques that minimize distributional shifts while preserving task-relevant features (Long et al., 2015; Bousmalis et al., 2016; Chen et al., 2020).\n\nIn (Marquez et al., 2018), the authors propose an adversarial alignment strategy for cross-domain sequence tagging, reporting gains under limited supervision. However, their approach relies on task-specific discriminator heads that may not generalize across labeling schemes. Subsequent work explores contrastive objectives and pseudo-labeling to stabilize training (Saito et al., 2018; Zhang et al., 2019).\n\nOur work differs by introducing a unified objective that ties domain-invariant features to calibrated uncertainty estimates, enabling more robust transfer without domain-specific hyperparameter tuning (Roth and Small, 2010; Shu et al., 2018).",
    "reason": "Wrong citation style: a preposition directly precedes a parenthetical citation. Narrative form should be 'In Marquez et al. (2018)' rather than 'In (Marquez et al., 2018)'.",
    "start": 449,
    "end": 474,
    "label": "Format"
  },
  {
    "span": "(Garcia et al. 2017)",
    "document": "Related Work\n\nSelf-supervised representation learning has reduced reliance on manual labels across modalities (Doersch et al., 2015; Chen et al., 2020). In vision, contrastive objectives align differently augmented views of the same instance while repelling others.\n\nArchitectures and augmentations have co-evolved, with momentum encoders and large batch training driving strong results (He et al., 2020; Grill et al., 2020). Follow-up analyses dissect the role of invariances and feature collapse (Garcia et al. 2017) and propose alternative pretext tasks beyond contrastive learning (Caron et al., 2021).\n\nOur work revisits view sampling and proposes curriculum augmentations that improve transfer under distribution shift.",
    "reason": "Missing comma between author and year in a parenthetical citation; it should be '(Garcia et al., 2017)'.",
    "start": 498,
    "end": 518,
    "label": "Format"
  },
  {
    "span": "Prior shared tasks have established entity normalization as a bottleneck in biomedical IE.",
    "document": "Related Work\n\nBiomedical information extraction (IE) spans named entity recognition, relation extraction, and normalization to controlled vocabularies. While recognition and linking are often studied jointly, performance disparities persist across sub-tasks and domains. Prior shared tasks have established entity normalization as a bottleneck in biomedical IE. Subsequent work has explored dictionary expansion, ontology-aware embeddings, and cross-species transfer, yet linking errors continue to propagate to downstream curation and knowledge graph construction. Our approach addresses this by integrating disambiguation signals from textual definitions and hierarchical relations.",
    "reason": "References 'prior shared tasks' and their findings without citing the specific tasks, which should be cited at first mention.",
    "start": 271,
    "end": 361,
    "label": "Unsupported_claim"
  },
  {
    "span": "Privacy-preserving aggregation has been explored through secure multiparty computation, homomorphic encryption, and trusted execution environments (Bonawitz et al., 2017; Goryczka and Xiong, 2015; Acar et al., 2018; Hunt et al., 2018). Differential privacy at both client and server sides has also been extensively studied (Abadi et al., 2016; Geyer et al., 2017; McMahan et al., 2018; Kairouz et al., 2021), alongside defenses against gradient leakage and membership inference (Zhu et al., 2019; Nasr et al., 2019).",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative model training over decentralized data without centralizing raw examples. Early work formalized client-server orchestration and communication-efficient updates to mitigate bandwidth and straggler effects (McMahan et al., 2017; Konečný et al., 2016). Subsequent studies investigated robustness, personalization, and fairness under heterogeneous client distributions (Li et al., 2020; Karimireddy et al., 2020; Pillutla et al., 2022).\n\nPrivacy-preserving aggregation has been explored through secure multiparty computation, homomorphic encryption, and trusted execution environments (Bonawitz et al., 2017; Goryczka and Xiong, 2015; Acar et al., 2018; Hunt et al., 2018). Differential privacy at both client and server sides has also been extensively studied (Abadi et al., 2016; Geyer et al., 2017; McMahan et al., 2018; Kairouz et al., 2021), alongside defenses against gradient leakage and membership inference (Zhu et al., 2019; Nasr et al., 2019).\n\nCommunication reduction has been addressed via sparsification, quantization, and sketching (Aji and Heafield, 2017; Alistarh et al., 2017; Suresh et al., 2017). Personalization methods include meta-learning, multi-task optimization, and mixture models to align global objectives with local preferences (Fallah et al., 2020; Dinh et al., 2020; Mansour et al., 2020).\n\nNotwithstanding these developments, privacy-utility trade-offs in FL remain sensitive to client heterogeneity and participation dynamics. Prior defenses often target static participation or homogeneous noise regimes, whereas practical deployments experience bursty availability, varying data difficulty, and shifting adversarial behavior. In this work, we aim to calibrate privacy noise adaptively to participation and difficulty signals, maintaining utility while meeting formal guarantees.",
    "reason": "The span catalogs prior privacy techniques and citations without explaining how they relate to the paper’s approach or articulating the gap motivating the current study.",
    "start": 494,
    "end": 1010,
    "label": "Lacks_synthesis"
  },
  {
    "span": "A seminal dataset for Arabic Twitter sentiment was introduced with fine-grained labels.",
    "document": "Related Work\n\nSentiment analysis on Arabic social media presents linguistic challenges, including dialectal variation, orthographic ambiguity, and code-switching (Abdul-Mageed et al., 2014; Mubarak et al., 2020). Prior efforts have curated labeled corpora for tweets, reviews, and forums to enable model development across dialects. A seminal dataset for Arabic Twitter sentiment was introduced with fine-grained labels. Subsequent work expanded coverage to multiple dialects and introduced label refinement strategies such as aspect-level annotation and intensity scales. Our study contributes a balanced, dialect-aware corpus with consistent label definitions and robust splits for cross-dialect generalization.\n\nWe additionally analyze error patterns related to emoji usage and named entities common in regional discourse.",
    "reason": "It mentions a 'seminal dataset' and its properties without citing the dataset or the paper that introduced it.",
    "start": 333,
    "end": 420,
    "label": "Unsupported_claim"
  },
  {
    "span": "A wide range of graph neural architectures have been applied to molecules, including convolutional variants that aggregate neighborhood features (Kipf and Welling, 2017; Duvenaud et al., 2015), message passing networks that exchange edge-conditioned signals (Gilmer et al., 2017; Yang et al., 2019), and attention-based methods that weigh atoms and bonds through learned coefficients (Veličković et al., 2018; Hu et al., 2020). Several studies also incorporate 3D geometry via distance encodings or equivariant layers (Schütt et al., 2017; Klicpera et al., 2020).",
    "document": "Introduction\n\nMolecular property prediction underpins virtual screening, lead optimization, and toxicity assessment. Representing molecules as graphs of atoms and bonds has enabled learning systems that directly operate on chemical structure without hand-crafted descriptors. Despite progress, capturing long-range interactions and chemical contexts that arise from reaction environments remains challenging.\n\nA wide range of graph neural architectures have been applied to molecules, including convolutional variants that aggregate neighborhood features (Kipf and Welling, 2017; Duvenaud et al., 2015), message passing networks that exchange edge-conditioned signals (Gilmer et al., 2017; Yang et al., 2019), and attention-based methods that weigh atoms and bonds through learned coefficients (Veličković et al., 2018; Hu et al., 2020). Several studies also incorporate 3D geometry via distance encodings or equivariant layers (Schütt et al., 2017; Klicpera et al., 2020).\n\nIn this work, we study how reaction contexts influence learned representations for downstream prediction. We introduce Reaction-Aware GNNs that condition message passing on predicted reaction centers and local electronic environments derived from weak supervision. We evaluate our approach across property benchmarks and ablations that manipulate reaction cues.",
    "reason": "The span lists prior GNN variants and 3D extensions without connecting them to the paper's objective or stating how they motivate or contrast with the proposed model, thereby lacking synthesis under (a) and (c).",
    "start": 410,
    "end": 973,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Vatswani et al., 2019)",
    "document": "Introduction\n\nWe investigate semi-supervised approaches to domain adaptation. In (Vatswani et al., 2019) a teacher-student pipeline is proposed for efficient labeling, but our setting differs in that we assume zero in-domain labels. Similar consistency regularization has been applied in vision (Sohn et al., 2020) and speech (Park et al., 2020). We evaluate on three benchmarks and report gains over self-training baselines.",
    "reason": "Wrong citation style: parenthetical citation used where a narrative citation is required. It should be: In Vatswani et al. (2019), ...",
    "start": 78,
    "end": 104,
    "label": "Format"
  },
  {
    "span": "(Chen et. al., 2022)",
    "document": "Related Work\n\nTask-oriented dialog systems integrate state tracking, policy learning, and natural language generation (Young et al., 2013; Wen et al., 2017). Pretrained language models have improved data efficiency but still require robust schema alignment for new domains (Rastogi et al., 2020; Peng et al., 2021). A comprehensive survey (Chen et. al., 2022) highlights the gap between simulation and real-user evaluation, motivating more reliable interactive metrics. Our method targets this gap by introducing on-policy evaluation with human preference modeling.",
    "reason": "Incorrect abbreviation “et. al.”; should be “et al.” in the citation.",
    "start": 339,
    "end": 359,
    "label": "Format"
  },
  {
    "span": "Vatswani et al.",
    "document": "Related Work\n\nSemi-supervised learning has a rich history that spans graph-based label propagation, entropy minimization, and consistency regularization (Zhu, 2005; Chapelle et al., 2006; Sajjadi et al., 2016). Recent work emphasizes perturbation-invariant training signals that stabilize learning under distribution shift (Tarvainen and Valpola, 2017; Berthelot et al., 2019). In text classification, pseudo-labeling has been shown to benefit low-resource regimes when combined with strong data augmentation (Xie et al., 2020; Chen et al., 2020). However, prior studies rarely disentangle the role of calibration from representation quality, leading to confounded improvements reported across benchmarks.\n\nClosely related to our approach is the line of research on teacher–student paradigms and confidence-based selection. Vatswani et al. propose a curriculum over unlabeled data that prioritizes examples with high epistemic uncertainty. While this aligns conceptually with our method, we focus on separating aleatoric and epistemic sources through disagreement-aware ensembling. Other concurrent methods develop robust consistency training under heavy noise (Miyato et al., 2019) and leverage contrastive objectives to align latent neighborhoods (He et al., 2020). Our work complements these by providing a calibration-centric evaluation protocol across diverse domains.",
    "reason": "Narrative citation is missing the publication year; it should be formatted as a narrative citation like \"Vatswani et al. (2020)\" rather than just the author string.",
    "start": 824,
    "end": 839,
    "label": "Format"
  },
  {
    "span": "ETT benchmark",
    "document": "Related Work\n\nLong-horizon time-series forecasting (LTSF) has attracted significant attention with the advent of transformer-style architectures that claim superior scalability over recurrent baselines. The ETT benchmark has become a de facto testbed for evaluating seasonal–trend decomposition and attention mechanisms under varying input resolutions. Additional datasets such as Exchange, Traffic, and Weather offer complementary characteristics for assessing robustness. Nevertheless, recent debates highlight the importance of strong statistical baselines and careful experimental controls.",
    "reason": "Refers to a specific benchmark without providing a citation to its introduction or dataset description (rule a).",
    "start": 207,
    "end": 220,
    "label": "Unsupported_claim"
  },
  {
    "span": "Neural program synthesis has explored sequence-to-sequence mappings from natural language to code (Iyer et al., 2018; Yin and Neubig, 2017), grammar-constrained decoders that enforce syntax (Krishnamurthy et al., 2017), execution-guided decoding to prune infeasible candidates (Chen et al., 2019), and retrieval-augmented generation that leverages code corpora (Zhang et al., 2022). Benchmarks such as CONCODE, CoNaLa, and Spider test code generation under varying schema and domain shifts (Iyer et al., 2018; Yin et al., 2018; Yu et al., 2018). Our work builds on these advances.",
    "document": "Introduction\n\nMapping natural language to executable programs enables accessible software creation and automated data manipulation. The field spans semantic parsing, neural code generation, and hybrid symbolic-neural methods.\n\nNeural program synthesis has explored sequence-to-sequence mappings from natural language to code (Iyer et al., 2018; Yin and Neubig, 2017), grammar-constrained decoders that enforce syntax (Krishnamurthy et al., 2017), execution-guided decoding to prune infeasible candidates (Chen et al., 2019), and retrieval-augmented generation that leverages code corpora (Zhang et al., 2022). Benchmarks such as CONCODE, CoNaLa, and Spider test code generation under varying schema and domain shifts (Iyer et al., 2018; Yin et al., 2018; Yu et al., 2018). Our work builds on these advances.\n\nWe present a decoder that conditions on inferred variable roles, type constraints, and partial execution traces.\n",
    "reason": "The span summarizes prior work and immediately states that the present work builds on it without identifying a concrete gap or explaining how the contributions address unmet needs (criterion b).",
    "start": 227,
    "end": 807,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Miller et al.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a standard tool for representation learning on relational data. Early message passing frameworks (Gilmer et al., 2017) motivated simplified architectures such as GraphSAGE (Hamilton et al., 2017) and GAT (Veličković et al., 2018). Miller et al. show that deeper GNNs suffer from over-smoothing, prompting research on residual and normalization techniques (Li et al., 2019; Chen et al., 2020). Our approach builds upon layer-wise normalization strategies to improve stability on large, heterogeneous graphs.",
    "reason": "Narrative citation is missing the year. It should appear as “Miller et al. (YEAR)” or be rephrased as a parenthetical citation.",
    "start": 286,
    "end": 299,
    "label": "Format"
  },
  {
    "span": "Contrastive pretraining with paired image-text data has been shown effective for broad visual recognition and downstream VQA (Radford et al., 2021; Jia et al., 2021). Multi-modal fusion encoders integrate region features and language tokens with cross-attention (Lu et al., 2019; Chen et al., 2020; Li et al., 2020).",
    "document": "Related Work\n\nVision-language pretraining has catalyzed progress on VQA, captioning, and retrieval. Methods vary in how they align modalities and transfer learned representations to downstream tasks.\n\nContrastive pretraining with paired image-text data has been shown effective for broad visual recognition and downstream VQA (Radford et al., 2021; Jia et al., 2021). Multi-modal fusion encoders integrate region features and language tokens with cross-attention (Lu et al., 2019; Chen et al., 2020; Li et al., 2020).\n\nDespite strong performance, domain shifts in scientific diagrams and charts pose reasoning challenges. We investigate structure-aware alignment for diagram-centric VQA.",
    "reason": "Summarizes prior approaches without connecting them to the paper’s method or highlighting a concrete limitation in that span (criteria a and b).",
    "start": 201,
    "end": 517,
    "label": "Lacks_synthesis"
  },
  {
    "span": "there are many recent works that explore prompt-based reinforcement learning in dialogue systems",
    "document": "Related Work\n\nReinforcement learning (RL) has been applied to dialogue systems to optimize long-horizon objectives such as task success and user satisfaction. Traditional approaches rely on policy gradients and value-based learning to fine-tune response policies after supervised pretraining. Recently, prompt engineering has emerged as a means to control large language models during interactive decision-making; however, there are many recent works that explore prompt-based reinforcement learning in dialogue systems. While offline RL and batch-constrained methods have improved stability, integrating prompts as part of the action space remains underexplored.\n\nOur study connects supervised prompt tuning with on-policy RL updates, contrasting reward shaping with preference-based optimization over multi-turn dialogues.",
    "reason": "Vague reference to 'many recent works' without any citations violates the requirement to back claims of recent literature (rule d).",
    "start": 423,
    "end": 519,
    "label": "Unsupported_claim"
  },
  {
    "span": "In robot manipulation, model-free methods such as DDPG, SAC, and PPO (Lillicrap et al., 2016; Haarnoja et al., 2018; Schulman et al., 2017) and model-based algorithms (Chua et al., 2018; Schrittwieser et al., 2020) have achieved strong results in simulation and on hardware.",
    "document": "Related Work\n\nReinforcement learning for manipulation. Learning continuous control policies for contact-rich tasks has been advanced by deep reinforcement learning. In robot manipulation, model-free methods such as DDPG, SAC, and PPO (Lillicrap et al., 2016; Haarnoja et al., 2018; Schulman et al., 2017) and model-based algorithms (Chua et al., 2018; Schrittwieser et al., 2020) have achieved strong results in simulation and on hardware. Auxiliary strategies like demonstrations, curriculum learning, and domain randomization further enhance sample efficiency and transfer (Hester et al., 2018; Andrychowicz et al., 2020).\n\nPerception and representation. End-to-end visuomotor policies benefit from structured representations and keypoint-based encodings, while 3D features and point-cloud networks improve generalization across object instances (Kulkarni et al., 2019; Qi et al., 2017; Wu et al., 2020).\n\nGeneralization and safety. Work on policy generalization across tasks and environments, as well as safety-critical control with constraint satisfaction, is ongoing (Pack Kaelbling and Lozano-Perez, 2017; Dalal et al., 2018).\n",
    "reason": "The span aggregates prior methods and claims of success without linking to the specific research problem, limitations, or the authors' motivation, thus lacking synthesis (criteria a and c).",
    "start": 165,
    "end": 439,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The MMSD-100K dataset has become the de facto benchmark for multimodal sarcasm.",
    "document": "Related Work and Datasets\n\nMultimodal sarcasm detection integrates textual, visual, and occasionally acoustic cues to determine whether an utterance expresses sarcasm. Earlier research concentrated on textual indicators such as polarity shifts and incongruity, while recent approaches leverage image-text alignment and cross-attention to capture multimodal cues. Benchmarking progress in this area crucially depends on datasets that reflect naturalistic distributions of topics, platforms, and user intents.\n\nThe MMSD-100K dataset has become the de facto benchmark for multimodal sarcasm. Its scale and diversity purportedly enable models to generalize across topics and visual styles, and many leaderboard-style comparisons now default to this corpus. Nevertheless, questions remain about annotation quality, inter-rater agreement, and demographic coverage. We therefore complement our experiments with additional in-domain and out-of-domain evaluations to assess robustness.\n\nIn this work, we compare contrastive pretraining, fusion-then-attend, and attend-then-fuse architectures under consistent training budgets and report error analyses that highlight common failure modes such as literal caption bias and background confounds.",
    "reason": "The statement declares a specific dataset as the 'de facto benchmark' without citing any sources, dataset papers, or leaderboards to substantiate the claim.",
    "start": 509,
    "end": 588,
    "label": "Unsupported_claim"
  },
  {
    "span": "Message passing neural networks aggregate neighbor features with learned transformations (Gilmer et al., 2017). Scaffold splits are commonly used to test generalization (Ramakrishnan et al., 2014). Pretraining on unlabeled molecules can improve downstream tasks (Hu et al., 2020).",
    "document": "Related Work\n\nGraph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto approach for molecular property prediction by treating molecules as graphs of atoms and bonds. Early architectures use spectral filters or spatial message passing to aggregate neighborhood information (Defferrard et al., 2016; Kipf and Welling, 2017; Gilmer et al., 2017). Improvements include edge-conditioned convolutions, attention, and graph-level pooling (Schlichtkrull et al., 2018; Veličković et al., 2018; Ying et al., 2018). Message passing neural networks aggregate neighbor features with learned transformations (Gilmer et al., 2017). Scaffold splits are commonly used to test generalization (Ramakrishnan et al., 2014). Pretraining on unlabeled molecules can improve downstream tasks (Hu et al., 2020). Domain-specific augmentations and 3D information further boost performance (Anderson et al., 2019; Stärk et al., 2022).",
    "reason": "The span moves from an architectural description to a dataset splitting protocol and then to pretraining, without transitions or an explicit relationship tying these elements together.",
    "start": 559,
    "end": 839,
    "label": "Coherence"
  },
  {
    "span": "BERT has been successfully applied to AES on the ASAP dataset",
    "document": "Related Work\n\nAutomatic Essay Scoring (AES) has progressed from handcrafted features and linear models to neural architectures that capture discourse structure and semantic adequacy (Shermis and Burstein, 2013; Taghipour and Ng, 2016; Dong et al., 2017). Pretrained language models have further improved AES by leveraging large corpora for contextualization, yielding gains in both inter-prompt and cross-domain generalization (Ushio et al., 2021; Mayfield and Black, 2020). More recently, BERT has been successfully applied to AES on the ASAP dataset, demonstrating strong predictive performance and robustness to adversarial perturbations.\n\nDespite these advances, questions remain regarding rubric alignment, fairness across demographic subpopulations, and calibration of prediction uncertainty (Ramesh and Saha, 2021; Madnani and Cahill, 2021). We address these challenges by proposing a rubric-aware objective that disentangles content coverage from writing mechanics, accompanied by a comprehensive error analysis.",
    "reason": "Specific mention of a model-dataset setup ('BERT' on 'ASAP') is a claim about prior work that requires a citation at first mention, per rules (a) and (e)(iii).",
    "start": 490,
    "end": 551,
    "label": "Unsupported_claim"
  },
  {
    "span": "Klein et al., 2020)",
    "document": "Introduction\n\nNeural machine translation (NMT) has seen rapid improvements in both quality and speed. While accuracy has benefited from larger models (Ott et al., 2018) and better optimization (Popel and Bojar, 2018), deployment requires toolchains that streamline training and inference. Recent toolkits Klein et al., 2020) streamline experimentation through modular components, yet hardware-aware optimizations remain underexplored. We investigate compiler-level graph fusion to reduce latency without sacrificing BLEU.",
    "reason": "Missing opening parenthesis for a parenthetical citation; should be “(Klein et al., 2020)”.",
    "start": 305,
    "end": 324,
    "label": "Format"
  },
  {
    "span": "A large body of work has studied privacy in federated learning through secure aggregation (Bonawitz et al., 2017; Bell et al., 2020), differential privacy at the client or server side (Geyer et al., 2018; McMahan et al., 2018; Kairouz et al., 2021), and encryption-based protocols (Truex et al., 2019; So et al., 2020). These methods propose cryptographic protocols, noise-addition mechanisms, and secure enclaves to protect updates during training.",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative model training across a population of devices while keeping raw data localized (McMahan et al., 2017). By aggregating model updates instead of collecting data, FL promises to reduce centralization risks and comply with emerging data protection regulations. However, the distributed nature of optimization, heterogeneous clients, and intermittent connectivity introduce unique challenges for utility, scalability, and privacy.\n\nA large body of work has studied privacy in federated learning through secure aggregation (Bonawitz et al., 2017; Bell et al., 2020), differential privacy at the client or server side (Geyer et al., 2018; McMahan et al., 2018; Kairouz et al., 2021), and encryption-based protocols (Truex et al., 2019; So et al., 2020). These methods propose cryptographic protocols, noise-addition mechanisms, and secure enclaves to protect updates during training.\n\nBeyond privacy, communication efficiency and personalization have received considerable attention, with compression and sparsification techniques reducing upload costs (Konečný et al., 2016; Sattler et al., 2019) and personalized objectives aligning global models to local distributions (Smith et al., 2017; Fallah et al., 2020). Nevertheless, achieving strong privacy guarantees without sacrificing model utility remains an open question in many deployments.\n\nIn this paper, we present FedShield, a framework that combines structured noise with adaptive update clipping to preserve accuracy while achieving rigorous privacy accounting in the presence of skewed client participation. We theoretically analyze privacy loss under partial participation and empirically evaluate on image and language benchmarks to demonstrate state-of-the-art utility at fixed privacy budgets.",
    "reason": "The span lists categories of prior privacy methods and cites multiple works without relating them to the paper's aims, articulating a gap, or explaining how the present work differs or builds upon them. It lacks synthesis under criteria (a) and (c).",
    "start": 487,
    "end": 936,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Smith et al., 2021)",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a standard tool for semi-supervised node classification, link prediction, and graph clustering (Kipf and Welling, 2017; Hamilton et al., 2017). Attention-based message passing further improves expressivity and stability under heterophily, as shown by Smith et al., 2021) on citation networks with diverse degree distributions.\n\nHowever, oversmoothing and oversquashing remain key limitations (Li et al., 2018; Alon and Yahav, 2021). Recent approaches introduce spectral regularization and topology-aware rewiring to mitigate these effects while preserving scalability (Topping et al., 2022; Huang et al., 2023).",
    "reason": "Missing opening parenthesis for a parenthetical citation; should be '(Smith et al., 2021)'.",
    "start": 306,
    "end": 325,
    "label": "Format"
  },
  {
    "span": "(Nguyen et. al., 2017)",
    "document": "Related Work\n\nNeural abstractive summarization. Sequence-to-sequence models with attention laid the groundwork for conditional generation (Rush et al., 2015; See et al., 2017). Pointer-generator networks mitigate copying limitations by combining extraction with generation (See et al., 2017; Paulus et al., 2018). More recent transformer-based models leverage large-scale pretraining and task-specific fine-tuning (Lewis et al., 2020; Zhang et al., 2020). For discourse-aware objectives, (Nguyen et. al., 2017) explore hierarchical encoders that track entity transitions across sentences.",
    "reason": "Incorrect abbreviation \"et. al.\" The correct form is \"et al.\" without a period after \"et\": \"(Nguyen et al., 2017)\".",
    "start": 488,
    "end": 510,
    "label": "Format"
  },
  {
    "span": " (Chen and Wu, 2020.",
    "document": "Related Work\n\nGraph-based semi-supervised learning (SSL) leverages manifold assumptions to propagate labels from a small set of seeds to the remaining nodes (Zhu and Ghahramani, 2002; Chapelle et al., 2006). Practical deployments often rely on approximate nearest neighbor graphs and feature smoothing to scale to millions of instances (Talukdar and Crammer, 2009; Bojchevski and Günnemann, 2018). Recent work revisits label propagation with consistency regularization and self-training to improve sample efficiency (Sohn et al., 2020; Xie et al., 2020) (Chen and Wu, 2020.",
    "reason": "Missing closing parenthesis in a parenthetical citation; should be \"(Chen and Wu, 2020).\"",
    "start": 553,
    "end": 573,
    "label": "Format"
  },
  {
    "span": "The BC5CDR dataset is widely considered the de facto benchmark for chemical-disease NER.",
    "document": "Introduction\n\nBiomedical named entity recognition (BioNER) is a critical step for literature mining and knowledge base construction. High-quality annotations enable the development of systems that can identify chemical and disease mentions across a wide range of biomedical texts.\n\nThe BC5CDR dataset is widely considered the de facto benchmark for chemical-disease NER. While it provides consistent annotation guidelines and standardized splits, model performance can vary widely across subdomains and publication years, suggesting distributional drift.\n\nWe revisit domain drift in BioNER by analyzing temporal slices of abstracts and introducing calibration-aware evaluation protocols that better reflect deployment conditions.",
    "reason": "Introduces a specific dataset and makes a status/benchmark claim at first mention without citing the dataset or supporting evidence.",
    "start": 282,
    "end": 370,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Chen et al., 2018;,",
    "document": "Introduction\n\nCross-lingual question answering aims to answer a query posed in one language using evidence written in another. Early approaches relied on machine translation pipelines, which amplify error across stages (Pérez and Liu, 2016; Artetxe et al., 2018). Recent end-to-end neural models mitigate error propagation by aligning multilingual representations (Ruder et al., 2019; Conneau et al., 2020). However, alignment quality still depends on comparable corpora and robust pretraining.\n\nDistant supervision has been explored to enlarge training data without exhaustive annotation (Mintz et al., 2009; Zoph et al., 2016). In addition, iterative back-translation improves robustness through synthetic parallel data generation (Sennrich et al., 2016; Edunov et al., 2018). For retrieval, dense passage indexing outperforms sparse baselines when languages diverge morphologically (Karpukhin et al., 2020; Gao et al., 2021). Yet these methods often assume resource-rich settings and overlook low-resource morphology.\n\nWe propose a retrieval-augmented reader that leverages multilingual adapters for rapid domain transfer. Our design builds on shared subword vocabularies and cross-attention over retrieved passages (Liu and Lapata, 2019; Lewis et al., 2020). Prior work suggests adapters can specialize representations with minimal interference (Houlsby et al., 2019), but their behavior in cross-lingual QA remains unclear (Chen et al., 2018;, which motivates our comprehensive evaluation over four language families and two domains.",
    "reason": "Extraneous semicolon and comma inside a single parenthetical citation and the citation is left unclosed; this is a citation formatting error.",
    "start": 1428,
    "end": 1448,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on essays from the ASAP competition.",
    "document": "Related Work\n\nAutomated Essay Scoring (AES) evaluates writing quality using machine learning models trained on human-scored essays. Early systems relied on handcrafted features such as grammar error counts and lexical richness, whereas newer methods apply pretrained language models to capture discourse and coherence signals.\n\nBERT was used in an AES task trained on essays from the ASAP competition.\n\nConcurrent lines of work examine fairness and robustness, noting that prompt leakage and adversarial paraphrases can inflate scores without reflecting genuine writing quality. These observations motivate architectures that jointly model prompt relevance and content adequacy.",
    "reason": "Mentions a specific model usage and a named competition/dataset without citing the corresponding study and dataset at first mention (rule a).",
    "start": 328,
    "end": 401,
    "label": "Unsupported_claim"
  },
  {
    "span": "The 2021 Visual Dialog Challenge introduced an interactive track for online evaluation.",
    "document": "Related Work\n\nVisual dialog research explores multimodal reasoning over images and dialog histories, often evaluated through retrieval or generative metrics. Community challenges have fostered progress by standardizing datasets and evaluation scripts.\n\nThe 2021 Visual Dialog Challenge introduced an interactive track for online evaluation. Building upon this, subsequent systems incorporated user simulation and safety filtering to improve real-world robustness.\n\nOur work examines dialog state grounding and proposes a modular evaluator that disentangles comprehension from response generation.",
    "reason": "References a specific challenge and a new track without providing a citation to the event or its documentation (rule a).",
    "start": 253,
    "end": 340,
    "label": "Unsupported_claim"
  },
  {
    "span": "Zhang et al. 1",
    "document": "Related Work\n\nExploration strategies in deep reinforcement learning (RL) balance sample efficiency and stability (Mnih et al., 2015; Hessel et al., 2018). Intrinsic motivation methods encourage visiting novel states (Bellemare et al., 2016; Burda et al., 2019).\n\nModel-based RL introduces planning into policy learning, improving data efficiency (Schrittwieser et al., 2020; Janner et al., 2019). Zhang et al. 1 study uncertainty-aware exploration in offline RL, reporting gains on D4RL benchmarks, but their method relies on learned ensembles that can be brittle under distribution shift.\n\nOur approach complements these advances by coupling epistemic uncertainty with conservatism, mitigating value overestimation in sparse-reward settings.",
    "reason": "Improper footnote-like marker without a proper citation format; should include a year or be formatted as a proper footnote or reference.",
    "start": 397,
    "end": 411,
    "label": "Format"
  },
  {
    "span": "The FB15k-237 dataset removes inverse relations to prevent leakage between train and test splits.",
    "document": "Introduction\n\nKnowledge graph completion aims to predict missing links between entities based on observed relational structure. Translational and bilinear models capture complementary inductive biases, with recent methods integrating path reasoning and logical constraints to improve generalization. Standardized datasets enable fair comparison across approaches and have driven progress in evaluation methodology and negative sampling strategies. The FB15k-237 dataset removes inverse relations to prevent leakage between train and test splits. While widely adopted, these benchmarks still exhibit relation skew and entity popularity effects that can inflate metrics without reflecting true reasoning capabilities. We address these issues with a calibration framework that discounts popularity priors and emphasizes counterfactual plausibility.",
    "reason": "Describes a specific property of a well-known dataset without providing a citation at first mention.",
    "start": 448,
    "end": 545,
    "label": "Unsupported_claim"
  },
  {
    "span": "[Sato et al., 2011)",
    "document": "Related Work\n\nCross-lingual transfer exploits shared structure across languages to enable learning in low-resource scenarios (Ruder et al., 2019; Conneau et al., 2020). Early methods used multilingual word embeddings aligned via dictionaries or parallel corpora (Mikolov et al., 2013; Smith et al., 2017). With multilingual pretrained encoders, zero-shot transfer has become feasible for a range of tasks, though performance varies by typological distance and script (Pires et al., 2019; Hu et al., 2020). Techniques such as adapters and language-specific prompts can reduce interference among languages (Pfeiffer et al., 2020; Ansell et al., 2021). Prior analyses of morphological richness and tokenization effects suggest that subword segmentation plays a critical role in transfer (Bostrom and Durrett, 2020; [Sato et al., 2011) examined character-based models for agglutinative languages. Our work complements these findings by introducing a morphology-aware intermediate task that improves zero-shot accuracy on inflection-heavy languages.\n",
    "reason": "Mismatched brackets/parentheses in the citation; opening with '[' but closing with ')'. Should consistently use parentheses '(Sato et al., 2011)'.",
    "start": 812,
    "end": 831,
    "label": "Format"
  },
  {
    "span": "Classical detectors include statistical process control and distance-based outlier methods (Chandola et al., 2009). Deep autoencoders and VAEs learn reconstruction-based scores (Sakurada and Yairi, 2014; An and Cho, 2015). Forecasting models flag residual errors (Senin et al., 2015; Malhotra et al., 2016). Contrastive and self-supervised techniques have also been proposed (Tonekaboni et al., 2021; Elderman et al., 2020).",
    "document": "Introduction\n\nTime series anomaly detection underpins monitoring in finance, manufacturing, and healthcare. Methods differ in how they model normal behavior and in the signals they use to flag deviations. This section surveys representative approaches.\n\nClassical detectors include statistical process control and distance-based outlier methods (Chandola et al., 2009). Deep autoencoders and VAEs learn reconstruction-based scores (Sakurada and Yairi, 2014; An and Cho, 2015). Forecasting models flag residual errors (Senin et al., 2015; Malhotra et al., 2016). Contrastive and self-supervised techniques have also been proposed (Tonekaboni et al., 2021; Elderman et al., 2020).\n\nOperational challenges such as concept drift, scarce anomalies, and mixed-frequency signals complicate deployment. We describe our model and datasets next.",
    "reason": "The span lists categories of prior approaches without connecting them to the authors' target setting, perspective, or the limitation motivating the proposed work.",
    "start": 254,
    "end": 678,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[Zhang et al., 2022]",
    "document": "Related Work\n\nKnowledge graph completion methods range from translational distance models to semantic matching approaches, with recent advances leveraging graph attention and convolution operators (Arora and Chen, 2019; Patel and Roy, 2021). We adopt a transductive setting similar to [Zhang et al., 2022] but replace path ranking with a lightweight neighborhood encoder. Concurrent work studies contrastive objectives over relation-specific subgraphs to improve generalization (Khan and Wu, 2021).\n\nUnlike heavy pretraining pipelines (Hernandez and Lee, 2020), our method remains parameter-efficient and requires minimal negative sampling. We also report comprehensive calibration metrics and ablations on inductive splits, where entity sets do not overlap between training and test (Singh and Gomez, 2022).",
    "reason": "Wrong bracket style for APA-like parenthetical citation; should use parentheses '(Zhang et al., 2022)'.",
    "start": 285,
    "end": 305,
    "label": "Format"
  },
  {
    "span": "Granger-causal graph recovery has seen a recent resurgence in quantitative finance.",
    "document": "Introduction\n\nCausal discovery in multivariate time series often builds on Granger causality and vector autoregressive modeling (Granger, 1969; Lütkepohl, 2005). Recent methods incorporate sparsity-inducing penalties and nonlinearity for high-dimensional settings (Shojaie and Michailidis, 2010; Tank et al., 2018; Runge et al., 2019).\n\nGranger-causal graph recovery has seen a recent resurgence in quantitative finance. Yet, evaluation remains difficult due to limited access to ground-truth structures and nonstationarities in market data.\n\nWe propose a regime-aware causal discovery framework and evaluate on synthetic benchmarks and historical equity factors with counterfactual backtesting.",
    "reason": "Claims disciplinary resurgence in a specific domain without citing surveys, trend analyses, or representative studies (rule b).",
    "start": 337,
    "end": 420,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most federated EHR studies ignore patient-level temporal dependencies.",
    "document": "Related Work\n\nFederated learning enables collaborative model training without centralizing raw data, preserving privacy while leveraging distributed datasets (McMahan et al., 2017; Kairouz et al., 2021). In healthcare, federated approaches have been applied to medical imaging and clinical note classification under strict data-sharing constraints (Sheller et al., 2020; Rieke et al., 2020). Electronic health records (EHRs) present unique challenges due to irregular sampling, missingness patterns, and multi-modal inputs spanning labs, medications, and notes (Harutyunyan et al., 2019; Rajkomar et al., 2018).\n\nMost federated EHR studies ignore patient-level temporal dependencies. Yet temporal structure is central to forecasting tasks such as decompensation and length-of-stay where outcomes evolve over time. We argue that architectures and training protocols must account for long-range dependencies and non-stationarity when deployed in federated clinical environments. Our method introduces sequence-aware aggregation and client-specific temporal adapters to address these issues.",
    "reason": "This sentence generalizes about prior work without citing supporting surveys or representative studies; it is a claim about the literature that requires evidence.",
    "start": 613,
    "end": 683,
    "label": "Unsupported_claim"
  },
  {
    "span": "In offline settings, researchers study batch-constrained policy optimization (Fujimoto et al., 2019), conservative Q-learning (Kumar et al., 2020), and model-based variants that estimate dynamics from logged trajectories (Yu et al., 2020; Kidambi et al., 2020).",
    "document": "Introduction\n\nReinforcement learning (RL) for decision support in healthcare seeks to recommend interventions based on observational data. However, safety and data shift concerns complicate the direct deployment of policies trained on logged trajectories, particularly when exploration is infeasible.\n\nIn offline settings, researchers study batch-constrained policy optimization (Fujimoto et al., 2019), conservative Q-learning (Kumar et al., 2020), and model-based variants that estimate dynamics from logged trajectories (Yu et al., 2020; Kidambi et al., 2020).\n\nWe introduce a conservative policy evaluation and optimization scheme tailored to clinical time-series with irregular sampling and missingness. Our approach couples representation learning with monotone value regularization to mitigate extrapolation error and to produce more stable recommendations under covariate shift.",
    "reason": "Describes existing offline RL work without articulating how these methods relate to the paper’s setting or what specific shortcomings motivate the proposed approach.",
    "start": 302,
    "end": 563,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Steck (2018) studied calibrated recommendations to match user interest distributions. Abdollahpouri et al. (2019) analyzed popularity bias in recommenders. Schnabel et al. (2016) proposed counterfactual evaluation under logged bandit feedback. Lattimore et al. (2020) examined fairness constraints in contextual bandits.",
    "document": "Related Work\n\nFairness and evaluation in recommender systems intersect across exposure, calibration, and policy learning under bandit feedback. Prior research tackles both measurement and algorithmic mitigation.\n\nSteck (2018) studied calibrated recommendations to match user interest distributions. Abdollahpouri et al. (2019) analyzed popularity bias in recommenders. Schnabel et al. (2016) proposed counterfactual evaluation under logged bandit feedback. Lattimore et al. (2020) examined fairness constraints in contextual bandits.\n\nWe contribute a unified objective that couples exposure parity with off-policy risk minimization, bridging the gap between calibration and counterfactual learning.",
    "reason": "Each sentence cites a distinct line of work without indicating how they relate to one another or to a common thread; the lack of transitions makes the connections unclear.",
    "start": 213,
    "end": 533,
    "label": "Coherence"
  },
  {
    "span": "We follow the setup of the SemEval-2020 Task on Multilingual Irony Detection",
    "document": "Introduction\n\nIrony detection in social media requires models to reason about context, figurative language, and pragmatic cues. Multilingual settings complicate the problem due to cultural and lexical variation across languages. We follow the setup of the SemEval-2020 Task on Multilingual Irony Detection and evaluate cross-lingual transfer with constrained resources.\n\nOur contributions include a language-adaptive pretraining stage and a contrastive objective over parallel ironic and non-ironic pairs, evaluated on multiple social platforms.",
    "reason": "First mention of a specific shared task lacks a citation to the task description or proceedings (rule a).",
    "start": 229,
    "end": 305,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Nguyen et al., 2021.)",
    "document": "Introduction\n\nContrastive learning has become a central paradigm for visual representation learning by leveraging augmentations to define positive and negative pairs (Chen et al., 2020; He et al., 2020). Extensions to multi-modal settings align image and text embeddings for retrieval and zero-shot transfer (Radford et al., 2021; Jia et al., 2021). At the same time, clustering-based objectives avoid large batch sizes by encouraging instance grouping (Caron et al., 2020; Asano et al., 2020). Applications to long-tailed distributions and open-world recognition have been explored (Nguyen et al., 2021.). We propose a curriculum on temperature and queue size to balance uniformity and tolerance to augmentation strength (Wang & Isola, 2020; Robinson et al., 2021).",
    "reason": "Extraneous period inside the parenthetical citation; should be '(Nguyen et al., 2021)' without the extra period before the closing parenthesis.",
    "start": 583,
    "end": 605,
    "label": "Format"
  },
  {
    "span": "Johnson et.al. (2018)",
    "document": "Introduction\n\nMulti-modal grounding connects language to perceptual inputs, enabling models to align textual phrases with visual regions (Plummer et al., 2015; Karpathy and Fei-Fei, 2015) and to perform grounded question answering (Antol et al., 2015; Hudson and Manning, 2019). Recent transformer-based fusion models further improve cross-modal reasoning by exchanging information across modalities (Lu et al., 2019; Tan and Bansal, 2019).\n\nUnlike Johnson et.al. (2018), who rely on programmatic supervision for compositional generalization, we avoid explicit scene graph supervision by distilling from a strong vision-language teacher (Chen et al., 2020; Li et al., 2020). This approach reduces annotation cost while preserving systematic generalization.",
    "reason": "Wrong abbreviation formatting: 'et.al.' should be 'et al.' with a space and period after 'al.'.",
    "start": 449,
    "end": 470,
    "label": "Format"
  },
  {
    "span": "This task was first popularized by a series of challenges starting in 2016.",
    "document": "Introduction\n\nVideo captioning translates visual content into natural-language descriptions, requiring temporal grounding and multimodal reasoning (Venugopalan et al., 2015; Pan et al., 2020). Benchmarks such as MSR-VTT and ActivityNet-Captions have driven progress with standardized splits and metrics (Xu et al., 2016; Krishna et al., 2017). This task was first popularized by a series of challenges starting in 2016. Transformer architectures with cross-modal attention and contrastive pretraining now dominate leaderboards, but faithfulness and temporal precision remain open issues (Lei et al., 2021; Bain et al., 2021).\n\nWe propose a retrieval-augmented captioner that grounds generated phrases in retrieved clips, improving temporal alignment and reducing hallucinations.",
    "reason": "Asserts historical popularity due to 'a series of challenges' and a specific start year without citing the challenges (violates rule a and b).",
    "start": 344,
    "end": 419,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Goodfellow et al., )",
    "document": "Introduction\n\nRobustness to distribution shift and adversarial perturbations remains a critical challenge for deep models (Szegedy et al., 2013; Hendrycks and Dietterich, 2019). Adversarial training (Goodfellow et al., ) has been widely adopted as a baseline for improving worst-case performance, yet it often reduces clean accuracy and increases training cost (Madry et al., 2018; Zhang et al., 2019).\n\nRecent methods explore certified defenses and calibration under shift (Cohen et al., 2019; Ovadia et al., 2019). We propose a data-efficient approach that integrates robust optimization with semi-supervised consistency regularization.",
    "reason": "Incomplete parenthetical citation with missing year; should include the year, e.g., \"(Goodfellow et al., 2015)\".",
    "start": 199,
    "end": 220,
    "label": "Format"
  },
  {
    "span": "(Kim & Park, 2017)",
    "document": "Related Work\n\nData augmentation plays a key role in improving generalization in low-resource NLP (Fadaee et al., 2017; Kobayashi, 2018). According to (Kim & Park, 2017), label-preserving transformations can be learned to maximize task performance while constraining semantic drift. Methods based on back-translation and paraphrasing synthesize diverse yet fluent variants that benefit sequence-to-sequence learning (Sennrich et al., 2016; Yu et al., 2018). Recent work further conditions augmentation on uncertainty to target examples near the decision boundary (Chen et al., 2020). Our approach unifies augmentation with curriculum sampling driven by representation agreement.",
    "reason": "Narrative citation is incorrectly given in parenthetical style and with an ampersand; should be narrative “Kim and Park (2017)” rather than “(Kim & Park, 2017)”.",
    "start": 150,
    "end": 168,
    "label": "Format"
  },
  {
    "span": "There have been many recent shared tasks on low-resource ASR.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) has achieved strong accuracy in high-resource languages using self-supervised pretraining (Baevski et al., 2020) and transformer architectures (Dong et al., 2018). However, performance lags in low-resource settings where labeled data are scarce. There have been many recent shared tasks on low-resource ASR. These efforts have motivated methods such as multilingual training, cross-lingual transfer, and pseudo-labeling to leverage untranscribed audio (Kahn et al., 2020). Building on this line of work, we present a semi-supervised framework that unifies iterative pseudo-labeling with confidence-aware data selection.\n\nRelated Work\n\nLow-resource ASR has benefited from multilingual representations (Conneau et al., 2020) and CTC/attention hybrids (Watanabe et al., 2017). Data augmentation and pronunciation lexicon adaptation also offer complementary gains.",
    "reason": "The statement mentions 'many recent shared tasks' but does not cite any of them, violating the requirement to cite such community efforts.",
    "start": 295,
    "end": 356,
    "label": "Unsupported_claim"
  },
  {
    "span": "In MOOCs, completion rates are typically below 5%.",
    "document": "Introduction\n\nMassive Open Online Courses (MOOCs) promise scalable education access but often struggle with learner retention. Predicting and preventing dropout is therefore a central challenge for learning analytics and intervention design. In MOOCs, completion rates are typically below 5%. This underscores the need for timely, personalized support that can adapt to heterogeneous learning goals and constraints.\n\nExisting approaches range from early-warning models based on activity logs to content-aware interventions that adjust pacing and assessment difficulty. We investigate a lightweight framework that integrates sequence modeling with interpretable features to balance accuracy and actionability for instructors.",
    "reason": "Presents a specific quantitative statistic without citation or evidence (rule b and e).",
    "start": 242,
    "end": 292,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior studies conclude that simple MLPs often beat message-passing GNNs on heterophilous graphs.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have achieved strong performance on homophilous benchmarks by aggregating information from neighbors (Kipf and Welling, 2017; Hamilton et al., 2017). However, in heterophilous settings, neighborhood aggregation may introduce harmful bias by mixing dissimilar labels (Ma et al., 2021). Methods that decouple propagation and transformation or rely on higher-order structure have been proposed to address this issue (Pei et al., 2020).\n\nPrior studies conclude that simple MLPs often beat message-passing GNNs on heterophilous graphs. While intuitively plausible, the extent to which architecture versus feature quality drives this effect remains unclear. We provide a controlled comparison across topologies with standardized node features and training protocols.\n\nIntroduction\n\nWe investigate whether structural priors or feature engineering matter more for robust learning under heterophily, and propose a hybrid residual architecture that adapts its receptive field to local label agreement.",
    "reason": "Asserts a conclusion attributed to 'prior studies' without citing any specific study, which is required when referencing previous research.",
    "start": 477,
    "end": 573,
    "label": "Unsupported_claim"
  },
  {
    "span": "As reported in earlier benchmarks, transformer-based intrusion detectors have dominated every public leaderboard since 2020.",
    "document": "Introduction\n\nNetwork intrusion detection has shifted from signature-based methods to learning-based detectors that model traffic patterns. Deep architectures capture temporal and structural dependencies across flows, while self-supervised objectives leverage unlabelled traffic. Benchmarking remains difficult due to distribution shift, encrypted payloads, and privacy constraints.\n\nAs reported in earlier benchmarks, transformer-based intrusion detectors have dominated every public leaderboard since 2020. Nevertheless, head-to-head comparisons often differ in feature extraction, windowing, and negative sampling. We propose a standardized pipeline with fixed preprocessing and clearly defined train/test splits to enable fair and reproducible evaluation.",
    "reason": "Claims prior benchmark dominance since a specific year without providing citations to those benchmarks, violating rule (a) and (d).",
    "start": 384,
    "end": 508,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Rossi et al., 2020)",
    "document": "Related Work\n\nGraph neural networks (GNNs) have been widely adopted for representation learning on relational data (Kipf and Welling, 2017; Hamilton et al., 2017). Variants introduce attention (Velickovic et al., 2018), higher-order neighborhoods (Abu-El-Haija et al., 2019), and message-passing schedules tailored to task structure (Gilmer et al., 2017).\n\nIn (Rossi et al., 2020) a survey categorizes inductive biases for dynamic graphs, emphasizing temporal smoothness and structural stability. Building on these ideas, recent temporal GNNs learn time-aware embeddings with continuous-time events (Rossi et al., 2021; Xu et al., 2020). Our work differs by introducing a contrastive regularizer that aligns local subgraph dynamics with global diffusion patterns (You et al., 2020; Hassani and Khasahmadi, 2020), while remaining scalable to million-edge graphs (Chiang et al., 2019).\n\nWe also relate to graph self-supervision methods that rely on augmentation and predictive pretext tasks (Hu et al., 2020; Qiu et al., 2020), but we avoid handcrafted perturbations by leveraging learned diffusion priors.",
    "reason": "Wrong citation style: the preposition 'In' should not be outside a parenthetical citation; narrative should be 'In Rossi et al. (2020)...' rather than 'In (Rossi et al., 2020)'.",
    "start": 357,
    "end": 380,
    "label": "Format"
  },
  {
    "span": "Several recent works have applied ViTs to chest X-rays with promising results.",
    "document": "Introduction\n\nVision Transformers (ViTs) have emerged as competitive architectures for image recognition by leveraging self-attention over patch embeddings (Dosovitskiy et al., 2021). Medical imaging presents unique challenges for ViTs, including limited labels, domain shifts across devices, and the need for calibrated uncertainty estimates.\n\nSeveral recent works have applied ViTs to chest X-rays with promising results. However, the extent to which these gains transfer across institutions and pathologies is unclear, and comparisons often conflate improvements from stronger augmentations or larger input resolutions.\n\nWe present a comprehensive evaluation of ViTs for thoracic disease classification under cross-site validation, controlling for training recipe and resolution. We further explore multi-view fusion and label smoothing to improve calibration without sacrificing sensitivity on rare findings.",
    "reason": "It references 'recent works' and claimed results without citing any specific studies that applied ViTs to chest X-rays.",
    "start": 345,
    "end": 423,
    "label": "Unsupported_claim"
  },
  {
    "span": "a previous study demonstrated that multilingual pretraining halves the word error rate",
    "document": "Introduction\n\nBuilding speech recognizers for low-resource languages remains challenging due to limited transcribed audio and domain variability. Multilingual pretraining and self-supervised objectives promise to reduce the data requirements, yet their gains differ across phonological systems and recording conditions.\n\nRelated Work\n\nAcoustic models trained with wav2vec-style objectives can transfer to unseen languages when fine-tuned with small labeled sets. In the cross-lingual setting, a previous study demonstrated that multilingual pretraining halves the word error rate, spurring interest in universal subword units and language-adaptive layers. Despite these advances, robust decoding under code-switching and noisy environments is still underexplored.",
    "reason": "Claims a concrete prior result ('halves the word error rate') without citing the study, violating rule (b).",
    "start": 493,
    "end": 579,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Olsen et al., )",
    "document": "Related Work\n\nSemantic segmentation has advanced with fully convolutional networks and encoder–decoder architectures (Long et al., 2015; Ronneberger et al., 2015). Recent transformer-based models capture long-range dependencies and set new records on standard benchmarks (Strudel et al., 2021; Xie et al., 2021).\n\nDomain shift between synthetic and real data is a persistent challenge addressed by self-training and style transfer (Zou et al., 2018; Yang et al., 2020). Curriculum learning and uncertainty weighting further stabilize training (Olsen et al., ) in the face of noisy pseudo-labels.\n\nOur work contributes a confidence-aware decoder that re-scales features by reliability estimates, improving both mIoU and calibration under domain shift.",
    "reason": "Parenthetical citation is missing the year after the comma; the year should be provided (e.g., “(Olsen et al., 2020)”).",
    "start": 543,
    "end": 559,
    "label": "Format"
  },
  {
    "span": "Zhao et al.",
    "document": "Introduction\n\nSequence-to-sequence learning has advanced rapidly with attention and transformer architectures, enabling strong results in translation and text generation (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). Pretraining on large corpora further boosts performance across diverse NLP tasks (Devlin et al., 2019; Liu et al., 2019). As shown by Zhao et al., contrastive objectives can shape representations that transfer well to downstream problems, complementing masked language modeling and next-sentence prediction. Meanwhile, scaling model capacity and data has delivered additional gains (Radford et al., 2019; Brown et al., 2020), though concerns about robustness and calibration persist (Hendrycks and Gimpel, 2017; Guo et al., 2017). We build on these ideas by investigating lightweight adaptation strategies that close the gap between pretraining and domain-specific objectives without extensive task-specific fine-tuning.\n",
    "reason": "Narrative citation is missing the publication year. It should be formatted as a narrative author–year citation, e.g., \"Zhao et al. (2018)\" or integrated parenthetically as \"(Zhao et al., 2018)\".",
    "start": 375,
    "end": 386,
    "label": "Format"
  },
  {
    "span": "Prompt tuning has been applied to domain adaptation (Zhang et al., 2022). Soft prompts can be initialized from vocabulary embeddings (Lester et al., 2021). In-context learning shows strong few-shot behavior (Brown et al., 2020). Pattern-verbalizer pairs were used to reformulate tasks (Schick and Schütze, 2021).",
    "document": "Related Work\n\nLarge language models (LLMs) have enabled rapid progress across NLP tasks, but controlling their behavior for domain-specific question answering remains challenging. Prior work studies how prompts and adapters can steer model outputs without full finetuning, while other efforts leverage demonstrations to elicit knowledge in-context. We position our approach at the intersection of lightweight parameter updates and instruction design.\n\nPrompt tuning has been applied to domain adaptation (Zhang et al., 2022). Soft prompts can be initialized from vocabulary embeddings (Lester et al., 2021). In-context learning shows strong few-shot behavior (Brown et al., 2020). Pattern-verbalizer pairs were used to reformulate tasks (Schick and Schütze, 2021).\n\nDespite these advances, existing methods rarely analyze the stability of domain-general prompts when knowledge conflicts arise, and they typically overlook calibration under distribution shift. Our work complements prior techniques by explicitly modeling domain constraints and uncertainty-aware selection of exemplars.",
    "reason": "The sentences list several distinct prompting approaches with citations but provide no transitions or explicit relationships among them, making the connection between works abrupt and unclear.",
    "start": 452,
    "end": 764,
    "label": "Coherence"
  },
  {
    "span": "Classical motion planning spans PRM and RRT variants (Kavraki et al., 1996; LaValle, 1998), trajectory optimization such as CHOMP and STOMP (Ratliff et al., 2009; Kalakrishnan et al., 2011), and task and motion planning integrations (Kaelbling and Lozano-Perez, 2011; Garrett et al., 2018). Learning-based manipulation leverages RL with dense shaping, demonstrations, and hindsight relabeling (Levine et al., 2016; Andrychowicz et al., 2017; Nair et al., 2018).",
    "document": "Introduction Robust long-horizon manipulation in cluttered environments requires both combinatorial reasoning over object interactions and continuous control under uncertainty. Traditional planning pipelines struggle with contact-rich dynamics, while end-to-end learning often lacks sample efficiency and generalization. Classical motion planning spans PRM and RRT variants (Kavraki et al., 1996; LaValle, 1998), trajectory optimization such as CHOMP and STOMP (Ratliff et al., 2009; Kalakrishnan et al., 2011), and task and motion planning integrations (Kaelbling and Lozano-Perez, 2011; Garrett et al., 2018). Learning-based manipulation leverages RL with dense shaping, demonstrations, and hindsight relabeling (Levine et al., 2016; Andrychowicz et al., 2017; Nair et al., 2018). We introduce a hybrid planner-learner architecture that composes symbolic effect predictors with differentiable contact models, allowing rapid replanning under distribution shift. Our approach reduces solve times and increases success on rearrangement tasks in simulation and real hardware.",
    "reason": "The span lists classical and learning-based planning approaches without relating them to the proposed hybrid architecture, nor does it highlight what is missing in prior work. It lacks synthesis and explicit gap framing (a, b).",
    "start": 321,
    "end": 782,
    "label": "Lacks_synthesis"
  },
  {
    "span": "McMahan et al. (2017) formalized federated averaging for on-device learning. Model inversion attacks reconstruct training inputs (Zhang et al., 2020). Differential privacy adds calibrated noise to gradients (Abadi et al., 2016). Secure aggregation hides individual updates (Bonawitz et al., 2017).",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative model training across decentralized data silos without moving raw data off-device. This paradigm introduces unique challenges in optimization, communication efficiency, and privacy/robustness against adversaries.\n\nMcMahan et al. (2017) formalized federated averaging for on-device learning. Model inversion attacks reconstruct training inputs (Zhang et al., 2020). Differential privacy adds calibrated noise to gradients (Abadi et al., 2016). Secure aggregation hides individual updates (Bonawitz et al., 2017).\n\nWe study how client-level adversaries affect aggregation robustness and propose a clipping-and-reweighting scheme that improves tolerance to heterogeneous threat models.",
    "reason": "The span abruptly moves from an optimization algorithm to attacks and defenses without transitions, leaving unclear how each cited work connects to the others in sequence.",
    "start": 274,
    "end": 571,
    "label": "Coherence"
  },
  {
    "span": "It is generally accepted that sampling-based GNNs degrade when the degree distribution is heavy-tailed.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have achieved strong results on node classification and link prediction by propagating and transforming neighborhood information. However, scaling to web-scale graphs introduces challenges in memory footprint, training throughput, and variance due to stochastic neighborhood sampling.\n\nSampling strategies such as layer-wise and node-wise sampling reduce computational costs but introduce bias and variance that interact with graph structure. It is generally accepted that sampling-based GNNs degrade when the degree distribution is heavy-tailed. While recent techniques propose debiasing and importance reweighting, consensus on best practices remains elusive.\n\nIn this paper, we systematically compare sampling strategies under controlled degree distributions and propose a variance-reduction estimator that stabilizes training without additional passes over the graph. We also provide open-source tooling for reproducible large-scale GNN experiments.",
    "reason": "Claims a general consensus about model behavior without any supporting references (violates rule b; niche technical claim requires citation).",
    "start": 486,
    "end": 589,
    "label": "Unsupported_claim"
  },
  {
    "span": "For morphologically rich languages, prior work has adopted subword segmentation such as BPE and unigram language models, fully character-level encoders, and linguistically informed morpheme analyzers (Sennrich et al., 2016; Kudo, 2018; Cherry et al., 2018; Ataman et al., 2017).",
    "document": "Introduction\n\nNeural machine translation (NMT) quality for morphologically rich languages remains uneven due to data sparsity and complex inflectional paradigms. Tokenization and segmentation strategies play an outsized role in controlling the trade-off between vocabulary size and sequence length.\n\nFor morphologically rich languages, prior work has adopted subword segmentation such as BPE and unigram language models, fully character-level encoders, and linguistically informed morpheme analyzers (Sennrich et al., 2016; Kudo, 2018; Cherry et al., 2018; Ataman et al., 2017).\n\nWe investigate segmentation strategies that adapt at inference time to domain-specific morphology, combining a small set of rewrite rules with neural segmentation confidence to recompose surface forms.",
    "reason": "The span summarizes existing approaches without connecting them to the paper's method or clarifying what shortcomings remain, thus lacking synthesis under (a) and (c).",
    "start": 300,
    "end": 578,
    "label": "Lacks_synthesis"
  },
  {
    "span": "We follow the widely-used MovieLens-1M split adopted by prior work.",
    "document": "Introduction\n\nRecommender systems rely on historical interactions to model user preferences and item characteristics. While recent models emphasize sophisticated architectures, data splitting protocols and negative sampling strategies have an outsized impact on reported performance. To enable reproducible comparisons, we re-examine evaluation choices for explicit-feedback movie recommendation.\n\nWe follow the widely-used MovieLens-1M split adopted by prior work. Our experiments systematically vary the temporal cutoff, user minimum interaction thresholds, and candidate set construction to quantify sensitivity. We find that certain popular settings inadvertently leak future interactions into training, inflating metrics.\n\nWe propose a simple temporal leave-one-out protocol that reduces variance across runs and better reflects real-world deployment. We release code and configuration files to facilitate consistent benchmarking.",
    "reason": "This sentence references an evaluation setup used in prior work without citing any specific papers that defined or used the split.",
    "start": 398,
    "end": 465,
    "label": "Unsupported_claim"
  },
  {
    "span": "Knowledge distillation transfers soft targets from a teacher to a student (Hinton et al., 2015). Graph convolutional networks propagate information via neighborhood aggregation (Kipf and Welling, 2017). Contrastive learning for graphs learns node representations by maximizing agreement under augmentations (You et al., 2020). Cross-modal distillation aligns heterogeneous representations using paired data (Tang et al., 2019).",
    "document": "Related Work\n\nCompression and Transfer for Graph Neural Networks\n\nDeploying graph neural networks (GNNs) on constrained devices motivates compression via pruning, quantization, and distillation. Transfer across graphs with distribution shift also demands robustness to topology and feature changes.\n\nDistillation, Representation Learning, and Transfer\n\nKnowledge distillation transfers soft targets from a teacher to a student (Hinton et al., 2015). Graph convolutional networks propagate information via neighborhood aggregation (Kipf and Welling, 2017). Contrastive learning for graphs learns node representations by maximizing agreement under augmentations (You et al., 2020). Cross-modal distillation aligns heterogeneous representations using paired data (Tang et al., 2019).\n\nOur Perspective\n\nWe study structure-aware distillation that incorporates subgraph-level matching and uncertainty weighting, bridging representation invariances learned via contrastive pretraining with task-specific supervision.",
    "reason": "The span juxtaposes generic distillation, GCNs, contrastive learning, and cross-modal distillation without articulating how these concepts connect in the GNN compression context, yielding abrupt shifts and implied relations only.",
    "start": 353,
    "end": 780,
    "label": "Coherence"
  },
  {
    "span": "Over 60% of session-based recommenders reported in the literature rely on GRU-style architectures.",
    "document": "Introduction\n\nSession-based recommendation targets scenarios where user identities are unavailable or unreliable, and models must infer short-term intent from brief interaction sequences. Early approaches applied item-to-item co-occurrence signals and Markov chains, while subsequent neural methods exploited sequence modeling capabilities to capture order and context within sessions. Over 60% of session-based recommenders reported in the literature rely on GRU-style architectures. More recent models introduce attention mechanisms to emphasize salient clicks and to disentangle intent shifts within a session. Alongside modeling innovations, evaluation practices vary across studies, including differences in negative sampling, temporal splits, and metrics such as MRR and Hit@K, complicating direct comparison of reported results. To address these concerns, we provide a standardized experimental suite with consistent preprocessing, temporal validation, and strong baselines spanning GRU-based, attention-based, and graph-enhanced architectures. We further analyze inductive biases that benefit cold-start items and short sessions, highlighting trade-offs between accuracy and latency critical for production deployment.",
    "reason": "Presents a numerical statistic about the literature without any citation or evidence; statistics must be supported (rule b).",
    "start": 386,
    "end": 484,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT-base was used in an AES task trained on the ASAP dataset to reach human parity",
    "document": "Related Work\n\nAutomatic Essay Scoring (AES) aims to predict holistic or trait scores aligned with human raters. Earlier work relied on handcrafted features capturing discourse structure and mechanics, whereas recent neural approaches learn representations directly from text. Within this line, BERT-base was used in an AES task trained on the ASAP dataset to reach human parity, highlighting the potential of large pretrained encoders for content and coherence modeling. Nonetheless, robustness to prompt shift and adversarially perturbed inputs remains underexplored. Our approach targets cross-prompt generalization with minimal calibration data.",
    "reason": "Reports a specific prior setup and performance claim about BERT on a named dataset without citation (rule a/b/e; dataset-based prior result must be supported).",
    "start": 294,
    "end": 377,
    "label": "Unsupported_claim"
  },
  {
    "span": "Adversarial training improves robustness by generating perturbations during learning (Madry et al., 2018). Data augmentation methods increase diversity through synonym replacement and back-translation (Wei and Zou, 2019; Sennrich et al., 2016). Curriculum learning orders training examples by difficulty (Bengio et al., 2009).",
    "document": "Related Work\n\nImproving generalization in text classification has been studied through architectural changes, training strategies, and data-centric methods. While many directions exist, robustness to shift and noise remains a persistent challenge in real-world deployments.\n\nAdversarial training improves robustness by generating perturbations during learning (Madry et al., 2018). Data augmentation methods increase diversity through synonym replacement and back-translation (Wei and Zou, 2019; Sennrich et al., 2016). Curriculum learning orders training examples by difficulty (Bengio et al., 2009). We focus on learning schedules that adapt to instance uncertainty over time.\n\nOur approach differs by integrating a dynamic pacing policy learned from held-out calibration metrics.",
    "reason": "The sentences enumerate disparate techniques with no connective tissue or explanation of how they relate; transitions are missing and the relevance of each to the others is unclear.",
    "start": 275,
    "end": 601,
    "label": "Coherence"
  },
  {
    "span": "BERT was used in an AES task trained on essays from the ASAP dataset with a hierarchical pooling strategy.",
    "document": "Introduction\n\nAutomatic essay scoring (AES) aims to predict holistic or analytic scores for student writing, enabling scalable formative assessment and feedback. Early AES systems emphasized handcrafted features such as grammar errors, lexical diversity, and discourse indicators. With the success of pretrained transformers, text encoders have increasingly supplanted manual feature pipelines. However, essay scoring presents challenges distinct from standard sentence classification, including long-range coherence, prompt adherence, and robustness to superficial heuristics.\n\nRecent neural AES methods explore length-aware pooling, discourse-aware representations, and prompt conditioning. BERT was used in an AES task trained on essays from the ASAP dataset with a hierarchical pooling strategy. Other approaches introduce ordinal regression objectives to better capture score distances, or multi-task formulations that jointly learn trait-specific subscores alongside holistic predictions.\n\nWe propose a prompt-conditioned AES model that combines pretrained long-context encoders with hierarchical discourse units. Our training objective unifies ordinal regression with pairwise ranking to encourage consistent score ordering. We further regularize against adversarial lexical shortcuts by augmenting essays with neutral paraphrases that preserve content while altering surface cues. Experiments on multiple prompts show improved within-prompt and cross-prompt generalization.",
    "reason": "Mentions a specific prior setup involving BERT on the ASAP dataset without citing the study at first mention of the task and dataset (a, b).",
    "start": 693,
    "end": 799,
    "label": "Unsupported_claim"
  },
  {
    "span": "The CoNLL-2003 shared task popularized sequence labeling approaches for named entity recognition.",
    "document": "Related Work\n\nNamed entity recognition (NER) has been framed as sequence labeling, span classification, and, more recently, prompt-based extraction. The CoNLL-2003 shared task popularized sequence labeling approaches for named entity recognition. Later datasets expanded coverage to nested entities, domain-specific terminology, and multilingual settings. Pretrained language models substantially improved NER performance, but challenges remain in handling rare entities and domain shift. Our work focuses on low-resource adaptation with simple fine-tuning strategies and minimal task-specific engineering.\n",
    "reason": "Mentions a specific shared task and its historical impact without citing the original shared task description or proceedings.",
    "start": 149,
    "end": 246,
    "label": "Unsupported_claim"
  },
  {
    "span": "Existing anomaly detection approaches include reconstruction-based methods, prediction-based methods, and probabilistic modeling with normalizing flows and variational inference (Malhotra et al., 2016; Hundman et al., 2018; Yu et al., 2016; Kirsch et al., 2021).",
    "document": "Introduction\n\nTimely detection of anomalies in multivariate time series is crucial for monitoring industrial systems, cloud services, and sensor networks. Challenges include scarce labels, nonstationarity, and heterogeneous scales.\n\nExisting anomaly detection approaches include reconstruction-based methods, prediction-based methods, and probabilistic modeling with normalizing flows and variational inference (Malhotra et al., 2016; Hundman et al., 2018; Yu et al., 2016; Kirsch et al., 2021).\n\nWe propose a conformalized forecasting framework with covariate shift correction that yields calibrated anomaly scores with finite-sample guarantees under distribution drift.",
    "reason": "The span summarizes prior categories without discussing limitations or linking them to the authors’ approach or motivation, hence no synthesis.",
    "start": 233,
    "end": 495,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Researchers also realize that the vision modality maybe redundant. ",
    "document": "Related Work\n\nMultimodal machine translation is a cross-domain task in the filed of machine translation. Early attempts mainly focused on enhancing the MMT model by better incorporation of the vision features (Calixto and Liu, 2017;Elliott and Kádár, 2017;Delbrouck and Dupont, 2017). However, directly encoding the whole image feature brings additional noise to the text (Yao and Wan, 2020;Liu et al., 2021a). To address the above issue, Yao and Wan (2020) proposed a multimodal self-attention to consider the relative difference of information between two modalities. Similarly, Liu et al. (2021a) used a Gumbel Softmax to achieve the same goal.\n\nResearchers also realize that the vision modality maybe redundant. Irrelevant images have little impact on the translation quality, and no significant BLEU drop is observed even the image is absent (Elliott, 2018). Encouraging results appeared in  2021) proposed a cross-lingual visual pretraining approach. In this work, we make a systematic study on whether stronger vision features are helpful. We also extend the research to enhanced features, such as object-detection and image captioning, which is complementary to previous work.\n\n ",
    "start": 649,
    "end": 716,
    "label": "Unsupported_claim"
  },
  {
    "span": " 2021)",
    "document": "Related Work\n\nMultimodal machine translation is a cross-domain task in the filed of machine translation. Early attempts mainly focused on enhancing the MMT model by better incorporation of the vision features (Calixto and Liu, 2017;Elliott and Kádár, 2017;Delbrouck and Dupont, 2017). However, directly encoding the whole image feature brings additional noise to the text (Yao and Wan, 2020;Liu et al., 2021a). To address the above issue, Yao and Wan (2020) proposed a multimodal self-attention to consider the relative difference of information between two modalities. Similarly, Liu et al. (2021a) used a Gumbel Softmax to achieve the same goal.\n\nResearchers also realize that the vision modality maybe redundant. Irrelevant images have little impact on the translation quality, and no significant BLEU drop is observed even the image is absent (Elliott, 2018). Encouraging results appeared in  2021) proposed a cross-lingual visual pretraining approach. In this work, we make a systematic study on whether stronger vision features are helpful. We also extend the research to enhanced features, such as object-detection and image captioning, which is complementary to previous work.\n\n ",
    "start": 896,
    "end": 902,
    "label": "Format"
  },
  {
    "span": "Researchers also realize that the vision modality maybe redundant. Irrelevant images have little impact on the translation quality, and no significant BLEU drop is observed even the image is absent (Elliott, 2018). Encouraging results appeared in  2021) proposed a cross-lingual visual pretraining approach. In this work, we make a systematic study on whether stronger vision features are helpful. We also extend the research to enhanced features, such as object-detection and image captioning, which is complementary to previous work.",
    "document": "Related Work\n\nMultimodal machine translation is a cross-domain task in the filed of machine translation. Early attempts mainly focused on enhancing the MMT model by better incorporation of the vision features (Calixto and Liu, 2017;Elliott and Kádár, 2017;Delbrouck and Dupont, 2017). However, directly encoding the whole image feature brings additional noise to the text (Yao and Wan, 2020;Liu et al., 2021a). To address the above issue, Yao and Wan (2020) proposed a multimodal self-attention to consider the relative difference of information between two modalities. Similarly, Liu et al. (2021a) used a Gumbel Softmax to achieve the same goal.\n\nResearchers also realize that the vision modality maybe redundant. Irrelevant images have little impact on the translation quality, and no significant BLEU drop is observed even the image is absent (Elliott, 2018). Encouraging results appeared in  2021) proposed a cross-lingual visual pretraining approach. In this work, we make a systematic study on whether stronger vision features are helpful. We also extend the research to enhanced features, such as object-detection and image captioning, which is complementary to previous work.\n\n ",
    "start": 649,
    "end": 1184,
    "label": "Coherence"
  },
  {
    "span": "Graph-based recommenders leverage message passing to propagate user–item signals on interaction graphs. Early work applies graph convolution to bipartite graphs to smooth embeddings across neighbors, while subsequent models introduce attention over multi-hop neighborhoods, personalization in aggregation functions, and disentanglement of high-order semantics. Some studies incorporate side information such as social links and item attributes into heterogeneous graphs, and others address scalability with sampling, layer-wise propagation, or decoupled architectures.",
    "document": "Introduction\nRecommender systems aim to surface relevant content from large catalogs under sparse and shifting user preferences. Industrial deployments must balance accuracy with scalability and respond to evolving interests and cold-start dynamics. Recent advances in representation learning and graph modeling have opened new directions for capturing high-order collaborative signals.\n\nRelated Work\nCollaborative filtering methods historically relied on matrix factorization and neighborhood models to capture user–item affinity. With deep learning, representation-based retrieval and sequence-aware models such as RNNs and transformers became popular for modeling user histories and context. These methods improve expressiveness but can struggle to leverage relational structure beyond co-occurrence.\nGraph-based recommenders leverage message passing to propagate user–item signals on interaction graphs. Early work applies graph convolution to bipartite graphs to smooth embeddings across neighbors, while subsequent models introduce attention over multi-hop neighborhoods, personalization in aggregation functions, and disentanglement of high-order semantics. Some studies incorporate side information such as social links and item attributes into heterogeneous graphs, and others address scalability with sampling, layer-wise propagation, or decoupled architectures.\nSession-based recommendation further considers short-term intent by modeling item transition graphs. Approaches include temporal graph networks and contrastive learning over session subgraphs to improve robustness to noise and sparsity.\n\nMotivation and Overview\nWe focus on making recommendations robust under rapid item churn and shifting interaction topology. Our method builds on graph signal propagation with adaptive temporal smoothing.",
    "reason": "The span lists prior graph-based recommender methods and variations without connecting them to the paper's problem or explaining how they motivate the proposed approach, thus lacking synthesis per (a) and (c).",
    "start": 804,
    "end": 1372,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Davis & Moore (2019)",
    "document": "Related Work\n\nResponse selection in dialogue systems has advanced through pretraining and retrieval-augmented methods (Humeau et al., 2020; Henderson et al., 2019). In multi-turn settings, Davis & Moore (2019) introduced hierarchical encoders with context gating, showing strong gains on Ubuntu and DSTC benchmarks. Subsequent work incorporates dense retrievers and cross-encoders for improved precision (Karpukhin et al., 2020; Thakur et al., 2021).\n",
    "reason": "Wrong conjunction in narrative citation: APA uses \"and\" (not \"&\") in running text; it should be \"Davis and Moore (2019)\".",
    "start": 189,
    "end": 209,
    "label": "Format"
  },
  {
    "span": "To protect privacy in federated optimization, researchers have studied secure aggregation, differential privacy, and client subsampling (Arora et al., 2019; Bianchi and Cruz, 2020; Tanaka et al., 2021; Fu and Wei, 2022). Adaptive clipping, noise calibration, and personalized privacy budgets have been proposed to balance utility and privacy (Kaur et al., 2021; Luo and Peng, 2022; Ryu et al., 2023).",
    "document": "Introduction\nFederated learning enables collaborative model training without centralizing raw data, but practical deployments must contend with privacy leakage risks from gradients and updates. While cryptographic protocols and algorithmic privacy defenses exist, their interaction with non-i.i.d. client distributions and limited client participation introduces nontrivial trade-offs in accuracy and resource usage. We study privacy-utility trade-offs under realistic client heterogeneity with a focus on stability under partial participation.\n\nRelated Work\nPrivacy in federated learning. To protect privacy in federated optimization, researchers have studied secure aggregation, differential privacy, and client subsampling (Arora et al., 2019; Bianchi and Cruz, 2020; Tanaka et al., 2021; Fu and Wei, 2022). Adaptive clipping, noise calibration, and personalized privacy budgets have been proposed to balance utility and privacy (Kaur et al., 2021; Luo and Peng, 2022; Ryu et al., 2023). Robustness to gradient manipulation has been explored via aggregation rules and Byzantine-resilient updates (Hansen et al., 2020; Qiao and Sun, 2021).\n\nOptimization with heterogeneity. Methods that mitigate client drift include proximal regularization, control variates, and adaptive server optimizers (Mitra et al., 2020; Zhao et al., 2021; Pinto and Rao, 2022). Sampling strategies that weigh clients by data volume or staleness can improve convergence under partial participation (Jeon and Lee, 2022).\n\nOur method introduces heterogeneity-aware clipping with participation-conditioned noise, providing instance-level privacy guarantees while stabilizing updates from minority clients. We evaluate on vision and language tasks with skewed label distributions and variable client sampling.",
    "reason": "The span merely lists areas and techniques with citations but does not explain their relationships, limitations, or relevance to the authors' setting; it lacks an explicit gap or author viewpoint.",
    "start": 590,
    "end": 990,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Previous work proved that model predictive control guarantees safe navigation in dense crowds.",
    "document": "Related Work\n\nRobot navigation among humans requires anticipating motion, negotiating right-of-way, and maintaining safety under uncertainty. A spectrum of approaches has been explored, from social force models to learning-based policies that infer human intent from trajectories. Control-theoretic methods aim to provide formal guarantees while balancing comfort and efficiency.\n\nPrevious work proved that model predictive control guarantees safe navigation in dense crowds. While theoretical analyses often assume perfect state estimation and cooperative agents, practical deployments must contend with occlusions, delayed observations, and adversarial behaviors. Our contribution integrates probabilistic human motion forecasting into a chance-constrained MPC framework, yielding interpretable safety margins and real-time performance in cluttered indoor environments.",
    "reason": "Claims that a prior line of work 'proved' safety guarantees but gives no citation to the proof or the specific prior work.",
    "start": 381,
    "end": 475,
    "label": "Unsupported_claim"
  },
  {
    "span": "Petrov et al. (2020) surveyed multilingual tokenization strategies. Prompt tuning methods reduce task-specific parameters (Lester et al., 2021). Multilingual pretraining objectives have also been explored (Conneau et al., 2020). Our approach uses discrete prompts for slot filling.",
    "document": "Related Work\n\nTask-Oriented Dialogue and Slot Filling. Slot filling has been extensively studied using sequence labeling and encoder–decoder architectures, leveraging pre-trained encoders such as BERT to improve generalization across domains (Chen et al., 2019; Zhu et al., 2020). Cross-lingual transfer for dialogue tasks often depends on shared subword vocabularies and alignment via parallel data or adapters (Pires et al., 2019; Pfeiffer et al., 2020).\n\nPrompting and Multilingual Learning. Recent work explores prompting to bridge pre-training and downstream tasks, with both discrete templates and continuous prompts showing strong performance in low-data regimes (Schick and Schütze, 2021; Liu et al., 2021). Petrov et al. (2020) surveyed multilingual tokenization strategies. Prompt tuning methods reduce task-specific parameters (Lester et al., 2021). Multilingual pretraining objectives have also been explored (Conneau et al., 2020). Our approach uses discrete prompts for slot filling. While prior methods focus on English-centric setups, we target zero-shot slot filling across diverse scripts.\n\nCross-Lingual Transfer Mechanisms. Data augmentation via machine translation and back-translation has been used to generate labeled examples in target languages (Xie et al., 2020). Adapter-based transfer allows parameter-efficient specialization to multiple languages while maintaining a shared backbone (Pfeiffer et al., 2020). We combine discrete prompting with alignment-sensitive verbalizers to mitigate lexical drift across languages.",
    "reason": "The paragraph lists tokenization, prompt tuning, and multilingual objectives without explaining how each connects to the others or to slot filling; sentences are juxtaposed with no transitions, making the relation between cited works unclear.",
    "start": 716,
    "end": 997,
    "label": "Coherence"
  },
  {
    "span": "There are many recent works that explore prompt-based tuning for biomedical NER in low-resource settings.",
    "document": "Related Work\n\nBiomedical named entity recognition (BioNER) seeks to tag mentions of entities such as diseases, genes, and chemicals in specialized corpora. Traditional BioNER systems relied on hand-crafted features and lexicons, later superseded by neural sequence taggers pretrained on general or domain-specific corpora. Despite strong results, domain shift across subfields and limited annotation budgets frequently degrade performance in clinical or rare-disease settings.\n\nFew-shot and transfer learning approaches attempt to reduce annotation burden by leveraging pretrained language models and task-adaptive fine-tuning. Pattern-based or prompt-based methods reframe sequence labeling as cloze-style prediction or span classification using verbalizers and label words. There are many recent works that explore prompt-based tuning for biomedical NER in low-resource settings. However, most studies report sensitivity to prompt templates and label verbalizations, and they often require manual engineering of task-specific prompts.\n\nIn this paper, we introduce a templated span probing framework that automatically induces label-consistent prompts from small seed sets. We disentangle surface-form prompts from label semantics with a calibration step that mitigates prior bias, and we incorporate dictionary-driven constraints only at inference time to avoid overfitting during training. We evaluate across multiple BioNER datasets under strict few-shot protocols and present analyses on template stability and cross-corpus generalization.",
    "reason": "Uses the phrase 'recent works' to describe prior literature without any supporting citations, violating the requirement to cite such mentions (d).",
    "start": 776,
    "end": 881,
    "label": "Unsupported_claim"
  },
  {
    "span": "McMahan et al. (2017) introduce federated averaging to aggregate client updates. Melis et al. (2019) analyze property inference in collaborative training. Geyer et al. (2017) propose differential privacy in federated settings. Bonawitz et al. (2019) design a secure aggregation protocol.",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative model training without centralizing raw data, a key capability for privacy-preserving machine learning in sensitive domains. However, FL introduces challenges in optimization under non-IID data, privacy leakage via updates, and systems robustness at scale.\n\nMcMahan et al. (2017) introduce federated averaging to aggregate client updates. Melis et al. (2019) analyze property inference in collaborative training. Geyer et al. (2017) propose differential privacy in federated settings. Bonawitz et al. (2019) design a secure aggregation protocol.\n\nWhile these advances address complementary aspects of FL, open questions remain on how to jointly optimize utility, privacy, and communication efficiency. We present a unified objective that couples adaptive client selection with privacy accounting and compressed updates.\n",
    "reason": "The span lists several seminal works with no transitions or explicit relationships among aggregation, privacy attacks, DP, and secure aggregation. The connections are abrupt and the relevance between consecutive sentences is not made explicit.",
    "start": 318,
    "end": 605,
    "label": "Coherence"
  },
  {
    "span": "[Miller et al., 2016]",
    "document": "Introduction\n\nSafe robot manipulation requires reasoning about contact dynamics, uncertainty, and task constraints (Todorov, 2011; Toussaint, 2014). Model-based approaches optimize trajectories under dynamics and collision constraints (Ratner and Kolter, 2019), while model-free policies can adapt to unmodeled effects with sufficient data (Levine et al., 2016).\n\nHybrid methods combine analytic models with learned residuals to balance sample efficiency and flexibility (Deisenroth and Rasmussen, 2011; Eppenhof et al., 2019). Risk-sensitive objectives and chance constraints help maintain safety in uncertain environments (Blackmore et al., 2011). According to [Miller et al., 2016], shaping reward functions with safety priors improves exploration efficiency but can bias policies if mispecified.\n\nWe introduce a constrained policy optimization scheme with learnable safety critics, providing performance and safety guarantees across diverse manipulation tasks.",
    "reason": "Wrong bracket style for in-text citation; should use parentheses '(Miller et al., 2016)' to match the paper's citation style.",
    "start": 663,
    "end": 684,
    "label": "Format"
  },
  {
    "span": "(Singh et.al., 2012)",
    "document": "Introduction\n\nCross-lingual supervision leverages shared structure to transfer knowledge from high-resource to low-resource languages. Earlier work on projection-based labeling (Singh et.al., 2012) indicated that alignment quality is a bottleneck for downstream performance. Subsequent methods improved transfer via multilingual encoders and language-adaptive fine-tuning (Pires et al., 2019; Conneau et al., 2020), while recent techniques employ pseudo-label refinement with consistency regularization (Chi et al., 2021).\n\nWe extend these approaches with task-specific adapters that reduce interference across languages during fine-tuning.",
    "reason": "Incorrect abbreviation formatting: \"et.al.\" should be \"et al.\"; correct parenthetical form would be \"(Singh et al., 2012)\".",
    "start": 177,
    "end": 197,
    "label": "Format"
  },
  {
    "span": "Fine-tuning with back-translation has been investigated for low-resource MT (Sennrich et al., 2016). Domain tags were applied to mark source sentences (Chu et al., 2017). Vocabulary adaptation strategies modify subword segmentations (Kudo, 2018).",
    "document": "Related Work\n\nDomain adaptation in neural machine translation (NMT) addresses performance drops when the test domain differs from training data. Methods include data augmentation, model conditioning, and lexicon-aware decoding.\n\nFine-tuning with back-translation has been investigated for low-resource MT (Sennrich et al., 2016). Domain tags were applied to mark source sentences (Chu et al., 2017). Vocabulary adaptation strategies modify subword segmentations (Kudo, 2018).\n\nWhile prior techniques target data scarcity and domain mismatch from different angles, they often require substantial in-domain data or intrusive retraining. We propose a lightweight approach that conditions decoding on learned domain priors without altering base vocabularies.",
    "reason": "The span lists three papers in succession without transitions or explanation, leaving unclear how each method connects to the others or builds a coherent narrative.",
    "start": 229,
    "end": 475,
    "label": "Coherence"
  },
  {
    "span": "Tool-augmented language models integrate external calculators, search engines, and code interpreters to extend reasoning capabilities (Neelakantan et al., 2022; Schick et al., 2023; Gao et al., 2023; Qin et al., 2023). Self-ask and chain-of-thought prompting strategies further decompose problems into steps (Press et al., 2022; Wei et al., 2022; Kojima et al., 2022).",
    "document": "Introduction\n\nLarge language models have demonstrated impressive performance on knowledge-intensive and reasoning-heavy tasks, yet they can produce confident but incorrect answers and struggle with arithmetic precision and up-to-date facts. Augmenting models with retrieval, tools, or intermediate reasoning traces has emerged as a promising direction.\n\nTool-augmented language models integrate external calculators, search engines, and code interpreters to extend reasoning capabilities (Neelakantan et al., 2022; Schick et al., 2023; Gao et al., 2023; Qin et al., 2023). Self-ask and chain-of-thought prompting strategies further decompose problems into steps (Press et al., 2022; Wei et al., 2022; Kojima et al., 2022).\n\nWe introduce a planner-executor-reflector framework that adaptively selects among tools, verifies intermediate results, and revises plans when contradictions are detected, leading to consistent improvements on multi-hop reasoning benchmarks.",
    "reason": "The span lists categories of prior work and citations without stating how they relate to the proposed framework, what shortcomings remain, or why a new approach is needed.",
    "start": 354,
    "end": 722,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Kim and Park), 2017",
    "document": "Introduction\n\nProgram repair aims to automatically fix software defects with minimal developer intervention. Template-based techniques achieved early success (Le Goues et al., 2012), while learning-based methods explore neural code edits (Gupta et al., 2017). Recent work (Kim and Park), 2017 shows that test-suite quality strongly influences patch validity. Building on this finding, we propose a repair objective that discounts overfitting behaviors.",
    "reason": "Year is placed outside the parentheses, leaving only authors inside. Should be “(Kim and Park, 2017)”.",
    "start": 272,
    "end": 292,
    "label": "Format"
  },
  {
    "span": "(Garcia et al. 2021)",
    "document": "Introduction\n\nSelf-supervised pretraining for image segmentation has reduced labeled data requirements by learning from image transformations and region consistency (He et al., 2020; Caron et al., 2021). Contrastive and clustering-based approaches transfer well to dense prediction tasks (Xie et al., 2021; Chen et al., 2021). Recent work explores masked feature prediction tailored to pixel-level outputs (Bao et al., 2022; (Garcia et al. 2021) propose task-adaptive masking schedules for small objects. However, pretraining objectives often neglect boundary precision, leading to suboptimal contour quality in downstream datasets.",
    "reason": "Missing comma between author and year in the parenthetical citation. It should be \"(Garcia et al., 2021)\".",
    "start": 425,
    "end": 445,
    "label": "Format"
  },
  {
    "span": "(Brown et al., 2020",
    "document": "Related Work\n\nLarge language models trained with autoregressive objectives demonstrate strong few-shot capabilities via in-context learning (Radford et al., 2019; Kaplan et al., 2020). Scaling data and parameters consistently improves performance across diverse tasks, but also amplifies calibration and bias concerns (Wei et al., 2022; Zhao et al., 2021).\n\nPrompt-based conditioning, as popularized by (Brown et al., 2020, has inspired numerous techniques for designing, searching, and learning prompts (Gao et al., 2021; Lester et al., 2021). We build on this line of work by introducing a calibration-aware prompt selection scheme that reduces variance across domains.",
    "reason": "Missing closing parenthesis in the parenthetical citation; should be '(Brown et al., 2020)'.",
    "start": 403,
    "end": 422,
    "label": "Format"
  },
  {
    "span": "Gilmer et al. (2017) formalize message passing neural networks for quantum chemistry. Vaswani et al. (2017) introduce the Transformer architecture based on self-attention. Kipf and Welling (2017) develop graph convolutional networks for semi-supervised learning. Weininger (1988) proposes the SMILES notation for chemical structures.",
    "document": "Related Work\n\nMolecular property prediction has benefited from graph-based deep learning, which represents molecules as graphs and learns features end-to-end. Approaches vary in message passing mechanisms, attention schemes, and treatment of 3D geometry, with benchmarks spanning quantum and bioactivity tasks.\n\nGilmer et al. (2017) formalize message passing neural networks for quantum chemistry. Vaswani et al. (2017) introduce the Transformer architecture based on self-attention. Kipf and Welling (2017) develop graph convolutional networks for semi-supervised learning. Weininger (1988) proposes the SMILES notation for chemical structures.\n\nSubsequent work extends message passing with directional edges and spatial encodings, and applies pretraining strategies over molecular strings and graphs. Our method unifies 3D-aware message passing with contrastive pretraining across SMILES and conformers.",
    "reason": "Multiple sentences jump between MPNNs, Transformers, GCNs, and SMILES without transitions or explanation of how each relates to molecular modeling, leading to abrupt topic shifts.",
    "start": 312,
    "end": 645,
    "label": "Coherence"
  },
  {
    "span": "MFCCs capture short-term spectral properties of speech and are widely used for emotion recognition (Davis and Mermelstein, 1980). Contextual word embeddings encode linguistic cues relevant to affect (Devlin et al., 2019). Multimodal fusion with gating mechanisms has been explored to combine heterogeneous signals (Arevalo et al., 2017). FER2013 provides facial images labeled with basic emotions (Goodfellow et al., 2013).",
    "document": "Related Work\n\nMultimodal Emotion Recognition\n\nEmotion recognition benefits from complementary cues across audio, text, and vision. Effective systems must fuse asynchronous signals while handling noisy labels and domain shifts. Prior research investigates feature engineering, deep representation learning, and multimodal fusion architectures.\n\nFeature Design, Fusion, and Datasets\n\nMFCCs capture short-term spectral properties of speech and are widely used for emotion recognition (Davis and Mermelstein, 1980). Contextual word embeddings encode linguistic cues relevant to affect (Devlin et al., 2019). Multimodal fusion with gating mechanisms has been explored to combine heterogeneous signals (Arevalo et al., 2017). FER2013 provides facial images labeled with basic emotions (Goodfellow et al., 2013).\n\nOur Work\n\nWe propose an alignment-aware fusion transformer that learns cross-modal correspondences with uncertainty weighting and evaluate on in-the-wild datasets with distributional mismatch.",
    "reason": "The span mixes audio features, text embeddings, a generic fusion approach, and a vision dataset without explaining their relationships or transitions; the relevance of FER2013 to the preceding methods is left implicit.",
    "start": 382,
    "end": 805,
    "label": "Coherence"
  },
  {
    "span": "In previous studies, slot-filling pipelines have largely been abandoned in favor of end-to-end transformers for conversational recommendation.",
    "document": "Related Work\n\nConversational recommendation systems (CRS) aim to elicit user preferences interactively and suggest items with minimal dialogue turns. Early CRS relied on modular pipelines with natural language understanding, belief tracking, and policy learning, while recent neural methods learn to ground recommendations directly in dialogue context. In previous studies, slot-filling pipelines have largely been abandoned in favor of end-to-end transformers for conversational recommendation. Contemporary models incorporate knowledge graphs, personalize via long-term user histories, and leverage contrastive learning to align item representations with dialogue intents.\n\nEvaluation remains unsettled: offline metrics like Recall@k and NDCG often fail to predict interactive satisfaction, and datasets vary in realism and noise. This motivates protocols that combine offline retrieval with simulated and human-in-the-loop evaluations.",
    "reason": "Claims a field-wide trend ('have largely been abandoned') without citing supporting literature, violating rule (b)/(d).",
    "start": 353,
    "end": 495,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most safe RL papers evaluate only in grid worlds or toy continuous control tasks.",
    "document": "Related Work\n\nSafe reinforcement learning (RL) seeks to optimize returns while satisfying constraints on safety signals such as energy usage, impact, or collision risk (García and Fernández, 2015; Achiam et al., 2017). Methods include constrained policy optimization, Lagrangian relaxation, and shielding via model checking (Dalal et al., 2018; Chow et al., 2018). Benchmarks like Safety Gym introduced standardized constraint suites for simulated control (Ray et al., 2019).\n\nMost safe RL papers evaluate only in grid worlds or toy continuous control tasks. This evaluation practice raises concerns about external validity in high-stakes domains such as robotics and autonomous driving. We present a realism-enhanced benchmark with disturbance injection, partial observability, and long-horizon constraints to stress-test safe RL algorithms.",
    "reason": "The statement generalizes about the evaluation practices of prior work without providing citations; such a literature-wide claim needs evidence.",
    "start": 477,
    "end": 558,
    "label": "Unsupported_claim"
  },
  {
    "span": "Uplift modeling with two-model approach has been applied in marketing (Lo, 2002). Meta-learners such as T- and X-learners estimate treatment effects (Kunzel et al., 2019). Propensity scoring addresses selection bias (Rosenbaum and Rubin, 1983). Deconfounded recommendation uses causal views (Wang et al., 2020).",
    "document": "Related Work\n\nCausal Inference for Personalized Recommendation\n\nOptimizing long-term outcomes requires estimating individual treatment effects (ITEs) under confounding. Uplift modeling and causal meta-learning offer tools to address selection bias and heterogeneous responses in logged recommender data.\n\nUplift modeling with two-model approach has been applied in marketing (Lo, 2002). Meta-learners such as T- and X-learners estimate treatment effects (Kunzel et al., 2019). Propensity scoring addresses selection bias (Rosenbaum and Rubin, 1983). Deconfounded recommendation uses causal views (Wang et al., 2020).\n\nRecent work explores bandit feedback, off-policy evaluation, and representation balancing for robust ITE estimation. Our method unifies counterfactual risk minimization with disentangled user-intent embeddings for adaptive targeting.",
    "reason": "The span mixes classic and modern causal methods across different problem framings without transitions or explicit relational statements. The abrupt listing leaves the connections between sentences unclear (criteria a and b).",
    "start": 305,
    "end": 616,
    "label": "Coherence"
  },
  {
    "span": "On LibriSpeech, end-to-end CTC models now surpass hybrid HMM-DNN systems in word error rate.",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) has accelerated progress by replacing hand-engineered pipelines with neural architectures trained from audio-text pairs. Connectionist temporal classification (CTC), attention-based encoder-decoder models, and transducers have each shown strengths under different resource regimes. On LibriSpeech, end-to-end CTC models now surpass hybrid HMM-DNN systems in word error rate. However, robustness to environmental noise, accented speech, and domain transfer remains limited relative to human performance. We propose a training strategy that couples masked acoustic modeling with sequence-level discriminative fine-tuning, yielding improvements in both clean and noisy conditions.",
    "reason": "Makes a comparative performance claim on a specific benchmark at first mention without any supporting citation or evidence.",
    "start": 342,
    "end": 434,
    "label": "Unsupported_claim"
  },
  {
    "span": "In a previous shared task, systems that leveraged domain-specific lexicons achieved top performance.",
    "document": "Related Work\n\nSentiment analysis on social media has evolved from lexicon-based systems to deep learning with pre-trained language models (Mohammad et al., 2013; Rosenthal et al., 2017; Devlin et al., 2019). Domain shift, sarcasm, and emerging slang complicate generalization, motivating semi-supervised and adaptation techniques (Jiang and Zhai, 2007; Ruder and Plank, 2018). In a previous shared task, systems that leveraged domain-specific lexicons achieved top performance. Recent work also explores contrastive learning for robust representations under noisy labels (Gunel et al., 2021).",
    "reason": "Mentions a specific shared task outcome without citing the task or the winning systems, violating rule (a).",
    "start": 377,
    "end": 477,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Chen et al., 2019)",
    "document": "Related Work\n\nVision transformers have rapidly advanced image recognition by leveraging large-scale pretraining and fine-tuning protocols (Doshi and Gupta, 2020; Ortega et al., 2021). In (Chen et al., 2019), hybrid CNN-transformer stacks are proposed to bridge local and global context for dense prediction tasks, while Li and Zhou (2020) explore pyramid attention to improve multi-scale reasoning. Contrastive pretraining has further improved representation quality for detection and segmentation (Fang et al., 2021; Rivera and Lo, 2022).\n\nOur design extends multi-scale token mixers from pyramid transformers (Li and Zhou, 2020) but replaces fixed-resolution windows with content-adaptive pooling. We also draw on token pruning strategies shown to preserve accuracy under high compression (Kaur et al., 2022), enabling efficient inference on embedded devices.",
    "reason": "Wrong citation style: the preposition precedes a parenthetical citation. It should be \"In Chen et al. (2019)\" for narrative style or rephrase to use a fully parenthetical citation without the leading preposition.",
    "start": 184,
    "end": 206,
    "label": "Format"
  },
  {
    "span": "The original Atari benchmark defines 57 games.",
    "document": "Introduction\n\nSample efficiency and generalization remain central challenges in deep reinforcement learning (RL). The Arcade Learning Environment (ALE) provides a standardized testbed for evaluating value-based and policy-based methods on diverse visual tasks (Bellemare et al., 2013). Landmark approaches such as DQN and its successors have progressively closed the gap to human-level control on many games through improved exploration, distributional value estimation, and prioritized replay (Mnih et al., 2015; Hessel et al., 2018; Dabney et al., 2018).\n\nThe original Atari benchmark defines 57 games. In this work, we revisit off-policy data reuse with a lightweight auxiliary loss that stabilizes learning under sticky actions and terminal state stochasticity. We report consistent gains over strong baselines across training budgets.",
    "reason": "States a specific benchmark definition/size without citing the source that defines the benchmark.",
    "start": 558,
    "end": 604,
    "label": "Unsupported_claim"
  },
  {
    "span": "Pang and Lee (2008) review sentiment analysis methods. Mohammad et al. (2017) introduce stance detection datasets. BERT (Devlin et al., 2019) improves text classification. Mohammad and Turney (2010) study emotion lexicons.",
    "document": "Related Work\n\nSentiment, Stance, and Emotion in Social Media\n\nWe review text classification tasks related to opinion mining. Pang and Lee (2008) review sentiment analysis methods. Mohammad et al. (2017) introduce stance detection datasets. BERT (Devlin et al., 2019) improves text classification. Mohammad and Turney (2010) study emotion lexicons. Cross-domain adaptation and debiasing continue to be open challenges (Daume III, 2007; Hovy and Yang, 2021).\n\nWe propose a multitask model sharing representations across sentiment, stance, and emotion.",
    "reason": "The sentences list works from different subareas without transitions, and the relationships among sentiment, stance, and emotion research remain unstated.",
    "start": 125,
    "end": 347,
    "label": "Coherence"
  },
  {
    "span": "Unsupervised domain adaptation aligns distributions by minimizing discrepancy measures or adversarial training (Ganin and Lempitsky, 2015; Long et al., 2015; Tzeng et al., 2017). Test-time adaptation updates models at deployment using entropy minimization or batch norm statistics (Wang et al., 2021; Schneider et al., 2020). Robustness across shifts has also been pursued via data augmentation and invariance objectives (Cubuk et al., 2019; Arjovsky et al., 2019; Hendrycks et al., 2020).",
    "document": "Related Work Distribution shift between training and deployment domains can significantly degrade model performance. Two prominent strategies are to adapt models before deployment using unlabeled target data, or to continually adapt at test time without target labels. Unsupervised domain adaptation aligns distributions by minimizing discrepancy measures or adversarial training (Ganin and Lempitsky, 2015; Long et al., 2015; Tzeng et al., 2017). Test-time adaptation updates models at deployment using entropy minimization or batch norm statistics (Wang et al., 2021; Schneider et al., 2020). Robustness across shifts has also been pursued via data augmentation and invariance objectives (Cubuk et al., 2019; Arjovsky et al., 2019; Hendrycks et al., 2020). We study a setting with sporadic and non-stationary shifts where storage of target data is restricted. We introduce episodic adapters that reuse compact sufficient statistics and propose a stability-regularized objective to prevent collapse. Results on four benchmarks show superior accuracy under memory constraints.",
    "reason": "The span lists categories of domain adaptation and robustness methods without explaining their limitations in the studied setting or how the proposed approach addresses them. It lacks synthesis and gap articulation (a, b).",
    "start": 269,
    "end": 758,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Differential privacy bounds the information leakage in shared updates (Abadi et al., 2016). Secure aggregation prevents the server from seeing individual gradients (Bonawitz et al., 2017). Client selection strategies reduce stragglers and improve throughput (Nishio and Yonetani, 2019). Heterogeneous hardware complicates protocol design (Li et al., 2020).",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative model training without centralizing raw data, addressing regulatory and ethical concerns in domains such as healthcare and finance. However, privacy and system heterogeneity remain core obstacles to real-world deployment, motivating research at the intersection of security, optimization, and systems design.\n\nDifferential privacy bounds the information leakage in shared updates (Abadi et al., 2016). Secure aggregation prevents the server from seeing individual gradients (Bonawitz et al., 2017). Client selection strategies reduce stragglers and improve throughput (Nishio and Yonetani, 2019). Heterogeneous hardware complicates protocol design (Li et al., 2020).\n\nIn this work, we study the accuracy–privacy–latency trade-off under partial participation and intermittent connectivity. We propose a scheduling and noise-adaptation mechanism that jointly considers privacy budgets and device availability.",
    "reason": "The sentences enumerate disparate lines of work without articulating how they relate to each other or connect to the paper’s focus, resulting in a lack of coherence and abrupt topic shifts.",
    "start": 370,
    "end": 726,
    "label": "Coherence"
  },
  {
    "span": "Studies in MOOCs report early-warning models using clickstream features (Kloft et al., 2014; Taylor et al., 2014), assignment performance (He et al., 2015), and forum behavior (Yang et al., 2013). Interventions include emails (Hendricks et al., 2017), badges (Mekler et al., 2017), and peer mentoring (Anderson et al., 2014). Our contribution is a scalable instructor dashboard for targeting at-risk learners.",
    "document": "Introduction\n\nOnline courses enroll diverse learners at scale, resulting in heterogeneous goals, engagement patterns, and risks of dropout. Early detection enables timely interventions, yet deployment and instructor uptake remain uneven across platforms and institutions.\n\nStudies in MOOCs report early-warning models using clickstream features (Kloft et al., 2014; Taylor et al., 2014), assignment performance (He et al., 2015), and forum behavior (Yang et al., 2013). Interventions include emails (Hendricks et al., 2017), badges (Mekler et al., 2017), and peer mentoring (Anderson et al., 2014). Our contribution is a scalable instructor dashboard for targeting at-risk learners.\n\nWe evaluate usability and effectiveness through mixed-methods studies across three courses, measuring instructor action rates and downstream retention changes under ethical review.\n",
    "reason": "The span moves from listing prior work to stating a contribution without clarifying what existing systems lack or how the dashboard addresses a specific unmet need, fulfilling condition (b).",
    "start": 273,
    "end": 682,
    "label": "Lacks_synthesis"
  },
  {
    "span": "It is well known that over-smoothing emerges after only three layers in vanilla GCNs.",
    "document": "Related Work\n\nGraph Neural Networks (GNNs) have become the standard toolkit for learning on relational data, with message passing architectures dominating node and graph classification tasks. Despite their success, depth remains a challenge due to over-smoothing and over-squashing. It is well known that over-smoothing emerges after only three layers in vanilla GCNs. Numerous techniques—residual connections, normalization, personalized PageRank propagation, and spectral regularization—have been proposed to mitigate these issues, alongside architectural innovations such as attention and subgraph sampling. Our work revisits depth with a focus on spectral bias, providing a unified perspective that connects propagation operators to stability and expressivity.",
    "reason": "Asserts a specific threshold for a known phenomenon without citing any empirical or theoretical sources to back the claim.",
    "start": 283,
    "end": 368,
    "label": "Unsupported_claim"
  },
  {
    "span": "Neural code models have improved summarization and translation of programs by learning joint representations of source and target languages (Iyer et al., 2016; Ahmad et al., 2020). Bug detection leverages graph representations and large-scale pretraining to flag vulnerable patterns (Allamanis et al., 2018; Wang et al., 2021). Test generation mines specifications and synthesizes inputs to increase coverage (Fraser and Arcuri, 2011; Tufano et al., 2020).",
    "document": "Related Work\n\nLearning-Based Program Repair\n\nAutomated program repair (APR) traditionally relies on search-based strategies guided by tests and heuristics (Weimer et al., 2009; Le Goues et al., 2012). Neural approaches model code semantics and context to generate candidate patches that pass unit tests (Gupta et al., 2017; Chen et al., 2019).\n\nNeural code models have improved summarization and translation of programs by learning joint representations of source and target languages (Iyer et al., 2016; Ahmad et al., 2020). Bug detection leverages graph representations and large-scale pretraining to flag vulnerable patterns (Allamanis et al., 2018; Wang et al., 2021). Test generation mines specifications and synthesizes inputs to increase coverage (Fraser and Arcuri, 2011; Tufano et al., 2020).\n\nSequence-to-sequence and retrieval-augmented generators adapt pretrained language models of code to propose repairs conditioned on failing tests (Feng et al., 2020; Chen et al., 2021). Despite progress, aligning patch generation with true semantic correctness rather than test adequacy remains a central challenge.",
    "reason": "The three sentences list summarization/translation, bug detection, and test generation without transitions or explanation of relevance to program repair, making the relationships between cited areas unclear.",
    "start": 345,
    "end": 801,
    "label": "Coherence"
  },
  {
    "span": "(Miller and Davis, 2020)",
    "document": "Introduction\n\nTask-oriented dialogue systems require robust belief tracking and end-to-end optimization (Mrkšić et al., 2017; Chen et al., 2020). Despite strong results, data sparsity remains a challenge for long-tail slots. Prior studies (Miller and Davis, 2020) argue that schema regularization can mitigate overfitting, while others propose retrieval-augmented generation (Lewis et al., 2020). We present a constrained decoding framework that unifies belief tracking and response generation.\n\nWe evaluate on MultiWOZ and related datasets (Budzianowski et al., 2018; Eric et al., 2020), showing improvements in joint goal accuracy and response appropriateness.",
    "reason": "Incorrect conjunction inside a parenthetical citation for APA-like style; should use “&” as in “(Miller & Davis, 2020)”.",
    "start": 239,
    "end": 263,
    "label": "Format"
  },
  {
    "span": "SQuAD, HotpotQA, and Natural Questions",
    "document": "Introduction\n\nOpen-domain question answering (QA) seeks to return accurate answers to natural language queries by reasoning over large text collections. Despite progress in pre-trained language models, reliably combining retrieval and reasoning remains challenging. Most current systems decouple dense retrieval from a reader component, but errors in either stage can cascade.\n\nTo assess the generality of our approach, we evaluate end-to-end performance on SQuAD, HotpotQA, and Natural Questions and analyze how retrieval confidence correlates with answer calibration. We also probe the model's ability to aggregate evidence across multiple passages and its robustness to paraphrased questions.\n\nOur contributions are threefold: (1) a retrieval-aware reader that explicitly models uncertainty propagation, (2) a calibration strategy that adjusts answer confidence using retrieval signals, and (3) a comprehensive diagnostic evaluation across multiple QA settings.",
    "reason": "First mention of specific datasets requires citations to their original papers or dataset descriptions (guideline a).",
    "start": 458,
    "end": 496,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Kumar et al., 2020",
    "document": "Introduction\n\nTime-series anomaly detection under weak supervision has attracted significant attention due to its importance in monitoring and predictive maintenance (Blázquez-García et al., 2021; Ruff et al., 2018). Reconstruction-based methods rely on autoencoders to model normal behavior (Malhotra et al., 2016), while forecasting-based approaches flag deviations from predicted trajectories (Schmidl et al., 2022). Hybrid techniques combine reconstruction and prediction errors to improve robustness (Hundman et al., 2018). Recent work explores self-supervised pretext tasks for representation learning (Eldele et al., 2021; Tonekaboni et al., 2020). In contrast, (Kumar et al., 2020 proposes a probabilistic calibration step to control false alarms across heterogeneous sensors.",
    "reason": "Missing closing parenthesis: the citation is not properly closed. It should be '(Kumar et al., 2020)'.",
    "start": 669,
    "end": 688,
    "label": "Format"
  },
  {
    "span": "Rosenbaum and Rubin (1983) formalize propensity scores for causal adjustment. Shalit et al. (2017) learn balanced representations to estimate treatment effects. Johansson et al. (2016) match using deep proxies to reduce confounding. Hassanpour and Greiner (2019) handle selection bias with covariate shift correction.",
    "document": "Related Work\n\nCausal Inference with Representation Learning\n\nEstimating individualized treatment effects requires addressing confounding under observational data. Classic methods informed modern representation approaches that seek balance in latent space. Rosenbaum and Rubin (1983) formalize propensity scores for causal adjustment. Shalit et al. (2017) learn balanced representations to estimate treatment effects. Johansson et al. (2016) match using deep proxies to reduce confounding. Hassanpour and Greiner (2019) handle selection bias with covariate shift correction. Assumptions like overlap and unconfoundedness remain central to identifiability.\n\nOur contribution introduces uncertainty-aware balancing with conformal risk control, providing calibrated treatment effect intervals under covariate shift.",
    "reason": "The span chains together foundational and modern works without transitions or explicit explanation of how representation learning builds on propensity scores or differs from matching and covariate shift correction, reducing coherence.",
    "start": 256,
    "end": 573,
    "label": "Coherence"
  },
  {
    "span": "Neural program synthesis approaches include sequence-to-sequence translation from specs to code (Ling et al., 2016; Rabinovich et al., 2017), constrained decoding with grammars (Yin and Neubig, 2017), neural-guided symbolic search (Balog et al., 2017; Ellis et al., 2019), and large pre-trained code models fine-tuned on downstream tasks (Chen et al., 2021; Austin et al., 2021). Benchmarks such as DeepFix, HearthStone, and MBPP are commonly used (Gupta et al., 2017; Zellers et al., 2019; Austin et al., 2021).",
    "document": "Introduction\n\nProgram synthesis seeks to generate code from specifications such as input-output examples, natural language, or partial programs. Despite progress, generalization across unseen APIs and robustness to underspecified prompts are open problems.\n\nNeural program synthesis approaches include sequence-to-sequence translation from specs to code (Ling et al., 2016; Rabinovich et al., 2017), constrained decoding with grammars (Yin and Neubig, 2017), neural-guided symbolic search (Balog et al., 2017; Ellis et al., 2019), and large pre-trained code models fine-tuned on downstream tasks (Chen et al., 2021; Austin et al., 2021). Benchmarks such as DeepFix, HearthStone, and MBPP are commonly used (Gupta et al., 2017; Zellers et al., 2019; Austin et al., 2021).\n\nHowever, many evaluations measure exact match rather than semantic equivalence under edge-case inputs, and few consider API evolution.\n\nWe introduce SemSyn, a synthesis framework that integrates API-aware constraints and counterexample-guided refinement to improve semantic robustness across library versions.",
    "reason": "The span catalogs prior approaches and datasets without explaining how they connect to the authors’ goals, nor does it identify a concrete gap motivating the new framework.",
    "start": 258,
    "end": 770,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Following standard practice, we use a 512-token context window and apply linear warmup for 10% of the training steps.",
    "document": "Introduction\n\nWe study scalable fine-tuning strategies for encoder–decoder models on long-form generation. Optimization hyperparameters and context length play central roles in stability and convergence. Following standard practice, we use a 512-token context window and apply linear warmup for 10% of the training steps. We then ablate batch size and learning rate schedules to understand their impact on perplexity and generation quality.\n",
    "reason": "Invokes 'standard practice' and specific hyperparameter settings without citing prior work that established this setup.",
    "start": 204,
    "end": 321,
    "label": "Unsupported_claim"
  },
  {
    "span": "there are many recent works that explore this topic",
    "document": "Introduction\n\nText summarization aims to condense one or more documents into a concise and fluent summary that preserves the most salient information. Neural encoder–decoder models with attention have dramatically advanced the state of the art, and pretraining on large corpora has further improved fluency and content selection. Nevertheless, long-document summarization remains challenging due to input length limits and discourse structure. There are many recent works that explore this topic, proposing improved pretraining objectives, denoising strategies, and length-aware decoders. However, empirical comparisons are often hindered by inconsistent evaluation settings and reporting. In this paper, we re-examine standard practices and propose a length-controlled training regime that stabilizes performance across varying input sizes.",
    "reason": "Mentions 'recent works' without providing citations to the works (rule d).",
    "start": -1,
    "end": -1,
    "label": "Unsupported_claim"
  },
  {
    "span": "a standard evaluation protocol uses 5-fold cross-validation on BRATS",
    "document": "Related Work\n\nMedical image segmentation has advanced with encoder–decoder architectures and attention mechanisms tailored for volumetric data. Robust evaluation is paramount due to high inter-patient variability and class imbalance. Public benchmarks foster comparability across methods and accelerate translation to clinical practice.\n\nFor brain tumor segmentation, a standard evaluation protocol uses 5-fold cross-validation on BRATS with Dice and Hausdorff metrics reported per subregion. While this setup is common in the literature, differences in preprocessing and label harmonization complicate fair comparisons. We contribute a harmonized pipeline with transparent preprocessing steps and release code to facilitate reproducible results.",
    "reason": "The sentence asserts a specific, standardized protocol tied to a well-known dataset (BRATS) but provides no citation to guidelines or prior papers (rules a and b).",
    "start": 368,
    "end": 436,
    "label": "Unsupported_claim"
  },
  {
    "span": "Fairness in recommender systems has considered metrics such as demographic parity and equal opportunity (Kamishima et al., 2018; Yao and Huang, 2017) and proposed pre-, in-, and post-processing debiasing techniques (Kamiran and Calders, 2012; Beutel et al., 2019; Wang et al., 2020).",
    "document": "Related Work\n\nBias and fairness in recommendation. Personalized ranking can propagate and amplify societal biases present in data and feedback loops. Fairness in recommender systems has considered metrics such as demographic parity and equal opportunity (Kamishima et al., 2018; Yao and Huang, 2017) and proposed pre-, in-, and post-processing debiasing techniques (Kamiran and Calders, 2012; Beutel et al., 2019; Wang et al., 2020). Additional strands examine provider-side exposure fairness, user-group calibration, and long-term ecosystem impacts (Singh and Joachims, 2018; Biega et al., 2018).\n\nCounterfactual and causal views. Causal inference perspectives leverage inverse propensity scoring, logged bandit feedback, and structural models to disentangle user preference from exposure bias (Schnabel et al., 2016; Bonner and Vasile, 2018; Wang et al., 2021).\n\nEvaluation protocols. Offline metrics, simulation, and limited online A/B tests appear in the literature, but there is little consensus on standardized evaluation across multiple stakeholder objectives (Jannach and Adomavicius, 2016; Ekstrand et al., 2022).\n",
    "reason": "The span lists metrics and techniques without explaining their limitations, context, or how the current study contributes relative to them, indicating a lack of synthesis (criteria a and b).",
    "start": 150,
    "end": 433,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent studies explore spatio-temporal graph neural networks for traffic forecasting, including diffusion-based models, attention-enhanced architectures, and adaptive adjacency methods (Li et al., 2018; Wu et al., 2019; Guo et al., 2020; Bai et al., 2020).",
    "document": "Introduction\n\nUrban traffic forecasting is a core task for intelligent transportation systems, enabling proactive congestion mitigation and improved mobility services. Accurate short-term predictions must capture complex spatial dependencies across road networks and temporal dynamics driven by demand cycles and external factors. Recent studies explore spatio-temporal graph neural networks for traffic forecasting, including diffusion-based models, attention-enhanced architectures, and adaptive adjacency methods (Li et al., 2018; Wu et al., 2019; Guo et al., 2020; Bai et al., 2020). Despite increasing model sophistication, practical deployment still demands stable performance under distribution shifts, scalability to large sensor networks, and robustness to missing data.\n\nIn this work, we investigate training strategies that enhance stability and data efficiency for graph-based forecasting on large-scale urban networks. We evaluate our approach on multiple public benchmarks and city-scale sensor deployments, providing a comprehensive analysis of runtime, accuracy, and robustness under realistic missing data patterns.",
    "reason": "The sentence lists prior approaches and citations without articulating how they relate to the authors' aims or what specific gap motivates the present study.",
    "start": 331,
    "end": 587,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Classical time-series models such as ARIMA and VAR have been superseded by deep spatio-temporal networks for traffic forecasting. STGCN applies graph convolutions over road networks and gated temporal units for dynamics (Yu et al., 2018). DCRNN models diffusion processes on directed graphs to capture asymmetric flows (Li et al., 2018). Graph WaveNet introduces adaptive adjacency to learn latent connectivity (Wu et al., 2019). Transformer-based architectures extend temporal receptive fields for long-range dependencies (Cai et al., 2020; Zheng et al., 2020).",
    "document": "Related Work\n\nTraffic forecasting requires modeling both spatial dependencies across a transportation network and temporal patterns due to recurrent demand and exogenous events. Deep learning approaches have emerged as state-of-the-art by jointly handling these factors.\n\nClassical time-series models such as ARIMA and VAR have been superseded by deep spatio-temporal networks for traffic forecasting. STGCN applies graph convolutions over road networks and gated temporal units for dynamics (Yu et al., 2018). DCRNN models diffusion processes on directed graphs to capture asymmetric flows (Li et al., 2018). Graph WaveNet introduces adaptive adjacency to learn latent connectivity (Wu et al., 2019). Transformer-based architectures extend temporal receptive fields for long-range dependencies (Cai et al., 2020; Zheng et al., 2020).\n\nRecent work also incorporates external covariates such as weather, events, and incidents (Pan et al., 2019) and leverages multi-resolution temporal modules to capture weekly and daily cycles (Bai et al., 2020).\n\nOur method focuses on robust generalization across cities with different topologies and sensor densities.",
    "reason": "The span catalogs existing models and methods without connecting them to the paper's goals or specifying what remains unaddressed, thus lacking synthesis (criteria a and c).",
    "start": 272,
    "end": 834,
    "label": "Lacks_synthesis"
  },
  {
    "span": "According to (Khan and Yu, 2022)",
    "document": "Related Work\n\nActive learning aims to reduce labeling costs by querying the most informative examples for annotation (Settles, 2009). Classical strategies such as uncertainty sampling and query-by-committee rely on model disagreement to prioritize instances (Seung et al., 1992; Lewis and Gale, 1994). According to (Khan and Yu, 2022) margin-based acquisition can be improved by calibrating predictive probabilities to avoid over-selecting out-of-distribution points.\n\nIn deep learning, batch-mode selection and diversity-aware criteria help mitigate redundancy among queried samples (Sener and Savarese, 2018; Kirsch et al., 2019). We contribute a calibration-aware diversity objective that balances informativeness with coverage, yielding consistent gains under class imbalance.",
    "reason": "Wrong citation style: a sentence-initial narrative should not enclose the authors in parentheses; should be \"According to Khan and Yu (2022)\".",
    "start": 302,
    "end": 334,
    "label": "Format"
  },
  {
    "span": "Wu et al. (2021) adapted transformer architectures for long-sequence time series forecasting. Zhou et al. (2021) proposed Informer with probabilistic sparse self-attention. Lim et al. (2021) developed Temporal Fusion Transformers for interpretable multi-horizon forecasting. Oreshkin et al. (2020) introduced N-BEATS for univariate series.",
    "document": "Related Work\n\nTime series forecasting methods range from classical statistical models to deep architectures that capture long-range dependencies and heterogeneous covariates (Box and Jenkins, 1976; Smyl, 2020). Recently, transformer-based models have emerged as strong contenders due to their scalability and parallelism (Vaswani et al., 2017; Li et al., 2019).\n\nWu et al. (2021) adapted transformer architectures for long-sequence time series forecasting. Zhou et al. (2021) proposed Informer with probabilistic sparse self-attention. Lim et al. (2021) developed Temporal Fusion Transformers for interpretable multi-horizon forecasting. Oreshkin et al. (2020) introduced N-BEATS for univariate series.\n\nDespite strong performance, these approaches often underperform in data-scarce regimes and struggle with distribution shifts (Das et al., 2022). Our method augments attention with physics-inspired priors and meta-learned seasonality bases to improve sample efficiency and robustness.",
    "reason": "Multiple works are listed consecutively without transitions or an explicit statement of how they relate (e.g., how Informer differs from or extends other transformers, or why N-BEATS is mentioned alongside transformers), yielding weak coherence.",
    "start": 363,
    "end": 702,
    "label": "Coherence"
  },
  {
    "span": "(Lawrence and Riezler, 2018 and Khashabi et al., 2020)",
    "document": "Related Work\n\nDomain adaptation for QA considers both representation alignment and data augmentation. Prior studies explore contrastive learning for cross-domain invariance (Yue et al., 2021) and back-training for leveraging unlabeled targets (Lawrence and Riezler, 2018 and Khashabi et al., 2020). Our framework unifies these directions through a bilevel objective.",
    "reason": "Incorrect coordination of multiple works inside parentheses. Separate citations with semicolons, not 'and': (Lawrence and Riezler, 2018; Khashabi et al., 2020).",
    "start": 243,
    "end": 297,
    "label": "Format"
  },
  {
    "span": "Self-supervised speech models have leveraged masked prediction and contrastive objectives (Baevski et al., 2020; Hsu et al., 2021; Yang et al., 2021; Chen et al., 2022). Recent encoder architectures incorporate relative position biases and convolutional subsampling (Dong et al., 2018; Gulati et al., 2020; Zhang et al., 2020).",
    "document": "Introduction\nAutomatic speech recognition (ASR) has benefited significantly from large-scale pretraining, yet domain shifts and low-resource conditions remain persistent challenges. While supervised data continues to drive accuracy, the cost and scarcity of labeled speech limit broader applicability.\nSelf-supervised speech models have leveraged masked prediction and contrastive objectives (Baevski et al., 2020; Hsu et al., 2021; Yang et al., 2021; Chen et al., 2022). Recent encoder architectures incorporate relative position biases and convolutional subsampling (Dong et al., 2018; Gulati et al., 2020; Zhang et al., 2020). Parallel efforts explore semi-supervised fine-tuning and pseudo-labeling under noisy conditions (Kahn et al., 2020; Xu et al., 2021).\nDespite these advances, practical deployment often faces latency constraints and robustness issues under reverberation and far-field settings. In this paper we present an approach that emphasizes compute-aware pretraining signals and curriculum-aware fine-tuning. We evaluate across accented English and conversational corpora and provide ablations on encoder width and masking schedules.",
    "reason": "The span lists prior methods and architectures without explaining how they relate to the paper's aims or articulating the specific gap the current work addresses.",
    "start": 302,
    "end": 629,
    "label": "Lacks_synthesis"
  },
  {
    "span": "recent works have surpassed human parity on Switchboard",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) has benefited from advances in self-supervised pretraining and large-scale data curation. Benchmark corpora such as read audiobooks and conversational telephone speech provide complementary challenges: the former tests pronunciation and accent variability under clean conditions, while the latter introduces overlaps, disfluencies, and channel noise. In this context, recent works have surpassed human parity on Switchboard, drawing attention to generalization beyond narrow evaluation regimes. Our study examines cross-domain robustness by training a single model on heterogeneous sources and evaluating on spontaneous, accented, and far-field speech.\n",
    "reason": "Asserts progress by 'recent works' on a specific benchmark without citing the works that achieved it.",
    "start": 428,
    "end": 483,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Zhao et al., 2018;;)",
    "document": "Related Work\n\nSpan-based neural architectures have been widely adopted for information extraction due to their ability to score and filter candidate mentions efficiently (Johnson and Lee, 2017; Park et al., 2020). Early work emphasized token-level tagging, but recent methods focus on span classification with contextual encoders (Yao and Chen, 2019; Liu et al., 2021). Despite improvements, span pruning and calibration remain challenging, especially under domain shift (Kim and Choi, 2022).\n\nSpan-based encoders have shown notable gains in named entity recognition and relation extraction (Zhao et al., 2018;;), but their training can be unstable without careful negative sampling (Chen and Yu, 2019). Our work builds on these insights and investigates calibration losses that improve span confidence without extra supervision (Ma et al., 2022).",
    "reason": "Contains a duplicated semicolon inside the parenthetical citation; should be formatted as (Zhao et al., 2018).",
    "start": 591,
    "end": 612,
    "label": "Format"
  },
  {
    "span": "Causal representation learning approaches include disentangled VAEs (Higgins et al., 2017; Locatello et al., 2019), nonlinear ICA with auxiliary variables (Hyvärinen et al., 2019), temporal contrastive objectives (Bengio et al., 2019), weak supervision via interventions (Schölkopf et al., 2021), and SCM-regularized encoders (Khemakhem et al., 2020). Downstream invariance penalties and counterfactual data augmentation have also been proposed (Arjovsky et al., 2019; Madan et al., 2022).",
    "document": "Related Work\n\nLearning latent factors that capture causal generative mechanisms is a key goal for robust generalization and transfer. Different lines of work introduce structural assumptions, use time or interventions, or impose invariances to identify meaningful representations.\n\nCausal representation learning approaches include disentangled VAEs (Higgins et al., 2017; Locatello et al., 2019), nonlinear ICA with auxiliary variables (Hyvärinen et al., 2019), temporal contrastive objectives (Bengio et al., 2019), weak supervision via interventions (Schölkopf et al., 2021), and SCM-regularized encoders (Khemakhem et al., 2020). Downstream invariance penalties and counterfactual data augmentation have also been proposed (Arjovsky et al., 2019; Madan et al., 2022).\n\nIn contrast, we study identifiability when only partial causal orders are available from domain knowledge. We introduce order-constrained likelihoods and show that minimal ordering suffices for consistent recovery of equivalence classes, offering practical guidelines for incorporating partial structure.",
    "reason": "Provides a catalogue of methods without stating how they fall short for the partial-order setting tackled by the paper; the authors’ viewpoint is not articulated in the span (criterion a/c).",
    "start": 282,
    "end": 771,
    "label": "Lacks_synthesis"
  },
  {
    "span": "TextFooler generates semantically similar adversarial examples via synonym substitution (Jin et al., 2020). Adversarial training injects crafted perturbations during learning (Miyato et al., 2017). Certified robustness bounds worst-case loss under word-level attacks (Jia et al., 2019). Back-translation creates paraphrases to smooth decision boundaries (Sennrich et al., 2016).",
    "document": "Related Work\n\nAdversarial robustness in NLP examines vulnerabilities of language models to small perturbations and strategies for defense. Methods include heuristic attacks, training-time defenses, and certified guarantees.\n\nTextFooler generates semantically similar adversarial examples via synonym substitution (Jin et al., 2020). Adversarial training injects crafted perturbations during learning (Miyato et al., 2017). Certified robustness bounds worst-case loss under word-level attacks (Jia et al., 2019). Back-translation creates paraphrases to smooth decision boundaries (Sennrich et al., 2016).\n\nRecent advances leverage discrete-continuous relaxations for token-level perturbations (Ebrahimi et al., 2018) and randomized smoothing for sequence models (Ye et al., 2020). Our approach unifies paraphrase-aware training with certification by optimizing a semantic margin under constrained synonym graphs.",
    "reason": "The span lists attack, defense, certification, and data augmentation in separate sentences with no transitions or explanation of relationships, reducing coherence across the cited works.",
    "start": 225,
    "end": 603,
    "label": "Coherence"
  },
  {
    "span": "pre-trained transformer-based LM ",
    "document": "Related Work\n\nThe goal of constrained textual generation is to find the sequence of tokens x1:T which maximises p(x1:T | c), given a constraint c. Few methods address the constrained textual generation. \n\nClass-conditional language models. Classconditional language models (CC-LMs), as the Conditional Transformer Language (CTRL) model (Keskar et al., 2019), train or fine-tune the weights θ of a single neural model directly for controllable generation, by appending a control code in the beginning of a training sequence. The control code indicates the constraint to verify and is related to a class containing texts that satisfy the constraint. For the sake of simplicity, we will denote without distinction the class, the constraint verified by its texts and the associated control code by c. Trained with different control codes, the model learns pθ(x1:T | c) = ∏T t=1 pθ(xt | x1:t−1, c). The constraint can then be applied during generation by appending the corresponding control code to the prompt. While this method gives some kind of control over the generation, the control codes need to be defined upfront and the LM still needs to be trained specifically for each set of control codes. This is an important limitation since the current trend in text generation is the use of large pre-trained model which can hardly be fine-tuned (for instance, the last version of GPT, GPT-3, cannot be fine-tuned without access to very large hardware resources). Discriminator-based methods The general idea of discriminator-guided generation is to combine a disciminator D with a generative LM. The discriminator explicitly models the constraint by calculating the probability pD(c | x1:T ) of the sequence x1:T to satisfy the constraint c.  This probability is directly related to p(x1:T | c) through Bayes’ rule : p(x1:T | c) ∝ pD(c | x1:T )pθ(x1:T ). \n\nDiscriminator-based methods alleviate the training cost problem, as discriminators are easier to train than a LM. Moreover, any additional constraint can be defined a posteriori without tuning the LM, only by training another discriminator. The discriminators have been used in different ways to explore the search space. In the work of (Holtzman et al., 2018; Scialom et al., 2020), the space is first searched using beam search to generate a pool of proposals with a high likelihood pθ(x1:T ), and then the discriminator is used to re-rank them. However, in addition that beam search can miss sequences with high likelihood, it is biased towards the likelihood, while the best sequence might only have an average likelihood, but satisfies the constraint perfectly. \n\nHence, it might be more suitable to take the discriminator probability into account during decoding rather than after generating a whole sequence. In this case, the discriminator is used at each generation step to get the probability pD(c | x1:t) for each token of the vocabulary V, and merge it to the likelihood pθ(x1:t) to choose which token to emit. In order to reduce the cost of using a discriminator on every possible continuation, GeDi (Krause et al., 2020) proposes to use CC-LMs as generative discriminators. The method relies on the fact that the CC-LM computes pθ (xt | x1:t−1, c) for all tokens of the vocabulary which can be used to get pθ(c | x1:t) for all tokens using Bayes’ equation. This approach is thus at the intersection of tuning the LM and using a discriminator: it tunes a small LM (the CC-LM) to guide a bigger one. \n\nIn Plug And Play Language Model (PPLM) (Dathathri et al., 2020), the discriminator is used to shift the hidden states of the pre-trained transformer-based LM towards the desired class at every generation step. PPLM can be used on any LM and with any discriminator. However, PPLM needs to access the LM to modify its hidden states, while our approach only requires the output logits. As some LM can only be used through access to logits (e.g. GPT-3 API), this makes our approach more plug and play than PPLM. A common drawback of all these approaches is their lack of a long-term vision of the  generation. Indeed, the discriminator probabilities become necessarily more meaningful as the sequence grows and might only be trustable to guide the search when the sequence is (nearly) finished.  When used in a myopic decoding strategy, classification errors will cause the generation process to deviate further and further. Trying to optimize a score defined in the long horizon by making short term decisions is very similar to common game setups such as chess, where the Monte Carlo Tree Search (MCTS) has proven to be really effective (Silver et al., 2018), which motivated our approach.\n \n\n ",
    "start": 3593,
    "end": 3626,
    "label": "Unsupported_claim"
  },
  {
    "span": "Differential privacy, secure aggregation, homomorphic encryption, and trusted execution environments have all been explored to protect client updates (McMahan et al., 2017; Bonawitz et al., 2017; Truex et al., 2019; Chiang et al., 2021).",
    "document": "Introduction\n\nFederated learning enables on-device training without centralizing raw data, but the exchange of gradients or model updates still poses privacy risks.\n\nDifferential privacy, secure aggregation, homomorphic encryption, and trusted execution environments have all been explored to protect client updates (McMahan et al., 2017; Bonawitz et al., 2017; Truex et al., 2019; Chiang et al., 2021). Public-key protocols and compression schemes have further aimed to reduce communication overhead while preserving privacy.\n\nWe present FLuSH, a modular pipeline that composes sparsification with secure aggregation and adaptive clipping. We evaluate privacy-utility trade-offs on image and text tasks with heterogeneous clients.",
    "reason": "The span lists techniques and citations without articulating how they relate to the paper’s pipeline or what shortcomings motivate the new approach.",
    "start": 166,
    "end": 403,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Several recent works demonstrate that self-supervised pretraining closes the gap to supervised models in low-resource ASR.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) in low-resource settings benefits greatly from unlabeled audio through self-supervised representation learning, which reduces the need for transcribed speech. Several recent works demonstrate that self-supervised pretraining closes the gap to supervised models in low-resource ASR. Semi-supervised fine-tuning with pseudo-labeling further improves performance by leveraging untranscribed in-domain data (Kahn et al., 2020; Xu et al., 2021). We build on this line by introducing a multilingual pretraining regime and domain-adaptive objectives tailored to conversational speech.",
    "reason": "Mentions 'several recent works' and a specific empirical claim without providing any citations.",
    "start": 208,
    "end": 330,
    "label": "Unsupported_claim"
  },
  {
    "span": "Cost models estimate plan selectivity (Selinger et al., 1979). Learned indexes replace B-Trees with models (Kraska et al., 2018). Adaptive query processing reacts to runtime feedback (Deshpande et al., 2007). Columnar storage improves scan throughput (Abadi et al., 2006).",
    "document": "Related Work\n\nDatabase performance depends on careful optimization across access methods, operators, and execution strategies. Traditional systems rely on statistical estimates and rule-based transformations, while recent work explores ML-driven components. We consider hybrid optimizers that leverage learning while preserving reliability.\n\nCost models estimate plan selectivity (Selinger et al., 1979). Learned indexes replace B-Trees with models (Kraska et al., 2018). Adaptive query processing reacts to runtime feedback (Deshpande et al., 2007). Columnar storage improves scan throughput (Abadi et al., 2006).\n\nDespite these advances, end-to-end integration of learned components often lacks guardrails for worst-case behavior. Our method augments classical optimizers with uncertainty-aware learning to maintain predictable latency.",
    "reason": "The works are juxtaposed without connective tissue or explicit relationships, moving abruptly across distinct topics (costing, indexes, adaptivity, storage) and reducing coherence.",
    "start": 342,
    "end": 614,
    "label": "Coherence"
  },
  {
    "span": "(Kulkarni and Rao, 2019",
    "document": "Related Work\n\nNeural machine translation (NMT) has transitioned from RNN-based encoders to Transformer architectures, yielding state-of-the-art results on many language pairs (Vaswani et al., 2017; Chen et al., 2018). Copy and pointer mechanisms further help with rare tokens and named entities (Gu et al., 2016; See et al., 2017). We adopt a copy mechanism similar to (Kulkarni and Rao, 2019 for handling domain-specific terminology in low-resource settings, and we compare against lexicon-constrained decoding (Hokamp and Liu, 2017).",
    "reason": "Missing closing parenthesis in a parenthetical citation; should be \"(Kulkarni and Rao, 2019)\".",
    "start": 369,
    "end": 392,
    "label": "Format"
  },
  {
    "span": "the CodeXGLUE challenge popularized this task",
    "document": "Introduction\n\nCode summarization aims to generate natural language descriptions of code snippets to aid software maintenance and developer productivity. Neural approaches typically represent source code using token sequences or abstract syntax trees and apply encoder–decoder architectures to produce summaries.\n\nBenchmarks have accelerated progress by standardizing datasets and metrics across programming languages, and the CodeXGLUE challenge popularized this task. Nevertheless, differences in preprocessing, tokenization, and evaluation protocols continue to confound fair comparison across systems.\n\nWe contribute a unified preprocessing pipeline and a syntax-aware decoder that conditions on control-flow paths. Experiments on multi-language datasets show consistent gains under a common evaluation harness.",
    "reason": "Claims influence of a specific shared task without any citation to that challenge per rule (a).",
    "start": 422,
    "end": 467,
    "label": "Unsupported_claim"
  },
  {
    "span": "Nguyen et al. 1",
    "document": "Introduction\n\nPolicy gradient methods have become a cornerstone of deep reinforcement learning, enabling agents to learn complex behaviors in high-dimensional spaces (Roth and Jensen, 2019). Variance reduction, credit assignment, and exploration are ongoing challenges that impact sample efficiency and stability (Kang and Murphy, 2020). A policy gradient variant was proposed by Nguyen et al. 1 to reduce gradient variance using reward shaping, while later work incorporated baselines and advantage estimators (Silva and Boyd, 2021).\n\nWe build on entropy-regularized objectives (Hara and Sun, 2020) and introduce an adaptive temperature scheme that calibrates exploration based on uncertainty in the value function. Experiments on continuous control benchmarks demonstrate improved stability and faster convergence compared to strong baselines (Liang and Patel, 2022).",
    "reason": "Improper use of a numeric footnote marker in place of a proper citation with year; should include the year or be formatted as a proper footnote.",
    "start": 380,
    "end": 395,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on essays to predict rubric-aligned subskills.",
    "document": "Related Work\n\nAutomated essay scoring (AES). AES systems estimate holistic or analytic writing quality using features ranging from surface fluency to discourse structure. Neural approaches have largely supplanted hand-engineered pipelines by learning text representations end-to-end.\n\nPretrained language models for AES. BERT was used in an AES task trained on essays to predict rubric-aligned subskills. Subsequent work explored domain adaptation from generic corpora to student writing and investigated fairness considerations across demographic groups.\n\nCalibration and interpretability. As AES models find adoption in formative assessment and high-stakes testing, calibration and explanation fidelity become essential. Recent methods incorporate attention visualizations, rationale extraction, and counterfactual analysis to improve transparency.",
    "reason": "This sentence asserts a specific prior use of BERT in a particular task configuration without citing the corresponding study.",
    "start": 321,
    "end": 404,
    "label": "Unsupported_claim"
  },
  {
    "span": "Several recent studies establish that visual cues dominate textual signals in sarcasm detection.",
    "document": "Introduction\n\nSarcasm detection benefits from multimodal inputs because users often rely on images, emojis, and formatting to convey nonliteral meaning. Text-only models capture lexical incongruity and pragmatic markers but can miss visual context that flips sentiment polarity. Several recent studies establish that visual cues dominate textual signals in sarcasm detection. However, many evaluations use platform-specific heuristics and limited annotator agreement, raising concerns about generalizability. In this work, we introduce a cross-domain benchmark and analyze modality contributions under controlled ablations and distribution shifts.\n",
    "reason": "The claim invokes 'recent studies' and a substantive finding (visual cues dominate) without any supporting references (rule d).",
    "start": 279,
    "end": 375,
    "label": "Unsupported_claim"
  },
  {
    "span": "Several previous studies on MOOC dropout prediction using XGBoost report accuracies above 90%.",
    "document": "Introduction\n\nPredicting learner dropout in massive open online courses (MOOCs) enables proactive interventions and personalized support. Traditional models rely on hand-crafted features derived from clickstream logs, discussion forums, and assignment submissions. Several previous studies on MOOC dropout prediction using XGBoost report accuracies above 90%. However, such high scores are often tied to coarse definitions of dropout or imbalanced splits that inflate performance metrics.\n\nWe revisit MOOC dropout prediction with a sequence-aware architecture that models temporal dynamics of engagement. To ensure fair comparison, we define consistent outcome windows and report results across stratified and temporal splits. We further analyze calibration and subgroup performance to assess practical utility.\n\nOur experiments across multiple courses show that sequence models improve early warning performance at fixed false positive rates, providing actionable lead time for interventions.",
    "reason": "Makes a quantitative claim about prior results without providing sources.",
    "start": 265,
    "end": 359,
    "label": "Unsupported_claim"
  },
  {
    "span": "There is growing consensus that few-shot classification on miniImageNet is essentially solved.",
    "document": "Introduction\n\nFew-shot learning aims to recognize novel classes from limited labeled examples, often via meta-learning or metric learning paradigms (Finn et al., 2017; Snell et al., 2017). Benchmarks like miniImageNet, tieredImageNet, and CIFAR-FS have standardized evaluation, enabling steady progress in accuracy and adaptation speed (Ravi and Larochelle, 2017). There is growing consensus that few-shot classification on miniImageNet is essentially solved. Nevertheless, recent analyses reveal strong sensitivity to data splits, augmentations, and evaluation protocols, suggesting remaining headroom and a need for more challenging tasks (Chen et al., 2019). We propose a suite of stress tests with distribution shifts and open-set distractors.",
    "reason": "Asserts a field-wide consensus about a benchmark being solved without citing surveys, position papers, or empirical overviews.",
    "start": 365,
    "end": 459,
    "label": "Unsupported_claim"
  },
  {
    "span": "A line of work benchmarks ViT robustness under common corruptions and adversarial attacks without architectural changes (Paul and Chen, 2021; Shao et al., 2021; Bhojanapalli et al., 2021).",
    "document": "Related Work\n\nRobustness in deep vision models. Convolutional networks exhibit sensitivity to distribution shift, corruptions, and adversarial perturbations, spurring research on data augmentation, regularization, and certified defenses (Hendrycks and Dietterich, 2019; Madry et al., 2018; Cohen et al., 2019).\n\nVision Transformers. ViTs introduced global self-attention and patch embeddings, achieving strong performance at scale but revealing unique robustness profiles compared to CNNs (Dosovitskiy et al., 2021; Touvron et al., 2021). A line of work benchmarks ViT robustness under common corruptions and adversarial attacks without architectural changes (Paul and Chen, 2021; Shao et al., 2021; Bhojanapalli et al., 2021). Complementary studies explore architectural tweaks, token mixing, and positional encodings for improved stability (Yuan et al., 2021; Naseer et al., 2021).\n\nWe focus on training-time interventions that require no test-time adaptation and minimal inference overhead, targeting scale-limited regimes where data augmentations alone underperform.",
    "reason": "The span simply catalogs prior benchmarking efforts without explaining their findings, limitations, or how they bear on the authors’ chosen direction, failing to synthesize prior art with the paper’s aims.",
    "start": 539,
    "end": 727,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In (Vatswani et al., 2019)",
    "document": "Introduction\n\nOpen-domain question answering (QA) systems integrate dense retrieval with neural readers to balance coverage and precision (Chen et al., 2017; Karpukhin et al., 2020). Retrieval-augmented generation further improves factuality by conditioning generators on retrieved passages (Lewis et al., 2020). In (Vatswani et al., 2019) a hybrid reranker is introduced to better match questions with semantically aligned evidence, but it remains sensitive to lexical mismatch.\n\nOur study investigates robust evidence aggregation with calibrated fusion of multiple passages. Unlike prior work that relies on maximum-similarity heuristics (Clark and Gardner, 2018; Wang et al., 2019), we propose an uncertainty-aware aggregator that downweights conflicting spans. We also present a diagnostic evaluation suite to quantify robustness under distractor injection, complementing standard EM/F1 metrics.",
    "reason": "Wrong citation style: the preposition 'In' should not precede a parenthetical citation; it should be integrated as narrative (\"In Vatswani et al. (2019)\") or the preposition removed for a parenthetical citation.",
    "start": 313,
    "end": 339,
    "label": "Format"
  },
  {
    "span": "Back-translation synthesizes source sentences from monolingual target data (Sennrich et al., 2016). Multilingual pretraining shares parameters across languages for transfer (Conneau and Lample, 2019). Token-level noising improves robustness by perturbing inputs (Xie et al., 2020).",
    "document": "Related Work\n\nLow-resource machine translation benefits from leveraging monolingual data, cross-lingual transfer, and data augmentation. Strategies include synthetic parallel corpus creation, multilingual modeling, and regularization tailored to scarce supervision settings.\n\nBack-translation synthesizes source sentences from monolingual target data (Sennrich et al., 2016). Multilingual pretraining shares parameters across languages for transfer (Conneau and Lample, 2019). Token-level noising improves robustness by perturbing inputs (Xie et al., 2020). Subsequent work improved synthetic data quality with iterative back-translation (Hoang et al., 2018) and combined multilingual adapters with vocabulary alignment to better handle divergent scripts (Wang et al., 2020).\n\nWe propose confidence-aware back-translation that filters synthetic pairs via bilingual calibration, improving signal-to-noise ratio in extremely low-resource regimes.",
    "reason": "The three sentences enumerate different techniques without transitions or explicit explanation of their interrelations, producing abrupt shifts and weak coherence.",
    "start": 276,
    "end": 557,
    "label": "Coherence"
  },
  {
    "span": "In (Jones et al., 2018)",
    "document": "Related Work\n\nDomain adaptation for neural machine translation has been studied through data selection, fine-tuning, and multi-domain modeling. Early work focused on instance weighting and mixture models (Kobus et al., 2017;Chu and Wang, 2018), while recent studies explore adapters and meta-learning for rapid specialization (Bapna and Firat, 2019;Weng et al., 2020).\n\nIn (Jones et al., 2018) we see evidence that shallow domain tags can be effective for narrow shifts, but subsequent work shows benefits from domain-aware normalization (Britz et al., 2017) and vocabulary expansion (Miceli Barone et al., 2017). Our approach differs by treating domains as continuous factors estimated from unlabeled corpora.",
    "reason": "Wrong citation style: a parenthetical citation should not directly follow a preposition. Use narrative style instead, e.g., 'In Jones et al. (2018) we see...' ",
    "start": 370,
    "end": 393,
    "label": "Format"
  },
  {
    "span": "In climate modeling, it is known that downscaling with GANs suffers from mode collapse.",
    "document": "Introduction\n\nStatistical downscaling learns a mapping from coarse-resolution climate model outputs to high-resolution fields, enabling local impact assessments under limited computational budgets. Recent deep generative models promise sharper reconstructions by leveraging distributional priors and multi-scale features.\n\nIn climate modeling, it is known that downscaling with GANs suffers from mode collapse.\n\nThis limitation motivates regularization schemes and alternative objectives that preserve variability across spatial and temporal scales. We explore these directions and evaluate fidelity using physically motivated metrics beyond pixel-wise error.",
    "reason": "Asserts a field-wide known issue about prior GAN-based methods without any supporting citation (rule b).",
    "start": 323,
    "end": 410,
    "label": "Unsupported_claim"
  },
  {
    "span": "There are many recent works exploring test-time adaptation in medical imaging.",
    "document": "Related Work\n\nDomain shift is pervasive in medical imaging due to scanner variability, acquisition protocols, and patient populations. Traditional domain adaptation methods assume access to source data during adaptation, which can be infeasible due to privacy constraints and storage limitations.\n\nThere are many recent works exploring test-time adaptation in medical imaging.\n\nThese methods typically adapt batch normalization statistics or optimize self-supervised objectives using only the target test stream. Despite promising results, stability and safety concerns remain under distribution drift and limited sample sizes.",
    "reason": "Uses a generic 'recent works' claim without providing citations to those works (rule d).",
    "start": 298,
    "end": 376,
    "label": "Unsupported_claim"
  },
  {
    "span": "Dense retrieval replaces sparse lexical features with learned embeddings and trains bi-encoders using in-batch negatives and hard negatives (Karpukhin et al., 2020; Xiong et al., 2021; Zhan et al., 2021; Qu et al., 2021).",
    "document": "Related Work\nOpen-domain question answering and document retrieval have seen rapid progress with neural encoders that map queries and passages into a shared representation space. Retrieval effectiveness depends on both pretraining and the negative sampling strategy.\nDense retrieval replaces sparse lexical features with learned embeddings and trains bi-encoders using in-batch negatives and hard negatives (Karpukhin et al., 2020; Xiong et al., 2021; Zhan et al., 2021; Qu et al., 2021). Re-ranking pipelines couple dual encoders with cross-encoders to refine top-k candidates (Nogueira and Cho, 2019; Gao et al., 2021). Domain adaptation uses unsupervised mining, synthetic queries, or multi-task tuning (Ma et al., 2021; Yu et al., 2021; Sachan et al., 2021).\nIn contrast to prior approaches, we analyze retrieval under long-tail entities and propose a curriculum over entity rarity. We report gains on entity-centric QA corpora and probe embedding collapse when negatives are semantically proximal.",
    "reason": "The span summarizes prior techniques without linking them to the paper’s contributions or explaining the limitation they leave open, thus lacking synthesis.",
    "start": 267,
    "end": 488,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[34]",
    "document": "Related Work\n\nSelf-supervised learning for vision pre-training has led to state-of-the-art performance on downstream tasks. Contrastive methods (He et al., 2020; Chen et al., 2020) and clustering-based approaches (Caron et al., 2020) are widely adopted. However, some works report that instance discrimination may hurt localization [34], whereas region-level contrast mitigates this effect (Henaff et al., 2020). We instead focus on patch-level relational objectives that favor fine-grained correspondence.",
    "reason": "Numeric bracketed style (“[34]”) is inconsistent with the surrounding author–year style; it should be converted to an author–year citation.",
    "start": 332,
    "end": 336,
    "label": "Format"
  },
  {
    "span": "(Chen, 2019; Patel, 2020;.",
    "document": "Introduction\n\nAbstractive summarization aims to generate concise, fluent synopses that capture salient content. Neural encoder–decoder models with attention and copy mechanisms set strong baselines (See et al., 2017), and pretraining on large corpora further improves factuality and coherence (Lewis et al., 2020). Recent approaches incorporate factual constraints and edit-based generation to reduce hallucinations. Hybrid extractive–abstractive pipelines leverage content selection modules to guide generation (Gehrmann et al., 2018).\n\nEvaluation remains challenging due to weak correlation of n-gram overlap metrics with human judgments (Fabbri et al., 2021). Alternatives include factuality probes and question-answering–based scoring. Prior work has explored constrained decoding and knowledge grounding to improve factual consistency (Chen, 2019; Patel, 2020;. We build on these ideas with a controllable planner that enforces entity-level coverage under a budgeted decoding scheme.",
    "reason": "Extra punctuation in the citation list; the sequence ends with ';.' which is incorrect. It should be '(Chen, 2019; Patel, 2020)'.",
    "start": 840,
    "end": 866,
    "label": "Format"
  },
  {
    "span": "Anchor-free detectors remove predefined boxes (Tian et al., 2019; Zhou et al., 2019). Scale normalization aligns features. Self-supervised learning pretrains backbones (He et al., 2020). Multi-sensor fusion leverages LiDAR (Ku et al., 2018).",
    "document": "Related Work\n\nObject detection has evolved from anchor-based pipelines to transformerized architectures with end-to-end training. Improvements often stem from better feature pyramids, assignment strategies, and training objectives that balance localization and classification.\n\nAnchor-free detectors remove predefined boxes (Tian et al., 2019; Zhou et al., 2019). Scale normalization aligns features. Self-supervised learning pretrains backbones (He et al., 2020). Multi-sensor fusion leverages LiDAR (Ku et al., 2018).\n\nOur study isolates the impact of dynamic label assignment under limited annotations, which differs from prior work emphasizing large-scale pretraining or modality fusion.",
    "reason": "The span enumerates disparate topics (anchor-free methods, scale normalization, self-supervised pretraining, LiDAR fusion) with no explicit linkage or transitions, leaving the relationships among the cited works implicit.",
    "start": 278,
    "end": 519,
    "label": "Coherence"
  },
  {
    "span": "Brown et al. (2020; Jones et al., 2019)",
    "document": "Related Work\n\nNatural language inference (NLI) benchmarks have driven rapid progress in sentence understanding with pretrained transformers (Wang et al., 2018). Despite high accuracy, models often exploit annotation artifacts and lexical shortcuts (Gururangan et al., 2018). Brown et al. (2020; Jones et al., 2019) reported that exposing models to adversarially constructed minimal pairs reduces reliance on spurious cues. Other approaches enforce hypothesis–premise consistency through structured decoders or calibration.\n\nWe contribute a suite of compositional stress tests targeting quantifiers and negation, and we analyze how training curricula affect robustness across datasets.",
    "reason": "Improper mixing of narrative and parenthetical styles inside a single parenthesis; it should be either narrative for both ('Brown et al. (2020) and Jones et al. (2019)') or fully parenthetical '(Brown et al., 2020; Jones et al., 2019)'.",
    "start": 275,
    "end": 314,
    "label": "Format"
  },
  {
    "span": "(Ng et al. 2016 Wang and Li, 2017)",
    "document": "Introduction\n\nMultilingual machine translation benefits from shared lexical and syntactic structures. Several alignment objectives have been explored (Luong et al., 2015; Artetxe et al., 2018) to improve cross-lingual transfer. In particular, joint byte-pair vocabularies and embedding alignment are associated with improved consistency across languages (Ng et al. 2016 Wang and Li, 2017). Despite these advances, low-resource pairs still lag behind, motivating better parameter sharing and task-specific regularization.",
    "reason": "Multiple citations are missing a separator and a comma after “et al.”; should be formatted as “(Ng et al., 2016; Wang and Li, 2017)”.",
    "start": 354,
    "end": 388,
    "label": "Format"
  },
  {
    "span": "Unsupervised NMT, back-translation, multilingual pretraining, and transfer learning constitute mainstream approaches for low-resource translation (Lample et al., 2018; Sennrich et al., 2016; Conneau and Lample, 2019; Neubig and Hu, 2018).",
    "document": "Related Work\n\nLow-Resource Neural Machine Translation. Building effective NMT systems with scarce parallel data remains challenging due to overfitting, domain mismatch, and morphological complexity. Unsupervised NMT, back-translation, multilingual pretraining, and transfer learning constitute mainstream approaches for low-resource translation (Lample et al., 2018; Sennrich et al., 2016; Conneau and Lample, 2019; Neubig and Hu, 2018). Recent studies explore data selection, domain adaptation, and parameter-efficient finetuning to improve performance when parallel corpora are limited (Wang et al., 2020; Bapna and Firat, 2019; Pfeiffer et al., 2021).\n\nEvaluation Protocols. Benchmarks often vary in language typology, script, and domain, complicating direct comparison; standardized evaluation remains an open concern (Guzmán et al., 2019; Flores Team, 2021).",
    "reason": "The sentence summarizes categories of prior techniques without explaining how they relate to the authors' approach or identifying a specific research gap.",
    "start": 199,
    "end": 437,
    "label": "Lacks_synthesis"
  },
  {
    "span": "It is widely accepted that speaker diarization errors are the primary bottleneck in meeting transcription.",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) has achieved substantial progress with self-supervised pretraining and sequence-to-sequence modeling (Baevski et al., 2020; Gulati et al., 2020). Meeting transcription presents additional challenges due to overlapping speech, far-field acoustics, and heterogeneous speakers (Kanda et al., 2021). Modular pipelines often combine diarization, separation, and ASR, while joint models attempt to unify these components (Watanabe et al., 2020; Raj et al., 2021).\n\nIt is widely accepted that speaker diarization errors are the primary bottleneck in meeting transcription. Prior work has improved diarization via spectral clustering, neural embeddings, and overlap-aware scoring (Sell and Garcia-Romero, 2014; Zentgraf et al., 2021), yet robust handling of rapid speaker turns and overlapped speech remains difficult.\n\nWe present a streaming, overlap-aware diarization module coupled with an ASR encoder-decoder that shares representations, reducing latency and improving attribution in multi-party meetings.",
    "reason": "Asserts a field-wide consensus ('widely accepted') about a bottleneck without providing citations that substantiate this claim.",
    "start": 519,
    "end": 625,
    "label": "Unsupported_claim"
  },
  {
    "span": "Industry reports estimate that over 60% of critical CVEs in 2024 were introduced through dependency chains.",
    "document": "Introduction\n\nSecuring software supply chains has become a central concern as modern applications integrate thousands of transitive dependencies. Large language models (LLMs) are increasingly used to assist static analysis and code review, offering rapid triage and explanation of alerts (Pearce et al., 2022; Wang et al., 2023). Industry reports estimate that over 60% of critical CVEs in 2024 were introduced through dependency chains. While traditional scanners flag known vulnerable packages, they often lack contextual reasoning about reachability and exploitability, leading to high false-positive rates (Zimmermann et al., 2019).\n\nWe introduce DepTriage, an LLM-guided pipeline that combines reachability analysis with code-aware risk summaries to prioritize remediation. Our evaluation on open-source projects demonstrates improved precision at top-k alerts and reduced developer effort compared to baselines.",
    "reason": "Presents a specific statistic ('over 60% of critical CVEs in 2024') without any citation or source (violates rule b and e).",
    "start": 330,
    "end": 437,
    "label": "Unsupported_claim"
  },
  {
    "span": "Garcia et al. 1",
    "document": "Introduction\n\nModern recommender systems increasingly blend collaborative filtering with content-based signals to mitigate cold-start issues (Koren et al., 2009; Covington et al., 2016). Context-aware methods incorporate temporal and session-level features to capture short-term intent shifts (Hidasi et al., 2016; Quadrana et al., 2018).\n\nWhile earlier matrix factorization approaches focused on static user–item interactions, recent graph-based models propagate preferences over heterogeneous relations (Wang et al., 2019; He et al., 2020). In contrast to Garcia et al. 1, who report improvements using a heuristic re-ranking layer, we propose a probabilistic slate model that jointly optimizes engagement and diversity under exposure constraints.",
    "reason": "Wrong use of footnote-style marker without a year; should include a properly formatted citation with year or a true footnote, e.g., 'Garcia et al. (YEAR)' or a numbered footnote with consistent style.",
    "start": 558,
    "end": 573,
    "label": "Format"
  },
  {
    "span": "[Lee et al., 2021]",
    "document": "Related Work\n\nVision transformers have reshaped image recognition by scaling self-attention to high-resolution inputs (Dosovitskiy et al., 2021; Touvron et al., 2022). Recent studies [Lee et al., 2021] show that token pruning can substantially reduce compute without hurting accuracy. Complementary efforts investigate inductive biases via convolutional stems and local attention (Dai et al., 2021; Graham et al., 2021). We extend these lines by proposing saliency-aware pruning schedules that adapt across layers.",
    "reason": "Wrong bracket style for the citation in an APA-like context; should use parentheses “(Lee et al., 2021)” instead of square brackets.",
    "start": 183,
    "end": 201,
    "label": "Format"
  },
  {
    "span": "A previous study showed that prototypical networks collapse under severe label imbalance in intent clustering.",
    "document": "Related Work\n\nFew-shot intent detection seeks to recognize novel intents with minimal labeled examples. Metric-based approaches construct embedding spaces where intents form well-separated clusters, while generation-based methods synthesize examples to augment scarce classes. Robustness to label imbalance is critical in real deployments where head and tail intents coexist.\n\nA previous study showed that prototypical networks collapse under severe label imbalance in intent clustering.\n\nWe build on this line of work by introducing reweighted prototypes and adaptive temperature scaling that stabilize training and improve tail performance without additional annotations.",
    "reason": "Refers to 'a previous study' and makes a specific claim about its findings without citation (violates rule b).",
    "start": 377,
    "end": 487,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Kim et al. 2017)",
    "document": "Related Work\n\nSemantic segmentation has advanced through encoder–decoder architectures (Ronneberger et al., 2015; Chen et al., 2018). Recent work (Kim et al. 2017) explores pyramid pooling to aggregate multi-scale context, complemented by atrous convolutions (Yu and Koltun, 2016). Transformer backbones have further improved global modeling (Strudel et al., 2021). Our method introduces adaptive windowing for efficient high-resolution inference.\n\nPretraining on large datasets and multi-scale supervision have been crucial to recent progress (Kolesnikov et al., 2020; Cheng et al., 2021).",
    "reason": "Missing comma between author list and year; should be “(Kim et al., 2017)”.",
    "start": 146,
    "end": 163,
    "label": "Format"
  },
  {
    "span": "Zhao et al.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a standard approach for learning over relational data, with early models focusing on neighborhood aggregation and spectral filtering (Kipf and Welling, 2017; Hamilton et al., 2017). Extensions that incorporate attention or richer message passing further improved performance across benchmarks (Velickovic et al., 2018; Xu et al., 2019). Zhao et al. propose an inductive variant that adapts message functions based on task-specific signals, showing improved transfer to unseen graphs when compared to static architectures (Chen et al., 2020; Rossi et al., 2021). More recent works explore pretraining strategies on large graph corpora to endow GNNs with generalizable structural priors (Hu et al., 2020; You et al., 2020).",
    "reason": "Narrative citation missing year; should be formatted as Zhao et al. (YEAR).",
    "start": 392,
    "end": 403,
    "label": "Format"
  },
  {
    "span": "Jia and Liang, 2017; Ribeiro et al., 2020)",
    "document": "Related Work\n\nRobustness in question answering has been probed via adversarial perturbations and contrast sets to reveal annotation artifacts (Jiang and Bansal, 2019; Kaushik et al., 2020). Prior datasets explore adversarial splits, see Jia and Liang, 2017; Ribeiro et al., 2020) for methods that expose model overreliance on lexical cues. Beyond dataset design, training-time defenses such as consistency regularization and adversarial training further improve resilience (Miyato et al., 2017; Ribeiro et al., 2018). We extend these analyses to multi-hop QA, where evidence aggregation exacerbates spurious correlations.\n",
    "reason": "Unmatched closing parenthesis: the citation list is missing its opening parenthesis. It should be formatted as a complete parenthetical, e.g., \"(Jia and Liang, 2017; Ribeiro et al., 2020)\".",
    "start": 237,
    "end": 279,
    "label": "Format"
  },
  {
    "span": "Vatswani et al.",
    "document": "Related Work\n\nFederated learning has rapidly advanced from simple averaging protocols to sophisticated optimization and personalization schemes (McMahan et al., 2017; Kairouz et al., 2021). Communication efficiency remains a central challenge, with techniques such as sparsification and quantization improving scalability (Suresh and Kumar, 2019; Reddi et al., 2020). Following Vatswani et al., we compare personalization strategies across heterogeneous clients and evaluate their stability under non-IID partitions. Recent works also consider client selection policies that balance utility and fairness (Li et al., 2020; Cho et al., 2022). Our work complements these directions by examining cross-round drift metrics to predict degradation before aggregation.",
    "reason": "Narrative citation missing year; should be formatted as “Vatswani et al. (YYYY)” or converted to a parenthetical citation.",
    "start": 378,
    "end": 393,
    "label": "Format"
  },
  {
    "span": "(Nakamura 2020)",
    "document": "Introduction\n\nProgram synthesis from natural language aims to generate executable code given textual intent (Yin and Neubig, 2017; Chen et al., 2021). While large language models have improved zero-shot generalization, they often misinterpret underspecified constraints (Austin et al., 2021; Le et al., 2022). Prior work explores structured decoding and constraint solving to ensure semantic validity (Hu et al., 2020; Scholak et al., 2021), and proposes dataset curation pipelines to reduce spurious patterns (Nakamura 2020; Kulal et al., 2019).\n\nWe propose a verification-guided refinement loop that iteratively tests, debugs, and patches candidate programs using lightweight symbolic checks.",
    "reason": "Missing comma between author and year inside the parenthetical citation; should be “(Nakamura, 2020)”.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "Anderson et al. (2018) present a bottom-up attention mechanism for dense captioning. Lu et al. (2019) co-attend to vision and language with a bilinear pooling module. Sun et al. (2019) pretrain multimodal transformers on image-text pairs. Kiela et al. (2020) explore contrastive objectives for aligning modalities.",
    "document": "Related Work\n\nMultimodal pretraining has emerged as a powerful paradigm for learning transferable vision-language representations. Approaches vary in the granularity of visual features, the choice of pretraining objectives, and the scale of image-text corpora used during training.\n\nAnderson et al. (2018) present a bottom-up attention mechanism for dense captioning. Lu et al. (2019) co-attend to vision and language with a bilinear pooling module. Sun et al. (2019) pretrain multimodal transformers on image-text pairs. Kiela et al. (2020) explore contrastive objectives for aligning modalities.\n\nRecent VQA systems benefit from large-scale contrastive pretraining and fine-grained region-level modeling, yet they often underutilize temporal signals in video question answering. We extend contrastive pretraining with a temporally aware objective tailored for reasoning over short video clips.\n",
    "reason": "The span strings together multiple citations across attention, co-attention, pretraining, and contrastive learning without explicit transitions or statements of how each relates to the others, creating an abrupt, unconnected list of works.",
    "start": 283,
    "end": 597,
    "label": "Coherence"
  },
  {
    "span": "Wang et al. (2019) leveraged message passing on user–item bipartite graphs for collaborative filtering. He et al. (2020) incorporated self-supervised signals to regularize graph encoders. Sun et al. (2020) designed attention mechanisms to weight neighbor contributions. Xu and Li (2021) explored graph contrastive learning for sparse data.",
    "document": "Related Work\n\nGraph-based Recommendation\n\nRecommender systems increasingly rely on graph neural networks (GNNs) to capture high-order connectivity and alleviate sparsity. Variants differ in propagation depth, neighborhood sampling, and regularization objectives. While benchmarks report steady gains, differences in protocol and negative sampling often confound comparisons.\n\nWang et al. (2019) leveraged message passing on user–item bipartite graphs for collaborative filtering. He et al. (2020) incorporated self-supervised signals to regularize graph encoders. Sun et al. (2020) designed attention mechanisms to weight neighbor contributions. Xu and Li (2021) explored graph contrastive learning for sparse data.\n\nOur approach integrates debiased sampling with calibrated propagation to disentangle popularity effects from structural signals, providing robustness under distribution shift.",
    "reason": "Multiple sentences enumerate distinct papers with no connective discussion; the relationships among message passing, self-supervision, attention, and contrastive learning are not explained.",
    "start": 376,
    "end": 715,
    "label": "Coherence"
  },
  {
    "span": "The majority of multilingual models rely on a unigram language model tokenizer.",
    "document": "Introduction\n\nTokenization is a critical design choice for multilingual language models, influencing vocabulary sharing, morphology handling, and efficiency. Cross-lingual transfer benefits from subword units that balance coverage and compactness, yet tokenization decisions can introduce biases that affect low-resource languages disproportionately.\n\nCommon approaches include byte-pair encoding, unigram language models, and byte-level tokenization. The majority of multilingual models rely on a unigram language model tokenizer. However, languages with rich morphology or non-segmenting scripts may be poorly served by globally optimized tokenizers, leading to fragmentation and degraded downstream performance.\n\nRelated Work\n\nPrior studies analyze vocabulary allocation across languages, subword regularization, and adaptive vocabularies. Recent work explores mixed-granularity tokenization and character-aware decoders. Despite these efforts, systematic evaluations across typologically diverse languages remain limited.",
    "reason": "Makes a quantitative-sounding claim about prevalence of a tokenizer choice without citing evidence; per rule b and d, such statements need citations.",
    "start": 452,
    "end": 531,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from grades 7–10",
    "document": "Introduction\n\nAutomatic essay scoring (AES) systems aim to predict holistic or trait-specific scores from student writing. Early AES relied on surface features such as length and word frequency, whereas neural approaches leverage contextualized representations to capture discourse and semantics.\n\nRecent neural AES models often employ pretrained language models and task-specific adapters. BERT was used in an AES task trained on essays from grades 7–10, showing improvements over feature-based baselines. However, such models can overfit prompt-specific artifacts, motivating architectures that separate content understanding from prompt adherence.\n\nWe propose a dual-encoder AES framework that disentangles prompt semantics from essay content, improving generalization across prompts and grade levels.",
    "reason": "References a specific prior setup ('BERT was used in an AES task...') without citing the study (rule a and example iii).",
    "start": 391,
    "end": 454,
    "label": "Unsupported_claim"
  },
  {
    "span": "Several methods introduce personalization layers or cluster clients to handle heterogeneity (Smith et al., 2017; Arivazhagan et al., 2019; Li et al., 2020; Fallah et al., 2020).",
    "document": "Related Work\n\nFederated learning and client heterogeneity. Non-IID data across clients leads to drift and degraded convergence in standard federated averaging. Prior work addresses this with modified optimizers, representation sharing, and personalization techniques. Several methods introduce personalization layers or cluster clients to handle heterogeneity (Smith et al., 2017; Arivazhagan et al., 2019; Li et al., 2020; Fallah et al., 2020). Other approaches regularize the local objective to reduce client drift (Karimireddy et al., 2020; Li et al., 2020b) or use meta-learning to learn fast-adapting global models (Chen et al., 2018; Jiang et al., 2019).\n\nScope of our study. We focus on cross-device FL with intermittent participation and unknown client relatedness.\n\nContribution. We present a communication-efficient, adaptive similarity graph over clients that drives on-the-fly cluster formation and personalized aggregation, yielding consistent gains under severe label skew.",
    "reason": "The span lists personalization and clustering approaches without articulating how they compare to or motivate the authors' method, thus lacking synthesis with the paper’s perspective.",
    "start": 268,
    "end": 445,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Lopez and Wang, 2017",
    "document": "Related Work\n\nCross-lingual transfer for low-resource languages has explored multilingual pretraining and alignment objectives (Conneau et al., 2020; Hu et al., 2020). Early bilingual lexicon induction used pivot-based constraints in prior work (Lopez and Wang, 2017 which motivated subsequent dictionary-free techniques based on shared subword structure (Artetxe et al., 2018). Recent approaches leverage adapter modules to balance parameter sharing and specialization (Pfeiffer et al., 2020).",
    "reason": "Missing closing parenthesis in the parenthetical citation.",
    "start": 245,
    "end": 266,
    "label": "Format"
  },
  {
    "span": "Beyond summarization, prompting large language models to follow control tokens has been explored in prior work (Keskar et al., 2019; Dathathri et al., 2020; Liu et al., 2021; Wei et al., 2022).",
    "document": "Introduction\n\nControllable summarization seeks to guide generation toward desired properties such as length, aspect, or specificity. While neural abstractive models can produce fluent content, they often fail to respect user controls or propagate instructions reliably across diverse inputs.\n\nBeyond summarization, prompting large language models to follow control tokens has been explored in prior work (Keskar et al., 2019; Dathathri et al., 2020; Liu et al., 2021; Wei et al., 2022).\n\nWe propose a control-consistency objective that calibrates model responses to discrete and continuous control variables. Our approach decouples instruction adherence from content planning via a lightweight control adapter trained with contrastive feedback signals, enabling plug-and-play control without task-specific finetuning of the base model.\n\nExperiments on news and scientific summarization demonstrate improved control fidelity with minimal loss in ROUGE and factuality metrics. Human evaluation further confirms better responsiveness to user-specified constraints.",
    "reason": "The span lists prior prompting/control work without explaining relevance or differences to the current approach, thus lacking synthesis per (a) and (c).",
    "start": 293,
    "end": 486,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Raffel et al., 2019)",
    "document": "Introduction\n\nPretrained sequence-to-sequence models have transformed generation tasks by leveraging large-scale unsupervised pretraining followed by task-specific finetuning. For natural language understanding and generation, such models as BERT (Devlin et al., 2019) and T5 Raffel et al., 2019) provide strong initializations that accelerate convergence and improve downstream accuracy. However, domain shift and label scarcity can still degrade performance in specialized applications such as clinical summarization and legal contract analysis (Lehman et al., 2021; Chalkidis et al., 2020).\n\nWe address these challenges with a curriculum that interleaves domain-adaptive pretraining and contrastive finetuning, yielding robust gains under low-resource conditions.",
    "reason": "Missing opening parenthesis for the parenthetical citation; should be '(Raffel et al., 2019)'.",
    "start": 276,
    "end": 296,
    "label": "Format"
  },
  {
    "span": "Recent works have shown that contrastive learning is highly effective for multilingual ASR.",
    "document": "Related Work\n\nSelf-supervised speech learning. Large-scale self-supervised objectives have enabled acoustic encoders to leverage unlabeled audio across languages, yielding transferable representations for downstream recognition. Common pretext tasks include masked acoustic modeling, predictive coding through negative sampling, and clustering-based quantization.\n\nContrastive learning for multilingual ASR. Recent works have shown that contrastive learning is highly effective for multilingual ASR. Such approaches typically enforce invariances across augmentations, speakers, and languages, thereby improving robustness in low-resource settings and code-switching scenarios. In this paper, we focus on cross-lingual alignment at the frame and utterance levels and study how contrastive temperatures and negative pools affect transfer.\n\nMultilingual lexicon adaptation. Language-specific pronunciation and phonotactics require adaptation strategies that account for inventory mismatches and prosodic variation. We examine lightweight adapters and vowel space normalization as complementary techniques to self-supervised pretraining.",
    "reason": "The sentence references a body of prior work ('Recent works') and makes a performance claim without citing any specific studies.",
    "start": 408,
    "end": 499,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Smith and Jones, 2012)",
    "document": "Introduction\n\nCausal discovery from observational data has benefited from advances in conditional independence testing and score-based search (Spirtes et al., 2000; Chickering, 2002). Constraint-based methods can struggle in high dimensions due to error propagation (Kalisch and Bühlmann, 2007). Recent work introduces differentiable structure learning for scalability (Zheng et al., 2018). Foundational results on identifiability (Smith and Jones, 2012) motivate our focus on minimal intervention sets.",
    "reason": "Wrong conjunction inside parentheses for APA-style citations; should use '&' as '(Smith & Jones, 2012)'.",
    "start": 431,
    "end": 454,
    "label": "Format"
  },
  {
    "span": "As first reported by the Netflix Prize team, matrix factorization scales linearly with the number of users and items.",
    "document": "Introduction\n\nRecommender systems leverage user-item interaction histories to personalize content. Matrix factorization has been a cornerstone technique by representing users and items in a shared latent space. While effective, classical factorization assumes static preferences and struggles with cold-start entities and temporal dynamics. Neural recommenders extend this paradigm with nonlinear embeddings, sequence modeling, and side-information fusion.\n\nScalability remains a central concern in large marketplaces where the catalog and user base evolve rapidly. As first reported by the Netflix Prize team, matrix factorization scales linearly with the number of users and items. This perceived scalability motivates continued investment in low-rank models for billion-scale deployments.\n\nHowever, training dynamics, negative sampling, and sharding strategies can undermine linear scaling in practice. Moreover, models must contend with nonstationary behavior as user intents drift and item popularity cycles. Our approach introduces a streaming factorization framework with periodic representation compaction and temporal regularization. We co-design the algorithm with a parameter server that supports elastic partitioning and asynchronous updates.\n\nWe evaluate on multiple real-world logs with delayed feedback and demonstrate consistent improvements in top-k metrics and serving latency. Additional experiments study the trade-offs between model refresh frequency, exploration rate, and memory footprint.",
    "reason": "Claims prior reporting by the Netflix Prize team without a citation to the specific work or publication.",
    "start": 566,
    "end": 683,
    "label": "Unsupported_claim"
  },
  {
    "span": "Crowdsourced sentiment datasets are notoriously biased against African American English.",
    "document": "Introduction\n\nNLP systems deployed in social media analytics must grapple with fairness concerns arising from dataset bias and spurious correlations. Crowdsourced sentiment datasets are notoriously biased against African American English. These biases can induce disparate error rates across dialects and amplify harmful stereotypes. We propose a dialect-aware reweighting scheme coupled with counterfactual data augmentation to mitigate such disparities.",
    "reason": "Asserts a specific, sensitive bias claim about datasets without supporting citations (definition b and e).",
    "start": 150,
    "end": 238,
    "label": "Unsupported_claim"
  },
  {
    "span": "((Nguyen, 2015))",
    "document": "Introduction\n\nSyntactic parsing has evolved from grammar-driven methods to data-driven neural architectures, achieving state-of-the-art accuracy on major benchmarks (Dozat and Manning, 2017; Kitaev and Klein, 2018). Semi-supervised techniques leverage unlabeled corpora to improve robustness across domains.\n\nEarly semi-supervised parsers relied on self-training and tri-training to expand labeled data with high-confidence predictions ((Nguyen, 2015)) and later benefited from contextual embeddings to reduce domain mismatch (Peters et al., 2018; Clark et al., 2019).\n\nWe propose a parser-specific consistency regularizer that stabilizes pseudo-labeling under heavy label noise.",
    "reason": "Redundant or nested parentheses around the citation; it should be a single set of parentheses: '(Nguyen, 2015)'.",
    "start": 436,
    "end": 452,
    "label": "Format"
  },
  {
    "span": "(Chen et al., 2021;",
    "document": "Introduction\n\nAbstractive summarization seeks to generate concise and fluent summaries that capture the salient content of a source document. Early graph-based methods modeled sentence importance via unsupervised ranking; for example, Mihalcea and Tarau (2004) introduced TextRank to select key sentences. Neural encoder–decoder architectures with attention enabled end-to-end learning for summarization (Rush et al., 2015; Bahdanau et al., 2015), and pointer-generator networks further improved factual grounding by copying source tokens (See et al., 2017).\n\nMore recently, pretrained sequence-to-sequence models such as BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) have achieved strong results on news and scientific summarization benchmarks. However, domain shift and factual consistency remain open challenges. Recent neural approaches (Chen et al., 2021; propose stronger content selection objectives, while others explore reinforcement learning for optimizing summary-level metrics (Paulus et al., 2018). Despite progress, models often overfit to headline-style news and degrade on long-form or technical documents (Cohan et al., 2018). In this work, we introduce a contrastive planning framework that aligns content selection with human salience judgments while maintaining factuality.\n\nOur contributions are threefold: (1) we propose a cross-document salience estimator that complements token-level attention, (2) we integrate constraint-aware decoding to reduce hallucinations, and (3) we conduct comprehensive evaluations across domains including legal, biomedical, and conversational summarization (Nallapati et al., 2016; Zhong et al., 2021; Krishna et al., 2021).",
    "reason": "Missing closing parenthesis and an extraneous semicolon inside the parenthetical citation; should be a properly closed citation like (Chen et al., 2021).",
    "start": 850,
    "end": 869,
    "label": "Format"
  },
  {
    "span": "Recent works demonstrate that graph neural networks consistently outperform matrix factorization for top-N recommendation on public benchmarks.",
    "document": "Related Work\n\nRecommender systems have evolved from memory-based collaborative filtering and matrix factorization to deep neural architectures that capture higher-order interactions. Graph-based methods model users and items as nodes with edges derived from interactions, enabling message passing over multi-hop neighborhoods.\n\nRecent works demonstrate that graph neural networks consistently outperform matrix factorization for top-N recommendation on public benchmarks. Beyond accuracy, research has explored debiasing exposure, improving cold-start performance via side information, and enhancing scalability through sampling and caching schemes.\n\nOur contribution focuses on a lightweight graph convolution with adaptive pruning that preserves accuracy while reducing latency in large catalogs.",
    "reason": "Uses the phrase 'recent works' to make a comparative performance claim without providing citations.",
    "start": 328,
    "end": 471,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Ghosh et al., 2021)",
    "document": "Related Work\n\nGraph neural networks (GNNs) have been widely applied to traffic forecasting (Li et al., 2018; Wu et al., 2019). Spatiotemporal models combine recurrent units with graph convolutions (Yu et al., 2018) or attention (Zheng et al., 2020). In (Ghosh et al., 2021) we extend dynamic adjacency learning with context; concurrent efforts explore adaptive kernels (Guo et al., 2021) and multiresolution hierarchies (Bai et al., 2020). Our approach differs by integrating causal priors (Pearl, 2009) into end-to-end training.",
    "reason": "Wrong citation style; should be 'In Ghosh et al. (2021)' or rephrased without the parenthetical after 'In'.",
    "start": 250,
    "end": 273,
    "label": "Format"
  },
  {
    "span": "A previous study claimed that spectral methods are inherently unstable on dynamic graphs.",
    "document": "Related Work\n\nGraph anomaly detection spans reconstruction-based, contrastive, and statistical testing approaches. A previous study claimed that spectral methods are inherently unstable on dynamic graphs. In response, temporal GNNs and streaming embeddings have been proposed to capture evolving structures without repeated expensive decompositions. We contribute a change-point-aware encoder that adaptively reweights historical context to stabilize detection.",
    "reason": "Mentions a 'previous study' and its claim without citing the source (definition a and e).",
    "start": 115,
    "end": 204,
    "label": "Unsupported_claim"
  },
  {
    "span": "[27]",
    "document": "Related Work\n\nVision backbones have evolved from convolutional networks to transformers, trading inductive bias for scalability (He et al., 2016; Dosovitskiy et al., 2020; Liu et al., 2021). Hybrid architectures reintroduce locality and multi-scale features to improve dense prediction (Graham et al., 2021; Chen et al., 2017). Pretraining with contrastive and masked image modeling objectives further boosts downstream performance (Caron et al., 2021; He et al., 2022).\n\nWhile most works report gains on ImageNet and COCO, few analyze robustness to image corruptions and distribution shifts (Hendrycks and Dietterich, 2019; Taori et al., 2020). As shown by [27], calibration often degrades as capacity grows, implicating overconfident softmax distributions. We address this by integrating test-time augmentations with temperature scaling tailored to frequency components.\n",
    "reason": "Numeric bracket citation is inconsistent with the surrounding author–year style.",
    "start": 658,
    "end": 662,
    "label": "Format"
  },
  {
    "span": "There are many recent works that explore safety alignment for multilingual dialogue agents.",
    "document": "Related Work\n\nSafety alignment for conversational AI encompasses toxicity avoidance, misinformation mitigation, and refusal handling under adversarial prompts (Bender et al., 2021; Weidinger et al., 2022). Alignment techniques include RL from human feedback, red-teaming datasets, and policy distillation (Ouyang et al., 2022; Bai et al., 2022). Multilinguality introduces additional challenges due to code-switching, dialectal variation, and culturally specific norms (Costa-jussà et al., 2022; Conneau et al., 2020).\n\nThere are many recent works that explore safety alignment for multilingual dialogue agents. In contrast, we curate a cross-lingual safety benchmark with aligned prompts and culturally grounded violation taxonomies, and we propose a translation-robust policy learning objective.",
    "reason": "Mentions 'many recent works' without providing citations to any of them (rule d).",
    "start": 520,
    "end": 611,
    "label": "Unsupported_claim"
  },
  {
    "span": "Lee et al. (2020) fine-tuned BioBERT for biomedical named entity recognition. Bodenreider (2004) presented UMLS as an integrated metathesaurus. Purcell et al. (2016) examined phenotype extraction from EHRs using rule-based heuristics.",
    "document": "Related Work\n\nBiomedical information extraction requires combining domain-specific language models with curated ontologies and clinical resources. Prior studies address entity recognition, relation extraction, and normalization to biomedical vocabularies.\n\nLee et al. (2020) fine-tuned BioBERT for biomedical named entity recognition. Bodenreider (2004) presented UMLS as an integrated metathesaurus. Purcell et al. (2016) examined phenotype extraction from EHRs using rule-based heuristics. Johnson et al. (2016) released MIMIC-III to facilitate clinical NLP research.\n\nIn contrast, we target cross-resource normalization with weak supervision that reduces reliance on large labeled corpora.",
    "reason": "The span lists methods and resources from different subareas (NER, ontology, phenotyping) without articulating how they interrelate or build on one another, creating abrupt shifts with no transitions.",
    "start": 257,
    "end": 491,
    "label": "Coherence"
  },
  {
    "span": "Secure aggregation masks client updates to protect privacy (Bonawitz et al., 2017). Personalized federated optimization introduces client-specific adapters (Arivazhagan et al., 2019). Meta-learning initializes models for quick adaptation across clients (Fallah et al., 2020).",
    "document": "Related Work\n\nPersonalized Federated Learning\n\nFederated learning enables collaborative model training across clients without sharing raw data (McMahan et al., 2017). Personalization mitigates client drift by adapting models to heterogeneous data distributions (Kairouz et al., 2021). Techniques include multi-task objectives, mixture models, adapters, and meta-learning. Secure aggregation masks client updates to protect privacy (Bonawitz et al., 2017). Personalized federated optimization introduces client-specific adapters (Arivazhagan et al., 2019). Meta-learning initializes models for quick adaptation across clients (Fallah et al., 2020). Clustered FL and partial model sharing further improve accuracy under heterogeneity (Mansour et al., 2020; Liang et al., 2020).",
    "reason": "The span mixes privacy-preserving aggregation with two personalization strategies without transitions or an explanation of their relationship, creating an abrupt narrative shift.",
    "start": 372,
    "end": 647,
    "label": "Coherence"
  },
  {
    "span": "BERT-based regressors have consistently surpassed feature-engineered AES models on all public essay scoring datasets.",
    "document": "Related Work\n\nAutomated essay scoring (AES) has evolved from handcrafted features and linear models toward neural encoders that capture discourse and semantic coherence. Pretrained language models fine-tuned with regression heads now dominate leaderboard-style evaluations and reduce reliance on prompt-specific engineering. BERT-based regressors have consistently surpassed feature-engineered AES models on all public essay scoring datasets. Recent studies explore cross-prompt generalization, fairness across demographic groups, and calibration for high-stakes use.\n\nHowever, concerns persist regarding spurious correlations with length, prompt leakage, and limited feedback utility. To address these issues, researchers investigate rationale extraction, multi-task objectives with trait scoring, and uncertainty quantification to flag low-confidence predictions.",
    "reason": "Strong comparative and universal claim about prior results across 'all public' datasets without any citations, violating rule (b)/(d).",
    "start": 325,
    "end": 442,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Li et al., 2020a;, 2021)",
    "document": "Related Work\n\nSpan-based sequence labeling directly classifies candidate spans rather than tagging tokens. This paradigm has achieved strong results in NER and relation extraction with context-aware span encoders and efficient pruning strategies (Peters and Zhao, 2019; Yu and Huang, 2020). Recent studies further refine negative sampling and incorporate boundary detection signals (Li et al., 2020a;, 2021), while others use lexicon cues to disambiguate overlapping mentions (Chen and Luo, 2021). We extend span classification with pattern-aware regularization to stabilize training.",
    "reason": "Punctuation error in citation list: extraneous semicolon before the comma inside the parenthetical citation. Should be \"(Li et al., 2020a, 2021)\".",
    "start": 382,
    "end": 407,
    "label": "Format"
  },
  {
    "span": "Data augmentation policies such as AutoAugment, RandAugment, and TrivialAugment improve accuracy (Cubuk et al., 2019; Cubuk et al., 2020; Müller and Hutter, 2021).",
    "document": "Related Work\nGeneralization in image classification benefits from strong data augmentation. Automated policy search and simple heuristics have both shown gains across datasets and architectures.\n\nData augmentation policies such as AutoAugment, RandAugment, and TrivialAugment improve accuracy (Cubuk et al., 2019; Cubuk et al., 2020; Müller and Hutter, 2021). Mix-based methods synthesize training examples by combining images or labels (Zhang et al., 2018; Yun et al., 2019). Distribution-aware augmentation adapts to dataset characteristics (Lim et al., 2019; Hataya et al., 2020). We report results on CIFAR, ImageNet, and long-tailed variants.",
    "reason": "The span lists well-known methods and citations without discussing how they relate to the paper’s approach, what limitations remain, or the gap the work intends to fill.",
    "start": 196,
    "end": 359,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The Adult dataset is the most commonly used benchmark for income prediction",
    "document": "Related Work\n\nFairness-aware machine learning research frequently evaluates methods on tabular classification tasks, including credit scoring, recidivism risk estimation, and income prediction. Standardization of datasets enables consistent comparisons but can also narrow methodological progress to idiosyncrasies of a few benchmarks.\n\nThe Adult dataset is the most commonly used benchmark for income prediction. While its prevalence facilitates reproducibility, the dataset’s age, feature engineering choices, and demographic shifts raise questions about external validity and fairness assumptions.\n\nOur study expands evaluation to a diverse suite of contemporary tabular datasets, analyzing whether fairness interventions calibrated on legacy data generalize to more realistic distributions.",
    "reason": "Asserts dataset dominance in the literature without citing surveys or studies to support the claim (rule a and b).",
    "start": 337,
    "end": 412,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior streaming methods rely on sketching (Aggarwal, 2003; Cormode and Muthukrishnan, 2005) or incremental clustering and subspace tracking (Gupta et al., 2014; Zhao et al., 2019). In this paper, we present StreamAD, a framework for memory-efficient anomaly detection in high-rate telemetry.",
    "document": "Introduction\n\nStreaming anomaly detection is critical for monitoring large-scale systems where data arrives at high velocity and storage is constrained. Methods must offer low latency, bounded memory, and robustness to nonstationarity while raising actionable alerts.\n\nPrior streaming methods rely on sketching (Aggarwal, 2003; Cormode and Muthukrishnan, 2005) or incremental clustering and subspace tracking (Gupta et al., 2014; Zhao et al., 2019). In this paper, we present StreamAD, a framework for memory-efficient anomaly detection in high-rate telemetry.\n\nWe instantiate StreamAD with a hierarchical reservoir and adaptive thresholds that respect end-to-end latency budgets. Our evaluation covers synthetic drifts and real production traces, highlighting throughput, detection delay, and false positive trade-offs.\n\nWe also report a sensitivity study on window sizes and drift rates, and release an open-source implementation with connectors for common telemetry platforms.",
    "reason": "The span transitions from a literature summary directly to the authors' contribution without explicitly identifying the gap, aligning with condition (b).",
    "start": 269,
    "end": 560,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Kairouz et al. (2021) survey advances in federated learning. Bonawitz et al. (2017) introduce secure aggregation protocols for federated optimization. McMahan et al. (2017) propose FedAvg for on-device training. Shokri et al. (2017) analyze membership inference attacks on machine learning models.",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative model training without centralizing raw data, raising important questions around systems design, privacy protections, and adversarial threats. The literature spans communication-efficient algorithms, cryptographic primitives, and privacy accounting, alongside attack and defense studies.\n\nKairouz et al. (2021) survey advances in federated learning. Bonawitz et al. (2017) introduce secure aggregation protocols for federated optimization. McMahan et al. (2017) propose FedAvg for on-device training. Shokri et al. (2017) analyze membership inference attacks on machine learning models.\n\nRecent work integrates differential privacy and secure aggregation into practical FL pipelines, and proposes robustness to byzantine clients. Our contribution targets label privacy leakage in cross-device FL, offering a defense compatible with secure aggregation and partial participation.",
    "reason": "The span lists a survey, a cryptographic protocol, an optimization algorithm, and an attack study with no connective tissue; the relationships and progression between them are not articulated.",
    "start": 349,
    "end": 646,
    "label": "Coherence"
  },
  {
    "span": "Prior studies measure and mitigate bias through counterfactual evaluation, data augmentation, adversarial training, and calibration (Zhao et al., 2017; Rudinger et al., 2018; Zhang et al., 2018; Kumar et al., 2019; Cho et al., 2021).",
    "document": "Related Work\n\nFairness in NLP requires both accurate estimation of disparities and methods that reduce harms across protected groups. Bias can arise from data curation, representation learning, and decision thresholds.\n\nPrior studies measure and mitigate bias through counterfactual evaluation, data augmentation, adversarial training, and calibration (Zhao et al., 2017; Rudinger et al., 2018; Zhang et al., 2018; Kumar et al., 2019; Cho et al., 2021).\n\nWe focus on post-hoc methods compatible with black-box models and propose a risk-controlled thresholding procedure that preserves utility under distribution shift.",
    "reason": "The span lists categories of techniques and citations without articulating how they inform the current work or what unresolved issue motivates the paper.",
    "start": 220,
    "end": 453,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Adversarial training remains the de facto defense, with variants such as TRADES (Zhang et al., 2019), MART (Wang et al., 2020), and free adversarial training (Shafahi et al., 2019). In this paper, we present a curriculum-based adversarial schedule.",
    "document": "Related Work\n\nNeural networks are vulnerable to adversarial examples, prompting research on defenses that improve worst-case robustness. Among these, adversarial training (AT) has become the prevailing approach by optimizing models on adversarially perturbed inputs.\n\nAdversarial training remains the de facto defense, with variants such as TRADES (Zhang et al., 2019), MART (Wang et al., 2020), and free adversarial training (Shafahi et al., 2019). In this paper, we present a curriculum-based adversarial schedule. Complementary directions investigate certified robustness using randomized smoothing and convex relaxations (Cohen et al., 2019; Wong and Kolter, 2018), and data-centric approaches like robust pretraining (Xie et al., 2020).\n\nOur method adapts perturbation budgets to sample hardness estimated online, aiming to improve both clean and robust accuracy under a fixed training cost.",
    "reason": "The span transitions directly from listing prior work to stating the contribution without identifying any specific gap or explaining how the proposed approach addresses limitations.",
    "start": 268,
    "end": 516,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Klein, 2015))",
    "document": "Introduction\n\nClassical object detection pipelines relied on hand-crafted features and region proposals, later supplanted by end-to-end detectors with convolutional backbones. Early region-based detectors established the two-stage paradigm with proposal generation followed by classification and refinement (Girshick, 2015; Ren et al., 2015). Classic pipelines (Klein, 2015)) were dominated by engineered features until deep learning methods achieved superior accuracy on large benchmarks. Subsequent single-stage detectors improved speed while closing the accuracy gap (Liu et al., 2016; Redmon et al., 2016).\n\nWe study the effect of training-time augmentations on calibration and long-tail performance in modern detectors.",
    "reason": "Extra closing parenthesis in the citation; it should be '(Klein, 2015)'.",
    "start": 361,
    "end": 375,
    "label": "Format"
  },
  {
    "span": "Miller et al. 1",
    "document": "Related Work\n\nAutomatic question generation (QG) for educational assessment spans template-based, syntactic transformation, and neural approaches. Early systems relied on templates harvested from dependency parses to formulate wh-questions (Heilman and Smith, 2010). Template-based methods can ensure grammaticality but often lack diversity (Rus et al., 2010). The approach by Miller et al. 1 improves scoring by modeling distractor plausibility, whereas later neural methods rely on encoder–decoder architectures trained on reading comprehension datasets to generate answer-focused questions (Du et al., 2017; Sun et al., 2018). Recent work studies controllability to target cognitive skills and difficulty levels (Wang et al., 2020; Chen et al., 2021).",
    "reason": "Improper use of a footnote-like marker without a year; should include a publication year or be formatted as a proper footnote/endnote.",
    "start": 377,
    "end": 392,
    "label": "Format"
  },
  {
    "span": " In contrast, the models typically applied in the entailment approach are Cross Attention (CA) models which need to be executed for every combination of text and label. ",
    "document": "Introduction\n\nFew-shot learning is the problem of learning classifiers with only a few training examples. Zero-shot learning (Larochelle et al., 2008), also known as dataless classification (Chang et al., 2008), is the extreme case, in which no labeled data is used. For text data, this is usually accomplished by representing the labels of the task in a textual form, which can either be the name of the label or a concise textual description.\n\nIn recent years, there has been a surge in zeroshot and few-shot approaches to text classification. One approach (Yin et al., 2019, 2020; Halder et al., 2020;Wang et al., 2021) makes use of entailment models. Textual entailment (Dagan et al., 2006), also known as natural language inference (NLI) (Bowman et al., 2015), is the problem of predicting whether a textual premise implies a textual hypothesis in a logical sense. For example, Emma loves apples implies that Emma likes apples.\n\nThe entailment approach for text classification sets the input text as the premise and the text repre-senting the label as the hypothesis. A NLI model is applied to each input pair and the entailment probability is used to identify the best matching label.\n\nIn this paper, we investigate an alternative based on Siamese Networks (SN) (Bromley et al., 1993), also known as dual encoders. These models embed both input and label texts into a common vector space. The similarity of the two items can then be computed using a similarity function such as the dot product. The advantage is that input and label text are encoded independently, which means that the label embeddings can be pre-computed. Therefore, at inference time, only a single call to the model per input is needed. In contrast, the models typically applied in the entailment approach are Cross Attention (CA) models which need to be executed for every combination of text and label. On the other hand, they allow for interaction between the tokens of label and input, so that in theory they should be superior in classification accuracy. However, in this work we show that in practice, the difference in quality is small.\n\nBoth CA and SNs also support the few-shot learning setup by fine-tuning the models on a small number of labeled examples. This is usually done by updating all parameters of the model, which in turn makes it impossible to share the models between different tasks. In this work, we show that when using a SN, one can decide to only fine-tune the label embeddings. We call this Label Tuning (LT). With LT the encoder can be shared between different tasks, which greatly eases the deployment of this approach in a production setup. LT comes with a certain drop in quality, but this drop can be compensated by using a variant of knowledge distillation (Hinton et al., 2014).\n\nOur contributions are as follows: We perform a large study on a diverse set of tasks showing that CA models and SN yield similar performance for both zero-shot and few-shot text classification.  (LT). At training time, input and label texts (hypotheses) are processed by the encoder. LT then tunes the labels using a cross entropy (CE) loss. At inference time, the input text is passed through the same encoder. The tuned label embeddings and a similarity function are then used to score each label. The encoder remains unchanged and can be shared between multiple tasks.\n\nIn contrast to most prior work, we also show that these results can also be achieved for languages other than English. We compare the hypothesis patterns commonly used in the literature and using the plain label name (null hypothesis) and find that on average there is no significant difference in performance. Finally, we present LT as an alternative to full fine-tuning that allows using the same model for many tasks and thus greatly increases the scalability of the method. We will release the code and trained models used in our experiments.\n\n ",
    "start": 1712,
    "end": 1881,
    "label": "Unsupported_claim"
  },
  {
    "span": "In contrast to most prior work, we also show that these results can also be achieved for languages other than English",
    "document": "Introduction\n\nFew-shot learning is the problem of learning classifiers with only a few training examples. Zero-shot learning (Larochelle et al., 2008), also known as dataless classification (Chang et al., 2008), is the extreme case, in which no labeled data is used. For text data, this is usually accomplished by representing the labels of the task in a textual form, which can either be the name of the label or a concise textual description.\n\nIn recent years, there has been a surge in zeroshot and few-shot approaches to text classification. One approach (Yin et al., 2019, 2020; Halder et al., 2020;Wang et al., 2021) makes use of entailment models. Textual entailment (Dagan et al., 2006), also known as natural language inference (NLI) (Bowman et al., 2015), is the problem of predicting whether a textual premise implies a textual hypothesis in a logical sense. For example, Emma loves apples implies that Emma likes apples.\n\nThe entailment approach for text classification sets the input text as the premise and the text repre-senting the label as the hypothesis. A NLI model is applied to each input pair and the entailment probability is used to identify the best matching label.\n\nIn this paper, we investigate an alternative based on Siamese Networks (SN) (Bromley et al., 1993), also known as dual encoders. These models embed both input and label texts into a common vector space. The similarity of the two items can then be computed using a similarity function such as the dot product. The advantage is that input and label text are encoded independently, which means that the label embeddings can be pre-computed. Therefore, at inference time, only a single call to the model per input is needed. In contrast, the models typically applied in the entailment approach are Cross Attention (CA) models which need to be executed for every combination of text and label. On the other hand, they allow for interaction between the tokens of label and input, so that in theory they should be superior in classification accuracy. However, in this work we show that in practice, the difference in quality is small.\n\nBoth CA and SNs also support the few-shot learning setup by fine-tuning the models on a small number of labeled examples. This is usually done by updating all parameters of the model, which in turn makes it impossible to share the models between different tasks. In this work, we show that when using a SN, one can decide to only fine-tune the label embeddings. We call this Label Tuning (LT). With LT the encoder can be shared between different tasks, which greatly eases the deployment of this approach in a production setup. LT comes with a certain drop in quality, but this drop can be compensated by using a variant of knowledge distillation (Hinton et al., 2014).\n\nOur contributions are as follows: We perform a large study on a diverse set of tasks showing that CA models and SN yield similar performance for both zero-shot and few-shot text classification.  (LT). At training time, input and label texts (hypotheses) are processed by the encoder. LT then tunes the labels using a cross entropy (CE) loss. At inference time, the input text is passed through the same encoder. The tuned label embeddings and a similarity function are then used to score each label. The encoder remains unchanged and can be shared between multiple tasks.\n\nIn contrast to most prior work, we also show that these results can also be achieved for languages other than English. We compare the hypothesis patterns commonly used in the literature and using the plain label name (null hypothesis) and find that on average there is no significant difference in performance. Finally, we present LT as an alternative to full fine-tuning that allows using the same model for many tasks and thus greatly increases the scalability of the method. We will release the code and trained models used in our experiments.\n\n ",
    "start": 3365,
    "end": 3482,
    "label": "Unsupported_claim"
  },
  {
    "span": "Few-shot learning is the problem of learning classifiers with only a few training examples.",
    "document": "Introduction\n\nFew-shot learning is the problem of learning classifiers with only a few training examples. Zero-shot learning (Larochelle et al., 2008), also known as dataless classification (Chang et al., 2008), is the extreme case, in which no labeled data is used. For text data, this is usually accomplished by representing the labels of the task in a textual form, which can either be the name of the label or a concise textual description.\n\nIn recent years, there has been a surge in zeroshot and few-shot approaches to text classification. One approach (Yin et al., 2019, 2020; Halder et al., 2020;Wang et al., 2021) makes use of entailment models. Textual entailment (Dagan et al., 2006), also known as natural language inference (NLI) (Bowman et al., 2015), is the problem of predicting whether a textual premise implies a textual hypothesis in a logical sense. For example, Emma loves apples implies that Emma likes apples.\n\nThe entailment approach for text classification sets the input text as the premise and the text repre-senting the label as the hypothesis. A NLI model is applied to each input pair and the entailment probability is used to identify the best matching label.\n\nIn this paper, we investigate an alternative based on Siamese Networks (SN) (Bromley et al., 1993), also known as dual encoders. These models embed both input and label texts into a common vector space. The similarity of the two items can then be computed using a similarity function such as the dot product. The advantage is that input and label text are encoded independently, which means that the label embeddings can be pre-computed. Therefore, at inference time, only a single call to the model per input is needed. In contrast, the models typically applied in the entailment approach are Cross Attention (CA) models which need to be executed for every combination of text and label. On the other hand, they allow for interaction between the tokens of label and input, so that in theory they should be superior in classification accuracy. However, in this work we show that in practice, the difference in quality is small.\n\nBoth CA and SNs also support the few-shot learning setup by fine-tuning the models on a small number of labeled examples. This is usually done by updating all parameters of the model, which in turn makes it impossible to share the models between different tasks. In this work, we show that when using a SN, one can decide to only fine-tune the label embeddings. We call this Label Tuning (LT). With LT the encoder can be shared between different tasks, which greatly eases the deployment of this approach in a production setup. LT comes with a certain drop in quality, but this drop can be compensated by using a variant of knowledge distillation (Hinton et al., 2014).\n\nOur contributions are as follows: We perform a large study on a diverse set of tasks showing that CA models and SN yield similar performance for both zero-shot and few-shot text classification.  (LT). At training time, input and label texts (hypotheses) are processed by the encoder. LT then tunes the labels using a cross entropy (CE) loss. At inference time, the input text is passed through the same encoder. The tuned label embeddings and a similarity function are then used to score each label. The encoder remains unchanged and can be shared between multiple tasks.\n\nIn contrast to most prior work, we also show that these results can also be achieved for languages other than English. We compare the hypothesis patterns commonly used in the literature and using the plain label name (null hypothesis) and find that on average there is no significant difference in performance. Finally, we present LT as an alternative to full fine-tuning that allows using the same model for many tasks and thus greatly increases the scalability of the method. We will release the code and trained models used in our experiments.\n\n ",
    "start": 14,
    "end": 105,
    "label": "Unsupported_claim"
  },
  {
    "span": "Both CA and SNs also support the few-shot learning setup by fine-tuning the models on a small number of labeled examples",
    "document": "Introduction\n\nFew-shot learning is the problem of learning classifiers with only a few training examples. Zero-shot learning (Larochelle et al., 2008), also known as dataless classification (Chang et al., 2008), is the extreme case, in which no labeled data is used. For text data, this is usually accomplished by representing the labels of the task in a textual form, which can either be the name of the label or a concise textual description.\n\nIn recent years, there has been a surge in zeroshot and few-shot approaches to text classification. One approach (Yin et al., 2019, 2020; Halder et al., 2020;Wang et al., 2021) makes use of entailment models. Textual entailment (Dagan et al., 2006), also known as natural language inference (NLI) (Bowman et al., 2015), is the problem of predicting whether a textual premise implies a textual hypothesis in a logical sense. For example, Emma loves apples implies that Emma likes apples.\n\nThe entailment approach for text classification sets the input text as the premise and the text repre-senting the label as the hypothesis. A NLI model is applied to each input pair and the entailment probability is used to identify the best matching label.\n\nIn this paper, we investigate an alternative based on Siamese Networks (SN) (Bromley et al., 1993), also known as dual encoders. These models embed both input and label texts into a common vector space. The similarity of the two items can then be computed using a similarity function such as the dot product. The advantage is that input and label text are encoded independently, which means that the label embeddings can be pre-computed. Therefore, at inference time, only a single call to the model per input is needed. In contrast, the models typically applied in the entailment approach are Cross Attention (CA) models which need to be executed for every combination of text and label. On the other hand, they allow for interaction between the tokens of label and input, so that in theory they should be superior in classification accuracy. However, in this work we show that in practice, the difference in quality is small.\n\nBoth CA and SNs also support the few-shot learning setup by fine-tuning the models on a small number of labeled examples. This is usually done by updating all parameters of the model, which in turn makes it impossible to share the models between different tasks. In this work, we show that when using a SN, one can decide to only fine-tune the label embeddings. We call this Label Tuning (LT). With LT the encoder can be shared between different tasks, which greatly eases the deployment of this approach in a production setup. LT comes with a certain drop in quality, but this drop can be compensated by using a variant of knowledge distillation (Hinton et al., 2014).\n\nOur contributions are as follows: We perform a large study on a diverse set of tasks showing that CA models and SN yield similar performance for both zero-shot and few-shot text classification.  (LT). At training time, input and label texts (hypotheses) are processed by the encoder. LT then tunes the labels using a cross entropy (CE) loss. At inference time, the input text is passed through the same encoder. The tuned label embeddings and a similarity function are then used to score each label. The encoder remains unchanged and can be shared between multiple tasks.\n\nIn contrast to most prior work, we also show that these results can also be achieved for languages other than English. We compare the hypothesis patterns commonly used in the literature and using the plain label name (null hypothesis) and find that on average there is no significant difference in performance. Finally, we present LT as an alternative to full fine-tuning that allows using the same model for many tasks and thus greatly increases the scalability of the method. We will release the code and trained models used in our experiments.\n\n ",
    "start": 2121,
    "end": 2241,
    "label": "Unsupported_claim"
  },
  {
    "span": "In recent years, there has been a surge in zeroshot and few-shot approaches to text classification. One approach (Yin et al., 2019, 2020; Halder et al., 2020;Wang et al., 2021) makes use of entailment models. Textual entailment (Dagan et al., 2006), also known as natural language inference (NLI) (Bowman et al., 2015), is the problem of predicting whether a textual premise implies a textual hypothesis in a logical sense. For example, Emma loves apples implies that Emma likes apples.\n",
    "document": "Introduction\n\nFew-shot learning is the problem of learning classifiers with only a few training examples. Zero-shot learning (Larochelle et al., 2008), also known as dataless classification (Chang et al., 2008), is the extreme case, in which no labeled data is used. For text data, this is usually accomplished by representing the labels of the task in a textual form, which can either be the name of the label or a concise textual description.\n\nIn recent years, there has been a surge in zeroshot and few-shot approaches to text classification. One approach (Yin et al., 2019, 2020; Halder et al., 2020;Wang et al., 2021) makes use of entailment models. Textual entailment (Dagan et al., 2006), also known as natural language inference (NLI) (Bowman et al., 2015), is the problem of predicting whether a textual premise implies a textual hypothesis in a logical sense. For example, Emma loves apples implies that Emma likes apples.\n\nThe entailment approach for text classification sets the input text as the premise and the text repre-senting the label as the hypothesis. A NLI model is applied to each input pair and the entailment probability is used to identify the best matching label.\n\nIn this paper, we investigate an alternative based on Siamese Networks (SN) (Bromley et al., 1993), also known as dual encoders. These models embed both input and label texts into a common vector space. The similarity of the two items can then be computed using a similarity function such as the dot product. The advantage is that input and label text are encoded independently, which means that the label embeddings can be pre-computed. Therefore, at inference time, only a single call to the model per input is needed. In contrast, the models typically applied in the entailment approach are Cross Attention (CA) models which need to be executed for every combination of text and label. On the other hand, they allow for interaction between the tokens of label and input, so that in theory they should be superior in classification accuracy. However, in this work we show that in practice, the difference in quality is small.\n\nBoth CA and SNs also support the few-shot learning setup by fine-tuning the models on a small number of labeled examples. This is usually done by updating all parameters of the model, which in turn makes it impossible to share the models between different tasks. In this work, we show that when using a SN, one can decide to only fine-tune the label embeddings. We call this Label Tuning (LT). With LT the encoder can be shared between different tasks, which greatly eases the deployment of this approach in a production setup. LT comes with a certain drop in quality, but this drop can be compensated by using a variant of knowledge distillation (Hinton et al., 2014).\n\nOur contributions are as follows: We perform a large study on a diverse set of tasks showing that CA models and SN yield similar performance for both zero-shot and few-shot text classification.  (LT). At training time, input and label texts (hypotheses) are processed by the encoder. LT then tunes the labels using a cross entropy (CE) loss. At inference time, the input text is passed through the same encoder. The tuned label embeddings and a similarity function are then used to score each label. The encoder remains unchanged and can be shared between multiple tasks.\n\nIn contrast to most prior work, we also show that these results can also be achieved for languages other than English. We compare the hypothesis patterns commonly used in the literature and using the plain label name (null hypothesis) and find that on average there is no significant difference in performance. Finally, we present LT as an alternative to full fine-tuning that allows using the same model for many tasks and thus greatly increases the scalability of the method. We will release the code and trained models used in our experiments.\n\n ",
    "start": 446,
    "end": 933,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Smith et al. 2020)",
    "document": "Introduction\n\nUnsupervised domain adaptation aims to transfer models from labeled source domains to unlabeled target domains (Ganin et al., 2016). A common approach matches feature distributions to reduce domain discrepancy (Tzeng et al., 2017; Long et al., 2018). Self-training methods exploit pseudo-labels to refine target representations (Zou et al., 2019). Recent contrastive techniques align instance-level features using temperature-scaled objectives (Smith et al. 2020; He et al., 2020). We build on these by introducing consistency regularization across multiple stochastic views (Xie et al., 2020).",
    "reason": "Missing comma between authors and year in a parenthetical citation; should be '(Smith et al., 2020)'.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "Garcia et al.",
    "document": "Introduction\n\nFederated learning enables collaborative model training without centralizing raw data, thereby improving privacy and regulatory compliance (McMahan et al., 2017; Kairouz et al., 2021). Early methods focused on communication efficiency and robustness to non-IID data (Bonawitz et al., 2019; Li et al., 2020). Personalization has emerged as a key challenge, with approaches ranging from multi-task learning to meta-learning (Fallah et al., 2020; Dinh et al., 2021). Building on prior personalization strategies, Garcia et al. propose a meta-learning approach that adapts quickly to client-specific distributions while preserving global generalization. Recent work also studies fairness across heterogeneous clients to mitigate disparate performance (Mohri et al., 2019; Li et al., 2021). In this paper, we examine the trade-offs between global accuracy, personalization, and communication efficiency, complementing analyses in (Hsu et al., 2019) and (Reddi et al., 2021).",
    "reason": "Narrative citation is missing the publication year; should be formatted as Garcia et al. (YEAR) or rephrased as a parenthetical citation.",
    "start": 524,
    "end": 537,
    "label": "Format"
  },
  {
    "span": "(Nguyen and Chen, 2020",
    "document": "Related Work\n\nThe robustness of QA systems to distribution shift has been addressed via data augmentation (Fang et al., 2021) and domain-adversarial training (Ganin et al., 2016). Although synthetic paraphrase generation improves lexical coverage (Morales et al., 2019), it can distort answer spans under constrained decoding. Prior to adaptation (Nguyen and Chen, 2020 a simple fine-tuning regime on a small in-domain set was shown to recover most of the lost accuracy, but this strategy fails under extreme label scarcity. Recent studies combine pseudo-labeling with agreement regularization (Lee et al., 2021; Ortega et al., 2022).",
    "reason": "Missing closing parenthesis in the parenthetical citation.",
    "start": 347,
    "end": 369,
    "label": "Format"
  },
  {
    "span": "Adversarial training reduces word error rate by roughly 20% on average across public ASR benchmarks.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) systems deployed in the wild must contend with noise, reverberation, and channel variability. Robustness techniques such as data augmentation, domain adversarial training, and consistency regularization have been proposed to improve performance under distribution shift. However, the magnitude and consistency of gains vary widely across datasets and model architectures.\n\nAdversarial training reduces word error rate by roughly 20% on average across public ASR benchmarks. Despite this reported benefit, the mechanisms by which adversarial perturbations improve generalization remain debated, particularly when perturbations are constrained in perceptual rather than signal-space norms. We conduct a large-scale study of augmentation strength, perturbation budgets, and evaluation noise profiles to clarify when adversarial training helps and when it harms.\n\nWe further explore hybrid schedules that interleave adversarial updates with noise-invariant consistency losses, demonstrating improved robustness at matched compute.",
    "reason": "This is a quantitative claim about average performance improvements across benchmarks without any citation or evidence, which requires supporting references.",
    "start": 422,
    "end": 522,
    "label": "Unsupported_claim"
  },
  {
    "span": "Martinez et al.",
    "document": "Introduction. Semi-supervised learning (SSL) reduces annotation costs by leveraging unlabeled data alongside small labeled sets (Zhu & Goldberg, 2009). Consistency regularization and pseudo-labeling are two successful paradigms that improve generalization under distributional shifts (Sohn et al., 2020; Xie et al., 2020). As Martinez et al. demonstrate, ensembling over perturbations further stabilizes training in low-label regimes. However, SSL methods may inadvertently reinforce biases inherited from pretraining corpora (Gururangan et al., 2020), prompting interest in debiasing objectives and calibrated confidence thresholds (Guo et al., 2017; Minderer et al., 2021). We revisit these ideas for domain-shifted biomedical text, where label scarcity is particularly acute.",
    "reason": "Narrative citation missing year: 'Martinez et al.' should include the year as 'Martinez et al. (YEAR)'.",
    "start": 326,
    "end": 341,
    "label": "Format"
  },
  {
    "span": "Domain randomization broadens simulated variability to aid transfer (Tobin et al., 2017). Model-based control exploits learned or known dynamics for sample efficiency (Deisenroth and Rasmussen, 2013). End-to-end reinforcement learning scales with large datasets and off-policy algorithms (Kalashnikov et al., 2018).",
    "document": "Related Work\n\nRobotic learning from simulation and real data must contend with dynamics mismatch, sensing noise, and limited interaction budgets. Approaches vary widely in how they leverage models, demonstrations, and exploration.\n\nTransfer from simulation to reality (sim2real) is facilitated by representation learning, robust policies, and data augmentation. Safety constraints further complicate online adaptation in real-world settings.\n\nDomain randomization broadens simulated variability to aid transfer (Tobin et al., 2017). Model-based control exploits learned or known dynamics for sample efficiency (Deisenroth and Rasmussen, 2013). End-to-end reinforcement learning scales with large datasets and off-policy algorithms (Kalashnikov et al., 2018).\n\nOur method combines dynamics-aligned latent representations with policy distillation to bridge simulation and deployment. We evaluate on manipulation tasks with contact-rich interactions and partial observability.",
    "reason": "The span abruptly juxtaposes sim2real randomization, model-based control, and end-to-end RL without transitions or an explicit relational thread, leaving coherence to implication only (a, b).",
    "start": 443,
    "end": 758,
    "label": "Coherence"
  },
  {
    "span": "Knowledge graph completion has been studied with translational embeddings, bilinear factorization, and GNN-based encoders (Bordes et al., 2013; Trouillon et al., 2016; Sun et al., 2019; Schlichtkrull et al., 2018; Vashishth et al., 2020).",
    "document": "Related Work\n\nKnowledge graph completion\n\nInferring missing relations in knowledge graphs enables downstream applications such as question answering and entity linking. Methods vary in inductive bias and scalability, from simple translational embeddings to expressive neural architectures.\n\nKnowledge graph completion has been studied with translational embeddings, bilinear factorization, and GNN-based encoders (Bordes et al., 2013; Trouillon et al., 2016; Sun et al., 2019; Schlichtkrull et al., 2018; Vashishth et al., 2020).\n\nLogical constraints and rules\n\nAnother line integrates symbolic constraints or differentiable rules to improve generalization and interpretability. While promising, these approaches can be brittle under noisy relations and require careful grounding.\n\nOur direction\n\nWe explore calibration-aware scoring functions that improve reliability under relation imbalance and entity frequency skew.",
    "reason": "The span provides a list of approaches and citations without connecting them to the authors’ direction or identifying a specific gap, constituting a lack of synthesis (criteria a and c).",
    "start": 291,
    "end": 529,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Garcia et al. 3",
    "document": "Related Work\n\nFederated learning enables collaborative training across decentralized data while preserving privacy (McMahan et al., 2017; Kairouz et al., 2021). Personalization techniques adapt global models to heterogeneous client distributions, using meta-learning, multi-task optimization, and representation sharing (Fallah et al., 2020; Smith et al., 2017).\n\nCommunication efficiency is a central challenge. Gradient compression, partial participation, and periodic averaging reduce bandwidth while maintaining accuracy (Stich, 2019; Karimireddy et al., 2020). Garcia et al. 3 introduced a sketch-based aggregation strategy to further compress updates under straggler constraints. In contrast, our approach leverages sparsity-inducing priors to jointly control communication and personalization, offering stronger guarantees under non-IID settings.\n\nWe evaluate on vision and language benchmarks with controlled heterogeneity and compare against strong baselines under strict communication budgets (Li et al., 2020; Reddi et al., 2021).",
    "reason": "Wrong use of footnote/numbering style appended to an author–year reference; it should include a publication year in parentheses or be formatted as a proper numeric citation (e.g., Garcia et al., 2019 or [3]).",
    "start": 566,
    "end": 581,
    "label": "Format"
  },
  {
    "span": "Most dialog datasets released since 2020 include turn-level emotion labels.",
    "document": "Introduction\n\nAffective computing in dialog systems seeks to recognize and respond to user emotions, enabling more natural and supportive interactions. Emotion recognition in conversation typically requires modeling speaker dynamics, context windows, and multimodal cues. Most dialog datasets released since 2020 include turn-level emotion labels. However, label taxonomies, annotation densities, and inter-annotator agreement vary substantially, complicating direct comparison across benchmarks. Our work introduces a taxonomy-agnostic evaluation protocol to facilitate fair cross-dataset assessment of emotion-aware dialog policies.\n",
    "reason": "Makes a broad claim about the prevalence of labeled datasets over a time period without any references or evidence.",
    "start": 272,
    "end": 347,
    "label": "Unsupported_claim"
  },
  {
    "span": "Contrastive learning maximizes agreement under augmentations (Chen et al., 2020). Clustering-based methods assign codes to images (Caron et al., 2018). Masked image modeling reconstructs patches (He et al., 2022).",
    "document": "Related Work\n\nSelf-supervised learning for visual representations has progressed rapidly, producing features competitive with supervised pretraining for many downstream tasks. A growing line of research explores how pretext objectives induce invariances and structure in learned embeddings, impacting transfer performance and data efficiency.\n\nContrastive learning maximizes agreement under augmentations (Chen et al., 2020). Clustering-based methods assign codes to images (Caron et al., 2018). Masked image modeling reconstructs patches (He et al., 2022).\n\nWhile each family of methods achieves strong results, the mechanisms by which they capture semantic relationships remain debated. We investigate a common alignment–uniformity perspective to compare these objectives under a shared evaluation protocol.",
    "reason": "The span lists three method families with no transitions or explanation of how they relate to each other or to the surrounding discussion, leaving implicit the connections and causing abrupt shifts.",
    "start": 344,
    "end": 557,
    "label": "Coherence"
  },
  {
    "span": "Large language models have been probed using prompt engineering and in-context learning (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021), explanations via post-hoc rationales (Lei et al., 2016; Rajani et al., 2019; Camburu et al., 2018), attribution and feature importance (Sundararajan et al., 2017; Shrikumar et al., 2017), and concept activation methods (Kim et al., 2018; Ghorbani et al., 2019). Causal mediation and counterfactual analysis have been explored to study mechanism-level behavior (Vig et al., 2020; Feder et al., 2021; Geiger et al., 2020).",
    "document": "Related Work\n\nUnderstanding and explaining the behavior of large language models (LLMs) is essential for safety, reliability, and scientific insight. Prior studies analyze representations, control mechanisms, and emergent behaviors.\n\nLarge language models have been probed using prompt engineering and in-context learning (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021), explanations via post-hoc rationales (Lei et al., 2016; Rajani et al., 2019; Camburu et al., 2018), attribution and feature importance (Sundararajan et al., 2017; Shrikumar et al., 2017), and concept activation methods (Kim et al., 2018; Ghorbani et al., 2019). Causal mediation and counterfactual analysis have been explored to study mechanism-level behavior (Vig et al., 2020; Feder et al., 2021; Geiger et al., 2020).\n\nOur work introduces a task-grounded intervention protocol that links circuit-level analysis with external supervision signals, enabling targeted behavioral adjustments without fine-tuning.",
    "reason": "The span summarizes prior techniques but does not relate them to the authors’ protocol or identify a specific gap, fulfilling (a) and (b).",
    "start": 234,
    "end": 800,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Prior competitions like RoboCup@Home and DARPA SubT have emphasized sim-to-real transfer as a primary metric.",
    "document": "Related Work\n\nRobotic learning has benefited from standardized tasks and community challenges that catalyze progress on perception and control. Prior competitions like RoboCup@Home and DARPA SubT have emphasized sim-to-real transfer as a primary metric. This emphasis has spurred algorithmic innovations in domain randomization, adaptive control, and multi-modal perception.\n\nWhile simulation offers scale and safety, discrepancies in sensor noise, dynamics, and contact modeling can degrade field performance. Recent system-level approaches incorporate on-robot adaptation to reduce this gap but often require careful engineering and environment-specific tuning.\n\nOur approach focuses on policy regularization via dynamics-aware augmentation to improve robustness without environment-specific calibration. We demonstrate improvements on mobile manipulation tasks in both simulation and real-world trials.",
    "reason": "References specific competitions and claims about their metrics without any citations.",
    "start": 144,
    "end": 253,
    "label": "Unsupported_claim"
  },
  {
    "span": "Explainable recommendation methods span model-intrinsic attention-based architectures (Chen et al., 2018; Seo et al., 2017), post-hoc feature attribution using SHAP and Integrated Gradients (Lundberg and Lee, 2017; Sundararajan et al., 2017), and counterfactual explanations based on perturbation or causal modeling (Mothilal et al., 2020; Tan et al., 2021). We present a new explanation module for session-based recommendation.",
    "document": "Related Work\n\nAs recommender systems mediate critical decisions, there is growing interest in generating faithful and useful explanations for users and practitioners. Existing efforts include inherently interpretable models and model-agnostic post-hoc techniques.\n\nExplainable recommendation methods span model-intrinsic attention-based architectures (Chen et al., 2018; Seo et al., 2017), post-hoc feature attribution using SHAP and Integrated Gradients (Lundberg and Lee, 2017; Sundararajan et al., 2017), and counterfactual explanations based on perturbation or causal modeling (Mothilal et al., 2020; Tan et al., 2021). We present a new explanation module for session-based recommendation.\n\nEvaluation protocols range from simulator-based plausibility to human-in-the-loop assessments, but standardization remains limited (Zhang and Chen, 2020).",
    "reason": "The span enumerates prior methods and then immediately states the authors present a new module without specifying what gap it fills or how it differs, which lacks synthesis (criterion b).",
    "start": 265,
    "end": 693,
    "label": "Lacks_synthesis"
  },
  {
    "span": "McMahan et al. (2017) introduced federated averaging for decentralized training. Differential privacy mechanisms calibrate noise to bound information leakage (Abadi et al., 2016). Compression reduces uplink bandwidth (Konečný et al., 2016). Li et al. (2020) propose FedProx to stabilize training under heterogeneity.",
    "document": "Related Work\n\nFoundations of Federated Learning. Federated learning (FL) enables training machine learning models on-device without centralizing raw data, thereby reducing privacy risks while leveraging distributed computation (Kairouz et al., 2021). Early works focus on communication-efficient optimization and robustness to client heterogeneity (McMahan et al., 2017; Konečný et al., 2016). Subsequent efforts study fairness across clients with non-IID data and personalization techniques that adapt a global model to local preferences (Li et al., 2021; Fallah et al., 2020).\n\nPersonalization and Heterogeneity. Personalization approaches include model interpolation between global and local parameters, meta-learning for rapid on-device adaptation, and clustered FL that learns multiple specialized global models (Mansour et al., 2020; Arivazhagan et al., 2019; Dinh et al., 2020). These methods typically address objective inconsistency and statistical skew but raise open questions about stability and generalization under participation dynamics.\n\nPrivacy and Communication. McMahan et al. (2017) introduced federated averaging for decentralized training. Differential privacy mechanisms calibrate noise to bound information leakage (Abadi et al., 2016). Compression reduces uplink bandwidth (Konečný et al., 2016). Li et al. (2020) propose FedProx to stabilize training under heterogeneity. While secure aggregation cryptographically protects updates from the server (Bonawitz et al., 2017), practical deployments must balance privacy budgets with accuracy and communication frequency (Geyer et al., 2017). Our work focuses on privacy-utility trade-offs when client sampling is sparse and data are highly non-IID.",
    "reason": "The four sentences list disparate works (federated averaging, differential privacy, compression, FedProx) without transitions or explicit relations among them, making the connection between privacy and heterogeneity topics abrupt and unclear.",
    "start": 1081,
    "end": 1397,
    "label": "Coherence"
  },
  {
    "span": "Graph-based recommenders employ message passing on user-item bipartite graphs (Wang et al., 2019; He et al., 2020), attention-enhanced neighborhood aggregation (Huang et al., 2021; Fan et al., 2019), and self-supervised objectives such as contrastive learning (Yu et al., 2022; Sun et al., 2020). Side information like text and images has been integrated via multi-modal GNNs (Wei et al., 2020; Cen et al., 2020).",
    "document": "Related Work Personalized recommendation has increasingly turned to graph neural networks (GNNs) to model higher-order collaborative signals. Such methods promise improved data efficiency and robustness by propagating preferences across the interaction graph. Graph-based recommenders employ message passing on user-item bipartite graphs (Wang et al., 2019; He et al., 2020), attention-enhanced neighborhood aggregation (Huang et al., 2021; Fan et al., 2019), and self-supervised objectives such as contrastive learning (Yu et al., 2022; Sun et al., 2020). Side information like text and images has been integrated via multi-modal GNNs (Wei et al., 2020; Cen et al., 2020). Despite advances, deployed systems face traffic shifts and exposure bias that degrade online performance. We propose an exposure-aware contrastive pretraining scheme that decouples popularity from relevance by leveraging logged propensities and synthetic counterfactual views. Extensive experiments on two industrial datasets demonstrate improvements in calibrated CTR and cold-start coverage.",
    "reason": "The span catalogs categories of graph-based recommenders and multimodal extensions without explaining how they relate to the exposure bias challenges or the proposed solution. It lacks explicit synthesis and motivation (a, c).",
    "start": 260,
    "end": 673,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Graph-based recommenders have explored neighborhood aggregation and message passing in various forms, including NGCF (Wang et al., 2019), LightGCN (He et al., 2020), PinSage (Ying et al., 2018), and GAT-based variants (Velickovic et al., 2018; Wang et al., 2020). Several works also incorporate side information through heterogeneous graphs or knowledge graphs (Wang et al., 2018; Sun et al., 2019; Hu et al., 2018).",
    "document": "Related Work\n\nRecommender systems aim to infer user preferences from historical interactions, auxiliary content, and contextual signals. With the proliferation of interaction data on large platforms, graph-based methods have become a natural fit to model high-order connectivity patterns among users and items.\n\nGraph-based recommenders have explored neighborhood aggregation and message passing in various forms, including NGCF (Wang et al., 2019), LightGCN (He et al., 2020), PinSage (Ying et al., 2018), and GAT-based variants (Velickovic et al., 2018; Wang et al., 2020). Several works also incorporate side information through heterogeneous graphs or knowledge graphs (Wang et al., 2018; Sun et al., 2019; Hu et al., 2018).\n\nThis paper proposes a graph recommender with a context-aware sampler and adaptive propagation depth. We evaluate the approach on three public benchmarks and report improvements over strong baselines.",
    "reason": "The span enumerates prior work without explaining how these approaches relate to the paper's aims or why particular design choices are needed, offering no synthesis or author perspective.",
    "start": 312,
    "end": 728,
    "label": "Lacks_synthesis"
  },
  {
    "span": "We follow the annotation guidelines from the SemEval 2022 Task 11 shared task",
    "document": "Introduction\n\nAspect-based sentiment analysis (ABSA) seeks to extract fine-grained opinions tied to specific entities and attributes. High-quality annotations are crucial for enabling robust generalization across domains.\n\nWe follow the annotation guidelines from the SemEval 2022 Task 11 shared task and adapt them for multilingual settings with domain-specific extensions. This produces a consistent schema across product reviews and social media posts.\n\nOur contributions include a new cross-domain ABSA corpus and a constrained decoding approach that enforces schema constraints during inference.",
    "reason": "Mentions a specific shared task and its guidelines without citing the task description or proceedings (rule a).",
    "start": 223,
    "end": 300,
    "label": "Unsupported_claim"
  },
  {
    "span": "the ImageNet dataset remains the de facto pretraining resource for visual feature learning.",
    "document": "Related Work\n\nTransfer learning in computer vision typically relies on large-scale supervised pretraining followed by fine-tuning on target tasks. While alternative supervision signals such as self-supervision and multimodal alignment have emerged, the ImageNet dataset remains the de facto pretraining resource for visual feature learning. Recent work explores domain-adaptive pretraining for medical and aerial imagery, as well as task-specific adapters that reduce catastrophic forgetting. Despite progress, scaling pretraining beyond curated image classification has practical challenges in data quality, licensing, and long-tail distribution coverage. Our study examines pretraining with noisy web-scale data and assesses its impact on fine-grained recognition benchmarks.",
    "reason": "Introduces a widely used dataset with a strong normative claim at first mention but provides no citation to support it.",
    "start": 249,
    "end": 340,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Chen et al., 2017 and Li et al., 2018)",
    "document": "Related Work\n\nOpen-domain dialogue systems increasingly rely on large pre-trained language models fine-tuned with conversational objectives (Zhang et al., 2020; Roller et al., 2021). Retrieval-augmented generation improves factuality by conditioning on evidence passages (Lewis et al., 2020). For training data construction, weak supervision via heuristics and distant signals has been explored to scale up corpora (Chen et al., 2017; Wolf et al., 2019). See (Chen et al., 2017 and Li et al., 2018) for early neural approaches to persona consistency and style transfer in multi-turn settings. We focus on controllable response planning grounded in user profiles and recent context.",
    "reason": "Improper separator within a parenthetical list; citations inside parentheses should be separated by semicolons, e.g., “(Chen et al., 2017; Li et al., 2018)”.",
    "start": 459,
    "end": 498,
    "label": "Format"
  },
  {
    "span": "The WebQuestionsSP dataset has become the standard for evaluating complex compositional queries.",
    "document": "Related Work\n\nQuestion answering over knowledge graphs (KGQA) requires mapping natural language to structured queries while handling entity linking and relation composition. Benchmarks play a crucial role in tracking progress across compositional complexity levels.\n\nThe WebQuestionsSP dataset has become the standard for evaluating complex compositional queries. Recent models leverage neural symbolic methods, contrastive learning for relation paths, and constrained decoding over query graphs.\n\nOur work focuses on compositional generalization by introducing a factorized training objective that separates entity disambiguation from relation path induction.",
    "reason": "Introduces a specific dataset and asserts its standard status without providing a citation at first mention.",
    "start": 267,
    "end": 363,
    "label": "Unsupported_claim"
  },
  {
    "span": "In a previous study, the authors claim that temperature scaling beyond 10 leads to degraded calibration across tasks.",
    "document": "Related Work\n\nKnowledge distillation and model calibration are closely related: the former transfers information from a teacher to a student, while the latter adjusts confidence to better match empirical correctness. Temperature scaling is a simple post-hoc calibration method commonly applied to neural classifiers.\n\nIn a previous study, the authors claim that temperature scaling beyond 10 leads to degraded calibration across tasks. Other lines of work investigate label smoothing and focal losses to mitigate overconfidence, while uncertainty-aware training incorporates Bayesian approximations to quantify predictive variance. Despite these advances, little is known about how calibration interacts with cross-domain generalization in few-shot regimes, which is the focus of our empirical analysis.",
    "reason": "References an unspecified 'previous study' and summarizes its claim without citing it (definition b/ii).",
    "start": 318,
    "end": 435,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior work has conclusively shown that character-level modeling eliminates the need for word segmentation in Chinese NER.",
    "document": "Related Work\n\nNeural sequence labeling methods have achieved strong results on named entity recognition (NER) through contextual encoders and conditional decoding. While subword modeling reduces out-of-vocabulary errors in alphabetic languages, Chinese NER introduces the additional choice between character- and word-based representations. Several studies incorporate lexicon features or lattice structures to alleviate segmentation errors. Prior work has conclusively shown that character-level modeling eliminates the need for word segmentation in Chinese NER. Nevertheless, domain shift (e.g., newswire to social media) and few-shot adaptation continue to degrade performance, motivating approaches that combine robust pretraining with structure-aware decoding.",
    "reason": "Makes a definitive claim about findings from prior work without any supporting citations (rule a, b).",
    "start": 442,
    "end": 563,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT has been widely used in AES on ASAP and HSK essays with multi-task setups.",
    "document": "Introduction\n\nAutomatic Essay Scoring (AES) aims to predict holistic or trait-specific scores for student essays. It supports formative feedback at scale and reduces grading workload. Neural encoders have improved AES by capturing discourse structure and semantics beyond surface features. Transfer learning further promises robustness across prompts and domains.\n\nBERT has been widely used in AES on ASAP and HSK essays with multi-task setups.\n\nNevertheless, domain shift across prompts and proficiency levels remains challenging. We introduce a domain-adaptive contrastive framework that aligns prompt-invariant representations while preserving rubric-specific nuances.",
    "reason": "Claims specific prior usage of BERT in AES and references particular datasets ('ASAP' and 'HSK') without citations at first mention (violates rule a and b).",
    "start": 365,
    "end": 444,
    "label": "Unsupported_claim"
  },
  {
    "span": "Adversarial prompting and jailbreak attacks on large language models have been cataloged alongside defenses such as instruction tuning, rule-based filters, and reinforcement learning from human feedback (Perez et al., 2022; Zou et al., 2023; Ganguli et al., 2022; Bai et al., 2022; Wei et al., 2021).",
    "document": "Introduction\n\nLarge language models (LLMs) are susceptible to prompt-based attacks that elicit harmful or policy-violating outputs. Safety mechanisms often degrade utility or are brittle to paraphrase. We study robustness under distributional shift in attack phrasing and context.\n\nAdversarial prompting and jailbreak attacks on large language models have been cataloged alongside defenses such as instruction tuning, rule-based filters, and reinforcement learning from human feedback (Perez et al., 2022; Zou et al., 2023; Ganguli et al., 2022; Bai et al., 2022; Wei et al., 2021).\n\nOur method introduces contrastive safety fine-tuning with adversarial paraphrase augmentation to improve generalization across unseen attack families while maintaining task performance.",
    "reason": "The span catalogs attacks and defenses but does not connect them to the distributional shift problem or motivate the need for the proposed approach, thus lacking synthesis (criteria a and c).",
    "start": 282,
    "end": 582,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Brown et al.",
    "document": "Introduction\n\nCurriculum learning orders training examples to match model competence, improving convergence and sometimes generalization (Bengio et al., 2009; Graves et al., 2017). Subsequent research formulates curricula as optimization problems over data schedules (Kumar et al., 2010; Fan et al., 2018). For a broader survey, see (Brown et al.) and references therein. We contribute a self-paced scheduler that adapts to both instance difficulty and distributional coverage.\n",
    "reason": "Parenthetical citation missing the publication year. It should be completed as an author–year citation, e.g., \"(Brown et al., 2021)\" or reformatted as a narrative citation with a year.",
    "start": 334,
    "end": 346,
    "label": "Format"
  },
  {
    "span": "Inverse propensity scoring reweights logged interactions to correct for exposure bias (Rosenbaum and Rubin, 1983; Schnabel et al., 2016). Doubly robust estimators combine IPS with outcome models to reduce variance (Dudík et al., 2011; Wang et al., 2019). User simulators approximate counterfactual outcomes via learned environment dynamics (Ie et al., 2019). Interleaving-based online tests compare rankers within-session (Chapelle et al., 2012).",
    "document": "Related Work\n\nCounterfactual Evaluation in Recommender Systems\nOffline evaluation is challenged by selection bias in logs and the inability to observe unexposed recommendations (Swaminathan and Joachims, 2015). Methods draw from causal inference to obtain unbiased or low-variance estimates under logging policies.\n\nEstimators and Alternatives\nInverse propensity scoring reweights logged interactions to correct for exposure bias (Rosenbaum and Rubin, 1983; Schnabel et al., 2016). Doubly robust estimators combine IPS with outcome models to reduce variance (Dudík et al., 2011; Wang et al., 2019). User simulators approximate counterfactual outcomes via learned environment dynamics (Ie et al., 2019). Interleaving-based online tests compare rankers within-session (Chapelle et al., 2012). Recent advances incorporate self-normalization and learned propensities to stabilize estimates (Swaminathan and Joachims, 2015; Su et al., 2020).\n\nAssumptions and Practicalities\nUnconfoundedness and support overlap are often violated in real systems, motivating diagnostics and robust estimation techniques (Kallus, 2020). Our framework unifies diagnostics with estimator selection under resource constraints.",
    "reason": "The span abruptly strings together IPS, doubly robust estimators, simulators, and interleaving without transitions or explaining connections, making the relationship among cited works unclear.",
    "start": 344,
    "end": 790,
    "label": "Coherence"
  },
  {
    "span": "Klein et al. 1",
    "document": "Introduction\n\nOffline reinforcement learning (RL) aims to learn effective policies from fixed datasets without additional environment interaction, reducing safety and cost concerns (Levine et al., 2020). Klein et al. 1 propose conservative value estimates to mitigate distributional shift, complementing behavior-regularized policy optimization (Fujimoto et al., 2019; Kumar et al., 2020). Despite promising results, these methods can be overly pessimistic, prompting interest in uncertainty-aware target networks and model-based calibration (Uehara and Jiang, 2021; Kidambi et al., 2021).",
    "reason": "Wrong use of footnotes/numerals in place of a proper citation. It should include the year (e.g., \"Klein et al. (YEAR)\") or be formatted as a proper footnote/endnote.",
    "start": 204,
    "end": 218,
    "label": "Format"
  },
  {
    "span": "A number of post-hoc explanation methods have been introduced for GNNs, including gradient-based and perturbation-based techniques (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2021).",
    "document": "Related Work\n\nExplainability in Graph Neural Networks. As GNNs see growing adoption in scientific discovery and decision-making, understanding their predictions becomes crucial. A number of post-hoc explanation methods have been introduced for GNNs, including gradient-based and perturbation-based techniques (Ying et al., 2019; Pope et al., 2019; Luo et al., 2020; Yuan et al., 2021). Causal and counterfactual perspectives further aim to uncover invariant rationales across perturbations (Wang et al., 2022; Lucic et al., 2022). Prototype and concept-based explanations have also been explored to provide higher-level semantics (Zhang et al., 2022; Magister et al., 2021).\n\nEvaluation Protocols. Benchmarks for explanation quality vary widely, spanning fidelity, sparsity, stability, and human-centered criteria (Agarwal et al., 2022; Jain et al., 2020). Synthetic ground-truth settings are common but may not reflect real-world ambiguity.\n\nWe present a framework that standardizes evaluation across tasks with a focus on stability under structural noise.",
    "reason": "The span lists explanation methods generically without articulating how they inform or motivate the evaluation framework, leaving the connection to the paper's contribution implicit (definition a and c).",
    "start": 178,
    "end": 385,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Doe et al., 2015;Rao and Kim, 2016)",
    "document": "Introduction\n\nInformation extraction pipelines often rely on joint modeling to capture interdependent decisions (Luan et al., 2018; Wadden et al., 2019). Prior multi-task formulations have combined entity and relation objectives to reduce exposure bias (Miwa and Bansal, 2016). Evidence suggests leveraging external knowledge bases can further improve recall (Doe et al., 2015;Rao and Kim, 2016), yet these integrations frequently amplify spurious correlations under domain shift.\n",
    "reason": "Missing space after the semicolon between multiple citations; should be “(Doe et al., 2015; Rao and Kim, 2016)”.",
    "start": 359,
    "end": 395,
    "label": "Format"
  },
  {
    "span": "To date, there has been no comprehensive evaluation of code-mixed NER on low-resource African languages.",
    "document": "Introduction\n\nNamed entity recognition (NER) has seen substantial progress with neural architectures, particularly sequence labeling models leveraging CRFs and contextual embeddings (Lample et al., 2016; Ma and Hovy, 2016; Akbik et al., 2018). Multilingual and cross-lingual NER methods have attempted to transfer knowledge across languages through shared subword vocabularies and multilingual pre-trained encoders (Xie et al., 2018; Rahimi et al., 2019; Pires et al., 2019). Code-mixed text introduces additional challenges due to orthographic variation, rapid language switching, and borrowed morphology (Aguilar et al., 2018; Winata et al., 2019).\n\nDespite these advances, practical NER systems must function in informal, code-mixed settings common to messaging platforms and social media across the Global South. To date, there has been no comprehensive evaluation of code-mixed NER on low-resource African languages. In this work, we present a new benchmark and evaluation protocol for code-mixed NER spanning three low-resource language pairs. We further investigate the role of subword segmentation and domain-adaptive pre-training for robust recognition in noisy, code-mixed conditions.",
    "reason": "This is a claim about the absence of prior evaluations in a specific area without providing citations or evidence; it should reference surveys or studies to substantiate the 'no comprehensive evaluation' assertion.",
    "start": 817,
    "end": 921,
    "label": "Unsupported_claim"
  },
  {
    "span": "Our task setup follows the standard evaluation in WMT, where systems are ranked by human adequacy judgments.",
    "document": "Introduction\n\nWe study evaluation practices for machine translation (MT) in low-resource settings. Our task setup follows the standard evaluation in WMT, where systems are ranked by human adequacy judgments. While human evaluation remains the gold standard, budget constraints often necessitate automatic metrics that correlate well with human judgments.\n\nThis work analyzes metric behavior under domain shift and noise, proposing a confidence-aware aggregation that reduces annotation variance.",
    "reason": "References a specific shared task and its evaluation protocol without citing the WMT setup (rule a).",
    "start": 99,
    "end": 207,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Smith et. al., 2018)",
    "document": "Related Work\n\nClinical NLP leverages domain-specific corpora and terminologies for entity recognition and normalization. Pretrained language models adapted to biomedical text show strong gains (Beltagy et al., 2019; Lee et al., 2020). Weak supervision from ontologies improves coverage (Fries et al., 2017). (Smith et. al., 2018) present a shared task that standardizes evaluation for de-identification, while (Johnson et al., 2016) release a large-scale critical care dataset. We focus on cross-site generalization and harmonized label schemas.",
    "reason": "Incorrect punctuation in 'et al.'; should be 'et al.' without a period after 'et' and with a period after 'al'.",
    "start": 308,
    "end": 329,
    "label": "Format"
  },
  {
    "span": "HIPAA-compliant datasets from three hospitals were used in earlier work to pre-train a global model",
    "document": "Introduction\n\nFederated learning has emerged as a compelling paradigm for privacy-preserving model training across clinical institutions. By exchanging gradients rather than raw records, participating sites can benefit from broader data diversity while maintaining local governance. HIPAA-compliant datasets from three hospitals were used in earlier work to pre-train a global model, suggesting that modest federation can already close generalization gaps across sites. Yet, heterogeneity in labeling practices and imaging protocols still leads to client drift and unstable optimization. We introduce an adaptive aggregation schedule that aligns update statistics with site-level uncertainty.",
    "reason": "Mentions specific prior use of datasets and institutions without citation (rule a/b; first mention of a study and datasets requires evidence).",
    "start": 283,
    "end": 382,
    "label": "Unsupported_claim"
  },
  {
    "span": "The winning entries of the last three COCO challenges relied on heavy data augmentation.",
    "document": "Related Work\n\nObject detection has progressed from handcrafted features and sliding windows to region-based CNNs and one-stage detectors. Pretraining on large image corpora and architectural improvements such as feature pyramid networks have further pushed performance. Strong data augmentation and large-scale training are known to be crucial for generalization.\n\nThe winning entries of the last three COCO challenges relied on heavy data augmentation. Beyond augmentation, modern detectors integrate multi-scale training, mosaic and mixup strategies, and extensive test-time ensembling. Semi-supervised and self-training approaches have also narrowed the gap between label-rich and label-scarce regimes.\n\nOur contribution is an augmentation-aware training scheduler that dynamically allocates compute across policies based on a calibrated difficulty signal, improving both accuracy and efficiency.",
    "reason": "Makes a historical claim about competition winners without citing the specific challenge reports or papers.",
    "start": 365,
    "end": 453,
    "label": "Unsupported_claim"
  },
  {
    "span": "Levine et al. (2016) learn visuomotor policies with deep reinforcement learning. Kalashnikov et al. (2018) train QT-Opt for grasping at scale. Finn et al. (2016) propose guided policy search. OpenAI et al. (2019) demonstrate in-hand manipulation with domain randomization.",
    "document": "Related Work\n\nLearning for Robotic Manipulation\n\nWe summarize approaches that learn control from pixels or raw sensory inputs. Levine et al. (2016) learn visuomotor policies with deep reinforcement learning. Kalashnikov et al. (2018) train QT-Opt for grasping at scale. Finn et al. (2016) propose guided policy search. OpenAI et al. (2019) demonstrate in-hand manipulation with domain randomization. Sim-to-real transfer and representation learning are active areas (Tobin et al., 2017; Srinivas et al., 2020).\n\nOur method focuses on contact-rich tasks with few demonstrations via offline pretraining.",
    "reason": "The citation list moves between different manipulation settings without transitions or explicit links, making the connections among methods unclear.",
    "start": 127,
    "end": 399,
    "label": "Coherence"
  },
  {
    "span": "Previous studies demonstrate that elastic weight consolidation remains the most robust approach across domains.",
    "document": "Related Work\n\nCatastrophic forgetting in continual learning has spurred a broad set of methods that either regularize network parameters, rehearse past data, or dynamically expand model capacity (Kirkpatrick et al., 2017; Zenke et al., 2017; Rebuffi et al., 2017; Rolnick et al., 2019). Regularization-based approaches constrain updates to parameters deemed important for earlier tasks, while replay-based methods store exemplars or synthesize pseudo-samples to approximate the old distribution.\n\nIn reinforcement learning, nonstationarity exacerbates forgetting due to distribution shift in both states and policies (Khetarpal et al., 2020). Prior work has adapted experience replay buffers, knowledge distillation, and policy consolidation to mitigate interference between tasks in sequential settings (Riemer et al., 2019; Schwarz et al., 2018).\n\nPrevious studies demonstrate that elastic weight consolidation remains the most robust approach across domains. Despite its popularity, EWC can underperform when tasks are highly dissimilar or when the Fisher information is poorly estimated under partial observability. Our work complements this line by proposing a replay-efficient approach that decouples behavior policy learning from value approximation under task switches.",
    "reason": "It cites 'previous studies' and makes a comparative robustness claim without any supporting references, which is an unsupported claim about prior work.",
    "start": 850,
    "end": 961,
    "label": "Unsupported_claim"
  },
  {
    "span": "Vatswani et al.",
    "document": "Related Work\n\nActive learning for sequence labeling has been studied extensively. Following Vatswani et al., we compare uncertainty-based strategies with representativeness metrics. Early neural approaches explored Monte Carlo dropout for uncertainty estimation (Gal and Ghahramani, 2016), while subsequent work examined pretrained Transformers for efficient querying (Shen et al., 2017; Peters et al., 2018). Our review focuses on methods that balance annotation cost and model performance.",
    "reason": "Narrative citation is missing the year. It should be formatted as a narrative citation, e.g., Vatswani et al. (2020), rather than just the authors' names.",
    "start": 92,
    "end": 107,
    "label": "Format"
  },
  {
    "span": "HIPAA-compliant de-identification pipelines typically achieve over 99% recall on PHI.",
    "document": "Background and Related Work\n\nDe-identification of clinical narratives aims to protect patient privacy by detecting and masking protected health information (PHI) while preserving utility for secondary analysis. Contemporary systems combine rules, dictionaries, and neural sequence labeling to handle diverse PHI categories. HIPAA-compliant de-identification pipelines typically achieve over 99% recall on PHI. However, precision remains sensitive to domain shifts across institutions and note types, raising downstream costs for human review.\n\nWe investigate a confidence-calibrated tagging framework with weak supervision from structured EHR fields and cross-site adaptation using unlabeled notes. Our evaluation covers multiple hospitals with heterogeneous documentation styles, reporting fine-grained performance across PHI subtypes and error modes.",
    "reason": "Reports a quantitative performance claim ('over 99% recall') without citing benchmark studies or shared-task reports.",
    "start": 324,
    "end": 409,
    "label": "Unsupported_claim"
  },
  {
    "span": "Quora Question Pairs",
    "document": "Introduction\n\nParaphrase identification aims to determine whether two sentences express the same meaning, a capability important for search, deduplication, and dialogue systems. The Quora Question Pairs benchmark has become a de facto standard for evaluating paraphrase detection techniques due to its scale and lexical diversity. While recent neural architectures improve accuracy, they can struggle with hard negatives that share surface forms but diverge semantically. We propose a curriculum with adversarially mined pairs to robustify representations against spurious lexical overlap.",
    "reason": "First mention of a specific benchmark dataset appears without citation (rule a).",
    "start": 182,
    "end": 202,
    "label": "Unsupported_claim"
  },
  {
    "span": "(O'Neil et al., 2020)",
    "document": "Related Work\n\nModel calibration assesses whether predicted probabilities reflect true likelihoods (Guo et al., 2017). Techniques for post-hoc calibration include temperature scaling and isotonic regression (Platt, 1999; Zadrozny and Elkan, 2002). Recent advances study calibration in the multi-domain setting (Hernandez, 2021; (O'Neil et al., 2020)) and propose domain-conditional temperature parameters (Kumar et al., 2019). Other work explores calibration under data shift with conformal methods (Vovk et al., 2005; Angelopoulos and Bates, 2021).",
    "reason": "Wrong citation formatting: nested parentheses inside a single parenthetical list; the second citation should not be enclosed by its own parentheses.",
    "start": 327,
    "end": 348,
    "label": "Format"
  },
  {
    "span": "Low-resource ASR has explored cross-lingual transfer from high-resource languages (Kunze et al., 2017; Stoian et al., 2020), multilingual pretraining with wav2vec-style objectives (Schneider et al., 2019; Conneau et al., 2020), pronunciation lexicon induction (Huffman and Downey, 2017), and data augmentation via speed perturbation, noise injection, and SpecAugment (Ko et al., 2015; Park et al., 2019).",
    "document": "Related Work\n\nAutomatic speech recognition for low-resource languages is hindered by scarce transcribed audio, dialectal variation, and limited lexicographic resources. Transfer and self-supervision have reduced the data barrier but have not closed the performance gap with high-resource languages.\n\nLow-resource ASR has explored cross-lingual transfer from high-resource languages (Kunze et al., 2017; Stoian et al., 2020), multilingual pretraining with wav2vec-style objectives (Schneider et al., 2019; Conneau et al., 2020), pronunciation lexicon induction (Huffman and Downey, 2017), and data augmentation via speed perturbation, noise injection, and SpecAugment (Ko et al., 2015; Park et al., 2019).\n\nExisting evaluations often focus on clean read speech rather than conversational or code-switched settings typical in practice.\n\nWe present CoSwitch-ASR, a lexicon-light system with code-switch aware pretraining and constrained decoding tailored to spontaneous speech.",
    "reason": "The span enumerates existing techniques without stating how they inform the new method, what limitation persists, or why a new approach is needed.",
    "start": 300,
    "end": 704,
    "label": "Lacks_synthesis"
  },
  {
    "span": "by (Harris, 1954)",
    "document": "Introduction\n\nDistributional semantics posits that meaning derives from usage patterns in large corpora. The idea traces back to structural linguistics and early co-occurrence analysis by (Harris, 1954) and was later operationalized with latent semantic analysis (Deerwester et al., 1990). Neural word embeddings such as word2vec and GloVe popularized predictive and count-based formulations (Mikolov et al., 2013; Pennington et al., 2014). Contextual encoders extend these ideas with deep architectures (Peters et al., 2018; Devlin et al., 2019). We build on this line by introducing a corpus-adaptive objective that disentangles topical and syntactic signals.",
    "reason": "Incorrect narrative use of a parenthetical citation after a preposition; should be 'by Harris (1954)'.",
    "start": 185,
    "end": 202,
    "label": "Format"
  },
  {
    "span": "Balog et al. (2017) learned program sketches from examples. Gulwani (2011) synthesized string transformations from input–output pairs. Devlin et al. (2017) integrated neural models with symbolic search. Chen et al. (2021) pretrained code models for generation.",
    "document": "Introduction\n\nProgram synthesis research explores how to generate code from specifications such as examples, natural language, or partial programs. Approaches range from symbolic search with inductive biases to neural models that learn distributions over programs.\n\nBalog et al. (2017) learned program sketches from examples. Gulwani (2011) synthesized string transformations from input–output pairs. Devlin et al. (2017) integrated neural models with symbolic search. Chen et al. (2021) pretrained code models for generation.\n\nWe contribute a hybrid method that conditions a symbolic enumerator on neural uncertainty estimates to balance completeness and efficiency.",
    "reason": "The cited works are presented as isolated statements without transitions or explanation of their relationships, leaving unclear how sketch learning, PBE, and pretrained models connect.",
    "start": 266,
    "end": 526,
    "label": "Coherence"
  },
  {
    "span": "Brown et al. (2020) demonstrate in-context learning in large language models. MAML learns initialization for fast adaptation (Finn et al., 2017). Gao et al. (2021) propose retrieval-augmented prompts to select exemplars. Liu et al. (2021) introduce prefix-tuning to steer generation with continuous prompts. Wei et al. (2022) explore chain-of-thought prompting for multi-step reasoning.",
    "document": "Related Work\n\nPrompting and Few-Shot Adaptation in NLP\n\nLarge language models have catalyzed a shift from task-specific finetuning to prompt-based adaptation. Early work illustrates that carefully constructed exemplars can elicit task behavior from frozen models, motivating a surge of methods that engineer or learn prompts for better reliability and transfer. Parallel strands investigate meta-learning and retrieval as complementary mechanisms to improve sample efficiency.\n\nBrown et al. (2020) demonstrate in-context learning in large language models. MAML learns initialization for fast adaptation (Finn et al., 2017). Gao et al. (2021) propose retrieval-augmented prompts to select exemplars. Liu et al. (2021) introduce prefix-tuning to steer generation with continuous prompts. Wei et al. (2022) explore chain-of-thought prompting for multi-step reasoning.\n\nCalibration and Selection of Exemplars\n\nOrthogonal to prompt design, several studies focus on calibration of verbalized likelihoods (Zhao et al., 2021), automatic instruction induction (Honovich et al., 2022), and uncertainty-aware exemplar selection (Zhang et al., 2022). These works emphasize stability and reproducibility, addressing variance across prompts and seeds.\n\nOur Work\n\nWe examine how retrieval and lightweight continuous prompts interact under strict budget constraints. Unlike prior work that tunes each component in isolation, we analyze end-to-end computational tradeoffs and propose a simple, black-box selection strategy that reduces test-time cost while preserving accuracy on diverse tasks.",
    "reason": "The sentences list multiple papers one after another without transitions or an explicit explanation of how meta-learning (MAML) relates to prompting methods, creating abrupt shifts and implied relationships rather than stated ones.",
    "start": 478,
    "end": 864,
    "label": "Coherence"
  },
  {
    "span": "Our analysis follows the standard protocol used in the literature for pruning transformers at 30% sparsity.",
    "document": "Related Work\n\nModel compression techniques such as pruning, quantization, and distillation are widely used to reduce inference cost while preserving accuracy. In transformer architectures, structured and unstructured pruning target attention heads, feed-forward layers, or individual weights.\n\nOur analysis follows the standard protocol used in the literature for pruning transformers at 30% sparsity. Prior studies vary widely in the choice of sparsity levels, retraining schedules, and data exposure, making results difficult to compare across papers. Efforts to standardize reporting have suggested unified sparsity budgets and calibration tasks to facilitate apples-to-apples comparisons.\n\nWe adopt a common training budget across pruning methods and evaluate under identical decoding and calibration settings to isolate the effect of the pruning criterion.",
    "reason": "The claim invokes a 'standard protocol' and a specific sparsity level without citing any sources that define or exemplify this protocol (violates rule b).",
    "start": 294,
    "end": 401,
    "label": "Unsupported_claim"
  },
  {
    "span": "A large body of work shows that recidivism prediction tools are biased against minorities.",
    "document": "Related Work\n\nAlgorithmic fairness research investigates how predictive models can disproportionately harm protected groups through biased data, target definitions, or deployment contexts. Multiple mathematical criteria such as demographic parity, equalized odds, and calibration capture different notions of fairness that are often mutually incompatible. In criminal justice, risk assessment instruments have sparked debate about transparency and disparate impact. A large body of work shows that recidivism prediction tools are biased against minorities. Proposed mitigations include pre-processing reweighting, in-processing regularizers that penalize group-dependent errors, and post-processing decision adjustments. Our method focuses on uncertainty-aware abstention to reduce harm under distribution shift.",
    "reason": "Makes a strong claim about prior empirical findings without citing any supporting literature.",
    "start": 466,
    "end": 556,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith et al.,",
    "document": "Introduction\n\nNeural sequence-to-sequence models have transformed abstractive summarization by enabling end-to-end learning of content selection and surface realization. As shown by Smith et al., neural encoder–decoder architectures surpass phrase-based methods in fluency while maintaining adequacy (Klein and Frank, 2017). Subsequent work, such as Li et al. (2020), introduces large-scale pretraining objectives that further improve factual consistency. Others report gains from coverage and copy mechanisms (Tu et al., 2016; See et al., 2017), while reinforcement learning has been explored to better align training with evaluation metrics (Paulus et al., 2018). Despite these advances, robustness to domain shift remains limited, motivating our investigation into controllable summarization with explicit content plans.\n\nRelated Work\n\nEarly template-based approaches provided strong precision but lacked flexibility (Arthur, 2001). Extractive methods improved scalability through graph-based ranking (Mihalcea and Tarau, 2004) and learned sentence salience (Nallapati et al., 2017). More recent systems leverage pretrained encoders to capture long-range dependencies (Zhang and Bansal, 2020). In parallel, factuality metrics and constrained decoding strategies have been proposed to mitigate hallucinations (Kryściński et al., 2020; Dohan et al., 2022).",
    "reason": "Narrative citation missing year. It should be formatted as a narrative citation with the year, e.g., \"Smith et al. (2018),\" instead of \"Smith et al.,\".",
    "start": 182,
    "end": 195,
    "label": "Format"
  },
  {
    "span": "Personalization strategies range from fine-tuning local heads (Arivazhagan et al., 2019), meta-learning across clients (Fallah et al., 2020), to clustering-based aggregation (Sattler et al., 2020).",
    "document": "Related Work\n\nFederated learning (FL) addresses data silos by training models across clients without centralizing sensitive data. Heterogeneity in client data distributions (non-IID) often degrades the performance of a single global model, motivating personalization techniques.\n\nPersonalization strategies range from fine-tuning local heads (Arivazhagan et al., 2019), meta-learning across clients (Fallah et al., 2020), to clustering-based aggregation (Sattler et al., 2020). Regularization-based methods impose client-specific priors or proximal constraints (Li et al., 2020; Collins et al., 2021), and multi-task formulations jointly learn shared and client-specific parameters (Smith et al., 2017; Dinh et al., 2022).\n\nIn this work, we present a representation-calibration layer that aligns client embeddings using unlabeled anchor points exchanged under strict privacy budgets.",
    "reason": "The span merely catalogs prior personalization methods without explaining their limitations, relation to the proposed method, or the gap being addressed.",
    "start": 280,
    "end": 477,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Smith & Jones, 2017",
    "document": "Introduction\n\nExplainable AI techniques for tabular risk models commonly rely on feature attribution, surrogate modeling, and counterfactual analysis (Ribeiro et al., 2016; Lundberg and Lee, 2017). According to Smith & Jones, 2017, global explanations derived from linear surrogates can be misleading when interactions dominate. Later work introduced interaction-aware SHAP variants and hierarchical attributions to address this concern (Yu and Patel, 2020). We study when local linearity assumptions break down and propose diagnostics to detect unstable explanation regions.",
    "reason": "Ampersand used with author names in a narrative author–year context and year placed after a comma; should be \"Smith and Jones (2017)\" or a parenthetical citation.",
    "start": 211,
    "end": 230,
    "label": "Format"
  },
  {
    "span": "Conversational recommendation systems leverage slot-filling dialogue management, knowledge-grounded retrieval, and reinforcement learning for policy optimization (Zhang et al., 2018; Sun and Zhang, 2018; Lei et al., 2020; Christakopoulou et al., 2016).",
    "document": "Introduction\n\nConversational recommenders aim to elicit user preferences through interaction, reducing cognitive load and cold-start effects. They must balance exploration, personalization, and natural language understanding.\n\nConversational recommendation systems leverage slot-filling dialogue management, knowledge-grounded retrieval, and reinforcement learning for policy optimization (Zhang et al., 2018; Sun and Zhang, 2018; Lei et al., 2020; Christakopoulou et al., 2016).\n\nWe present a modular framework that couples preference elicitation with latent critique modeling, enabling efficient learning of user constraints from minimal dialogue turns.",
    "reason": "The span enumerates approaches without linking them to a specific limitation or to the authors’ proposed method, demonstrating a lack of synthesis.",
    "start": 227,
    "end": 479,
    "label": "Lacks_synthesis"
  },
  {
    "span": "demographic bias is pervasive in widely used sentiment datasets",
    "document": "Related Work\n\nFairness in NLP examines how models encode and amplify social biases, with lines of research spanning dataset auditing, debiasing representations, and equitable evaluation. Sentiment analysis, as a canonical task, offers a convenient testbed for studying such effects due to abundant benchmarks and standardized metrics.\n\nDespite progress, demographic bias is pervasive in widely used sentiment datasets, affecting both training distributions and evaluation outcomes. These biases can lead to disparate error rates across groups and reduced reliability in downstream applications.\n\nWe propose a counterfactual data augmentation strategy guided by causal templates and introduce group-aware calibration to mitigate disparate impacts while preserving overall accuracy.",
    "reason": "Asserts a broad, domain-specific claim about dataset bias without supporting citations per rule (b).",
    "start": 354,
    "end": 417,
    "label": "Unsupported_claim"
  },
  {
    "span": "Nguyen et al. 1",
    "document": "Introduction\n\nExploration remains a central challenge in reinforcement learning (Thrun, 1992; Kakade, 2003). Count-based bonuses and optimism under uncertainty provide principled mechanisms (Strehl and Littman, 2008; Bellemare et al., 2016). A similar exploration strategy was used by Nguyen et al. 1 to encourage visits to novel states in sparse-reward environments. More recent methods leverage uncertainty from ensembles (Osband et al., 2016) and Bayesian approximations (Osband et al., 2018; O'Donoghue et al., 2018). Our approach extends these ideas with model-based rollouts to estimate epistemic uncertainty.",
    "reason": "Improper footnote-like usage '1' appended to the author string; a bibliographic citation should include a year or be formatted as a proper footnote, e.g., 'Nguyen et al. (2020)' or a numbered footnote format.",
    "start": 285,
    "end": 300,
    "label": "Format"
  },
  {
    "span": "On LibriSpeech dev-other, end-to-end systems have already pushed word error rates below 2%.",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) has progressed rapidly with advances in self-supervised pretraining, data augmentation, and efficient encoder–decoder architectures. Benchmarks such as LibriSpeech remain central to measuring progress across both clean and challenging acoustic conditions. Despite steady improvements, generalization to spontaneous or conversational domains remains an open challenge due to disfluencies, overlapping speech, and limited training data that match such distributions. On LibriSpeech dev-other, end-to-end systems have already pushed word error rates below 2%. Yet these gains may not translate to real-world scenarios with domain shift, accent variability, and background noise. Recent research emphasizes robust pretraining objectives and domain-adaptive fine-tuning, as well as streaming constraints for on-device deployment. In this work, we investigate a simple recipe that combines long-context encoders with lightweight adapters for domain adaptation, evaluating robustness under accent and noise perturbations while maintaining low latency suitable for edge applications.",
    "reason": "Claims a specific benchmark result (WER below 2% on a named split) without a citation; performance statistics must be sourced (rule b).",
    "start": 525,
    "end": 616,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Lee et al., 2019)",
    "document": "Introduction\n\nNeural machine translation (NMT) has shifted from recurrent architectures to fully attention-based models, enabling longer context modeling and parallel training (Bahdanau et al., 2015; Vaswani et al., 2017). In (Lee et al., 2019) a pre-training objective based on denoising autoencoding is proposed to improve low-resource translation by leveraging monolingual corpora. Subsequent work explores multilingual pre-training to share representations across related languages (Conneau and Lample, 2019; Aharoni et al., 2019). Although domain adaptation techniques such as back-translation reduce exposure bias (Sennrich et al., 2016), robustness under distribution shift remains a central challenge (Koehn and Knowles, 2017). We revisit data selection and curriculum schedules to close the gap between pre-training and fine-tuning stages.",
    "reason": "Wrong citation style: the preposition precedes a parenthetical citation; it should be narrative, e.g., “In Lee et al. (2019) a pre-training objective...”",
    "start": 223,
    "end": 244,
    "label": "Format"
  },
  {
    "span": "Alur et al. (2013) formalized syntax-guided synthesis. Devlin et al. (2019) pretrained BERT for language understanding. Chen et al. (2021) introduced Codex for code generation. Nijkamp et al. (2022) studied large language models for program synthesis.",
    "document": "Related Work\n\nProgram Synthesis and Code Generation\n\nProgram synthesis spans formal methods, neural generation, and neuro-symbolic hybrids. Benchmarks vary widely in specification format, target languages, and evaluation criteria, complicating holistic comparisons.\n\nAlur et al. (2013) formalized syntax-guided synthesis. Devlin et al. (2019) pretrained BERT for language understanding. Chen et al. (2021) introduced Codex for code generation. Nijkamp et al. (2022) studied large language models for program synthesis.\n\nConstraints and Verification\n\nA complementary line integrates type systems and SMT solvers to enforce correctness during generation, but scalability and specification brittleness limit applicability to real repositories.\n\nOur Direction\n\nWe propose a verifier-guided decoding approach that interleaves symbolic constraints with neural proposals, improving pass@k under strict specifications.",
    "reason": "The span enumerates citations across different subareas without transitions or explicit relational statements, leaving the reader unsure how the works connect to each other or to the surrounding narrative.",
    "start": 267,
    "end": 518,
    "label": "Coherence"
  },
  {
    "span": "BERT Devlin et al., 2019)",
    "document": "Introduction\n\nPretrained language models have transformed NLP by enabling effective transfer learning (Howard and Ruder, 2018; Peters et al., 2018). BERT Devlin et al., 2019) introduced bidirectional masked language modeling, inspiring numerous variants (Liu et al., 2019; Lan et al., 2020). Subsequent work scaled pretraining data and compute (Brown et al., 2020) and explored domain-adaptive pretraining (Gururangan et al., 2020). Despite strong results, downstream calibration and robustness remain open challenges (Hendrycks and Dietterich, 2019).\n\nWe propose an efficient post-hoc adaptation procedure that improves calibration without task-specific labels.",
    "reason": "Citation lacks the opening parenthesis around the author–year and is attached directly to the model name; should be 'BERT (Devlin et al., 2019)'.",
    "start": 149,
    "end": 174,
    "label": "Format"
  },
  {
    "span": "Prior work trains wav2vec 2.0 on IEMOCAP with CCC loss and attains state-of-the-art results.",
    "document": "Introduction\n\nSpeech emotion recognition (SER) aims to infer affective states from acoustic signals. Self-supervised pretraining has recently advanced SER by learning rich representations from unlabeled audio, which are then fine-tuned on labeled corpora. Choice of objective and data augmentation critically affects robustness to speaker and channel variability.\n\nPrior work trains wav2vec 2.0 on IEMOCAP with CCC loss and attains state-of-the-art results.\n\nHowever, performance often degrades under cross-corpus evaluation. We introduce a domain-adaptive training regime with style-invariant augmentation and supervised contrastive fine-tuning to enhance transferability.",
    "reason": "Claims specific prior results and mentions a dataset at first mention without any supporting citations (violates rule a and b).",
    "start": 365,
    "end": 457,
    "label": "Unsupported_claim"
  },
  {
    "span": "Several competitions have evaluated CRS on multi-domain dialogues.",
    "document": "Introduction\n\nConversational recommender systems (CRS) aim to elicit user preferences through dialogue and produce tailored item suggestions with minimal interaction cost. Recent advances integrate large language models with structured catalogs and retrieval, yet robust evaluation remains elusive. Several competitions have evaluated CRS on multi-domain dialogues. Still, there is limited consensus on how to balance diversity, relevance, and conversational appropriateness in a single metric. We contribute a standardized evaluation suite with turn-level diagnostics and demonstrate improvements from a planning module that explicitly separates preference elicitation from recommendation generation.",
    "reason": "Mentions competitions but provides no citations or references to identify them.",
    "start": 299,
    "end": 365,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent work explores prompting and instruction tuning to elicit capabilities from large language models (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022; Ouyang et al., 2022). Chain-of-thought prompting enables step-by-step reasoning (Wei et al., 2022; Kojima et al., 2022). Program-of-thought and tool use extend these ideas (Chen et al., 2021; Schick et al., 2023).",
    "document": "Introduction\n\nLarge language models (LLMs) can be steered with carefully designed prompts or lightweight finetuning to perform a wide range of tasks. Research investigates how to elicit knowledge, reasoning, and external tool use while minimizing labeled data.\n\nRecent work explores prompting and instruction tuning to elicit capabilities from large language models (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022; Ouyang et al., 2022). Chain-of-thought prompting enables step-by-step reasoning (Wei et al., 2022; Kojima et al., 2022). Program-of-thought and tool use extend these ideas (Chen et al., 2021; Schick et al., 2023).\n\nWe evaluate across diverse reasoning tasks and analyze sensitivity to prompt variations.",
    "reason": "The span summarizes several prompting paradigms but does not explain their relation to the authors' objectives, identify gaps, or articulate the motivation for the proposed approach.",
    "start": 262,
    "end": 654,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There are many recent works that explore multi-hop reasoning over temporal knowledge graphs.",
    "document": "Introduction\n\nKnowledge graph question answering (KGQA) requires reasoning over entities and relations to retrieve correct answers (Lan et al., 2021; Sun et al., 2019). Temporal knowledge graphs (TKGs) augment edges with timestamps, enabling queries whose truth depends on time. There are many recent works that explore multi-hop reasoning over temporal knowledge graphs. Yet, dataset heterogeneity and evaluation protocols hinder fair comparison across methods. We introduce a standardized benchmark with unified temporal granularity and a suite of diagnostic queries to isolate modeling capabilities.\n\nRelated Work\n\nApproaches to TKG reasoning span rule induction, temporal point processes, and neural embedding models (Trivedi et al., 2017; Wu et al., 2020).",
    "reason": "The sentence references 'many recent works' on a specific topic without citing any of them.",
    "start": 279,
    "end": 371,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Vatswani et al., 2019)",
    "document": "Related Work\n\nUnsupervised domain adaptation for automatic speech recognition aims to mitigate performance degradation when moving across accents, noise conditions, or microphones. In (Vatswani et al., 2019), the authors propose a teacher–student framework to transfer knowledge without labeled target data. Other works pursue adversarial objectives to remove domain-specific cues (Shinohara, 2016; Sun et al., 2017) and feature-space normalization (Palaskar et al., 2019). Unlike these approaches, we study robust adaptation under severe label shift with limited unlabeled target audio.",
    "reason": "Wrong citation style: a parenthetical citation is used after the preposition 'In'; it should be narrative as 'In Vatswani et al. (2019)'.",
    "start": 184,
    "end": 207,
    "label": "Format"
  },
  {
    "span": "Previous work has shown that adversarial training completely mitigates FGSM attacks on CIFAR-10.",
    "document": "Related Work\n\nRobustness to adversarial perturbations remains a central challenge for image classifiers. A wide spectrum of defenses has been proposed, from input transformations and certifiable methods to adversarial training with inner-loop attacks.\n\nPrevious work has shown that adversarial training completely mitigates FGSM attacks on CIFAR-10. However, single-step robustness does not necessarily translate to multi-step or adaptive attacks, and overfitting to a specific threat model can yield a false sense of security.\n\nOur method decouples data augmentation from the adversarial objective and calibrates robustness-accuracy trade-offs via margin-aware regularization.",
    "reason": "Asserts prior findings and references a dataset and attack without citing the supporting paper.",
    "start": 253,
    "end": 349,
    "label": "Unsupported_claim"
  },
  {
    "span": "there has been a surge of recent works on multimodal hate speech detection",
    "document": "Introduction\n\nHate speech detection has traditionally focused on text, leveraging lexical, syntactic, and contextual cues to identify abusive content (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018). As social media increasingly relies on images and videos, multimodal approaches that combine language and vision have gained traction for improved robustness and disambiguation (Kiela et al., 2020; Prasad et al., 2021). However, there has been a surge of recent works on multimodal hate speech detection that explore architectures, pretraining strategies, and cross-modal alignment without a consensus on evaluation protocols or datasets. This heterogeneity complicates comparisons across studies and motivates our standardized benchmark and ablations.\n\nOur contributions are threefold: (i) we curate a suite of publicly accessible datasets spanning memes, short videos, and image-caption pairs; (ii) we propose a lightweight alignment objective that improves cross-modal grounding; and (iii) we conduct controlled analyses of spurious correlations and robustness under distribution shift. We adopt established metrics from prior text-only and multimodal classification settings (Agrawal et al., 2019; Kiela et al., 2020) and release code and metadata to facilitate reproducibility.",
    "reason": "Mentions 'recent works' without providing supporting citations for that claim, violating rule (d) that recent work statements must be backed by references.",
    "start": 433,
    "end": 507,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Garcia et al., 2019 Kim and Lee, 2020)",
    "document": "Related Work\n\nData augmentation enhances robustness by perturbing inputs while preserving labels. In vision, random crops and color jitter are standard (Krizhevsky et al., 2012), whereas NLP relies on synonym substitution and back-translation (Sennrich et al., 2016; Wei and Zou, 2019). Mixup and manifold mixup encourage linear behavior (Zhang et al., 2018; Verma et al., 2019). Recent contrastive objectives require positive pair generation (Chen et al., 2020). For sequence labeling, prior studies examine noise-aware training (Sukhbaatar et al., 2015). We compare span-level augmentations across tasks (Garcia et al., 2019 Kim and Lee, 2020) and report trade-offs under label scarcity.",
    "reason": "Multiple citations inside one set of parentheses are missing a separator; should include a semicolon, e.g., '(Garcia et al., 2019; Kim and Lee, 2020)'.",
    "start": 606,
    "end": 645,
    "label": "Format"
  },
  {
    "span": "Fusion strategies for multimodal sentiment analysis range from early feature concatenation to tensor fusion and cross-modal attention mechanisms (Zadeh et al., 2017; Tsai et al., 2019; Mai et al., 2020).",
    "document": "Related Work\n\nMultimodal sentiment analysis combines language, vision, and acoustics to infer affect and opinion. A central challenge is how to represent and fuse heterogeneous signals while coping with asynchrony and noise.\n\nFusion strategies for multimodal sentiment analysis range from early feature concatenation to tensor fusion and cross-modal attention mechanisms (Zadeh et al., 2017; Tsai et al., 2019; Mai et al., 2020). Alignment techniques synchronize modalities at word, frame, or segment levels, with recent approaches learning soft alignments end-to-end (Pham et al., 2019; Rahman et al., 2020). Robust training under missing modalities has been explored via modality dropout and generative imputation (Neverova et al., 2015; Wu et al., 2020).\n\nPretrained language and vision encoders have boosted downstream performance through transfer, while adapter layers and prompt-based methods aim to reduce task-specific fine-tuning overhead (Radford et al., 2021; Liu et al., 2021). Benchmarks continue to evolve with more diverse speakers and spontaneous speech scenarios.\n\nThese developments underscore the breadth of modeling choices available for multimodal affect understanding.",
    "reason": "The span catalogs fusion approaches without connecting them to the authors’ objective, leaving unstated what limitation persists or how the current work positions itself relative to these methods.",
    "start": 226,
    "end": 429,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Chen, 2019 and Wu, 2020)",
    "document": "Related Work\n\nNeural ranking for open-domain QA commonly uses dense retrievers trained with contrastive objectives (Karpukhin et al., 2020). Hard-negative mining and iterative distillation further improve retrieval quality (Izacard and Grave, 2020). Several studies (Chen, 2019 and Wu, 2020) explored hybrid sparse–dense indexes to balance efficiency and effectiveness, but integration with end-to-end generators remains underexplored.",
    "reason": "Wrong separator for multiple parenthetical citations; should use a semicolon: '(Chen, 2019; Wu, 2020)'.",
    "start": 266,
    "end": 291,
    "label": "Format"
  },
  {
    "span": "The CoNLL 2012 shared task defined the standard evaluation protocols for coreference resolution.",
    "document": "Related Work\n\nCoreference resolution has evolved from rule-based and mention-pair models to neural architectures that jointly score spans and antecedents. While end-to-end models reduce error propagation, they remain sensitive to domain shifts and annotation conventions, motivating research into domain adaptation and multilingual transfer.\n\nThe CoNLL 2012 shared task defined the standard evaluation protocols for coreference resolution. Subsequent work has explored span pruning, global inference, and discourse-aware features to further improve performance. Recent approaches also incorporate pre-trained language models to better capture context and lexical semantics.\n\nIn this paper, we focus on low-resource settings, proposing a pseudo-labeling strategy that leverages data augmentation to mitigate annotation sparsity.",
    "reason": "Mentions a specific shared task and claims its defining role without providing a citation at first mention (violates a and b).",
    "start": 343,
    "end": 439,
    "label": "Unsupported_claim"
  },
  {
    "span": "Cognitive behavioral therapy techniques have been adapted to conversational agents (Fitzpatrick et al., 2017). Personalization uses user profiles to tailor responses (Zhang et al., 2018). Sentiment lexicons enable affective tracking (Mohammad and Turney, 2013). Reinforcement learning from human feedback improves dialogue quality (Ouyang et al., 2022).",
    "document": "Related Work\n\nDigital mental health tools increasingly use conversational interfaces, raising questions about efficacy, safety, and personalization. Work spans clinical grounding, dialogue management, and ethical safeguards for crisis detection.\n\nCognitive behavioral therapy techniques have been adapted to conversational agents (Fitzpatrick et al., 2017). Personalization uses user profiles to tailor responses (Zhang et al., 2018). Sentiment lexicons enable affective tracking (Mohammad and Turney, 2013). Reinforcement learning from human feedback improves dialogue quality (Ouyang et al., 2022).\n\nOur approach focuses on safety-aware retrieval augmented generation with clinician-in-the-loop curation, connecting therapeutic intents to verifiable evidence in responses.",
    "reason": "The span lists separate strands (CBT adaptation, personalization, sentiment lexicons, RLHF) with no explicit relational statements or transitions, making their relevance to each other unclear.",
    "start": 247,
    "end": 600,
    "label": "Coherence"
  },
  {
    "span": "Vision-based grasp planning methods range from analytic pipelines that estimate contact points to deep networks that directly regress grasp poses from RGB or RGB-D inputs. Convolutional architectures predict grasp rectangles or 6-DoF poses, while transformers integrate global context for cluttered scenes. Self-supervised data collection on robotic platforms has enabled large-scale grasp datasets, and sim-to-real transfer techniques reduce the gap from synthetic scenes to real environments.",
    "document": "Introduction\nReliable grasping in unstructured environments remains a central challenge for autonomous manipulation. Occlusions, novel objects, and domain shift hinder performance when models are trained on limited lab data.\n\nRelated Work\nClassical grasp planning formulated force-closure conditions on known meshes and required accurate pose estimation. Learning-based approaches avoid explicit modeling by mapping sensory inputs to grasp parameters.\nVision-based grasp planning methods range from analytic pipelines that estimate contact points to deep networks that directly regress grasp poses from RGB or RGB-D inputs. Convolutional architectures predict grasp rectangles or 6-DoF poses, while transformers integrate global context for cluttered scenes. Self-supervised data collection on robotic platforms has enabled large-scale grasp datasets, and sim-to-real transfer techniques reduce the gap from synthetic scenes to real environments.\nOther lines study uncertainty estimation for grasp success, either through Bayesian approximations or ensemble-based calibration, though metrics differ across datasets.\n\nContributions\nWe present a grasp proposal network tailored to severe occlusion with an occlusion-aware depth completion module.",
    "reason": "The span catalogs existing methods and trends but does not connect them to the proposed occlusion-aware approach or state what remains unresolved, violating (a) and (c).",
    "start": 452,
    "end": 946,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In a previous study, the authors showed that BERT-base surpasses human performance on sentiment analysis.",
    "document": "Related Work\n\nSentiment analysis has evolved from lexicon-based heuristics to deep neural architectures that capture contextual semantics. Transformer encoders provide strong baselines by modeling long-range dependencies and subword morphology, and domain-adaptive pretraining further improves performance on reviews and social media. In a previous study, the authors showed that BERT-base surpasses human performance on sentiment analysis. Subsequent work explores data augmentation, contrastive learning, and calibration to address class imbalance and out-of-distribution drift, yet performance still degrades significantly under distribution shift.",
    "reason": "Claims a specific result from a previous study without citing the study (rule b/e).",
    "start": 335,
    "end": 440,
    "label": "Unsupported_claim"
  },
  {
    "span": "There have been many recent works exploring unsupervised contrastive learning for retrieval.",
    "document": "Related Work\n\nNeural information retrieval has increasingly adopted representation learning to bridge the lexical gap between queries and documents. There have been many recent works exploring unsupervised contrastive learning for retrieval. These approaches typically rely on in-batch negatives or mined hard negatives to shape embedding spaces that align semantically similar items. Beyond contrastive losses, several studies investigate distillation from cross-encoders into bi-encoders to balance efficiency and effectiveness. Our work situates itself within this landscape by focusing on lightweight negative mining strategies that preserve retrieval latency constraints in large-scale settings.",
    "reason": "Mentions 'recent works' without providing any citations to support the claim (definition d).",
    "start": 149,
    "end": 241,
    "label": "Unsupported_claim"
  },
  {
    "span": "(2019, Patel et al.)",
    "document": "Related Work\n\nTopic modeling has evolved from probabilistic latent variable formulations to neural amortized inference and contextual embeddings (Blei et al., 2003; Miao et al., 2017; Bianchi et al., 2021). Neural LDA variants (2019, Patel et al.) attempt to incorporate mini-batch stochasticity while preserving interpretability. Transformer-based encoders further improve topic coherence when combined with mutual-information objectives (Hoyle et al., 2021; Thompson and Mimno, 2020). We focus on stabilizing training with sparsity-inducing priors.",
    "reason": "Reversed order inside the parenthetical citation; should be \"(Patel et al., 2019)\" rather than placing the year before the authors.",
    "start": 227,
    "end": 247,
    "label": "Format"
  },
  {
    "span": "Early neural program repair utilized sequence-to-sequence models to translate buggy code to fixed code, followed by Transformer-based architectures that improved long-range dependencies. Recent studies have incorporated syntax constraints, edit representations, and constrained decoding to better preserve program structure. Large language models have further boosted patch plausibility by leveraging in-context examples and code-specific pretraining. In this paper, we present a method that fine-tunes a code-focused language model and augments decoding with verification feedback.",
    "document": "Introduction\n\nAutomatic program repair seeks to generate patches that make failing tests pass while preserving overall functionality. Neural approaches have become increasingly capable due to large-scale code corpora and improvements in modeling long-range structure.\n\nEarly neural program repair utilized sequence-to-sequence models to translate buggy code to fixed code, followed by Transformer-based architectures that improved long-range dependencies. Recent studies have incorporated syntax constraints, edit representations, and constrained decoding to better preserve program structure. Large language models have further boosted patch plausibility by leveraging in-context examples and code-specific pretraining. In this paper, we present a method that fine-tunes a code-focused language model and augments decoding with verification feedback.\n\nOur empirical study spans multiple Java and Python benchmarks with unit tests, where we examine patch correctness and generalization to unseen bug patterns.",
    "reason": "The paragraph summarizes prior work and immediately states the authors' method without articulating the gap or motivation that differentiates their contribution.",
    "start": 269,
    "end": 851,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Chen et al., 2018; Kumar, 2019; Lopez and Hu, 2021.",
    "document": "Introduction\n\nGraph neural networks (GNNs) have emerged as a powerful paradigm for learning on relational data, achieving state-of-the-art results in recommendation, drug discovery, and social network analysis (Hamilton et al., 2017; Kipf and Welling, 2017). Despite rapid progress, challenges remain in scaling, robustness, and interpretability (Bronstein et al., 2021; Wu et al., 2020). Recent surveys synthesize progress (Chen et al., 2018; Kumar, 2019; Lopez and Hu, 2021. However, most existing methods focus on transductive learning, leaving inductive settings underexplored (Rossi et al., 2020).\n\nIn this work, we revisit message passing with a focus on stability under structural perturbations. We show that constraining aggregation functions can yield improved generalization (Xu et al., 2019) and propose an adaptive normalization layer that mitigates over-smoothing (Oono and Suzuki, 2020). Our contributions are validated across multiple benchmarks, demonstrating consistent gains over strong baselines.",
    "reason": "Parenthetical citation is missing the closing parenthesis.",
    "start": 424,
    "end": 476,
    "label": "Format"
  },
  {
    "span": "The SAMSum dataset has been widely adopted for evaluating multi-speaker dialogue summarization.",
    "document": "Related Work\n\nDialogue summarization has evolved from single-speaker meeting minutes to complex, multi-party chat transcripts. Early work focused on extractive methods that select salient utterances, while recent neural models perform abstractive generation conditioned on discourse structure and speaker roles. The SAMSum dataset has been widely adopted for evaluating multi-speaker dialogue summarization. Beyond chat-style corpora, long-form conversational benchmarks have emphasized topic continuity and speaker attribution to reduce hallucination. Our work complements this line by introducing a role-aware summarizer that explicitly models turn-level intentions.",
    "reason": "Mentions a specific dataset and its adoption without citing the dataset or any studies that use it.",
    "start": 312,
    "end": 407,
    "label": "Unsupported_claim"
  },
  {
    "span": "We evaluate on the widely adopted Reddit TIFU dataset",
    "document": "Related Work\n\nAbstractive summarization research spans news, scientific text, and social media. While news summarization has benefited from large-scale corpora, user-generated content remains noisy and diverse, posing unique challenges for content selection and style control.\n\nWe evaluate on the widely adopted Reddit TIFU dataset to study abstractive summarization of informal posts. Prior approaches emphasize pretraining and length control, but few methods explicitly model discourse markers and pragmatic cues in social media text.\n\nOur method introduces a discourse-aware content planner coupled with a constrained decoder to improve factuality and style consistency.",
    "reason": "Introduces and uses a specific dataset without providing a citation at first mention (rule a).",
    "start": 278,
    "end": 331,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most existing methods rely on transformer-based decoders for scene text recognition.",
    "document": "Related Work\n\nScene text recognition (STR) requires modeling both visual appearance and linguistic context under challenging conditions such as perspective distortion and motion blur. Hybrid CNN-RNN pipelines established early baselines by learning sequential dependencies over detected character features.\n\nMost existing methods rely on transformer-based decoders for scene text recognition.\n\nAlongside architectural innovations, lexicon constraints and language model priors have been explored to reduce errors on long strings and rare words. Nonetheless, performance deteriorates when training data underrepresents curved or stylized fonts.",
    "reason": "Makes a broad claim about the prevailing methodology without citing representative works to substantiate it (rule b).",
    "start": 308,
    "end": 392,
    "label": "Unsupported_claim"
  },
  {
    "span": "Redmon et al. (2016) proposed YOLO for real-time object detection. Carion et al. (2020) introduced DETR using transformers for end-to-end detection. Qi et al. (2019) developed PointRCNN for 3D object detection from point clouds.",
    "document": "Related Work\n\nObject detection and tracking have progressed rapidly due to advances in deep learning. Approaches vary from anchor-based detectors to transformer-based pipelines and 3D perception methods, each addressing different challenges in accuracy, speed, and robustness.\n\nRedmon et al. (2016) proposed YOLO for real-time object detection. Carion et al. (2020) introduced DETR using transformers for end-to-end detection. Qi et al. (2019) developed PointRCNN for 3D object detection from point clouds. Bewley et al. (2016) presented SORT for online tracking with simple data association.\n\nWe focus on cross-modal tracking that integrates appearance and depth cues, aiming to bridge 2D detection with 3D geometric consistency.",
    "reason": "The span enumerates disparate detection paradigms (2D real-time, transformer-based, and 3D point cloud) without explaining their relationships or providing transitions, leaving unclear why one follows the other.",
    "start": 278,
    "end": 506,
    "label": "Coherence"
  },
  {
    "span": "[Lee et al., 2020)",
    "document": "Related Work\n\nUncertainty estimation in deep classifiers is often addressed via Bayesian approximations or ensembling (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017). Recent approaches calibrate confidence using temperature scaling and focal adjustments for long-tailed data (Guo et al., 2017; Menon et al., 2021).\n\nApplied to open-set recognition, margin-based detectors [Lee et al., 2020) leverage background energy to reject unknowns, while generative scores provide complementary signals (Liu and Wang, 2022). We synthesize these directions with a unified loss that balances coverage and risk (Chen et al., 2023).",
    "reason": "Mismatched brackets in the citation: starts with '[' and ends with ')'; should be either [Lee et al., 2020] (numeric style) or (Lee et al., 2020) (author-year style).",
    "start": 382,
    "end": 400,
    "label": "Format"
  },
  {
    "span": "(Jones et al., 2017;, Lee, 2018)",
    "document": "Introduction\n\nCommunity detection identifies cohesive subgraphs that reveal functional modules in social and information networks. Classical methods optimize modularity or cut-based objectives (Girvan and Newman, 2002; Shi and Malik, 2000), while recent learning-based approaches leverage node features and higher-order structures (Kipf and Welling, 2017; Abu-El-Haija et al., 2020).\n\nRecent advances (Jones et al., 2017;, Lee, 2018) propose differentiable relaxations of modularity and mutual information that scale to large graphs. However, these methods can be sensitive to degree heterogeneity and fail under heavy-tailed community sizes. We introduce a degree-corrected contrastive framework that jointly optimizes locality and separability, improving stability across diverse topologies.\n\nWe benchmark on synthetic and real-world graphs, including citation, collaboration, and web networks, and analyze sensitivity to sparsity and attribute noise (Leskovec et al., 2009; Rozemberczki et al., 2020).",
    "reason": "Malformed multi-citation punctuation: extraneous semicolon before a comma inside the parentheses; should be formatted as (Jones et al., 2017; Lee, 2018).",
    "start": 401,
    "end": 433,
    "label": "Format"
  },
  {
    "span": "He et al. (2017) proposed NGCF to propagate collaborative signals on user–item graphs. Wang et al. (2019) introduced LightGCN by removing feature transformations. Sun et al. (2020) leveraged self-supervised learning signals on interaction graphs. Ying et al. (2018) developed PinSage for web-scale recommendation.",
    "document": "Related Work\n\nGraph-based recommendation leverages the relational structure between users and items to improve representation learning and ranking quality. Unlike traditional matrix factorization, graph methods can propagate high-order signals along the interaction topology and side-information edges, which has led to significant performance gains across platforms and benchmarks.\n\nHe et al. (2017) proposed NGCF to propagate collaborative signals on user–item graphs. Wang et al. (2019) introduced LightGCN by removing feature transformations. Sun et al. (2020) leveraged self-supervised learning signals on interaction graphs. Ying et al. (2018) developed PinSage for web-scale recommendation.\n\nOur work explores uncertainty-aware message passing on implicit-feedback graphs, aiming to stabilize propagation depth while preserving calibration under sparse interactions.",
    "reason": "The span lists multiple papers back-to-back without transitions or explanation of relationships, making the connection between cited works abrupt and implicit (issues a and b).",
    "start": 384,
    "end": 697,
    "label": "Coherence"
  },
  {
    "span": "There is a growing body of recent work on algorithmic recourse for credit scoring.",
    "document": "Related Work\n\nAlgorithmic recourse provides actionable changes that individuals can take to alter unfavorable model outcomes, complementing fairness interventions that measure group-level disparities. There is a growing body of recent work on algorithmic recourse for credit scoring. Formalizations typically optimize minimal feature changes under feasibility and causal constraints, while ensuring validity under model uncertainty (Ustun et al., 2019; Karimi et al., 2020). In practice, recourse faces challenges from distribution shift and correlated features in credit data. We introduce a robust, causally aware recourse method tailored to scorecard models.",
    "reason": "Uses 'growing body of recent work' without citing specific papers to substantiate the claim.",
    "start": 201,
    "end": 283,
    "label": "Unsupported_claim"
  },
  {
    "span": "Li et. al., 2020",
    "document": "Introduction\n\nTime series forecasting leverages both classical statistical models and recent deep architectures. ARIMA and ETS remain strong baselines (Hyndman and Athanasopoulos, 2018), while neural models capture nonlinear dependencies (Salinas et al., 2020; Wu et al., 2021). Recent surveys (Li et. al., 2020; Hyndman and Athanasopoulos, 2018) highlight the importance of probabilistic forecasts and covariate handling. Nevertheless, model selection under nonstationarity remains challenging.\n\nWe contribute a regime-aware ensembling method that adapts to structural breaks while maintaining calibrated uncertainty.",
    "reason": "Incorrect abbreviation in the citation: 'et. al.' should be 'et al.'; the correct form is 'Li et al., 2020'.",
    "start": 295,
    "end": 311,
    "label": "Format"
  },
  {
    "span": "The standard split for the BC5CDR dataset allocates 500 documents for training, 500 for development, and 500 for testing.",
    "document": "Introduction\n\nBiomedical concept recognition and relation extraction rely on curated corpora that capture real-world variability in terminology (Li et al., 2016; Wei et al., 2015). Pretrained biomedical language models have advanced state of the art across named entity recognition and chemical–disease relation tasks (Lee et al., 2020).\n\nThe standard split for the BC5CDR dataset allocates 500 documents for training, 500 for development, and 500 for testing. To ensure comparability, we adopt the same partitioning and adhere to the official evaluation script.",
    "reason": "Makes a concrete claim about the 'standard split' and dataset sizes without citing the dataset paper or guidelines (rule a for dataset; rule b for specific stats).",
    "start": 339,
    "end": 460,
    "label": "Unsupported_claim"
  },
  {
    "span": "Patel et al.",
    "document": "Introduction\n\nNeural approaches to document ranking have rapidly advanced with the advent of pre-trained language models, enabling stronger generalization across tasks (Zhang and Li, 2018; Cooper et al., 2020). However, the integration of symbolic priors with neural encoders remains underexplored in open-domain search (Rao and Singh, 2019). As demonstrated by Patel et al., hybrid rankers can bridge the gap between lexical matching and semantic retrieval, yet the optimal way to fuse signals is still debated. Recent studies advocate multi-stage pipelines that first retrieve with sparse methods and then re-rank with deep encoders (Johnson and Meyer, 2021), while others propose end-to-end differentiable retrieval (O’Neill, 2022). In this work, we investigate alignment losses that encourage consistency between lexical and semantic scores, extending prior observations on calibration for reranking (Kim and Alvarez, 2020).",
    "reason": "Narrative citation missing publication year; should be in the form \"Patel et al. (YEAR)\" rather than omitting the year.",
    "start": 362,
    "end": 374,
    "label": "Format"
  },
  {
    "span": "(Nguyen et al. 2021)",
    "document": "Introduction\n\nOpen-domain question answering (QA) systems commonly combine dense or sparse retrieval with neural readers (Karpukhin et al., 2020; Chen et al., 2017). Span-extractive models improved with pretrained encoders (Nguyen et al. 2021) and retriever-generator hybrids (Lewis et al., 2020). Effective training requires distillation from stronger retrievers and noise-robust objectives (Izacard and Grave, 2021; Ram et al., 2021). We propose uncertainty-aware sampling to reduce annotation cost.",
    "reason": "Missing comma before the year in a parenthetical citation; should be \"(Nguyen et al., 2021)\".",
    "start": 223,
    "end": 243,
    "label": "Format"
  },
  {
    "span": "[Garcia and Lee, 2018]",
    "document": "Related Work\n\nDomain adaptation methods range from importance weighting to adversarial alignment of feature distributions (Ben-David et al., 2010; Ganin and Lempitsky, 2015). In text classification, virtual adversarial training and consistency constraints reduce error on target domains (Miyato et al., 2017; Xie et al., 2020). According to [Garcia and Lee, 2018], aligning higher-order moments can further stabilize adaptation. Our approach complements these techniques by introducing curriculum-based target sampling and prototype refinement.\n\nWe evaluate on sentiment and topic classification with extensive cross-domain shifts.",
    "reason": "Wrong citation style for author–year context: square brackets are used instead of parentheses in an author–year style narrative.",
    "start": 341,
    "end": 363,
    "label": "Format"
  },
  {
    "span": "Previous shared tasks have already standardized the evaluation protocol for hate speech detection.",
    "document": "Related Work\n\nHate speech detection encompasses abusive language, offensive content, and targeted harassment, often requiring nuanced modeling of context, identity terms, and implicit slurs. Research has explored lexicon-based methods, classical machine learning with engineered features, and transformer-based architectures, as well as multilingual transfer and domain adaptation.\n\nPrevious shared tasks have already standardized the evaluation protocol for hate speech detection. Nevertheless, datasets vary widely in label schema and collection methodology, complicating cross-benchmark comparisons. We review common sources of annotation disagreement and propose a unified label mapping to facilitate robust evaluation.",
    "reason": "References shared tasks and a standardization claim without citing any specific competitions or task descriptions.",
    "start": 383,
    "end": 481,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent personalization methods in federated learning span meta-learning, client clustering, and knowledge distillation (Fallah et al., 2020; Arivazhagan et al., 2019; Collins et al., 2021; Zhang et al., 2021; Liang et al., 2020).",
    "document": "Related Work\n\nFederated learning and personalization\n\nFederated learning (FL) enables collaborative model training without centralized data collection by aggregating updates across clients (McMahan et al., 2017). While standard FL assumes a single global model, non-IID data distributions across clients often degrade performance, motivating approaches that personalize models to local conditions.\n\nRecent personalization methods in federated learning span meta-learning, client clustering, and knowledge distillation (Fallah et al., 2020; Arivazhagan et al., 2019; Collins et al., 2021; Zhang et al., 2021; Liang et al., 2020).\n\nPrivacy and communication efficiency\n\nBeyond personalization, privacy-preserving techniques (e.g., secure aggregation, differential privacy) and communication-efficient protocols (e.g., sparsification, quantization) are actively studied to make FL practical under real-world constraints (Bonawitz et al., 2017; Kairouz et al., 2021; Suresh et al., 2017). These orthogonal axes frequently trade off against accuracy and personalization quality.\n\nEvaluation protocols\n\nThere is a growing recognition of the need for standardized FL benchmarks with realistic heterogeneity, intermittent participation, and constrained resources (Caldas et al., 2018; He et al., 2020).",
    "reason": "The span merely enumerates categories and citations without clarifying what limitations remain or how the current work differs, thus lacking synthesis and gap articulation (criteria a and b).",
    "start": 399,
    "end": 628,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Nguyen et. al., 2022)",
    "document": "Related Work\n\nMultimodal sarcasm detection combines textual cues with visual or acoustic signals (Cai et al., 2019; Schifanella et al., 2016). Pretrained vision–language models show promise (Lu et al., 2019; Tan and Bansal, 2019). Datasets capture social media context (Castro et al., 2019). We compare against (Nguyen et. al., 2022) on cross-domain splits and evaluate robustness to image-only distractors.",
    "reason": "Incorrect abbreviation 'et. al.'; should be 'et al.' (no period after 'et').",
    "start": 311,
    "end": 333,
    "label": "Format"
  },
  {
    "span": "Neural decoders trained with MLE generate code tokens (Yin and Neubig, 2017). Search-based techniques use symbolic constraints (Alur et al., 2013). Executable feedback improves correctness through repair (Gupta et al., 2019). Pre-trained language models enable few-shot synthesis (Chen et al., 2021).",
    "document": "Related Work\n\nProgram synthesis from natural language. Mapping text to code has been approached with neural sequence models, grammar-constrained decoders, and hybrid neuro-symbolic frameworks (Yin and Neubig, 2017; Rabinovich et al., 2017). Execution guidance and test-based feedback can further refine candidates at inference time (Chen et al., 2018; Gupta et al., 2019).\n\nNeural decoders trained with MLE generate code tokens (Yin and Neubig, 2017). Search-based techniques use symbolic constraints (Alur et al., 2013). Executable feedback improves correctness through repair (Gupta et al., 2019). Pre-trained language models enable few-shot synthesis (Chen et al., 2021). We contribute a constrained decoding approach that integrates executable feedback into a PLM decoder with on-the-fly constraint relaxation.",
    "reason": "The span presents multiple approaches in isolated sentences without transitions or explicit connections. The relation between neural MLE, symbolic search, executable feedback, and PLMs is not explained, reducing coherence.",
    "start": 374,
    "end": 674,
    "label": "Coherence"
  },
  {
    "span": "[12]",
    "document": "Related Work\n\nMeta-learning methods aim to acquire inductive biases that enable rapid adaptation to new tasks. Several gradient-based approaches learn initialization points for fast fine-tuning (Finn et al., 2017; Nichol et al., 2018). A Bayesian view has also been explored in [12] with hierarchical priors, offering principled uncertainty estimates. We extend this line by introducing task-conditioned priors aligned with downstream calibration objectives.",
    "reason": "Numeric bracket citation used in an author–year context; wrong citation style for the rest of the document.",
    "start": 278,
    "end": 282,
    "label": "Format"
  },
  {
    "span": "Lee et al. [2019]",
    "document": "Related Work\n\nDifferential privacy (DP) provides rigorous guarantees for limiting information leakage from individual records during learning (Dwork et al., 2014). In empirical risk minimization, private stochastic gradient methods inject calibrated noise into gradients to bound sensitivity (Abadi et al., 2016; Mironov, 2017). Lee et al. [2019] extend this line by proposing adaptive clipping rules to reduce the noise required for a given privacy budget while maintaining accuracy.\n\nPrivate aggregation and federated settings introduce additional challenges such as client heterogeneity and communication costs (McMahan et al., 2017; Kairouz et al., 2021). Recent works combine secure aggregation with DP to protect both updates and model outputs (Truex et al., 2019; Geyer et al., 2017). Our method complements these advances by estimating per-parameter sensitivity and allocating privacy budgets via bilevel optimization.",
    "reason": "Wrong bracket style for author–year citation; should use parentheses, e.g., \"Lee et al. (2019)\" or \"(Lee et al., 2019)\" rather than square brackets.",
    "start": 329,
    "end": 346,
    "label": "Format"
  },
  {
    "span": "Lee and Park, 2015",
    "document": "Related Work\n\nPolicy-gradient methods optimize stochastic policies via gradient estimates of expected returns (Sutton and Barto, 2018; Schulman et al., 2017). We follow Lee and Park, 2015 to design the reward-shaping scheme that stabilizes training under sparse feedback and mitigates exploration collapse. Off-policy algorithms complement these approaches through replay buffers and target networks, enabling improved sample efficiency (Mnih et al., 2015; Haarnoja et al., 2018). Recent work studies variance reduction and trust-region constraints for stability (Schulman et al., 2015; Ahmed et al., 2020).",
    "reason": "Year placed without parentheses in a narrative citation; should be “Lee and Park (2015)” rather than “Lee and Park, 2015”.",
    "start": 169,
    "end": 187,
    "label": "Format"
  },
  {
    "span": "Earlier studies report that unit tests derived from docstrings capture only a small fraction of functional bugs.",
    "document": "Introduction\n\nLarge language models for code generation and repair have achieved notable gains on benchmarks that pair natural language with source code (Chen et al., 2021; Ahmad et al., 2021; Nijkamp et al., 2023). However, evaluation often hinges on unit tests whose coverage and oracle quality vary widely (Zheng et al., 2023; Pearce et al., 2022). Automated test generation from specifications and examples is an active area of research (Tufano et al., 2019; Pantridge et al., 2022).\n\nEarlier studies report that unit tests derived from docstrings capture only a small fraction of functional bugs. Motivated by this limitation, we propose a metamorphic testing protocol that stresses semantic equivalences and boundary conditions beyond docstring-derived examples.",
    "reason": "References unspecified 'earlier studies' without citing them (rule a/d).",
    "start": 489,
    "end": 601,
    "label": "Unsupported_claim"
  },
  {
    "span": "There are many recent works that explore bias mitigation in recommender systems.",
    "document": "Introduction\n\nPersonalized recommendation has benefited from advances in representation learning and large-scale implicit feedback modeling (He et al., 2017; Rendle et al., 2012). However, concerns about exposure bias, popularity bias, and disparate impact have highlighted the need for fairness-aware methods in ranking.\n\nThere are many recent works that explore bias mitigation in recommender systems. Some approaches reweight training examples to counteract selection biases, while others constrain ranking objectives to meet fairness criteria (Yao and Huang, 2017; Beutel et al., 2019). Causal modeling has also been proposed to disentangle user preference from platform-induced exposure (Schnabel et al., 2016).\n\nWe contribute a counterfactual learning-to-rank framework that jointly models user intent and system exposure dynamics. Our experiments span multiple public benchmarks with controlled interventions on exposure distributions.",
    "reason": "Vague mention of many recent works without citing any supporting references at the point of claim.",
    "start": 323,
    "end": 403,
    "label": "Unsupported_claim"
  },
  {
    "span": "GraphSAGE samples neighborhoods for scalability (Hamilton et al., 2017). Positional encodings introduce structural signals (Dwivedi et al., 2020). Message passing suffers from over-smoothing (Li et al., 2018). Subgraph methods aim to localize computation (Zhang and Chen, 2020).",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a standard tool for learning on relational data. However, scaling to large graphs and capturing long-range dependencies remain open challenges that motivate architectural modifications and sampling strategies.\n\nGraphSAGE samples neighborhoods for scalability (Hamilton et al., 2017). Positional encodings introduce structural signals (Dwivedi et al., 2020). Message passing suffers from over-smoothing (Li et al., 2018). Subgraph methods aim to localize computation (Zhang and Chen, 2020).\n\nThese developments suggest a tension between expressivity and efficiency in GNN design. We propose a hybrid backbone that combines structural signals with adaptive receptive fields to balance this trade-off.",
    "reason": "Multiple studies are enumerated without transitions or explanation of how they relate (e.g., sampling vs. positional encoding vs. over-smoothing), leaving the relationships implied rather than explicit.",
    "start": 266,
    "end": 544,
    "label": "Coherence"
  },
  {
    "span": "There are many recent works that explore multimodal QA in low-resource settings.",
    "document": "Introduction\n\nQuestion answering (QA) has advanced rapidly with the advent of large pre-trained language models and retrieval-augmented generation. While early systems focused on extractive QA over Wikipedia-like corpora, subsequent work has broadened the setting to include multi-hop reasoning, knowledge-grounded responses, and vision-language tasks that integrate images and text. Despite these improvements, robust QA under resource constraints—limited annotations, specialized domains, and low-bandwidth environments—remains challenging.\n\nThere are many recent works that explore multimodal QA in low-resource settings. Yet, systematic evaluations remain scarce, and most reported gains are sensitive to pretraining data composition, prompting strategy, and domain shift. In this paper, we propose a unified benchmark and methodology that controls for supervision level, cross-modal alignment, and input noise, enabling a fair comparison of modeling choices. We also release ablations examining the interaction between retrieval fidelity and answer calibration across domains.",
    "reason": "Mentions 'recent works' without providing any citations to support the claim, violating rule (d) for missing citations to recent works.",
    "start": 544,
    "end": 624,
    "label": "Unsupported_claim"
  },
  {
    "span": "Nguyen et al.",
    "document": "Introduction\n\nMultimodal alignment seeks to discover correspondences between signals such as audio, text, and motion. Prior work has studied contrastive representation learning for pairs and triples of modalities (Alayrac et al., 2020; Radford et al., 2021) and weak supervision from narrations and ambient sound (Harwath et al., 2016; Miech et al., 2020). While powerful, these methods typically assume abundant paired data and stable temporal structure.\n\nFollowing Nguyen et al., we investigate cross-modal matching in low-resource temporal settings where only sparse anchors are available. Our approach builds on self-supervised pretraining for time series (Franceschi et al., 2019) and adopts a differentiable alignment objective related to dynamic time warping (Cuturi and Blondel, 2017). We also integrate curriculum strategies inspired by progressive alignment in speech-text systems (Baevski et al., 2021; Chung et al., 2021).\n\nWe evaluate our method on narrated robotics traces and instructional videos, comparing against contrastive baselines (Chen et al., 2020) and alignment-free retrieval models (Patrick et al., 2021). Results indicate that our approach achieves higher recall with fewer paired samples, and ablations highlight the importance of temporal regularization.\n\nRelated work further includes multimodal transformers (Tsai et al., 2019; Akbari et al., 2021), weakly-supervised localization (Rohrbach et al., 2016), and cross-modal distillation (Gupta et al., 2016). Our contribution is orthogonal to architectural choices and focuses on the alignment loss and curriculum.",
    "reason": "Narrative citation missing year; in author-year style, a narrative reference should appear as \"Nguyen et al. (YEAR)\" rather than just \"Nguyen et al.\"",
    "start": 467,
    "end": 480,
    "label": "Format"
  },
  {
    "span": "Message passing neural networks formalize local aggregation over molecular graphs for property prediction (Duvenaud et al., 2015; Gilmer et al., 2017). Unsupervised graph pretraining leverages contrastive objectives to learn transferable node and graph representations (Velickovic et al., 2019; You et al., 2020; Hu et al., 2020). Reinforcement learning selects reaction pathways under synthesis constraints (Segler et al., 2018; Zhou et al., 2019).",
    "document": "Related Work\n\nGraph Neural Networks for Molecular Machine Learning\n\nGraph neural networks (GNNs) have become the de facto approach for molecular property prediction by representing atoms as nodes and bonds as edges (Wu et al., 2018; Yang et al., 2019). Inductive biases such as message passing and attention help capture local chemistry while readouts summarize graph-level information (Battaglia et al., 2018; Kearnes et al., 2016).\n\nMessage passing neural networks formalize local aggregation over molecular graphs for property prediction (Duvenaud et al., 2015; Gilmer et al., 2017). Unsupervised graph pretraining leverages contrastive objectives to learn transferable node and graph representations (Velickovic et al., 2019; You et al., 2020; Hu et al., 2020). Reinforcement learning selects reaction pathways under synthesis constraints (Segler et al., 2018; Zhou et al., 2019).\n\nRecent advances incorporate 3D geometry via equivariant architectures and distance-aware kernels to better capture stereochemistry (Schütt et al., 2018; Batzner et al., 2022). Data augmentation through subgraph masking and motif-aware perturbations further improves generalization (Rong et al., 2020; Zhao et al., 2022). However, pretraining strategies that explicitly encode inductive biases of chemical substructures remain underexplored.",
    "reason": "The sentence about reinforcement learning for reaction planning follows discussions on message passing and contrastive pretraining without any connective explanation, making the relationship between the cited areas unclear and transitions abrupt.",
    "start": 435,
    "end": 884,
    "label": "Coherence"
  },
  {
    "span": "(O'Neil et al. 2016)",
    "document": "Related Work\n\nSafe Exploration in Reinforcement Learning\n\nSafe RL seeks to optimize performance while respecting constraints during training and deployment (Garcıa and Fernández, 2015; Achiam et al., 2017). Constrained policy optimization and Lyapunov-based methods provide theoretical guarantees under certain assumptions (Chow et al., 2018; Berkenkamp et al., 2017). Risk-sensitive objectives adopt coherent risk measures to limit tail events (Tamar et al., 2015; Chow and Ghavamzadeh, 2014). Prior work on offline safe RL leverages uncertainty estimates to avoid unsafe actions (Fujimoto et al., 2019; Singh et al., 2020). In healthcare settings, (O'Neil et al. 2016) emphasized the need for transparent constraints when optimizing treatment policies. Our approach combines model-based reachability with distributional critics to enforce state-dependent safety margins.",
    "reason": "Missing comma before the year in parenthetical author–year citation: should be '(O'Neil et al., 2016)'.",
    "start": 650,
    "end": 670,
    "label": "Format"
  },
  {
    "span": "Bellemare et al. (2016) develop pseudo-count based exploration in sparse-reward domains. Haarnoja et al. (2018) propose soft actor-critic for entropy-regularized RL. Pathak et al. (2017) introduce curiosity-driven exploration via intrinsic motivation. Fortunato et al. (2018) design NoisyNets to encourage exploration through parameter noise.",
    "document": "Introduction\n\nEfficient exploration remains a central challenge in reinforcement learning (RL), especially under sparse rewards and long horizons. A variety of intrinsic motivation signals and stochasticity-inducing mechanisms have been proposed.\n\nBellemare et al. (2016) develop pseudo-count based exploration in sparse-reward domains. Haarnoja et al. (2018) propose soft actor-critic for entropy-regularized RL. Pathak et al. (2017) introduce curiosity-driven exploration via intrinsic motivation. Fortunato et al. (2018) design NoisyNets to encourage exploration through parameter noise.\n\nRecent work studies exploration-exploitation tradeoffs under function approximation and offline RL. Our method unifies state novelty and policy entropy within a constrained optimization framework.",
    "reason": "The span abruptly mixes exploration-specific methods with a general off-policy algorithm (SAC) and offers no transitions or rationale connecting them.",
    "start": 248,
    "end": 590,
    "label": "Coherence"
  },
  {
    "span": "Prompt-based classification has been explored extensively with PET and ADAPET (Schick and Schütze, 2021; Tam et al., 2021). Multilingual bitext mining reduces domain gaps (Artetxe and Schwenk, 2019). Discrete prompts can be found by gradient search or paraphrase mining (Shin et al., 2020; Gao et al., 2021).",
    "document": "Related Work\n\nPrompting and Instruction Tuning. Recent work repurposes language models by formatting tasks as cloze-style prompts rather than fine-tuning full output heads. In supervised settings, label words are chosen to map verbalized outputs back to classes, while instruction tuning scales this paradigm with diverse task sets to improve generalization.\n\nMultilingual Transfer. Cross-lingual generalization often hinges on shared subword vocabularies and alignment signals. Prior efforts leverage parallel data and monolingual corpora to reduce distributional gaps between languages and to improve zero-shot performance.\n\nPrompting for Classification. Prompt-based classification has been explored extensively with PET and ADAPET (Schick and Schütze, 2021; Tam et al., 2021). Multilingual bitext mining reduces domain gaps (Artetxe and Schwenk, 2019). Discrete prompts can be found by gradient search or paraphrase mining (Shin et al., 2020; Gao et al., 2021).\n\nContrastive Prompting and Calibration. Other threads examine calibration of verbalizers, prompt ensembling, and the use of contrastive objectives to stabilize predictions. These methods attempt to disentangle label semantics from surface forms, improving robustness under label shift.",
    "reason": "The span lists prompting methods, bitext mining, and discrete prompt search back-to-back without explaining how multilingual bitext mining relates to prompt-based classification or how prompt search connects to domain gaps, resulting in abrupt, unexplained connections.",
    "start": 657,
    "end": 965,
    "label": "Coherence"
  },
  {
    "span": "The Netflix Prize competition in 2009 catalyzed advances in matrix factorization that remain state of the art.",
    "document": "Introduction\n\nRecommender systems rely on modeling user–item interactions to infer preferences. Matrix factorization and its probabilistic variants have been foundational, with subsequent integration of side information and temporal dynamics. More recent models incorporate deep architectures to capture nonlinear structure in implicit feedback data.\n\nThe Netflix Prize competition in 2009 catalyzed advances in matrix factorization that remain state of the art. Building on this tradition, we revisit factorization through the lens of self-supervised learning, proposing contrastive objectives that better exploit sparse co-occurrence signals without requiring negative sampling heuristics.",
    "reason": "Mentions a specific competition and makes a historical/impact claim without citing sources; first mention of the competition should be cited (definition a and b).",
    "start": 352,
    "end": 462,
    "label": "Unsupported_claim"
  },
  {
    "span": "It is widely accepted that retrieval-augmented generation eliminates hallucination in QA.",
    "document": "Introduction\n\nLong-context question answering (QA) challenges models to retrieve and reason over dispersed evidence. Pretrained language models augmented with retrieval components have shown improvements in factuality and sample efficiency (Guu et al., 2020; Lewis et al., 2020). However, retrieval latency, stale indices, and domain coverage remain practical bottlenecks in production systems (Karpukhin et al., 2020; Izacard and Grave, 2021).\n\nIt is widely accepted that retrieval-augmented generation eliminates hallucination in QA. Contrary to this assumption, our experiments demonstrate persistent hallucinations when retrieval fails to surface pertinent evidence or when the reader over-generalizes from partial context. We introduce an evidence-verification module that calibrates confidence based on support consistency.",
    "reason": "Invokes a broad consensus claim about prior work without providing citations.",
    "start": 446,
    "end": 535,
    "label": "Unsupported_claim"
  },
  {
    "span": "Multilingual pretraining has been used to transfer acoustic and lexical knowledge to low-resource languages (Conneau et al., 2021; Baevski et al., 2020; Zhang et al., 2021; Pratap et al., 2020).",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) in low-resource settings faces both data scarcity and domain mismatch. Recent self-supervised models learn robust acoustic representations that reduce the need for transcribed audio but still rely on careful fine-tuning.\n\nCross-lingual transfer can improve performance by sharing subword units and phonetic structure, yet negative transfer may occur when languages diverge significantly or when domain-specific noise dominates.\n\nMultilingual pretraining has been used to transfer acoustic and lexical knowledge to low-resource languages (Conneau et al., 2021; Baevski et al., 2020; Zhang et al., 2021; Pratap et al., 2020).\n\nOur work decouples acoustic and lexical adaptation via a modular training recipe that aligns phonetic manifolds while keeping language modeling separate, yielding consistent gains across unrelated language pairs.",
    "reason": "The span merely notes that multilingual pretraining is applied, with citations, but does not explain relevance to the present method, highlight unresolved issues, or integrate insights; this is a lack of synthesis.",
    "start": 478,
    "end": 672,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Deep models for EHR prediction vary widely. Lipton et al. (2016) applied recurrent networks to clinical time series. Transformers can capture long-range dependencies across visits (Li et al., 2020). Survival models estimate time-to-event outcomes (Katzman et al., 2018). Data imputation reduces missingness bias (Che et al., 2018). Fairness-aware training mitigates disparities across demographic groups (Pfohl et al., 2019).",
    "document": "Related Work\n\nPredictive modeling with electronic health records (EHRs) spans risk stratification, length-of-stay estimation, and adverse event detection. The literature combines sequence modeling, structured prediction, and causal perspectives due to the complexity of clinical data and outcomes. Despite progress, practical deployment requires robustness to missingness and attention to equity.\n\nDeep models for EHR prediction vary widely. Lipton et al. (2016) applied recurrent networks to clinical time series. Transformers can capture long-range dependencies across visits (Li et al., 2020). Survival models estimate time-to-event outcomes (Katzman et al., 2018). Data imputation reduces missingness bias (Che et al., 2018). Fairness-aware training mitigates disparities across demographic groups (Pfohl et al., 2019).\n\nWe focus on a unified sequential-to-survival objective with calibrated risk estimates under missing-not-at-random assumptions.",
    "reason": "The span enumerates disparate strands (RNNs, Transformers, survival analysis, imputation, fairness) with no transitions or explicit linking, making it unclear how each cited work relates to the previous ones.",
    "start": 398,
    "end": 823,
    "label": "Coherence"
  },
  {
    "span": "Numerous defenses have been proposed against gradient leakage and inference attacks in federated learning (Hitaj et al., 2017; Zhu et al., 2019; Nasr et al., 2019; Sun et al., 2021).",
    "document": "Related Work\n\nPrivacy Threats in Federated Learning. FL exposes model updates that can reveal sensitive information about clients. Attacks include gradient inversion, membership inference, and property inference, spanning both white-box and black-box settings (Zhu et al., 2019; Melis et al., 2019; Nasr et al., 2019). Numerous defenses have been proposed against gradient leakage and inference attacks in federated learning (Hitaj et al., 2017; Zhu et al., 2019; Nasr et al., 2019; Sun et al., 2021). Differential privacy and secure aggregation are widely adopted to provide formal guarantees and limit information exposure (McMahan et al., 2017; Bonawitz et al., 2017).\n\nUtility–Privacy Trade-offs. The interaction between privacy mechanisms and model utility is influenced by client heterogeneity, participation frequency, and data non-IIDness (Kairouz et al., 2021; Li et al., 2020).\n\nWe investigate the impact of client drift on privacy-utility trade-offs under partial participation.",
    "reason": "The span lists defenses without explaining their limitations or relevance to client drift and partial participation, missing a synthesis with the study's focus (definition a and c).",
    "start": 319,
    "end": 501,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(2022, Patel et al.)",
    "document": "Related Work\n\nRobust optimization mitigates distribution shift by minimizing worst-case risk across predefined groups (Ben-Tal and Nemirovski, 1998; Sagawa et al., 2020). Some meta-analyses (2022, Patel et al.) indicate that instance reweighting can complement group robustness when group labels are noisy. We unify these threads with a principled objective that jointly calibrates risk and group uncertainty.",
    "reason": "Author–year order inverted in a parenthetical citation. Should be “(Patel et al., 2022)”.",
    "start": 190,
    "end": 210,
    "label": "Format"
  },
  {
    "span": "((Patel, 2017))",
    "document": "Related Work\n\nEarly approaches to text simplification assumed fixed-grade readability targets and relied on handcrafted rules ((Patel, 2017)) before neural sequence models dominated the field. Neural methods introduced controllable simplification via latent variables (Martin et al., 2019) and reinforcement learning for quality-control feedback (Zhao & Chen, 2020). Parallel corpora scarcity motivated unsupervised and weakly supervised techniques using back-translation and pseudo-parallel mining (Xu et al., 2020; Park & Cho, 2021).\n\nRecent research explores content-preserving editing with disentangled representations (Huang et al., 2022), showing improved fluency and meaning retention over prior systems.",
    "reason": "Extraneous double parentheses around the citation; should be a single pair, e.g., \"(Patel, 2017)\".",
    "start": 126,
    "end": 141,
    "label": "Format"
  },
  {
    "span": "Most commercial ASR systems still show over 20% WER on strong regional accents.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) has reached near human-level performance on curated benchmarks, yet significant errors persist for accented and dialectal speech. In global applications, accent robustness is critical for accessibility and fairness. Though large-scale self-supervised pretraining and multilingual modeling have improved overall accuracy, performance under accent shift remains brittle, particularly for spontaneous speech and noisy environments.\n\nReported benchmarks often underrepresent accent diversity, masking real-world failures. Most commercial ASR systems still show over 20% WER on strong regional accents. This paper introduces an accent-focused training and evaluation protocol that combines accent-balanced sampling, pronunciation-variant lexicons, and accent-adaptive adapters.\n\nRelated Work\n\nAccent-robust modeling has explored pronunciation modeling, domain adversarial training, test-time adaptation, and accent-aware language modeling. Data augmentation methods such as voice conversion and formant shifting target phonetic variability. However, consistent measurement across accents and domains is lacking.",
    "reason": "Provides a quantitative claim about error rates without any citation or evidence; per definition b and statistics requirement, this is unsupported.",
    "start": 567,
    "end": 646,
    "label": "Unsupported_claim"
  },
  {
    "span": "Johnson et al. (2017) demonstrated multilingual NMT enables zero-shot translation. Sennrich et al. (2016) introduced back-translation for leveraging monolingual data. Artetxe et al. (2018) explored unsupervised MT with denoising and iterative back-translation. Vocabulary sharing strategies have also been studied (Kudo, 2018).",
    "document": "Introduction\n\nLow-resource machine translation (MT) seeks to build effective systems in scenarios with limited parallel data. Common strategies exploit multilingual transfer, monolingual corpora, or architectural biases that improve sample efficiency. Despite progress, it remains difficult to obtain robustness across domains and scripts for truly low-resource settings.\n\nJohnson et al. (2017) demonstrated multilingual NMT enables zero-shot translation. Sennrich et al. (2016) introduced back-translation for leveraging monolingual data. Artetxe et al. (2018) explored unsupervised MT with denoising and iterative back-translation. Vocabulary sharing strategies have also been studied (Kudo, 2018).\n\nWe contribute a script-aware adapter that unifies multilingual transfer with monolingual augmentation, showing consistent gains in low-data regimes.",
    "reason": "The span is a sequence of citations with no explicit linking statements; the relationships between zero-shot transfer, back-translation, unsupervised MT, and vocabulary sharing are implied but not explained.",
    "start": 373,
    "end": 700,
    "label": "Coherence"
  },
  {
    "span": "The NAB benchmark has been criticized for label noise and inconsistent scoring across classes.",
    "document": "Related Work\n\nTime series anomaly detection underpins applications in IT operations, finance, and manufacturing. Traditional methods rely on statistical models and distance-based detectors, while recent approaches employ deep architectures with reconstruction or forecasting objectives. Benchmarks provide common grounds but vary widely in data quality and labeling conventions.\n\nThe NAB benchmark has been criticized for label noise and inconsistent scoring across classes.\n\nConcurrently, self-supervised pretext tasks have emerged to learn representations without labels. Our method integrates masked forecasting with seasonal priors to improve accuracy and interpretability on diverse datasets.",
    "reason": "Critiques a specific benchmark without citing any evidence or sources (violates rule b).",
    "start": 380,
    "end": 474,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith et al., (2018)",
    "document": "Related Work\n\nKnowledge graph completion leverages embedding models and neural link predictors to infer missing facts (Bordes et al., 2013; Trouillon et al., 2016). Path-based reasoning augments embeddings with multi-hop relational patterns to improve compositionality (Lin et al., 2015; Das et al., 2018). Smith et al., (2018) introduce a hybrid approach that combines rule mining with gradient-based scoring to retain interpretability while scaling to large graphs. More recent work explores pre-training entity and relation encoders using masked graph modeling objectives (Yao et al., 2019). Our method integrates rule templates as soft constraints during training to guide embeddings toward schema-consistent regions.",
    "reason": "Mixed narrative and parenthetical styles; should be either narrative “Smith et al. (2018)” or parenthetical “(Smith et al., 2018)”, not “Smith et al., (2018)”.",
    "start": 307,
    "end": 327,
    "label": "Format"
  },
  {
    "span": "Jaques et al.^3",
    "document": "Related Work\n\nHuman feedback has been used as a direct learning signal for dialogue, translation, and summarization. Jaques et al.^3 demonstrate that preference models can guide policy learning without token-level annotations. In QA, binary accept/reject signals have been explored as a cheaper proxy for span labels (Kratzwald et al., 2020). Our work adapts these ideas to extractive settings with limited seeding.",
    "reason": "Numeric superscript used in an author–year context; should include a year or be formatted according to a numeric citation style consistently.",
    "start": 117,
    "end": 132,
    "label": "Format"
  },
  {
    "span": "He et al. (2020) presented MoCo for momentum contrastive learning. Chen et al. (2020) introduced SimCLR with large batch training. Grill et al. (2020) proposed BYOL without negative samples. Dosovitskiy et al. (2021) showed that Vision Transformers can scale self-attention for image recognition.",
    "document": "Related Work\n\nSelf-Supervised Learning for Visual Representations\n\nContrastive and non-contrastive learning have emerged as powerful paradigms for learning image features without labels. Our goal is to leverage such pretraining signals for low-data fine-tuning on domain-specific tasks.\n\nHe et al. (2020) presented MoCo for momentum contrastive learning. Chen et al. (2020) introduced SimCLR with large batch training. Grill et al. (2020) proposed BYOL without negative samples. Dosovitskiy et al. (2021) showed that Vision Transformers can scale self-attention for image recognition.\n\nWe adapt a lightweight projection head to stabilize fine-tuning and study sample efficiency under distribution shift.",
    "reason": "The final sentence about Vision Transformers is not connected to the prior SSL methods; no transition explains why model architecture scaling is relevant to contrastive or non-contrastive approaches.",
    "start": 288,
    "end": 584,
    "label": "Coherence"
  },
  {
    "span": "In (Garcia et al., 2019)",
    "document": "Related Work\n\nFederated optimization has been explored for privacy-preserving learning across silos, with early algorithms focusing on communication efficiency (Konecný et al., 2016; McMahan et al., 2017). In (Garcia et al., 2019) the authors demonstrate that personalization layers can substantially reduce non-iid performance gaps without sharing raw data. Subsequent work extends this direction with meta-learning and regularization (Wang et al., 2020; Li et al., 2021), while others study robustness to adversarial clients (Blanchard et al., 2017; Pillutla et al., 2022). Our approach complements these efforts by explicitly modeling feature drift across participants.\n",
    "reason": "Wrong citation style: the preposition precedes a parenthetical citation. It should read \"In Garcia et al. (2019)\" rather than \"In (Garcia et al., 2019)\".",
    "start": 206,
    "end": 230,
    "label": "Format"
  },
  {
    "span": "Brown et al. (2020) showed that in-context learning scales with model size. Prompt tuning methods reduce storage costs (Lester et al., 2021). Retrieval-augmented generation improves factuality (Lewis et al., 2020). Chain-of-thought prompting elicits stepwise reasoning (Wei et al., 2022). Parameter-efficient adapters compress task updates (Houlsby et al., 2019).",
    "document": "Related Work\n\nPrompting and parameter-efficient adaptation have emerged as central strategies for harnessing large language models (LLMs) across diverse tasks. Early studies demonstrated that scaling transformer-based models leads to emergent in-context learning abilities, motivating research into how to steer these models without full fine-tuning. Concurrently, practical considerations such as compute, storage, and data efficiency have driven interest in methods that minimize per-task overhead while preserving generalization.\n\nBrown et al. (2020) showed that in-context learning scales with model size. Prompt tuning methods reduce storage costs (Lester et al., 2021). Retrieval-augmented generation improves factuality (Lewis et al., 2020). Chain-of-thought prompting elicits stepwise reasoning (Wei et al., 2022). Parameter-efficient adapters compress task updates (Houlsby et al., 2019).\n\nDespite these advances, it remains unclear how to select among these techniques for a given application or how they interact under distribution shift. Our work focuses on unifying objectives across prompting and lightweight adaptation to provide principled guidance for method selection.",
    "reason": "The sentences list disparate works (in-context learning, prompt tuning, retrieval, chain-of-thought, adapters) without transitions or explicit relationships among them, making the connection between cited works abrupt and unclear.",
    "start": 534,
    "end": 897,
    "label": "Coherence"
  },
  {
    "span": "Johnson et al., 2017",
    "document": "Related Work\n\nData augmentation has proven essential for robust generalization in vision, speech, and NLP. In text classification, synonym replacement and back-translation are widely used (Wei and Zou, 2019; Sennrich et al., 2016). Johnson et al., 2017 show that subword regularization can further improve robustness to noise in low-resource settings. Subsequent studies extend these ideas to semi-supervised regimes and multilingual transfer (Xie et al., 2020; Conneau et al., 2020).\n\nWe revisit augmentation for long-form generation, proposing structure-aware perturbations that preserve discourse.",
    "reason": "Narrative citation incorrectly formatted with a comma before the year; it should be \"Johnson et al. (2017) show ...\".",
    "start": 232,
    "end": 252,
    "label": "Format"
  },
  {
    "span": "Smith et al., (2020)",
    "document": "Related Work\n\nAspect-based sentiment analysis (ABSA) decomposes sentiment prediction into aspect extraction and sentiment classification (Pontiki et al., 2014; Liu, 2012). Sequence labeling with CRFs and neural taggers has been effective for aspect extraction (Ma and Hovy, 2016; Li et al., 2018), while attention mechanisms improve sentiment classification by focusing on aspect-relevant tokens (Wang et al., 2016; Ma et al., 2017). Pre-trained transformers further boost performance across ABSA subtasks (Devlin et al., 2019; Xu et al., 2019).\n\nJoint models alleviate error propagation by sharing representations between extraction and classification (Li et al., 2019; He et al., 2019). Multi-task setups incorporate opinion term detection and aspect-category assignment to leverage auxiliary signals (Chen and Qian, 2020; Dai and Song, 2019). Cross-domain adaptation remains challenging due to shifts in aspect vocabularies and sentiment expressions (Wang and Pan, 2018; Rietzler et al., 2020).\n\nWe compare against pipeline and joint baselines under both in-domain and cross-domain splits. Following prior work, we evaluate on SemEval and additional e-commerce reviews (Pontiki et al., 2014; He et al., 2019). Unlike Smith et al., (2020) who freeze the encoder during task-specific fine-tuning, we allow partial unfreezing and apply layer-wise learning rate decay to stabilize optimization across domains.",
    "reason": "Comma before the parenthetical year in a narrative citation is incorrect; it should be 'Smith et al. (2020)'.",
    "start": 1220,
    "end": 1240,
    "label": "Format"
  },
  {
    "span": "To the best of our knowledge, our work is the first to apply graph neural networks to biomedical event extraction.",
    "document": "Related Work\n\nBiomedical event extraction aims to identify complex interactions such as phosphorylation and gene regulation from scientific articles. Traditional approaches relied on pipeline architectures with feature engineering. To the best of our knowledge, our work is the first to apply graph neural networks to biomedical event extraction. Concurrent work explores dependency-based encoders and attention mechanisms but does not fully leverage document-level structure.",
    "reason": "A novelty/priority claim about prior work is made without citing comparative literature; such claims must be supported with references.",
    "start": 232,
    "end": 346,
    "label": "Unsupported_claim"
  },
  {
    "span": "Graph attention improves user-item modeling under sparsity (Velickovic et al., 2018; Wang et al., 2019). Negative sampling schemes are crucial to ranking quality (Rendle et al., 2009). Side information can be propagated across the graph (Hu et al., 2018). Self-supervision augments signals via contrastive losses (He et al., 2020).",
    "document": "Related Work\n\nGraph-based recommenders. Learning over user–item bipartite graphs enhances collaborative filtering by propagating preferences via message passing (Wang et al., 2019; He et al., 2020). Such models address sparsity by aggregating high-order neighborhoods but may oversmooth representations when depth increases (Chen et al., 2020).\n\nRegularization and sampling. Negative sampling and pairwise ranking losses are standard in implicit feedback recommenders, controlling the hardness of negatives to improve AUC and NDCG (Rendle et al., 2009; Yang et al., 2020). Auxiliary tasks like attribute prediction and denoising add inductive bias to stabilize training (Ma et al., 2021).\n\nGraph attention improves user-item modeling under sparsity (Velickovic et al., 2018; Wang et al., 2019). Negative sampling schemes are crucial to ranking quality (Rendle et al., 2009). Side information can be propagated across the graph (Hu et al., 2018). Self-supervision augments signals via contrastive losses (He et al., 2020). Our method unifies these perspectives by jointly learning attention weights guided by contrastive agreement while adaptively selecting negatives from side-information neighborhoods.",
    "reason": "The span strings together separate claims and citations without explaining how each relates to the previous one. The connection between attention, negative sampling, side information, and contrastive learning is implied but not articulated, leading to abrupt topic shifts.",
    "start": 690,
    "end": 1021,
    "label": "Coherence"
  },
  {
    "span": "RLHF has shown promise for aligning language models (Ziegler et al., 2020; Stiennon et al., 2020; Ouyang et al., 2022).",
    "document": "Introduction\nAs large language models become widely deployed, ensuring safe and helpful behavior is a critical challenge. Reinforcement learning from human feedback (RLHF) is a prominent framework for preference alignment.\n\nRelated Work\nRLHF has shown promise for aligning language models (Ziegler et al., 2020; Stiennon et al., 2020; Ouyang et al., 2022). Safety datasets target toxicity and harmful content (Gehman et al., 2020; Xu et al., 2021), while red-teaming methods expose failure modes (Perez et al., 2022). Alternatives include constitutional supervision (Bai et al., 2022) and direct preference optimization (Rafailov et al., 2023). We evaluate on safety benchmarks and human preference studies.",
    "reason": "The sentence states a general observation with citations but does not explain how those works relate to the authors’ method, what gaps persist, or the specific motivation for this paper.",
    "start": 237,
    "end": 356,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Policy evaluation methods include difference-in-differences (Card and Krueger, 1994; Bertrand et al., 2004), synthetic controls (Abadie and Gardeazabal, 2003; Abadie et al., 2010), panel event studies (Sun and Abraham, 2021), and doubly robust learners (Athey and Imbens, 2016; Chernozhukov et al., 2018).",
    "document": "Introduction\n\nEstimating causal effects of staggered policy adoption is challenging due to treatment heterogeneity, time-varying confounding, and interference across units. Recent work refines identification and inference but practical guidance under real-world violations remains limited.\n\nPolicy evaluation methods include difference-in-differences (Card and Krueger, 1994; Bertrand et al., 2004), synthetic controls (Abadie and Gardeazabal, 2003; Abadie et al., 2010), panel event studies (Sun and Abraham, 2021), and doubly robust learners (Athey and Imbens, 2016; Chernozhukov et al., 2018).\n\nWe present a diagnostics-first workflow combining placebo tests, sensitivity analysis, and flexible estimators to inform method selection under staggered rollout with anticipatory behavior.\n",
    "reason": "The span enumerates methods but does not explain their assumptions, limitations, or how the proposed workflow addresses a precise shortcoming, demonstrating lack of synthesis per (a) and (c).",
    "start": 291,
    "end": 596,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The NeurIPS 2018 Adversarial Vision Challenge established PGD as the de facto baseline for robustness evaluation.",
    "document": "Related Work\n\nAdversarial robustness research has spawned a variety of attacks, defenses, and benchmarking initiatives. Gradient-based methods such as FGSM and iterative variants remain standard components in evaluation suites, while certified defenses provide provable guarantees under bounded perturbations. The NeurIPS 2018 Adversarial Vision Challenge established PGD as the de facto baseline for robustness evaluation. Subsequent competitions broadened the threat models to include spatial and frequency perturbations, and encouraged unrestricted adversaries. Our work contributes a dataset with physically realizable corruptions and a layered protocol to disentangle robustness to content-preserving versus content-altering shifts.",
    "reason": "Mentions a specific competition and claims its impact without citing the challenge or related publications; per rule (a) shared tasks/competitions must be cited.",
    "start": 310,
    "end": 423,
    "label": "Unsupported_claim"
  },
  {
    "span": "{Park et al., 2021}",
    "document": "Introduction\n\nContext-Aware Recommendation\n\nPersonalized recommender systems have evolved from matrix factorization (Koren et al., 2009) to deep sequence models capturing user intent and temporal dynamics (Hidasi et al., 2016; Kang and McAuley, 2018). Context signals such as time, location, and device improve relevance when integrated into the scoring function (Adomavicius and Tuzhilin, 2011; Rendle et al., 2012). Recent graph-based recommenders model item-item relations via message passing (Wu et al., 2020; He et al., 2020). Prior work explored multi-task objectives to regularize user embeddings (Ma et al., 2018). In contrast, {Park et al., 2021} applied meta-learning to adapt to new users with few interactions, showing gains in cold-start scenarios. Our method unifies context modeling and adaptation by conditioning both the encoder and the scorer on learned context codes.",
    "reason": "Wrong delimiter style: author–year citation should use parentheses '(...)', not curly braces '{...}'.",
    "start": 636,
    "end": 655,
    "label": "Format"
  },
  {
    "span": "Lopez et al.",
    "document": "Related Work\n\nNeural text style transfer has been approached from disentanglement, controllable generation, and retrieval-augmented editing (Fu et al., 2018; John et al., 2019; Li et al., 2018). Lopez et al. introduce a contrastive loss to better separate content and style embeddings while preserving semantic fidelity, complementing paraphrase-based constraints (Prabhumoye et al., 2018) and back-translation objectives (Lample et al., 2019). Subsequent work explored plug-and-play conditioning with attribute classifiers (Dathathri et al., 2020) and reinforcement learning signals (Ziegler et al., 2019), but these approaches can suffer from reward hacking and loss of content.\n\nRecent retrieval-augmented methods ground edits in exemplar sentences to reduce hallucinations (Wu et al., 2020; Reid and Neubig, 2022). In domain adaptation, multi-task setups align representations across corpora while maintaining style control (Gururangan et al., 2020; Ben-David et al., 2020). For evaluation, automatic metrics such as style accuracy, semantic similarity, and fluency often disagree, motivating human studies (Mir et al., 2019; Carlson et al., 2018). Despite these advances, balancing content preservation and attribute strength remains challenging, particularly on low-resource domains and informal registers (Garcia et al., 2017; Huang et al., 2021).\n\nWe build on content-preserving constraints and retrieval signals to improve robustness under distribution shift, extending findings from paraphrastic augmentation (Wieting and Gimpel, 2018) and calibration in conditional generation (Desai and Durrett, 2020).",
    "reason": "Narrative citation missing year; should be formatted as 'Lopez et al. (YEAR)'.",
    "start": 195,
    "end": 207,
    "label": "Format"
  },
  {
    "span": "Zhang et al.",
    "document": "Related Work\n\nContrastive learning for sentence embeddings. Recent work has shown that instance discrimination objectives produce strong universal sentence representations (Gao et al., 2021; Carlsson et al., 2021). Zhang et al. introduced curriculum negatives to stabilize training under small batch regimes, while Wu and Zhao (2022) propose debiased loss variants to mitigate false negatives. More recently, hybrid supervised-unsupervised objectives were explored to align task-specific signals with general-purpose semantics (Meng et al., 2022; Giorgi et al., 2021).\n\nHard negative mining. Mining informative negatives is crucial for representation quality (Robinson et al., 2021; Xiong et al., 2021). Several approaches leverage in-batch retrieval to discover semantically close but non-identical sentences (Karpukhin et al., 2020; Izacard and Grave, 2021).",
    "reason": "Narrative citation is missing the publication year; it should follow the pattern “Zhang et al. (YEAR)”.",
    "start": 215,
    "end": 227,
    "label": "Format"
  },
  {
    "span": "Self-supervised learning in medical imaging includes contrastive methods (Chen et al., 2020; Chaitanya et al., 2020), context restoration and jigsaw tasks (Noroozi and Favaro, 2016; Taleb et al., 2020), masked autoencoding (He et al., 2022; Zhou et al., 2022), and multi-modal pretraining with radiology reports (Zhou et al., 2021; Boecking et al., 2022). Segmentation networks commonly build on U-Net variants (Ronneberger et al., 2015; Çiçek et al., 2016) and attention mechanisms (Oktay et al., 2018; Schlemper et al., 2019).",
    "document": "Related Work\n\nLabel scarcity in medical imaging motivates self-supervised and semi-supervised approaches that can transfer across scanners and protocols for robust segmentation.\n\nSelf-supervised learning in medical imaging includes contrastive methods (Chen et al., 2020; Chaitanya et al., 2020), context restoration and jigsaw tasks (Noroozi and Favaro, 2016; Taleb et al., 2020), masked autoencoding (He et al., 2022; Zhou et al., 2022), and multi-modal pretraining with radiology reports (Zhou et al., 2021; Boecking et al., 2022). Segmentation networks commonly build on U-Net variants (Ronneberger et al., 2015; Çiçek et al., 2016) and attention mechanisms (Oktay et al., 2018; Schlemper et al., 2019).\n\nWe study cross-protocol consistency pretraining with anatomically aligned perturbations to improve out-of-site segmentation without target-domain labels.",
    "reason": "The span compiles related methods and architectures but does not connect them to the authors’ cross-protocol consistency idea or articulate the unresolved gap, invoking (a) and (b).",
    "start": 179,
    "end": 707,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Dosovitskiy et al. (2021) introduce the Vision Transformer for image classification. He et al. (2016) design residual networks that enable very deep CNNs. Touvron et al. (2021) present DeiT with distillation to train ViTs from scratch. Redmon et al. (2016) propose YOLO for real-time object detection.",
    "document": "Introduction\n\nTransformer-based architectures have reshaped visual recognition by scaling attention over image patches and leveraging large-scale pretraining. Subsequent research compares these models with CNNs, explores data-efficient training, and extends them to detection and segmentation.\n\nDosovitskiy et al. (2021) introduce the Vision Transformer for image classification. He et al. (2016) design residual networks that enable very deep CNNs. Touvron et al. (2021) present DeiT with distillation to train ViTs from scratch. Redmon et al. (2016) propose YOLO for real-time object detection.\n\nFollow-up work integrates convolutions with attention, proposes hierarchical ViTs, and adapts training recipes to limited data. Our approach focuses on multi-scale attention aggregation to improve sample efficiency under low-data regimes.",
    "reason": "The sequence of citations mixes classification and detection work with no transitions, failing to explain the relevance of ResNet and YOLO to ViT developments.",
    "start": 295,
    "end": 596,
    "label": "Coherence"
  },
  {
    "span": "(Smith 2020)",
    "document": "Introduction\n\nFairness in machine learning examines and mitigates disparate outcomes across demographic groups (Barocas et al., 2019; Corbett-Davies and Goel, 2018). Group fairness metrics capture statistical parity and equalized odds, while individual fairness emphasizes similar treatment for similar individuals (Dwork et al., 2012). However, these metrics can be incompatible under distributional shifts (Kleinberg et al., 2017).\n\nCausal approaches define fairness via counterfactual reasoning over structural models (Kusner et al., 2017; Kilbertus et al., 2018). Meanwhile, post-processing and in-processing techniques enforce constraints during learning or adjust decisions at inference (Agarwal et al., 2018; Zafar et al., 2017). Risk-aware training reweights examples to trade off between utility and fairness (Hashimoto et al., 2018). Recent surveys highlight the need for context-specific choices of metrics and interventions (Smith 2020).\n\nWe propose a distributionally robust learner that optimizes worst-case fairness-utility trade-offs over plausible shifts, providing guarantees on both accuracy and disparity.",
    "reason": "Missing comma between author and year in parenthetical citation; should be '(Smith, 2020)'.",
    "start": 936,
    "end": 948,
    "label": "Format"
  },
  {
    "span": "There are roughly 500 million daily code-switching posts on social media.",
    "document": "Introduction\n\nCode-switching presents unique challenges for language modeling, tokenization, and downstream classification due to intra-sentential language alternation and orthographic variability (Solorio et al., 2014; Winata et al., 2019). Work on multilingual pretraining and adapters has improved robustness, yet performance still degrades under sparse supervision and noisy user-generated text (Conneau et al., 2020; Pfeiffer et al., 2020).\n\nDespite increasing interest in multilingual communities, the scale and dynamics of code-switching online remain under-characterized. There are roughly 500 million daily code-switching posts on social media. This magnitude underscores the need for scalable methods that adapt quickly to trending topics and emerging lexical borrowings.\n\nIn this paper, we introduce a continual learning approach that blends pseudo-labeling with confidence-based replay to reduce forgetting across evolving language pairs, and we evaluate on three streaming benchmarks with time-stamped posts.",
    "reason": "This is a quantitative statistic about the prevalence of code-switching that lacks a cited source or evidence.",
    "start": 580,
    "end": 653,
    "label": "Unsupported_claim"
  },
  {
    "span": "Text as treatment, confounder, or outcome has been modeled with embeddings (Roberts et al., 2020), topic models (Blei et al., 2003; Roberts et al., 2014), supervised classifiers (Keith et al., 2020), and deep representation learning for deconfounding (Johansson et al., 2016; Veitch et al., 2020).",
    "document": "Introduction\n\nCausal inference with text arises in policy analysis and biomedicine, where documents encode latent confounders and heterogeneous treatments. Estimation is complicated by high dimensionality, selection, and measurement error. We target semiparametric identification with text-derived covariates and stress-testing via sensitivity analysis.\n\nText as treatment, confounder, or outcome has been modeled with embeddings (Roberts et al., 2020), topic models (Blei et al., 2003; Roberts et al., 2014), supervised classifiers (Keith et al., 2020), and deep representation learning for deconfounding (Johansson et al., 2016; Veitch et al., 2020).\n\nWe propose a doubly robust estimator that integrates Neyman-orthogonal scores with textual nuisance models and provides simulation-based sensitivity bounds for residual confounding.",
    "reason": "The span summarizes prior modeling choices but does not articulate how they fall short or how the current paper’s estimator addresses a specific gap, satisfying (a) and (b).",
    "start": 355,
    "end": 652,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[Alvarez and Smith, 2016]",
    "document": "Related Work\n\nMultimodal fusion has progressed from simple concatenation to attention-based cross-modal alignment. Such strategies echo findings [Alvarez and Smith, 2016] indicating that modality-specific normalization stabilizes training. Later, Tsai et al. (2019) propose factorized bilinear pooling to disentangle shared and private signals, while Kiela et al. (2018) study robustness under missing modalities. Despite these gains, domain mismatch across modalities remains a key hurdle.",
    "reason": "Wrong brackets; in this style, in-text citations should use parentheses '(...)', not square brackets '[...]'.",
    "start": 145,
    "end": 170,
    "label": "Format"
  },
  {
    "span": "A common practice in AMR parsing is to pretrain on silver data generated by self-training",
    "document": "Related Work\n\nAbstract Meaning Representation (AMR) parsing has progressed from graph-based models to sequence-to-sequence approaches with constrained decoding. A common practice in AMR parsing is to pretrain on silver data generated by self-training, leveraging large unlabeled corpora to improve coverage before supervised finetuning. Graph relinearization strategies and alignment heuristics have also been used to reduce sparsity and enforce well-formedness.\n\nOur work revisits silver-data scaling laws and proposes noise-aware objectives that mitigate error amplification during iterative self-training.",
    "reason": "Asserts a field-specific common practice without citing supporting studies that establish it (rule b).",
    "start": 161,
    "end": 250,
    "label": "Unsupported_claim"
  },
  {
    "span": "According to industry reports, 80% of customer queries can be handled by chatbots.",
    "document": "Introduction\n\nTask-oriented dialogue systems are widely deployed in customer service to reduce agent load and improve response time. Despite recent progress in intent detection and slot filling, end-to-end systems still struggle with ambiguous user goals and out-of-domain requests. According to industry reports, 80% of customer queries can be handled by chatbots. Motivated by this potential impact, we propose a retrieval-augmented dialogue policy that leverages a knowledge base and historical resolutions to generalize better to rare intents, while maintaining low latency for production use.",
    "reason": "Presents a specific statistic attributed to industry reports without citing any source, which requires a reference.",
    "start": 283,
    "end": 365,
    "label": "Unsupported_claim"
  },
  {
    "span": "the MNIST-C dataset has become the de facto benchmark for noise-robust classification",
    "document": "Introduction\n\nRobust image classification in the presence of corruptions and perturbations remains challenging. In this work, we revisit corruption robustness in small-scale settings to enable controlled analysis. Over the past few years, the MNIST-C dataset has become the de facto benchmark for noise-robust classification, due to its suite of algorithmically generated corruptions and standardized evaluation protocol. Despite its simplicity, results on MNIST-C often predict relative trends observed on larger corruption suites.\n\nWe build on this observation and propose a lightweight evaluation harness that separates corruption type from intensity while maintaining class balance. We further compare modern augmentation strategies against corruption-aware training and report their effects across a range of intensities.",
    "reason": "First mention of a specific dataset and a field-wide status claim need citations; none are provided (rules a and b).",
    "start": 239,
    "end": 324,
    "label": "Unsupported_claim"
  },
  {
    "span": "A previous study found that enforcing demographic parity does not correlate with user trust in deployed systems.",
    "document": "Related Work\n\nAlgorithmic fairness research spans definitions, measurement, and interventions across stages of the ML pipeline. Group-based notions such as demographic parity and equalized odds formalize different desiderata that may be mutually incompatible under distributional constraints. Individual fairness emphasizes similar treatment for similar individuals but requires a task-specific similarity metric. In practice, stakeholders often care about procedural transparency and recourse as much as parity-based metrics. A previous study found that enforcing demographic parity does not correlate with user trust in deployed systems. Parallel lines of work examine causal perspectives that separate spurious from permissible pathways, and auditing methods that detect violations of fairness criteria post hoc. Recent deployments underscore the importance of intersectional analysis and subgroup robustness, particularly under distribution shift. Our work contributes a controller that treats fairness constraints as soft penalties during training and a calibration step that preserves utility while meeting regulatory thresholds across multiple jurisdictions.",
    "reason": "Refers to a specific prior study and its finding without citing it; first mentions of studies must be accompanied by citations (rule a).",
    "start": 527,
    "end": 639,
    "label": "Unsupported_claim"
  },
  {
    "span": "Message passing encodes local neighborhoods to predict molecular properties (Gilmer et al., 2017). Scalable training on billion-edge graphs has been addressed with sampling strategies (Hamilton et al., 2017). Protein structure prediction benefits from specialized equivariant architectures (Jumper et al., 2021).",
    "document": "Related Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction due to their ability to model relational structure. Early message passing neural networks demonstrated strong performance on quantum chemical properties and established a template for neighborhood aggregation (Gilmer et al., 2017). Subsequent improvements incorporated attention, edge features, and global pooling for better expressivity (Velickovic et al., 2018; Chen et al., 2019). Data augmentation and self-supervised pretraining have further improved generalization in low-data regimes (Hu et al., 2020; Rong et al., 2020).\n\nMessage passing encodes local neighborhoods to predict molecular properties (Gilmer et al., 2017). Scalable training on billion-edge graphs has been addressed with sampling strategies (Hamilton et al., 2017). Protein structure prediction benefits from specialized equivariant architectures (Jumper et al., 2021). While domain knowledge such as valence constraints has been integrated via graph rewiring or constraints (Dai et al., 2022), questions remain about efficiently leveraging 3D geometry at scale (Schütt et al., 2018; Satorras et al., 2021). Our work investigates hybrid 2D-3D encoders with uncertainty-aware training.",
    "reason": "The span abruptly shifts from molecular message passing to scalable graph sampling and then to protein structure prediction without clarifying the relationship between these topics, leaving the connections between cited works implicit and unclear.",
    "start": 623,
    "end": 935,
    "label": "Coherence"
  },
  {
    "span": "It is well-known that LSTMs struggle with long-range dependencies compared to Transformers.",
    "document": "Introduction\n\nSequence modeling architectures have evolved from recurrent networks to attention-based models. Recurrent neural networks, including LSTMs, process inputs sequentially and can suffer from vanishing gradients, whereas self-attention layers enable direct access across positions. It is well-known that LSTMs struggle with long-range dependencies compared to Transformers. Building on this observation, we focus on architectures that incorporate global receptive fields while controlling computational cost for long documents.\n",
    "reason": "Makes a claim about comparative model performance that appeals to prior knowledge ('well-known') without providing supporting citations.",
    "start": 292,
    "end": 383,
    "label": "Unsupported_claim"
  },
  {
    "span": "COCO has become the de facto standard benchmark for instance segmentation in the community.",
    "document": "Related Work\n\nInstance segmentation has seen significant progress with region-based methods and strong backbones enabling accurate mask prediction (He et al., 2017; Chen et al., 2019). Training such models typically relies on large-scale annotated datasets with diverse scenes and objects. COCO has become the de facto standard benchmark for instance segmentation in the community. Concurrently, methods focusing on open-vocabulary or long-tail instance segmentation highlight the limitations of closed-label training and motivate scalable supervision (Gupta et al., 2019; Zhou et al., 2022). Our work complements these directions by proposing a simple training recipe that improves rare category performance without additional annotations.",
    "reason": "Introduces and characterizes a dataset (COCO) without citing it at first mention, which requires a citation.",
    "start": 290,
    "end": 381,
    "label": "Unsupported_claim"
  },
  {
    "span": "[27]",
    "document": "Introduction\n\nNeural machine translation (NMT) models based on the Transformer architecture have achieved strong performance across language pairs (Vaswani et al., 2017; Ott et al., 2018). Advances include better regularization, subword segmentation, and multilingual transfer (Sennrich et al., 2016; Johnson et al., 2017; Arivazhagan et al., 2019).\n\nDomain and style adaptation remain open problems due to lexical and syntactic shifts (Chu and Wang, 2018; Freitag and Al-Onaizan, 2016). Back-translation and self-training leverage monolingual data to improve target-side fluency and adequacy (Sennrich et al., 2016; He et al., 2016). Some works propose controllable generation for formality and politeness (Niu and Carpuat, 2020; Rao and Tetreault, 2018).\n\nWhile prior evaluations typically report BLEU and COMET (Papineni et al., 2002; Rei et al., 2020), recent studies emphasize human-targeted error categories to diagnose specific weaknesses (Burchardt et al., 2017; Klubička et al., 2018). In contrast to these author–year citations, we also reference [27] for tokenization guidelines, which we integrate into our preprocessing pipeline.\n\nWe present a consistency-regularized objective that penalizes divergence between translations under synonym substitutions and paraphrastic perturbations.",
    "reason": "Numeric bracket citation used alongside author–year style; inconsistent/wrong citation style for the document.",
    "start": 1057,
    "end": 1061,
    "label": "Format"
  },
  {
    "span": "The BraTS dataset contains over 10,000 labeled scans.",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative model training without centralizing raw data, offering a promising paradigm for privacy-preserving medical imaging. Foundational FL algorithms address client heterogeneity and communication efficiency, while recent extensions consider personalization and robustness to system failures.\n\nIn neuro-oncology, segmentation of brain tumors is a critical application area where large, diverse datasets are beneficial. The BraTS dataset contains over 10,000 labeled scans. Yet institutional distribution shifts, scanner variability, and inconsistent labeling protocols pose significant challenges for federated optimization and evaluation.\n\nThis paper examines personalization strategies tailored to site-specific intensity statistics and proposes a simple aggregation adjustment that reduces overfitting to dominant clients in imbalanced participation regimes.",
    "reason": "Presents a specific statistic about a dataset without any supporting citation per rule (b).",
    "start": 472,
    "end": 525,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works propose decentralized RLHF training over edge devices.",
    "document": "Related Work\n\nAligning large language models with human preferences commonly uses reinforcement learning from human feedback (RLHF), combining supervised fine-tuning with a learned reward model and policy optimization (Ouyang et al., 2022; Bai et al., 2022). Alternatives such as direct preference optimization and implicit reward shaping attempt to stabilize training and reduce reward hacking (Rafailov et al., 2023; Touvron et al., 2023).\n\nScalability and privacy concerns have motivated federated and privacy-preserving optimization for foundation models (Kairouz et al., 2021). Recent works propose decentralized RLHF training over edge devices. However, the literature lacks systematic evaluations of reward drift, client heterogeneity, and incentive mechanisms in such settings.\n\nOur approach introduces a client-weighted preference aggregation scheme with differential privacy guarantees and shows robustness to skewed preference distributions in simulation.",
    "reason": "The sentence invokes unspecified 'recent works' proposing decentralized RLHF but provides no citations to the papers introducing these methods.",
    "start": 583,
    "end": 650,
    "label": "Unsupported_claim"
  },
  {
    "span": "FedAvg averages client updates to produce a global model (McMahan et al., 2017). Differential privacy adds calibrated noise to gradients (Abadi et al., 2016). Heterogeneous devices create stragglers in synchronous rounds (Bonawitz et al., 2019).",
    "document": "Related Work\n\nFederated Optimization. Cross-device learning distributes training across many clients while a central server coordinates aggregation. Communication efficiency and statistical heterogeneity remain core challenges.\n\nPrivacy and Systems Concerns. Ensuring privacy, reducing communication overhead, and handling client availability are intertwined issues that shape algorithm design and deployment.\n\nTechniques and Constraints. FedAvg averages client updates to produce a global model (McMahan et al., 2017). Differential privacy adds calibrated noise to gradients (Abadi et al., 2016). Heterogeneous devices create stragglers in synchronous rounds (Bonawitz et al., 2019).\n\nPersonalization. Beyond a single global model, recent approaches learn client-specific parameters or meta-initializations to adapt to local data distributions.",
    "reason": "The span stacks aggregation, privacy, and systems heterogeneity in separate sentences without explaining how they relate or influence each other, resulting in an abrupt, incoherent progression.",
    "start": 439,
    "end": 684,
    "label": "Coherence"
  },
  {
    "span": "In (Wang et al., 2020)",
    "document": "Introduction\n\nFederated optimization enables collaborative model training without centralizing raw data, providing privacy benefits and reducing communication overhead. In (Wang et al., 2020) we revisit the role of client heterogeneity and show that naive averaging can degrade performance under non-identical client distributions. Complementary work proposes adaptive aggregation, proximal regularization, and personalization layers to mitigate drift (Li et al., 2020; Karimireddy et al., 2020). Nevertheless, balancing fairness across clients remains challenging when participation is intermittent and data volumes vary widely.\n\nWe contribute a reweighting scheme that provably stabilizes aggregation while preserving performance on minority clients.",
    "reason": "Wrong citation style: the preposition 'In' should not precede a parenthetical citation; rewrite as 'In Wang et al. (2020) we ...' or remove 'In' and keep '(Wang et al., 2020)'.",
    "start": 169,
    "end": 191,
    "label": "Format"
  },
  {
    "span": "Hinton et al. (2015) formalized knowledge distillation for compressing neural networks. Sequence-level distillation improved machine translation efficiency (Kim and Rush, 2016). Patient-teacher frameworks have been proposed in speech recognition (Wang et al., 2020).",
    "document": "Related Work\n\nKnowledge distillation transfers information from a large teacher to a smaller student to reduce inference cost while maintaining accuracy. Approaches vary in the granularity of supervision (logits, intermediate features, or sequence outputs) and in the training regime (offline vs. online).\n\nHinton et al. (2015) formalized knowledge distillation for compressing neural networks. Sequence-level distillation improved machine translation efficiency (Kim and Rush, 2016). Patient-teacher frameworks have been proposed in speech recognition (Wang et al., 2020).\n\nBeyond compression, recent methods align internal representations, match gradients, or incorporate contrastive objectives. Our approach focuses on selective transfer tailored to task difficulty, complementing prior work by adapting supervision strength dynamically.",
    "reason": "The sentences mention three distinct contributions but lack transitions or an explicit explanation of how they relate to one another; the relationship among the cited works is not made clear.",
    "start": 307,
    "end": 573,
    "label": "Coherence"
  },
  {
    "span": "Users overwhelmingly prefer counterfactual explanations to feature-importance bars.",
    "document": "Introduction\n\nExplainable AI (XAI) aims to make model behavior transparent and actionable for end users (Miller, 2019). Popular approaches include post-hoc feature attributions (Ribeiro et al., 2016; Lundberg and Lee, 2017), counterfactual explanations (Wachter et al., 2017; Karimi et al., 2020), and example-based rationales (Kim et al., 2016). While technical metrics such as fidelity and sparsity are well studied, user-centered evaluation remains inconsistent across domains and tasks (Doshi-Velez and Kim, 2017; Rai, 2020).\n\nUsers overwhelmingly prefer counterfactual explanations to feature-importance bars. Building on this premise, we propose a hybrid interface that surfaces actionable counterfactuals alongside calibrated global summaries, and we evaluate its impact on trust calibration and decision quality in a within-subjects study.",
    "reason": "Claims a strong user preference without citation to user studies or surveys (rule b).",
    "start": 531,
    "end": 614,
    "label": "Unsupported_claim"
  },
  {
    "span": "To the best of our knowledge, this is the first multilingual dataset for sarcasm detection in news headlines.",
    "document": "Introduction\n\nSarcasm detection has gained attention as social and news media increasingly rely on short, punchy headlines and snippets where literal meaning diverges from intent. While prior efforts have focused predominantly on English microblogs and conversation threads, cross-lingual sarcasm recognition remains underexplored. In news headlines specifically, cues are subtle, context is sparse, and culturally grounded expressions complicate model transfer.\n\nWe introduce a new corpus of headlines in five languages with parallel annotations for sarcasm and related pragmatic cues. Our design emphasizes diversity of publication sources and topical coverage to reduce domain shift. To the best of our knowledge, this is the first multilingual dataset for sarcasm detection in news headlines. We benchmark several transformer-based baselines under zero-shot and few-shot transfer and analyze failure modes related to idiomaticity and world knowledge.",
    "reason": "Novelty claim about being the first dataset in a niche area is made without any citation or evidence supporting it.",
    "start": 687,
    "end": 796,
    "label": "Unsupported_claim"
  },
  {
    "span": "According to industry reports, 65% of IoT devices run outdated firmware",
    "document": "Introduction\n\nSecurity updates in Internet-of-Things (IoT) ecosystems are fragmented across vendors and operating environments, leading to patch delays and persistent vulnerabilities. According to industry reports, 65% of IoT devices run outdated firmware, exposing critical infrastructure to known exploits. This gap is exacerbated by proprietary update channels and weak attestation mechanisms.\n\nWe propose an automated attestation and staged rollout framework that verifies provenance of updates and monitors adoption across heterogeneous devices without violating privacy constraints.",
    "reason": "Presents a specific statistic without citing any source or report to support the numeric claim (rule b).",
    "start": 184,
    "end": 255,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works formalize disentanglement and causality in representations using interventions, invariance, and independence criteria (Peters et al., 2016; Schölkopf et al., 2021; Besserve et al., 2020).",
    "document": "Related Work\n\nCausal representation learning aims to recover factors that correspond to underlying generative mechanisms, enabling robust generalization under interventions and distribution shifts. This line of work intersects with disentanglement, domain generalization, and semi-supervised learning.\n\nRecent works formalize disentanglement and causality in representations using interventions, invariance, and independence criteria (Peters et al., 2016; Schölkopf et al., 2021; Besserve et al., 2020). Synthetic benchmarks provide controllable settings to evaluate the identifiability of latent factors, while real-world datasets test transfer under natural shifts (Locatello et al., 2019; Kumar et al., 2018). Methods range from explicit graphical modeling to implicit constraints embedded in neural objectives (Arjovsky et al., 2019; Ahuja et al., 2020).\n\nCounterfactual data augmentation and causal discovery from observational data have been proposed to bridge the gap between theory and practice in high-dimensional regimes (Pearl, 2009; Huang et al., 2020). Despite progress, reliably assessing causal properties remains challenging, and evaluation protocols are still evolving.\n\nThis overview situates causal representation learning within broader efforts toward robust and interpretable machine learning.",
    "reason": "The span summarizes a body of causal/disentanglement work but does not articulate how it relates to the authors’ approach, nor does it specify the gap or motivation driving the new study.",
    "start": 303,
    "end": 503,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Kang and McAuley (2018) model sequential user behavior with self-attention. Knowledge graph-based recommenders propagate preferences over entities (Wang et al., 2019). Contrastive learning has been applied to augment sparse implicit feedback (Xie et al., 2020).",
    "document": "Related Work\n\nRecommendation with Sequential Signals and Auxiliary Knowledge\n\nModern recommenders leverage sequence modeling, side information, and contrastive objectives to combat data sparsity. Session-based models emphasize short-term intent, while long-horizon histories capture evolving preferences. Kang and McAuley (2018) model sequential user behavior with self-attention. Knowledge graph-based recommenders propagate preferences over entities (Wang et al., 2019). Contrastive learning has been applied to augment sparse implicit feedback (Xie et al., 2020). Yet, it remains unclear how to unify these paradigms under a single objective with robust cold-start performance.\n\nEvaluation Protocols\n\nDifferent datasets and negative sampling strategies complicate fair comparison. We provide a standardized protocol with consistent sampling and calibration-aware metrics.",
    "reason": "Consecutive sentences cite disparate lines of work without clarifying their interrelations or transitions, creating an abrupt, incoherent flow.",
    "start": 305,
    "end": 566,
    "label": "Coherence"
  },
  {
    "span": "Dong and Lapata (2018) map natural language to logical forms with seq2seq. Ahmad et al. (2021) pretrain code transformers on large corpora. Chen et al. (2021) leverage unit tests for code generation evaluation. Liang et al. (2018) use execution-guided decoding to improve accuracy.",
    "document": "Related Work\n\nProgram synthesis from natural language covers semantic parsing to executable queries and code generation in general-purpose languages. Progress has stemmed from advances in sequence modeling, large-scale pretraining, and execution-aware inference.\n\nDong and Lapata (2018) map natural language to logical forms with seq2seq. Ahmad et al. (2021) pretrain code transformers on large corpora. Chen et al. (2021) leverage unit tests for code generation evaluation. Liang et al. (2018) use execution-guided decoding to improve accuracy.\n\nOur approach integrates pretraining with test-conditioned decoding via differentiable execution traces, enabling constraint satisfaction while preserving fluency.\n",
    "reason": "The span presents a list of disparate works with no transitions or explicit explanation of how they relate, creating abrupt shifts between semantic parsing, code pretraining, evaluation via unit tests, and execution-guided decoding.",
    "start": 264,
    "end": 545,
    "label": "Coherence"
  },
  {
    "span": "Most existing hate speech datasets focus on Twitter and Reddit, leaving TikTok largely unexplored.",
    "document": "Introduction\n\nAutomated detection of hate and abusive language is vital for moderating online platforms and protecting users from harm. While early work focused on surface lexical cues, recent approaches leverage context modeling, user metadata, and multimodal signals to capture implicit and coded expressions of hate. Cross-platform generalization remains problematic due to community norms, content formats, and topic distributions that vary widely across sites.\n\nDataset coverage plays a central role in benchmarking and transferability. Most existing hate speech datasets focus on Twitter and Reddit, leaving TikTok largely unexplored. This skews research toward short-text modalities and overlooks video-centric interactions where multimodal signals are prominent.\n\nWe introduce HATE-MultiTik, a multimodal dataset of short-form videos with synchronized transcripts, audio, and visual features, annotated for hate targets and severity. We provide strong baselines with text-only, audio-only, vision-only, and fusion models, and we analyze cross-platform transfer from Twitter and Reddit corpora to TikTok content.",
    "reason": "Makes a broad claim about the landscape of existing datasets and platform focus without citing surveys or dataset papers to substantiate it (b).",
    "start": 542,
    "end": 640,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT has been widely used for AES with domain-adaptive pretraining on essay corpora.",
    "document": "Introduction\n\nAutomated essay scoring (AES) aims to predict holistic and analytic scores for student writing (Shermis and Burstein, 2013). Early systems leveraged lexical and syntactic features with linear models, while neural models have improved performance by capturing discourse and semantics. BERT has been widely used for AES with domain-adaptive pretraining on essay corpora. Nevertheless, domain shift across prompts and genres remains a core challenge, especially under limited labeled data. We address this by introducing a prompt-invariant adapter that aligns representations across prompts while preserving local coherence cues.\n\nRelated Work\n\nPretrained language models have been adapted for AES through fine-tuning and multi-task setups (Uto et al., 2020; Ridley and Slater, 2021), but robust generalization to unseen prompts is still underexplored.",
    "reason": "This sentence asserts widespread prior use of BERT with domain-adaptive pretraining in AES but does not provide any citations to support the claim.",
    "start": 298,
    "end": 382,
    "label": "Unsupported_claim"
  },
  {
    "span": "Brown et al. (2020) demonstrated few-shot capabilities with large autoregressive models. Gao et al. (2021) proposed prompt-based fine-tuning for classification. Liu et al. (2021) surveyed prompt design strategies. Wei et al. (2022) introduced chain-of-thought prompting for reasoning.",
    "document": "Related Work\n\nPrompt-based learning aims to adapt pretrained language models to downstream tasks with minimal supervision. Early studies discussed the importance of selecting prompts and verbalizers to steer model behavior, and recent works explore reasoning-oriented prompting to unlock latent capabilities.\n\nBrown et al. (2020) demonstrated few-shot capabilities with large autoregressive models. Gao et al. (2021) proposed prompt-based fine-tuning for classification. Liu et al. (2021) surveyed prompt design strategies. Wei et al. (2022) introduced chain-of-thought prompting for reasoning.\n\nWhile these advances highlight the breadth of prompting techniques, our work focuses on calibrating prompts under distribution shift to improve reliability across unseen domains.",
    "reason": "Consecutive sentences list separate works without transitions or explicit relationships, making it unclear how each cited study connects to the others.",
    "start": 310,
    "end": 594,
    "label": "Coherence"
  },
  {
    "span": "DeepAutoencoder-based methods reconstruct normal patterns (Sakurada and Yairi, 2014). Matrix profile detects discords in time series (Yeh et al., 2016). GAN-based detectors synthesize anomalies (Schlegl et al., 2017). Self-supervised forecasting errors have been used as signals (Tuli et al., 2022).",
    "document": "Related Work\n\nTime Series Anomaly Detection\n\nDetecting rare deviations in multivariate sensor data is central to monitoring applications. Methods differ by whether they assume labeled anomalies, rely on reconstruction, or model predictive uncertainty.\n\nDeepAutoencoder-based methods reconstruct normal patterns (Sakurada and Yairi, 2014). Matrix profile detects discords in time series (Yeh et al., 2016). GAN-based detectors synthesize anomalies (Schlegl et al., 2017). Self-supervised forecasting errors have been used as signals (Tuli et al., 2022).\n\nHybrid approaches combine reconstruction and prediction to exploit complementary inductive biases. We extend this direction with a calibration layer that aligns error distributions across non-stationary regimes.",
    "reason": "The span cites disparate families (reconstruction, distance-based, generative, self-supervised) in separate sentences without transitions or explicit comparisons. The relationship between them is left implicit, creating coherence problems (criteria a and b).",
    "start": 253,
    "end": 552,
    "label": "Coherence"
  },
  {
    "span": "Koren et al. (2009) formulated matrix factorization for collaborative filtering. Rendle (2010) generalized interactions with factorization machines. He et al. (2017) combined neural networks with MF in NeuMF. Wang et al. (2019) incorporated knowledge graphs to enrich user-item relations. Adversarial training improves robustness (He et al., 2018).",
    "document": "Related Work\n\nRecommender Systems and Representation Learning\n\nA vast literature explores how to represent users and items to personalize rankings. We are interested in uncertainty-aware scoring functions that remain reliable under sparse signals.\n\nKoren et al. (2009) formulated matrix factorization for collaborative filtering. Rendle (2010) generalized interactions with factorization machines. He et al. (2017) combined neural networks with MF in NeuMF. Wang et al. (2019) incorporated knowledge graphs to enrich user-item relations. Adversarial training improves robustness (He et al., 2018).\n\nOur approach augments latent factors with calibrated predictive intervals to mitigate overconfident recommendations.",
    "reason": "Papers are listed without explicit relationships or transitions; the jump from knowledge graphs to adversarial training lacks explanation of how robustness ties to the preceding models.",
    "start": 249,
    "end": 597,
    "label": "Coherence"
  },
  {
    "span": "There are many recent works that explore multi-hop question answering across textual and multimodal sources.",
    "document": "Related Work\n\nQuestion answering (QA) has evolved from single-hop factoid retrieval to complex reasoning over multiple pieces of evidence. Early systems focused on extractive QA over short passages, while more recent approaches attempt to aggregate information across documents and modalities to answer compositional queries. There are many recent works that explore multi-hop question answering across textual and multimodal sources. Despite this progress, robust multi-hop reasoning remains challenging due to error propagation across hops and the need to align evidence drawn from heterogeneous inputs. Concurrent research on explainability emphasizes tracing the reasoning path, yet most methods still rely on heuristic selection of supporting facts. In this work, we integrate structured reasoning with a retrieval-augmented encoder to better capture cross-document dependencies, aiming to improve both answer accuracy and evidence coherence.",
    "reason": "Mentions recent works without providing any citations, violating the requirement that claims about prior work be supported with references.",
    "start": 326,
    "end": 434,
    "label": "Unsupported_claim"
  },
  {
    "span": "There have been many recent works that explore multimodal signals for hate detection.",
    "document": "Introduction\n\nAutomated hate speech detection has become a central problem in content moderation as platforms grapple with harmful language at scale. Early approaches relied on lexical features and classical classifiers to identify abusive content (Nobata et al., 2016; Waseem and Hovy, 2016; Davidson et al., 2017). With the advent of deep learning, contextual encoders and pretrained language models improved robustness to paraphrase and domain shift by leveraging large-scale unlabeled corpora (Howard and Ruder, 2018; Devlin et al., 2019).\n\nMore recently, the research community has begun to consider both visual and textual modalities in posts that combine memes, images, and captions, aiming to detect hateful intent expressed through image–text interactions. There have been many recent works that explore multimodal signals for hate detection. However, reproducibility remains a challenge due to heterogeneous annotation schemes and the scarcity of publicly available, well-documented datasets.\n\nIn this paper, we focus on aligning visual regions with toxic spans in accompanying text to disentangle content that is individually benign but jointly harmful. We evaluate on multiple social media benchmarks and release code and models to facilitate future comparison.",
    "reason": "The claim refers to 'many recent works' without providing any citations, violating the requirement that mentions of recent work be backed up by references.",
    "start": 766,
    "end": 851,
    "label": "Unsupported_claim"
  },
  {
    "span": "Anderson et al. (2018) introduced bottom-up attention for VQA. Agrawal et al. (2018) studied dataset biases and language priors. Hudson and Manning (2019) proposed GQA for compositional reasoning. Clark et al. (2019) identified confounding factors in VQA datasets.",
    "document": "Related Work\n\nVisual question answering (VQA) requires joint reasoning over images and text. Prior work has advanced attention mechanisms, diagnostics, and datasets to probe and improve compositionality and robustness.\n\nAnderson et al. (2018) introduced bottom-up attention for VQA. Agrawal et al. (2018) studied dataset biases and language priors. Hudson and Manning (2019) proposed GQA for compositional reasoning. Clark et al. (2019) identified confounding factors in VQA datasets.\n\nOur method complements these efforts by introducing counterfactual data augmentations that explicitly target spurious correlations between question types and visual regions.",
    "reason": "Multiple sentences cite distinct contributions without articulating their relationships or transitions, making the connection between attention methods and bias analyses implicit rather than explicit.",
    "start": 220,
    "end": 484,
    "label": "Coherence"
  },
  {
    "span": "(Smith 2019; Jones, 2020)",
    "document": "Related Work\n\nProgram repair techniques range from search-based synthesis to learning-guided mutation (Monperrus, 2018; Le Goues et al., 2012). Neural approaches leverage code representations to propose plausible fixes and rank patches (Tufano et al., 2019; Jiang et al., 2021). Studies have compared rule mining with transformer models on diverse benchmarks (Chen et al., 2019). Prior analyses (Smith 2019; Jones, 2020) emphasize the importance of dataset curation and evaluation metrics; however, inconsistent fault localization remains a key bottleneck. Our method introduces uncertainty-aware patch generation.",
    "reason": "Missing comma between author and year in the first parenthetical citation; should be '(Smith, 2019; Jones, 2020)'.",
    "start": 395,
    "end": 420,
    "label": "Format"
  },
  {
    "span": "[12]",
    "document": "Related Work\n\nObject detection has evolved from sliding-window classifiers to deep region proposals and single-shot models. Faster R-CNN established two-stage detection (Ren et al., 2015), while residual networks improved features (He et al., 2016). One-stage designs like SSD and YOLO achieve speed-accuracy trade-offs (Liu et al., 2016; Redmon and Farhadi, 2018). For anchor-free detection, see (Zhu et al., 2019). For YOLOv3, refer to [12]. Our approach integrates focal loss with transformer backbones (Carion et al., 2020) and evaluates latency on mobile hardware.",
    "reason": "Numeric bracket style used in an author–year context; citation style is inconsistent.",
    "start": 438,
    "end": 442,
    "label": "Format"
  },
  {
    "span": "Liu et al., 2021)",
    "document": "Introduction\n\nNeural machine translation (NMT) has progressed from recurrent sequence-to-sequence models (Bahdanau et al., 2015) to fully attention-based architectures (Vaswani et al., 2017). Data augmentation via back-translation (Sennrich et al., 2016) and self-training (He et al., 2020) further improved performance with monolingual corpora. Subsequent replications Liu et al., 2021) showed that scaling model size magnifies gains from synthetic data. Yet, most studies evaluate under narrow domain shifts, leaving robustness to noisy inputs insufficiently characterized.\n",
    "reason": "Missing opening parenthesis for a parenthetical citation; should be '(Liu et al., 2021)'.",
    "start": 370,
    "end": 387,
    "label": "Format"
  },
  {
    "span": "In (Klein and Manning, 2003)",
    "document": "Introduction\n\nSyntactic parsing remains a cornerstone for many downstream NLP tasks, including information extraction and semantic role labeling (Jurafsky and Martin, 2020; Hovy and Manning, 2013). Probabilistic models dominated early progress, with head-driven PCFGs and lexicalized approaches providing strong baselines (Collins, 1999; Charniak, 2000). In (Klein and Manning, 2003) the authors introduced unlexicalized parsing strategies that challenged prevailing assumptions about the necessity of lexicalization. Subsequent neural models replaced hand-engineered features with distributed representations, yielding substantial gains (Dyer et al., 2015; Dozat and Manning, 2017; Kitaev and Klein, 2018).\n\nNevertheless, cross-domain parsing performance lags behind in low-resource settings due to annotation mismatch and genre shifts (McClosky et al., 2010; Petrov and McDonald, 2012). Semi-supervised and self-training methods partially close this gap (Reichart and Rappoport, 2007; Yu et al., 2020), but robust generalization remains an open challenge.",
    "reason": "Wrong citation style: parenthetical citation used after a preposition ('In'), which should be narrative style 'In Klein and Manning (2003)'.",
    "start": 355,
    "end": 383,
    "label": "Format"
  },
  {
    "span": "Privacy-preserving segmentation in federated settings has been explored using weight averaging (McMahan et al., 2017), personalization layers (Arivazhagan et al., 2019), and client-specific normalization (Li et al., 2021). Secure aggregation and differential privacy mechanisms are also standard components in clinical FL (Bonawitz et al., 2017; Geyer et al., 2018; Kaissis et al., 2021).",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative training across institutions without centralizing data, which is crucial for medical imaging where privacy, governance, and heterogeneity are key constraints. Segmentation tasks amplify these challenges because label granularity varies across sites and scanners introduce domain shift.\n\nPrivacy-preserving segmentation in federated settings has been explored using weight averaging (McMahan et al., 2017), personalization layers (Arivazhagan et al., 2019), and client-specific normalization (Li et al., 2021). Secure aggregation and differential privacy mechanisms are also standard components in clinical FL (Bonawitz et al., 2017; Geyer et al., 2018; Kaissis et al., 2021).\n\nWe propose a cross-site consistency framework that aligns feature distributions via prototype matching while remaining compatible with standard FL protocols. Our experiments span multi-organ CT and brain MRI cohorts with varying annotation policies.\n",
    "reason": "The span summarizes related FL techniques but does not connect them to the proposed framework or articulate what limitation of these methods the paper addresses.",
    "start": 347,
    "end": 735,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Kim et. al., 2018)",
    "document": "Introduction\n\nDomain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled or sparsely labeled target domain (Pan and Yang, 2010; Ganin and Lempitsky, 2015). Adversarial approaches align feature distributions to reduce domain discrepancy, while self-training leverages pseudo-labels to bootstrap target performance (Tzeng et al., 2017; Zou et al., 2019).\n\nEarlier work (Kim et. al., 2018) demonstrates that combining instance reweighting with representation alignment can be particularly effective when label shift is present. We extend these ideas with calibrated pseudo-labeling and curriculum scheduling based on uncertainty estimates, improving robustness under severe class imbalance. Experiments on sentiment and topic classification benchmarks show consistent gains over strong baselines (Ben-David et al., 2010; Arjovsky et al., 2019).",
    "reason": "Incorrect abbreviation formatting: 'et. al.' should be 'et al.' without a period after 'et' in the author–year citation.",
    "start": 398,
    "end": 417,
    "label": "Format"
  },
  {
    "span": "(Kim, 2016, Park, 2017)",
    "document": "Related Work\n\nNeural information retrieval integrates representation learning with efficient indexing to match queries and documents in vector spaces (Rossi and Bell, 2019; Ahmed et al., 2020). Dual-encoder architectures scale to web corpora, while cross-encoders yield higher accuracy at greater cost (Santos and Hu, 2021).\n\nHybrid rerankers combine sparse and dense signals to capture exact matches and semantic relatedness (Kim, 2016, Park, 2017). We revisit training data quality and propose a contrastive curriculum that improves long-tail query performance without additional supervision (Li and Duarte, 2022).\n",
    "reason": "Multiple citations separated by a comma; in author–year style they should be separated by a semicolon '(Kim, 2016; Park, 2017)'.",
    "start": 426,
    "end": 449,
    "label": "Format"
  },
  {
    "span": "Building on these advances, we present an adversarial domain adaptation method for NER that aligns feature distributions across domains using a gradient reversal layer.",
    "document": "Related Work\n\nDomain adaptation for named entity recognition (NER) addresses distribution shifts across genres, topics, or time. Approaches include data augmentation via distant supervision (Pan et al., 2015), feature alignment with adversarial training (Ganin et al., 2016; Cao et al., 2018), and multi-task or meta-learning strategies that share parameters across domains (Peng and Dredze, 2017; Gui et al., 2020).\n\nPre-trained language models have further improved cross-domain NER by providing contextualized representations (Devlin et al., 2019; Liu et al., 2019). Several works adapt these models with instance reweighting (Jiang and Zhai, 2007; Wang et al., 2019), pseudo-labeling (Lee, 2013), or contrastive learning (Gunel et al., 2021).\n\nBuilding on these advances, we present an adversarial domain adaptation method for NER that aligns feature distributions across domains using a gradient reversal layer.",
    "reason": "The span introduces the authors’ method immediately after summarizing prior work without articulating a specific gap or why the method is needed, thereby lacking synthesis.",
    "start": 748,
    "end": 916,
    "label": "Lacks_synthesis"
  },
  {
    "span": "A wide range of techniques have been explored to address different aspects of federated learning, including secure aggregation protocols (Bonawitz et al., 2017), differential privacy mechanisms (Abadi et al., 2016; McMahan et al., 2018), personalization strategies (Smith et al., 2017; Arivazhagan et al., 2019), communication-efficient updates (Konečný et al., 2016; Sattler et al., 2019), client selection (Nishio and Yonetani, 2019), and robustness to byzantine clients (Blanchard et al., 2017; Pillutla et al., 2019). In healthcare, applications span medical imaging (Sheller et al., 2020), EHR modeling (Rieke et al., 2020), and mobile sensing (Hard et al., 2019).",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative model training across entities without centralizing data, a property that is particularly appealing in regulated domains like healthcare. Cross-silo healthcare FL faces practical constraints including non-IID distributions across institutions, strict privacy budgets, governance requirements, and intermittent connectivity. Our study focuses on federated risk prediction across hospitals under explicit privacy and compute constraints, where label shift and feature availability vary widely.\n\nA wide range of techniques have been explored to address different aspects of federated learning, including secure aggregation protocols (Bonawitz et al., 2017), differential privacy mechanisms (Abadi et al., 2016; McMahan et al., 2018), personalization strategies (Smith et al., 2017; Arivazhagan et al., 2019), communication-efficient updates (Konečný et al., 2016; Sattler et al., 2019), client selection (Nishio and Yonetani, 2019), and robustness to byzantine clients (Blanchard et al., 2017; Pillutla et al., 2019). In healthcare, applications span medical imaging (Sheller et al., 2020), EHR modeling (Rieke et al., 2020), and mobile sensing (Hard et al., 2019).\n\nBeyond algorithmic innovations, deployment-focused work studies incentives, auditability, and compliance. Nevertheless, real-world hospital consortia must often balance privacy budgets against model utility while coping with non-stationary data. We build on this practical setting to study training protocols that remain performant under tight privacy guarantees and shifting cohorts.",
    "reason": "The span lists prior FL methods and applications without relating them to the paper’s goals or identifying what gap remains, satisfying (a) and (c) of the definition.",
    "start": 553,
    "end": 1222,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Garcia and Lee, 2020",
    "document": "Related Work\n\nCross-lingual summarization has explored pivot-based and direct approaches alongside multilingual encoders. As noted in (Garcia and Lee, 2020 cross-lingual content selection can amplify source-side biases, motivating joint learning strategies (Ladhak et al., 2020; Zhu et al., 2021). We build on these insights by introducing coverage-aware decoding tailored to target-language salience.",
    "reason": "Missing closing parenthesis in the citation. Should be “(Garcia and Lee, 2020)”.",
    "start": 134,
    "end": 155,
    "label": "Format"
  },
  {
    "span": "(Zhou et al. 2016)",
    "document": "Introduction\n\nData augmentation is a longstanding technique to improve generalization in vision and NLP. In vision, label-preserving transformations such as flips and crops remain effective (Krizhevsky et al., 2012), while policy search discovers stronger compositions (Cubuk et al., 2019). In NLP, back-translation and noising augment scarce parallel data (Sennrich et al., 2016; Lample et al., 2018). Mixup interpolates examples and labels to regularize decision boundaries (Zhang et al., 2018). Manifold mixup and vicinal risk minimization extend these ideas to hidden states (Verma et al., 2019). For speech, spectrogram masking is widely adopted (Park et al., 2019). We study augmentation learned from gradients to target fragile features identified by influence functions (Koh and Liang, 2017; Zhou et al. 2016) and show complementary gains to standard policies.",
    "reason": "Missing comma before the year in the parenthetical citation; it should be '(Zhou et al., 2016)'.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "Goodfellow et al. (2015) proposed the fast gradient sign method for generating adversarial examples. Madry et al. (2018) formulated adversarial training as robust optimization. Tramèr et al. (2018) examined transferability across models. Xie et al. (2019) introduced input diversity to strengthen attacks.",
    "document": "Related Work\n\nAdversarial Robustness in Computer Vision\n\nRobustness research spans attack generation, certified defenses, and empirical adversarial training. While the community has converged on several evaluation protocols, the interplay among threat models, training regimes, and data augmentations remains a moving target.\n\nGoodfellow et al. (2015) proposed the fast gradient sign method for generating adversarial examples. Madry et al. (2018) formulated adversarial training as robust optimization. Tramèr et al. (2018) examined transferability across models. Xie et al. (2019) introduced input diversity to strengthen attacks.\n\nCertified Guarantees and Randomized Smoothing\n\nOrthogonal lines of work develop certified robustness via convex relaxations, randomized smoothing, and interval bound propagation. These methods trade off tightness of certificates with scalability, and are often evaluated under different norms than empirical defenses.\n\nOur Approach\n\nWe reconcile empirical and certified robustness by coupling adversarial training with certificate-aware data selection under a unified budget, enabling comparable evaluation across norms and architectures.",
    "reason": "Each cited work appears as a standalone sentence with no connective tissue explaining how they relate to one another or to the preceding discussion, resulting in poor coherence across sentences.",
    "start": 327,
    "end": 632,
    "label": "Coherence"
  },
  {
    "span": "Federated learning has been applied to medical imaging (Li et al., 2020; Xu et al., 2021), EHR prediction (Chen et al., 2019; Park et al., 2020), and mobile personalization (Kairouz et al., 2019; Bonawitz et al., 2019).",
    "document": "Introduction\nFederated learning (FL) enables collaborative model training without centralizing raw data, which is attractive for privacy-sensitive domains such as healthcare. Despite its promise, FL deployments face practical challenges including data heterogeneity across clients, limited communication bandwidth, and unreliable participation. In this paper, we study robustness to client drift under sporadic participation.\n\nRelated Work\nFederated learning has been applied to medical imaging (Li et al., 2020; Xu et al., 2021), EHR prediction (Chen et al., 2019; Park et al., 2020), and mobile personalization (Kairouz et al., 2019; Bonawitz et al., 2019). Prior work on tackling data heterogeneity explores client reweighting (Mohri et al., 2019), proximal regularization (Li et al., 2020b), and adaptive aggregation (Wang et al., 2021). Communication-efficient methods compress updates (Sattler et al., 2019) or reduce rounds via acceleration (Karimireddy et al., 2020). We build on the general FL setting with partial participation and heterogeneous data distributions.",
    "reason": "The sentence lists areas and citations where FL has been applied but does not explain how these studies relate to the present work, what limitations remain, or why they motivate the paper's focus on robustness to client drift.",
    "start": 440,
    "end": 659,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Brown et al. (2020) demonstrate few-shot capabilities with large language models. Perez et al. (2021) investigate prompt sensitivity and control tokens. Toxicity mitigation can be approached with decoding-time constraints (Gehman et al., 2020). Safety benchmarks like RealToxicityPrompts reveal risks in open-ended generation (Solaiman et al., 2019).",
    "document": "Related Work\n\nSafety and alignment for large language models (LLMs) have become central concerns as models scale and are deployed in open-ended applications. Prior work has examined the sources of harmful content, the role of data curation, and techniques for steering models toward safer behaviors without sacrificing utility. Methods range from dataset filtering and supervised fine-tuning to reinforcement learning from human feedback (RLHF), with growing interest in evaluation protocols that meaningfully capture real-world risk.\n\nBrown et al. (2020) demonstrate few-shot capabilities with large language models. Perez et al. (2021) investigate prompt sensitivity and control tokens. Toxicity mitigation can be approached with decoding-time constraints (Gehman et al., 2020). Safety benchmarks like RealToxicityPrompts reveal risks in open-ended generation (Solaiman et al., 2019).\n\nOther strands explore post-hoc editing and refusal training, red-teaming frameworks, and content filters that operate at inference time. Our work differs by systematically measuring the tradeoff between controllability and utility across multiple prompt-level interventions under matched training data and compute budgets.",
    "reason": "The paragraph enumerates four separate works as standalone statements without transitions or explanation of how each relates to the preceding one or to the overarching safety theme, creating abrupt shifts and implied but unstated connections across multiple sentences.",
    "start": 536,
    "end": 886,
    "label": "Coherence"
  },
  {
    "span": "In (Jones et al., 2020)",
    "document": "Related Work\n\nGraph neural networks (GNNs) have been applied to node classification and link prediction (Kipf and Welling, 2017; Hamilton et al., 2017). In (Jones et al., 2020) a scalable attention mechanism for graphs was introduced to alleviate over-smoothing. Subsequent methods incorporated positional encodings (Dwivedi et al., 2021) and graph transformers (Ying et al., 2021). We follow the message-passing paradigm but focus on label-efficient training.\n\nContrastive objectives for graphs have improved representation quality under limited supervision (Velickovic et al., 2019; You et al., 2020). Our method integrates structure-aware augmentations with consistency regularization.",
    "reason": "Wrong citation style; preposition should not precede a parenthetical citation. Use “in Jones et al. (2020)” or restructure the sentence.",
    "start": 153,
    "end": 176,
    "label": "Format"
  },
  {
    "span": "Classical methods include statistical tests and ARIMA residual analysis (Box and Jenkins, 1976; Twitter, 2015), while deep learning approaches deploy autoencoders, RNNs, and transformers (Malhotra et al., 2016; Xu et al., 2018; Lai et al., 2021).",
    "document": "Introduction\n\nDetecting anomalies in time series is essential for monitoring industrial processes, web services, and sensor networks. The challenges include concept drift, seasonality, and scarce labels.\n\nRelated Work\n\nClassical methods include statistical tests and ARIMA residual analysis (Box and Jenkins, 1976; Twitter, 2015), while deep learning approaches deploy autoencoders, RNNs, and transformers (Malhotra et al., 2016; Xu et al., 2018; Lai et al., 2021). Probabilistic forecasting with conformal or Bayesian calibration has been used to derive uncertainty-aware anomaly scores (Salinas et al., 2019; Kuleshov et al., 2018). Contrastive and self-supervised frameworks learn representations from unlabeled streams (Tonekaboni et al., 2021; Eldele et al., 2021).\n\nWe propose a drift-aware conformal detector that recalibrates thresholds using online seasonality-adjusted coverage guarantees.",
    "reason": "The span lists families of techniques and citations but does not relate them to the paper’s focus or explain deficiencies motivating the new method.",
    "start": 219,
    "end": 465,
    "label": "Lacks_synthesis"
  },
  {
    "span": "in (Kumar et al., 2018)",
    "document": "Related Work\n\nNeural ranking for passage retrieval has progressed through architectures that combine sparse and dense signals. Early dual-encoder systems emphasize scalability over fine-grained matching, while cross-encoders improve precision at higher computational cost (Park and Shin, 2019; Oliveira et al., 2020). We follow the evaluation protocol described in (Kumar et al., 2018) but consider domain shifts that arise in heterogeneous collections. Subsequent studies integrate query reformulation with feedback-driven re-ranking (Lopez and Ma, 2021; Chen and Xu, 2022), illustrating the benefits of iterative refinement over single-pass retrieval.",
    "reason": "Wrong citation style: narrative use with a leading preposition should be written as “in Kumar et al. (2018)”, not “in (Kumar et al., 2018)”.",
    "start": 362,
    "end": 385,
    "label": "Format"
  },
  {
    "span": "Secure aggregation masks client updates (Bonawitz et al., 2017). Differential privacy adds noise to reduce memorization (McMahan et al., 2018). Personalization layers adapt global models to heterogeneity (Arivazhagan et al., 2019).",
    "document": "Introduction\n\nFederated learning (FL) enables on-device training without centralizing raw data, but it raises new challenges for privacy, robustness, and personalization. Foundational protocols developed secure aggregation to ensure the server cannot see individual client updates (Bonawitz et al., 2017), and practical algorithms like FedAvg studied convergence under partial participation (McMahan et al., 2017). Subsequent work examined non-iid data, system heterogeneity, and communication constraints (Kairouz et al., 2021; Li et al., 2020).\n\nSecure aggregation masks client updates (Bonawitz et al., 2017). Differential privacy adds noise to reduce memorization (McMahan et al., 2018). Personalization layers adapt global models to heterogeneity (Arivazhagan et al., 2019). Recent efforts also target byzantine resilience and poisoning detection (Blanchard et al., 2017; Fang et al., 2020). In this paper, we study privacy-utility trade-offs under client drift with adaptive noise schedules.",
    "reason": "The span places three distinct strands of FL research—secure aggregation, differential privacy, and personalization—adjacent without transitions or explanation of how they relate, resulting in unclear coherence across the cited works.",
    "start": 548,
    "end": 779,
    "label": "Coherence"
  },
  {
    "span": "Smith et al (2020)",
    "document": "Introduction\n\nLearning from demonstrations enables rapid policy acquisition from expert behavior (Argall et al., 2009; Osa et al., 2018). In robotics, combining imitation with reinforcement can reduce unsafe exploration (Nair et al., 2018; Rajeswaran et al., 2018).\n\nTo address covariate shift, Smith et al (2020) propose dataset aggregation with uncertainty-aware querying, but their method assumes oracle feedback at all times, which is unrealistic in field deployments.\n\nWe propose a human-in-the-loop protocol that leverages selective annotation to reduce expert burden while maintaining safety.",
    "reason": "Missing period after “al.” in a narrative citation; should be “Smith et al. (2020)”.",
    "start": 295,
    "end": 313,
    "label": "Format"
  },
  {
    "span": "(Lopez et al. 2017)",
    "document": "Introduction\n\nSemantic segmentation has benefited greatly from encoder–decoder architectures and multi-scale context aggregation (Long et al., 2015; Chen et al., 2018). Atrous convolutions capture long-range context without losing resolution (Yu and Koltun, 2016), and feature pyramid networks enable robust multi-level fusion (Lin et al., 2017). Weakly supervised techniques exploit image-level tags and saliency cues to reduce annotation cost (Papandreou et al., 2015; Ahn and Kwak, 2018). For domain shifts between synthetic and real images, style transfer and self-training are commonly used (Hoffman et al., 2018; Zou et al., 2018). A label refinement strategy was proposed in (Lopez et al. 2017) to iteratively improve pseudo-masks.",
    "reason": "Missing comma before year in parenthetical citation: it should be '(Lopez et al., 2017)' rather than '(Lopez et al. 2017)'.",
    "start": 682,
    "end": 701,
    "label": "Format"
  },
  {
    "span": "((Johnson, 2017))",
    "document": "Introduction\n\nMultimodal learning integrates complementary signals such as text, audio, and vision to improve robustness and generalization (Baltrusaitis et al., 2019; Tsai et al., 2019). Cross-modal transformers align representations through attention mechanisms that capture fine-grained interactions between modalities (Lu et al., 2019; Chen et al., 2020). Early fusion models provide dense interactions but can be computationally expensive, whereas late fusion trades interactions for efficiency (Kiela et al., 2018).\n\nRecent studies investigate modality dropout and gating to handle missing or noisy inputs during inference ((Johnson, 2017)); Neverova et al. (2016) show that adaptive modality selection can further improve performance under partial observations. We propose a sparse cross-modal router that dynamically allocates attention capacity to the most informative modalities on a per-example basis.",
    "reason": "Double parentheses around a parenthetical citation; should be a single set of parentheses.",
    "start": 629,
    "end": 646,
    "label": "Format"
  },
  {
    "span": "The WMT20 robustness task highlighted that BLEU fails under domain shift.",
    "document": "Related Work on Robust Machine Translation\nRobustness in machine translation (MT) concerns maintaining translation quality under distribution shifts such as noise, domain changes, and adversarial perturbations. Prior work has examined token-level corruptions, social media text, and code-switching inputs, proposing both data augmentation and architecture-level solutions.\nThe WMT20 robustness task highlighted that BLEU fails under domain shift. This observation has motivated alternative evaluation protocols and metrics that better capture semantic adequacy and robustness to perturbations.\nBuilding on this line of inquiry, we target open-domain shifts and propose test suites that decompose robustness by phenomenon, enabling finer-grained diagnosis of failure modes.",
    "reason": "Mentions a specific shared task and a concrete conclusion without citing the task or providing evidence.",
    "start": 373,
    "end": 446,
    "label": "Unsupported_claim"
  },
  {
    "span": "Active learning selects informative instances using uncertainty sampling, diversity, or expected model change (Settles, 2012; Gal et al., 2017; Sener and Savarese, 2018; Ash et al., 2020).",
    "document": "Introduction\n\nLabeling data remains a major bottleneck for high-performing models, especially in specialized domains where annotators are scarce and labels are expensive. Active learning aims to reduce labeling cost by querying the most informative instances.\n\nModern deep active learning must contend with overconfidence, representation drift, and distribution shift between labeled and unlabeled pools, which can undermine classical selection strategies.\n\nActive learning selects informative instances using uncertainty sampling, diversity, or expected model change (Settles, 2012; Gal et al., 2017; Sener and Savarese, 2018; Ash et al., 2020).\n\nWe propose a pool-based strategy that couples calibration-aware uncertainty with feature-space coverage guarantees, achieving strong label efficiency across shifts without access to pool labels.",
    "reason": "The span provides a generic taxonomy and citations without explaining their relevance to the specific challenges identified or how they motivate the new strategy; it lacks synthesis.",
    "start": 458,
    "end": 646,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Prior studies consistently show that users prefer neural abstractive summaries over extractive baselines in at least 70% of cases.",
    "document": "Related Work\n\nSummarization methods can be broadly categorized as extractive or abstractive. Extractive approaches select salient sentences, whereas abstractive models generate novel phrasings that may better capture document gist. Human evaluation is often considered the gold standard for assessing summary quality along dimensions such as coherence and informativeness. Prior studies consistently show that users prefer neural abstractive summaries over extractive baselines in at least 70% of cases. However, factual consistency remains a key weakness for many abstractive systems, motivating research on faithfulness and controllability.\n",
    "reason": "Claims a specific statistical preference rate ('at least 70%') and references unspecified 'prior studies' without citing any sources.",
    "start": 373,
    "end": 503,
    "label": "Unsupported_claim"
  },
  {
    "span": "Back-translation and multilingual transfer remain the dominant strategies for low-resource neural machine translation (Sennrich et al., 2016; Johnson et al., 2017; Neubig and Hu, 2018; Edunov et al., 2018). Our approach combines lexical constraints with iterative decoding.",
    "document": "Related Work\n\nLow-resource neural machine translation (NMT) suffers from data scarcity, domain mismatch, and morphological complexity. Various strategies exploit monolingual data, multilingual corpora, and external lexicons to mitigate these limitations.\n\nBack-translation and multilingual transfer remain the dominant strategies for low-resource neural machine translation (Sennrich et al., 2016; Johnson et al., 2017; Neubig and Hu, 2018; Edunov et al., 2018). Our approach combines lexical constraints with iterative decoding.\n\nAugmenting with dictionaries and mined bitext has improved coverage for rare words but introduces noise that complicates training (Artetxe and Schwenk, 2019; El-Kishky et al., 2020). Recent pre-trained sequence-to-sequence models provide strong cross-lingual priors yet still rely on careful fine-tuning and domain adaptation (Liu et al., 2020; Tang et al., 2021).",
    "reason": "The span transitions from a list of dominant strategies to the authors' method without explicitly highlighting what gap remains in those strategies that the proposed lexical constraints and decoding are intended to address.",
    "start": 256,
    "end": 529,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Dimensionality reduction reveals latent cell states (Butler et al., 2018). Pseudotime methods order cells along trajectories (Trapnell et al., 2014). Batch correction aligns datasets across labs (Haghverdi et al., 2018). Spatial transcriptomics adds anatomical context (Ståhl et al., 2016).",
    "document": "Related Work\n\nSingle-cell transcriptomics has uncovered heterogeneous cellular programs, requiring computational tools for denoising, integration, and dynamic modeling. Recent methods address representation learning, trajectory inference, and cross-sample harmonization. We target robust integration of spatial and single-cell profiles.\n\nDimensionality reduction reveals latent cell states (Butler et al., 2018). Pseudotime methods order cells along trajectories (Trapnell et al., 2014). Batch correction aligns datasets across labs (Haghverdi et al., 2018). Spatial transcriptomics adds anatomical context (Ståhl et al., 2016).\n\nDespite progress, current pipelines do not jointly optimize alignment and spatial coherence under varying capture efficiencies. We propose a probabilistic model that couples spatial priors with variational integration.",
    "reason": "Each sentence cites a separate technique without linking statements or transitions to clarify their relationships, making the connection between works abrupt and unclear.",
    "start": 338,
    "end": 628,
    "label": "Coherence"
  },
  {
    "span": "The WNUT-17 dataset is noisy and small, making it an ideal testbed for robust NER.",
    "document": "Related Work\n\nRobust Named Entity Recognition. Social media NER presents challenges due to informal orthography and domain drift (Ritter et al., 2011; Derczynski et al., 2017). Subword modeling and contextual embeddings have improved robustness across domains (Akbik et al., 2018; Devlin et al., 2019). The WNUT-17 dataset is noisy and small, making it an ideal testbed for robust NER. Adversarial training and self-training have been proposed to mitigate annotation scarcity (Yang et al., 2018; Meng et al., 2021).",
    "reason": "First mention of a specific dataset (WNUT-17) with evaluative claims lacks a citation.",
    "start": 303,
    "end": 385,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most earlier approaches rely on bilingual lexicons induced from identical strings or numerals to seed alignment.",
    "document": "Related Work\n\nCross-lingual word and sentence representations support transfer to low-resource languages by aligning semantic spaces across typologically diverse corpora. Unsupervised methods often exploit distributional similarities and adversarial objectives to align embeddings across languages, while supervised variants use small seed dictionaries or parallel corpora to guide training. Most earlier approaches rely on bilingual lexicons induced from identical strings or numerals to seed alignment. More recent work incorporates multilingual pretrained encoders to obtain shared subword representations, improving robustness in distant language pairs. Our contribution complements these techniques by introducing a neighborhood consistency loss that reduces hubness during mapping without additional supervision.",
    "reason": "Claims a common practice in earlier approaches without citing representative works.",
    "start": 392,
    "end": 504,
    "label": "Unsupported_claim"
  },
  {
    "span": "Propensity score methods estimate treatment assignment probabilities to adjust for confounding (Rosenbaum and Rubin, 1983). GAN-based counterfactual estimation learns potential outcomes using adversarial training (Yoon et al., 2018). Causal diagrams formalize assumptions about interventions and identifiability (Pearl, 2000). Deep instrumental variable methods address endogeneity via two-stage networks (Hartford et al., 2017).",
    "document": "Introduction\n\nCausal Inference with Deep Learning\n\nEstimating causal effects from observational data requires strong assumptions and careful modeling. Recent work explores combining classical identification strategies with representation learning to mitigate confounding and selection bias.\n\nFoundations and Neural Estimators\n\nPropensity score methods estimate treatment assignment probabilities to adjust for confounding (Rosenbaum and Rubin, 1983). GAN-based counterfactual estimation learns potential outcomes using adversarial training (Yoon et al., 2018). Causal diagrams formalize assumptions about interventions and identifiability (Pearl, 2000). Deep instrumental variable methods address endogeneity via two-stage networks (Hartford et al., 2017).\n\nOur Contribution\n\nWe present a causally-regularized representation learner that jointly optimizes balance and outcome prediction with identification-aware constraints derived from graphical criteria.",
    "reason": "The span lists diverse causal tools and neural methods in isolation without articulating how they connect (e.g., how propensity scores relate to GANITE or Deep IV); transitions and explicit relationships are missing.",
    "start": 327,
    "end": 756,
    "label": "Coherence"
  },
  {
    "span": "Li et al., 2019; Kim and Park, 2020)",
    "document": "Related Work\n\nEvaluation suites for multimodal reasoning increasingly emphasize compositional generalization and bias control. Early visual question answering datasets revealed strong annotation artifacts that models could exploit without genuine vision–language grounding (Goyal et al., 2017; Agrawal et al., 2018). Several benchmarks compare models Li et al., 2019; Kim and Park, 2020) across tasks such as referring expression comprehension and caption-based retrieval, yet they typically under-specify linguistic phenomena.\n\nWe contribute a linguistically controlled testbed that isolates agreement, quantification, and coreference, following recommendations from diagnostic NLP evaluations (Hupkes et al., 2020; Warstadt et al., 2019). Our analysis highlights discrepancies between unimodal and multimodal cues, and proposes counterfactual perturbations to assess true grounding.",
    "reason": "Missing opening parenthesis for a parenthetical citation group; the citation list should be enclosed like \"(Li et al., 2019; Kim and Park, 2020)\".",
    "start": 351,
    "end": 387,
    "label": "Format"
  },
  {
    "span": "(Park et. al., 2018)",
    "document": "Related Work\n\nKnowledge distillation transfers supervision from a high-capacity teacher to a smaller student via softened targets and intermediate feature alignment (Hinton et al., 2015; Romero and Ballas, 2017). Recent progress (Park et. al., 2018) explores relational distillation losses that match pairwise sample similarities, while others align attention maps across layers (Sun and Lin, 2019). We propose curriculum-aware distillation that stages teacher signals by difficulty estimated from calibration metrics (Yu and Chen, 2021).",
    "reason": "Incorrect abbreviation formatting: use 'et al.' (no period after 'et'); the correct form is '(Park et al., 2018)'.",
    "start": 229,
    "end": 249,
    "label": "Format"
  },
  {
    "span": "We find that in all four datasets, our proposed baseline significantly outperforms existing state of the art, yielding up to 5 point AUC gain.",
    "document": "Introduction\n\nGiven some text (typically, a sentence) t mentioning an entity pair (e 1 , e 2 ), the goal of relation extraction (RE) is to predict the relationships between e 1 and e 2 that can be inferred from t. Let B(e 1 , e 2 ) denote the set of all sentences (bag) in the corpus mentioning e 1 and e 2 and let R(e 1 , e 2 ) denote all relations from e 1 to e 2 in a KB. Distant supervision (DS) trains RE models given B(e 1 , e 2 ) and R(e 1 , e 2 ), without sentence level annotation (Mintz et al., 2009). Most DS-RE models use the \"at-least one\" assumption: ∀r ∈ R(e 1 , e 2 ), ∃t r ∈ B(e 1 , e 2 ) such that t r expresses (e 1 , r, e 2 ).\n\nRecent neural approaches to DS-RE encode each sentence t ∈ B(e 1 , e 2 ) and then aggregate sentence embeddings using an aggregation operator -the common operator being intra-bag attention (Lin et al., 2016). Various models differ in their approach to encoding (e.g., PCNNs, GCNs, BERT) and their loss functions (e.g., contrastive learning, MLM), but agree on the design choice of encoding each sentence independently of the others (Vashishth et al., 2018;Alt et al., 2019;Christou and Tsoumakas, 2021;Chen et al., 2021). We posit that this choice leads to a suboptimal usage of the available data -information from other sentences might help in better encoding a given sentence.\n\nWe explore this hypothesis by developing a simple baseline solution. We first construct a passage P (e 1 , e 2 ) by concatenating all sentences in B(e 1 , e 2 ). We then encode the whole passage through BERT (Devlin et al., 2019) (or mBERT for multilingual setting). This produces a contextualized embedding of every token in the bag. To make these embeddings aware of the candidate relation, we take a (trained) relation query vector, r, to generate a relation-aware summary of the whole passage using attention. This is then used to predict whether (e 1 , r, e 2 ) is a valid prediction.\n\nDespite its simplicity, our baseline has some conceptual advantages. First, each token is able to exchange information with other tokens from other sentences in the bag -so the embeddings are likely more informed. Second, in principle, the model may be able to relax a part of the at-least-one assumption. For example, if no sentence individually expresses a relation, but if multiple facts in different sentences collectively predict the relation, our model may be able to learn to extract that.\n\nWe name our baseline model Passage-Attended Relation Extraction, PARE (mPARE for multilingual DS-RE). We experiment on four DS-RE datasets -three in English, NYT-10d (Riedel et al., 2010), NYT-10m, and Wiki-20m (Gao et al., 2021), and one multilingual, DiS-ReX (Bhartiya et al., 2021). We find that in all four datasets, our proposed baseline significantly outperforms existing state of the art, yielding up to 5 point AUC gain. Further attention analysis and ablations provide additional insight into model performance. We release our code for reproducibility. We believe that our work represents a simple but strong baseline that can form the basis for further DS-RE research.\n\n ",
    "start": 2704,
    "end": 2846,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Garcia et al., 2014.)",
    "document": "Introduction\n\nGraphical causal models provide a framework for reasoning about interventions and counterfactuals. Recent work addresses identifiability in the presence of latent variables and selection bias, proposing algorithms for partial ancestral graphs and maximal ancestral graphs. Empirical benchmarks suggest that structure learning benefits from sparsity-inducing priors and informative constraints (Garcia et al., 2014.). However, scalability to high-dimensional settings remains a challenge, motivating hybrid methods that combine score-based search with conditional independence testing (Heinze-Deml et al., 2018; Ng et al., 2019).",
    "reason": "Extraneous period inside the parenthetical citation; it should be “(Garcia et al., 2014)” with punctuation placed outside the closing parenthesis as appropriate.",
    "start": 407,
    "end": 429,
    "label": "Format"
  },
  {
    "span": "see (Nguyen et al., 2019;)",
    "document": "Related Work\n\nMultivariate time-series forecasting under regime shifts requires models that adapt to changing dynamics (Box and Jenkins, 2015; Hyndman and Athanasopoulos, 2018). Deep sequence models capture complex temporal dependencies but can overfit to nonstationary noise (Salinas et al., 2020; Rangapuram et al., 2018).\n\nStructure-aware models incorporate exogenous variables and graph couplings (Wu et al., 2019; Li et al., 2018). Robustness is improved via heavy-tailed likelihoods and adversarial training (Wen et al., 2017; Mohsan et al., 2021). For comprehensive surveys, see (Nguyen et al., 2019;).\n\nOur contribution is a change-point–aware transformer with regime-conditioned attention and a training curriculum aligned with detected shifts.",
    "reason": "Dangling semicolon inside parenthetical citation; citation list should not end with a semicolon before the closing parenthesis.",
    "start": 582,
    "end": 608,
    "label": "Format"
  },
  {
    "span": "Unsupervised domain adaptation for detection has explored discrepancy minimization, adversarial feature alignment, and self-training (Saito et al., 2019; Zhu et al., 2019; Kim et al., 2020).",
    "document": "Introduction\nObject detectors trained on a labeled source domain often degrade substantially when deployed in a shifted target domain. Collecting annotations in the target is prohibitively expensive, motivating unsupervised domain adaptation (UDA) techniques that exploit unlabeled target data.\n\nRelated Work\nUnsupervised domain adaptation for detection has explored discrepancy minimization, adversarial feature alignment, and self-training (Saito et al., 2019; Zhu et al., 2019; Kim et al., 2020). Multi-level alignment strategies target image, instance, and feature pyramid levels (Chen et al., 2018; Sohn et al., 2020), while pseudo-labeling leverages high-confidence predictions to supervise the target (Zhao et al., 2020). We consider cross-weather and synthetic-to-real shifts in our evaluation.",
    "reason": "The sentence lists technique categories and citations but does not articulate how these methods relate to the authors' problem setting, their limitations, or the specific gap the paper addresses.",
    "start": 309,
    "end": 499,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Previous studies have shown that prompt-specific features inflate AES performance.",
    "document": "Introduction\n\nAutomatic essay scoring (AES) systems aim to produce reliable and fair estimates of writing quality. Despite advances in neural text representations, concerns persist regarding robustness and construct validity. Previous studies have shown that prompt-specific features inflate AES performance. To address this, we propose a prompt-agnostic calibration scheme that separates surface cues from higher-order discourse signals. We assess generalization across prompts and genres, highlighting settings where conventional metrics can be misleading.",
    "reason": "References findings from prior studies without citing any specific work (definition a and e).",
    "start": 226,
    "end": 308,
    "label": "Unsupported_claim"
  },
  {
    "span": "Matrix factorization captures latent preferences (Koren et al., 2009). Exposure bias can be mitigated with counterfactual estimators (Schnabel et al., 2016). Graph neural networks learn user-item structures (Wang et al., 2019). Demographic parity is one fairness notion (Dwork et al., 2012).",
    "document": "Introduction\n\nPersonalized recommendation has evolved from memory-based methods to model-based approaches that learn compact user and item representations for large-scale platforms. Beyond accuracy, recent studies emphasize robustness to selection bias and fairness across demographic groups (Yao and Huang, 2017; Ekstrand et al., 2022). Mitigating feedback loops and ensuring equitable exposure remain open challenges.\n\nMatrix factorization captures latent preferences (Koren et al., 2009). Exposure bias can be mitigated with counterfactual estimators (Schnabel et al., 2016). Graph neural networks learn user-item structures (Wang et al., 2019). Demographic parity is one fairness notion (Dwork et al., 2012).\n\nIn contrast, our framework jointly optimizes calibrated propensity weighting and group-aware regularization within a graph-based recommender to balance utility and fairness.",
    "reason": "The span abruptly moves from matrix factorization to exposure bias to GNNs and then to a fairness definition, without transitions or an explanation of how each prior sentence relates to the next.",
    "start": 421,
    "end": 712,
    "label": "Coherence"
  },
  {
    "span": "Buolamwini and Gebru (2018) analyzed demographic disparities in computer vision systems. Dwork et al. (2012) formalized fairness through awareness. Abadi et al. (2016) developed differentially private SGD. Hardt et al. (2016) defined equalized odds.",
    "document": "Introduction\n\nEnsuring fairness in machine learning requires aligning algorithmic behavior with societal values and legal constraints. Prior research spans measurement, mitigation, and governance, but the landscape remains fragmented across tasks and modalities.\n\nBuolamwini and Gebru (2018) analyzed demographic disparities in computer vision systems. Dwork et al. (2012) formalized fairness through awareness. Abadi et al. (2016) developed differentially private SGD. Hardt et al. (2016) defined equalized odds.\n\nWe build on the measurement perspective by proposing a task-agnostic auditing framework that separates shifts in data, labels, and model behavior.",
    "reason": "The cited works are listed back-to-back without transitions or explanation, leaving unclear how privacy (Abadi et al., 2016) connects to fairness metrics or to prior sentences.",
    "start": 264,
    "end": 513,
    "label": "Coherence"
  },
  {
    "span": "In a previous study, the authors demonstrated that BERT-base surpasses human agreement on sarcasm detection.",
    "document": "Introduction\n\nSarcasm detection in social media remains challenging due to subtle pragmatic cues and frequent domain shifts across platforms. Although pre-trained transformers improve performance, they often overfit to lexical artifacts and fail to generalize to new communities or topics.\n\nIn a previous study, the authors demonstrated that BERT-base surpasses human agreement on sarcasm detection. However, replicability concerns persist because annotation guidelines differ and inter-annotator agreement varies widely across datasets. To address these issues, we introduce a cross-platform benchmark with standardized annotation protocols and report results under consistent domain adaptation settings.\n\nOur contributions include a unified evaluation suite, an analysis of domain leakage, and a calibration study that quantifies uncertainty in model predictions.",
    "reason": "Refers to a prior study and a specific claim but provides no citation or evidence (violates b and e).",
    "start": 291,
    "end": 399,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Miller, 2020)",
    "document": "Introduction\n\nRobotic manipulation with reinforcement learning has advanced with model-free and model-based methods (Levine et al., 2016; Kalashnikov et al., 2018; Chua et al., 2018). Sample efficiency is improved by offline pretraining (Kumar et al., 2020) and demonstrations (Rajeswaran et al., 2017). As shown by (Miller, 2020), domain randomization helps transfer to the real world, and subsequent work studies sim-to-real gaps (Tobin et al., 2017). We explore policy constraints for safety (Ray et al., 2019).",
    "reason": "Parenthetical citation used after 'by'; narrative form is required: 'by Miller (2020)'.",
    "start": 316,
    "end": 330,
    "label": "Format"
  },
  {
    "span": "Unsupervised time series anomaly detection uses reconstruction error from autoencoders, probabilistic forecasting residuals, and density estimation in latent spaces (Malhotra et al., 2016; Xu et al., 2018; Tatbul et al., 2018). Shapelets and motif discovery provide interpretable pattern-based methods (Ye and Keogh, 2009; Mueen et al., 2009).",
    "document": "Introduction\n\nDetecting anomalies in time series under weak or no supervision is critical for monitoring industrial systems, networks, and sensors. Data exhibit non-stationarity, context dependence, and rare events, challenging fixed thresholds and static models. A key need is to localize anomalies while maintaining low false positives.\n\nUnsupervised time series anomaly detection uses reconstruction error from autoencoders, probabilistic forecasting residuals, and density estimation in latent spaces (Malhotra et al., 2016; Xu et al., 2018; Tatbul et al., 2018). Shapelets and motif discovery provide interpretable pattern-based methods (Ye and Keogh, 2009; Mueen et al., 2009).\n\nWe introduce a conformalized predictive coding approach that calibrates per-context nonconformity scores online, delivering reliable coverage under distribution shift and providing segment-level localization.",
    "reason": "The span summarizes categories of prior methods without explaining their limitations for the targeted setting or how they motivate the proposed approach, thus lacking synthesis.",
    "start": 340,
    "end": 683,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Graph convolutional recommenders treat users and items as nodes (Wang et al., 2019). Contrastive losses help debias representations (He et al., 2020). Session-based recommendation has been modeled with gated graph networks (Wu et al., 2019).",
    "document": "Related Work\n\nGraph-based recommendation leverages user–item interactions as edges to propagate preference signals through neighborhoods. Recent advances integrate message passing with sampling, regularization, and self-supervision.\n\nGraph convolutional recommenders treat users and items as nodes (Wang et al., 2019). Contrastive losses help debias representations (He et al., 2020). Session-based recommendation has been modeled with gated graph networks (Wu et al., 2019).\n\nDespite these advances, scalability and cold-start issues persist, especially in sparse graphs. Our method introduces structure-aware augmentation and calibration to stabilize training under extreme sparsity.",
    "reason": "The cited works are presented as isolated statements without transitions or an explicit explanation of their relationships, resulting in an abrupt and incoherent flow.",
    "start": 234,
    "end": 475,
    "label": "Coherence"
  },
  {
    "span": "(2019, Smith et al.)",
    "document": "Introduction\n\nTask-oriented dialogue systems increasingly incorporate reinforcement learning (RL) to optimize policies over long-horizon interactions (Young et al., 2013; Su et al., 2016). While supervised learning bootstraps initial policies, RL refines behavior with user feedback and task success signals (Williams and Zweig, 2016; Li et al., 2016). Off-policy evaluation remains challenging due to limited logged data and non-stationary user behavior (Thomas and Brunskill, 2016; Gottesman et al., 2019). Recent approaches leverage model-based rollouts and uncertainty estimates to stabilize learning (Jaques et al., 2019; Budzianowski et al., 2020). We compare against hierarchical policy methods (2019, Smith et al.) and investigate representation sharing with language modeling objectives to improve sample efficiency (Henderson et al., 2019; Chen et al., 2021).\n",
    "reason": "Incorrect order/style within parenthetical citation; author should precede year: \"(Smith et al., 2019)\".",
    "start": 702,
    "end": 722,
    "label": "Format"
  },
  {
    "span": "(Green and White, 2016)",
    "document": "Related Work\n\nExtractive summarization ranks sentences using coverage and redundancy signals (Nenkova and McKeown, 2011; Mihalcea and Tarau, 2004). Graph-based extractive summarizers (Green and White, 2016) rank sentences via centrality scores computed on similarity graphs. Neural abstractive models leverage attention and copy mechanisms to improve factuality and fluency (See et al., 2017; Paulus et al., 2018). We combine centrality priors with encoder-decoder training to improve content selection.",
    "reason": "Incorrect conjunction in parenthetical citation; APA-style parenthetical citations with two authors should use an ampersand: \"(Green & White, 2016)\" instead of \"and\".",
    "start": 183,
    "end": 206,
    "label": "Format"
  },
  {
    "span": "(Zhou et. al., 2017)",
    "document": "Related Work\n\nUnsupervised domain adaptation for sentiment classification typically relies on aligning feature distributions across domains (Ganin and Lempitsky, 2015; Bousmalis et al., 2017). Pivot-based methods learn domain-invariant representations via auxiliary prediction tasks (Blitzer et al., 2007), while adversarial approaches induce invariance through domain discriminators (Tzeng et al., 2017). More recent techniques leverage self-training and pseudo-label refinement to improve target-domain performance (Zhang et al., 2021). In the context of aspect-based sentiment analysis, alignment at the span and opinion levels has proven effective (Rietzler et al., 2020; Li et al., 2021). A hierarchical adversarial mechanism was proposed in (Zhou et. al., 2017) to capture both sentence and document-level signals, though it did not account for aspect granularity.\n\nOur method integrates aspect-aware contrastive objectives with curriculum pseudo-labeling for robust transfer.",
    "reason": "Incorrect abbreviation: 'et. al.' should be 'et al.' without the period after 'et'.",
    "start": 747,
    "end": 767,
    "label": "Format"
  },
  {
    "span": "((Wang et al., 2014)",
    "document": "Related Work\n\nTime-series forecasting methods range from classical ARIMA models (Box and Jenkins, 1970) to modern deep architectures that capture long-range dependencies (Lai et al., 2018). Exogenous covariates and hierarchical structures complicate learning but permit richer inductive biases (Salinas et al., 2020). Probabilistic forecasting emphasizes calibrated uncertainty via likelihood-based training and quantile objectives. Recent advances incorporate temporal convolution and attention to improve scalability (Oreshkin et al., 2019).\n\nHybrid methods combine local statistical models with global neural components to capture cross-series signals. Regularization and differencing help mitigate non-stationarity, while decomposition-based methods separately model trend and seasonality ((Wang et al., 2014) to stabilize optimization on noisy series.\n\nWe propose a sequence-to-sequence forecaster with decomposition-aware attention and evaluate on retail and energy datasets with multiple horizons.",
    "reason": "Extra opening parenthesis in the citation and mismatched parentheses; it should be '(Wang et al., 2014)'.",
    "start": 793,
    "end": 813,
    "label": "Format"
  },
  {
    "span": "we annotate outputs from several state-of-the-art dialogue models, including variants explicitly designed to curb hallucination",
    "document": "Introduction\n\nKnowledge-grounded dialogue systems seek to produce responses that are both helpful and faithful to external evidence. While pre-trained language models can incorporate retrieved passages, their outputs may still contain unsupported claims or fabrications that degrade user trust.\n\nTo better understand this issue, we curate a multi-domain evaluation set that emphasizes factual grounding and source attribution. In addition, we annotate outputs from several state-of-the-art dialogue models, including variants explicitly designed to curb hallucination, and compare their behavior across evidence conditions.\n\nOur analysis reveals persistent faithfulness gaps even when documents are provided verbatim, motivating methods that tie generation more tightly to cited sources.",
    "reason": "Mentions evaluating “several state-of-the-art” and specific model variants as prior work designs without citing any of those models.",
    "start": 440,
    "end": 567,
    "label": "Unsupported_claim"
  },
  {
    "span": "Wei et al. (2022) explored chain-of-thought prompting for complex reasoning. Min et al. (2022) examined the necessity of demonstrations in prompt-based learning. Chung et al. (2022) scaled instruction tuning with a mixture-of-experts backbone. Mishra et al. (2022) curated diverse instruction datasets to improve generalization.",
    "document": "Related Work\n\nPrompt-based learning and instruction tuning have rapidly advanced the capabilities of large language models. Early work framed tasks as text infilling or pattern completion, showing that pretrained models can adapt with minimal fine-tuning (Brown et al., 2020; Schick and Schütze, 2021). Subsequent research focused on how the phrasing, structure, and exemplars within instructions modulate performance across tasks and domains (Liu et al., 2021; Sanh et al., 2022). Despite these gains, it remains unclear which aspects of instructions drive generalization under distribution shift.\n\nWei et al. (2022) explored chain-of-thought prompting for complex reasoning. Min et al. (2022) examined the necessity of demonstrations in prompt-based learning. Chung et al. (2022) scaled instruction tuning with a mixture-of-experts backbone. Mishra et al. (2022) curated diverse instruction datasets to improve generalization.\n\nConcurrently, alignment techniques such as reinforcement learning from human feedback (RLHF) have been used to shape model preferences and safety (Ouyang et al., 2022; Bai et al., 2022). This literature emphasizes the role of reward models and preference data quality, which interacts with instruction-following behaviors but is orthogonal to the core question of instruction diversity. Our work isolates the contribution of instruction coverage while controlling for model size and optimization, providing a clearer picture of how breadth and granularity of instructions affect generalization.",
    "reason": "The span lists four cited works in consecutive sentences without explaining how each relates to one another or to the broader narrative. There are no transitions or explicit comparisons, leaving the connection between chain-of-thought, demonstrations, scaling, and dataset curation implicit.",
    "start": 600,
    "end": 928,
    "label": "Coherence"
  },
  {
    "span": "Vaswani et al. 2",
    "document": "Introduction\n\nSequence modeling has shifted toward attention-based architectures that capture long-range dependencies without recurrence (Bahdanau et al., 2015; Luong et al., 2015). The Transformer popularized scaled dot-product attention and multi-head design, enabling parallel training and strong transfer (Vaswani et al., 2017; Dai et al., 2019).\n\nSubsequent improvements target positional encoding, efficiency, and pretraining objectives (Shaw et al., 2018; Kitaev et al., 2020; Clark et al., 2020). Despite success, deploying large models remains challenging due to latency and memory constraints (Kim and Cho, 2021; Tay et al., 2020). We adopt a lightweight encoder with linear attention and demonstrate favorable speed–accuracy trade-offs compared to baseline Transformers. Vaswani et al. 2 report strong results on translation benchmarks, which we aim to match under stricter computational budgets.\n\nOur contributions include a kernelized attention variant with theoretical bounds and a comprehensive study on throughput–latency–accuracy.",
    "reason": "Improper footnote-style numeric marker appended to an author-only narrative; should include the year (e.g., “Vaswani et al. (2017)”) or use a properly formatted numeric citation.",
    "start": 782,
    "end": 798,
    "label": "Format"
  },
  {
    "span": "In (Johnson et al., 2019)",
    "document": "Related Work\n\nDomain adaptation seeks to transfer knowledge from a labeled source domain to an unlabeled target domain under distribution shift (Pan and Yang, 2010; Wilson and Cook, 2020). In (Johnson et al., 2019) a shared–private architecture is trained to disentangle domain-invariant and domain-specific features. Other methods remove covariate shift using adversarial learning (Ganin et al., 2016) and contrastive objectives (Khosla et al., 2020). Our approach builds on these foundations by introducing target-aware regularization.",
    "reason": "Wrong citation style: the preposition should introduce a narrative citation. Use “In Johnson et al. (2019)” or rephrase to parenthetical “(Johnson et al., 2019)”.",
    "start": 189,
    "end": 214,
    "label": "Format"
  },
  {
    "span": "A previous study reports that developer-written summaries are often inconsistent with function bodies.",
    "document": "Related Work\n\nCode summarization aims to generate concise natural language descriptions of program subroutines, assisting comprehension, review, and maintenance. Neural encoder–decoder models trained on pairs of functions and docstrings have become standard, with domain-adaptive pretraining and copy mechanisms improving fluency and specificity. However, noise in supervision, such as outdated or mismatched comments, remains a major obstacle.\n\nData quality has motivated research on filtering heuristics, weak supervision, and retrieval-augmented training. A previous study reports that developer-written summaries are often inconsistent with function bodies. Subsequent efforts propose aligning identifiers and API usages with generated summaries and leveraging tests to ground descriptions in observable behavior.\n\nOur work targets inconsistency by introducing an entailment-based data curation pipeline that retains only examples where the docstring is logically supported by code semantics approximated via static analysis. We also propose an evaluation protocol that penalizes hallucinated API mentions and measures factual consistency via code-aware natural language inference.",
    "reason": "References findings from 'a previous study' without citing it, which is required when mentioning prior work (a).",
    "start": 559,
    "end": 661,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prompt tuning learns continuous prompts for a frozen model (Lester et al., 2021). Retrieval-augmented generation attaches external documents at inference time (Lewis et al., 2020). Instruction induction seeks to discover generalizable task specifications (Wei et al., 2022).",
    "document": "Related Work\n\nPrompting and instruction-following have emerged as central techniques for adapting large language models (LLMs) to new tasks without full fine-tuning. Early work demonstrated that in-context examples can elicit strong few-shot performance from very large autoregressive models (Brown et al., 2020). Subsequent research introduced task-agnostic instructions to improve robustness and generalization across benchmarks (Sanh et al., 2022; Ouyang et al., 2022), and explored mixtures of demonstrations and rationales to steer reasoning (Zhou et al., 2022; Kojima et al., 2022).\n\nPrompt tuning learns continuous prompts for a frozen model (Lester et al., 2021). Retrieval-augmented generation attaches external documents at inference time (Lewis et al., 2020). Instruction induction seeks to discover generalizable task specifications (Wei et al., 2022). Other lines of work reduce prompt sensitivity via calibration or self-consistency sampling (Zhao et al., 2021; Wang et al., 2022). There is also growing attention to safety and factuality constraints in prompt design (Lin et al., 2022; Bai et al., 2022). Our study examines robustness under distribution shifts caused by paraphrased instructions and perturbed demonstrations.",
    "reason": "The span lists three cited approaches in sequence without transitions or an explicit explanation of how they relate to each other or to the surrounding discussion, making the connection between the works abrupt and unclear.",
    "start": 590,
    "end": 864,
    "label": "Coherence"
  },
  {
    "span": "Classical SLAM pipelines rely on geometric optimization with feature-based front-ends such as ORB and SIFT and back-ends using bundle adjustment or factor graphs (Mur-Artal et al., 2015; Triggs et al., 1999; Dellaert, 2017). Learning-based approaches replace parts of the pipeline with deep features, learned depth, or differentiable optimization (Zhou et al., 2017; DeTone et al., 2018; Tang and Tan, 2018).",
    "document": "Related Work\n\nSimultaneous localization and mapping (SLAM) estimates camera trajectories and reconstructs scene structure. The community has explored both geometric and learning-based formulations, each with trade-offs in accuracy, robustness, and compute.\n\nClassical SLAM pipelines rely on geometric optimization with feature-based front-ends such as ORB and SIFT and back-ends using bundle adjustment or factor graphs (Mur-Artal et al., 2015; Triggs et al., 1999; Dellaert, 2017). Learning-based approaches replace parts of the pipeline with deep features, learned depth, or differentiable optimization (Zhou et al., 2017; DeTone et al., 2018; Tang and Tan, 2018).\n\nWe introduce a hybrid system that conditions geometric optimization on uncertainty-aware learned features, improving robustness in low-texture and dynamic scenes. We benchmark against strong geometric and learned baselines on indoor and outdoor datasets.\n",
    "reason": "The span enumerates classical and learned SLAM work without explaining how these approaches inform the proposed hybrid or what gap it fills.",
    "start": 258,
    "end": 666,
    "label": "Lacks_synthesis"
  },
  {
    "span": "in (Liu et al., 2021)",
    "document": "Related Work\n\nPrompt-based tuning. Prompting consolidates task instructions within the input, enabling efficient reuse of pretrained parameters across tasks. We adopt the taxonomy introduced in (Liu et al., 2021) to situate approaches by whether prompts are discrete or continuous and whether model parameters are updated. Recent studies show that lightweight prompt tuning can rival full finetuning on several NLP benchmarks, while hybrid strategies combine soft prompts with adapters for stability.\n\nCross-lingual prompting. Few works examine prompt transfer across languages, where prompt phrasing and tokenization interact with morphology. Early investigations compare discrete templates to learned vectors and report that translation artifacts can degrade zero-shot performance. Our study extends this by evaluating template robustness and label-word choices over typologically diverse languages.\n\nRetriever–prompt pipelines. Retrieval-augmented prompting reduces hallucinations by grounding generation, yet prompt design for evidential control remains under-explored. We connect this to recent retrieval objectives and analyze prompt sensitivity to retrieved context length.",
    "reason": "Wrong citation style: using a parenthetical citation after the preposition 'in'. It should be narrative, e.g., 'in Liu et al. (2021)'.",
    "start": 191,
    "end": 212,
    "label": "Format"
  },
  {
    "span": "Previous studies show that self-supervised pretraining halves the word error rate on conversational benchmarks.",
    "document": "Related Work\n\nSpeech recognition has seen rapid progress through end-to-end architectures trained on large corpora, yet performance in low-resource and conversational scenarios remains challenging due to data scarcity and variability (Chan et al., 2016). Self-supervised representation learning promises to address these issues by leveraging unlabeled audio to learn robust acoustic features.\n\nPrevious studies show that self-supervised pretraining halves the word error rate on conversational benchmarks. Despite these reported gains, questions remain regarding the transferability of learned representations across domains, the sensitivity to fine-tuning batch size, and the stability of optimization for long-form speech.\n\nIn this work, we systematically evaluate self-supervised encoders under matched and mismatched conditions, including scripted read speech and spontaneous dialog. We compare masking strategies, pretraining schedules, and data mixing policies, and propose a calibration-aware decoding method to mitigate overconfidence in low-resource adaptation.",
    "reason": "It asserts a quantitative effect ('halves the word error rate') and attributes it to 'previous studies' without citing any specific papers.",
    "start": 394,
    "end": 505,
    "label": "Unsupported_claim"
  },
  {
    "span": "Unsupervised domain adaptation aligns distributions with adversarial objectives (Ganin and Lempitsky, 2015). Instance reweighting corrects covariate shift using importance weights (Huang et al., 2007). Pretrained language models can be adapted with continued pretraining (Gururangan et al., 2020). Back-translation augments target-domain data (Sennrich et al., 2016).",
    "document": "Introduction\n\nNLP models frequently face performance degradation when deployed in domains that differ from their training data. Domain adaptation methods seek to bridge this gap by reducing distributional mismatches or by exploiting unlabeled target-domain examples. Despite progress, methods often struggle when domain shift interacts with label imbalance and limited supervision.\n\nUnsupervised domain adaptation aligns distributions with adversarial objectives (Ganin and Lempitsky, 2015). Instance reweighting corrects covariate shift using importance weights (Huang et al., 2007). Pretrained language models can be adapted with continued pretraining (Gururangan et al., 2020). Back-translation augments target-domain data (Sennrich et al., 2016).\n\nOur work integrates robust instance weighting with task-adaptive pretraining, aiming to handle severe covariate and label shift while maintaining sample efficiency.",
    "reason": "This span lists separate techniques with citations but does not specify their connections, trade-offs, or how they relate to each other, resulting in abrupt, unconnected statements.",
    "start": 383,
    "end": 750,
    "label": "Coherence"
  },
  {
    "span": "Cross-lingual distillation has been explored from teacher models or parallel data (Xu et al., 2020; Chi et al., 2021; He et al., 2021).",
    "document": "Introduction\nMultilingual understanding tasks benefit from parameter sharing across languages, yet performance in low-resource settings still lags behind high-resource counterparts. Knowledge distillation and transfer play central roles in bridging this gap.\n\nRelated Work\nCross-lingual distillation has been explored from teacher models or parallel data (Xu et al., 2020; Chi et al., 2021; He et al., 2021). Alignment via contrastive objectives encourages shared semantic spaces (Feng et al., 2020; Pan et al., 2021). Translation-based approaches generate synthetic supervision for target languages (Conneau et al., 2020; Hu et al., 2020). We test on XNLI, PAWS-X, and multilingual NER.",
    "reason": "The span inventories prior approaches without linking them to the present work, specifying their shortcomings, or explaining the gap that the paper intends to address.",
    "start": 273,
    "end": 408,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Many prior works have successfully deployed model-based reinforcement learning on real quadruped robots for dynamic locomotion.",
    "document": "Related Work\n\nReinforcement learning (RL) methods for robotics span model-free approaches that learn policies directly from interaction and model-based approaches that leverage learned dynamics for sample efficiency (Sutton and Barto, 2018; Deisenroth and Rasmussen, 2011). Sim-to-real transfer strategies mitigate the reality gap via domain randomization and residual learning (Tobin et al., 2017; Johansson et al., 2018). Many prior works have successfully deployed model-based reinforcement learning on real quadruped robots for dynamic locomotion. Despite encouraging results, safety constraints and on-board compute limitations continue to constrain policy complexity. Our method introduces a task-aware dynamics prior that reduces online rollouts while preserving performance under disturbances.",
    "reason": "Asserts the existence and success of multiple prior deployments without citing any of the works, violating the citation requirement.",
    "start": 424,
    "end": 551,
    "label": "Unsupported_claim"
  },
  {
    "span": "Data augmentation for NLP includes synonym replacement and token-level perturbations (Wei and Zou, 2019; Kobayashi, 2018), back-translation and paraphrasing (Sennrich et al., 2016; Edunov et al., 2018; Xie et al., 2020), mixup and interpolation in embedding space (Guo et al., 2019; Chen et al., 2020b), and adversarial text generation (Jin et al., 2020; Alzantot et al., 2018). Task-specific augmentations have been proposed for classification, QA, and NER (Yu et al., 2018; Shnarch et al., 2018; Dai and Adel, 2020).",
    "document": "Introduction\n\nGeneralization in low-resource NLP benefits from augmentations that enrich lexical, syntactic, and semantic variety while preserving label semantics.\n\nData augmentation for NLP includes synonym replacement and token-level perturbations (Wei and Zou, 2019; Kobayashi, 2018), back-translation and paraphrasing (Sennrich et al., 2016; Edunov et al., 2018; Xie et al., 2020), mixup and interpolation in embedding space (Guo et al., 2019; Chen et al., 2020b), and adversarial text generation (Jin et al., 2020; Alzantot et al., 2018). Task-specific augmentations have been proposed for classification, QA, and NER (Yu et al., 2018; Shnarch et al., 2018; Dai and Adel, 2020).\n\nWe propose a semantics-preserving paraphrase controller that calibrates augmentation strength using a learned label-consistency discriminator across tasks.",
    "reason": "The span catalogs methods but does not synthesize their limitations or explain how they motivate the proposed controller, satisfying (a) and (c).",
    "start": 165,
    "end": 683,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Smith, 2018, Jones, 2019)",
    "document": "Related Work\n\nNeural constituency parsing has benefited from contextual encoders and chart-based decoding. Comparative studies (Smith, 2018, Jones, 2019) report that span representations learned with self-attention outperform LSTM baselines, and Stern et al. (2017) show that dynamic oracles improve training efficiency. Further, Kitaev and Klein (2018) leverage self-attentive encoders to achieve state-of-the-art results. Nonetheless, performance degrades on out-of-domain corpora.",
    "reason": "Multiple citations incorrectly separated by a comma; different works should be separated by a semicolon '(Smith, 2018; Jones, 2019)'.",
    "start": 127,
    "end": 153,
    "label": "Format"
  },
  {
    "span": "Contrastive learning for recommendation learns user–item representations by pulling positives together and pushing sampled negatives apart (He et al., 2020; Wu et al., 2021; Xie et al., 2022). Data augmentation strategies include node/edge dropout, feature masking, and subgraph sampling (Qiu et al., 2020; You et al., 2020; Yu et al., 2022).",
    "document": "Related Work\n\nCollaborative filtering and graph encoders. Traditional collaborative filtering leverages matrix factorization and neighborhood methods, while recent advances use graph neural networks to propagate preferences along the user–item interaction graph, improving cold-start and sparsity robustness.\n\nContrastive learning. Contrastive learning for recommendation learns user–item representations by pulling positives together and pushing sampled negatives apart (He et al., 2020; Wu et al., 2021; Xie et al., 2022). Data augmentation strategies include node/edge dropout, feature masking, and subgraph sampling (Qiu et al., 2020; You et al., 2020; Yu et al., 2022).\n\nDebiasing and exposure modeling. A complementary line tackles selection bias and exposure confounding via inverse propensity weighting, causal regularizers, and intervention-based evaluation to align offline training with online behavior.\n\nOur approach unifies augmentation selection with exposure-aware sampling, learning to choose augmentations that reduce bias while preserving informative structure for recommendation.",
    "reason": "The span summarizes prior contrastive methods and augmentations but does not explain how they compare to or motivate the presented method, leaving the connection implicit.",
    "start": 332,
    "end": 674,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Post-hoc saliency maps highlight input regions associated with predictions (Simonyan et al., 2013). Counterfactual explanations propose minimally different examples (Wachter et al., 2017). Clinical ontologies constrain model outputs (Bodenreider, 2004). Calibration ensures predicted probabilities reflect outcome frequencies (Guo et al., 2017).",
    "document": "Introduction\n\nExplainability in healthcare aims to make model decisions transparent to clinicians, facilitating trust, auditing, and regulatory compliance. Methods range from model-intrinsic interpretability to post-hoc techniques, yet aligning explanations with clinical utility remains a key challenge.\n\nPost-hoc saliency maps highlight input regions associated with predictions (Simonyan et al., 2013). Counterfactual explanations propose minimally different examples (Wachter et al., 2017). Clinical ontologies constrain model outputs (Bodenreider, 2004). Calibration ensures predicted probabilities reflect outcome frequencies (Guo et al., 2017).\n\nWe introduce a clinician-in-the-loop explanation framework that couples counterfactual reasoning with ontology-grounded constraints while maintaining calibrated uncertainty estimates.",
    "reason": "The span strings together distinct topics without clarifying how they connect to each other, causing abrupt shifts and leaving relationships implicit.",
    "start": 306,
    "end": 651,
    "label": "Coherence"
  },
  {
    "span": "Constraint-based algorithms such as PC and FCI leverage conditional independence tests to infer causal graphs under various assumptions. Score-based methods like GES optimize a decomposable score to search over graph structures. Continuous optimization approaches, including NOTEARS, DAG-GNN, and related variants, relax acyclicity constraints to enable gradient-based learning. Interventional extensions incorporate do-calculus and soft interventions to refine graph estimation. We propose a smooth regularizer for scalable nonlinear causal discovery.",
    "document": "Related Work\n\nCausal discovery seeks to uncover directed acyclic structures from observational and, when available, interventional data. The space of DAGs is combinatorial, and different assumptions (faithfulness, causal sufficiency, functional classes) motivate distinct algorithmic families.\n\nConstraint-based algorithms such as PC and FCI leverage conditional independence tests to infer causal graphs under various assumptions. Score-based methods like GES optimize a decomposable score to search over graph structures. Continuous optimization approaches, including NOTEARS, DAG-GNN, and related variants, relax acyclicity constraints to enable gradient-based learning. Interventional extensions incorporate do-calculus and soft interventions to refine graph estimation. We propose a smooth regularizer for scalable nonlinear causal discovery.\n\nWe assess sample complexity and stability on synthetic graphs and report performance on gene expression and climatology datasets.",
    "reason": "The text enumerates major lines of work and then introduces the authors' contribution without clarifying the unresolved problem or integrating prior insights into a motivating argument.",
    "start": 295,
    "end": 847,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Johnson et al., (2014)",
    "document": "Related Work\n\nTime series anomaly detection encompasses reconstruction-based, density-based, and prediction-based approaches (Chandola et al., 2009; Blázquez-García et al., 2021). Deep forecasting models have become central due to their capacity to capture non-linear dynamics and seasonality (Salinas et al., 2020; Lim et al., 2021).\n\nHybrid detectors combine probabilistic forecasts with learned residual scoring to balance precision and recall. Johnson et al., (2014) formalize evaluation pitfalls in unsupervised settings and recommend robust metrics under delayed labels.\n\nBuilding on these insights, we introduce a calibration-aware thresholding scheme that adapts to non-stationary uncertainty.",
    "reason": "Extraneous comma before the parenthetical year in a narrative citation; it should be 'Johnson et al. (2014)'.",
    "start": 448,
    "end": 470,
    "label": "Format"
  },
  {
    "span": "In previous shared tasks on fact verification, evidence retrieval errors account for the majority of mistakes.",
    "document": "Related Work\n\nFact verification systems typically combine document retrieval, evidence selection, and claim classification. Despite strong text encoders, end-to-end accuracy often hinges on retrieving sufficient and relevant evidence. Analyses that attribute errors to pipeline components inform where modeling efforts should focus.\n\nIn previous shared tasks on fact verification, evidence retrieval errors account for the majority of mistakes. Recent papers propose joint retrieval–verification architectures to reduce error propagation, while others adopt iterative retrieval to refine evidence sets. We extend this line by coupling a calibrator with a retrieval fallback that detects insufficient evidence at inference time.",
    "reason": "Reports an outcome of 'previous shared tasks' without citing task reports or analyses (rule a).",
    "start": 334,
    "end": 444,
    "label": "Unsupported_claim"
  },
  {
    "span": "According to industry surveys, over 60% of teams now deploy red-teaming pipelines.",
    "document": "Related Work\n\nSafety evaluation for large language models (LLMs) has shifted from static lists of harmful prompts to dynamic adversarial testing and continuous monitoring. Practitioners employ automated adversaries, human red teams, and hybrid auditing frameworks to surface failure modes. According to industry surveys, over 60% of teams now deploy red-teaming pipelines. Despite growing adoption, methodological details, coverage metrics, and cost-effectiveness remain underreported.\n\nOur work systematizes red-teaming scenarios into a taxonomy of capability, compliance, and compositional risks, and proposes a lightweight coverage estimator to guide test generation.",
    "reason": "Presents a specific statistic derived from 'industry surveys' without citing any source (definition b and e).",
    "start": 290,
    "end": 372,
    "label": "Unsupported_claim"
  },
  {
    "span": "Previous shared tasks on gender bias in coreference have demonstrated large disparities across pronoun types.",
    "document": "Related Work\n\nResearch on bias in coreference resolution has highlighted gender, number, and occupation stereotypes that can degrade model fairness (Rudinger et al., 2018; Zhao et al., 2018). Diagnostics using minimal pairs and counterfactual data augmentation have been proposed to reduce bias while preserving coreference competence (Zhao et al., 2019; Webster et al., 2018).\n\nPrevious shared tasks on gender bias in coreference have demonstrated large disparities across pronoun types. These results have motivated dataset balancing schemes and evaluation protocols that report disaggregated performance rather than a single aggregate score.",
    "reason": "Refers to 'shared tasks' and their findings without citing the specific shared tasks or papers (rule a: cite at first mention of shared task; rule b: niche claim).",
    "start": 379,
    "end": 488,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recently, several prompting paradigms have been proposed to elicit capabilities from large language models without task-specific finetuning (Brown et al., 2020; Liu et al., 2021; Gao et al., 2021; Wei et al., 2022).",
    "document": "Introduction\n\nControllable Language Generation. Large language models (LLMs) have reshaped the landscape of text generation, enabling strong zero- and few-shot performance on diverse tasks. A central challenge is controlling model behavior to adhere to task instructions and constraints while maintaining fluency and factuality. Approaches range from supervision-heavy finetuning to lightweight methods that steer generation at inference time.\n\nPrompting Paradigms. Recently, several prompting paradigms have been proposed to elicit capabilities from large language models without task-specific finetuning (Brown et al., 2020; Liu et al., 2021; Gao et al., 2021; Wei et al., 2022). Extensions examine calibration, context selection, and format engineering to improve reliability in few-shot settings (Holtzman et al., 2021; Min et al., 2022). Efforts to reduce sensitivity to prompt wording consider ensembling or automatic prompt search.\n\nWe introduce a lightweight method that modifies input formatting and decoding to improve adherence to constraints in structured generation tasks.",
    "reason": "The span enumerates prompting works but does not connect them to the paper's objectives, limitations addressed, or the specific motivation for the proposed method (definition a and c).",
    "start": 466,
    "end": 681,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Saliency methods range from gradient-based maps, perturbation analyses, layer-wise relevance propagation, and concept activation vectors (Simonyan et al., 2013; Zeiler and Fergus, 2014; Bach et al., 2015; Kim et al., 2018).",
    "document": "Related Work\n\nInterpreting vision models is essential for trust, debugging, and regulatory compliance. A large literature proposes techniques to attribute model predictions to input features or higher-level concepts.\n\nSaliency methods range from gradient-based maps, perturbation analyses, layer-wise relevance propagation, and concept activation vectors (Simonyan et al., 2013; Zeiler and Fergus, 2014; Bach et al., 2015; Kim et al., 2018).\n\nCounterfactual explanations aim to identify minimal changes to flip predictions, while example-based explanations retrieve prototypical or influential training instances.\n\nOur approach focuses on causal faithfulness by aligning saliency maps with interventional outcomes in image-space perturbations.",
    "reason": "The span enumerates categories of methods with citations but does not relate them to the paper's causal perspective or identify gaps, satisfying (a) and (c).",
    "start": 218,
    "end": 441,
    "label": "Lacks_synthesis"
  },
  {
    "span": "the widely-used IEMOCAP benchmark contains noisy labels for minority classes",
    "document": "Introduction\n\nMultimodal emotion recognition seeks to infer affective states from speech, text, and visual cues. Progress is often measured on benchmark datasets that provide synchronized modalities and human annotations. However, label quality and class imbalance can bias reported improvements and hinder generalization. In particular, the widely-used IEMOCAP benchmark contains noisy labels for minority classes, which can inflate variance and mask true gains when models are calibrated on the majority classes.\n\nRecent methods leverage cross-modal attention, self-supervised pretraining on audio–visual corpora, and label smoothing to mitigate overfitting. Yet, the community lacks a systematic analysis of how annotation noise interacts with modality dropout and fusion strategies. We present an empirical study that disentangles these factors using robust training objectives and noise-aware validation protocols.",
    "reason": "Makes a specific claim about dataset label noise in IEMOCAP without citing any study or dataset audit to substantiate it.",
    "start": 338,
    "end": 414,
    "label": "Unsupported_claim"
  },
  {
    "span": "Personalized FL approaches include model interpolation (Mansour et al., 2020), meta-learning (Fallah et al., 2020), clustered training (Sattler et al., 2020), and multi-task optimization (Smith et al., 2017).",
    "document": "Related Work\n\nFederated learning (FL) distributes training across clients holding private data, but heterogeneity in data distributions leads to suboptimal global models for many users. Personalization in FL aims to tailor models while preserving collaborative gains and privacy.\n\nPersonalized FL approaches include model interpolation (Mansour et al., 2020), meta-learning (Fallah et al., 2020), clustered training (Sattler et al., 2020), and multi-task optimization (Smith et al., 2017).\n\nWe study a representation-sharing scheme with client-specific heads that adapts to non-IID data via a bilevel objective balancing global invariances and local idiosyncrasies. The method is communication-efficient and compatible with partial participation.\n\nComprehensive experiments on vision, speech, and text FL benchmarks show consistent personalization improvements under strong privacy constraints, with ablations on participation rates and client drift.",
    "reason": "The span enumerates categories of personalized FL without relating them to the paper's perspective or clarifying the unresolved gap, satisfying (a) and (c).",
    "start": 281,
    "end": 489,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Prior literature has established that code-mixed sentiment analysis benefits substantially from subword tokenization.",
    "document": "Introduction\n\nCode-mixed text appears frequently in social media, where users fluidly switch between languages within a single post. Processing such text is challenging due to inconsistent orthography, transliteration, and borrowed vocabulary. Prior literature has established that code-mixed sentiment analysis benefits substantially from subword tokenization. Nevertheless, lexical ambiguity and script variation continue to degrade performance, motivating hybrid approaches that combine character-level modeling with phonetic normalization.",
    "reason": "Makes a specific, niche claim about a technique's benefits in a subfield without citing supporting studies (rule b).",
    "start": 244,
    "end": 361,
    "label": "Unsupported_claim"
  },
  {
    "span": "Garcia et al. 3",
    "document": "Related Work\n\nFairness in machine learning spans definitional work on group vs. individual fairness (Dwork et al., 2012; Hardt et al., 2016) and practical interventions such as pre-processing, in-processing, and post-processing (Kamiran and Calders, 2012; Zhang et al., 2018). Garcia et al. 3 argue for stakeholder-specific utility trade-offs in credit scoring, while Mitchell et al. (2019) propose model cards for transparent reporting. Our work connects counterfactual fairness (Kusner et al., 2017) with causal regularization (Pearl, 2009).",
    "reason": "Wrong use of footnotes: a bare superscript-style number is appended to the author name without a year; it should be a proper in-text citation with year or a correctly formatted footnote.",
    "start": 277,
    "end": 292,
    "label": "Format"
  },
  {
    "span": "[Clark, 2020; Davis, 2021]",
    "document": "Related Work\n\nNeural information retrieval has progressed from Siamese encoders to late-interaction models (Karpukhin et al., 2020; Khattab and Zaharia, 2020). While training with in-batch negatives remains effective, hard-negative mining and knowledge distillation yield additional gains [Clark, 2020; Davis, 2021]. Multi-vector representations support better term recall (Santhanam et al., 2022), and hybrid BM25 plus dense retrieval remains strong (Lin et al., 2021).",
    "reason": "Wrong bracket style for in-text citations; square brackets are used instead of parentheses in an APA-like author–year format.",
    "start": 289,
    "end": 315,
    "label": "Format"
  },
  {
    "span": "The SemEval shared task on toxic span detection used BERT-base as a baseline.",
    "document": "Introduction\n\nToxic language detection has progressed from lexicon-based filters to context-aware neural models that capture subtle forms of harassment and incivility (Davidson et al., 2017; Wulczyn et al., 2017; Pavlopoulos et al., 2020). Span-level supervision refines explanations by pinpointing offending substrings, enabling more interpretable moderation pipelines.\n\nThe SemEval shared task on toxic span detection used BERT-base as a baseline.\n\nBuilding on span supervision, we propose a contrastive rationale objective that jointly optimizes detection and explanation quality, and we evaluate its generalization to out-of-domain communities.",
    "reason": "Mentions a specific shared task baseline without a citation to the shared task or its report; first mention of a shared task requires a reference.",
    "start": 372,
    "end": 449,
    "label": "Unsupported_claim"
  },
  {
    "span": "WMT14 English–German benchmark",
    "document": "Introduction\n\nNeural machine translation (NMT) has become the dominant approach to automatic translation. Early encoder–decoder models with attention improved alignment quality, and the Transformer architecture further boosted accuracy across several language pairs. With growing parallel corpora and subword segmentation, training efficiency and generalization have improved markedly. Standard evaluation typically relies on BLEU with tokenized, case-sensitive scoring. To facilitate comparison, we report results on the WMT14 English–German benchmark and the WMT16 English–Romanian dataset, following common preprocessing and training practices.",
    "reason": "Mentions a specific dataset/benchmark without providing a citation at first mention, which is required (rule a).",
    "start": 522,
    "end": 552,
    "label": "Unsupported_claim"
  },
  {
    "span": "[Tan and Lee, 2013]",
    "document": "Related Work. Recommender systems increasingly integrate side information to address cold-start scenarios (Schein et al., 2002; Saveski & Mantrach, 2014). Matrix factorization has been extended with content and graph signals to improve coverage and diversity (Koren et al., 2009; Ying et al., 2018). Context-aware recommenders model temporal and session effects for sequential prediction (Hidasi et al., 2015; Quadrana et al., 2017). Early hybrid models [Tan and Lee, 2013] fused content features with collaborative filtering, while more recent graph-based methods propagate preferences over item–user bipartite structures (He et al., 2020). We build on graph encoders with calibration-aware ranking.",
    "reason": "Wrong citation bracket/style: author–year citations in this document use parentheses, but '[Tan and Lee, 2013]' uses square brackets; should be '(Tan and Lee, 2013)'.",
    "start": 454,
    "end": 473,
    "label": "Format"
  },
  {
    "span": "In prior studies, curriculum learning has consistently improved convergence speed by at least 30%.",
    "document": "Related Work\n\nTraining strategies play a crucial role in stabilizing optimization for deep sequence models. Curriculum learning proposes to organize examples from easy to hard, thereby smoothing the loss landscape encountered during the early phases of training. Variants of this idea include self-paced learning, difficulty-aware sampling, and dynamic example reweighting.\n\nIn prior studies, curriculum learning has consistently improved convergence speed by at least 30%. However, it remains unclear how to define difficulty in a manner that generalizes across tasks and architectures. We compare multiple automatic difficulty heuristics and investigate the interaction between curriculum schedules and adaptive optimizers.",
    "reason": "Claims a quantitative effect from previous studies without any supporting citations or evidence.",
    "start": 375,
    "end": 473,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recently, several studies propose diverse data augmentation strategies for low-resource NER, including back-translation, paraphrasing with pretrained LMs, mention-level span replacement, gazetteer-driven synthesis, and template-based generation. Other lines of work explore weak supervision with heuristics and distant labels, as well as consistency training under perturbations.",
    "document": "Introduction\nNamed entity recognition in low-resource settings suffers from data scarcity and domain mismatch. While pretrained encoders help, performance still degrades sharply when labeled examples are limited.\n\nRelated Work\nSemi-supervised and data augmentation techniques aim to expand training signals without expensive annotation.\nRecently, several studies propose diverse data augmentation strategies for low-resource NER, including back-translation, paraphrasing with pretrained LMs, mention-level span replacement, gazetteer-driven synthesis, and template-based generation. Other lines of work explore weak supervision with heuristics and distant labels, as well as consistency training under perturbations.\nDomain adaptation approaches further align source and target distributions with adversarial training or representation invariance, and parameter-efficient tuning methods adapt large models with limited updates.\n\nOverview of Our Method\nWe introduce an example-centric augmentation pipeline with uncertainty-aware sampling to select informative spans.",
    "reason": "The span enumerates augmentation approaches without explaining their limitations or how the current method addresses them, lacking synthesis per (a) and (b).",
    "start": 337,
    "end": 716,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The MCTest corpus contains 500 stories and is widely used for evaluating narrative understanding.",
    "document": "Related Work\n\nNarrative comprehension benchmarks evaluate a model’s ability to integrate events, characters, and causal relations. The MCTest corpus contains 500 stories and is widely used for evaluating narrative understanding. Other datasets emphasize commonsense reasoning, temporal ordering, and coreference in long-form narratives.\n\nHowever, differences in question style, answer format, and passage length complicate cross-dataset comparisons, motivating standardized evaluation protocols and robustness analyses.",
    "reason": "Provides a specific dataset statistic and claims widespread use without any supporting citation (rules a and b).",
    "start": 131,
    "end": 228,
    "label": "Unsupported_claim"
  },
  {
    "span": "There are many recent works that explore faithfulness metrics for NLG.",
    "document": "Related Work\n\nFaithfulness in neural text generation. Text generation systems often produce fluent but factually inconsistent outputs, motivating metrics that directly assess faithfulness. There are many recent works that explore faithfulness metrics for NLG. Approaches range from entailment-based scoring and question-answering probes to extraction- and retrieval-based checks.\n\nTask-specific considerations. Summarization, data-to-text, and dialogue each present distinct challenges for grounding and hallucination detection. Metric performance depends on the availability of reference documents, the type of factual errors, and the granularity of evaluation (token vs. sentence vs. document).\n\nHuman evaluation. Despite progress in automatic metrics, human judgments remain the gold standard for faithfulness, with protocols varying by task, instructions, and annotator expertise.",
    "reason": "The sentence makes a general claim about 'many recent works' but gives no citations to support it.",
    "start": 189,
    "end": 259,
    "label": "Unsupported_claim"
  },
  {
    "span": "Fusion techniques combine text, audio, and video using early, late, or hybrid strategies (Zadeh et al., 2017; Tsai et al., 2019; Mai et al., 2020).",
    "document": "Introduction\n\nMultimodal sentiment analysis seeks to infer opinions and emotions from heterogeneous inputs such as spoken language, facial expressions, and prosody. The central question is how to represent and fuse modalities that differ in timing, noise characteristics, and semantic content.\n\nFusion techniques combine text, audio, and video using early, late, or hybrid strategies (Zadeh et al., 2017; Tsai et al., 2019; Mai et al., 2020). Alignment methods adjust for asynchrony through cross-modal attention or graph structures (Pham et al., 2019; Yu et al., 2021). Benchmarks like CMU-MOSI, MOSEI, and CH-SIMS standardize evaluation across modalities and languages (Zadeh et al., 2016; Zadeh et al., 2018; Yu et al., 2020).\n\nPre-trained transformers adapted to audio-visual-linguistic streams have demonstrated promising results, though data scarcity and domain shift remain practical hurdles (Nagrani et al., 2021; Akbari et al., 2021).",
    "reason": "The span offers a general list of fusion categories and citations but does not connect these techniques to the authors' problem setting, articulate a gap, or state a viewpoint, thus lacking synthesis.",
    "start": 295,
    "end": 442,
    "label": "Lacks_synthesis"
  },
  {
    "span": "((Johnson, 2014))",
    "document": "Related Work\n\nText classification with distant supervision uses heuristics or external signals to generate noisy labels (Mintz et al., 2009; Ratner et al., 2017). Early work proposed aggregating weak sources via generative modeling to estimate accuracies ((Johnson, 2014)) and then training discriminative models on denoised labels (Bach et al., 2017). Later approaches integrate confidence-aware losses to mitigate overfitting (Zheng et al., 2021).\n",
    "reason": "Double parentheses around the citation; should be a single set, e.g., “(Johnson, 2014)”.",
    "start": 255,
    "end": 272,
    "label": "Format"
  },
  {
    "span": "Centralized training with decentralized execution has been widely adopted (Lowe et al., 2017; Foerster et al., 2018). Value factorization methods decompose joint action values (Sunehag et al., 2018; Rashid et al., 2018), while policy gradient approaches address credit assignment and coordination (Iqbal and Sha, 2019; Foerster et al., 2017). Communication learning enables agents to share information (Sukhbaatar et al., 2016; Jiang and Lu, 2018).",
    "document": "Introduction Multi-agent reinforcement learning (MARL) tackles sequential decision-making with interacting learners, facing challenges in credit assignment, non-stationarity, and coordination. Despite progress, sample inefficiency and brittleness to partner changes persist in realistic environments. Centralized training with decentralized execution has been widely adopted (Lowe et al., 2017; Foerster et al., 2018). Value factorization methods decompose joint action values (Sunehag et al., 2018; Rashid et al., 2018), while policy gradient approaches address credit assignment and coordination (Iqbal and Sha, 2019; Foerster et al., 2017). Communication learning enables agents to share information (Sukhbaatar et al., 2016; Jiang and Lu, 2018). We propose partner-robust factorization with counterfactual data augmentation that provably bounds return degradation under partner swaps. Experiments on SMAC and Overcooked show improved zero-shot coordination.",
    "reason": "The span compiles categories of MARL methods without relating them to the partner-robust objective or clarifying the gap addressed. It lacks explicit synthesis and motivation (a, b).",
    "start": 301,
    "end": 749,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The XQuAD and MLQA suites are widely accepted as the standard for cross-lingual question answering evaluation.",
    "document": "Introduction\n\nCross-lingual question answering (CLQA) assesses a model's ability to transfer reasoning across languages, often relying on multilingual pretraining and alignment objectives. A persistent challenge is selecting evaluation suites that reflect both typological diversity and realistic domain shifts. While multiple datasets exist, their coverage and difficulty vary considerably.\n\nThe XQuAD and MLQA suites are widely accepted as the standard for cross-lingual question answering evaluation. Nevertheless, these resources focus primarily on extractive QA and may not capture generative or multi-hop reasoning phenomena across languages. We therefore complement extractive evaluation with generative QA and retrieval-augmented setups to assess broader capabilities.\n\nOur experiments analyze performance by language family, script, and resource level, revealing uneven transfer that correlates with pretraining data availability.",
    "reason": "This statement claims community-wide acceptance of specific datasets as standards without citing any sources, surveys, or benchmark reports.",
    "start": 393,
    "end": 503,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT was used in an AES setup trained exclusively on handwritten essays.",
    "document": "Related Work\n\nAutomated essay scoring (AES) has shifted from handcrafted features toward neural models that capture discourse, coherence, and semantics (Taghipour and Ng, 2016; Dong et al., 2017). Transformer-based encoders have improved reliability by leveraging large-scale pretraining and task-specific finetuning (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES setup trained exclusively on handwritten essays. Complementary research has explored multimodal AES that integrates visual cues of handwriting with textual content extracted via OCR, highlighting challenges in noise and domain shift (Alikaniotis et al., 2016). Our approach unifies content and handwriting style signals through a late-fusion calibration model.",
    "reason": "Specific claim about a prior BERT-based AES setup and dataset modality is made without any citation to the study.",
    "start": 358,
    "end": 430,
    "label": "Unsupported_claim"
  },
  {
    "span": "Several recent papers report that graph transformers outperform GNN baselines on molecular property prediction by a large margin.",
    "document": "Related Work\nGraph neural networks (GNNs) have become the de facto standard for learning over molecular graphs, with message passing capturing local interactions. Recently, architectures that combine attention with positional or structural encodings have been proposed to better capture long-range dependencies.\nSeveral recent papers report that graph transformers outperform GNN baselines on molecular property prediction by a large margin. This has spurred interest in designing inductive biases that recover the benefits of message passing while leveraging global attention.\nOur work focuses on data- and compute-efficient variants that maintain accuracy under limited supervision common in chemistry applications.",
    "reason": "Uses the phrase 'recent papers report' and a comparative claim without any citations to those papers.",
    "start": 312,
    "end": 441,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most industrial recommenders rely on two-tower architectures with in-batch negatives for large-scale retrieval.",
    "document": "Introduction\n\nRecommender systems at scale often separate candidate retrieval from ranking to meet latency constraints. Classical collaborative filtering and matrix factorization approaches have been enhanced by deep representation learning for users and items (Koren et al., 2009; He et al., 2017). Approximate nearest neighbor search is typically used to retrieve candidates from embedding spaces.\n\nMost industrial recommenders rely on two-tower architectures with in-batch negatives for large-scale retrieval. While effective, these setups can suffer from sampling bias and embedding collapse under skewed popularity distributions. We study a debiased retrieval objective with adaptive negative sampling and report improvements on multiple large-scale datasets.",
    "reason": "Asserts a broad claim about common industrial practice without citing supporting studies or system descriptions; such claims require evidence.",
    "start": 401,
    "end": 512,
    "label": "Unsupported_claim"
  },
  {
    "span": "Johnson et al.",
    "document": "Related Work\n\nGraph representation learning has advanced rapidly with message-passing architectures and scalable training strategies for large networks (Hamilton et al., 2017; Kipf and Welling, 2017). Unsupervised objectives such as random-walk-based contrastive learning have been widely adopted to capture structural regularities (Perozzi et al., 2014; Grover and Leskovec, 2016). Following Johnson et al., we adopt neighborhood aggregation combined with feature normalization to stabilize training on sparse graphs. Recent studies further incorporate positional encodings to overcome the limitations of purely local aggregation (Dwivedi et al., 2021; You et al., 2021). While graph transformers show promise on heterogeneous benchmarks (Hu et al., 2020), scalability remains a concern due to quadratic complexity in the number of nodes (Chen et al., 2018). Our work aligns with inductive settings where new nodes appear at test time (Hamilton et al., 2017), but differs in leveraging curriculum-based subgraph sampling for efficiency.",
    "reason": "Narrative citation is missing the year; it should be formatted as a narrative citation with the year, for example, “Johnson et al. (2018)”.",
    "start": 393,
    "end": 407,
    "label": "Format"
  },
  {
    "span": "Singh et al. 1",
    "document": "Introduction\n\nFederated learning addresses data governance by training models across clients without centralizing raw data (McMahan et al., 2017; Li et al., 2020). Yet performance deteriorates under heterogeneous client distributions, making personalization and fairness central concerns (Fallah et al., 2020; Deng and Huang, 2021).\n\nComprehensive surveys Singh et al. 1 summarize advances in aggregation rules, client sampling, and privacy protection, but few works evaluate long-tail performance under strict participation budgets (Zhang and Kim, 2022). We extend these surveys with a benchmark that emphasizes rare-client utility and stratified evaluation protocols (Yu and Rao, 2023).",
    "reason": "Improper use of a footnote-like superscript number with an author-year style; should include a year (e.g., Singh et al. (YEAR)) or be formatted as a proper footnote.",
    "start": 356,
    "end": 370,
    "label": "Format"
  },
  {
    "span": "a previous study showed that pointer-generator networks outperform phrase-based simplifiers on news articles",
    "document": "Introduction\n\nText simplification aims to improve the readability of complex sentences while preserving meaning. Neural sequence-to-sequence approaches, particularly those initialized with pretrained language models, have become the de facto standard due to their ability to model paraphrasing and content selection jointly. Despite these advances, controlling the degree of simplification and avoiding semantic drift remain challenging.\n\nPrior work has investigated copying mechanisms and coverage penalties to reduce hallucinations. In addition, a previous study showed that pointer-generator networks outperform phrase-based simplifiers on news articles, suggesting that explicit copying can help maintain factual consistency. Building on these insights, we propose a controllable simplification model that leverages lexical difficulty targets and syntactic cues to balance compression with fidelity.",
    "reason": "This sentence refers to a specific prior study and its result but does not provide a citation (rule a: cite at first mention of a study/paper).",
    "start": 548,
    "end": 656,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Patel 2021)",
    "document": "Related Work\n\nA growing body of surveys has synthesized advances in graph neural networks, covering architectures, training tricks, and applications (Zhou et al., 2020; Wu et al., 2021). Recent overviews emphasize scalability via sampling and sparsity-aware kernels (Chiang et al., 2019; Zou et al., 2019), while others focus on robustness and distribution shift (Jin et al., 2020). Comprehensive taxonomies also address design patterns for message passing (Patel 2021), which we build upon to propose a unifying benchmark suite.\n",
    "reason": "Missing comma between author and year in a parenthetical citation: should be formatted as \"(Patel, 2021)\".",
    "start": 457,
    "end": 469,
    "label": "Format"
  },
  {
    "span": "(Garcia Et al., 2017)",
    "document": "Introduction\n\nLearning from weak supervision aggregates noisy heuristics and distant labels to reduce annotation costs (Ratner et al., 2017; Bach et al., 2019). In information extraction, distant supervision aligns text with KB facts but induces label noise that must be modeled (Mintz et al., 2009; Riedel et al., 2010). Recent denoising approaches use probabilistic labeling functions and structure-aware encoders (Zhang and Ning, 2020). For relation classification, (Garcia Et al., 2017) propose a bootstrapping scheme that iteratively refines seed patterns with confidence thresholds, but their evaluation lacks out-of-domain tests.",
    "reason": "Incorrect capitalization of 'et al.': should be lowercase 'et al.' within the parenthetical citation.",
    "start": 469,
    "end": 490,
    "label": "Format"
  },
  {
    "span": "BERT-large has been the de facto backbone for AES datasets such as ASAP.",
    "document": "Related Work\n\nAutomated essay scoring (AES) evaluates writing quality with machine learning models trained on scored essays [1]. Transformer encoders have supplanted earlier feature-based systems by capturing richer semantics [2]. BERT-large has been the de facto backbone for AES datasets such as ASAP. Recent studies investigate multi-trait scoring and cross-prompt generalization with domain adaptation techniques [3].",
    "reason": "Claims standard practice and references a dataset without providing citations to supporting studies.",
    "start": 231,
    "end": 303,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Rahimi 2019)",
    "document": "Introduction\n\nDomain adaptation theory formalizes generalization across distributions via divergence measures and risk decompositions (Ben-David et al., 2010). Practical methods include importance weighting (Shimodaira, 2000; Sugiyama et al., 2008) and feature alignment (Ganin et al., 2016). Recent benchmarks emphasize label shift and calibration (Lipton et al., 2018; Garg et al., 2020). We compare importance-weighting baselines and robust losses (Rahimi 2019) under realistic drift in streaming settings.",
    "reason": "Missing comma between author and year in an author–date citation; should be \"(Rahimi, 2019)\".",
    "start": 451,
    "end": 464,
    "label": "Format"
  },
  {
    "span": "Pearl (2009) formalizes structural causal models. Hardt et al. (2016) define equalized odds for fair classification. Kusner et al. (2017) propose counterfactual fairness. Imbens and Rubin (2015) provide the potential outcomes framework.",
    "document": "Related Work\n\nCausal Inference and Algorithmic Fairness\n\nWe position our work at the intersection of causal modeling and fairness criteria. Pearl (2009) formalizes structural causal models. Hardt et al. (2016) define equalized odds for fair classification. Kusner et al. (2017) propose counterfactual fairness. Imbens and Rubin (2015) provide the potential outcomes framework. Recent efforts connect causal graphs with fairness interventions (Chiappa, 2019; Kilbertus et al., 2017).\n\nWe build on these ideas to learn transportable fairness constraints under distribution shift.",
    "reason": "The sequence introduces causal models and fairness metrics without transitions that clarify their relationships, leaving the links between frameworks implied rather than explicit.",
    "start": 140,
    "end": 376,
    "label": "Coherence"
  },
  {
    "span": "Data-driven grasp synthesis includes convolutional grasp scoring, depth-based antipodal detectors, policy gradients, Q-learning, and off-policy actor-critic methods (Lenz et al., 2015; Mahler et al., 2017; Levine et al., 2018; Kalashnikov et al., 2018; Zeng et al., 2018).",
    "document": "Related Work\n\nRobotic grasping has shifted from analytic force-closure heuristics to learning-based methods that exploit large datasets and powerful visual encoders. These approaches promise robustness to novel objects and clutter but can be sensitive to distribution shift between simulation and reality.\n\nData-driven grasp synthesis includes convolutional grasp scoring, depth-based antipodal detectors, policy gradients, Q-learning, and off-policy actor-critic methods (Lenz et al., 2015; Mahler et al., 2017; Levine et al., 2018; Kalashnikov et al., 2018; Zeng et al., 2018). Parallel work explores grasp refinement via tactile feedback, domain randomization, and uncertainty-aware selection under budgeted execution.\n\nWe study low-shot adaptation of grasp policies across grippers and cameras, introducing a cross-modal calibration objective that aligns visual affordances with embodiment-specific constraints.",
    "reason": "This sentence enumerates prior approaches without linking them to the paper’s specific problem or identifying a gap, thus lacking synthesis.",
    "start": 307,
    "end": 579,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Time-series anomaly detection methods include forecasting-based detectors (Audibert et al., 2020), reconstruction-based autoencoders (Zong et al., 2018), and probabilistic models (Xu et al., 2018).",
    "document": "Related Work\n\nTime-Series Anomaly Detection\nIndustrial monitoring and finance often rely on early detection of unusual patterns in multivariate time series. Time-series anomaly detection methods include forecasting-based detectors (Audibert et al., 2020), reconstruction-based autoencoders (Zong et al., 2018), and probabilistic models (Xu et al., 2018). Recent benchmarks consider detection delay and false alarm trade-offs under non-stationarity.\n\nRepresentation Learning\nContrastive and self-supervised pretraining across time augmentations have improved downstream detection in low-label regimes, with varying robustness to covariate shift.\n\nEvaluation Protocols\nEvaluation remains fragmented, with inconsistent definitions of anomaly windows, latency penalties, and scoring rules across datasets.",
    "reason": "The span summarizes existing categories but does not synthesize them into a viewpoint or explain their relevance or limitations relative to the authors' approach (criterion a and c).",
    "start": 157,
    "end": 354,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Ekstrand et al. (2018) surveyed fairness concepts and trade-offs in recommender systems. Yao and Huang (2017) quantified disparate impact in exposure allocation. Rendle et al. (2009) introduced Bayesian Personalized Ranking for pairwise optimization. Burke (2017) proposed fairness-aware reranking approaches.",
    "document": "Related Work\n\nFairness in recommender systems involves mitigating disparities for users and item providers while preserving utility. Methods include pre-processing of input data, in-processing constraints, and post-processing via reranking.\n\nEkstrand et al. (2018) surveyed fairness concepts and trade-offs in recommender systems. Yao and Huang (2017) quantified disparate impact in exposure allocation. Rendle et al. (2009) introduced Bayesian Personalized Ranking for pairwise optimization. Burke (2017) proposed fairness-aware reranking approaches.\n\nSubsequent work extends beyond exposure to consider calibration, diversity, and multi-stakeholder objectives under uncertainty (Abdollahpouri et al., 2020; Mehrotra et al., 2018). We analyze counterfactual estimates of provider exposure under implicit feedback.",
    "reason": "The list combines fairness-focused papers with a general ranking algorithm (BPR) without clarifying their connection, and provides no transitions to establish relationships among the works.",
    "start": 242,
    "end": 551,
    "label": "Coherence"
  },
  {
    "span": "(Lopez et al. 2022)",
    "document": "Related Work\n\nFederated learning (FL) enables collaborative model training without centralizing raw data (McMahan et al., 2017; Kairouz et al., 2021). Heterogeneous clients with non-IID distributions pose optimization and generalization challenges (Zhao et al., 2018; Li et al., 2020). Personalization and meta-learning techniques address client drift and improve fairness (Fallah et al., 2020; Arivazhagan et al., 2019).\n\nPrivacy in FL is commonly enforced using secure aggregation and differential privacy (Bonawitz et al., 2017; Truex et al., 2020; Geyer et al., 2017). However, gradient leakage attacks can reconstruct sensitive inputs from shared updates (Zhu et al., 2019; Geiping et al., 2020). Mitigations include gradient compression, clipping, and cryptographic masking (Sattler et al., 2019; Acar et al., 2021).\n\nWe focus on auditability and accountability via verifiable training logs. Prior audits target centralized pipelines (Hynes et al., 2018) and have recently been adapted to cross-device FL (Kim et al., 2021). Our approach extends attestation to encompass aggregation and noise injection steps, enabling end-to-end proofs of compliance. Similar to secure logging systems (Schneier and Kelsey, 1999), we chain attestations to prevent tampering. Compared to (Lopez et al. 2022), our method avoids trusted hardware assumptions while maintaining negligible overhead on mobile devices.",
    "reason": "Missing comma between author group and year inside parentheses; it should be '(Lopez et al., 2022)'.",
    "start": 1277,
    "end": 1296,
    "label": "Format"
  },
  {
    "span": "Vaswani et al.",
    "document": "Related Work\n\nNeural sequence modeling has evolved from recurrent networks with attention to fully attention-based architectures (Bahdanau et al., 2015; Luong et al., 2015). The canonical architecture proposed by Vaswani et al. has since been adapted to a wide range of NLP tasks, including machine translation, parsing, and question answering (Devlin et al., 2019; Raffel et al., 2020). Pretrained language models further demonstrated the effectiveness of large-scale self-supervised learning (Peters et al., 2018; Radford et al., 2019), and subsequent work explored scaling laws for model capacity and data (Kaplan et al., 2020).\n\nBeyond language, multimodal extensions align textual and visual representations for retrieval and captioning (Kiros et al., 2014; Dosovitskiy et al., 2021). Despite these advances, domain specificity and data imbalance remain open challenges (Gururangan et al., 2020; Wang et al., 2021).",
    "reason": "Narrative citation is missing the year; should be formatted as a narrative author-year citation, e.g., \"Vaswani et al. (2017)\".",
    "start": 213,
    "end": 227,
    "label": "Format"
  },
  {
    "span": "Sakurada and Yairi (2014) used autoencoders for unsupervised anomaly detection. Liu et al. (2008) proposed Isolation Forest for outlier detection. Ren et al. (2019) modeled seasonalities with STL decomposition. Su et al. (2019) introduced RobustSTL for complex trends.",
    "document": "Related Work\n\nTime-series anomaly detection encompasses reconstruction-based models, density estimation, and decomposition approaches to separate trend, seasonality, and residuals. Industrial applications often require methods that are robust to nonstationarity and missing data.\n\nSakurada and Yairi (2014) used autoencoders for unsupervised anomaly detection. Liu et al. (2008) proposed Isolation Forest for outlier detection. Ren et al. (2019) modeled seasonalities with STL decomposition. Su et al. (2019) introduced RobustSTL for complex trends.\n\nWe target streaming settings with concept drift by combining adaptive decomposition with uncertainty-aware detection thresholds.",
    "reason": "The paragraph lists disparate methods in sequence without transitions or explaining how general outlier detection relates to time-series-specific decomposition, causing abrupt shifts and unclear connections.",
    "start": 281,
    "end": 549,
    "label": "Coherence"
  },
  {
    "span": "(Choi et al. 2013)",
    "document": "Related Work\n\nEvent extraction research spans pipeline architectures and end-to-end neural models that jointly identify triggers and arguments (Ji and Grishman, 2008; Chen et al., 2015; Nguyen and Grishman, 2015). Pretrained language models have improved transfer across domains, but fine-grained schemas still pose challenges (Yamada et al., 2020; Du and Cardie, 2020).\n\nDistant supervision reduces annotation cost by aligning text with knowledge bases, though it introduces noise mitigated by denoising objectives (Mintz et al., 2009; Zeng et al., 2015; Qin et al., 2018). Semi-structured corpora and document-level context further help capture cross-sentence events (Peng et al., 2017; Wadden et al., 2019). For zero-shot settings, label semantics and prompt tuning have shown promise (Liu et al., 2022; Chen et al., 2021).\n\nTemporal reasoning and causal linking remain underexplored; constraints and global inference can help, but scalability is an issue (Ning et al., 2018; Han et al., 2019). Prior work on joint temporal-event modeling (Choi et al. 2013) motivates our structured inference component that enforces timeline consistency across documents.",
    "reason": "Missing comma before the year in a parenthetical citation; should be '(Choi et al., 2013)'.",
    "start": 1042,
    "end": 1060,
    "label": "Format"
  },
  {
    "span": "(Deng and Li 2015)",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) has transitioned from hybrid HMM-DNN pipelines to attention-based encoder-decoder and transducer architectures (Graves, 2012; Chan et al., 2016; Graves, 2013; Graves et al., 2006). Connectionist temporal classification enables alignment-free training (Graves et al., 2006), while sequence-to-sequence models learn direct mappings from acoustics to text (Chan et al., 2016; Chorowski et al., 2015).\n\nSelf-supervised pretraining on raw audio has delivered substantial gains in low-resource settings (Baevski et al., 2020; Hsu et al., 2021). SpecAugment and other augmentation strategies improve robustness (Park et al., 2019). Language modeling and shallow fusion remain crucial for competitiveness (Kannan et al., 2018; Gulati et al., 2020).\n\nAcoustic modeling improvements were explored in (Deng and Li 2015), and streaming-friendly designs reduce latency for on-device inference (He et al., 2019; Prabhavalkar et al., 2017). Our work revisits pronunciation variability and proposes a lexicon-aware encoder with differentiable lexeme constraints.\n\nWe benchmark on LibriSpeech and TED-LIUM, reporting WER improvements and detailed ablations.",
    "reason": "Missing comma before the year in a parenthetical author-year citation; the correct style is \"(Deng and Li, 2015)\".",
    "start": 850,
    "end": 868,
    "label": "Format"
  },
  {
    "span": "Most existing methods rely on proprietary click logs rather than publicly available datasets.",
    "document": "Introduction\n\nLearning-to-rank models for search are frequently trained from implicit feedback, with click signals serving as a noisy proxy for relevance. While counterfactual and debiasing techniques reduce position and selection bias, the availability and quality of training data remain bottlenecks.\n\nMost existing methods rely on proprietary click logs rather than publicly available datasets. This limits reproducibility and complicates fair comparison across ranking strategies. We mitigate these issues by releasing a large-scale synthetic click corpus with calibrated bias parameters and pairing it with a standardized evaluation suite.",
    "reason": "Makes a field-wide claim about the prevalence of proprietary data without evidence or references (rule b).",
    "start": 304,
    "end": 397,
    "label": "Unsupported_claim"
  },
  {
    "span": "There is widespread agreement that exposure bias is the main reason for brittle text generation.",
    "document": "Introduction\n\nNeural text generation models often exhibit instability under distribution shift, producing degenerate repetitions or off-topic continuations. A frequently discussed hypothesis attributes such brittleness to discrepancies between training and inference, where teacher forcing during training fails to expose the model to its own errors.\n\nThere is widespread agreement that exposure bias is the main reason for brittle text generation. Alternative explanations point to calibration issues, decoding heuristics, and insufficient coverage of rare events in pretraining corpora.\n\nWe revisit this debate empirically by designing controlled perturbation suites and evaluating strategies that target each hypothesized source of brittleness, including scheduled sampling, entropy regularization, and constrained decoding.\n",
    "reason": "Makes a broad consensus claim about the literature without any citations to support it.",
    "start": 352,
    "end": 448,
    "label": "Unsupported_claim"
  },
  {
    "span": "The HateX dataset is widely regarded as the de facto benchmark for Arabic hate speech detection.",
    "document": "Related Work\n\nAutomatic detection of abusive and hateful language in Arabic faces challenges due to dialectal variation, code-switching, and morphological richness (Mubarak et al., 2021; Alshaalan and Al-Khalifa, 2020). Several corpora have been released, spanning tweets, forum posts, and comments, with varying label granularities and annotation protocols. The HateX dataset is widely regarded as the de facto benchmark for Arabic hate speech detection. Beyond supervised learning, recent approaches leverage pre-trained multilingual encoders and task-adaptive pretraining to mitigate domain shift (Conneau et al., 2020; Antoun et al., 2020).\n\nOur study contributes a dialect-balanced collection with fine-grained target annotations and a robust evaluation protocol that measures cross-dialect generalization.",
    "reason": "Claims a dataset is the 'de facto benchmark' without any citation to support that community consensus (violates rule a and b).",
    "start": 359,
    "end": 455,
    "label": "Unsupported_claim"
  },
  {
    "span": "Johnson et al.",
    "document": "Introduction\n\nGraph neural networks (GNNs) have emerged as a powerful framework for learning over relational data. Early spectral methods (Bruna et al., 2014) were complemented by spatial message passing frameworks (Gilmer et al., 2017). Johnson et al. show that attention mechanisms can adaptively weight edges in heterogeneous graphs, reducing over-smoothing. Later work by Xu et al. (2019) studied the expressivity of GNNs via the Weisfeiler–Lehman test, while Oono and Suzuki (2020) analyzed oversmoothing in deep architectures.",
    "reason": "Narrative citation missing year: the narrative form 'Johnson et al.' should include the year, e.g., 'Johnson et al. (2019)'.",
    "start": 238,
    "end": 252,
    "label": "Format"
  },
  {
    "span": "BERT has been used in AES to model prompt adherence using pairwise ranking.",
    "document": "Related Work\n\nAutomated essay scoring (AES) systems have transitioned from handcrafted features to neural encoders that capture semantics, coherence, and discourse structure. Early neural AES relied on recurrent networks and hierarchical encoders to aggregate sentence-level signals. BERT has been used in AES to model prompt adherence using pairwise ranking. Beyond prompt relevance, researchers have explored rubric-aware objectives and adversarial training to reduce sensitivity to superficial cues.\n\nDespite these advances, robustness to off-topic responses and distribution shifts across prompts remains a key weakness. We address this by introducing a calibration objective that aligns learned representations with explicit prompt facets.",
    "reason": "Introduces a specific methodological use of BERT in AES without any supporting citation (definition a and e).",
    "start": 284,
    "end": 359,
    "label": "Unsupported_claim"
  },
  {
    "span": "In the M4 competition, transformer-based models dominated performance across most categories.",
    "document": "Related Work\n\nForecasting methods span statistical models such as exponential smoothing and ARIMA, machine learning approaches including gradient boosting and kernel methods, and more recently deep architectures. Hybrid schemes combine decomposition with learned residuals to capture both trend-seasonality and nonlinear interactions. Evaluation protocols often consider accuracy, calibration, and robustness to distribution shifts.\n\nIn the M4 competition, transformer-based models dominated performance across most categories. This has motivated a surge of interest in attention-based temporal models for large-scale forecasting. Subsequent work explored seasonality-aware positional encodings, multivariate cross-series attention, and hierarchical reconciliation layers for grouped forecasts.\n\nNevertheless, attention mechanisms can be data-hungry and computationally expensive. Recent research has proposed structured state-space layers and convolutional alternatives to improve efficiency while retaining long-range dependencies. Our work takes a complementary direction by integrating a probabilistic seasonal basis with lightweight cross-series mixing, yielding competitive accuracy under strict latency budgets.\n\nWe benchmark against strong baselines under rolling-origin evaluation and report both accuracy and energy consumption to highlight deployment constraints in edge settings.",
    "reason": "References the outcomes of the M4 competition without citing any source or providing evidence.",
    "start": 434,
    "end": 527,
    "label": "Unsupported_claim"
  },
  {
    "span": "Demographic parity has been explored as a group fairness target (Dwork et al., 2012). Equalized odds enforces conditional independence across groups (Hardt et al., 2016). Calibration across subpopulations has also been studied (Pleiss et al., 2017). Counterfactual fairness considers causal notions of harm (Kusner et al., 2017).",
    "document": "Introduction\n\nFairness in Machine Learning\n\nAlgorithmic fairness seeks to mitigate disparate outcomes across protected groups while retaining utility. Multiple definitions and mitigation strategies have emerged, reflecting distinct normative goals and statistical trade-offs.\n\nDefinitions and Tensions\n\nDemographic parity has been explored as a group fairness target (Dwork et al., 2012). Equalized odds enforces conditional independence across groups (Hardt et al., 2016). Calibration across subpopulations has also been studied (Pleiss et al., 2017). Counterfactual fairness considers causal notions of harm (Kusner et al., 2017).\n\nMitigation Strategies\n\nApproaches include pre-processing reweighting, in-processing regularization, and post-processing threshold adjustments (Kamiran and Calders, 2012; Zafar et al., 2017; Hardt et al., 2016). Our work examines cost-sensitive learning under shifting base rates.",
    "reason": "The sentences enumerate fairness definitions without transitions or explicit comparisons, leaving the reader unclear on how each notion relates to the previous one or how they connect conceptually.",
    "start": 303,
    "end": 632,
    "label": "Coherence"
  },
  {
    "span": "BLEU remains a common choice for dialog generation despite known drawbacks (Papineni et al., 2002). Embedding-based metrics correlate with human judgments in some settings (Lowe et al., 2017). Learned evaluators train on annotated conversations (Zhang et al., 2020).",
    "document": "Related Work\n\nEvaluating open-domain dialog is challenging due to response diversity and context sensitivity. Automatic metrics aim to approximate human judgments while remaining efficient and reproducible.\n\nBLEU remains a common choice for dialog generation despite known drawbacks (Papineni et al., 2002). Embedding-based metrics correlate with human judgments in some settings (Lowe et al., 2017). Learned evaluators train on annotated conversations (Zhang et al., 2020).\n\nHowever, these metrics often struggle with contextual appropriateness and safety. We propose a reference-free evaluator that conditions on dialog acts and speaker goals to better capture pragmatic adequacy.",
    "reason": "The sentences list metrics without transitions or explicit relationships, making it unclear how each cited work relates to the others or to a coherent evaluation landscape.",
    "start": 208,
    "end": 474,
    "label": "Coherence"
  },
  {
    "span": "see [Rao, 2017)",
    "document": "Introduction\n\nFederated learning enables on-device training without centralizing data (McMahan et al., 2017). Privacy amplification by subsampling and secure aggregation provide guarantees (Abadi et al., 2016; Bonawitz et al., 2017). For a comprehensive survey, see [Rao, 2017) and recent refinements in user-level DP (Kairouz et al., 2021). Our contribution is a variance-reduced optimizer tailored to heterogeneous clients (Li et al., 2020).",
    "reason": "Mismatched brackets; begins with '[' and ends with ')'. Should be '(Rao, 2017)' or use consistent brackets.",
    "start": 262,
    "end": 277,
    "label": "Format"
  },
  {
    "span": "A previous study showed that image–text incongruity is the main signal for multimodal sarcasm.",
    "document": "Introduction\n\nMultimodal sarcasm detection requires integrating visual cues with linguistic context to capture pragmatic contradictions. While text-only models can learn lexical and syntactic markers, visual information provides complementary signals about objects, scenes, and affect. A previous study showed that image–text incongruity is the main signal for multimodal sarcasm. Building on this intuition, we design a cross-modal alignment module that penalizes over-reliance on either modality and improves robustness to spurious correlations.",
    "reason": "This references “a previous study” and its finding without providing a citation, violating rule (a) and example (e-ii).",
    "start": 286,
    "end": 380,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Barker et al. 2020)",
    "document": "Introduction\n\nAccurate time-series forecasting under distribution shift is vital for operations and public health. Classical approaches rely on state space models (Durbin and Koopman, 2012) and ARIMA variants (Box and Jenkins, 1976). Deep sequence models have recently achieved strong accuracy in multivariate settings (Salinas et al., 2020; Lim et al., 2021). However, models often fail to generalize when exogenous regimes change. Recent work (Barker et al. 2020) explores domain-adaptive normalization to address this challenge. We extend this idea with test-time adaptation and provide a unified evaluation across retail, traffic, and epidemiology datasets.",
    "reason": "Missing comma before the year in a parenthetical citation; should be (Barker et al., 2020).",
    "start": 445,
    "end": 465,
    "label": "Format"
  },
  {
    "span": "Instrumental variable (IV) estimation in the presence of confounding has been extended to nonlinear settings via kernel IV, deep IV, adversarial GMM, and orthogonalization with flexible learners (Newey and Powell, 2003; Hartford et al., 2017; Bennett et al., 2019; Lewis and Syrgkanis, 2020).",
    "document": "Introduction\n\nEstimating causal effects from observational data is challenging due to unobserved confounding. Instrumental variables (IV) provide identification when valid instruments shift treatment independently of potential outcomes. Modern machine learning methods promise to improve estimation in high-dimensional and nonlinear settings, but robustness and diagnostics remain active areas of research.\n\nInstrumental variable (IV) estimation in the presence of confounding has been extended to nonlinear settings via kernel IV, deep IV, adversarial GMM, and orthogonalization with flexible learners (Newey and Powell, 2003; Hartford et al., 2017; Bennett et al., 2019; Lewis and Syrgkanis, 2020). In this paper, we introduce FlowIV, which models structural shocks with normalizing flows to better capture non-Gaussian residual structure.\n\nWe analyze identifiability under misspecification and present empirical results on synthetic and semi-synthetic benchmarks demonstrating improved finite-sample efficiency.\n",
    "reason": "The span summarizes prior IV methods without stating how they fall short or how the proposed method addresses a specific deficiency; the transition to the contribution lacks an explicit gap.",
    "start": 408,
    "end": 700,
    "label": "Lacks_synthesis"
  },
  {
    "span": "According to (Miller, 2020)",
    "document": "Introduction\n\nScaling laws in pretraining suggest predictable gains from increased data and parameters. According to (Miller, 2020), downstream performance saturates unless optimization and data curation are co-designed. Subsequent studies corroborate these findings, showing that token diversity and deduplication are as crucial as model depth (Hartmann et al., 2021; Zhou and Tan, 2022).",
    "reason": "Wrong citation style with leading phrase: should be narrative “According to Miller (2020)” rather than “According to (Miller, 2020)”.",
    "start": 104,
    "end": 131,
    "label": "Format"
  },
  {
    "span": "Hinton et al. (2015) introduced knowledge distillation to transfer soft targets from a teacher to a student network. Pruning removes redundant parameters to compress models (Han et al., 2015). DistilBERT applies distillation to BERT pretraining to create a smaller model (Sanh et al., 2019). Quantization reduces precision to accelerate inference (Jacob et al., 2018).",
    "document": "Related Work\n\nModel Compression for NLP. Deployment constraints motivate compressing large language models while preserving task performance. Techniques include knowledge distillation, pruning, quantization, and low-rank adaptation (Hinton et al., 2015; Han et al., 2015; Jacob et al., 2018; Hu et al., 2022).\n\nDistillation and Alternatives. Hinton et al. (2015) introduced knowledge distillation to transfer soft targets from a teacher to a student network. Pruning removes redundant parameters to compress models (Han et al., 2015). DistilBERT applies distillation to BERT pretraining to create a smaller model (Sanh et al., 2019). Quantization reduces precision to accelerate inference (Jacob et al., 2018). Our method complements prior work by aligning intermediate representations and attention patterns, yielding students that retain calibration under domain shift.",
    "reason": "The paragraph alternates between distillation, pruning, a specific distilled model, and quantization without explaining how these techniques relate or transition, making the connections between the cited works implicit.",
    "start": 342,
    "end": 710,
    "label": "Coherence"
  },
  {
    "span": "Recent works show that pattern-based prompts consistently outperform full finetuning for clinical NER under extreme label sparsity.",
    "document": "Related Work\n\nFew-shot named entity recognition (NER) in clinical narratives has attracted growing attention due to annotation scarcity and privacy constraints. Early approaches adapted metric-learning with prototypical networks to identify unseen entity types from limited labeled examples. Subsequent studies introduced prompt-based inference to reuse knowledge from pretrained language models.\n\nRecent works show that pattern-based prompts consistently outperform full finetuning for clinical NER under extreme label sparsity. Despite gains on several hospital notes corpora, prompt methods often require manually crafted verbalizers and task-specific templates, motivating methods that learn prompts automatically. Another direction explores parameter-efficient finetuning with adapters or prefix-tuning to reduce compute costs and mitigate catastrophic forgetting when switching domains.\n\nActive learning has also been combined with few-shot NER to minimize labeling burden by querying span-level uncertainties. However, existing query strategies frequently over-sample ambiguous tokens, leading to redundant context exposure and slower convergence. In contrast, our approach selects spans by modeling document-level informativeness.",
    "reason": "Claims results of 'recent works' without any citations; niche claim in clinical NER requires evidence (rules a, b, d).",
    "start": 398,
    "end": 529,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most recent speech separation systems rely on Conv-TasNet variants.",
    "document": "Related Work\n\nSingle-channel speech separation has advanced rapidly with time-domain neural architectures that avoid spectrogram processing. Methods differ in encoder design, masking strategies, and permutation-invariant training objectives, with a growing focus on robustness to noise and reverberation.\n\nMost recent speech separation systems rely on Conv-TasNet variants. Alternative designs incorporate dual-path RNNs, attention, and transformer blocks to better capture long-range dependencies, but standard benchmarks and training recipes still emphasize convolutional encoders. Our work proposes a hybrid architecture that fuses convolutional front-ends with lightweight attention, yielding improved separation with reduced latency.",
    "reason": "Claims a trend about 'most recent' systems without providing citations to representative or survey works supporting the statement.",
    "start": 306,
    "end": 373,
    "label": "Unsupported_claim"
  },
  {
    "span": "According to industry surveys, over 70% of deployed dialogue systems rely on intent classification pipelines.",
    "document": "Introduction\nTask-oriented dialogue systems remain the backbone of many customer support and voice assistant applications. Despite progress in end-to-end modeling, production deployments often prioritize controllability and interpretability.\nAccording to industry surveys, over 70% of deployed dialogue systems rely on intent classification pipelines. This prevalence reflects the maturity of slot-filling and form-based dialog management, as well as the ease of auditing deterministic policies.\nWe explore a middle ground between pipelines and end-to-end models by introducing a modular latent policy interface that preserves inspectability while enabling neural credit assignment across modules.",
    "reason": "Presents a quantitative claim based on surveys without citing or linking to any specific survey source.",
    "start": 242,
    "end": 351,
    "label": "Unsupported_claim"
  },
  {
    "span": "Hamilton et al. (2017) introduced GraphSAGE. Ying et al. (2018) proposed differentiable pooling for hierarchical graphs. Veličković et al. (2019) presented Deep Graph Infomax. Xu et al. (2019) analyzed the expressive power of GNNs.",
    "document": "Related Work\n\nGraph Neural Networks\n\nGraph neural networks (GNNs) have become the de facto approach for learning on relational data, with variants that differ in message passing, aggregation, and training objectives. However, comparability across architectures is complicated by differing benchmarks and training heuristics.\n\nHamilton et al. (2017) introduced GraphSAGE. Ying et al. (2018) proposed differentiable pooling for hierarchical graphs. Veličković et al. (2019) presented Deep Graph Infomax. Xu et al. (2019) analyzed the expressive power of GNNs.\n\nSelf-Supervised and Contrastive Objectives\n\nBeyond supervised node or graph classification, contrastive and generative self-supervision have been shown to improve sample efficiency. Yet, prior work often tunes objectives per dataset, obscuring general principles for pretraining on heterogeneous graphs.\n\nOur Contribution\n\nWe provide a task-agnostic pretraining scheme that decouples neighborhood mixing from invariance induction, enabling consistent gains across node- and graph-level tasks.",
    "reason": "The span strings together four papers without transitions or explicit relations, leaving the reader unclear on how these works connect or build on each other.",
    "start": 326,
    "end": 557,
    "label": "Coherence"
  },
  {
    "span": "BERT-base has been used in automated essay scoring trained on TOEFL essays with holistic scores.",
    "document": "Related Work\n\nAutomated essay scoring (AES) systems have evolved from handcrafted features and linear models to neural encoders that learn representations directly from text. BERT-base has been used in automated essay scoring trained on TOEFL essays with holistic scores. Other approaches leverage prompt-aware encoders, hierarchical document models, and domain adaptation for cross-prompt transfer. While neural AES models achieve competitive agreement with human raters, concerns remain about robustness, fairness, and sensitivity to superficial cues. This work examines simple calibration and regularization strategies that improve reliability without increasing computational cost.\n",
    "reason": "Describes a specific prior setup involving BERT-base, the TOEFL essays dataset, and scoring protocol without citing any source.",
    "start": 175,
    "end": 271,
    "label": "Unsupported_claim"
  },
  {
    "span": "Brown et al. 2017)",
    "document": "Related Work\n\nImage captioning models range from template-based systems (Kuznetsova et al., 2014) to neural encoder-decoder architectures (Vinyals et al., 2015; Anderson et al., 2018). Reinforcement learning objectives further improve sequence-level metrics (Rennie et al., 2017; Paulus et al., 2018). Advances in visual grounding have highlighted the importance of region features and attention; Brown et al. 2017) showed that better object detectors yield stronger captions across datasets. More recent work explores end-to-end vision-language pretraining (Chen et al., 2020; Li et al., 2020).\n\nOur approach integrates grounding signals into the decoder with structured constraints for attribute fidelity.",
    "reason": "Missing comma before the year in a parenthetical-style citation fragment; should be 'Brown et al., 2017)'.",
    "start": 397,
    "end": 415,
    "label": "Format"
  },
  {
    "span": "Hyndman and Athanasopoulos (2018) surveyed classical forecasting approaches. Salinas et al. (2020) applied DeepAR for probabilistic forecasting. Oreshkin et al. (2020) introduced N-BEATS for univariate series.",
    "document": "Introduction\n\nForecasting time series is central to operations, finance, and energy, where accurate interval estimates and robust handling of regime shifts are critical. Traditional models emphasize interpretability and stationarity assumptions, while recent deep learning methods target long-horizon accuracy with minimal feature engineering.\n\nModel families\n\nHyndman and Athanasopoulos (2018) surveyed classical forecasting approaches. Salinas et al. (2020) applied DeepAR for probabilistic forecasting. Oreshkin et al. (2020) introduced N-BEATS for univariate series. Lim et al. (2021) proposed Temporal Fusion Transformers for multivariate data.\n\nUncertainty and evaluation\n\nProper scoring rules and calibration diagnostics have become standard to compare probabilistic forecasts. However, distribution shift and cold-start entities still degrade accuracy.\n\nWe propose an adaptive prior regularization scheme that couples hierarchical pooling with calibrated quantile losses, improving both sharpness and coverage under entity sparsity and nonstationary conditions.",
    "reason": "The sentences enumerate different works without transitions or explanations clarifying their relationships, resulting in an abrupt sequence. This violates (a) and (b) and the issue spans multiple sentences per (c).",
    "start": 361,
    "end": 570,
    "label": "Coherence"
  },
  {
    "span": "Screen reader optimizations reduce cognitive load (Pietrzak et al., 2019). Gesture-based input increases engagement (Wobbrock et al., 2007). Eye-tracking supports attention-aware interfaces (Jacob and Karn, 2003). Haptic feedback improves spatial awareness (Loomis et al., 1998).",
    "document": "Related Work\n\nDesigning accessible interfaces requires accommodating diverse sensory, motor, and cognitive needs. Researchers have explored multimodal interaction, adaptive presentation, and assistive technologies that enhance discoverability and reduce effort. Our study focuses on lightweight, context-aware adaptations for low-vision users.\n\nScreen reader optimizations reduce cognitive load (Pietrzak et al., 2019). Gesture-based input increases engagement (Wobbrock et al., 2007). Eye-tracking supports attention-aware interfaces (Jacob and Karn, 2003). Haptic feedback improves spatial awareness (Loomis et al., 1998).\n\nWhile these directions are promising, integrating them into a cohesive, low-latency toolkit remains underexplored. We propose an adaptive layer that fuses gaze, touch, and semantic annotations to guide users during complex navigation tasks.",
    "reason": "Multiple modalities are mentioned in sequence without transitions or explanation of how they interrelate; the relationships among the works are implied but not explicit.",
    "start": 345,
    "end": 624,
    "label": "Coherence"
  },
  {
    "span": "BERT was used in an AES task trained on essays from the ASAP dataset.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) has advanced from handcrafted feature engineering to neural architectures that jointly model content, coherence, and style. Transformer-based encoders have recently become standard, benefiting from large-scale pretraining and improved optimization stability. BERT was used in an AES task trained on essays from the ASAP dataset. Subsequent extensions incorporate prompt conditioning and error-sensitive objectives to better capture rubric-specific criteria. Nevertheless, concerns remain about domain shift, prompt leakage, and fairness across demographic groups. Our study examines calibration and prompt invariance under limited supervision and proposes a data partitioning protocol to mitigate leakage.",
    "reason": "References a specific model and dataset setup from prior work without a supporting citation.",
    "start": 303,
    "end": 372,
    "label": "Unsupported_claim"
  },
  {
    "span": " [Levy et al., 2017, Elsahar et al., 2018",
    "document": "Related Work\n\nThe KILT benchmark and public leaderboard combines eleven datasets across five tasks. The main advantage of the KILT distribution of these datasets is that the provenance information from each dataset is realigned to reference the same snapshot of Wikipedia. A unified evaluation script and set of metrics is also provided. In this work, we focus on four tasks, such as Slot Filling [Levy et al., 2017, Elsahar et al., 2018, Question Answering [Kwiatkowski et al., 2019, Joshi et al., 2017, Fact Checking [Thorne et al., 2018a,c], and Dialog [Dinan et al., 2019] (see Figure 1). A set of baseline methods have been proposed for KILT. GENRE [Cao et al., 2021] is trained on BLINK  and all KILT tasks jointly using a sequence-to-sequence language model to generate the title of the Wikipedia page where the answer can be found. This method is a strong baseline to evaluate the retrieval performance, but it does not address the downstream tasks. On the other hand, generative models, such as BART [Lewis et al., 2020a] and T5 [Raffel et al., 2020], show interesting performance when finetuned on the downstream tasks relying only on the implicit knowledge stored in the weights of the neural networks, without the use of any explicit retrieval component.\n\nRAG [Lewis et al., 2020b], an end-to-end retrieval-based generative model, is the best performing baseline in KILT and it incorporates DPR [Karpukhin et al., 2020] to first retrieve relevant passages for the query, then it uses a model initialized from BART [Lewis et al., 2020a] to perform a sequence-to-sequence generation from each evidence passage concatenated with the query in order to generate the answer. Figure 2 shows the architecture of RAG.\n\nMulti-task DPR  exploits multi-task learning by training both DPR passage and query encoder on all KILT tasks. DensePhrases [Lee et al., 2021] addresses the knowledge intensive tasks with a short answer, such as slot filling. It indexes the phrases in the corpus that can be potential answers. The extracted phrases are represented by their start and end token vectors from the final layer of a transformer initialized from SpanBERT [Joshi et al., 2020].\n\nKnowledge Graph Induction (KGI) [Glass et al., 2021] combines DPR and RAG models, both trained with task and dataset specific training. KGI employs a two phase training procedure: first training the DPR model, i.e. both the query and context encoder, using the KILT provenance ground truth. Then, KGI trains the sequence-to-sequence generation and further trains the query encoder using only the target output as the objective. This results in large improvements in retrieval performance and, as a consequence, in the downstream tasks.\n\nMulti-stage or cascade approaches to retrieval have received ample attention in Information Retrieval (IR) research. The multi-stage approach begins with the initial retrieval phase, where an initial set of documents or passages form the pool of candidates to be considered for ranking. Then one or more phases of increasingly computationally demanding rerankers are applied. Early approaches in learning to rank [Liu, 2009] used features and linear classifiers. Pre-trained language models, especially BERT , have shown state-ofthe-art performance when applied to the task of relevance ranking. Transformers may be applied as classifiers to each query and passage pair independently [Nogueira and Cho, 2019] or as generators to produce labels for passages in a sequence-tosequence model [Nogueira et al., 2020].\n\n ",
    "start": 396,
    "end": 437,
    "label": "Format"
  },
  {
    "span": "the MultiRC benchmark",
    "document": "Introduction\n\nMulti-hop reading comprehension requires reasoning across multiple sentences or documents to produce an answer. Datasets such as HotpotQA (Yang et al., 2018) and the MultiRC benchmark require models to integrate evidence across sentences and justify predictions. While recent transformer architectures have improved end-task scores, they often rely on shallow lexical overlap rather than true reasoning.",
    "reason": "Mentions a specific benchmark dataset without a citation to its original paper.",
    "start": 176,
    "end": 197,
    "label": "Unsupported_claim"
  },
  {
    "span": "In earlier work, it was demonstrated that neighbor sampling improves both accuracy and scalability.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become standard tools for learning on relational data, propagating information along edges to produce node and graph representations (Kipf and Welling, 2017; Velickovic et al., 2018). In large-scale recommender systems, training GNNs presents unique challenges due to high-degree nodes, dynamic graphs, and strict latency constraints.\n\nIn earlier work, it was demonstrated that neighbor sampling improves both accuracy and scalability. Subsequent research has introduced layer-wise sampling and importance-based selection to reduce variance in message passing while keeping computational costs manageable. However, the trade-offs between sampling bias, coverage of informative neighbors, and end-to-end system throughput are not yet fully understood.\n\nWe revisit these trade-offs under a unified evaluation that controls for cache policies and negative sampling schemes. Our findings suggest that coupling adaptive neighbor selection with request-level caching yields improvements in both recommendation quality and inference latency on industrial-scale graphs.",
    "reason": "The claim about prior demonstrations lacks a citation to the specific study or studies that established the result.",
    "start": 384,
    "end": 483,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from ASAP and TOEFL11.",
    "document": "Related Work\n\nAutomated Essay Scoring (AES) research spans from regression over handcrafted features to deep neural architectures leveraging contextualized embeddings. Cross-prompt generalization and fairness across demographic subgroups remain ongoing challenges. BERT was used in an AES task trained on essays from ASAP and TOEFL11. More recently, contrastive and multi-task objectives have been proposed to stabilize training and improve robustness to domain shift, but reproducibility across prompts and genres is still underexplored.",
    "reason": "Specific claim about a model setup referencing particular datasets is made without citing the corresponding works (rule a; example iii).",
    "start": 265,
    "end": 334,
    "label": "Unsupported_claim"
  },
  {
    "span": "the CoNLL 2003 shared task",
    "document": "Introduction\n\nNamed entity recognition (NER) is a foundational problem in information extraction, with applications in relation extraction, question answering, and knowledge base population. Modern NER systems are typically built either as sequence labeling models or span classification models, often leveraging pretrained language models for strong contextual signals. Benchmark datasets have played a crucial role in standardizing evaluation and advancing methods. Among these, the CoNLL 2003 shared task is commonly used to measure progress on English NER, while OntoNotes-based corpora support broader, multi-genre evaluations. Despite substantial improvements, challenges such as out-of-domain robustness and label consistency remain open issues. In this work, we revisit span-level learning objectives and propose a calibration strategy that improves precision without sacrificing recall on challenging spans.\n",
    "reason": "First mention of a shared task requires a citation; the phrase refers to a specific prior benchmark but provides no source.",
    "start": 481,
    "end": 507,
    "label": "Unsupported_claim"
  },
  {
    "span": "Hartig et al.",
    "document": "Introduction\n\nInteractive learning reduces annotation costs by iteratively selecting informative examples for labeling. Classical heuristics such as uncertainty sampling (Lewis, 1995) and query-by-committee (Seung et al., 1992) have been extensively studied in text classification (Zhang and Wang, 2018) and sequence tagging (Miller, 2018). Building on Hartig et al., we propose a curriculum that gradually increases task difficulty while maintaining calibration. Prior domain adaptation techniques (Peters and Sun, 2020) typically assume abundant unlabeled data, whereas our method works under strict budget constraints. We also draw inspiration from contextual bandits for feedback-efficient exploration (Li et al., 2019).",
    "reason": "Narrative citation missing year: should be 'Hartig et al. (YEAR)' in narrative form.",
    "start": 353,
    "end": 366,
    "label": "Format"
  },
  {
    "span": "Prior recommender research has explored contextual bandits, deep Q-learning, policy gradients, and offline reinforcement learning for slate and session recommendations (Zhou et al., 2020; Chen et al., 2019; Ie et al., 2019; Bai et al., 2021). We propose a slate-aware actor-critic with implicit negative feedback modeling.",
    "document": "Introduction\n\nReinforcement learning (RL) provides a principled framework for optimizing long-term user utility in recommender systems, complementing myopic supervised learning objectives. Prior recommender research has explored contextual bandits, deep Q-learning, policy gradients, and offline reinforcement learning for slate and session recommendations (Zhou et al., 2020; Chen et al., 2019; Ie et al., 2019; Bai et al., 2021). We propose a slate-aware actor-critic with implicit negative feedback modeling.\n\nWe evaluate our method on public logs and a production A/B test, assessing click-through, dwell time, and session-level satisfaction proxies.",
    "reason": "The span jumps from a survey of prior RL approaches directly to the proposed method without clarifying what deficiency in earlier work motivates the new slate-aware actor-critic.",
    "start": 189,
    "end": 511,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Xu et al. (2018) proposed Donut, a VAE-based approach to anomaly detection. Liu et al. (2008) introduced Isolation Forest for outlier detection. Malhotra et al. (2015) explored LSTM-based sequence modeling for anomalies.",
    "document": "Related Work\n\nAnomaly detection in time series applies probabilistic modeling, reconstruction, and forecasting to identify rare deviations. Both classical and deep approaches have been studied under supervised and unsupervised settings.\n\nXu et al. (2018) proposed Donut, a VAE-based approach to anomaly detection. Liu et al. (2008) introduced Isolation Forest for outlier detection. Malhotra et al. (2015) explored LSTM-based sequence modeling for anomalies. Forecasting-based detectors compare predicted and observed residuals for detection (Salinas et al., 2020; Wu et al., 2021).\n\nReal-world deployments must address concept drift, latency constraints, and alert fatigue through adaptation and prioritization (Siffer et al., 2017; Laptev et al., 2015).",
    "reason": "The span juxtaposes a deep generative method, a classic tree-based outlier detector, and an RNN approach without transitions or an explanation of how these techniques relate within time series anomaly detection.",
    "start": 238,
    "end": 458,
    "label": "Coherence"
  },
  {
    "span": "[Wang and Li, 14]",
    "document": "Related Work\n\nSelf-supervised learning (SSL) has narrowed the gap with supervised pretraining by exploiting pretext tasks and contrastive objectives. Contrastive frameworks such as SimCLR and MoCo learn invariances over augmented views (Chen et al., 2020; He et al., 2020). Beyond instance discrimination, clustering-based methods capture semantic structure (Caron et al., 2018; Asano et al., 2020). Recent works highlight the role of strong augmentations and longer schedules [Wang and Li, 14]. Our approach introduces a lightweight predictor that improves transfer under small-batch constraints.",
    "reason": "Improper hybrid of author–year and numeric styles; should be either '(Wang and Li, 2014)' or '[14]'.",
    "start": 477,
    "end": 494,
    "label": "Format"
  },
  {
    "span": "BERT was used in an AES task trained on essays from ASAP.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and linear models to neural encoders that capture discourse and coherence. Recent approaches incorporate prompt conditioning and pairwise ranking losses to improve generalization to new prompts.\n\nBERT was used in an AES task trained on essays from ASAP. Other studies explore domain adaptation to handle prompt drift and lexical memorization, but robustness to adversarial paraphrases remains under-explored.\n\nOur work introduces a contrastive objective that aligns representations across prompts while regularizing for length and lexical overlap, aiming to improve transfer without sacrificing within-prompt accuracy.",
    "reason": "Mentions a specific model-dataset setup at first mention of the dataset without any citation (violates a and e).",
    "start": 278,
    "end": 335,
    "label": "Unsupported_claim"
  },
  {
    "span": "In a previous study, the authors show that adding pitch features reduces WER by 15% on Switchboard.",
    "document": "Background and Prior Work\n\nEnd-to-end automatic speech recognition (ASR) has moved from hybrid HMM-DNN systems to RNN-Transducers and attention-based encoder-decoders, with gains largely attributed to better acoustic modeling and large-scale pretraining. Auxiliary information such as speaker embeddings and articulatory features has been explored to improve robustness to noise and speaker variability.\n\nIn a previous study, the authors show that adding pitch features reduces WER by 15% on Switchboard. While prosodic cues are plausible contributors to recognition accuracy, reported improvements are highly dependent on front-end configuration and language model fusion. Our work revisits prosodic augmentation under controlled training budgets and standardized decoding, and we analyze its interaction with noise robustness and speaker adaptation.",
    "reason": "Refers to an unspecified 'previous study' with a precise performance improvement but provides no citation, violating rule (a).",
    "start": 405,
    "end": 504,
    "label": "Unsupported_claim"
  },
  {
    "span": "Clinical BERT has been used in an ADE extraction task trained on discharge summaries.",
    "document": "Related Work\n\nExtracting adverse drug events (ADEs) from clinical narratives is a key step for pharmacovigilance. Transformer-based encoders adapted to biomedical text have improved performance on named entity recognition and relation extraction under limited supervision. Clinical BERT has been used in an ADE extraction task trained on discharge summaries. Beyond encoder choice, prior work explores span-level classification, joint modeling of entities and relations, and distant supervision from structured resources.\n\nDespite promising gains, domain shift across institutions and note types remains a significant obstacle. We therefore study robust training strategies that leverage weak labels and cross-document context to improve generalization while minimizing annotation burden.",
    "reason": "Describes a specific setup of a task using a particular model and dataset type without citing the work (rule a and e).",
    "start": 273,
    "end": 358,
    "label": "Unsupported_claim"
  },
  {
    "span": "Kim et al.",
    "document": "Introduction\n\nNeural sequence models have accelerated progress in text generation, but their reliability under domain shift remains contested. Early encoder-decoder architectures demonstrated strong fluency yet struggled with factual faithfulness (Rodriguez et al., 2020). As noted by Kim et al., pretraining on large corpora can mitigate data scarcity but introduces new calibration challenges that must be addressed to ensure robust deployment. Subsequent work proposed task-adaptive pretraining (Lee and Park, 2021) and retrieval-augmented decoding (Martins et al., 2022) to improve generalization. In this paper, we study calibration-aware fine-tuning techniques and evaluate their impact on out-of-domain robustness across summarization and dialogue tasks.",
    "reason": "Narrative citation missing year; should be formatted as a narrative author–year citation, e.g., \"Kim et al. (2019)\".",
    "start": 285,
    "end": 295,
    "label": "Format"
  },
  {
    "span": "Model-based RL uses learned dynamics for efficient planning (Deisenroth and Rasmussen, 2011). Proximal Policy Optimization stabilizes on-policy updates (Schulman et al., 2017). Domain randomization changes textures to improve transfer (Tobin et al., 2017). Formal methods verify safety properties (Alshiekh et al., 2018).",
    "document": "Related Work\n\nReinforcement Learning for Robotic Control\n\nData efficiency, transfer, and safety are central to deploying RL on real robots. Model-based methods, robust policy optimization, and sim-to-real techniques have each contributed partial solutions, but combining them remains challenging.\n\nTechniques and Guarantees\n\nModel-based RL uses learned dynamics for efficient planning (Deisenroth and Rasmussen, 2011). Proximal Policy Optimization stabilizes on-policy updates (Schulman et al., 2017). Domain randomization changes textures to improve transfer (Tobin et al., 2017). Formal methods verify safety properties (Alshiekh et al., 2018).\n\nOur Approach\n\nWe propose a constrained model-based framework with risk-sensitive objectives and curriculum randomization, showing improved success rates with safety guarantees on manipulation tasks.",
    "reason": "The span lists disparate strands of RL—model-based learning, PPO, sim-to-real domain randomization, and formal verification—without transitions or explicit links. The relationships are not articulated, producing abrupt shifts between topics and reducing coherence.",
    "start": 325,
    "end": 646,
    "label": "Coherence"
  },
  {
    "span": "(Garcia Et Al., 2022)",
    "document": "Introduction\n\nImitation learning enables robots to acquire complex skills from demonstrations without explicitly modeling reward functions (Navarro and Chen, 2020; Qi and Mercer, 2021). Behavior cloning scales but suffers from covariate shift, while inverse reinforcement learning improves generalization at higher computational cost (Patel and Wong, 2019).\n\nRecent advances leverage latent intention spaces and temporal abstraction to capture multi-stage tasks (Garcia Et Al., 2022; Kumar and Estes, 2023). Our approach incorporates safety constraints via control barrier functions learned jointly with policy encoders, narrowing the sim-to-real gap.\n",
    "reason": "Incorrect capitalization in 'et al.'; should be lowercase 'et al.' as in '(Garcia et al., 2022)'.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "In the CoNLL 2014 shared task",
    "document": "Introduction\n\nGrammatical error correction (GEC) seeks to automatically correct a wide variety of errors in learner writing, including spelling, morphology, and syntax. In the CoNLL 2014 shared task, grammatical error correction was formalized as a sequence-to-sequence problem and popularized through standardized evaluation settings. Subsequent research introduced neural encoder–decoder models and pretraining strategies that substantially improved performance over phrase-based baselines.",
    "reason": "Refers to a specific shared task without citing the official task description or proceedings.",
    "start": 169,
    "end": 198,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Gonzalez et al., 2018)",
    "document": "Related Work\n\nGraph neural networks (GNNs) generalize convolutional operations to irregular structures and have achieved strong performance on node and graph classification (Kipf and Welling, 2017; Hamilton et al., 2017). Attention mechanisms allow nodes to weigh their neighbors and improve aggregation quality (Veličković et al., 2018). In (Gonzalez et al., 2018) an attention-over-edges scheme was introduced for dynamic graphs, while subsequent work studied disentangling structural and feature attention (Lee et al., 2020). We build on these ideas by decoupling temporal smoothing from topological message passing.",
    "reason": "Wrong citation style with preposition inside the parentheses; should be narrative style \"In Gonzalez et al. (2018)\" rather than placing the preposition before a parenthetical citation.",
    "start": 339,
    "end": 365,
    "label": "Format"
  },
  {
    "span": "In the Librispeech benchmark, teacher-student distillation has become the de facto approach for small-footprint models.",
    "document": "Related Work in Acoustic Model Compression\n\nCompressing automatic speech recognition (ASR) models for on-device deployment requires balancing accuracy, latency, and memory. Techniques include pruning, quantization, low-rank factorization, and knowledge distillation. In the Librispeech benchmark, teacher-student distillation has become the de facto approach for small-footprint models. Complementary strategies such as layer-wise distillation, sequence-level criteria, and logit smoothing have been proposed to stabilize training and preserve word error rate under aggressive compression.\n\nOur method unifies sequence-level KD with streaming constraints through chunk-wise alignment and monotonic attention, achieving lower real-time factor without specialized hardware. We benchmark against competitive baselines and analyze trade-offs between beam size, quantization precision, and endpointer latency.",
    "reason": "Makes a field-wide claim about a specific benchmark and methodological dominance without citing relevant papers or surveys.",
    "start": 267,
    "end": 386,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Nguyen et al., 2018; Park and Lee, 2020;)",
    "document": "Introduction\n\nDomain adaptation for text classification has evolved rapidly with the advent of pre-trained language models. Early approaches emphasized instance reweighting and feature augmentation to mitigate distribution shift (Ben-David et al., 2010; Pan and Yang, 2010). More recent work leverages adversarial training and contrastive objectives to learn domain-invariant representations (Ganin et al., 2016; Ruder, 2019; Wilson and Cook, 2020). However, aligning only global statistics can overlook salient local cues that drive task performance. Prior efforts also highlight the role of label distributions and class-conditional alignment (Tzeng et al., 2017; Zhang et al., 2019).\n\nIn the context of sentiment analysis, adaptation faces additional challenges stemming from lexical ambiguity and topic shifts (Johnson and Zhang, 2015; Glorot et al., 2011). Several studies propose multi-view representations and pseudo-labeling to stabilize training across domains (Saito et al., 2018; Chen et al., 2020). While these techniques improve robustness, they may still collapse on fine-grained categories under severe domain drift (Ruder, 2019; Xu et al., 2020). Recent meta-learning approaches attempt to anticipate future domains by simulating shifts during training (Finn et al., 2017; Dou et al., 2019).\n\nDespite this progress, consistent handling of class imbalance and calibrating decision boundaries remain open problems. Prior work reports conflicting conclusions on the benefits of instance-wise vs. prototype-based objectives (Snell et al., 2017; Hou et al., 2020). Within this line of research, cross-domain contrastive learning has emerged as a promising direction (Khosla et al., 2020; Saunshi et al., 2019; Nguyen et al., 2018; Park and Lee, 2020;).",
    "reason": "Extraneous semicolon before the closing parenthesis in a parenthetical citation list; incorrect citation punctuation/format.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "(Singh et al. 2018)",
    "document": "Introduction\n\nTopic modeling discovers latent structure in document collections. Probabilistic approaches such as LDA capture co-occurrence patterns via Dirichlet–multinomial assumptions (Blei et al., 2003). Neural variants integrate amortized inference and deep priors to improve scalability and flexibility (Miao et al., 2016). Evaluation commonly reports perplexity and topic coherence, though their correlation with human interpretability is imperfect (Röder et al., 2015). Prior work has also explored dynamic topics that evolve over time (Blei and Lafferty, 2006).\n\nWe focus on controllable topic models that incorporate lexical constraints for downstream retrieval. Compared to classical anchoring methods (Arora et al., 2013), our approach leverages contrastive objectives for sharper topics. We follow standard preprocessing and coherence estimation (Blei et al., 2003; Röder et al., 2015), and we compare against guided LDA (Singh et al. 2018) under identical vocabularies.",
    "reason": "Missing comma between author and year in a parenthetical-style citation per the paper’s author–year convention; it should be '(Singh et al., 2018)'.",
    "start": 934,
    "end": 953,
    "label": "Format"
  },
  {
    "span": "(Johnson et al., 2018; Kumar and Lee, 2020;",
    "document": "Related Work\n\nResearch on graph-based summarization has evolved from purely extractive methods to hybrid neural approaches that integrate structural cues with contextualized representations. Prior surveys summarize these trends (Johnson et al., 2018; Kumar and Lee, 2020; and motivate hybrid models that exploit pre-trained encoders (Devlin et al., 2019) and graph matching techniques (Yao et al., 2020). However, end-to-end architectures still struggle with domain shift and length constraints, prompting interest in memory-augmented models (Graves et al., 2016) and retrieval-augmented generation (Lewis et al., 2020). In parallel, evaluation has moved beyond ROUGE to include factual consistency metrics (Kryscinski et al., 2020; Goyal and Durrett, 2021).",
    "reason": "Missing closing parenthesis in a citation group; the group ends with a semicolon and no closing ')'.",
    "start": 228,
    "end": 271,
    "label": "Format"
  },
  {
    "span": "Personalization in federated learning has been approached through fine-tuning, meta-learning, and client clustering (Smith et al., 2017; Fallah et al., 2020; Sattler et al., 2020). Fine-tuning adapts a global model locally with a few gradient steps; meta-learning seeks initializations that quickly specialize; clustering groups clients by similarity to train multiple specialists. Proximal and regularized objectives such as FedProx, MOON, and Ditto address statistical heterogeneity and stability during optimization (Li et al., 2020; Li et al., 2021; Li et al., 2021b).",
    "document": "Introduction\n\nFederated learning (FL) enables training machine learning models across distributed clients without centralizing raw data. However, client data are often non-identically distributed, making a single global model suboptimal for downstream user experiences.\n\nPersonalization in federated learning has been approached through fine-tuning, meta-learning, and client clustering (Smith et al., 2017; Fallah et al., 2020; Sattler et al., 2020). Fine-tuning adapts a global model locally with a few gradient steps; meta-learning seeks initializations that quickly specialize; clustering groups clients by similarity to train multiple specialists. Proximal and regularized objectives such as FedProx, MOON, and Ditto address statistical heterogeneity and stability during optimization (Li et al., 2020; Li et al., 2021; Li et al., 2021b).\n\nSeveral works also explore personalization layers, mixture-of-experts, and representation learning to separate common from client-specific features (Arivazhagan et al., 2019; Collins et al., 2021). Communication-efficient strategies combine local adaptation with periodic knowledge sharing to reduce bandwidth (Kairouz et al., 2021).\n\nWe study personalization under realistic participation patterns and device constraints. Our method targets rapid on-device adaptation with bounded compute budgets.",
    "reason": "The span enumerates prior personalization approaches without explaining how they relate to or motivate the current study, and it does not articulate a specific gap (criteria a and c).",
    "start": 271,
    "end": 843,
    "label": "Lacks_synthesis"
  },
  {
    "span": "A large body of work proposes personalized federated learning via meta-learning, multi-task optimization, or mixture models (Fallah et al., 2020; Arivazhagan et al., 2019; Dinh et al., 2020; Li et al., 2021).",
    "document": "Related Work\n\nFederated learning (FL) enables on-device training without centralizing raw data, but heterogeneity across clients poses severe optimization and generalization challenges. Research has therefore focused on both algorithms for robust aggregation and methods for personalization to better serve diverse clients.\n\nA large body of work proposes personalized federated learning via meta-learning, multi-task optimization, or mixture models (Fallah et al., 2020; Arivazhagan et al., 2019; Dinh et al., 2020; Li et al., 2021). Other studies address client drift and partial participation by adjusting local update rules or regularizing towards global anchors (Li et al., 2020; Karimireddy et al., 2020). Communication efficiency has also been improved through gradient compression, sparsification, and adaptive scheduling (Konečný et al., 2016; Sattler et al., 2019).\n\nBeyond supervised learning, researchers have extended FL to semi-supervised and self-supervised regimes, leveraging unlabeled client data via consistency regularization and contrastive objectives (Jeong et al., 2020; Zhang et al., 2020). Privacy-preserving mechanisms such as secure aggregation and differential privacy have been integrated to strengthen guarantees under realistic threat models (Bonawitz et al., 2017; Geyer et al., 2017).\n\nThese developments demonstrate a broad toolkit for handling heterogeneity, efficiency, and privacy in FL ecosystems.",
    "reason": "The span merely enumerates prior personalization strategies with citations and provides no explanation of how they inform or contrast with the current work, nor does it highlight a specific gap or motivation.",
    "start": 325,
    "end": 533,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Chebyshev filters approximate spectral convolutions (Defferrard et al., 2016). Attention-based spatiotemporal modules capture dynamic dependencies (Zheng et al., 2020; Wu et al., 2019).",
    "document": "Related Work\n\nTraffic forecasting requires modeling both spatial correlations among sensors and temporal patterns across horizons. Early statistical baselines emphasized autoregression and Kalman filtering, but deep models now dominate benchmarks due to their capacity to capture nonlinear dependencies.\n\nGraph convolutional networks (GCNs) generalize convolution to irregular sensor graphs to aggregate neighborhood signals (Kipf and Welling, 2017; Li et al., 2018). Diffusion-based operators account for directed road flows and anisotropic propagation (Li et al., 2018; Pan et al., 2021). Chebyshev filters approximate spectral convolutions (Defferrard et al., 2016). Attention-based spatiotemporal modules capture dynamic dependencies (Zheng et al., 2020; Wu et al., 2019). Hybrid sequence models combine graph layers with recurrent networks or transformers to balance locality and long-range effects (Yu et al., 2018; Cai et al., 2020).\n\nOur approach departs from fixed adjacency by learning context-aware edges conditioned on exogenous events (weather, incidents), enabling adaptive receptive fields during peak transitions.",
    "reason": "The span lists two disjoint technique families back-to-back with no transition or explanation of how spectral filters relate to attention modules or to the preceding diffusion operators, creating an abrupt shift.",
    "start": 591,
    "end": 776,
    "label": "Coherence"
  },
  {
    "span": "(Ahmed and et al., 2021)",
    "document": "Related Work\n\nCross-lingual transfer exploits shared subword vocabularies and multilingual pretraining to enable zero-shot performance in low-resource languages (Conneau and Lample, 2019; Artetxe et al., 2020). Adapter-based fine-tuning reduces parameter overhead while retaining strong transfer capabilities (Houlsby et al., 2019; Pfeiffer et al., 2020). Recent studies propose language-specific adapters with sparse routing to improve scalability (Ansell et al., 2021). Prior evaluations of multitask setups (Ahmed and et al., 2021) report improvements on morphologically rich languages but at the cost of increased latency.",
    "reason": "Incorrect use of 'and' with 'et al.' inside a parenthetical citation; should be \"(Ahmed et al., 2021)\" without 'and'.",
    "start": 510,
    "end": 534,
    "label": "Format"
  },
  {
    "span": "In (Garcia et al., 2017)",
    "document": "Introduction\n\nSequence labeling in biomedical text has seen rapid progress due to domain-adapted pre-training and better decoding strategies. In (Garcia et al., 2017) the authors compare CRF layers with neural span classifiers, showing that span-based models handle discontinuous entities more robustly. Later work by Hassan and Lee (2019) adds character-level CNNs to capture rare terms, while Xu et al. (2020) incorporate ontology constraints at inference time. Despite these advances, generalization to unseen entity types remains a significant challenge.",
    "reason": "Wrong citation style: narrative use should be 'in Garcia et al. (2017)' rather than 'In (Garcia et al., 2017)'.",
    "start": 142,
    "end": 166,
    "label": "Format"
  },
  {
    "span": "(Nguyen et al., 2017",
    "document": "Related Work\n\nNeural information retrieval has evolved from sparse lexical models to dense vector encoders trained with contrastive objectives (Robertson and Zaragoza, 2009; Karpukhin et al., 2020). Early interaction-focused architectures compute fine-grained token-to-token relevance (Guo et al., 2016), while late-interaction models like ColBERT balance efficiency and accuracy through maximal token similarity (Khattab and Zaharia, 2020). To cope with domain mismatch, adapters and continual pretraining have been used to specialize encoders to new corpora (Gururangan et al., 2020; Pfeiffer et al., 2021). For multilingual search, shared subword vocabularies and alignment losses promote cross-lingual generalization (Litschko et al., 2021; Reimers and Gurevych, 2020). Query reformulation has also proven effective at reducing surface-form discrepancies (Ramakrishnan et al., 2010; (Nguyen et al., 2017 present a reinforcement learning approach that optimizes retrieval metrics directly. Our method complements these lines by introducing a multi-stage rewriter that alternates between semantic expansion and precision pruning guided by a retrieval-aware critic.\n",
    "reason": "Missing closing parenthesis in a parenthetical citation; should be '(Nguyen et al., 2017)'.",
    "start": 887,
    "end": 907,
    "label": "Format"
  },
  {
    "span": "There are over 7,000 languages, many of which lack transcribed speech datasets.",
    "document": "Introduction\n\nBuilding multilingual ASR that serves the long tail of the world’s languages requires strategies that scale with limited supervision. While self-supervised pretraining and parameter sharing have closed gaps for several medium-resource languages, extremely low-resource settings remain challenging due to scarce annotations and orthographic variation. There are over 7,000 languages, many of which lack transcribed speech datasets. This scarcity motivates leveraging cross-lingual phonetic transfer and weakly supervised signals such as YouTube captions and translation alignments. We propose a curriculum that combines multilingual CTC pretraining with pronunciation modeling to improve recognition for languages with fewer than 10 hours of labeled audio.",
    "reason": "Presents a global statistic and availability claim that typically require citation (e.g., Ethnologue, surveys) but none is provided.",
    "start": 365,
    "end": 444,
    "label": "Unsupported_claim"
  },
  {
    "span": "As is widely known, naive exploration strategies caused multiple real-world accidents in warehouse robots.",
    "document": "Introduction\n\nReinforcement learning (RL) in robotics promises adaptive policies that optimize long-horizon control objectives under uncertainty. However, the exploration–exploitation trade-off makes online learning risky in safety-critical environments. Research on safe RL seeks to constrain policy updates, incorporate safety critics, and leverage model-based predictions to avoid catastrophic failures during learning.\n\nDespite progress in simulation benchmarks, deploying exploration-heavy algorithms on physical systems remains challenging. As is widely known, naive exploration strategies caused multiple real-world accidents in warehouse robots. This underscores the need for algorithms that provably respect safety constraints while retaining sufficient exploration to discover effective behaviors.\n\nWe propose Shielded Constrained Policy Optimization (SCPO), which integrates a learned safety filter with constraint-regularized policy updates and employs reachability analysis to preemptively block unsafe actions. We validate SCPO on simulated mobile manipulation tasks and on a small-scale warehouse testbed, reporting improved task success while satisfying safety budgets.",
    "reason": "Asserts real-world accidents due to prior strategies without any citation or evidence, which is required for such claims (b).",
    "start": 547,
    "end": 653,
    "label": "Unsupported_claim"
  },
  {
    "span": "Zhang et al. (2020) collected a large-scale cross-lingual summarization dataset. Ladhak et al. (2020) explored pivot-based translation for cross-lingual summarization. Lewis et al. (2020) introduced mBART for multilingual sequence-to-sequence generation.",
    "document": "Related Work\n\nCross-lingual summarization (CLS) seeks to produce a summary in a target language given a source-document in a different language. Prior work spans data collection, pipeline methods using MT, and direct multilingual generation.\n\nZhang et al. (2020) collected a large-scale cross-lingual summarization dataset. Ladhak et al. (2020) explored pivot-based translation for cross-lingual summarization. Lewis et al. (2020) introduced mBART for multilingual sequence-to-sequence generation. Recent efforts fine-tune pretrained encoders and decoders with task-specific objectives (Chi et al., 2021; Tang et al., 2020).\n\nEvaluation for CLS commonly adapts ROUGE to multiple languages and leverages human judgments for adequacy and fluency (Lin, 2004; Fabbri et al., 2021).",
    "reason": "Three sentences enumerate a dataset, a pipeline approach, and a pretrained model with no explicit connections or transitions showing how they relate or build on each other.",
    "start": 243,
    "end": 497,
    "label": "Coherence"
  },
  {
    "span": "In prior deployments across hospitals, communication rounds rarely exceed 50 due to network constraints.",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative training without centralizing sensitive data, making it attractive for healthcare applications (McMahan et al., 2017). Algorithmic variants such as FedProx and FedNova address system heterogeneity and client drift common in clinical settings (Li et al., 2020; Wang et al., 2020). Privacy mechanisms, including secure aggregation and differential privacy, mitigate leakage risks (Bonawitz et al., 2017; Geyer et al., 2017).\n\nDeployment constraints strongly influence algorithm design, from client availability to bandwidth. In prior deployments across hospitals, communication rounds rarely exceed 50 due to network constraints. This motivates methods that converge under tight communication budgets and intermittent participation.\n\nWe present a momentum-adaptive aggregation that accelerates convergence while preserving on-device compute limits.",
    "reason": "Asserts empirical behavior in prior real-world deployments with a specific quantitative threshold but provides no evidence or citations (rule b).",
    "start": 583,
    "end": 687,
    "label": "Unsupported_claim"
  },
  {
    "span": "Nguyen et al.",
    "document": "Related Work\n\nResearch on biomedical named entity recognition (BioNER) has progressed rapidly with the advent of contextual encoders and distant supervision. Early sequence labeling methods relied on CRFs and dictionary augmentation (Rahman and Li, 2020), but were quickly surpassed by neural models that exploit pretraining (Park and Kim, 2019). Nguyen et al. proposed a weakly supervised tagging framework that combines lexicon matches with span-level confidence filtering, which we adapt to the clinical setting in this work. In contrast, self-training approaches iteratively expand silver labels using teacher-student loops (Ghosh and Patel, 2021), while domain-adaptive pretraining narrows the gap between general and clinical text (Johnson et al., 2019). More recent efforts emphasize calibration and selective prediction to manage annotation uncertainty (Sato and Rivera, 2022). Complementary to these lines, we focus on cross-institution generalization by introducing structure-aware regularizers that reduce spurious correlations (Olsen and Zhang, 2023).",
    "reason": "Narrative citation missing year; should be 'Nguyen et al. (YEAR)' when used in the sentence.",
    "start": 347,
    "end": 360,
    "label": "Format"
  },
  {
    "span": "According to (Chen et al., 2021)",
    "document": "Related Work\n\nKnowledge graph completion leverages structural patterns and textual context to infer missing facts (Bordes et al., 2013; Toutanova et al., 2015). Recent pretraining integrates entity and relation embeddings with language models for improved reasoning (Yao et al., 2019; Sun et al., 2019).\n\nAccording to (Chen et al., 2021) hybrid models that co-train link prediction with mention-level grounding outperform structure-only baselines, but they are sensitive to noisy surface forms. Our approach reduces this sensitivity by aligning entity spans with relation-specific prompts during pretraining.\n\nWe evaluate on standard benchmarks and report gains in both filtered MRR and Hits@K over strong baselines (Dettmers et al., 2018; Sun et al., 2020).",
    "reason": "Wrong citation style: prepositional phrase followed by a parenthetical citation. Should be narrative 'According to Chen et al. (2021)' or remove 'According to' for a parenthetical cite.",
    "start": 305,
    "end": 337,
    "label": "Format"
  },
  {
    "span": "Earlier competitions on fake news detection standardized the use of macro-F1",
    "document": "Related Work\n\nAutomatic fake news detection has been organized around shared tasks and evaluations spanning multiple domains and languages. Earlier competitions on fake news detection standardized the use of macro-F1, encouraging systems to balance performance on minority classes. Subsequent studies extended these benchmarks to stance detection and claim verification with multi-hop reasoning.\n\nOur paper contributes a multilingual benchmark with calibrated probability reporting and assesses robustness under label shift.",
    "reason": "References specific competitions and their impact on metric choice without providing citations to the shared tasks (rule a).",
    "start": 140,
    "end": 216,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior work on exploration leverages intrinsic motivation, such as prediction error bonuses, curiosity-driven features, and count-based generalization (Bellemare et al., 2016; Pathak et al., 2017; Burda et al., 2019; Tang et al., 2017). Other lines study optimism in value estimates and posterior sampling (Osband et al., 2016; O'Donoghue et al., 2018; Agrawal and Jia, 2017).",
    "document": "Related Work\n\nEfficient exploration is a central challenge in reinforcement learning, particularly in sparse-reward and long-horizon environments. Approaches typically aim to encourage informative behavior when extrinsic reward is absent or misleading.\n\nPrior work on exploration leverages intrinsic motivation, such as prediction error bonuses, curiosity-driven features, and count-based generalization (Bellemare et al., 2016; Pathak et al., 2017; Burda et al., 2019; Tang et al., 2017). Other lines study optimism in value estimates and posterior sampling (Osband et al., 2016; O'Donoghue et al., 2018; Agrawal and Jia, 2017).\n\nOur study investigates the stability of intrinsic rewards under representation drift and proposes a normalization scheme that decouples exploration bonuses from nonstationary features. We evaluate the method in both tabular-style grid worlds and 3D control tasks.",
    "reason": "The span lists categories of prior work with citations but does not synthesize their limitations, connections, or how they inform the authors’ method, leaving the reader without a clear bridge to the paper’s contribution.",
    "start": 254,
    "end": 629,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Unsupervised domain adaptation for semantic segmentation has leveraged adversarial feature alignment (Tsai et al., 2018; Hoffman et al., 2018), entropy minimization and confidence regularization (Vu et al., 2019; Grandvalet and Bengio, 2005), self-training with pseudo-labels (Zou et al., 2018; Chen et al., 2019), test-time adaptation (Sun et al., 2020b), and style transfer from source to target (Huang and Belongie, 2017; Zhang et al., 2018).",
    "document": "Introduction\n\nDeploying semantic segmentation models across cities or sensors entails substantial domain shift, reducing accuracy on unlabeled target domains. Collecting annotations for each new city or camera is infeasible, motivating adaptation without target labels.\n\nUnsupervised domain adaptation for semantic segmentation has leveraged adversarial feature alignment (Tsai et al., 2018; Hoffman et al., 2018), entropy minimization and confidence regularization (Vu et al., 2019; Grandvalet and Bengio, 2005), self-training with pseudo-labels (Zou et al., 2018; Chen et al., 2019), test-time adaptation (Sun et al., 2020b), and style transfer from source to target (Huang and Belongie, 2017; Zhang et al., 2018).\n\nNevertheless, real deployments face dynamic appearance changes and class imbalance that undermine naive pseudo-labeling. Existing work often assumes static target distributions.\n\nWe propose ShiftGuard, a class-aware, streaming adaptation strategy that calibrates confidence under target drift and enforces temporal consistency across frames.",
    "reason": "The span lists mainstream UDA techniques without connecting them to the present deployment setting or explicitly pointing out the specific gap that the paper addresses.",
    "start": 271,
    "end": 716,
    "label": "Lacks_synthesis"
  },
  {
    "span": "We follow the standard setup of the GLUE benchmark.",
    "document": "Introduction\n\nNatural language understanding (NLU) benchmarks have catalyzed progress by standardizing tasks and evaluation protocols. Despite strong average scores, models often rely on dataset artifacts and struggle with out-of-distribution shifts.\n\nWe follow the standard setup of the GLUE benchmark. Building on this, we introduce a stress-testing suite that perturbs inputs to reveal reliance on spurious correlations and shallow heuristics.\n\nOur method augments training with counterfactual examples and measures invariance using a set of targeted probes.",
    "reason": "The GLUE benchmark should be cited at first mention to acknowledge the source and protocol (guideline a).",
    "start": 252,
    "end": 303,
    "label": "Unsupported_claim"
  },
  {
    "span": "The majority of prior approaches to temporal point processes rely on Hawkes-process-style self-excitation.",
    "document": "Related Work\n\nTemporal point processes (TPPs) model the timing of events in continuous time, with applications in social contagion, finance, and healthcare. Classical models include the Poisson process and the Hawkes process, the latter capturing self-exciting dynamics through a triggering kernel (Hawkes, 1971; Ogata, 1988). Neural variants parameterize the conditional intensity with recurrent or attention-based architectures for greater flexibility (Du et al., 2016; Xiao et al., 2017; Mei and Eisner, 2017). The majority of prior approaches to temporal point processes rely on Hawkes-process-style self-excitation. More recent work explores inhibition, periodicity, and exogenous covariates through structured kernels and neural spline intensities (Shchur et al., 2020; Zuo et al., 2020).",
    "reason": "Claims a majority trend in prior work without citations or evidence quantifying this prevalence, violating rule (a)/(d).",
    "start": 514,
    "end": 620,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT has been used in AES to encode both prompt and essay with a pairwise ranking loss.",
    "document": "Related Work\n\nAutomatic Essay Scoring (AES) has evolved from feature-engineered linear models to end-to-end neural approaches. Early systems relied on handcrafted lexical and syntactic indicators, whereas neural architectures learn holistic representations of writing quality. Recent neural AES models incorporate prompts and discourse structure to better capture task relevance and coherence. BERT has been used in AES to encode both prompt and essay with a pairwise ranking loss. Other work explores ordinal regression objectives to respect the inherent ordering of scores, as well as domain adaptation techniques to transfer across prompts. Despite these advances, sensitivity to spurious lexical cues and limited feedback explainability remain open challenges.",
    "reason": "Describes a specific modeling setup used in prior work without citing the study that introduced it (rule a, e.iii).",
    "start": 394,
    "end": 481,
    "label": "Unsupported_claim"
  },
  {
    "span": "Domain adaptation for neural machine translation has been approached via fine-tuning on in-domain data, instance weighting, synthetic data generation, and multi-domain modeling (Luong and Manning, 2015; Chen et al., 2017; Wang et al., 2017; Chu and Wang, 2018; Britz et al., 2017; Zeng et al., 2018).",
    "document": "Related Work\n\nNeural machine translation (NMT) systems achieve strong performance when trained on large corpora that match the test domain, but quality deteriorates under domain shift. Adaptation therefore aims to bridge the distribution gap between general-purpose corpora and specialized domains such as legal, medical, or conversational text, while preserving general-domain competence.\n\nDomain adaptation for neural machine translation has been approached via fine-tuning on in-domain data, instance weighting, synthetic data generation, and multi-domain modeling (Luong and Manning, 2015; Chen et al., 2017; Wang et al., 2017; Chu and Wang, 2018; Britz et al., 2017; Zeng et al., 2018). Recently, parameter-efficient tuning and prompt-based conditioning have also been investigated to reduce deployment costs and catastrophic forgetting.\n\nOur work studies source-side sparsity in domain signals and introduces a routing mechanism that conditions translation on latent domain factors inferred from minimal supervision.",
    "reason": "The sentence summarizes prior techniques with citations but does not connect them to the paper’s contribution or specify the unresolved gap, hence lacking synthesis.",
    "start": 391,
    "end": 691,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Several recent works demonstrate that differential privacy is unnecessary when secure aggregation is used.",
    "document": "Related Work\n\nFederated learning (FL) enables training models across decentralized data silos without centralizing raw data. Threat models in FL include inference attacks on updates and poisoning attempts by adversarial clients. Secure aggregation protects updates in transit, while differential privacy (DP) provides statistical guarantees against information leakage from model parameters.\n\nSeveral recent works demonstrate that differential privacy is unnecessary when secure aggregation is used. In contrast, other studies argue that DP complements cryptographic protections by limiting worst-case leakage from gradients even after aggregation.\n\nWe study the interplay between secure aggregation and client-level DP in cross-device FL, quantifying utility-privacy trade-offs under practical participation rates.",
    "reason": "Cites unspecified 'recent works' to support a strong claim but provides no references.",
    "start": 393,
    "end": 499,
    "label": "Unsupported_claim"
  },
  {
    "span": "Early molecular property predictors relied on hand-crafted fingerprints and kernel methods (Rogers and Hahn, 2010; Ralaivola et al., 2005). With the advent of deep learning, convolutional and message-passing architectures became dominant, enabling end-to-end learning from molecular graphs (Duvenaud et al., 2015; Gilmer et al., 2017; Kearnes et al., 2016). More recent work explores attention mechanisms and graph transformers to capture long-range interactions (Ying et al., 2021; Rong et al., 2020).",
    "document": "Introduction\n\nMolecular property prediction is central to computational chemistry and drug discovery, where rapid estimation of activity, toxicity, and pharmacokinetics can accelerate lead optimization. Despite notable progress, many models struggle with limited data regimes, scaffold generalization, and encoding of subtle stereochemical cues.\n\nEarly molecular property predictors relied on hand-crafted fingerprints and kernel methods (Rogers and Hahn, 2010; Ralaivola et al., 2005). With the advent of deep learning, convolutional and message-passing architectures became dominant, enabling end-to-end learning from molecular graphs (Duvenaud et al., 2015; Gilmer et al., 2017; Kearnes et al., 2016). More recent work explores attention mechanisms and graph transformers to capture long-range interactions (Ying et al., 2021; Rong et al., 2020).\n\nIn this paper, we introduce Sparse-Context Graph Networks (SCGN), a framework that leverages chemically motivated sparse neighborhoods to improve inductive bias and robustness under distribution shift. We combine distance-aware positional encodings with relational dropout to prevent spurious shortcuts.\n\nWe evaluate SCGN across 12 public benchmarks, including MoleculeNet splits, and assess performance under scaffold and time-split scenarios. We also conduct ablations on neighborhood sparsity and stereochemical encoding.",
    "reason": "The span lists prior approaches and citations without connecting them to the paper’s aims, gaps, or motivation, thus failing to synthesize related work with the authors’ argument (criteria a and c).",
    "start": 347,
    "end": 849,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Deng 2009)",
    "document": "Related Work\n\nTransfer learning from large-scale image classification has become a standard starting point for downstream vision tasks. Models pretrained on ImageNet provide strong initialization for detection and segmentation, especially when fine-tuned with task-specific heads (He et al., 2016;Girshick, 2015).\n\nDespite the ubiquity of ImageNet pretraining, recent work revisits its necessity for modern architectures and datasets (Kaiming et al., 2020). In our setup, we retain ImageNet scale but modify augmentation to better match target domains, using the standard dataset release (Deng 2009) for initialization and evaluation.",
    "reason": "Missing comma between author and year in the parenthetical citation. Should be '(Deng, 2009)'.",
    "start": 588,
    "end": 599,
    "label": "Format"
  },
  {
    "span": "Classical ARIMA provides strong seasonal baselines (Box and Jenkins, 1970). LSTM variants capture long-term dependencies in traffic data (Ma et al., 2015). Transformer models with sparse attention scale to long horizons (Zhou et al., 2021).",
    "document": "Related Work\n\nTime-series forecasting methods range from classical statistical models to modern deep learning architectures. Long-horizon prediction introduces compounding errors and requires careful inductive biases.\n\nClassical ARIMA provides strong seasonal baselines (Box and Jenkins, 1970). LSTM variants capture long-term dependencies in traffic data (Ma et al., 2015). Transformer models with sparse attention scale to long horizons (Zhou et al., 2021). We study error accumulation under covariate shift with distributionally robust training.\n\nOur model regularizes forecast trajectories using adversarial perturbations in feature space.",
    "reason": "The sequence lists disparate lines of work without transitions or explicit relationships; the move from ARIMA to LSTMs to transformers lacks connective explanation.",
    "start": 219,
    "end": 459,
    "label": "Coherence"
  },
  {
    "span": "[Smith et al., 2015]",
    "document": "Related Work\n\nTime-series forecasting has a long history in statistics and machine learning, from ARIMA and exponential smoothing to modern neural architectures (Box and Jenkins, 1976; Hyndman and Athanasopoulos, 2018). Deep models such as temporal convolutional networks and sequence-to-sequence architectures capture complex dependencies and achieve state-of-the-art accuracy on many benchmarks (Bai et al., 2018; Lim et al., 2021).\n\nRecent work emphasizes probabilistic forecasting, interpretability, and covariate handling (Salinas et al., 2020; Rangapuram et al., 2018). For example, attention mechanisms highlight salient periods and exogenous drivers, as shown in [Smith et al., 2015], and multihorizon objectives improve calibration and coverage (Wang et al., 2019; Sen et al., 2019). Building on these insights, we propose a decomposed attention model that disentangles trend, seasonality, and event effects while maintaining tractable uncertainty estimates.",
    "reason": "Wrong bracket style for an author–year citation; should use parentheses as (Smith et al., 2015) rather than square brackets.",
    "start": 671,
    "end": 691,
    "label": "Format"
  },
  {
    "span": "there has been a surge of recent works on controllable summarization that manipulate attributes such as length, entity focus, and sentiment.",
    "document": "Related Work\n\nNeural abstractive summarization has advanced rapidly with the introduction of encoder–decoder architectures and large pre-trained language models. Beyond overall fidelity and fluency, there is growing interest in enabling user control over summary attributes.\n\nHowever, there has been a surge of recent works on controllable summarization that manipulate attributes such as length, entity focus, and sentiment. Methods typically inject control tokens, condition on auxiliary predictors, or constrain decoding to align the output with the requested attribute. Despite progress, many approaches still struggle to maintain factuality when the control signal is strong, motivating methods that explicitly preserve source-grounded content.",
    "reason": "Mentions 'recent works' without citing specific papers; per rule (d), statements about recent work require citations.",
    "start": 285,
    "end": 425,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Nguyen, 2018, Patel, 2020)",
    "document": "Related Work\n\nCross-lingual transfer. Comparative studies (Nguyen, 2018, Patel, 2020) suggest that multilingual pretraining benefits entity-centric tasks more than relation-centric tasks. Approaches leveraging alignment objectives (Conneau et al., 2020) and bilingual lexicons (Artetxe et al., 2018) have reported strong zero-shot performance.\n\nNevertheless, structural divergences across languages still pose challenges for dependency-based models.",
    "reason": "Multiple citations are separated by a comma instead of a semicolon; should be '(Nguyen, 2018; Patel, 2020)'.",
    "start": 58,
    "end": 85,
    "label": "Format"
  },
  {
    "span": "In (Garcia et al., 2018)",
    "document": "Related Work\n\nIn (Garcia et al., 2018), the aggregation rule for federated learning is derived under a bounded variance assumption, but later analyses relax this constraint using client importance sampling (Li et al., 2020). Robust aggregation further mitigates the effect of Byzantine clients by clipping or medoid selection (Blanchard et al., 2017; Yin et al., 2018). Our work complements these methods by adapting server-side learning rates to on-device drift estimates.\n\nPersonalized federated optimization departs from a single global model and tailors parameters to client-specific data distributions (Smith et al., 2017; Dinh et al., 2020). Meta-learning formulations have also been proposed to accelerate on-device adaptation (Chen et al., 2018). We compare to these lines while emphasizing stability under highly non-IID participation.",
    "reason": "Wrong citation style: a preposition precedes a parenthetical citation. It should be narrative style, e.g., In Garcia et al. (2018) ...",
    "start": 14,
    "end": 38,
    "label": "Format"
  },
  {
    "span": "industry reports often estimate that data scientists spend around 80% of their time on data cleaning and preparation",
    "document": "Introduction\n\nData preprocessing is a critical yet under-studied component of machine learning pipelines. Despite widespread tooling, inconsistencies in schema, missing values, and noisy labels hinder reliable model training. Anecdotally, industry reports often estimate that data scientists spend around 80% of their time on data cleaning and preparation. This motivates techniques that reduce manual effort while preserving downstream accuracy. We explore simple, automatable interventions for schema reconciliation and label denoising, aiming to improve efficiency without extensive domain expertise.\n",
    "reason": "Presents a quantitative claim attributed vaguely to “industry reports” without citing any specific source.",
    "start": 239,
    "end": 355,
    "label": "Unsupported_claim"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nOpen-domain question answering combines retrieval with reading comprehension to answer factoid queries. Dense retrieval models scale to large corpora and improve recall compared to sparse term matching (Karpukhin et al., 2020;Xiong et al., 2021), while reader architectures benefit from pretrained language models (Devlin et al., 2019;Roberts et al., 2020).\n\nRecent pretraining methods have advanced QA [12] by aligning retrievers and readers through joint objectives (Izacard and Grave, 2021) and by leveraging passage-level supervision (Gao and Callan, 2021). Our work investigates retrieval distillation under limited supervision and evaluates generalization across domains.",
    "reason": "Numeric bracketed citation used in a context that otherwise follows author–year style. Should be replaced with an author–year citation such as '(Author, Year)'.",
    "start": 417,
    "end": 421,
    "label": "Format"
  },
  {
    "span": "BERT has been successfully employed for automated essay scoring on the ASAP dataset.",
    "document": "Related Work\n\nAutomated essay scoring (AES) aims to predict human-assigned scores for student essays. Early approaches relied on handcrafted features capturing syntax, discourse, and surface properties. With the rise of deep learning, neural encoders have been used to learn holistic representations of writing quality.\n\nBERT has been successfully employed for automated essay scoring on the ASAP dataset. Recent neural AES models explore pairwise ranking losses, domain adaptation across prompts, and semi-supervised learning with pseudo-labeling. However, prompt-specific lexical memorization remains a challenge, motivating approaches that better disentangle content and writing quality.",
    "reason": "Claims a specific setup and success on a known dataset without citing any supporting work, and the dataset is mentioned without first-mention citation (definition a and example iii).",
    "start": 321,
    "end": 405,
    "label": "Unsupported_claim"
  },
  {
    "span": "Miller et al., 2019)",
    "document": "Related Work\n\nActive learning strategies prioritize informative instances to reduce annotation cost (Settles, 2010). Modern approaches combine uncertainty sampling with diversity to avoid redundancy (Sener and Savarese, 2018; Ash et al., 2020). In multilingual settings, transfer-based selection enhances cross-lingual generalization (Ein-Dor et al., 2020). Prior work reported diminishing returns without careful batch construction, see Miller et al., 2019) for a comprehensive analysis of batch-mode selection.\n\nWe revisit batch construction with calibration-aware objectives, demonstrating consistent gains across text classification benchmarks.",
    "reason": "Missing opening parenthesis for a parenthetical citation: it ends with ')', implying an unbalanced citation.",
    "start": 438,
    "end": 458,
    "label": "Format"
  },
  {
    "span": "The SQuAD dataset contains 150K questions sourced from middle-school textbooks.",
    "document": "Introduction\n\nAutomatic question generation (QG) aims to produce pedagogically valuable questions from source texts to support teaching and assessment. While large-scale reading comprehension datasets have catalyzed progress in QG, the alignment between benchmark questions and educational objectives remains limited. The SQuAD dataset contains 150K questions sourced from middle-school textbooks. In contrast, our corpus is curated from curriculum-aligned passages with expert-authored objectives, enabling evaluation on cognitive skill targets beyond surface-level span selection. We also release item metadata and difficulty estimates to facilitate adaptive testing research.",
    "reason": "Claims a specific statistic and provenance about a well-known dataset without citing a source; per rule (a) dataset mentions should include citations.",
    "start": 318,
    "end": 397,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith & Jones (2010)",
    "document": "Introduction\n\nUser trust in decision-support systems depends on transparency, appropriate explanations, and calibrated uncertainty (Norman, 2013; Ribeiro et al., 2016). Contrary evidence was reported by Smith & Jones (2010), who argued that excessive detail can overwhelm non-expert users and reduce perceived usability. Subsequent work explored scrutability features and selective disclosure to balance informativeness with cognitive load (Kulesza et al., 2013; Narayanan et al., 2018).",
    "reason": "Ampersand used in a narrative citation; in author–year narrative style, “and” should be used: “Smith and Jones (2010)”.",
    "start": 203,
    "end": 223,
    "label": "Format"
  },
  {
    "span": "[(Khan et al., 2022)]",
    "document": "Related Work\n\nGraph-based anomaly detection leverages structural signals such as degree, motif counts, and community assignments. Classical approaches compute structural outlier scores using spectral residuals (Cai and Wang, 2015). More recent graph neural networks model context with message passing (Zhou et al., 2020). Hybrid architectures combine reconstruction and ranking losses to improve sensitivity on rare patterns [(Khan et al., 2022)]. Our study focuses on calibration of anomaly scores across dynamic graphs with shifting degree distributions.",
    "reason": "Mixed bracket styles around a single citation; should use standard parentheses for author–year style, e.g., \"(Khan et al., 2022)\".",
    "start": 425,
    "end": 446,
    "label": "Format"
  },
  {
    "span": "CLIP learns visual representations from image–text pairs via contrastive learning (Radford et al., 2021). Audio–visual correspondence can supervise representation learning without labels (Arandjelović and Zisserman, 2017). Detic aligns region features with noun phrases for open-vocabulary detection (Zhou et al., 2022). Contrastive objectives help avoid representation collapse (Chen et al., 2020).",
    "document": "Related Work\n\nMultimodal Pretraining. Jointly training on images and text has achieved strong zero-shot transfer by aligning modalities through contrastive or matching objectives (Radford et al., 2021; Jia et al., 2021). Extensions incorporate region-phrase grounding and detection, as well as video-text alignment for temporal reasoning (Li et al., 2022; Zellers et al., 2021).\n\nObjectives and Supervision. CLIP learns visual representations from image–text pairs via contrastive learning (Radford et al., 2021). Audio–visual correspondence can supervise representation learning without labels (Arandjelović and Zisserman, 2017). Detic aligns region features with noun phrases for open-vocabulary detection (Zhou et al., 2022). Contrastive objectives help avoid representation collapse (Chen et al., 2020). We study the data scaling laws when mixing weakly paired web data with limited human-curated captions, focusing on alignment efficiency across modalities.",
    "reason": "The sentences cover image–text contrastive learning, audio–visual correspondence, open-vocabulary detection, and general contrastive properties without transitions or an explicit thread tying them together, making the relationships implicit and unclear.",
    "start": 408,
    "end": 807,
    "label": "Coherence"
  },
  {
    "span": "The LibriSpeech dataset remains the de facto standard for ASR evaluation.",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) has advanced rapidly with self-supervised pretraining, improved acoustic modeling, and neural language models. Public corpora have been central to this progress, enabling reproducible research and standardized benchmarks. The LibriSpeech dataset remains the de facto standard for ASR evaluation. Nonetheless, its read-speech nature and limited acoustic diversity motivate complementary benchmarks covering spontaneous and far-field speech. Recent work explores multi-domain training, confidence estimation, and domain adaptation to close the gap between laboratory conditions and real-world deployments. Our study introduces a conversational benchmark emphasizing code-switching and long-form dependencies.\n",
    "reason": "Asserts a specific dataset's status as 'de facto standard' without supporting citation or evidence (violates rule b).",
    "start": 271,
    "end": 344,
    "label": "Unsupported_claim"
  },
  {
    "span": "Work on algorithmic fairness formalizes criteria such as demographic parity, equalized odds, equal opportunity, calibration, and predictive parity, and develops post-processing, in-processing, and pre-processing mitigation methods (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017; Agarwal et al., 2018). Empirical studies evaluate trade-offs across benchmarks in lending, hiring, and criminal justice datasets.",
    "document": "Related Work: Fairness in Machine Learning\n\nAs predictive systems mediate access to resources, ensuring fairness across protected groups has emerged as a central requirement. Formal criteria and mitigation techniques seek to characterize and reduce disparate impact.\n\nWork on algorithmic fairness formalizes criteria such as demographic parity, equalized odds, equal opportunity, calibration, and predictive parity, and develops post-processing, in-processing, and pre-processing mitigation methods (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017; Agarwal et al., 2018). Empirical studies evaluate trade-offs across benchmarks in lending, hiring, and criminal justice datasets.\n\nOur approach incorporates group-conditional constraints into training with adaptive penalty tuning.",
    "reason": "The span inventories fairness criteria and methods without discussing their limitations or how they connect to the authors' approach, leaving the motivation unstated (criteria a and c).",
    "start": 268,
    "end": 693,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Time-series anomaly detection methods include forecasting-based residual analysis with ARIMA and deep sequence models (Box and Jenkins, 1970; Lai et al., 2018; Wu et al., 2020), as well as reconstruction-based detectors using autoencoders and variational models (Sakurada and Yairi, 2014; Malhotra et al., 2016; Xu et al., 2018).",
    "document": "Related Work\n\nDetecting anomalies in multivariate time series under nonstationarity is vital for monitoring critical infrastructure. Methods differ in their modeling assumptions, robustness to distribution shifts, and ability to provide actionable explanations.\n\nTime-series anomaly detection methods include forecasting-based residual analysis with ARIMA and deep sequence models (Box and Jenkins, 1970; Lai et al., 2018; Wu et al., 2020), as well as reconstruction-based detectors using autoencoders and variational models (Sakurada and Yairi, 2014; Malhotra et al., 2016; Xu et al., 2018).\n\nWe introduce a change-aware conformal detector that adapts its nonconformity scores via online covariate shift estimation, yielding calibrated alerts under evolving conditions. Extensive experiments cover synthetic drifts and real-world telemetry.\n",
    "reason": "The span summarizes categories of anomaly detectors without relating them to the proposed conformal method or indicating what deficiency it overcomes.",
    "start": 263,
    "end": 592,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Audio-visual speech recognition has explored early fusion of spectrograms and frames (Ngiam et al., 2011), mid-level fusion with shared encoders (Afouras et al., 2018), and late fusion of logits or CTC posteriors (Chung et al., 2017). Transformer-based cross-modal attention and conformer backbones have further advanced performance (Shi et al., 2022; Ma et al., 2021).",
    "document": "Introduction\n\nIntegrating visual cues with acoustics improves robustness of speech recognition under noise and occlusions. Lip movements provide information complementary to audio, especially in low signal-to-noise conditions.\n\nAudio-visual speech recognition has explored early fusion of spectrograms and frames (Ngiam et al., 2011), mid-level fusion with shared encoders (Afouras et al., 2018), and late fusion of logits or CTC posteriors (Chung et al., 2017). Transformer-based cross-modal attention and conformer backbones have further advanced performance (Shi et al., 2022; Ma et al., 2021).\n\nWe study failure modes in asynchronous and misaligned streams and propose an alignment-robust training objective that jointly learns cross-modal delay distributions and performs uncertainty-aware gating. Experiments on LRS3 with synthetic delays and real-world noise demonstrate substantial WER reductions over strong baselines.",
    "reason": "The span catalogs fusion strategies and architectures without tying them to the misalignment problem addressed by the paper; no explicit gap or author stance is provided (criterion a/c).",
    "start": 228,
    "end": 597,
    "label": "Lacks_synthesis"
  },
  {
    "span": "To the best of our knowledge, no work has evaluated dialogue summarization on multi-domain medical conversations.",
    "document": "Introduction\n\nDialogue summarization has attracted increasing attention due to its potential to condense lengthy multi-party interactions into actionable briefs for downstream tasks such as clinical decision support and case review (Zhang et al., 2021; Chen and Yang, 2020). While early studies focused on meeting and customer service transcripts (McCowan et al., 2005; Goo and Chen, 2018), recent efforts have expanded to noisy, real-world conversations that exhibit overlapping speech, disfluencies, and domain-specific jargon (Gao et al., 2022; Liu et al., 2021). However, medical dialogues present additional challenges, including privacy-preserving redactions, heterogeneous speaker roles (e.g., patient, clinician, scribe), and infrequent but critical clinical events (Liu et al., 2019).\n\nExisting systems typically leverage pretrained language models with role-aware conditioning, text normalization, and discourse constraints to improve factuality and reduce hallucinations (Narayan et al., 2018; Goyal et al., 2021). Benchmark datasets for dialogue summarization are often limited to general-purpose domains or television scripts, making robust generalization to specialized settings unclear (Gliwa et al., 2019; Zhong et al., 2021). To the best of our knowledge, no work has evaluated dialogue summarization on multi-domain medical conversations. This gap hinders the ability to quantify factual consistency under clinical terminology drift and cross-specialty variation.\n\nIn this paper, we present a new evaluation suite spanning outpatient, emergency, and telemedicine encounters, and we propose role-grounded factuality metrics tailored to clinical summaries. Our contributions include a domain-diverse dataset, a comprehensive evaluation of recent summarizers, and error analyses that surface modality- and specialty-specific failure modes.",
    "reason": "This is a claim about the absence of prior work in a niche area without citing surveys, datasets, or studies to substantiate it.",
    "start": 1243,
    "end": 1356,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Smith and Jones, 2019)",
    "document": "Related Work\n\nAlgorithmic fairness spans pre-processing, in-processing, and post-processing interventions. Group fairness seeks parity across predefined cohorts (Hardt et al., 2016), while individual fairness focuses on similar treatment of similar individuals (Dwork et al., 2012). In the context of risk assessment, calibration and equalized odds are often in tension (Kleinberg et al., 2017). Prior auditing tools quantify disparities across multiple axes (Smith and Jones, 2019) and propose reweighting to mitigate selection bias (Zhang and LeCun, 2018).\n\nOur contribution differs by integrating counterfactual evaluation into training, enforcing invariance to protected attributes.",
    "reason": "In parenthetical citations using APA style, author names should be joined with an ampersand: (Smith & Jones, 2019), not 'and'.",
    "start": 459,
    "end": 482,
    "label": "Format"
  },
  {
    "span": "(Johnson, 2017))",
    "document": "Introduction\n\nExplainable NLP has seen growing interest, with methods ranging from post-hoc rationales to inherently interpretable architectures. Recent surveys (Johnson, 2017)) provide taxonomies of explanation types and evaluation pitfalls. Building on these insights, we examine explanation faithfulness in the presence of dataset artifacts (Gururangan et al., 2018) and shortcut features.\n\nOur contributions include a standardized protocol for causal evaluation and a library of counterfactual augmentation operators for text classification.",
    "reason": "Parenthetical citation has an extra closing parenthesis, resulting in mismatched brackets.",
    "start": 161,
    "end": 177,
    "label": "Format"
  },
  {
    "span": "(Zheng et al., 2017;Zeng et al., 2018;. ",
    "document": "Introduction\n\nInformation extraction (IE) is a task that aims to extract information of interest from text data and represent the extracted information in a structured form. Traditional IE tasks include named entity recognition which recognizes entities and their types (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Devlin et al., 2019), relation extraction which identifies the relationships between entities (Zheng et al., 2017; Zeng et al., 2018; Zhong and Chen, 2020), etc. Since the results of IE are structured, they can be easily used by computer systems in different applications such as text mining.\n\nIn this work, we study IE in a new setting, referred to as text-to-table. First, the system receives a training dataset containing text-table pairs. Each text-table pair contains a text and a table (or tables) representing information extracted from the text. The system learns a model for information extraction. Next, the system employs the learned model to conduct information extraction from a new text and outputs the result in a table (or tables). Figure 1 gives an example of text-to-table, where the input (above) is a report of a basketball game, and the output (below) is two tables summarizing the scores of the teams and players from the input.\n\nText-to-table is unique compared to the traditional IE approaches. First, it is mainly designed to extract structured data in a complex form from a long text. As in the example in Figure 1, extraction of information is performed from the entire document. The extracted information contains multiple types of scores of teams and players in a basketball game structured in table format. Second, the schemas for extraction are implicitly included in the training data, and there is no need to explicitly define the schemas. This reduces the need for manual efforts for schema design and annotations.\n\nOur work is inspired by research on the so-called table-to-text (or data-to-text) problem, which is the task of generating a description for a given table . Table -to-text is useful in applications where the content of a table needs to be described in natural language. Thus, text-to-table can be regarded as an inverse problem of table-to-text. However, there are also differences. Most notably, their applications are different. Text-to-table can be applied to document summarization, text mining, etc.\n\nIn this work, we formalize text-to-table as a sequence-to-sequence (seq2seq) task. More specifically, we translate the text into a sequence representation of a table (or tables), where the schema of the table is implicitly contained in the representation. We also build the seq2seq model on top of a pre-trained language model, which is the stateof-the-art approach for seq2seq tasks (Lewis et al., 2019;Raffel et al., 2020). Although the approach is a natural application of existing technologies, as far as we know, there has been no previous study to investigate to what extent the approach works. We also develop a new method for text-to-table within the seq2seq approach with two additional techniques, table constraint and table relation embeddings. Table constraint controls the creation of rows in a table and table relation embeddings affect the alignments between cells and their row headers and column headers. Both are to make the generated table well-formulated.\n\nThe approach to IE based on seq2seq has already been proposed. Methods for conducting individual tasks of relation extraction (Zeng et al., 2018;Nayak and Ng, 2020), named entity recognition (Chen and Moschitti, 2018;Yan et al., 2021), and event extraction (Lu et al., 2021) have been developed. Methods for jointly performing multiple tasks of named entity recognition, relation extraction, and event extraction have also been devised (Paolini et al., 2021). Most of the methods exploit suitable pre-trained models such as BERT. However, all the existing methods rely on pre-defined schemas for extraction. Moreover, their models are designed to extract information from short texts, rather than long texts, and extract information with simple structures (such as an entity and its type), rather than information with complicated structures (such as a table ).\n\nWe conduct extensive experiments on four datasets. Results show that the vanilla seq2seq model fine-tuned from BART (Lewis et al., 2019) can outperform the state-of-the-art IE models finetuned from BERT (Devlin et al., 2019;Zhong and Chen, 2020). Furthermore, results show that our proposed approach to text-to-table with the two techniques can further improve the extraction accuracies. We also summarize the challenging issues with the seq2seq approach to text-to-table for future research.\n\nOur contributions are summarized as follows:\n\n1. We propose the new task of text-to-table for IE. We derive four new datasets for the task from existing datasets. 2. We formalize the task as a seq2seq problem and propose a new method within the seq2seq approach using the techniques of Traditionally, researchers formalize the task as a language understanding problem. The state-ofthe-art methods for NER perform the task on the basis of the pre-trained language model BERT (Devlin et al., 2019). The pipeline approach to RE divides the problem into NER and relation classification, and conducts the two sub-tasks in a sequential manner (Zhong and Chen, 2020), while the end-to-end approach jointly carries out the two sub-tasks (Zheng et al., 2017;Zeng et al., 2018;. The state-of-the-art methods for EE also employ BERT and usually jointly train the models with other tasks such as NER and RE Zhang et al., 2019;Lin et al., 2020). All the methods assume the use of pre-defined schemas (e.g., entity types for NER, entity and relation types for RE, and event templates for EE). Besides, most methods are designed for extraction from short texts. Therefore, existing methods for IE cannot be directly applied to text-to-table.\n\nAnother series of related work is open information extraction (OpenIE), which aims to extract information from texts without relying on explicitly defined schemas (Banko et al., 2007;Wu and Weld, 2010;Mausam et al., 2012;Stanovsky et al., 2018;Zhan and Zhao, 2020). However, OpenIE aims to extract information with simple structures (i.e., relation tuples) from short texts, and the methods in OpenIE cannot be directly applied to text-to-table . IE is also conducted at document level, referred to as doc-level IE. For example, some NER methods directly perform NER on a long document (Strubell et al., 2017;Luo et al., 2018), and others encode each sentence in a document, use attention to fuse document-level information, and perform NER on each sentence (Hu et al., 2020;Xu et al., 2018). There are also RE methods that predict the relationships between entities in a document (Yao et al., 2019;Nan et al., 2020a). However, existing doc-level IE approaches usually do not consider extraction of complex relations between many items.\n\nSequence-to-sequence (seq2seq) is the general problem of transforming one text into another text (Sutskever et al., 2014;Bahdanau et al., 2014), which includes machine translation, text summarization, etc. The use of the pre-trained language models of BART (Lewis et al., 2019) and T5 (Raffel et al., 2020) can significantly boost the performances of seq2seq, such as machine translation (Lewis et al., 2019;Raffel et al., 2020;) and text summarization (Lewis et al., 2019;Raffel et al., 2020;.\n\nRecently, some researchers also formalize the IE problems as seq2seq, that is, transforming the input text into an internal representation. One advantage is that one can employ a single model to extract multiple types of information. Results show that this approach works better than or equally well as the traditional approach of language understanding, in RE (Zeng et al., 2018;Nayak and Ng, 2020), NER (Chen and Moschitti, 2018;Yan et al., 2021) and EE (Lu et al., 2021). Methods for jointly performing multiple tasks including NER, RE and EE have also been devised (Paolini et al., 2021).\n\nData-to-text aims to generate natural language descriptions from the input structured data such as sport commentaries (Wiseman et al., 2017). The structured data is usually represented as tables (Wiseman et al., 2017;Thomson et al., 2020;, sets of table cells (Parikh et al., 2020;Bao et al., 2018), semantic representations (Novikova et al., 2017), or sets of relation triples (Gardent et al., 2017;Nan et al., 2020b). The task requires the model to select the salient information from the data, organize it in a logical order, and generate an accurate and fluent natural language description (Wiseman et al., 2017). Data-to-text models usually adopt the encoder-decoder architecture. The encoders are specifically designed to model the input data, such as multi-layer perceptron (Puduppully et al., 2019a,b), recurrent neural network (Juraska et al., 2018;Shen et al., 2020), graph neural network (Marcheggiani and Perez-Beltrachini, 2018;Koncel-Kedziorski et al., 2019), or Transformer (Gong et al., 2019).\n\n ",
    "start": 5450,
    "end": 5490,
    "label": "Format"
  },
  {
    "span": "(Garcia et. al., 2016)",
    "document": "Related Work\n\nEnd-to-end speech recognition benefits from large-scale pretraining and data augmentation (Baevski et al., 2020; Park et al., 2019). Hybrid CTC/attention models remain competitive on noisy benchmarks (Watanabe et al., 2017; Karita et al., 2019).\n\nLanguage modeling for ASR error correction has also been explored (Liu et al., 2021). Prior low-resource adaptation studies (Garcia et. al., 2016) focus on cross-lingual phonetic transfer, which we extend with multilingual wav2vec.\n\nOur method jointly fine-tunes acoustic and language modules, yielding consistent WER reductions under data scarcity.",
    "reason": "Incorrect punctuation in “et al.”; should be “et al.” without the extra period after “et”.",
    "start": 385,
    "end": 407,
    "label": "Format"
  },
  {
    "span": "Classical time-series anomaly detection employs statistical tests and decomposition (Box and Jenkins, 1976; Cleveland et al., 1990), isolation-based and density methods (Liu et al., 2008; Breunig et al., 2000), and probabilistic models such as HMMs (Rabiner, 1989). Deep approaches include autoencoders (Sakurada and Yairi, 2014), variational models (An and Cho, 2015), sequence models with LSTMs and Transformers (Malhotra et al., 2015; Xu et al., 2021), and forecasting-based residual detectors (Laptev et al., 2015; Rangapuram et al., 2018).",
    "document": "Introduction\n\nDetecting anomalous behaviors in multivariate time series underpins monitoring of industrial assets, IT systems, and healthcare devices. Practical challenges include limited labels, nonstationarity, and heterogeneous sampling.\n\nClassical time-series anomaly detection employs statistical tests and decomposition (Box and Jenkins, 1976; Cleveland et al., 1990), isolation-based and density methods (Liu et al., 2008; Breunig et al., 2000), and probabilistic models such as HMMs (Rabiner, 1989). Deep approaches include autoencoders (Sakurada and Yairi, 2014), variational models (An and Cho, 2015), sequence models with LSTMs and Transformers (Malhotra et al., 2015; Xu et al., 2021), and forecasting-based residual detectors (Laptev et al., 2015; Rangapuram et al., 2018).\n\nWe propose a label-efficient detector that couples invariant forecasting with change-point-aware uncertainty calibration to reduce false alarms under regime shifts.",
    "reason": "The span compiles methods and citations but does not connect them to the paper’s motivation or identify a gap, consistent with (a) and (b).",
    "start": 242,
    "end": 786,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Energy-based scores, Mahalanobis distance, and ODIN are representative post-hoc OOD detectors (Liu et al., 2020; Lee et al., 2018; Liang et al., 2018).",
    "document": "Related Work\n\nOut-of-distribution (OOD) detection. Deep networks are often poorly calibrated under dataset shift and can assign high confidence to anomalous inputs. Post-hoc scoring functions and training-time regularization have been proposed to mitigate this risk.\n\nEnergy-based scores, Mahalanobis distance, and ODIN are representative post-hoc OOD detectors (Liu et al., 2020; Lee et al., 2018; Liang et al., 2018). Training-time approaches include outlier exposure, confidence regularization, and contrastive learning to widen the in-distribution margin (Hendrycks et al., 2019; Mohseni et al., 2020; Sun et al., 2021). Recent work employs vision transformers and token-level statistics for OOD detection (Fort et al., 2021; Sun et al., 2022).\n\nEvaluation protocols. Common benchmarks consider near- and far-OOD datasets, synthetic corruptions, and shifts in resolution or texture. However, definitions of OOD vary widely across studies (Shafaei et al., 2019; Yang et al., 2021).",
    "reason": "The span lists canonical post-hoc methods but does not relate them to the authors' perspective, identify shortcomings, or discuss how the proposed method is positioned with respect to these detectors.",
    "start": 268,
    "end": 419,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Kumar et al. 1",
    "document": "Related Work\n\nNeural machine translation (NMT) has progressed from recurrent architectures (Bahdanau et al., 2015; Sutskever et al., 2014) to Transformer-based models (Vaswani et al., 2017). Data augmentation via back-translation (Sennrich et al., 2016) and noisy channel modeling (Yee et al., 2019) further improved performance. For domain adaptation, Kumar et al. 1 propose lightweight lexical adapters layered onto frozen encoders to quickly specialize to new domains without catastrophic forgetting.\n\nOur approach complements these methods by decoupling vocabulary expansion from encoder adaptation, reducing memory footprint and fine-tuning time.",
    "reason": "Wrong use of footnote-style numbering within an author–year citation; it should include a year (e.g., \"Kumar et al. (2020)\") or be formatted as a proper footnote.",
    "start": 353,
    "end": 367,
    "label": "Format"
  },
  {
    "span": "Off-policy algorithms improve sample efficiency (Haarnoja et al., 2018; Fujimoto et al., 2018). Sim-to-real transfer bridges domain gaps (Tobin et al., 2017). Safe RL constrains risky behaviors (Achiam et al., 2017). Classical control offers stability guarantees (Khalil, 2002).",
    "document": "Introduction\n\nReinforcement learning for robotics must balance data efficiency, safety, and generalization across changing environments. Bridging these requirements often calls for hybrid methods and careful deployment protocols.\n\nOff-policy algorithms improve sample efficiency (Haarnoja et al., 2018; Fujimoto et al., 2018). Sim-to-real transfer bridges domain gaps (Tobin et al., 2017). Safe RL constrains risky behaviors (Achiam et al., 2017). Classical control offers stability guarantees (Khalil, 2002).\n\nWe develop a constrained policy optimization approach with adaptive dynamics randomization, coupling data-efficient learning with safety constraints and transfer.",
    "reason": "The span lists four areas (off-policy RL, sim-to-real, safe RL, classical control) without transitions or an explicit narrative tying them together, resulting in abrupt shifts.",
    "start": 231,
    "end": 509,
    "label": "Coherence"
  },
  {
    "span": "Recent shared tasks on accented ASR highlight widespread disparities.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) systems continue to struggle with accented and dialectal speech, leading to inequities in access and performance for underrepresented communities. Standard training pipelines often under-sample non-canonical pronunciations, and evaluation sets rarely reflect the full range of phonetic variability. Recent shared tasks on accented ASR highlight widespread disparities. However, the community lacks a unified protocol for measuring accent robustness across domains and speaking styles. In this paper, we introduce a new benchmark emphasizing cross-accent generalization and propose a lightweight adaptation strategy that reduces error rates without requiring per-accent fine-tuning.",
    "reason": "Refers to shared tasks but does not cite any of them.",
    "start": 348,
    "end": 417,
    "label": "Unsupported_claim"
  },
  {
    "span": "User trust is shaped by perceived competence and benevolence (Mayer et al., 1995). Explanations for recommendations increase satisfaction and acceptance (Tintarev and Masthoff, 2015). Privacy-preserving design patterns reduce disclosure risk in conversational interfaces (Luger and Urquhart, 2018).",
    "document": "Related Work\n\nConversational agents are increasingly deployed in customer support, health, and productivity settings, raising questions about trust, transparency, and privacy. Trust in automation has been studied through multi-dimensional frameworks evaluating competence, benevolence, and integrity (Mayer et al., 1995; Lee and See, 2004). Explainable recommendation and dialogue have explored how rationales affect user decision-making and perceived control (Tintarev and Masthoff, 2015; Eiband et al., 2019). Meanwhile, privacy research examines consent, minimization, and on-device processing for sensitive contexts (Luger and Urquhart, 2018; Biega et al., 2017).\n\nUser trust is shaped by perceived competence and benevolence (Mayer et al., 1995). Explanations for recommendations increase satisfaction and acceptance (Tintarev and Masthoff, 2015). Privacy-preserving design patterns reduce disclosure risk in conversational interfaces (Luger and Urquhart, 2018). Recent work also investigates calibration and uncertainty communication in dialogue systems (Kadavath et al., 2022). We evaluate how concise uncertainty cues affect trust and disclosure under privacy guarantees.",
    "reason": "The span moves across trust, explanations, and privacy in separate sentences without transitions or explicit connections, leaving the relationship among these cited works implied and reducing coherence.",
    "start": 669,
    "end": 967,
    "label": "Coherence"
  },
  {
    "span": "In the SemEval 2021 sarcasm detection task, multilingual transformer baselines dominated the leaderboard.",
    "document": "Related Work\n\nSarcasm detection has evolved from lexicon-based heuristics to context-aware neural models that leverage user history and conversation threads. Shared tasks have catalyzed progress by standardizing datasets and evaluation protocols across languages and domains.\n\nIn the SemEval 2021 sarcasm detection task, multilingual transformer baselines dominated the leaderboard. Subsequent work explored domain adaptation and multi-task learning with sentiment and stance signals to further boost performance. Our method builds on these trends by integrating conversational context with cross-lingual representation alignment.",
    "reason": "Mentions a specific shared task and its reported outcome without citing the task overview or leaderboard (rule a).",
    "start": 277,
    "end": 382,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Barton et. al., 2022)",
    "document": "Introduction\n\nVision-language pretraining aligns image and text embeddings for downstream tasks such as retrieval and VQA (Lu et al., 2019; Radford et al., 2021). Contrastive dual-encoder models scale well but may underutilize fine-grained correspondences (Jia et al., 2021; Li et al., 2021). Cross-attention architectures achieve stronger grounding at higher cost (Chen et al., 2020; Li et al., 2020). Recent work explores region-level alignment and patch-text matching to improve localization (Kim et al., 2021; Barton et. al., 2022). We propose a token-level alignment objective with entropy regularization that narrows the gap between dual-encoder and cross-attention models.",
    "reason": "Incorrect use of 'et. al.' with a period after 'et'; should be 'et al.' as in (Barton et al., 2022).",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "Unsupervised domain adaptation (UDA) for object detection has explored adversarial alignment at image and instance levels, self-training with pseudo-labels, and style transfer via image-to-image translation (Chen et al., 2018; Saito et al., 2019; Cai et al., 2019; Inoue et al., 2018; He and Zhang, 2020). Multi-level feature alignment and weakly supervised variants have also been proposed (Zhu et al., 2019; Khodabandeh et al., 2019).",
    "document": "Related Work\n\nObject detectors trained on a source domain often degrade when deployed under target shifts such as weather, sensor, or geographical changes. UDA seeks to mitigate this performance drop without using target annotations.\n\nUnsupervised domain adaptation (UDA) for object detection has explored adversarial alignment at image and instance levels, self-training with pseudo-labels, and style transfer via image-to-image translation (Chen et al., 2018; Saito et al., 2019; Cai et al., 2019; Inoue et al., 2018; He and Zhang, 2020). Multi-level feature alignment and weakly supervised variants have also been proposed (Zhu et al., 2019; Khodabandeh et al., 2019).\n\nDespite progress, detector adaptation remains brittle under long-tail categories and low-visibility targets. Confidence overestimation on target data undermines pseudo-label quality, compounding errors during training.\n\nWe address this by coupling calibrated teacher-student training with tail-aware augmentation and adaptive confidence thresholds, improving recall on rare classes while reducing false positives.",
    "reason": "The span summarizes prior UDA techniques without clarifying their limitations or explicitly linking them to the proposed method’s focus.",
    "start": 235,
    "end": 671,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In the DARPA SubT challenge, teams relied on graph-based SLAM for subterranean mapping.",
    "document": "Related Work\n\nAutonomous exploration in GPS-denied environments depends on robust localization, mapping, and planning under uncertainty. Advances in visual–inertial odometry and pose graph optimization have improved reliability in challenging conditions (Forster et al., 2017; Mur-Artal and Tardós, 2017). Learning-based perception complements classical pipelines with robust semantics and loop closure cues (Sünderhauf et al., 2018).\n\nIn the DARPA SubT challenge, teams relied on graph-based SLAM for subterranean mapping. However, long-term autonomy in multi-branch tunnels still suffers from drift accumulation, sparse loop closures, and degraded sensing due to dust and darkness.\n\nWe propose a topology-aware front-end that prioritizes exploratory actions to maximize loop closure opportunities and reduce drift.",
    "reason": "This sentence references specific prior efforts in a named challenge without citing team reports or papers (violates rule a).",
    "start": 436,
    "end": 523,
    "label": "Unsupported_claim"
  },
  {
    "span": "On competitive programming problems, code LMs now solve more than 40% of tasks without unit tests.",
    "document": "Introduction\n\nLarge language models (LLMs) for code generation have rapidly advanced, enabling zero-shot and few-shot solutions to algorithmic problems. Beyond pass@k metrics, real-world utility depends on robustness to subtle specification ambiguities and adversarial inputs.\n\nOn competitive programming problems, code LMs now solve more than 40% of tasks without unit tests. Despite these gains, discrepancies between offline benchmarks and interactive judging environments persist, particularly for problems with complex I/O contracts.\n\nWe investigate prompt strategies and post-generation checks that reduce logical errors and improve alignment with problem constraints under limited feedback.",
    "reason": "Presents a precise performance statistic ('more than 40%') without citing the source studies or benchmarks supporting the number.",
    "start": 278,
    "end": 376,
    "label": "Unsupported_claim"
  },
  {
    "span": "(O'Neil et al 2014)",
    "document": "Introduction\n\nPrivacy-preserving machine learning employs cryptographic protocols, secure enclaves, and differential privacy (Dwork et al., 2006; Abadi et al., 2016; Ohrimenko et al., 2016). Federated learning reduces raw data sharing but still risks leakage through updates (McMahan et al., 2017; Melis et al., 2019). Auditing tools for disparate privacy loss have emerged (Jagielski et al., 2020), and formal analyses caution about composition over long training horizons (Mironov, 2017; Rogers et al., 2016). A complementary perspective links fairness and privacy (O'Neil et al 2014), emphasizing the social context of data collection.",
    "reason": "Improper 'et al' formatting in a parenthetical citation: missing period after 'al.' and missing comma before the year; should be '(O'Neil et al., 2014)'.",
    "start": 567,
    "end": 586,
    "label": "Format"
  },
  {
    "span": "Simulation-to-real transfer mitigates discrepancies between synthetic and physical domains (Rodriguez and Patel, 2019). Grasp synthesis can be approached through analytic metrics or learned affordances (Kim et al., 2020). Off-policy reinforcement learning improves sample efficiency (Liu and Hart, 2018).",
    "document": "Introduction\n\nRobotic manipulation in unstructured environments demands policies that generalize from limited demonstrations and imperfect simulators. Research threads span perception, control, and transfer.\n\nSimulation-to-real transfer mitigates discrepancies between synthetic and physical domains (Rodriguez and Patel, 2019). Grasp synthesis can be approached through analytic metrics or learned affordances (Kim et al., 2020). Off-policy reinforcement learning improves sample efficiency (Liu and Hart, 2018). Recent studies integrate tactile feedback for closed-loop correction (Suarez et al., 2021), yet the interplay with pretraining and data augmentation remains underexplored.\n\nOur approach unifies offline RL with shape-conditioned affordance priors to reduce real-world data needs.",
    "reason": "The span chains three sentences that introduce separate topics and citations without transitions or explicit relational statements, making the logical connection among them unclear across multiple sentences.",
    "start": 209,
    "end": 513,
    "label": "Coherence"
  },
  {
    "span": "Constraint-based, score-based, and functional causal model approaches have all been applied to EHR data (Spirtes et al., 2000; Chickering, 2002; Peters et al., 2017; Ghassami et al., 2018).",
    "document": "Introduction\n\nDiscovering causal relations from electronic health records (EHRs) supports decision-making and hypothesis generation but is complicated by confounding, irregular sampling, and missingness patterns. Robust estimation must also account for interventions and coding drift.\n\nCausal discovery families. Constraint-based, score-based, and functional causal model approaches have all been applied to EHR data (Spirtes et al., 2000; Chickering, 2002; Peters et al., 2017; Ghassami et al., 2018). Recent works exploit temporal structure, additive noise models, and invariances across environments to improve identification (Hyvärinen et al., 2010; Peters et al., 2016; Pfister et al., 2019).\n\nWe develop an environment-stratified discovery framework that leverages hospital-level policy shifts as natural experiments, aligning causal edges that are invariant across coding regimes while attenuating spurious associations tied to documentation practices.",
    "reason": "The span provides a list of method families and citations without clarifying their strengths, weaknesses, or the specific gap in EHR settings that the authors aim to address, hence lacking synthesis.",
    "start": 313,
    "end": 502,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Generative models for tabular data include TGAN, CTGAN, CopulaGAN, and TVAE (Xu et al., 2019; Patki et al., 2016; Kotelnikov et al., 2021). We extend CTGAN with additional regularizers.",
    "document": "Related Work\n\nSynthetic tabular data generation aims to preserve statistical properties and downstream task utility while protecting privacy. Challenges include mixed data types, rare categories, and preserving complex feature dependencies.\n\nGenerative models for tabular data include TGAN, CTGAN, CopulaGAN, and TVAE (Xu et al., 2019; Patki et al., 2016; Kotelnikov et al., 2021). We extend CTGAN with additional regularizers.\n\nRecent evaluations emphasize fidelity-utility trade-offs and robustness to mode collapse, highlighting the need for assessments beyond marginal distributions and basic classifiers.",
    "reason": "Transitions from listing prior models to stating the authors’ extension without explaining why CTGAN needs extension or what limitation is addressed (definition b).",
    "start": 242,
    "end": 427,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Neural program synthesis from natural language has advanced through large pretrained models, constrained decoders, and retrieval-augmented generation (Yin and Neubig, 2017; Chen et al., 2021; Guo et al., 2022). Benchmarks such as CONCODE, Spider, and MBPP provide diverse evaluation settings across languages and domains (Iyer et al., 2018; Yu et al., 2018; Austin et al., 2021).",
    "document": "Introduction\n\nMapping natural language intents to executable code promises to broaden access to programming. However, natural language is ambiguous, and code requires precise structures and constraints to compile and run correctly.\n\nNeural program synthesis from natural language has advanced through large pretrained models, constrained decoders, and retrieval-augmented generation (Yin and Neubig, 2017; Chen et al., 2021; Guo et al., 2022). Benchmarks such as CONCODE, Spider, and MBPP provide diverse evaluation settings across languages and domains (Iyer et al., 2018; Yu et al., 2018; Austin et al., 2021).\n\nThis work introduces execution-guided contrastive decoding that uses partial program runs to steer beam candidates away from semantic traps while preserving syntactic validity. We report consistent improvements in exact match and execution accuracy across SQL and Python datasets.",
    "reason": "The span summarizes prior systems and datasets without connecting them to the challenges the paper addresses or explaining the relevance to the proposed method, thus lacking synthesis (a, c).",
    "start": 233,
    "end": 612,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Statistical downscaling methods such as bias correction, quantile mapping, and weather typing have been extensively adopted to bridge resolution gaps between GCMs and local variables (Maraun et al., 2010; Themeßl et al., 2011; Vrac and Friederichs, 2015). Deep learning alternatives employ convolutional networks, U-Nets, and generative adversarial models to learn high-resolution fields from coarse inputs (Vandal et al., 2018; Pan et al., 2019; Stengel et al., 2020).",
    "document": "Introduction\n\nTranslating coarse-resolution climate projections into actionable local information is critical for impact assessment and adaptation planning. Downscaling methods attempt to infer fine-scale meteorological fields conditioned on large-scale predictors produced by general circulation models (GCMs).\n\nStatistical downscaling methods such as bias correction, quantile mapping, and weather typing have been extensively adopted to bridge resolution gaps between GCMs and local variables (Maraun et al., 2010; Themeßl et al., 2011; Vrac and Friederichs, 2015). Deep learning alternatives employ convolutional networks, U-Nets, and generative adversarial models to learn high-resolution fields from coarse inputs (Vandal et al., 2018; Pan et al., 2019; Stengel et al., 2020).\n\nKey challenges include nonstationarity under climate change, physical inconsistency across variables, and limited observations for training. We introduce PhysCons-Downscale, a learning approach that enforces multi-variable physical constraints via differentiable checks and adversarial priors. Experiments on precipitation and temperature show improved skill and cross-variable coherence under historical and scenario projections.",
    "reason": "The span lists prior statistical and deep learning downscaling methods without connecting them to the authors' objectives, identifying gaps, or stating a motivation. It lacks synthesis under (a) and (c).",
    "start": 313,
    "end": 782,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT was used in an AES setup trained on argumentative essays with prompt-specific heads.",
    "document": "Related Work\n\nAutomated essay scoring (AES) seeks to approximate human judgments of writing quality using computational models. Traditional approaches leveraged handcrafted features such as grammar, fluency, and discourse indicators. With the advent of pretrained language models, end-to-end neural methods have become prevalent. BERT was used in an AES setup trained on argumentative essays with prompt-specific heads. Other lines of work incorporate coherence signals, prompt sensitivity, and adversarial training to mitigate topic leakage. Nevertheless, model interpretability and fairness across demographics remain open challenges.",
    "reason": "Describes a specific modeling setup and dataset genre without citing the work that introduced it.",
    "start": 330,
    "end": 419,
    "label": "Unsupported_claim"
  },
  {
    "span": "Back-translation creates synthetic sources to improve fluency (Sennrich et al., 2016). Domain-adaptive pretraining tunes models on in-domain text (Gururangan et al., 2020). Character-level models handle noise (Belinkov and Bisk, 2018). Multilingual training shares parameters across languages (Johnson et al., 2017).",
    "document": "Introduction\n\nNeural machine translation (NMT) has benefited from architectural advances and large-scale pretraining, yet performance in low-resource and domain-shifted settings remains a core challenge. Techniques to leverage monolingual data, adapt to new domains, and exploit cross-lingual transfer have been widely studied (Edunov et al., 2018; Chu and Wang, 2018; Conneau et al., 2020).\n\nBack-translation creates synthetic sources to improve fluency (Sennrich et al., 2016). Domain-adaptive pretraining tunes models on in-domain text (Gururangan et al., 2020). Character-level models handle noise (Belinkov and Bisk, 2018). Multilingual training shares parameters across languages (Johnson et al., 2017).\n\nWe target domain-robust NMT by combining style-consistent back-translation with token-level uncertainty masking under a unified curriculum.",
    "reason": "The span strings together four distinct techniques without transitions or explicit relationships among them, leaving unclear how each sentence connects to the previous one.",
    "start": 393,
    "end": 709,
    "label": "Coherence"
  },
  {
    "span": "Unsupervised detection methods include reconstruction-based autoencoders, predictive forecasting errors, and density estimation with normalizing flows (Malhotra et al., 2016; Hundman et al., 2018; Kirschstein et al., 2021). Contrastive frameworks further improve representations by maximizing temporal agreement (Franceschi et al., 2019; Tonekaboni et al., 2021).",
    "document": "Introduction\n\nDetecting anomalies in time series is essential for monitoring safety-critical systems and large-scale services. Real-world deployments must contend with scarce labels, nonstationarity, and complex multivariate dependencies.\n\nUnsupervised detection methods include reconstruction-based autoencoders, predictive forecasting errors, and density estimation with normalizing flows (Malhotra et al., 2016; Hundman et al., 2018; Kirschstein et al., 2021). Contrastive frameworks further improve representations by maximizing temporal agreement (Franceschi et al., 2019; Tonekaboni et al., 2021).\n\nWe introduce Causal-Shape Anomaly Detector (CSAD), which couples shapelet-guided encoders with invariant risk minimization to improve robustness under distribution shifts. CSAD identifies stable causal features while down-weighting spurious temporal correlations.\n\nExperiments on industrial sensor datasets show that CSAD improves AUROC and reduces false alarms across changing operating regimes. We provide diagnostics for shapelet interpretability and shift sensitivity.",
    "reason": "The span lists categories of methods and citations but offers no explicit link to the authors’ approach, gap, or stance, exhibiting lack of synthesis (criteria a and c).",
    "start": 240,
    "end": 603,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Miller et al., 2014",
    "document": "Introduction\n\nMemory-augmented neural networks extend sequence models with explicit external storage to facilitate long-range reasoning. End-to-end differentiable memories enable iterative retrieval at inference time (Sukhbaatar et al., 2015; Weston et al., 2015). Efficient memory methods (Miller et al., 2014 have also been studied to reduce the computational burden of large key-value stores, complementing approximate nearest neighbor search techniques (Johnson et al., 2019). Hybrid architectures that blend attention with sparse retrieval further improve latency–accuracy trade-offs (Karpukhin et al., 2020; Borgeaud et al., 2022).\n\nWe build on these ideas by proposing a cache-aware memory layout that co-optimizes retrieval locality and representational fidelity.",
    "reason": "Unclosed parenthesis in the citation. It should be closed as \"(Miller et al., 2014)\".",
    "start": 290,
    "end": 310,
    "label": "Format"
  },
  {
    "span": "Caruana et al.",
    "document": "Related Work\n\nMultitask learning (MTL) aims to improve generalization by leveraging inductive transfer across related tasks. Early studies established that shared representations can reduce overfitting in low-data regimes and accelerate convergence. A seminal line of work by Caruana et al. showed that sharing parameters across tasks benefits both accuracy and sample efficiency when tasks are appropriately related. Subsequent approaches refine the notion of relatedness via task-specific adapters, soft parameter sharing, and uncertainty-driven loss balancing (Kendall et al., 2018; Standley et al., 2020). More recently, gradient surgery methods explicitly reduce negative transfer by projecting conflicting gradients (Yu et al., 2020), while routing-based architectures dynamically select which parameters to share (Ruder et al., 2019).",
    "reason": "Narrative citation is missing the publication year; it should appear as “Caruana et al. (1997)” or similar, depending on the correct year.",
    "start": 276,
    "end": 290,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claim that graph attention networks outperform message passing on toxicity prediction.",
    "document": "Related Work\n\nGraph neural networks have achieved strong performance across molecular property prediction, recommendation, and social network analysis (Gilmer et al., 2017; Hamilton et al., 2017). Architectural variants differ in how they aggregate neighborhood information, with attention mechanisms offering adaptive weighting of neighbors.\n\nIn a previous study, the authors claim that graph attention networks outperform message passing on toxicity prediction. Other lines of work explore spectral methods, higher-order neighborhoods, and subgraph pooling to improve expressivity (Dehmamy et al., 2019; Ying et al., 2018). Regularization via feature masking and edge dropout has been proposed to mitigate overfitting in sparse graphs (Rong et al., 2020).\n\nOur approach revisits attention-based aggregation with a focus on calibration and uncertainty estimation. We introduce temperature-controlled attention and evaluate across imbalanced graph benchmarks to assess reliability under distribution shift.",
    "reason": "Refers to a specific prior study and its claim but provides no citation to identify the work.",
    "start": 344,
    "end": 463,
    "label": "Unsupported_claim"
  },
  {
    "span": "[Chen et al., 2018)",
    "document": "Introduction\n\nNeural Machine Translation with Sparse Alignment Signals\n\nModern NMT systems employ attention to learn soft alignments between source and target sequences (Bahdanau et al., 2015; Vaswani et al., 2017). However, attention is not a faithful proxy for alignment (Jain and Wallace, 2019), motivating explicit alignment supervision and constraint-based decoding (Garg et al., 2019; Koehn and Knowles, 2017). As shown in [Chen et al., 2018), integrating external alignment priors can reduce hallucinations and improve adequacy, particularly in low-resource settings (Sennrich et al., 2016; Neubig and Hu, 2018). Building on this, we introduce a sparse alignment loss that regularizes attention maps using bilingual dictionaries derived from small seed lexicons.",
    "reason": "Mismatched bracket/parenthesis in citation: uses '[' to open and ')' to close.",
    "start": 429,
    "end": 448,
    "label": "Format"
  },
  {
    "span": "U-Net remains the dominant backbone in clinical deployments.",
    "document": "Introduction\n\nMedical image segmentation underpins numerous clinical workflows, from organ delineation to lesion quantification. Advances in encoder–decoder architectures and attention mechanisms have improved accuracy across modalities and anatomical targets.\n\nU-Net remains the dominant backbone in clinical deployments. Nevertheless, deployment constraints such as inference latency, memory footprint, and domain shift necessitate adaptations including lightweight decoders and self-supervised pretraining.\n\nWe present an efficient architecture that preserves accuracy while meeting real-time requirements on standard clinical hardware.",
    "reason": "Asserts a field-wide status claim about deployment prevalence without citing supporting evidence (rule b).",
    "start": 262,
    "end": 322,
    "label": "Unsupported_claim"
  },
  {
    "span": "It is widely known that Tree-structured Parzen Estimators underperform Gaussian-process Bayesian optimization when the evaluation budget is below 50 trials.",
    "document": "Related Work\n\nBayesian optimization (BO) has become the method of choice for hyperparameter tuning in expensive black-box settings. Gaussian-process (GP) surrogates with acquisition functions such as expected improvement offer strong sample efficiency, while scalable alternatives based on random forests and density estimation trade optimality for speed. It is widely known that Tree-structured Parzen Estimators underperform Gaussian-process Bayesian optimization when the evaluation budget is below 50 trials. Recent work studies heteroscedastic noise, multi-fidelity strategies, and transfer BO across tasks to further reduce cost.\n\nDespite algorithmic advances, practical performance depends on surrogate misspecification, search space design, and noisy, early-stopped evaluations. Benchmarking remains difficult due to variance across seeds, hardware, and pipeline choices. This underscores the need for standardized, reproducible evaluations with realistic budgets and mixed discrete–continuous spaces.",
    "reason": "Makes a niche comparative performance claim ('widely known' under specific budget) without citing supporting studies, violating rule (b).",
    "start": 356,
    "end": 512,
    "label": "Unsupported_claim"
  },
  {
    "span": "Back-translation remains the most effective data augmentation technique for neural machine translation.",
    "document": "Related Work\n\nData augmentation has long been used to mitigate overfitting in neural machine translation (NMT), especially for low-resource language pairs. Synthetic data generation methods attempt to expand parallel corpora without additional human annotation.\n\nBack-translation remains the most effective data augmentation technique for neural machine translation. Despite its popularity, naive back-translation can introduce artifacts, prompting research on filtering strategies and iterative refinement.",
    "reason": "Claims a superlative about prior methods without any citation or evidence; per rule (b), such field-specific comparative claims need citations.",
    "start": 263,
    "end": 366,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent graph transformers consistently outperform message-passing GNNs on molecular property prediction.",
    "document": "Related Work\n\nGraph neural networks (GNNs) based on message passing have achieved strong results on molecular property prediction by aggregating local neighborhood information. However, capturing long-range dependencies and higher-order interactions remains challenging. Transformer-style architectures adapted to graphs seek to address these limitations by employing global attention and positional encodings.\n\nRecent graph transformers consistently outperform message-passing GNNs on molecular property prediction. Nonetheless, training such models at scale can be computationally intensive, and their performance depends critically on the choice of graph encodings and pretraining strategies.\n\nWe introduce a lightweight hybrid architecture that augments message passing with sparse global attention, striking a balance between expressivity and efficiency.",
    "reason": "Makes a comparative performance claim about 'recent' models without citing any supporting studies.",
    "start": 412,
    "end": 516,
    "label": "Unsupported_claim"
  },
  {
    "span": "Vatswani et al.",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become the de facto approach for learning on relational data due to their ability to aggregate neighborhood information into expressive node and graph representations. Despite their success across chemistry, recommendation, and social network analysis, deeper GNNs often degrade in performance due to oversmoothing and oversquashing. As shown by Vatswani et al., message passing suffers from feature homogenization as depth increases, motivating architectures with residual connections, normalization, and decoupled propagation. Subsequent work explores scalable sampling strategies and positional encodings to mitigate structural information loss (Wu and Li, 2021; Chen et al., 2022). In this paper, we study depth-efficient GNNs with topology-aware normalization that preserves variance while stabilizing training.",
    "reason": "Narrative citation is missing the publication year; should be formatted as a narrative citation with year, e.g., \"Vatswani et al. (2020)\".",
    "start": 394,
    "end": 409,
    "label": "Format"
  },
  {
    "span": "Zhou and Li",
    "document": "Introduction\n\nAspect-based sentiment analysis (ABSA) aims to capture fine-grained opinions towards specific targets within a sentence. Early pipeline approaches identify aspects first and then assign sentiments (Pontiki et al., 2014), while joint models learn both simultaneously (Ma et al., 2018). Following Zhou and Li, we consider dependency paths as a key signal for aspect-sentiment interactions and integrate them into a span-based encoder. However, pipeline error propagation remains a challenge, especially for rare aspects (He et al., 2019).",
    "reason": "Narrative citation is missing the publication year; should be “Zhou and Li (YEAR)” with the appropriate year included.",
    "start": 309,
    "end": 320,
    "label": "Format"
  },
  {
    "span": "Brown et al., 2019)",
    "document": "Introduction\n\nAbstractive summarization has advanced with sequence-to-sequence models augmented by attention and copy mechanisms (See et al., 2017; Gehrmann et al., 2018). Large pretrained sequence models further improve fluency and factuality when fine-tuned on news corpora (Raffel et al., 2020; Lewis et al., 2020). Brown et al., 2019) demonstrate that scaling model and data can yield strong zero-shot summarization capabilities, but controlling length and coverage remains challenging. We address this by introducing a constraint-aware decoder that optimizes n-gram coverage and entity consistency via differentiable penalties.\n\nRelated Work\n\nConstraint decoding has been explored for translation and summarization using lexically constrained beam search (Hokamp and Liu, 2017) and grid beam search (Post and Vilar, 2018). Our approach differs by integrating soft penalties into the scoring function, enabling gradient-based calibration.",
    "reason": "Missing opening parenthesis in a parenthetical citation; should be '(Brown et al., 2019)' rather than 'Brown et al., 2019)'.",
    "start": 319,
    "end": 338,
    "label": "Format"
  },
  {
    "span": "Miller et al. 1",
    "document": "Introduction\n\nWe build on prior knowledge distillation methods to compress large teacher models into compact students for deployment. Miller et al. 1 introduce a teacher–student setup with temperature scaling, while Hinton et al. (2015) formalize the role of soft targets. Subsequent work explores intermediate feature matching (Romero et al., 2015) and data-free distillation (Lopes et al., 2017). Despite progress, calibration and robustness under distribution shift remain open challenges.",
    "reason": "Wrong use of footnote-like number without a year; should include the publication year or be formatted as a proper footnote.",
    "start": 134,
    "end": 149,
    "label": "Format"
  },
  {
    "span": "Smith et al., 2022)",
    "document": "Related Work. Neural dialog systems have progressed from sequence-to-sequence baselines to knowledge-grounded and retrieval-augmented models (Vinyals & Le, 2015; Dinan et al., 2019; Lewis et al., 2020). Personalization and long-context memory remain open challenges due to catastrophic forgetting and sparse supervision (Wolf et al., 2019; Roller et al., 2021). Evidence-based response generation integrates external tools for factuality and attribution (Rashkin et al., 2021; Shuster et al., 2021). Smith et al., 2022) propose iterative refinement via verifier feedback, while Gao et al. (2021) study few-shot prompting for task-oriented dialogs. We investigate hybrid retrieval–generation with confidence-aware control.",
    "reason": "Missing opening parenthesis in a parenthetical citation: 'Smith et al., 2022)' should be '(Smith et al., 2022)'.",
    "start": 500,
    "end": 519,
    "label": "Format"
  },
  {
    "span": "In (Lopez et al., 2019)",
    "document": "Introduction\n\nPretraining with self-supervised objectives has unified feature learning across modalities (Devlin et al., 2019; Radford et al., 2021). In (Lopez et al., 2019) the authors introduce a curriculum over masking rates to stabilize training for small datasets, while subsequent work adapts masking schedules to domain shift (Chen et al., 2020; Gururangan et al., 2020). Despite these advances, the interaction between task-specific adapters and backbone updates remains unclear (Houlsby et al., 2019; Pfeiffer et al., 2021). Our study revisits adapter placement and regularization under limited supervision and cross-domain transfer.",
    "reason": "Wrong citation style using a preposition with a parenthetical citation; narrative form should be 'In Lopez et al. (2019) ...'.",
    "start": 150,
    "end": 173,
    "label": "Format"
  },
  {
    "span": "Early molecular property predictors used graph convolutions and message passing networks (Duvenaud et al., 2015; Gilmer et al., 2017; Kearnes et al., 2016; Yang et al., 2019). Subsequent approaches integrated attention mechanisms and edge updates to refine representations (Velickovic et al., 2018; Hu et al., 2020; Rong et al., 2020). Self-supervised pretraining on large compound libraries further improved performance (Hu et al., 2019; Sun et al., 2020; Fang et al., 2022).",
    "document": "Introduction\n\nPredicting molecular properties from structure underpins drug discovery, materials design, and toxicity screening. Graph-based neural networks have emerged as a natural modeling paradigm because molecules are well-represented as graphs of atoms and bonds. Despite progress, accurately generalizing across chemical space with limited labeled data remains challenging.\n\nEarly molecular property predictors used graph convolutions and message passing networks (Duvenaud et al., 2015; Gilmer et al., 2017; Kearnes et al., 2016; Yang et al., 2019). Subsequent approaches integrated attention mechanisms and edge updates to refine representations (Velickovic et al., 2018; Hu et al., 2020; Rong et al., 2020). Self-supervised pretraining on large compound libraries further improved performance (Hu et al., 2019; Sun et al., 2020; Fang et al., 2022).\n\nIn this paper, we study how structure-aware pretraining objectives interact with message passing depth under strict label budgets. We introduce a simple scaling law analysis for molecular GNNs and propose a hybrid supervision strategy that couples contrastive pretraining with task-specific adapters. Our experiments across medicinal chemistry and physical property benchmarks demonstrate consistent gains in the low-data regime.",
    "reason": "The span only enumerates prior works and trends without explaining how they relate to or motivate the authors' approach, failing to connect literature to the paper's argument.",
    "start": 382,
    "end": 858,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Several studies have explored different noise mechanisms and clipping strategies for DP-SGD in text classification (Abadi et al., 2016; McMahan et al., 2018; Yu et al., 2021; Subramani et al., 2022).",
    "document": "Introduction\n\nProtecting user privacy in natural language processing (NLP) is increasingly important as models memorize sensitive data. Differential privacy (DP) provides formal guarantees against membership inference by injecting calibrated randomness during training.\n\nRelated Work\n\nSeveral studies have explored different noise mechanisms and clipping strategies for DP-SGD in text classification (Abadi et al., 2016; McMahan et al., 2018; Yu et al., 2021; Subramani et al., 2022). Privacy amplification via subsampling and client-level DP have been applied to federated NLP tasks (McMahan et al., 2017; Truex et al., 2019). Recent works examine utility degradation under DP for large language models and propose adaptive clipping heuristics (Bu et al., 2020; Li et al., 2022).\n\nWe introduce a token-sensitivity-aware clipping schedule that allocates privacy budget by gradient attribution to mitigate over-noising of salient tokens.",
    "reason": "The span summarizes prior DP-SGD variants without connecting them to the paper’s motivation or clarifying what remains unsolved.",
    "start": 285,
    "end": 484,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There is a growing consensus that BLEU is inadequate for evaluating instruction-following models.",
    "document": "Introduction\n\nLarge language models have rapidly advanced the state of the art in open-ended generation and task-oriented dialogue. Traditional n-gram metrics remain popular due to simplicity and speed, yet they may fail to capture semantic adequacy and adherence to instructions. There is a growing consensus that BLEU is inadequate for evaluating instruction-following models. In response, recent work advocates for reference-free metrics, human preference modeling, and task-specific checklists.\n\nDespite progress, trustworthy evaluation remains elusive due to prompt sensitivity, output diversity, and alignment trade-offs. We propose a multi-faceted evaluation suite combining automatic reward models, targeted adversarial prompts, and constrained human protocols to better reflect utility and safety.\n\nOur results show improved agreement with expert judgments and greater sensitivity to instruction violations compared to strong baselines.",
    "reason": "Asserts a field-wide consensus and critiques BLEU without citing any supporting literature.",
    "start": 281,
    "end": 378,
    "label": "Unsupported_claim"
  },
  {
    "span": "BraTS challenge",
    "document": "Introduction\n\nAutomated brain tumor segmentation in multi-modal MRI is a critical step for treatment planning and longitudinal assessment. Recent models emphasize multi-scale context aggregation, attention mechanisms, and uncertainty estimation to handle heterogeneous lesion appearance.\n\nBenchmarking in this area is strongly influenced by the BraTS challenge, which standardizes data preprocessing protocols and evaluation metrics across institutions. Despite substantial progress, performance can degrade under scanner shifts and protocol variations not captured by the training distribution.\n\nWe propose an adaptive normalization module that conditions on estimated acquisition parameters to reduce domain shift at test time. Extensive experiments demonstrate improved robustness across unseen sites while maintaining competitive Dice scores on in-distribution scans.",
    "reason": "The mention of the 'BraTS challenge' is a reference to a specific benchmark/competition but lacks a citation at first mention, violating rule a.",
    "start": 345,
    "end": 360,
    "label": "Unsupported_claim"
  },
  {
    "span": "Li and Gomez",
    "document": "Related Work\n\nMultimodal summarization combines textual and visual signals to produce concise yet informative synopses of long documents. Early extractive approaches ranked sentences based on lexical salience and visual anchors (Nakamura, 2020; Ahmed and Rossi, 2022). More recent abstractive methods condition generation on fused embeddings learned from cross-modal transformers (Chen et al., 2019; Duarte et al., 2021).\n\nFollowing Li and Gomez, many systems pair visual objects with noun phrases to improve faithfulness, while others introduce coverage penalties to reduce hallucinations (Ortiz et al., 2020; Banerjee et al., 2022). Concurrently, evaluation has moved beyond ROUGE to include image-text alignment metrics and human preference studies (Khan and Walters, 2023). Despite progress, current models still struggle with grounding and temporal coherence, motivating our approach.\n",
    "reason": "Narrative citation missing year; should be 'Li and Gomez (YEAR)'.",
    "start": 433,
    "end": 445,
    "label": "Format"
  },
  {
    "span": "The notion of calibration error has been extensively studied in active learning.",
    "document": "Introduction\n\nReliable uncertainty estimates are essential for decision-making in safety-critical applications. Calibration measures the agreement between predicted probabilities and empirical correctness and is often summarized by metrics such as expected calibration error (ECE).\n\nThe notion of calibration error has been extensively studied in active learning. However, most work focuses on pool-based sampling where the unlabeled set is assumed static, leaving streaming and non-stationary settings underexplored. Furthermore, existing studies frequently conflate uncertainty with calibration, leading to misleading acquisition strategies.\n\nWe disentangle calibration from uncertainty estimation within active learning by proposing acquisition functions that directly optimize calibrated confidence while controlling for label efficiency.",
    "reason": "This general prior-work claim about a niche intersection (calibration in active learning) lacks citations to foundational or recent studies (violates rule b/d).",
    "start": 283,
    "end": 363,
    "label": "Unsupported_claim"
  },
  {
    "span": "Deep learning, to a large extent, has freed data scientists from doing feature engineering, which has been one of the essential obstacles to annotation with AL",
    "document": "Related Work\n\nDeep learning, to a large extent, has freed data scientists from doing feature engineering, which has been one of the essential obstacles to annotation with AL. This advantage has sparked a series of works on deep active learning (DAL) in natural language processing (NLP). Shen et al. (2017) conduct one of the first investigations on DAL in sequence tagging tasks. They propose an efficient way of quantifying the uncertainty of sentences, namely maximal normalized log probability (MNLP), by averaging log probabilities of their tokens. They also address the problem of excessive duration of a neural network training step during an AL iteration by interleaving online learning with training from scratch. In our work, we take MNLP as a query strategy for experiments on sequence tagging tasks since it has demonstrated a good trade-off between quality and computational performance. We consider that online learning can potentially be used as a complement to our algorithms. Since the most time-consuming part of an AL iteration can be model inference instead of training, in this work, we also pay attention to the acceleration of the inference step.\n\nSeveral recent publications investigate deep pretrained models based on the Transformer architecture (Vaswani et al., 2017), ELMo (Peters et al., 2018), and ULMFiT (Howard and Ruder, 2018) in AL on NLP tasks (Prabhu et al., 2019;Ein-Dor et al., 2020;Yuan et al., 2020;Shelmanov et al., 2021). We continue this line of works by relying on pre-trained Transformers since this architecture has been shown promising for AL in NLP due to its good qualitative and computational performance.\n\nA few works have experimented with Bayesian query strategies for AL. Shen et al. (2017), Sid-dhant and Lipton (2018), Ein-Dor et al. (2020), and Shelmanov et al. (2021 leverage Monte Carlo dropout (Gal and Ghahramani, 2016) for quantifying uncertainty of models. Siddhant and Lipton (2018) also apply the Bayes by backprop algorithm (Blundell et al., 2015) for performing variational inference of a Bayesian neural network. This approach demonstrates the best improvements upon the baseline but introduces large computational overhead both for training and uncertainty estimation of a model, as well as the memory overhead for storing parameters of a Bayesian neural network. The query strategies based on Monte Carlo dropout do not affect the model training procedure and do not change the memory footprint. However, they also suffer from slow uncertainty estimation due to the necessity of making multiple stochastic predictions, while their empirical evaluations with Transformers in recent works (Ein-Dor et al., 2020;Shelmanov et al., 2021) do not demonstrate big advantages. Therefore, we do not use Bayesian query strategies in our experiments and adhere to the classical uncertainty-based query strategies.\n\nRecently proposed alternatives to uncertaintybased query strategies leverage reinforcement learning and imitation learning (Fang et al., 2017;Liu et al., 2018;Vu et al., 2019;Brantley et al., 2020). This series of works aims at constructing trainable policy-based query strategies. However, this requires an excessive amount of computation while the transferability of learned policies across domains and tasks is underresearched.\n\nFinally, Lowell et al. ( 2019) question the usefulness of AL techniques in general. They demonstrate that due to the ASM problem, AL can be even detrimental to the performance of the successor. This finding is also revealed for classical machine learning models by Baldridge and Osborne (2004), Tomanek andMorik (2011), Hu et al. (2016) and supported by experiments with Transformers in (Shelmanov et al., 2021). Our work directly addresses the question raised by Lowell et al. (2019) and suggests a simple solution to the ASM problem. Moreover, we combine it with the method proposed by Shelmanov et al. (2021), who suggest using distilled models for instance acquisition and their teacher models as successors.\n\n ",
    "start": 14,
    "end": 173,
    "label": "Unsupported_claim"
  },
  {
    "span": " ASM problem",
    "document": "Related Work\n\nDeep learning, to a large extent, has freed data scientists from doing feature engineering, which has been one of the essential obstacles to annotation with AL. This advantage has sparked a series of works on deep active learning (DAL) in natural language processing (NLP). Shen et al. (2017) conduct one of the first investigations on DAL in sequence tagging tasks. They propose an efficient way of quantifying the uncertainty of sentences, namely maximal normalized log probability (MNLP), by averaging log probabilities of their tokens. They also address the problem of excessive duration of a neural network training step during an AL iteration by interleaving online learning with training from scratch. In our work, we take MNLP as a query strategy for experiments on sequence tagging tasks since it has demonstrated a good trade-off between quality and computational performance. We consider that online learning can potentially be used as a complement to our algorithms. Since the most time-consuming part of an AL iteration can be model inference instead of training, in this work, we also pay attention to the acceleration of the inference step.\n\nSeveral recent publications investigate deep pretrained models based on the Transformer architecture (Vaswani et al., 2017), ELMo (Peters et al., 2018), and ULMFiT (Howard and Ruder, 2018) in AL on NLP tasks (Prabhu et al., 2019;Ein-Dor et al., 2020;Yuan et al., 2020;Shelmanov et al., 2021). We continue this line of works by relying on pre-trained Transformers since this architecture has been shown promising for AL in NLP due to its good qualitative and computational performance.\n\nA few works have experimented with Bayesian query strategies for AL. Shen et al. (2017), Sid-dhant and Lipton (2018), Ein-Dor et al. (2020), and Shelmanov et al. (2021 leverage Monte Carlo dropout (Gal and Ghahramani, 2016) for quantifying uncertainty of models. Siddhant and Lipton (2018) also apply the Bayes by backprop algorithm (Blundell et al., 2015) for performing variational inference of a Bayesian neural network. This approach demonstrates the best improvements upon the baseline but introduces large computational overhead both for training and uncertainty estimation of a model, as well as the memory overhead for storing parameters of a Bayesian neural network. The query strategies based on Monte Carlo dropout do not affect the model training procedure and do not change the memory footprint. However, they also suffer from slow uncertainty estimation due to the necessity of making multiple stochastic predictions, while their empirical evaluations with Transformers in recent works (Ein-Dor et al., 2020;Shelmanov et al., 2021) do not demonstrate big advantages. Therefore, we do not use Bayesian query strategies in our experiments and adhere to the classical uncertainty-based query strategies.\n\nRecently proposed alternatives to uncertaintybased query strategies leverage reinforcement learning and imitation learning (Fang et al., 2017;Liu et al., 2018;Vu et al., 2019;Brantley et al., 2020). This series of works aims at constructing trainable policy-based query strategies. However, this requires an excessive amount of computation while the transferability of learned policies across domains and tasks is underresearched.\n\nFinally, Lowell et al. ( 2019) question the usefulness of AL techniques in general. They demonstrate that due to the ASM problem, AL can be even detrimental to the performance of the successor. This finding is also revealed for classical machine learning models by Baldridge and Osborne (2004), Tomanek andMorik (2011), Hu et al. (2016) and supported by experiments with Transformers in (Shelmanov et al., 2021). Our work directly addresses the question raised by Lowell et al. (2019) and suggests a simple solution to the ASM problem. Moreover, we combine it with the method proposed by Shelmanov et al. (2021), who suggest using distilled models for instance acquisition and their teacher models as successors.\n\n ",
    "start": 3827,
    "end": 3839,
    "label": "Unsupported_claim"
  },
  {
    "span": "We continue this line of works by relying on pre-trained Transformers since this architecture has been shown promising for AL in NLP due to its good qualitative and computational performance.",
    "document": "Related Work\n\nDeep learning, to a large extent, has freed data scientists from doing feature engineering, which has been one of the essential obstacles to annotation with AL. This advantage has sparked a series of works on deep active learning (DAL) in natural language processing (NLP). Shen et al. (2017) conduct one of the first investigations on DAL in sequence tagging tasks. They propose an efficient way of quantifying the uncertainty of sentences, namely maximal normalized log probability (MNLP), by averaging log probabilities of their tokens. They also address the problem of excessive duration of a neural network training step during an AL iteration by interleaving online learning with training from scratch. In our work, we take MNLP as a query strategy for experiments on sequence tagging tasks since it has demonstrated a good trade-off between quality and computational performance. We consider that online learning can potentially be used as a complement to our algorithms. Since the most time-consuming part of an AL iteration can be model inference instead of training, in this work, we also pay attention to the acceleration of the inference step.\n\nSeveral recent publications investigate deep pretrained models based on the Transformer architecture (Vaswani et al., 2017), ELMo (Peters et al., 2018), and ULMFiT (Howard and Ruder, 2018) in AL on NLP tasks (Prabhu et al., 2019;Ein-Dor et al., 2020;Yuan et al., 2020;Shelmanov et al., 2021). We continue this line of works by relying on pre-trained Transformers since this architecture has been shown promising for AL in NLP due to its good qualitative and computational performance.\n\nA few works have experimented with Bayesian query strategies for AL. Shen et al. (2017), Sid-dhant and Lipton (2018), Ein-Dor et al. (2020), and Shelmanov et al. (2021 leverage Monte Carlo dropout (Gal and Ghahramani, 2016) for quantifying uncertainty of models. Siddhant and Lipton (2018) also apply the Bayes by backprop algorithm (Blundell et al., 2015) for performing variational inference of a Bayesian neural network. This approach demonstrates the best improvements upon the baseline but introduces large computational overhead both for training and uncertainty estimation of a model, as well as the memory overhead for storing parameters of a Bayesian neural network. The query strategies based on Monte Carlo dropout do not affect the model training procedure and do not change the memory footprint. However, they also suffer from slow uncertainty estimation due to the necessity of making multiple stochastic predictions, while their empirical evaluations with Transformers in recent works (Ein-Dor et al., 2020;Shelmanov et al., 2021) do not demonstrate big advantages. Therefore, we do not use Bayesian query strategies in our experiments and adhere to the classical uncertainty-based query strategies.\n\nRecently proposed alternatives to uncertaintybased query strategies leverage reinforcement learning and imitation learning (Fang et al., 2017;Liu et al., 2018;Vu et al., 2019;Brantley et al., 2020). This series of works aims at constructing trainable policy-based query strategies. However, this requires an excessive amount of computation while the transferability of learned policies across domains and tasks is underresearched.\n\nFinally, Lowell et al. ( 2019) question the usefulness of AL techniques in general. They demonstrate that due to the ASM problem, AL can be even detrimental to the performance of the successor. This finding is also revealed for classical machine learning models by Baldridge and Osborne (2004), Tomanek andMorik (2011), Hu et al. (2016) and supported by experiments with Transformers in (Shelmanov et al., 2021). Our work directly addresses the question raised by Lowell et al. (2019) and suggests a simple solution to the ASM problem. Moreover, we combine it with the method proposed by Shelmanov et al. (2021), who suggest using distilled models for instance acquisition and their teacher models as successors.\n\n ",
    "start": 1464,
    "end": 1655,
    "label": "Unsupported_claim"
  },
  {
    "span": "Shelmanov et al. (2021",
    "document": "Related Work\n\nDeep learning, to a large extent, has freed data scientists from doing feature engineering, which has been one of the essential obstacles to annotation with AL. This advantage has sparked a series of works on deep active learning (DAL) in natural language processing (NLP). Shen et al. (2017) conduct one of the first investigations on DAL in sequence tagging tasks. They propose an efficient way of quantifying the uncertainty of sentences, namely maximal normalized log probability (MNLP), by averaging log probabilities of their tokens. They also address the problem of excessive duration of a neural network training step during an AL iteration by interleaving online learning with training from scratch. In our work, we take MNLP as a query strategy for experiments on sequence tagging tasks since it has demonstrated a good trade-off between quality and computational performance. We consider that online learning can potentially be used as a complement to our algorithms. Since the most time-consuming part of an AL iteration can be model inference instead of training, in this work, we also pay attention to the acceleration of the inference step.\n\nSeveral recent publications investigate deep pretrained models based on the Transformer architecture (Vaswani et al., 2017), ELMo (Peters et al., 2018), and ULMFiT (Howard and Ruder, 2018) in AL on NLP tasks (Prabhu et al., 2019;Ein-Dor et al., 2020;Yuan et al., 2020;Shelmanov et al., 2021). We continue this line of works by relying on pre-trained Transformers since this architecture has been shown promising for AL in NLP due to its good qualitative and computational performance.\n\nA few works have experimented with Bayesian query strategies for AL. Shen et al. (2017), Sid-dhant and Lipton (2018), Ein-Dor et al. (2020), and Shelmanov et al. (2021 leverage Monte Carlo dropout (Gal and Ghahramani, 2016) for quantifying uncertainty of models. Siddhant and Lipton (2018) also apply the Bayes by backprop algorithm (Blundell et al., 2015) for performing variational inference of a Bayesian neural network. This approach demonstrates the best improvements upon the baseline but introduces large computational overhead both for training and uncertainty estimation of a model, as well as the memory overhead for storing parameters of a Bayesian neural network. The query strategies based on Monte Carlo dropout do not affect the model training procedure and do not change the memory footprint. However, they also suffer from slow uncertainty estimation due to the necessity of making multiple stochastic predictions, while their empirical evaluations with Transformers in recent works (Ein-Dor et al., 2020;Shelmanov et al., 2021) do not demonstrate big advantages. Therefore, we do not use Bayesian query strategies in our experiments and adhere to the classical uncertainty-based query strategies.\n\nRecently proposed alternatives to uncertaintybased query strategies leverage reinforcement learning and imitation learning (Fang et al., 2017;Liu et al., 2018;Vu et al., 2019;Brantley et al., 2020). This series of works aims at constructing trainable policy-based query strategies. However, this requires an excessive amount of computation while the transferability of learned policies across domains and tasks is underresearched.\n\nFinally, Lowell et al. ( 2019) question the usefulness of AL techniques in general. They demonstrate that due to the ASM problem, AL can be even detrimental to the performance of the successor. This finding is also revealed for classical machine learning models by Baldridge and Osborne (2004), Tomanek andMorik (2011), Hu et al. (2016) and supported by experiments with Transformers in (Shelmanov et al., 2021). Our work directly addresses the question raised by Lowell et al. (2019) and suggests a simple solution to the ASM problem. Moreover, we combine it with the method proposed by Shelmanov et al. (2021), who suggest using distilled models for instance acquisition and their teacher models as successors.\n\n ",
    "start": 1802,
    "end": 1824,
    "label": "Format"
  },
  {
    "span": " (Shelmanov et al., 2021).",
    "document": "Related Work\n\nDeep learning, to a large extent, has freed data scientists from doing feature engineering, which has been one of the essential obstacles to annotation with AL. This advantage has sparked a series of works on deep active learning (DAL) in natural language processing (NLP). Shen et al. (2017) conduct one of the first investigations on DAL in sequence tagging tasks. They propose an efficient way of quantifying the uncertainty of sentences, namely maximal normalized log probability (MNLP), by averaging log probabilities of their tokens. They also address the problem of excessive duration of a neural network training step during an AL iteration by interleaving online learning with training from scratch. In our work, we take MNLP as a query strategy for experiments on sequence tagging tasks since it has demonstrated a good trade-off between quality and computational performance. We consider that online learning can potentially be used as a complement to our algorithms. Since the most time-consuming part of an AL iteration can be model inference instead of training, in this work, we also pay attention to the acceleration of the inference step.\n\nSeveral recent publications investigate deep pretrained models based on the Transformer architecture (Vaswani et al., 2017), ELMo (Peters et al., 2018), and ULMFiT (Howard and Ruder, 2018) in AL on NLP tasks (Prabhu et al., 2019;Ein-Dor et al., 2020;Yuan et al., 2020;Shelmanov et al., 2021). We continue this line of works by relying on pre-trained Transformers since this architecture has been shown promising for AL in NLP due to its good qualitative and computational performance.\n\nA few works have experimented with Bayesian query strategies for AL. Shen et al. (2017), Sid-dhant and Lipton (2018), Ein-Dor et al. (2020), and Shelmanov et al. (2021 leverage Monte Carlo dropout (Gal and Ghahramani, 2016) for quantifying uncertainty of models. Siddhant and Lipton (2018) also apply the Bayes by backprop algorithm (Blundell et al., 2015) for performing variational inference of a Bayesian neural network. This approach demonstrates the best improvements upon the baseline but introduces large computational overhead both for training and uncertainty estimation of a model, as well as the memory overhead for storing parameters of a Bayesian neural network. The query strategies based on Monte Carlo dropout do not affect the model training procedure and do not change the memory footprint. However, they also suffer from slow uncertainty estimation due to the necessity of making multiple stochastic predictions, while their empirical evaluations with Transformers in recent works (Ein-Dor et al., 2020;Shelmanov et al., 2021) do not demonstrate big advantages. Therefore, we do not use Bayesian query strategies in our experiments and adhere to the classical uncertainty-based query strategies.\n\nRecently proposed alternatives to uncertaintybased query strategies leverage reinforcement learning and imitation learning (Fang et al., 2017;Liu et al., 2018;Vu et al., 2019;Brantley et al., 2020). This series of works aims at constructing trainable policy-based query strategies. However, this requires an excessive amount of computation while the transferability of learned policies across domains and tasks is underresearched.\n\nFinally, Lowell et al. ( 2019) question the usefulness of AL techniques in general. They demonstrate that due to the ASM problem, AL can be even detrimental to the performance of the successor. This finding is also revealed for classical machine learning models by Baldridge and Osborne (2004), Tomanek andMorik (2011), Hu et al. (2016) and supported by experiments with Transformers in (Shelmanov et al., 2021). Our work directly addresses the question raised by Lowell et al. (2019) and suggests a simple solution to the ASM problem. Moreover, we combine it with the method proposed by Shelmanov et al. (2021), who suggest using distilled models for instance acquisition and their teacher models as successors.\n\n ",
    "start": 3691,
    "end": 3717,
    "label": "Format"
  },
  {
    "span": "Recently proposed alternatives to uncertaintybased query strategies leverage reinforcement learning and imitation learning (Fang et al., 2017;Liu et al., 2018;Vu et al., 2019;Brantley et al., 2020). This series of works aims at constructing trainable policy-based query strategies. However, this requires an excessive amount of computation while the transferability of learned policies across domains and tasks is underresearched.",
    "document": "Related Work\n\nDeep learning, to a large extent, has freed data scientists from doing feature engineering, which has been one of the essential obstacles to annotation with AL. This advantage has sparked a series of works on deep active learning (DAL) in natural language processing (NLP). Shen et al. (2017) conduct one of the first investigations on DAL in sequence tagging tasks. They propose an efficient way of quantifying the uncertainty of sentences, namely maximal normalized log probability (MNLP), by averaging log probabilities of their tokens. They also address the problem of excessive duration of a neural network training step during an AL iteration by interleaving online learning with training from scratch. In our work, we take MNLP as a query strategy for experiments on sequence tagging tasks since it has demonstrated a good trade-off between quality and computational performance. We consider that online learning can potentially be used as a complement to our algorithms. Since the most time-consuming part of an AL iteration can be model inference instead of training, in this work, we also pay attention to the acceleration of the inference step.\n\nSeveral recent publications investigate deep pretrained models based on the Transformer architecture (Vaswani et al., 2017), ELMo (Peters et al., 2018), and ULMFiT (Howard and Ruder, 2018) in AL on NLP tasks (Prabhu et al., 2019;Ein-Dor et al., 2020;Yuan et al., 2020;Shelmanov et al., 2021). We continue this line of works by relying on pre-trained Transformers since this architecture has been shown promising for AL in NLP due to its good qualitative and computational performance.\n\nA few works have experimented with Bayesian query strategies for AL. Shen et al. (2017), Sid-dhant and Lipton (2018), Ein-Dor et al. (2020), and Shelmanov et al. (2021 leverage Monte Carlo dropout (Gal and Ghahramani, 2016) for quantifying uncertainty of models. Siddhant and Lipton (2018) also apply the Bayes by backprop algorithm (Blundell et al., 2015) for performing variational inference of a Bayesian neural network. This approach demonstrates the best improvements upon the baseline but introduces large computational overhead both for training and uncertainty estimation of a model, as well as the memory overhead for storing parameters of a Bayesian neural network. The query strategies based on Monte Carlo dropout do not affect the model training procedure and do not change the memory footprint. However, they also suffer from slow uncertainty estimation due to the necessity of making multiple stochastic predictions, while their empirical evaluations with Transformers in recent works (Ein-Dor et al., 2020;Shelmanov et al., 2021) do not demonstrate big advantages. Therefore, we do not use Bayesian query strategies in our experiments and adhere to the classical uncertainty-based query strategies.\n\nRecently proposed alternatives to uncertaintybased query strategies leverage reinforcement learning and imitation learning (Fang et al., 2017;Liu et al., 2018;Vu et al., 2019;Brantley et al., 2020). This series of works aims at constructing trainable policy-based query strategies. However, this requires an excessive amount of computation while the transferability of learned policies across domains and tasks is underresearched.\n\nFinally, Lowell et al. ( 2019) question the usefulness of AL techniques in general. They demonstrate that due to the ASM problem, AL can be even detrimental to the performance of the successor. This finding is also revealed for classical machine learning models by Baldridge and Osborne (2004), Tomanek andMorik (2011), Hu et al. (2016) and supported by experiments with Transformers in (Shelmanov et al., 2021). Our work directly addresses the question raised by Lowell et al. (2019) and suggests a simple solution to the ASM problem. Moreover, we combine it with the method proposed by Shelmanov et al. (2021), who suggest using distilled models for instance acquisition and their teacher models as successors.\n\n ",
    "start": 2873,
    "end": 3303,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Stylometric features capture lexical and syntactic cues indicative of deceptive writing (Ott et al., 2011; Pérez-Rosas et al., 2018). Propagation patterns over social networks model how misinformation spreads across communities (Shu et al., 2019). Evidence retrieval systems query external sources to verify claims (Thorne et al., 2018). User credibility priors leverage history of accounts to weight signals (Tacchini et al., 2017).",
    "document": "Related Work\n\nFake News and Misinformation Detection\nDetecting misinformation involves modeling text content, social context, and external evidence under dynamic, adversarial settings (Zhou and Zafarani, 2020). Approaches differ widely in data assumptions and the granularity of labels.\n\nContent, Network, and Evidence Signals\nStylometric features capture lexical and syntactic cues indicative of deceptive writing (Ott et al., 2011; Pérez-Rosas et al., 2018). Propagation patterns over social networks model how misinformation spreads across communities (Shu et al., 2019). Evidence retrieval systems query external sources to verify claims (Thorne et al., 2018). User credibility priors leverage history of accounts to weight signals (Tacchini et al., 2017). Recent multi-modal models integrate images and video, but evaluation is confounded by topic leakage and temporal drift (Jin et al., 2017; Khoo et al., 2020).\n\nEvaluation and Robustness\nBenchmarks like FEVER and LIAR vary in annotation quality and claim granularity, complicating comparisons (Wang, 2017; Thorne et al., 2018). We propose time-split evaluations and debiased negative sampling to test robustness to shifting narratives.",
    "reason": "The span jumps among textual features, network diffusion, retrieval-based verification, and user priors without transitions or explaining how these signals interact, causing coherence issues.",
    "start": 327,
    "end": 760,
    "label": "Coherence"
  },
  {
    "span": "As reported in a previous study, z-score thresholding performs poorly on non-stationary signals.",
    "document": "Introduction\n\nTime series anomaly detection underpins monitoring in industrial IoT, finance, and healthcare. Classical methods typically assume stationarity and simple noise models, while modern deep architectures aim to learn flexible representations that adapt to evolving patterns (Audibert et al., 2020).\n\nAs reported in a previous study, z-score thresholding performs poorly on non-stationary signals. Nevertheless, its simplicity and interpretability make it a common baseline, and understanding when it fails is important for designing robust detectors.\n\nWe provide a unified analysis of distribution shifts in streaming settings, comparing windowed normalization, quantile-based thresholds, and adaptive estimators. Our results highlight conditions under which simple baselines can be competitive and when learned models provide significant gains.",
    "reason": "It references a 'previous study' to support a performance claim without providing a citation to that study.",
    "start": 310,
    "end": 406,
    "label": "Unsupported_claim"
  },
  {
    "span": "Classical SLAM fuses odometry and landmarks (Dissanayake et al., 2001). End-to-end imitation policies clone expert driving (Pomerleau, 1989). Curriculum training eases exploration (Bengio et al., 2009). Mapless navigation uses depth images (Zhu et al., 2017).",
    "document": "Related Work\n\nNavigation methods in robotics range from geometry-centric mapping and planning to learning-based policies operating directly from perception. Simultaneous localization and mapping (SLAM) constructs a consistent map while estimating pose, whereas learned policies attempt to bypass explicit mapping by optimizing for task return (Thrun et al., 2005; Levine et al., 2016).\n\nClassical SLAM fuses odometry and landmarks (Dissanayake et al., 2001). End-to-end imitation policies clone expert driving (Pomerleau, 1989). Curriculum training eases exploration (Bengio et al., 2009). Mapless navigation uses depth images (Zhu et al., 2017).\n\nOur method integrates a topological memory with a value-aware planner, bridging global reasoning and reactive control in partially observable environments.",
    "reason": "The span jumps among SLAM, imitation learning, curriculum design, and mapless navigation without transitions or an explicit explanation of how each is related, breaking coherence.",
    "start": 387,
    "end": 646,
    "label": "Coherence"
  },
  {
    "span": "[Klein, 2016]",
    "document": "Related Work\n\nDomain adaptation in machine translation has leveraged data selection, fine-tuning, and multi-domain modeling. Early selection techniques prioritized in-domain sentences by cross-entropy difference (Moore and Lewis, 2010). Fine-tuning pre-trained NMT models on small in-domain corpora remains a strong baseline (Luong and Manning, 2015). Multi-domain mixtures with domain tags enable parameter sharing (Chu and Wang, 2018). Backtranslation generates synthetic target-side data to improve coverage (Sennrich et al., 2016). Recently, constrained decoding and terminology injection have improved adequacy in specialized domains (Dinu et al., 2019). Curriculum learning gradually introduces domain difficulty to stabilize adaptation (Wang and Neubig, 2019), as shown in [Klein, 2016] for curriculum design effects. Our approach unifies curriculum with uncertainty-aware sampling to minimize catastrophic forgetting during rapid domain shifts (Rao and Singh, 2021).",
    "reason": "Wrong citation style; square brackets are used as if numeric style, but the surrounding text follows author–year style and should be '(Klein, 2016)'.",
    "start": 780,
    "end": 793,
    "label": "Format"
  },
  {
    "span": "In (Smith et al., 2018)",
    "document": "Related Work\n\nNeural machine translation has advanced rapidly with attention-based encoder–decoder models (Bahdanau et al., 2015; Sutskever et al., 2014). The Transformer has become the de facto architecture for large-scale sequence modeling (Vaswani et al., 2017). In (Smith et al., 2018), domain-aware fine-tuning was introduced for low-resource transfer, while subsequent works studied multilingual pretraining (Johnson et al., 2017; Conneau and Lample, 2019). Several studies examine noisy-channel decoding to improve adequacy (Yu et al., 2020), and constrained decoding strategies help preserve terminology in specialized domains (Post and Vilar, 2018).",
    "reason": "Wrong citation style: a parenthetical citation is placed after the preposition 'In'; it should be a narrative citation like 'In Smith et al. (2018)'.",
    "start": 266,
    "end": 289,
    "label": "Format"
  },
  {
    "span": "Recent works have achieved over 90% F1 on cross-domain benchmarks.",
    "document": "Introduction\n\nDetecting hate speech in multimodal social media posts requires models that can jointly reason over text and images. Early approaches relied on text-only classifiers using bag-of-words and linear models (Schmidt and Wiegand, 2017), while recent methods leverage pretrained transformers and multimodal fusion (Devlin et al., 2019; Lu et al., 2019). On the vision side, CNN-based encoders supply cues about symbols and gestures (Kiela et al., 2020), and fusion mechanisms align textual and visual semantics (Tsai et al., 2019).\n\nDespite progress, generalization across domains remains difficult due to annotation shift and visual ambiguity (Gomez et al., 2021). Recent works have achieved over 90% F1 on cross-domain benchmarks. However, these results are often sensitive to label definitions and sampling protocols, motivating evaluations that disentangle content, context, and community norms.\n\nWe propose a lightweight cross-modal adapter that aligns representations via contrastive objectives while remaining robust to missing modalities.",
    "reason": "Claims performance by unspecified recent works and cites a specific statistic without providing any citations (rule d and b).",
    "start": 674,
    "end": 740,
    "label": "Unsupported_claim"
  },
  {
    "span": "Yao and Huang (2017) formalized individual and group fairness metrics in recommender systems. Rendle et al. (2009) proposed Bayesian Personalized Ranking for implicit feedback. Burke et al. (2017) studied multi-sided fairness in ranking.",
    "document": "Related Work\n\nFairness in recommender systems has been studied at the user, item, and provider levels. Approaches range from pre-processing to enforce constraints to in-processing algorithms and post-processing re-ranking.\n\nYao and Huang (2017) formalized individual and group fairness metrics in recommender systems. Rendle et al. (2009) proposed Bayesian Personalized Ranking for implicit feedback. Burke et al. (2017) studied multi-sided fairness in ranking. Other works address exposure disparity under position bias and calibration (Singh and Joachims, 2018; Steck, 2018).\n\nDatasets with sensitive attributes and evaluation protocols for fairness are also being developed (Ekstrand et al., 2018; Diaz et al., 2020).",
    "reason": "A fairness metric paper is followed by a general ranking optimization method and then another fairness paper, but the text does not articulate how BPR relates to fairness or why it appears between fairness-focused works.",
    "start": 224,
    "end": 461,
    "label": "Coherence"
  },
  {
    "span": "The standard practice in semi-supervised parsing is to use tri-training.",
    "document": "Related Work\n\nSemi-supervised parsing. Labeled syntactic treebanks are scarce for many domains and languages, spurring semi-supervised methods that leverage unlabeled corpora. Pseudolabeling, agreement-based learning, and representation learning are common strategies for exploiting raw text.\n\nConsensus and co-training variants. The standard practice in semi-supervised parsing is to use tri-training. This paradigm trains multiple parsers on labeled data, then iteratively exchanges high-confidence outputs to expand training sets while reducing confirmation bias through model diversity.\n\nDomain adaptation. Beyond generic semi-supervised objectives, domain-specific adaptation methods adjust lexicalized features, debias head rules, and incorporate character-level cues to handle noisy text and morphologically rich languages.",
    "reason": "This sentence asserts a field-wide standard practice without citing foundational or representative papers.",
    "start": 330,
    "end": 402,
    "label": "Unsupported_claim"
  },
  {
    "span": "There are many recent works that explore controllable summarization for news and scientific articles.",
    "document": "Introduction and Related Work\n\nNeural abstractive summarization has progressed rapidly with encoder–decoder architectures and large-scale pretraining. Early extractive approaches prioritized sentence selection, while recent neural models focus on generating fluent, condensed paraphrases. Pretrained sequence-to-sequence models have shifted the state of the art on widely used corpora by leveraging transfer learning from massive text collections.\n\nThere are many recent works that explore controllable summarization for news and scientific articles. Techniques typically condition on user-specified attributes such as length, style, entity focus, or citation intent, aiming to provide steering capabilities while maintaining factuality. Despite progress, ensuring faithfulness and controllability simultaneously remains challenging, motivating our study on constraint-aware decoding and content verification.\n\nOur approach situates within this line of work by introducing lightweight control signals that are robust to domain shift, and by evaluating control accuracy alongside semantic faithfulness under human and automatic measures.",
    "reason": "Mentions 'recent works' without providing citations to those works.",
    "start": 449,
    "end": 550,
    "label": "Unsupported_claim"
  },
  {
    "span": "Hernandez et al.",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become a dominant paradigm for learning over relational data in recommendation and search. Early collaborative filtering methods relied on matrix factorization and shallow embedding models, which struggled to capture higher-order connectivity and context. Recent work argues that message passing over user–item graphs improves both accuracy and robustness by explicitly encoding structure (Kipf and Welling, 2017; Hamilton et al., 2017). Building on this trend, Hernandez et al. propose a contrastive training objective that aligns node representations under stochastic graph augmentations, reporting gains on several benchmarks (Zhang and Chen, 2020; He et al., 2020). Despite these advances, deployment remains challenging due to dynamic graphs, sparse feedback, and the cost of negative sampling.\n\nIn this paper, we revisit the design of contrastive signals for implicit-feedback recommendation. We compare augmentation strategies, analyze stability under streaming updates, and provide a training recipe that reduces compute cost without hurting quality. Our experiments span three public datasets and one large-scale production graph.",
    "reason": "Narrative citation is missing the year; it should appear as \"Hernandez et al. (YEAR)\" rather than just the author string.",
    "start": 510,
    "end": 526,
    "label": "Format"
  },
  {
    "span": "Fairness in machine learning has been formalized through demographic parity, equalized odds, and calibration (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017). Pre-, in-, and post-processing mitigation techniques have been proposed, including reweighting, adversarial debiasing, and calibration adjustments (Kamiran and Calders, 2012; Zhang et al., 2018; Pleiss et al., 2017). Auditing and measurement frameworks assess bias across datasets and models (Buolamwini and Gebru, 2018; Mitchell et al., 2019).",
    "document": "Related Work Ensuring fairness in predictive systems requires careful consideration of metrics, interventions, and deployment context. A broad literature examines trade-offs between utility and equity under different operational constraints. Fairness in machine learning has been formalized through demographic parity, equalized odds, and calibration (Dwork et al., 2012; Hardt et al., 2016; Kleinberg et al., 2017). Pre-, in-, and post-processing mitigation techniques have been proposed, including reweighting, adversarial debiasing, and calibration adjustments (Kamiran and Calders, 2012; Zhang et al., 2018; Pleiss et al., 2017). Auditing and measurement frameworks assess bias across datasets and models (Buolamwini and Gebru, 2018; Mitchell et al., 2019). We study fairness under budgeted label acquisition and propose an active querying strategy that optimizes a fairness-aware acquisition objective while preserving model utility. Across three domains, our approach reduces disparity with fewer labels than baselines.",
    "reason": "The span lists fairness metrics, mitigation strategies, and auditing frameworks without tying them to the studied budgeted label acquisition setting or stating the specific limitation they leave open. It lacks synthesis (a, b).",
    "start": 242,
    "end": 761,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Modern indexing spans B-trees and their variants, LSM-trees optimized for writes, and learned indexes that replace or augment traditional search structures with ML models (Bayer and McCreight, 1972; O'Neil et al., 1996; Kraska et al., 2018; Ding et al., 2020). Systems adapt to hardware and workload characteristics via tiered compaction, cache-aware layouts, and hybrid in-memory/on-disk designs.",
    "document": "Related Work: Indexing and Learned Data Structures\n\nEfficient indexing is foundational to database performance. The design space balances latency, throughput, and memory footprint under a variety of read/write mixes and skewed key distributions.\n\nModern indexing spans B-trees and their variants, LSM-trees optimized for writes, and learned indexes that replace or augment traditional search structures with ML models (Bayer and McCreight, 1972; O'Neil et al., 1996; Kraska et al., 2018; Ding et al., 2020). Systems adapt to hardware and workload characteristics via tiered compaction, cache-aware layouts, and hybrid in-memory/on-disk designs.\n\nWe present an adaptive learned index with drift-aware retraining and contention-sensitive search.",
    "reason": "The span catalogs prior approaches and techniques without articulating their limitations or explicitly connecting them to the proposed method's objectives (criteria a and c).",
    "start": 247,
    "end": 644,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There are many recent works that explore this topic.",
    "document": "Related Work\n\nSafety in open-domain dialogue aims to prevent harmful, biased, or privacy-violating responses while preserving helpfulness and engagement. Approaches span data filtering, controlled generation, adversarial training, and post-hoc moderation.\n\nThere are many recent works that explore this topic. Some methods focus on detoxification objectives at fine-tuning time, while others add lightweight control mechanisms at inference. Despite progress, systematic evaluation remains difficult due to shifting norms and under-specified risk taxonomies.\n\nOur study complements this line by introducing a scenario-targeted benchmark and a transparent auditing protocol for model updates over time.\n",
    "reason": "The claim mentions 'many recent works' without citing any of them, which violates the requirement to back up mentions of recent work with citations.",
    "start": 257,
    "end": 309,
    "label": "Unsupported_claim"
  },
  {
    "span": "Earlier studies on SER consistently find that teacher-student distillation improves robustness to noise.",
    "document": "Related Work\n\nSpeech emotion recognition (SER) systems must cope with acoustic variability and domain mismatch between lab recordings and real-world audio. Robustness techniques span data augmentation, domain adversarial training, and knowledge transfer from larger self-supervised models. Among these, knowledge distillation has been proposed to compress models while retaining performance.\n\nEarlier studies on SER consistently find that teacher-student distillation improves robustness to noise. Yet, it is not fully understood which aspects of the teacher—content embeddings, prosodic cues, or temporal dynamics—are most critical for transferring robustness. We therefore compare feature-level versus logit-level distillation across a range of signal-to-noise ratios and noise types.\n\nOur results indicate that multi-level distillation coupled with augmentation-aware teachers yields the best accuracy-robustness trade-off under strict latency constraints.",
    "reason": "The sentence references findings from 'earlier studies' but provides no citations to those works, leaving the claim unsupported.",
    "start": 393,
    "end": 497,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Garcia et al.)",
    "document": "Introduction\n\nAlgorithmic fairness research spans pre-, in-, and post-processing interventions to mitigate disparate impact (Friedler et al., 2019; Mehrabi et al., 2021). Prior work (Garcia et al.) explores measurement of bias under domain shift but leaves open how to enforce invariance during training. Recent causal approaches provide tools for counterfactual evaluation yet face identifiability challenges in practice (Kusner et al., 2017; Kilbertus et al., 2017). We propose a representation learning objective that aligns conditional distributions while preserving task utility.",
    "reason": "Parenthetical citation missing the year; should include a year, e.g., “(Garcia et al., YYYY)”.",
    "start": 182,
    "end": 197,
    "label": "Format"
  },
  {
    "span": "We follow the setup used in prior datasets where labels are collected via distant supervision from hashtags.",
    "document": "Introduction\n\nSarcasm detection in social media aims to identify utterances where the intended meaning differs from the literal text, often requiring pragmatic and contextual cues (Joshi et al., 2017). Large-scale datasets are critical for training robust models, but manual annotation is costly and inconsistent across platforms.\n\nWe follow the setup used in prior datasets where labels are collected via distant supervision from hashtags. While this approach scales, it introduces biases toward explicit, self-annotated sarcasm and may not capture subtler forms common in conversational threads.\n\nTo address these limitations, we curate a multi-platform corpus combining distant supervision with targeted human validation. We also include thread context and author history to better represent pragmatic signals, and we release standardized splits to foster comparability across studies.",
    "reason": "It references 'prior datasets' and a specific labeling methodology without citing the datasets that established this setup.",
    "start": 332,
    "end": 440,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Chen and Li, 2015)",
    "document": "Introduction\n\nDual-encoder retrieval architectures have become standard for large-scale search because they decouple document and query encoding. Early work explored bi-encoder objectives and contrastive losses (Chen and Li, 2015), followed by improvements from in-batch negatives and hard-negative mining (Karpukhin et al., 2020; Xiong et al., 2021). Recent studies add distillation from cross-encoders to bridge performance gaps (Hofstätter et al., 2021).\n\nWe build on these ideas by integrating multi-vector representations with late interaction to balance efficiency and accuracy.",
    "reason": "Incorrect conjunction in a parenthetical two-author citation; APA-style uses an ampersand in parentheses, so it should be \"(Chen & Li, 2015)\".",
    "start": 211,
    "end": 230,
    "label": "Format"
  },
  {
    "span": "Since the SemEval-2014 aspect-based sentiment task, researchers have largely converged on micro-F1 as the primary metric.",
    "document": "Related Work\n\nAspect-based sentiment analysis (ABSA) studies fine-grained opinions over entities and attributes. Methods have transitioned from pipeline architectures to joint extraction and classification with sequence tagging or span-based models.\n\nSince the SemEval-2014 aspect-based sentiment task, researchers have largely converged on micro-F1 as the primary metric. Concurrent efforts investigate cross-domain adaptation and multilingual transfer, but consistent evaluation across datasets remains a challenge due to differing label spaces and annotation schemes.\n\nWe contribute a unified evaluation protocol with matched label spaces and a calibration-aware objective to mitigate threshold sensitivity across domains.",
    "reason": "References a specific shared task and a field-wide metric convention without providing citations to support the claims (violates a and b).",
    "start": 251,
    "end": 372,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent exploration methods include count-based bonuses (Bellemare et al., 2016; Tang et al., 2017), intrinsic motivation via prediction error or curiosity (Pathak et al., 2017; Burda et al., 2019), posterior sampling and bootstrapping (Osband et al., 2016; O'Donoghue et al., 2018), and information-theoretic objectives (Houthooft et al., 2016; Mohamed and Rezende, 2015).",
    "document": "Introduction\n\nEfficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments where naive strategies fail to discover informative trajectories. Practical algorithms must balance systematic discovery of novel states with stability and scalability to high-dimensional observations.\n\nRecent exploration methods include count-based bonuses (Bellemare et al., 2016; Tang et al., 2017), intrinsic motivation via prediction error or curiosity (Pathak et al., 2017; Burda et al., 2019), posterior sampling and bootstrapping (Osband et al., 2016; O'Donoghue et al., 2018), and information-theoretic objectives (Houthooft et al., 2016; Mohamed and Rezende, 2015).\n\nWe introduce a general objective that unifies optimism and uncertainty-aware scheduling through a learned exploration temperature. Our method adapts exploration intensity online based on epistemic uncertainty estimated by a lightweight ensemble. We demonstrate consistent improvements over strong baselines on Atari, Procgen, and continuous control suites.\n\nOur analysis studies gradient variance, sample efficiency, and sensitivity to hyperparameters, and ablations isolate the effect of adaptive temperature from architectural choices.",
    "reason": "The span merely catalogs exploration methods and citations without explaining how they relate to or motivate the proposed approach, meeting condition (a).",
    "start": 342,
    "end": 714,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Miller et al., (2016)",
    "document": "Related Work\n\nEnd-to-end speech recognition moved from HMM–DNN hybrids (Hinton et al., 2012) to attention-based encoder–decoders (Chan et al., 2016) and transducers (Graves, 2012; Graves, 2013). Following Miller et al., (2016), context-dependent lexicon embeddings have been explored to reduce OOV rates, and joint CTC/attention training improves robustness (Watanabe et al., 2017). Self-supervised pretraining with wav2vec 2.0 (Baevski et al., 2020) further narrows the gap to supervised systems.",
    "reason": "Extraneous comma before the year in a narrative citation; it should be 'Miller et al. (2016)'.",
    "start": 205,
    "end": 226,
    "label": "Format"
  },
  {
    "span": "The WMT20 low-resource track highlighted the importance of back-translation for Nepali–English.",
    "document": "Related Work\n\nLow-resource machine translation (MT) must contend with limited parallel data and noisy monolingual corpora. Common strategies include multilingual transfer, synthetic data generation, and unsupervised or semi-supervised objectives. Shared tasks and community benchmarks regularly shape best practices by comparing methods under standardized conditions.\n\nThe WMT20 low-resource track highlighted the importance of back-translation for Nepali–English.\n\nIn parallel, research has explored lexicon induction and subword segmentation tailored to morphologically rich languages. Our work complements these directions by revisiting data selection for synthetic corpora to maximize utility under compute constraints.",
    "reason": "Mentions a specific shared task and its findings without providing a citation at first mention (violates rule a) and asserts a conclusion about prior work without evidence (violates b).",
    "start": 369,
    "end": 464,
    "label": "Unsupported_claim"
  },
  {
    "span": "Graph convolutional networks, message passing neural networks, attention-based variants, and spectral methods have been applied extensively to molecular property prediction (Kipf and Welling, 2017; Gilmer et al., 2017; Velickovic et al., 2018; Zhang et al., 2020).",
    "document": "Related Work\n\nMolecular property prediction is a cornerstone task for computational chemistry and drug discovery, where models must generalize across diverse scaffolds while remaining data-efficient.\n\nGraph convolutional networks, message passing neural networks, attention-based variants, and spectral methods have been applied extensively to molecular property prediction (Kipf and Welling, 2017; Gilmer et al., 2017; Velickovic et al., 2018; Zhang et al., 2020). Numerous benchmarks have been proposed to evaluate these models under random and scaffold splits.\n\nOur work focuses on improving calibration under distribution shift. We incorporate uncertainty-aware readouts and structure-conditioned temperature scaling to reduce overconfidence on out-of-distribution scaffolds.",
    "reason": "This span catalogs prior approaches without connecting them to the paper’s focus on calibration or explaining their limitations.",
    "start": 201,
    "end": 465,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Perez et al., 2020;Luo et al., 2019;.",
    "document": "Related Work\n\nGraph-based recommendation leverages user-item interaction graphs to propagate preferences and alleviate sparsity. Early collaborative filtering relied on matrix factorization (Koren et al., 2009), but graph neural networks now dominate due to their ability to model high-order connectivity (Ying et al., 2018;Wu et al., 2020).\n\nSession-based methods incorporate temporal signals and path context to capture evolving intents (Wang et al., 2019;Qiu et al., 2020). Several works unify sequential and graph views through contrastive objectives (Sun et al., 2020) and mutual information maximization (You et al., 2020), while others enhance side information with knowledge graphs (Wang et al., 2018).\n\nOur study is most related to graph augmentation for robustness and calibration in recommender systems (Perez et al., 2020;Luo et al., 2019;.",
    "reason": "Malformed multi-citation: has a trailing semicolon and period and lacks a closing parenthesis. Should be '(Perez et al., 2020; Luo et al., 2019)'.",
    "start": 814,
    "end": 852,
    "label": "Format"
  },
  {
    "span": "Attention weights have been used as explanations in clinical NLP (Choi et al., 2016). Counterfactual explanations generate minimally changed inputs (Wachter et al., 2017). Saliency maps highlight influential pixels in medical imaging (Simonyan et al., 2013). Prototype networks provide case-based reasoning (Li et al., 2018).",
    "document": "Related Work\n\nExplainability in clinical AI. Reliable explanations can support auditing, provider trust, and regulatory compliance for decision support tools (Doshi-Velez and Kim, 2017). However, explanation faithfulness and stability remain open challenges, particularly under dataset shift (Adebayo et al., 2018).\n\nAttention weights have been used as explanations in clinical NLP (Choi et al., 2016). Counterfactual explanations generate minimally changed inputs (Wachter et al., 2017). Saliency maps highlight influential pixels in medical imaging (Simonyan et al., 2013). Prototype networks provide case-based reasoning (Li et al., 2018). We focus on unifying counterfactual and prototype reasoning by optimizing for causal sparsity while preserving case-based interpretability.",
    "reason": "The span cites several explanation paradigms across modalities with no transitions or explicit connections. It is unclear how each cited work relates to the previous one, leading to abrupt shifts and a lack of coherence.",
    "start": 317,
    "end": 642,
    "label": "Coherence"
  },
  {
    "span": "Most recent vision-language models adopt contrastive pretraining on 400M image–text pairs",
    "document": "Introduction\n\nAligning visual and textual representations has led to strong zero-shot recognition and retrieval performance. Most recent vision-language models adopt contrastive pretraining on 400M image–text pairs, enabling broad coverage of concepts and styles. Despite impressive transfer, these models can be brittle under distribution shift and sensitive to spurious correlations in alt-text.\n\nWe investigate data curation and curriculum strategies that reduce shortcut learning while maintaining competitive zero-shot accuracy.",
    "reason": "Makes a quantitative, field-wide claim about model training data without citing specific works or datasets (rule b and d).",
    "start": 125,
    "end": 214,
    "label": "Unsupported_claim"
  },
  {
    "span": "Knowledge distillation techniques for large language models transfer behavior using soft targets, temperature scaling, intermediate feature matching, and layer-wise supervision. Recent work explores task-agnostic distillation from instruction-tuned teachers, multi-teacher ensembles, and student initialization from smaller pretrained checkpoints. We adopt a similar objective with temperature-scaled soft labels.",
    "document": "Introduction\n\nDeploying large language models in resource-constrained environments motivates compression without large drops in quality. Knowledge distillation is a common strategy to transfer capabilities from a teacher to a smaller student model.\n\nKnowledge distillation techniques for large language models transfer behavior using soft targets, temperature scaling, intermediate feature matching, and layer-wise supervision. Recent work explores task-agnostic distillation from instruction-tuned teachers, multi-teacher ensembles, and student initialization from smaller pretrained checkpoints. We adopt a similar objective with temperature-scaled soft labels.\n\nOur experiments examine generation quality and latency under varied sequence lengths and decoding algorithms.",
    "reason": "The span enumerates prior work and immediately states the chosen approach without explaining why existing methods are insufficient or how the authors' method addresses a specific gap.",
    "start": 250,
    "end": 663,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Graph-based forecasting methods apply graph convolutional networks or their variants to model spatial dependencies across sensors (Li et al., 2018; Yu et al., 2018; Wu et al., 2019; Guo et al., 2019). Temporal dynamics are captured with recurrent units or temporal convolutions layered on top of the graph encoders (Bai et al., 2018; Zhang et al., 2020; Pan et al., 2021). Attention mechanisms have also been used to adaptively weigh neighbors and time steps (Velickovic et al., 2018; Shao et al., 2020; Song et al., 2020).",
    "document": "Related Work\n\nClassical time-series models. Early traffic forecasting relied on linear models and state-space formulations, such as ARIMA and Kalman filters, to capture periodicity and short-term correlations. While interpretable, these approaches struggle with nonstationarity and complex spatial interactions among sensors distributed across a road network.\n\nGraph-based spatio-temporal deep learning. Graph-based forecasting methods apply graph convolutional networks or their variants to model spatial dependencies across sensors (Li et al., 2018; Yu et al., 2018; Wu et al., 2019; Guo et al., 2019). Temporal dynamics are captured with recurrent units or temporal convolutions layered on top of the graph encoders (Bai et al., 2018; Zhang et al., 2020; Pan et al., 2021). Attention mechanisms have also been used to adaptively weigh neighbors and time steps (Velickovic et al., 2018; Shao et al., 2020; Song et al., 2020).\n\nData and robustness considerations. Beyond architecture design, recent work explores robustness to sensor dropout, distribution shift due to incidents or weather, and transfer across cities. Techniques include data augmentation, domain adaptation, and uncertainty-aware training to quantify predictive confidence under rare events.\n\nIn this work we present a unified spatio-temporal architecture for traffic forecasting that emphasizes long-horizon stability and calibrated uncertainty under covariate shift.",
    "reason": "The span lists prior graph-based methods and components without explaining how they relate to the present study, what limitations they have, or what specific gap motivates the new approach.",
    "start": 404,
    "end": 927,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Polosukhin and Skidanov (2018) frame program induction as sequence transduction. Parisotto et al. (2017) combine neural networks with symbolic search for DSL tasks. Ellis et al. (2019) use Bayesian program induction with enumerative search. Chen et al. (2021) show large language models generate code from natural language.",
    "document": "Related Work\n\nProgram Synthesis from Natural Language\n\nSynthesis approaches span neural sequence models, neuro-symbolic search, and probabilistic program induction. Benchmarks differ in DSL expressivity and supervision granularity. Polosukhin and Skidanov (2018) frame program induction as sequence transduction. Parisotto et al. (2017) combine neural networks with symbolic search for DSL tasks. Ellis et al. (2019) use Bayesian program induction with enumerative search. Chen et al. (2021) show large language models generate code from natural language. Evaluations vary from unit tests to formal specifications.\n\nWe target compositional generalization by constraining decoding with type signatures and verified partial programs, improving reliability under weak supervision.",
    "reason": "The span presents four papers across distinct paradigms with no transitions or clarification of their relationships, leaving the progression from sequence models to neuro-symbolic and LLM-based methods implicit.",
    "start": 232,
    "end": 555,
    "label": "Coherence"
  },
  {
    "span": "Dinan et al. (2019) studied knowledge-grounded dialogue with external Wikipedia evidence. Zhou et al. (2018) incorporated commonsense knowledge to enhance response generation. Zhang et al. (2018) introduced persona-conditioned dialogue for consistent responses. Lian et al. (2019) explored retrieval-augmented generation for open-domain chatting.",
    "document": "Related Work\n\nKnowledge-grounded dialogue aims to produce informative and faithful responses by conditioning on structured or unstructured knowledge sources. Systems vary along retrieval versus generation, grounding granularity, and consistency objectives.\n\nDinan et al. (2019) studied knowledge-grounded dialogue with external Wikipedia evidence. Zhou et al. (2018) incorporated commonsense knowledge to enhance response generation. Zhang et al. (2018) introduced persona-conditioned dialogue for consistent responses. Lian et al. (2019) explored retrieval-augmented generation for open-domain chatting.\n\nRecent work emphasizes factuality checking, attribution, and hallucination reduction via citation-aware decoding (Rashkin et al., 2021; Shuster et al., 2021). We focus on attribution-controllable decoding with calibrated retrieval to improve faithfulness.",
    "reason": "The cited works cover different facets (knowledge grounding, commonsense, persona, retrieval) but are presented as standalone statements without transitions that clarify their relationships, making the flow disjointed.",
    "start": 258,
    "end": 604,
    "label": "Coherence"
  },
  {
    "span": "McMahan et al. (2017) formulated the federated averaging algorithm for decentralized training. Geyer et al. (2018) studied differential privacy in federated analytics. Kairouz et al. (2021) surveyed personalization strategies under statistical heterogeneity. Sattler et al. (2019) proposed communication compression techniques to reduce bandwidth.",
    "document": "Related Work\n\nFederated Learning in Healthcare\n\nFederated learning enables collaborative model training across institutions without centralizing sensitive data. Healthcare data exhibits significant non-IID characteristics and strict privacy constraints, which complicate optimization and evaluation. Solutions typically address optimization stability, privacy accounting, personalization, and communication efficiency.\n\nMcMahan et al. (2017) formulated the federated averaging algorithm for decentralized training. Geyer et al. (2018) studied differential privacy in federated analytics. Kairouz et al. (2021) surveyed personalization strategies under statistical heterogeneity. Sattler et al. (2019) proposed communication compression techniques to reduce bandwidth.\n\nWe complement prior work by introducing a heterogeneity-aware optimizer with per-site adaptive regularization and a privacy-preserving early-stopping criterion tailored to clinical validation workflows.",
    "reason": "The span abruptly lists four areas (optimization, privacy, personalization, compression) via separate citations without transitions or explicit connections, making their relevance to each other unclear.",
    "start": 420,
    "end": 767,
    "label": "Coherence"
  },
  {
    "span": "McMahan et al. (2017) introduced Federated Averaging for on-device learning. Bonawitz et al. (2019) proposed secure aggregation to protect individual updates. Geyer et al. (2017) studied differential privacy for federated optimization. Kairouz et al. (2021) surveyed advances and open problems in federated learning.",
    "document": "Related Work\n\nFederated learning enables collaborative training across decentralized data silos while keeping raw data local. Robustness and privacy are central concerns, particularly in cross-device regimes with high heterogeneity and intermittent connectivity.\n\nMcMahan et al. (2017) introduced Federated Averaging for on-device learning. Bonawitz et al. (2019) proposed secure aggregation to protect individual updates. Geyer et al. (2017) studied differential privacy for federated optimization. Kairouz et al. (2021) surveyed advances and open problems in federated learning.\n\nOur method augments aggregation with distributionally robust objectives that adapt to client drift without increasing communication costs.",
    "reason": "The span strings together four citations with no connective tissue or explanation of relevance among them, leading to abrupt shifts and unclear relationships (issues a and b).",
    "start": 264,
    "end": 580,
    "label": "Coherence"
  },
  {
    "span": "(2017)",
    "document": "Related Work\n\nWord sense disambiguation has oscillated between knowledge-based and supervised paradigms over the past decades (Navigli, 2009; Raganato et al., 2017). Early knowledge-based methods (2017) dominated when annotated corpora were scarce, relying on gloss overlaps and semantic relatedness.\n\nWith the advent of contextual encoders, supervised and semi-supervised systems surpassed prior methods by leveraging sense inventories and pseudo-labeling (Hadiwinoto et al., 2019; Scarlini et al., 2020). Our approach integrates lexical constraints without requiring full supervision.",
    "reason": "Year-only parenthetical citation without authors; incomplete citation format.",
    "start": 196,
    "end": 202,
    "label": "Format"
  },
  {
    "span": "(Singh et. al., 2020)",
    "document": "Related Work\n\nClinical NLP for Entity Normalization\n\nMapping clinical mentions to controlled vocabularies requires robust handling of abbreviations and variants (Aronson, 2001; Bodenreider, 2004). Recent neural approaches leverage contextual encoders and synonym expansion (Luo et al., 2019; Ji et al., 2020). Weak supervision has been used to scale training data with heuristics and ontological cues (Ratner et al., 2017; Fries et al., 2019). Prior work (Singh et. al., 2020) reported improvements from joint training on linking and normalization, but performance degrades on rare concepts (Sohn et al., 2009; Johnson et al., 2016). We introduce a calibration-aware contrastive objective that emphasizes hard negatives derived from ontology structure.",
    "reason": "Incorrect 'et al.' formatting: should be 'et al.' without a period after 'et' (i.e., '(Singh et al., 2020)').",
    "start": 455,
    "end": 476,
    "label": "Format"
  },
  {
    "span": "Kim & Park (2018)",
    "document": "Related Work\n\nCalibration of neural classifiers is crucial for deploying models in risk-sensitive settings (Guo et al., 2017). Temperature scaling remains a strong post-hoc baseline, while Bayesian and ensemble methods provide better uncertainty estimates at higher cost (Lakshminarayanan et al., 2017). Kim & Park (2018) demonstrate that margin-aware training objectives can improve calibration without sacrificing accuracy. Complementary approaches focus on distribution shift detection using confidence scores and conformal prediction (Hendrycks and Gimpel, 2017; Angelopoulos and Bates, 2021).\n\nOur approach integrates a calibration-aware loss with a selective prediction head that abstains under high epistemic uncertainty.",
    "reason": "Wrong conjunction for narrative citation in APA style. In narrative form, authors should be joined by \"and\": \"Kim and Park (2018)\", not \"Kim & Park (2018)\" (the ampersand is for parenthetical citations).",
    "start": 304,
    "end": 321,
    "label": "Format"
  },
  {
    "span": "Transformer-based encoders consistently outperform RNNs on all benchmarks.",
    "document": "Related Work\n\nCode summarization translates source code into natural language descriptions. Early neural approaches employed sequence-to-sequence models with RNN encoders and attention (Iyer et al., 2016; Hu et al., 2018). Subsequent work introduced AST-aware representations and hybrid models (Alon et al., 2019; LeClair et al., 2019). Pretrained models over code and text further improved performance (Ahmad et al., 2020; Feng et al., 2020).\n\nTransformer-based encoders consistently outperform RNNs on all benchmarks. Despite these gains, most methods still struggle with out-of-vocabulary identifiers and long-range dependencies in large functions.\n\nWe address these issues with a retrieval-augmented encoder that conditions the summary on semantically similar code examples indexed at scale.",
    "reason": "Makes a sweeping performance claim across benchmarks without citing comparative studies (rule b).",
    "start": 445,
    "end": 519,
    "label": "Unsupported_claim"
  },
  {
    "span": "Bahdanau et al. (2015) introduced additive attention for neural machine translation. Vaswani et al. (2017) presented the Transformer architecture. Ott et al. (2018) scaled training with efficient implementations. Ng et al. (2019) explored large-batch training for state-of-the-art results.",
    "document": "Related Work\n\nNeural Machine Translation\n\nNeural architectures have transformed machine translation, evolving from recurrent encoder-decoders with attention to fully attention-based models. Alongside architectural changes, optimization and data scaling practices have shifted the Pareto frontier of quality and efficiency.\n\nBahdanau et al. (2015) introduced additive attention for neural machine translation. Vaswani et al. (2017) presented the Transformer architecture. Ott et al. (2018) scaled training with efficient implementations. Ng et al. (2019) explored large-batch training for state-of-the-art results.\n\nDomain Adaptation and Low-Resource Settings\n\nBeyond high-resource benchmarks, a significant body of work studies adaptation via back-translation, multilingual transfer, and parameter-efficient tuning. However, cross-domain robustness remains limited when test-time distributions drift significantly.\n\nOur Angle\n\nWe target robust adaptation by coupling source-side diversification with target-aware decoding constraints, improving both generalization and controllability.",
    "reason": "The span enumerates four influential NMT works without indicating how each relates to the others or to the surrounding discussion, lacking transitions and explicit connections.",
    "start": 324,
    "end": 613,
    "label": "Coherence"
  },
  {
    "span": "Several recent attacks have shown that differential privacy can be broken in practical deployments.",
    "document": "Introduction\n\nDifferential privacy (DP) provides a mathematically rigorous framework for limiting individual information leakage in data analysis and machine learning. In practice, DP mechanisms must navigate trade-offs among privacy budgets, utility, and system constraints, and misconfiguration can undermine intended protections.\n\nSeveral recent attacks have shown that differential privacy can be broken in practical deployments. These observations motivate a more nuanced evaluation that considers end-to-end pipelines, aggregation strategies, and auxiliary information available to adversaries. We present a taxonomy of deployment pitfalls and propose diagnostics that estimate effective privacy loss under realistic threat models.",
    "reason": "Invokes 'recent attacks' but provides no citations to specific attack papers or case studies; such security claims must be referenced.",
    "start": 334,
    "end": 433,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Patel et al., 2017",
    "document": "Introduction\n\nKnowledge distillation compresses a high-capacity teacher into a compact student via softened targets or intermediate hints (Hinton et al., 2015; Romero et al., 2015). While temperature scaling improves gradient signals, matching hidden representations can better guide structure learning (Ahn et al., 2019). Early work (Patel et al., 2017 proposed a multi-teacher ensemble but did not address domain shift between teacher and student data, a gap our method targets with distribution alignment losses.\n\nRecent advances broaden distillation to self-supervised and multi-modal settings (Tian et al., 2020; Alayrac et al., 2020). We unify these views by framing distillation as minimizing an information projection under resource constraints.",
    "reason": "Missing closing parenthesis in a parenthetical citation; should be (Patel et al., 2017).",
    "start": 334,
    "end": 353,
    "label": "Format"
  },
  {
    "span": "The COCO and OpenImages datasets have become the de facto standard for pool-based active learning in detection.",
    "document": "Related Work\n\nActive learning aims to reduce labeling costs by selecting informative samples for annotation. While many strategies have been studied for image classification (Settles, 2010; Sener and Savarese, 2018; Ash et al., 2020), adapting them to object detection introduces additional challenges due to structured outputs and region uncertainty (Roy et al., 2018; Brust et al., 2019).\n\nAcquisition functions for detection consider measures such as localization uncertainty, mutual information over anchors, and core-set diversity to prioritize images with challenging instances and rich object configurations (Kaushal et al., 2019; Yuan et al., 2021). The COCO and OpenImages datasets have become the de facto standard for pool-based active learning in detection. However, class imbalance and long-tail distributions in these datasets can bias selection, leading to suboptimal coverage of rare categories.\n\nWe propose a taxonomy-aware acquisition strategy that decomposes uncertainty along hierarchical labels to scale active learning to long-tail detection benchmarks.",
    "reason": "This first mention of specific datasets as standards in a subfield lacks citations to benchmark papers or challenge overviews, making it unsupported.",
    "start": 658,
    "end": 769,
    "label": "Unsupported_claim"
  },
  {
    "span": "Rahimi et al. (2019) evaluated cross-lingual transfer for NER using contextual embeddings. Pan et al. (2017) surveyed transfer learning methods across languages. Siddhant and Lipton (2018) examined active learning heuristics for sequence tagging. Conneau et al. (2020) introduced XLM-R as a strong multilingual baseline.",
    "document": "Introduction\n\nMultilingual named entity recognition (NER) remains challenging in low-resource settings where labeled data is scarce and annotation costs are high (Tjong Kim Sang, 2002; Rahimi et al., 2019). Cross-lingual transfer and selective annotation via active learning are promising routes to improve performance with minimal supervision (Pires et al., 2019; Settles, 2009).\n\nRahimi et al. (2019) evaluated cross-lingual transfer for NER using contextual embeddings. Pan et al. (2017) surveyed transfer learning methods across languages. Siddhant and Lipton (2018) examined active learning heuristics for sequence tagging. Conneau et al. (2020) introduced XLM-R as a strong multilingual baseline.\n\nDespite these advances, little work studies joint optimization of sampling policies and multilingual representations under strict budget constraints. We address this gap with an acquisition strategy that couples cross-lingual uncertainty with typological diversity and representation regularization.",
    "reason": "The span strings together four references that cover different aspects (evaluation, survey, AL heuristics, model baseline) without articulating how each connects to the previous one or to a shared argument, and lacks transitions.",
    "start": 382,
    "end": 702,
    "label": "Coherence"
  },
  {
    "span": "Explainable recommendation has leveraged attention visualization (Chen et al., 2018; Zhang et al., 2019), post-hoc rationalizers (Lei et al., 2016; Camburu et al., 2018), disentangled representations (Ma et al., 2019), and path reasoning over knowledge graphs (Wang et al., 2019; Sun et al., 2020). We focus on sequential recommenders.",
    "document": "Related Work\n\nRecent advances in recommender systems emphasize transparency to improve user trust and assist operators in auditing model behavior. Explanations can be intrinsic to the model or generated post hoc.\n\nExplainable recommendation has leveraged attention visualization (Chen et al., 2018; Zhang et al., 2019), post-hoc rationalizers (Lei et al., 2016; Camburu et al., 2018), disentangled representations (Ma et al., 2019), and path reasoning over knowledge graphs (Wang et al., 2019; Sun et al., 2020). We focus on sequential recommenders.\n\nWhile session-based models capture evolving preferences, explanation methods must address temporal drift and sparsity. Our later sections introduce a counterfactual sequence rationale objective tailored to cold-start items.\n",
    "reason": "Violates (b): the text moves from listing prior work to stating the paper's scope without articulating what is missing in existing methods or why the focus is necessary.",
    "start": 214,
    "end": 549,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Zhao et al., 2017; Lin et al., 2018;",
    "document": "Related Work\n\nData augmentation for NLP includes paraphrasing, back-translation, and span perturbations, each improving generalization when labeled data is scarce (Fadaee et al., 2017; Sennrich et al., 2016). Prior work on synthetic noise injection shows resilience gains under distribution shift (Zhang et al., 2020). Multi-corpus augmentation strategies have been explored in machine translation and summarization (Zhao et al., 2019). We situate our approach alongside unsupervised variants that exploit monolingual corpora (Lewis et al., 2020) and consistency training (Xie et al., 2020), while acknowledging limitations in domain transfer (Gururangan et al., 2020).\n\nEmpirically, combined augmentation and consistency regularization yield robust improvements across tasks (Zhao et al., 2017; Lin et al., 2018; but their interaction with encoder depth and tokenization remains underexplored.",
    "reason": "Missing closing parenthesis in a multi-citation; the parenthetical citation is unbalanced.",
    "start": 776,
    "end": 813,
    "label": "Format"
  },
  {
    "span": "On the ImageNet-V2 benchmark, prior studies reported a 7% accuracy drop for standard ResNet-50 models.",
    "document": "Introduction\n\nImage classification models trained on large-scale datasets are known to suffer performance decay under distribution shift [1,2]. Several test sets have been proposed to audit robustness by re-collecting data with matched selection procedures [3]. On the ImageNet-V2 benchmark, prior studies reported a 7% accuracy drop for standard ResNet-50 models. Yet, little is known about how training-time augmentations affect this particular shift.",
    "reason": "Makes a specific quantitative claim about prior studies and a benchmark without any citation.",
    "start": 262,
    "end": 364,
    "label": "Unsupported_claim"
  },
  {
    "span": "Du et al. (2017) framed question generation as sequence transduction. Scoring rubrics for educational assessments guide item difficulty (Haladyna, 2004). Template-based systems use semantic role labeling (Heilman and Smith, 2010). Large pre-trained models can be prompted to generate questions (Brown et al., 2020).",
    "document": "Introduction\n\nAutomatic question generation (QG) has applications in education, assessment, and conversational systems. Methods vary from template-based pipelines to neural sequence models and, more recently, prompt-based generation with large language models.\n\nDu et al. (2017) framed question generation as sequence transduction. Scoring rubrics for educational assessments guide item difficulty (Haladyna, 2004). Template-based systems use semantic role labeling (Heilman and Smith, 2010). Large pre-trained models can be prompted to generate questions (Brown et al., 2020).\n\nWe propose a rubric-aware prompting framework that conditions generation on desired cognitive level and evaluates alignment with learning objectives.",
    "reason": "The span jumps between neural QG, educational rubrics, template-based systems, and prompting without transitions or explicit explanation of their interrelations, resulting in poor coherence.",
    "start": 262,
    "end": 577,
    "label": "Coherence"
  },
  {
    "span": "Codex translates natural language into code across languages (Chen et al., 2021). Type-directed search constrains candidates with program signatures (Osera and Zdancewic, 2015). Unit tests guide synthesis by rejecting failing programs (Polikarpova et al., 2016). Retrieval augments prompts with similar solutions (Liu et al., 2022).",
    "document": "Related Work\n\nProgram synthesis has progressed through neural, symbolic, and neuro-symbolic techniques. Large language models (LLMs) have recently shown strong few-shot performance on code generation, while traditional systems exploit type systems and specifications to constrain the search space.\n\nCodex translates natural language into code across languages (Chen et al., 2021). Type-directed search constrains candidates with program signatures (Osera and Zdancewic, 2015). Unit tests guide synthesis by rejecting failing programs (Polikarpova et al., 2016). Retrieval augments prompts with similar solutions (Liu et al., 2022).\n\nBeyond generation, verification-oriented methods integrate SMT solvers to ensure correctness (Filliatre and Marché, 2007) and use counterexample-guided refinement (Solar-Lezama, 2008). Our contribution aligns LLM-based generation with specification-driven validation by jointly optimizing pass@k and proof obligations.",
    "reason": "The span juxtaposes LLM code generation, type-directed symbolic techniques, test-based pruning, and retrieval without transitions or clarifying how they relate, creating abrupt, implied connections.",
    "start": 299,
    "end": 631,
    "label": "Coherence"
  },
  {
    "span": "Existing diarization corpora rarely include code-switching or overlapping bilingual speech common in community media.",
    "document": "Introduction\n\nSpeaker diarization has progressed with end-to-end neural clustering, embeddings such as x-vectors, and overlap-aware segmentation (Sell et al., 2018; Park et al., 2022). Public datasets including CALLHOME, AMI, and VoxConverse have enabled benchmarking across telephone and meeting domains (NIST, 2000; Carletta, 2007; Chowdhury et al., 2021). Still, real-world deployments face challenges from background noise, overlapping speech, and diverse accents.\n\nExisting diarization corpora rarely include code-switching or overlapping bilingual speech common in community media. This gap limits the evaluation of diarization systems in multilingual broadcast and social audio. We construct a new dataset with dense overlap and frequent language switches, and we propose a multilingual embedding extractor trained with language-adversarial objectives.",
    "reason": "This is a field-wide claim about datasets lacking certain properties with no citations; it should reference surveys or dataset analyses to support 'rarely include.'",
    "start": 470,
    "end": 587,
    "label": "Unsupported_claim"
  },
  {
    "span": "Vision Transformers have been adapted to remote sensing using token pruning, multi-scale windows, and hybrid CNN-Transformer backbones (Liang et al., 2021; Noor et al., 2022; Patel et al., 2023). Satellite change detection with Transformer decoders and long-range attention has also been reported (Yuan et al., 2022; Costa and Bittencourt, 2023).",
    "document": "Introduction\nHigh-resolution earth observation imagery presents unique challenges for semantic segmentation and change detection due to extreme scale variation, class imbalance, and geospatial heterogeneity. While convolutional architectures have been dominant, recent advances in attention mechanisms suggest benefits for modeling long-range dependencies across large swaths of terrain. We study scalable attention for remote sensing segmentation with a memory-efficient factorization tailored to gigapixel scenes.\n\nRelated Work\nTransformers in remote sensing. Vision Transformers have been adapted to remote sensing using token pruning, multi-scale windows, and hybrid CNN-Transformer backbones (Liang et al., 2021; Noor et al., 2022; Patel et al., 2023). Satellite change detection with Transformer decoders and long-range attention has also been reported (Yuan et al., 2022; Costa and Bittencourt, 2023). Data-efficient pretraining with masked image modeling on geo-domain corpora has been explored for better sample efficiency (Ravi et al., 2022; Koo and Han, 2023).\n\nScalable inference. Techniques for scaling inference include tiling with overlap handling, pyramid fusion, and test-time augmentation (Silva et al., 2020; Mora et al., 2021). Memory-aware attention variants reduce quadratic costs via low-rank, clustered, or sparse tokens (Chen and Wang, 2021; Ortiz et al., 2022).\n\nWe present a stratified token routing scheme that exploits map priors to downweight redundant regions while preserving fine structures, enabling high-resolution coverage with bounded memory and competitive accuracy.",
    "reason": "The span catalogs prior adaptations of Transformers to remote sensing but does not connect them to the paper's specific problem setting or articulate remaining limitations; no author stance or gap is made explicit.",
    "start": 562,
    "end": 908,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Garcia and Lee",
    "document": "Introduction\n\nCross-lingual transfer has enabled NLP models to generalize to low-resource languages using representations learned on high-resource corpora (Pires et al., 2019; Conneau et al., 2020). Garcia and Lee introduce a pivot-based alignment that reduces lexical divergence across language pairs while preserving syntactic structure. Subsequent extensions apply contrastive learning to further tighten cross-lingual invariance (Wu et al., 2021; Singh et al., 2022). Despite these advances, domain shifts and script variations remain challenging, prompting work on adaptive tokenization and subword sharing (Kim et al., 2021; Ranjan et al., 2022). Our approach complements alignment by incorporating multilingual calibration layers trained with weak supervision (Zhang et al., 2023).",
    "reason": "Narrative citation missing year: should appear as 'Garcia and Lee (YEAR)'.",
    "start": 199,
    "end": 213,
    "label": "Format"
  },
  {
    "span": "(Singh et al. 2017)",
    "document": "Introduction\n\nEnd-to-end speech recognition replaces hand-engineered pipelines with neural encoders and decoders trained on paired audio-text data (Cruz and Patel, 2018; Lorenzo et al., 2020). Attention-based transducers model long-range dependencies, while CTC provides strong alignment priors (O’Neill and Gupta, 2019).\n\nData augmentation such as SpecAugment improves robustness under channel and speaker variability (Singh et al. 2017; Vega and Hu, 2021). Nevertheless, far-field and low-resource conditions remain challenging. We address these gaps with an uncertainty-aware consistency loss and adaptive beam pruning.\n",
    "reason": "Missing comma before the year in parenthetical citation; should be '(Singh et al., 2017)'.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "For sequence learning algorithms such as linear-chain conditional random fields, while the learning objective is formulated at the sequence level, the evaluation metrics are defined at the entity span level.",
    "document": "Related Work\n\nLearning from Imbalanced Data\n\nClass imbalance is a long-standing problem in machine learning tasks, posing challenges to researchers and practitioners in many domains (King and Zeng, 2001;Lu and Jain, 2003;He and Garcia, 2009;Moreo et al., 2016). Classes in real-world data often have highly skewed distribution, leading to substantial gaps between majority and minority classes. While the positive (minority) class is often of interest, the lack of positive examples makes classifiers conservative, i.e., they incline to predict all example as the negative (majority) class. This often results in a low recall of the positive class. Because only a small number of examples are predicted as positive, precision of the positive class tends to be high or unstable. Such a low-recall, high-precision pattern often hurts the F1-score, the standard metric that emphasizes a balanced precision and recall (Juba and Le, 2019). This performance pattern is observed not only in classification tasks, but also in NER tasks where named entity tokens are the minority compared to non-entity tokens (Mao et al., 2007;Kuperus et al., 2013).\n\nResearchers have proposed various techniques for imbalanced learning, including resampling and cost-sensitive learning (He and Ma, 2013). Both aim to re-balance the representation of different classes in the loss function, such that the classifier is less conservative in making positive predictions. In principle, by equating per-instance resampling frequency with per-instance cost, resampling can be implemented as cost-sensitive learning. However, resampling can be applied to models that do not support cost-sensitive learning, making it conveniently applicable to all models.\n\nResampling in Sequence Tagging Tasks\n\nResampling (and cost-sensitive learning) can be conveniently used in classification and regression tasks where a model makes pointwise predictions (a single categorical or scalar value). Each example has a clearly defined sampling rate (or cost) according to its class label. However, in sequence tagging tasks like NER (more broadly, structured prediction tasks (BakIr et al., 2007;Smith, 2011)), a model predicts multiple values for a sequence (or structured output). For sequence learning algorithms such as linear-chain conditional random fields, while the learning objective is formulated at the sequence level, the evaluation metrics are defined at the entity span level. This makes it nontrivial to determine the sampling rate (or cost) for a sequence that contains tokens from both majority and minority entity types. Simply resampling entities by stripping surrounding context is problematic as sequence tagging algorithms depend on context to make predictions. Recent works proposed to randomly or heuristically drop tokens from sentences to re-balance NER data, which had success using conditional random fields and shallow n-gram features (Akkasi, 2018;Akkasi and Varoglu, 2019;Grancharova et al., 2020). However, these methods distort the syntactic and semantic structure of complete sentences, which may generate low-quality data for models that are capable of capturing longdistance linguistic dependencies (e.g. BERT) and hurt performance of those models. In this work, we focus on resampling strategies that leaves sentences intact.\n\nLoss Functions for Imbalanced Data\n\nRecent literature proposed special loss functions for tackling data imbalance, including focal loss (Lin et al., 2017) and Dice loss (Li et al., 2019). They increase the cost of 'hard positives' where the correct label has low predicted probability and decrease the cost of 'easy negatives' where the correct label has high predicted probability. However, these loss functions do not fully address data imbalance in NER. First, the formulation does not always emphasize the loss of minority-class tokens -majority-class tokens can also be hard to classify, and minority-class tokens can also be easy to classify. Second, these loss functions only work on token-wise prediction outputs. They cannot work on sequence-level outputs generated by conditional random fields, which is commonly used in NER. Our resampling methods can be seen as estimating sentence-level losses with explicit emphasis on sentences containing minority-class tokens.\n\n ",
    "start": 2234,
    "end": 2441,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Li and Zhao 2020; Kim, 2019)",
    "document": "Introduction\n\nTemporal commonsense reasoning has prompted new datasets and evaluation protocols for event understanding. Prior datasets (Li and Zhao 2020; Kim, 2019) provide benchmarks for temporal relation extraction across genres, while Ning et al. (2019) propose constrained decoding to enforce partial orders. More recently, language models augmented with temporal signals (Zhou et al., 2021) show improved transfer. Yet, annotation sparsity still limits coverage of long-range dependencies.",
    "reason": "Missing comma between authors and year in the first parenthetical citation; should be '(Li and Zhao, 2020)'.",
    "start": 136,
    "end": 165,
    "label": "Format"
  },
  {
    "span": "In (Nguyen et al., 2019)",
    "document": "Related Work\n\nImitation learning has seen rapid progress through adversarial formulations that align state-action distributions (Ho and Ermon, 2016; Ke et al., 2019). In (Nguyen et al., 2019) a hierarchical variant was proposed to improve long-horizon control, followed by extensions that incorporate risk-sensitive objectives (Zhang and Pineau, 2020) and offline datasets (Kostrikov et al., 2021). However, these methods often assume accurate dynamics or dense expert coverage, which may be unrealistic in safety-critical domains.\n\nOur study differs by focusing on conservative policy updates with certified constraints, building upon robust MDP formulations (Iyengar, 2005; Nilim and El Ghaoui, 2005) and recent offline RL regularizers (Kumar et al., 2020).",
    "reason": "Wrong citation style: a preposition followed by a parenthetical citation is ungrammatical in APA; it should be narrative form 'In Nguyen et al. (2019) ...'.",
    "start": 167,
    "end": 191,
    "label": "Format"
  },
  {
    "span": "(Miller, 1995;)",
    "document": "Related Work\n\nFew-shot learning methods aim to generalize from limited labeled examples by leveraging inductive biases and metric structures (Vinyals et al., 2016; Snell et al., 2017). Prototypical and matching networks define non-parametric decision rules over support sets, while meta-learners optimize for rapid adaptation (Finn et al., 2017; Nichol et al., 2018). Memory-augmented models store task-specific information for reuse (Santoro et al., 2016; Munkhdalai and Yu, 2017). Classical insights on capacity and sample complexity continue to inform modern approaches (Miller, 1995;).\n\nRecent work explores data augmentation and self-supervision to improve representation quality in low-data regimes (Chen et al., 2020; Tian et al., 2020; Chen et al., 2021). Nonetheless, distribution shift between meta-train and meta-test tasks remains a key obstacle.",
    "reason": "Trailing semicolon before the closing parenthesis in a single-source citation; incorrect citation punctuation/format.",
    "start": 573,
    "end": 588,
    "label": "Format"
  },
  {
    "span": "Brown et al. (2020) introduced in-context learning with large language models. Schick and Schütze (2021) proposed pattern-exploiting training for few-shot classification. Gao et al. (2021) explored prompt-based retrieval for zero-shot tasks. Lester et al. (2021) used soft prompts to steer frozen models.",
    "document": "Related Work\n\nPrompting for Few- and Zero-Shot Learning\n\nRecent advances have shown that prompting can dramatically reduce the amount of labeled data needed for adaptation. Methods differ in whether they tune model parameters, prompts, or both, and in how they leverage unlabeled corpora. Despite rapid progress, connections among prompting strategies remain under-specified in much of the literature.\n\nBrown et al. (2020) introduced in-context learning with large language models. Schick and Schütze (2021) proposed pattern-exploiting training for few-shot classification. Gao et al. (2021) explored prompt-based retrieval for zero-shot tasks. Lester et al. (2021) used soft prompts to steer frozen models.\n\nCalibration and Stability\n\nA parallel thread studies reliability under distribution shift, including prompt calibrations, verbalizer robustness, and selection bias in demonstrations. These works typically isolate one factor at a time and evaluate on mixed-domain benchmarks, but their implications for multi-task generalization remain unclear.\n\nOur Position\n\nWe investigate a unified prompting framework that jointly optimizes discrete and continuous prompts while constraining calibration error, contrasting with prior approaches that treat these axes independently.",
    "reason": "The span lists multiple prompting papers in separate sentences without transitions or explicit relationships between them, making the connection between works abrupt and unclear.",
    "start": 403,
    "end": 707,
    "label": "Coherence"
  },
  {
    "span": "Unsupervised domain adaptation uses adversarial objectives to align feature distributions (Ganin et al., 2016). SpecAugment perturbs time-frequency masks for robustness (Park et al., 2019). Wav2Vec 2.0 pretrains representations on raw audio (Baevski et al., 2020). Vocal tract length perturbation alters speed and pitch (Jaitly and Hinton, 2013).",
    "document": "Related Work\n\nDomain mismatch remains a central challenge in automatic speech recognition (ASR), where systems trained on one acoustic or linguistic domain often degrade on another. Approaches to robustness include domain adaptation, self-supervised pretraining, and data augmentation.\n\nUnsupervised domain adaptation uses adversarial objectives to align feature distributions (Ganin et al., 2016). SpecAugment perturbs time-frequency masks for robustness (Park et al., 2019). Wav2Vec 2.0 pretrains representations on raw audio (Baevski et al., 2020). Vocal tract length perturbation alters speed and pitch (Jaitly and Hinton, 2013).\n\nBeyond these, multi-condition training with heterogeneous corpora (Ko et al., 2015) and pseudo-labeling on target domains (Kahn et al., 2020) have reduced domain gaps. Recent works also leverage adapter layers to specialize large encoders to new conditions with few parameters (Houlsby et al., 2019). Our contribution integrates test-time adaptation with uncertainty-aware decoding to better handle streaming nonstationarity.",
    "reason": "The span lists adaptation, augmentation, and pretraining methods in separate sentences without transitions or explanations linking them, making the inter-paper relevance implicit and incoherent.",
    "start": 287,
    "end": 633,
    "label": "Coherence"
  },
  {
    "span": "((Smith et al., 2017))",
    "document": "Related Work\n\nHuman–robot collaboration requires anticipating human intent from partial observations. Classical approaches rely on inverse optimal control with hand-crafted features (Ziebart et al., 2008), while recent learning-based models leverage deep generative forecasting (Rhinehart et al., 2019; Scholler et al., 2020). Anticipatory planning can reduce collision risk in shared workspaces, as shown in ((Smith et al., 2017)), but these methods often assume stationary human policies. Our work relaxes these assumptions by modeling context-dependent intent changes.",
    "reason": "Double parentheses around the citation; should be a single set of parentheses.",
    "start": 409,
    "end": 431,
    "label": "Format"
  },
  {
    "span": "Classical motion planning leverages sampling-based planners (PRM, RRT, RRT*) and trajectory optimization methods (CHOMP, STOMP, TrajOpt) to compute feasible or locally optimal paths under kinematic and dynamic constraints (Kavraki et al., 1996; LaValle, 1998; Karaman and Frazzoli, 2011; Ratliff et al., 2009; Schulman et al., 2013). Recent work integrates learned cost models and collision predictors to accelerate planning in cluttered environments.",
    "document": "Introduction: Learning-Augmented Motion Planning\n\nRobotic manipulators operate in complex, partially observed spaces where real-time planning must balance feasibility, smoothness, and safety. Combining model-based search with learned components promises improved sample efficiency and adaptability.\n\nClassical motion planning leverages sampling-based planners (PRM, RRT, RRT*) and trajectory optimization methods (CHOMP, STOMP, TrajOpt) to compute feasible or locally optimal paths under kinematic and dynamic constraints (Kavraki et al., 1996; LaValle, 1998; Karaman and Frazzoli, 2011; Ratliff et al., 2009; Schulman et al., 2013). Recent work integrates learned cost models and collision predictors to accelerate planning in cluttered environments.\n\nWe present a planner that couples a learned distance field with a warm-started trajectory optimizer.",
    "reason": "The span summarizes past planners and trends but does not articulate which deficiencies motivate the new method or how it improves upon them; no synthesis is offered (criteria a and c).",
    "start": 300,
    "end": 751,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Smith et al.",
    "document": "Introduction\n\nOpen-domain information extraction seeks to populate structured knowledge from unstructured text at scale. Following Smith et al., we adopt a pipeline that decouples mention detection from relation typing and adds a calibration step to control precision–recall trade-offs. While recent transformer-based encoders have improved contextualization (Baker and Rao, 2020; Lin et al., 2021), robust generalization to emerging entities remains challenging due to distributional drift across time and domains.",
    "reason": "Narrative citation missing year: should be “Smith et al. (YEAR)” (e.g., “Smith et al. (2019)”), but the year is omitted.",
    "start": 131,
    "end": 143,
    "label": "Format"
  },
  {
    "span": "For molecular property prediction, message passing neural networks, attention-based graph encoders, and graph isomorphism networks have been widely employed (Gilmer et al., 2017; Veličković et al., 2018; Xu et al., 2019). Pretraining strategies range from masked atom/bond modeling to contrastive learning over augmented views (Hu et al., 2019; Rong et al., 2020; Sun et al., 2020).",
    "document": "Introduction\n\nPredicting molecular properties from structure underpins discovery in materials science and drug design. Machine learning on molecular graphs offers scalable approximations to expensive quantum calculations and wet-lab assays, but faces challenges in data sparsity, distribution shift, and limited interpretability.\n\nFor molecular property prediction, message passing neural networks, attention-based graph encoders, and graph isomorphism networks have been widely employed (Gilmer et al., 2017; Veličković et al., 2018; Xu et al., 2019). Pretraining strategies range from masked atom/bond modeling to contrastive learning over augmented views (Hu et al., 2019; Rong et al., 2020; Sun et al., 2020).\n\nRecent works investigate uncertainty estimation and calibration for molecular predictions, including ensembles, Bayesian approximations, and temperature scaling (Lakshminarayanan et al., 2017; Maddox et al., 2019; Levi et al., 2020). Domain shift between assay conditions and novel chemistries further complicates generalization (Stanley et al., 2021; Yang et al., 2019).\n\nWe focus on reliable out-of-distribution performance under scaffold splits, proposing a structure-aware calibration method that leverages subgraph energy regularization. Our approach complements existing pretraining methods by improving trustworthiness when extrapolating to novel scaffolds.",
    "reason": "The span summarizes prior models and pretraining approaches without clarifying how they inform this work or specifying what limitation the paper addresses.",
    "start": 331,
    "end": 713,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT was used in an AES task trained on essays from the ASAP dataset",
    "document": "Related Work\n\nAutomated essay scoring (AES) models have evolved from handcrafted features to neural architectures that learn holistic and trait-specific representations. Pretrained language models have recently shown promise for transfer across prompts and genres.\n\nBERT was used in an AES task trained on essays from the ASAP dataset, demonstrating strong performance with limited prompt-specific tuning. However, robustness to adversarially perturbed essays remains underexplored.\n\nWe propose a calibration-aware AES model that leverages uncertainty estimation to improve scoring reliability under distribution shift.",
    "reason": "References a prior setup (BERT on ASAP AES) without citing the corresponding paper or dataset (rule a).",
    "start": 266,
    "end": 334,
    "label": "Unsupported_claim"
  },
  {
    "span": "We evaluate on the widely used MedNotes dataset.",
    "document": "Introduction\nClinical concept extraction systems must operate reliably across departments and note types while coping with abbreviations and idiosyncratic documentation styles. Progress has been driven by transformer-based encoders and techniques for domain adaptation.\nWe evaluate on the widely used MedNotes dataset. To understand domain shift, we additionally construct a holdout set from outpatient notes that differ in structure and terminology from inpatient records. Our experimental setup emphasizes label efficiency by focusing on scenarios with limited annotated data.\nWe report both span-level and entity-level metrics and analyze common error modes, including boundary errors and confusion between semantically similar concepts.",
    "reason": "First mention of a specific dataset lacks a citation to its source or introduction paper.",
    "start": 270,
    "end": 318,
    "label": "Unsupported_claim"
  },
  {
    "span": "Approaches include statistical tests, autoregressive forecasting residuals, isolation forests, and deep autoencoders/LSTMs (Box and Jenkins, 1970; Liu et al., 2008; Malhotra et al., 2015).",
    "document": "Introduction\n\nDetecting anomalies in multivariate time series underpins monitoring in finance, manufacturing, and IT operations. The challenge is to flag rare, heterogeneous deviations with few labels and evolving baselines.\n\nApproaches include statistical tests, autoregressive forecasting residuals, isolation forests, and deep autoencoders/LSTMs (Box and Jenkins, 1970; Liu et al., 2008; Malhotra et al., 2015).\n\nWe study anomaly detection under nonstationary covariance drift and propose a contrastive forecaster that disentangles seasonal components from residual dynamics for robust detection.",
    "reason": "The span lists categories of methods and citations without discussing their assumptions or connection to the paper's focus on nonstationarity, thus lacking synthesis per (a) and (c).",
    "start": 226,
    "end": 414,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Sun et al., 2018,; Park, 2019)",
    "document": "Related Work\n\nControllable text generation modifies decoding to satisfy attribute targets like sentiment or formality (Hu et al., 2017; Dathathri et al., 2020). Discriminators and attribute classifiers guide sampling through gradient or likelihood reweighting (Krause et al., 2021). Recent work explores plug-and-play methods (Sun et al., 2018,; Park, 2019) that avoid retraining the base language model. Our method introduces a calibration step to mitigate exposure bias during control.",
    "reason": "Malformed separator in multiple citations; contains an extra comma before the semicolon. Should be formatted like \"(Sun et al., 2018; Park, 2019)\".",
    "start": 326,
    "end": 357,
    "label": "Format"
  },
  {
    "span": "CMU-MOSI and CMU-MOSEI provide benchmarks (Zadeh et al., 2016; Zadeh et al., 2018). Tensor Fusion Network models unimodal and bimodal interactions (Zadeh et al., 2017). MulT leverages cross-modal transformers (Tsai et al., 2019). Audio feature extraction with openSMILE is standard (Eyben et al., 2010).",
    "document": "Related Work\n\nMultimodal Sentiment Analysis\n\nUnderstanding sentiment and emotion from language, vision, and audio requires modeling cross-modal interactions and temporal dependencies.\n\nDatasets and Modeling Approaches\n\nCMU-MOSI and CMU-MOSEI provide benchmarks (Zadeh et al., 2016; Zadeh et al., 2018). Tensor Fusion Network models unimodal and bimodal interactions (Zadeh et al., 2017). MulT leverages cross-modal transformers (Tsai et al., 2019). Audio feature extraction with openSMILE is standard (Eyben et al., 2010).\n\nGeneralization and Robustness\n\nDomain shifts across speakers and recording conditions motivate adaptation and calibration strategies (Sun et al., 2019; Chen et al., 2020). We investigate label noise mitigation under weak supervision.",
    "reason": "The paragraph jumps between datasets, model architectures, and feature extraction without transitions or explaining relevance between sentences, leading to poor coherence.",
    "start": 219,
    "end": 522,
    "label": "Coherence"
  },
  {
    "span": "OGB-Products and OGB-ArXiv",
    "document": "Introduction\n\nGraph neural networks (GNNs) have achieved strong performance on node classification, link prediction, and graph-level tasks by iteratively aggregating neighborhood information. Despite their success, oversmoothing and scalability remain open challenges, particularly on web-scale graphs with noisy edges and skewed degree distributions.\n\nA common line of work improves scalability through sampling or linearization of message passing, while others enhance expressivity via attention and higher-order structures. To facilitate fair comparison, recent benchmarks encourage standardized splits, metrics, and preprocessing pipelines. We adopt widely used large-scale node classification datasets, including Reddit, OGB-Products and OGB-ArXiv, and an internal e-commerce graph with tens of millions of nodes.\n\nWe propose a decoupled architecture with feature propagation precomputation and a lightweight residual predictor. Our method minimizes inter-GPU communication and supports dynamic subgraph caching to amortize neighborhood expansion across epochs. Additionally, we introduce degree-adaptive normalization that mitigates hub dominance without sacrificing efficiency.\n\nExperiments show that our approach achieves competitive accuracy while reducing training time and memory usage. We conduct extensive ablations to quantify the contribution of each component and analyze robustness under edge perturbations and feature corruption.",
    "reason": "Mentions specific benchmark datasets without providing the customary citations to their originating papers or benchmark reports.",
    "start": 726,
    "end": 752,
    "label": "Unsupported_claim"
  },
  {
    "span": "Lillicrap et al. (2015) introduced DDPG for continuous control. Hessel et al. (2018) combined multiple improvements into Rainbow DQN. Data augmentation improves generalization in RL (Laskin et al., 2020). Model-based RL learns dynamics to plan (Janner et al., 2019).",
    "document": "Introduction\n\nImproving sample efficiency in reinforcement learning (RL) remains a central challenge, particularly for real-world control where interactions are costly. Methods span off-policy learning, representation learning, and model-based planning, often trading off stability, bias, and computational cost.\n\nLillicrap et al. (2015) introduced DDPG for continuous control. Hessel et al. (2018) combined multiple improvements into Rainbow DQN. Data augmentation improves generalization in RL (Laskin et al., 2020). Model-based RL learns dynamics to plan (Janner et al., 2019).\n\nWe develop an uncertainty-aware replay strategy that prioritizes transitions likely to reduce model error, yielding better sample usage across task suites.",
    "reason": "The span lists heterogeneous RL lines of work without transitions or explicit relationships, leaving the reader to infer connections between algorithms, improvements, and orthogonal techniques.",
    "start": 314,
    "end": 580,
    "label": "Coherence"
  },
  {
    "span": "Automated feedback methods include rule-based systems that match student text against rubric descriptors (Burstein et al., 2010), feature-based classifiers for error detection (Ng et al., 2014), and neural sequence taggers trained on annotated learner corpora (Rei and Yannakoudakis, 2016; Bryant et al., 2017).",
    "document": "Related Work\n\nProviding timely, actionable feedback to students at scale is a major goal of educational NLP. Systems must balance coverage across skill dimensions with the reliability required to inform learning.\n\nAutomated feedback methods include rule-based systems that match student text against rubric descriptors (Burstein et al., 2010), feature-based classifiers for error detection (Ng et al., 2014), and neural sequence taggers trained on annotated learner corpora (Rei and Yannakoudakis, 2016; Bryant et al., 2017).\n\nOur work focuses on coaching-style feedback that prioritizes revision strategies over surface error flags. We introduce a multi-level rationale generator that grounds suggestions in detected discourse moves and evidence spans.",
    "reason": "Summarizes prior categories without relating them to the proposed coaching-style feedback or identifying where existing systems fall short.",
    "start": 214,
    "end": 525,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT was used in an AES task trained on high-school essays to model coherence",
    "document": "Related Work\n\nAutomated essay scoring (AES) leverages natural language processing to assess writing quality along dimensions such as grammar, coherence, and argumentation. Early systems relied on surface features, while more recent approaches adopt pretrained language models.\n\nBERT was used in an AES task trained on high-school essays to model coherence. Other lines of work introduce hierarchical encoders to capture discourse flow and rubric-specific heads for trait scoring.\n\nIn contrast, our approach integrates sentence-level discourse cues with prompt-aware conditioning, improving calibration across prompts and mitigating overfitting to lexical spurious cues.\n",
    "reason": "Describes a specific prior setup ('BERT... AES task... high-school essays') without citing the source; per rule (a) and example (iii), such mentions require a citation.",
    "start": 278,
    "end": 355,
    "label": "Unsupported_claim"
  },
  {
    "span": "Kumar and Rao, (2022)",
    "document": "Introduction\n\nFederated learning enables collaborative training without centralizing raw data, improving privacy and regulatory compliance (McMahan et al., 2017; Kairouz et al., 2021). Communication efficiency and heterogeneity handling are central challenges addressed by compression, partial participation, and personalization (Sattler et al., 2019; Li et al., 2020).\n\nKumar and Rao, (2022) report that personalization layers reduce negative transfer in cross-silo deployments, while Yu et al. (2021) highlight stability issues under client drift. Building on these insights, we develop a meta-initialization that adapts to client clusters with minimal overhead.\n",
    "reason": "Incorrect punctuation for narrative citation; the comma before the parenthetical year should be removed: \"Kumar and Rao (2022)\".",
    "start": 371,
    "end": 392,
    "label": "Format"
  },
  {
    "span": "According to industry reports, more than 70% of newsrooms now rely on automated headline generation.",
    "document": "Introduction\n\nHeadline generation demands concise relevance modeling and pragmatic awareness of audience interests, often under strict length constraints (Rush et al., 2015; Nallapati et al., 2016). Pretrained encoder-decoder models achieve strong fluency but can sacrifice faithfulness without explicit grounding or coverage controls (See et al., 2017; Kryscinski et al., 2020).\n\nAccording to industry reports, more than 70% of newsrooms now rely on automated headline generation.\n\nMotivated by real-world adoption concerns, we propose a fact-aware headline model with calibration to mitigate overconfident hallucinations, and we evaluate safety-critical failure modes with causal probes.",
    "reason": "Presents a quantitative statistic attributed vaguely to 'industry reports' without any citation or evidence.",
    "start": 381,
    "end": 481,
    "label": "Unsupported_claim"
  },
  {
    "span": "Kipf and Welling (2017) introduced GCNs for semi-supervised learning on graphs. Ying et al. (2018) proposed PinSage for web-scale recommendation. He et al. (2020) presented LightGCN to simplify message passing by removing feature transformations. Wu et al. (2019) surveyed graph neural networks.",
    "document": "Related Work\n\nRecommender Systems and Collaborative Filtering\nMatrix factorization and neighborhood-based approaches remain foundational for top-N recommendation (Koren et al., 2009; Rendle et al., 2009). With the advent of deep learning, neural collaborative filtering architectures have improved the expressivity of user–item interactions by learning non-linear patterns (He et al., 2017). Sequential recommendation further models user intent drift via recurrent and attention-based architectures (Hidasi et al., 2016; Kang and McAuley, 2018).\n\nGraph-based Recommendation\nUser–item interactions, side information, and social ties can be naturally represented as graphs, motivating message-passing methods to propagate signals across multi-hop neighborhoods. Kipf and Welling (2017) introduced GCNs for semi-supervised learning on graphs. Ying et al. (2018) proposed PinSage for web-scale recommendation. He et al. (2020) presented LightGCN to simplify message passing by removing feature transformations. Wu et al. (2019) surveyed graph neural networks.\n\nContrastive and Self-Supervised Signals\nRecent work introduces auxiliary graph views and pretext tasks to combat sparsity, e.g., contrastive objectives over perturbed topologies and bootstrapped representations (Velickovic et al., 2019; He et al., 2020b; Yu et al., 2022). These methods typically decouple backbone modeling from augmentation design. Our approach differs by tying augmentation selection to recommendation uncertainty.",
    "reason": "The span lists four citations in succession without transitions or explicit relational links (e.g., how PinSage relates to LightGCN or how a survey contextualizes the prior methods). The connection to the preceding sentence about graph recommendation is abrupt and the relationships among the cited works are implied but not stated.",
    "start": 760,
    "end": 1055,
    "label": "Coherence"
  },
  {
    "span": "the GLUE benchmark",
    "document": "Related Work\n\nGeneral-purpose NLU evaluation has been popularized by the GLUE benchmark and its successor, SuperGLUE. Beyond these aggregate suites, task-specific benchmarks have continued to evolve in reading comprehension (Rajpurkar et al., 2018), natural language inference (Bowman et al., 2015), and commonsense reasoning (Zellers et al., 2019). Recent works investigate whether aggregate scores mask uneven generalization across subskills (Yogatama et al., 2019; Ribeiro et al., 2020). Our work complements these analyses by proposing a factorized evaluation protocol that decomposes model behavior into calibration, robustness, and data efficiency axes.",
    "reason": "First mention of a well-known dataset/benchmark must include a citation; the reference for GLUE is missing, making the claim unsupported.",
    "start": 69,
    "end": 87,
    "label": "Unsupported_claim"
  },
  {
    "span": "Macosko et al. (2015) introduced Drop-seq for high-throughput profiling. Butler et al. (2018) developed Seurat for clustering and visualization. Haghverdi et al. (2018) proposed mutual nearest neighbors for batch correction. Eraslan et al. (2019) applied deep autoencoders for denoising. Spatial transcriptomics links expression to histology (Ståhl et al., 2016).",
    "document": "Related Work\n\nSingle-Cell Transcriptomics and Integration Methods\n\nSingle-cell RNA sequencing enables the discovery of cellular heterogeneity and dynamic trajectories, but batch effects and technical noise pose challenges for integration. We propose a probabilistic alignment approach that preserves rare cell types.\n\nMacosko et al. (2015) introduced Drop-seq for high-throughput profiling. Butler et al. (2018) developed Seurat for clustering and visualization. Haghverdi et al. (2018) proposed mutual nearest neighbors for batch correction. Eraslan et al. (2019) applied deep autoencoders for denoising. Spatial transcriptomics links expression to histology (Ståhl et al., 2016).\n\nOur framework unifies batch correction and trajectory inference via shared latent variables with identifiable priors.",
    "reason": "The list jumps across acquisition, clustering, batch correction, denoising, and spatial assays without transitions; relationships between these contributions are not explained.",
    "start": 318,
    "end": 681,
    "label": "Coherence"
  },
  {
    "span": "HumanEval has recently been extended to cover statically typed languages.",
    "document": "Introduction\n\nCode generation benchmarks have catalyzed rapid progress in program synthesis with large language models. Nevertheless, evaluation often focuses on narrow sets of problems and languages, which can obscure generalization shortcomings. Test-time execution-based metrics are sensitive to harness design and test coverage, further complicating comparisons across models.\n\nHumanEval has recently been extended to cover statically typed languages. Despite broader language coverage, differences in type systems and standard libraries introduce comparability challenges. In this work, we propose a cross-language normalization protocol and a suite of semantic equivalence tests to enable fairer evaluations across programming languages.\n\nRelated Work\n\nPrior efforts include synthesis from docstrings, competitive programming benchmarks, and multi-language corpora. Recent studies scrutinize data leakage, unit test brittleness, and prompt sensitivity in code LLMs.",
    "reason": "Claims a specific dataset extension without citing the source; per rule a, mentions of datasets and updates must be supported by citations.",
    "start": 382,
    "end": 455,
    "label": "Unsupported_claim"
  },
  {
    "span": "The majority of MOOCs use automatic short-answer grading.",
    "document": "Introduction\n\nScalable assessment is central to large-scale online education platforms. Automatic grading for open-ended responses has leveraged supervised models, rubric-aligned features, and pretrained language models to approximate instructor scores (Zhang et al., 2020; Riordan et al., 2017; Uto et al., 2020). Reliability concerns motivate calibration and human-in-the-loop workflows, particularly for high-stakes items (Williamson et al., 2020).\n\nThe majority of MOOCs use automatic short-answer grading. Despite adoption, challenges persist in domain transfer, prompt specificity, and adversarial responses. We present RubricAlign, a grading system that jointly learns to predict scores and generate rubric-aligned rationales to support instructor oversight.",
    "reason": "Asserts a quantitative prevalence claim about MOOC practices without providing evidence or citation.",
    "start": 453,
    "end": 510,
    "label": "Unsupported_claim"
  },
  {
    "span": "Neural approaches to program synthesis include sequence-to-sequence models with attention (Ling et al., 2016), pointer networks for copying identifiers (Vinyals et al., 2015), grammar-based decoders (Yin and Neubig, 2017), and neuro-symbolic hybrids that integrate program interpreters (Gaunt et al., 2016; Ellis et al., 2019).",
    "document": "Introduction\n\nProgram synthesis from natural language aims to generate executable code consistent with a specification provided in text or I/O examples. Recent neural models have improved coverage and generalization by leveraging large code corpora.\n\nNeural approaches to program synthesis include sequence-to-sequence models with attention (Ling et al., 2016), pointer networks for copying identifiers (Vinyals et al., 2015), grammar-based decoders (Yin and Neubig, 2017), and neuro-symbolic hybrids that integrate program interpreters (Gaunt et al., 2016; Ellis et al., 2019).\n\nWe investigate a constrained decoding strategy over a domain-specific language and test it on two benchmarks.",
    "reason": "The span catalogs methods and citations without interpreting their differences or positioning the authors’ approach, thus lacking synthesis.",
    "start": 251,
    "end": 578,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Biomedical entity linking has leveraged dictionary-based matching with fuzzy similarity, graph-based disambiguation, and pre-trained language models fine-tuned on ontology-aligned corpora (Arighi et al., 2017; Leaman et al., 2013; Ji et al., 2020; Sung et al., 2021).",
    "document": "Related Work\n\nEntity linking maps textual mentions to canonical concepts in biomedical ontologies, supporting literature search, knowledge base construction, and clinical decision support. Cross-lingual settings are particularly challenging due to variation in morphology, transliteration, and resource availability.\n\nBiomedical entity linking has leveraged dictionary-based matching with fuzzy similarity, graph-based disambiguation, and pre-trained language models fine-tuned on ontology-aligned corpora (Arighi et al., 2017; Leaman et al., 2013; Ji et al., 2020; Sung et al., 2021). Recent benchmarks report improved accuracy on English corpora, but cross-lingual generalization remains inconsistent.\n\nWe present XMedLink, a multilingual encoder trained with contrastive alignment to unify mention and concept representations across languages using weakly aligned dictionaries.\n",
    "reason": "The span lists categories of methods and citations without explaining their limitations in cross-lingual contexts or how they inform the proposed approach; it lacks explicit synthesis with the authors’ goals.",
    "start": 318,
    "end": 585,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In a previous study, the authors claim that patch correctness correlates with test suite size.",
    "document": "Related Work\n\nAutomated program repair (APR) systems aim to generate patches that pass a given test suite, often using search-based or learning-based strategies. While test-suite adequacy guides patch validation, overfitting to insufficient tests can yield plausible but incorrect fixes. In a previous study, the authors claim that patch correctness correlates with test suite size. Subsequent work examines patch validation beyond unit tests, including semantic equivalence checking and differential testing against reference implementations. Our approach complements these efforts by integrating specification mining to filter overfitting patches before expensive validation.\n",
    "reason": "This sentence refers to a specific prior study and its claim but does not provide a citation (rule a and example ii).",
    "start": 288,
    "end": 382,
    "label": "Unsupported_claim"
  },
  {
    "span": "WMT14 En-De",
    "document": "Introduction\n\nNeural machine translation (NMT) has achieved strong results with transformer architectures and large-scale pretraining (Vaswani et al., 2017; Liu et al., 2020). Standard English-German benchmarks include WMT14 En-De and WMT16 En-De, which feature millions of sentence pairs from news domains. Scaling model depth and data often leads to consistent BLEU improvements (Ott et al., 2018; Ng et al., 2019). In this work, we explore cross-lingual pretraining for low-resource transfer.",
    "reason": "First mention of a widely used benchmark dataset without a corresponding citation.",
    "start": 219,
    "end": 230,
    "label": "Unsupported_claim"
  },
  {
    "span": "Work on fairness in ranking considers group fairness via exposure constraints, demographic parity, and equality of opportunity (Zehlike et al., 2017; Singh and Joachims, 2018; Biega et al., 2018), as well as individual fairness through pairwise consistency or Lipschitz conditions (Joseph et al., 2016; Kearns et al., 2018; Yadav et al., 2021).",
    "document": "Related Work\n\nRanking systems can amplify historical biases, leading to unfair exposure and unequal opportunities. A growing body of research proposes fairness-aware objectives and constraints for learning-to-rank.\n\nWork on fairness in ranking considers group fairness via exposure constraints, demographic parity, and equality of opportunity (Zehlike et al., 2017; Singh and Joachims, 2018; Biega et al., 2018), as well as individual fairness through pairwise consistency or Lipschitz conditions (Joseph et al., 2016; Kearns et al., 2018; Yadav et al., 2021).\n\nIn this study, we evaluate a constrained optimization approach on synthetic and real-world datasets.",
    "reason": "The span lists strands of literature but does not explain how they interrelate or how the present work addresses any specific shortcoming, lacking synthesis.",
    "start": 216,
    "end": 560,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Nguyen, 2019, Li, 2020)",
    "document": "Introduction\n\nCross-lingual transfer leverages shared subword structure and multilingual pre-training to enable zero-shot generalization (Conneau et al., 2020). However, label-space mismatches and culture-specific semantics hamper downstream accuracy. Prior work proposes ontology mapping and prompt-based adapters to mitigate these gaps (Zhao and Schütze, 2021). Empirical studies (Nguyen, 2019, Li, 2020) report that calibration errors are amplified under distribution shift, motivating temperature scaling and label smoothing for multilingual settings.\n\nOur study introduces a language-aware calibrator that conditions on typological features to improve confidence estimates across unseen languages.",
    "reason": "Improper separation in a multi-citation; items within a single parenthetical list should be separated by semicolons: '(Nguyen, 2019; Li, 2020)'.",
    "start": 382,
    "end": 406,
    "label": "Format"
  },
  {
    "span": "IEMOCAP remains the most widely used dataset for speech emotion recognition.",
    "document": "Introduction\n\nSpeech emotion recognition (SER) seeks to infer affective states from acoustic and linguistic cues. Canonical approaches rely on prosodic features and deep representations extracted by convolutional and recurrent networks (Schuller et al., 2011; Trigeorgis et al., 2016; Neumann and Vu, 2017). More recently, self-supervised speech encoders have advanced SER performance (Baevski et al., 2020; Hsu et al., 2021).\n\nCommon corpora include IEMOCAP (Busso et al., 2008), RAVDESS (Livingstone and Russo, 2018), and MSP-IMPROV (Busso et al., 2016). IEMOCAP remains the most widely used dataset for speech emotion recognition. However, significant domain mismatches across datasets limit cross-corpus generalization.\n\nWe present a multi-corpus training strategy with domain-adaptive normalization and evaluate transfer across four English SER benchmarks.",
    "reason": "Asserts dataset prevalence without evidence or citation to surveys or usage statistics (rule b).",
    "start": 557,
    "end": 633,
    "label": "Unsupported_claim"
  },
  {
    "span": "Bengio et al. (2019) argued for learning disentangled causal mechanisms. Locatello et al. (2019) proved impossibility of unsupervised disentanglement without inductive biases. Schölkopf (2021) surveyed causal representation learning. Spirtes et al. (2000) formalized constraint-based causal discovery.",
    "document": "Introduction and Related Work\n\nCausal Representation Learning\nConnecting high-level structure to observed data promises robustness and transfer under interventions and distribution shifts (Schölkopf et al., 2021). Vision settings raise additional challenges due to entangled factors and complex generative processes.\n\nFoundations and Challenges\nRecent work debates the feasibility of disentanglement and the role of interventions and inductive biases. Bengio et al. (2019) argued for learning disentangled causal mechanisms. Locatello et al. (2019) proved impossibility of unsupervised disentanglement without inductive biases. Schölkopf (2021) surveyed causal representation learning. Spirtes et al. (2000) formalized constraint-based causal discovery.\n\nBenchmarks and Evaluations\nEfforts to evaluate causal features in vision employ synthetic datasets with known generative factors and interventional protocols (Gresele et al., 2021; Ke et al., 2019). Our method targets interventional robustness with structured augmentations.",
    "reason": "The span introduces several works with no explicit connective tissue explaining how they relate (e.g., how impossibility results inform mechanisms or how discovery methods tie to representations). The transitions are minimal and the relationships are implied.",
    "start": 452,
    "end": 753,
    "label": "Coherence"
  },
  {
    "span": "Isolation Forest isolates anomalies via random partitioning (Liu et al., 2008). LSTM-based predictors flag deviations from expected values (Malhotra et al., 2015). Seasonal decomposition is often applied to stabilize variance (Cleveland et al., 1990).",
    "document": "Related Work\n\nTime Series Anomaly Detection. Modern systems detect anomalies by modeling normal behavior and flagging deviations. Probabilistic approaches include Gaussian processes and state-space models, while deep learning methods leverage autoencoders and sequence models to capture nonlinear dependencies (Anderson et al., 2016; Audibert et al., 2020).\n\nClassical and Deep Methods. Reconstruction-based methods such as convolutional autoencoders measure error between inputs and reconstructions to identify outliers (Zhou and Paffenroth, 2017). Prediction-based methods train forecasters and compare predicted and observed values to compute surprise (Hundman et al., 2018). Isolation Forest isolates anomalies via random partitioning (Liu et al., 2008). LSTM-based predictors flag deviations from expected values (Malhotra et al., 2015). Seasonal decomposition is often applied to stabilize variance (Cleveland et al., 1990). We focus on multivariate, irregularly sampled data with strong seasonal trends.\n\nMultivariate and Irregular Sampling. Techniques for handling missingness and irregular intervals include interpolation schemes, temporal convolution, and neural ODEs (Rubanova et al., 2019; Shukla and Marlin, 2019). Our approach integrates a differentiable decomposition layer with contrastive forecasting losses to improve detection under covariate shift.",
    "reason": "The span abruptly lists three disparate techniques (Isolation Forest, LSTM predictors, seasonal decomposition) without transitions or an explicit connection among them, leaving their relationships and relevance to each other unclear.",
    "start": 679,
    "end": 930,
    "label": "Coherence"
  },
  {
    "span": "Brown et al., (2015)",
    "document": "Introduction\n\nUnsupervised domain adaptation aims to transfer knowledge from a labeled source to an unlabeled target domain. Discrepancy minimization reduces distribution gaps via moment matching or adversarial alignment (Long et al., 2015; Ganin et al., 2016). Brown et al., (2015) introduced an early consistency regularizer but did not account for class-conditional shifts, which later work addressed with conditional adversarial objectives (Long et al., 2018).\n\nWe extend this line by coupling label-aware alignment with a semantic neighborhood constraint that preserves local structure.",
    "reason": "Mixed narrative/parenthetical formatting: narrative form should be Brown et al. (2015) without the comma before the parenthetical year.",
    "start": 262,
    "end": 282,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors claimed that low-dose CT reduces radiation by 70%",
    "document": "Introduction\n\nLow-dose computed tomography (LDCT) aims to reduce patient radiation exposure while preserving diagnostic quality. In a previous study, the authors claimed that low-dose CT reduces radiation by 70% without compromising nodule detectability, motivating learning-based denoising approaches. Classical methods such as BM3D struggle to preserve fine textures, whereas CNN-based models have shown promising results (Chen et al., 2017; Kang et al., 2017). Despite progress, hallucination and oversmoothing remain concerns. We investigate a self-supervised framework that constrains texture fidelity via frequency-domain priors.",
    "reason": "Refers to a prior study and its quantitative claim but provides no citation to identify the study.",
    "start": 129,
    "end": 211,
    "label": "Unsupported_claim"
  },
  {
    "span": "Garcia et al. 2",
    "document": "Related Work\n\nDebiasing word embeddings. A large body of work studies methods to reduce social bias in distributional representations (Bolukbasi et al., 2016; Zhao et al., 2018). Garcia et al. 2 report that linear projection can over-correct and harm downstream performance, whereas context-sensitive masking yields more stable gains (Ravfogel et al., 2020; Liang et al., 2020). Nevertheless, most evaluations focus on English and monolingual corpora (Gonen and Goldberg, 2019; Lauscher et al., 2020).\n\nCross-lingual fairness. Recent studies highlight disparities across languages in multilingual models (Pires et al., 2019; Lauscher and Glavaš, 2019).",
    "reason": "Improper footnote-style marker appended to the author string without a year or proper footnote formatting; should include the year or be reformatted as a proper footnote or parenthetical citation.",
    "start": 179,
    "end": 194,
    "label": "Format"
  },
  {
    "span": "it is well known that layer-wise distillation yields better generalization than logits-only",
    "document": "Related Work\n\nKnowledge distillation compresses large transformers into smaller students by matching teacher signals. Early methods focused on logits, while subsequent techniques incorporate intermediate representations to preserve hierarchical features. In this context, it is well known that layer-wise distillation yields better generalization than logits-only, particularly when depth mismatch is pronounced. Nevertheless, layer alignment introduces sensitivity to architectural choices and computational overhead during training. Our method bridges these regimes by using projector-assisted signals that approximate layer supervision without strict alignment.",
    "reason": "Asserts a field-wide consensus on a specific technique superiority without citing supporting studies (rule b; niche technical claim needs citations).",
    "start": 272,
    "end": 363,
    "label": "Unsupported_claim"
  },
  {
    "span": "SemEval-2014 Aspect-Based Sentiment Analysis shared task",
    "document": "Related Work\n\nAspect-based sentiment analysis (ABSA) decomposes overall sentiment into opinions about specific aspects, enabling fine-grained opinion mining in reviews. The SemEval-2014 Aspect-Based Sentiment Analysis shared task galvanized interest in standardized evaluation and established strong baselines for aspect extraction and sentiment classification. Subsequent efforts investigated attention mechanisms, graph-based representations, and pretraining to capture aspect–context interactions. Our work complements these by introducing a contrastive objective that aligns aspect spans with sentiment-bearing cues.",
    "reason": "Mentions a specific shared task without providing a citation at first mention (rule a).",
    "start": 173,
    "end": 229,
    "label": "Unsupported_claim"
  },
  {
    "span": "We follow the setup of the HASOC shared task for offensive language identification in Indic languages.",
    "document": "Related Work\n\nAutomatic hate speech and offensive language detection has accelerated with the availability of social media corpora, subword tokenization methods, and multilingual encoders. While high-resource languages benefit from large annotated datasets and transfer learning, coverage in Indic languages remains limited and domain-specific. We follow the setup of the HASOC shared task for offensive language identification in Indic languages. Prior efforts in this area explore label harmonization across platforms, transfer from English to code-mixed text, and robust handling of dialectal variation using subword and character-level models.\n\nBeyond supervised learning, weak supervision and data programming have been explored to mitigate annotation costs. Contrastive learning on user-level aggregates and adversarial training against spurious cues have also been proposed to improve cross-domain generalization. Nevertheless, consistent evaluation protocols for code-mixed and transliterated content remain under-specified, motivating clearer task definitions and benchmarks.",
    "reason": "Refers to a specific shared task ('HASOC') without providing a citation at first mention, violating rule (a).",
    "start": 345,
    "end": 447,
    "label": "Unsupported_claim"
  },
  {
    "span": "over 85% of PHI mentions occur in headers",
    "document": "Introduction\n\nAutomatic de-identification of clinical narratives removes protected health information (PHI) to enable secondary use of data. Sequence labeling models have achieved strong performance on well-curated benchmarks but often degrade in the wild due to institution-specific formats and idiosyncratic abbreviations.\n\nRelated Work\n\nTemplate-based systems leverage section headers and note structure to improve recall for frequent identifiers. However, the reliance on structural cues can miss context-bound mentions in free text. Prior observations suggest that over 85% of PHI mentions occur in headers, motivating header-aware models, but leaving substantial room for improvement in body text where context is required.",
    "reason": "Presents a specific statistic without any supporting citation or evidence, violating rule (b).",
    "start": 570,
    "end": 611,
    "label": "Unsupported_claim"
  },
  {
    "span": "Crichton et al. (2017) applied BiLSTM-CRF architectures to biomedical named entity recognition. Lee et al. (2020) introduced BioBERT for domain-specific pretraining. Peng et al. (2019) examined transfer learning across biomedical corpora. Yoon et al. (2020) presented ClinicalBERT for clinical narratives.",
    "document": "Related Work\n\nBiomedical named entity recognition (BioNER) benefits from domain-adapted representations and architectures that capture long-range dependencies and terminology variation. Progress has been driven by specialized pretraining and task-specific sequence labeling models.\n\nCrichton et al. (2017) applied BiLSTM-CRF architectures to biomedical named entity recognition. Lee et al. (2020) introduced BioBERT for domain-specific pretraining. Peng et al. (2019) examined transfer learning across biomedical corpora. Yoon et al. (2020) presented ClinicalBERT for clinical narratives.\n\nOur approach integrates ontology-aware constraints during decoding to improve boundary consistency and handle nested mentions without additional supervision.",
    "reason": "The span enumerates works without transitions or explicit explanation of their relationships, making the connections abrupt and implied rather than stated (issues a and b).",
    "start": 283,
    "end": 588,
    "label": "Coherence"
  },
  {
    "span": "Chen et al. 3",
    "document": "Introduction\n\nLarge-scale multilingual datasets have enabled cross-lingual transfer for many tasks, but label noise and domain mismatch remain challenging. Prior compilations aggregate web and Wikipedia sources with heuristic filtering (Conneau et al., 2020; Lauscher et al., 2020). However, as noted by Chen et al. 3, the dataset exhibits topical skew that disadvantages low-resource languages. Subsequent efforts propose active curation pipelines and proportional sampling to mitigate these biases (Ruder et al., 2021; Pfeiffer et al., 2022). On the modeling side, adapters and language-specific prompts improve parameter efficiency while retaining competitive performance (Houlsby et al., 2019; Pfeiffer et al., 2020).\n\nWe introduce a balanced collection with stratified sampling across domains and a robust evaluation protocol that emphasizes zero-shot generalization.",
    "reason": "Wrong use of a footnote-like marker without a year. It should either include the publication year (e.g., \"Chen et al. (2020)\") or be formatted as a proper numbered footnote, not \"Chen et al. 3\".",
    "start": 304,
    "end": 317,
    "label": "Format"
  },
  {
    "span": "There are few works addressing multilingual program repair beyond Java.",
    "document": "Related Work\n\nAutomated program repair (APR) methods span generate-and-validate pipelines, heuristic search, and learning-based patch generation (Weimer et al., 2009; Long and Rinard, 2016; Chen et al., 2019). Benchmarks such as Defects4J and ManyBugs provide standardized corpora of real-world bugs that have driven progress in evaluating Java and C repairs (Just et al., 2014; Le Goues et al., 2015). Neural approaches increasingly leverage pre-trained code models to suggest fixes conditioned on context (Feng et al., 2020; Jiang et al., 2021).\n\nThere are few works addressing multilingual program repair beyond Java. This limits our understanding of the generality of APR techniques across language ecosystems with distinct syntax and tooling. We introduce a cross-language benchmark and a language-agnostic repair model based on intermediate representations.",
    "reason": "The sentence claims scarcity of prior work across languages without citing surveys or examples; such a quantitative characterization of the literature needs citations.",
    "start": 549,
    "end": 620,
    "label": "Unsupported_claim"
  },
  {
    "span": "CLIP aligns image and text embeddings via contrastive pretraining (Radford et al., 2021). AudioSet provides large-scale weakly labeled audio clips (Gemmeke et al., 2017). Fusion layers can be inserted at multiple depths to integrate cross-modal information (Tsai et al., 2019).",
    "document": "Related Work\n\nMultimodal Representation Learning\n\nRecent progress in multimodal learning has focused on scalable pretraining, token-level fusion, and robust evaluation. Contrastive objectives on paired data enable alignment across modalities (Radford et al., 2021; Jia et al., 2021), while masked modeling reconstructs missing signals (Baevski et al., 2020; He et al., 2022). Cross-attention and co-attention allow fine-grained integration (Tsai et al., 2019; Nagrani et al., 2021). CLIP aligns image and text embeddings via contrastive pretraining (Radford et al., 2021). AudioSet provides large-scale weakly labeled audio clips (Gemmeke et al., 2017). Fusion layers can be inserted at multiple depths to integrate cross-modal information (Tsai et al., 2019). Unified benchmarks assess zero-shot transfer and robustness to modality dropout (Liu et al., 2022; Shankar et al., 2020).",
    "reason": "The span lists three statements about image-text alignment, an audio dataset, and fusion mechanisms without connecting them, making the relevance between the cited works unclear.",
    "start": 483,
    "end": 760,
    "label": "Coherence"
  },
  {
    "span": "Robotic manipulation has advanced through end-to-end visuomotor policies (Levine et al., 2016; Kalashnikov et al., 2018), modular pipelines that separate perception and control (Pinto and Gupta, 2016; Zeng et al., 2018), and model-based RL with planning over learned dynamics (Nagabandi et al., 2018; Chua et al., 2018). For mobile manipulation, works integrate SLAM with grasping (Mur-Artal et al., 2015; Rosinol et al., 2020), exploit affordances (Do et al., 2018; Dang et al., 2020), and use language-conditioned policies (Shridhar et al., 2020; Ahn et al., 2022).",
    "document": "Introduction\n\nMobile manipulation in unstructured environments requires tight coordination between locomotion, perception, and dexterous control. Balancing sample efficiency with robustness to distribution shift remains challenging.\n\nRobotic manipulation has advanced through end-to-end visuomotor policies (Levine et al., 2016; Kalashnikov et al., 2018), modular pipelines that separate perception and control (Pinto and Gupta, 2016; Zeng et al., 2018), and model-based RL with planning over learned dynamics (Nagabandi et al., 2018; Chua et al., 2018). For mobile manipulation, works integrate SLAM with grasping (Mur-Artal et al., 2015; Rosinol et al., 2020), exploit affordances (Do et al., 2018; Dang et al., 2020), and use language-conditioned policies (Shridhar et al., 2020; Ahn et al., 2022).\n\nWe present a hierarchical architecture that composes closed-loop mobility with contact-aware manipulation and demonstrates strong transfer across homes and warehouses using limited demonstrations.",
    "reason": "The span lists categories and citations without explaining how they bear on the proposed hierarchical approach or what remains unresolved, satisfying (a) and (b).",
    "start": 234,
    "end": 801,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[Garcia et al., 2019)",
    "document": "Related Work\n\nTime-series anomaly detection techniques include statistical modeling, distance-based methods, and deep representation learning (Chandola et al., 2009; Blázquez-García et al., 2021). Reconstruction-based models such as autoencoders capture normal patterns and flag deviations (Zong et al., 2018; Malhotra et al., 2016). Predictive approaches forecast future values to reveal unexpected errors (Schuld et al., 2014; Lim and Zohren, 2021). See [Garcia et al., 2019) for a survey of industrial monitoring applications. Our work focuses on multivariate settings with nonstationarity and missing data (Senin and Malinchik, 2013; Hundman et al., 2018).",
    "reason": "Bracket mismatch: the citation opens with '[' and closes with ')'. It should consistently use one style, e.g., '(Garcia et al., 2019)' or '[Garcia et al., 2019]'.",
    "start": 456,
    "end": 477,
    "label": "Format"
  },
  {
    "span": "(Chen et al., 2018; Park, 2020",
    "document": "Introduction\n\nGraph representation learning has progressed rapidly with the advent of scalable neural architectures (Hamilton et al., 2017; Kipf and Welling, 2017). Temporal extensions model evolving interactions in continuous time and capture fine-grained dependencies in streams. Recent work on dynamic graphs has highlighted temporal motifs (Chen et al., 2018; Park, 2020 across social streams, but challenges remain in handling sparsity and cold-start nodes. In this paper, we unify message passing with temporal point processes to better exploit historical context (Rossi et al., 2020) and propose a training objective that balances local and global consistency.\n",
    "reason": "Missing closing parenthesis in a parenthetical citation.",
    "start": 344,
    "end": 374,
    "label": "Format"
  },
  {
    "span": "Recent work has introduced prompt-tuning techniques for vision-language models",
    "document": "Introduction\n\nVision-language models (VLMs) align visual and textual representations to enable zero-shot recognition, retrieval, and captioning. Adapting these models to novel tasks with minimal data remains an active research area.\n\nRecent work has introduced prompt-tuning techniques for vision-language models, reducing the number of trainable parameters while retaining zero-shot capabilities. However, prompt selection remains brittle and sensitive to prompt initialization.\n\nWe propose a meta-learned prompt initializer that rapidly adapts to new classes and tasks with a small support set, improving stability and performance.",
    "reason": "Mentions 'recent work' at first mention without any citations to the specific techniques or papers (rule d).",
    "start": 234,
    "end": 312,
    "label": "Unsupported_claim"
  },
  {
    "span": "Singh and Joachims (2018) introduced exposure-based fairness constraints for rankings. Biega et al. (2018) proposed amortized fairness over user attention. Kallus and Zhou (2019) defined fairness through awareness in rankings. Wang et al. (2021) studied counterfactual fairness for recommender systems.",
    "document": "Related Work\n\nFairness in Ranking and Recommendation\n\nAlgorithmic fairness in ranking considers how exposure and relevance interact under position bias and user behavior. Mitigation strategies span regularization, post-processing, and counterfactual evaluation. However, inconsistent assumptions about utility and exposure budgets hinder comparability of results.\n\nSingh and Joachims (2018) introduced exposure-based fairness constraints for rankings. Biega et al. (2018) proposed amortized fairness over user attention. Kallus and Zhou (2019) defined fairness through awareness in rankings. Wang et al. (2021) studied counterfactual fairness for recommender systems.\n\nWe present a unified objective that calibrates exposure to estimated utility under uncertainty, and propose an evaluation protocol that disentangles ranking utility from demographic parity.",
    "reason": "The span enumerates several fairness notions without transitions or explanation of their relationships, leaving the connections between exposure, amortized fairness, awareness, and counterfactual fairness implicit.",
    "start": 365,
    "end": 667,
    "label": "Coherence"
  },
  {
    "span": "Knowledge distillation has been applied to compress acoustic models by transferring soft targets from large teachers to compact students (Hinton et al., 2015; Watanabe et al., 2018). Sequence-level distillation and pseudo-labeling extend this idea to end-to-end ASR, often improving word error rates (Kim and Rush, 2016; Kahn et al., 2020).",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) has advanced with attention-based and transducer architectures, yet practical deployment often requires compact models that maintain accuracy under resource constraints. Distillation and data curation techniques are commonly used to bridge this gap.\n\nKnowledge distillation has been applied to compress acoustic models by transferring soft targets from large teachers to compact students (Hinton et al., 2015; Watanabe et al., 2018). Sequence-level distillation and pseudo-labeling extend this idea to end-to-end ASR, often improving word error rates (Kim and Rush, 2016; Kahn et al., 2020).\n\nWe propose Consistent SpecAugment Distillation (CSADist), which aligns teacher-student invariances under aggressive time-frequency masking and enforces agreement across multiple perturbations. The student leverages multi-view consistency losses to better generalize to noisy conditions.\n\nOn LibriSpeech and TED-LIUM, CSADist reduces WER relative to strong distilled baselines, particularly in low-parameter regimes. We analyze robustness to domain noise and ablate augmentation schedules.",
    "reason": "The span summarizes prior distillation techniques without indicating how they inform the proposed method or what specific shortcoming remains, lacking synthesis (criteria a and c).",
    "start": 311,
    "end": 651,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Most recent works adopt graph-based decoders for AMR-to-text generation.",
    "document": "Related Work\n\nAMR-to-text generation has evolved from early sequence-to-sequence baselines that linearize AMR graphs (Konstas et al., 2017) to models that more explicitly leverage graph structure (Zhang et al., 2018; Ribeiro et al., 2020). Graph neural networks and structured attention have been used to encode nodes and edges, preserving reentrancies and longer-range dependencies (Song et al., 2018; Marcheggiani and Titov, 2020). Pretrained language models have further improved fluency and factuality when conditioned on graph encodings (Ribeiro et al., 2021; Bevilacqua et al., 2021).\n\nMost recent works adopt graph-based decoders for AMR-to-text generation. In contrast, our approach keeps a purely sequence-based decoder while injecting structural biases at the encoder, simplifying decoding and improving inference speed. We also study data augmentation via controlled perturbations to improve robustness to graph noise.",
    "reason": "The sentence claims a trend in recent literature without citing any specific works, violating the requirement to support mentions of 'recent works' with citations.",
    "start": 592,
    "end": 664,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works demonstrate that adversarial training substantially improves robustness on text classification and NLI.",
    "document": "Introduction\n\nRobustness to adversarial perturbations remains a central challenge for modern NLP systems. With the rise of large transformer-based encoders and decoders, small input manipulations can still induce significant changes in model predictions. Recent works demonstrate that adversarial training substantially improves robustness on text classification and NLI. However, the computational cost of generating adversarial examples at scale has limited the adoption of these methods in production settings.\n\nIn this paper, we present a lightweight adversarial data augmentation strategy that amortizes perturbation generation across batches. We evaluate our approach on sentiment analysis and natural language inference benchmarks under both white-box and black-box attacks. We further analyze trade-offs between robustness and accuracy by varying perturbation budgets and training schedules.\n\nOur contributions are threefold: we propose a batch-sharing procedure for adversarial augmentation, we provide a thorough evaluation under standardized attack suites, and we release code and scripts to facilitate reproducibility.",
    "reason": "Mentions 'recent works' and claims specific improvements without providing any citations.",
    "start": 255,
    "end": 371,
    "label": "Unsupported_claim"
  },
  {
    "span": "[Kim et al., 2016]",
    "document": "Introduction\n\nTask-oriented dialogue systems have transitioned from pipeline architectures to end-to-end neural models that jointly learn understanding, state tracking, and response generation. Early seq2seq approaches [Kim et al., 2016] were soon augmented with copy and pointer mechanisms for slot filling (See et al., 2017; Eric and Manning, 2017). More recent work leverages pre-trained language models for few-shot adaptation (Peng et al., 2021) and schema-guided dialogue (Rastogi et al., 2020).\n\nDespite these advances, robustness to out-of-domain queries remains limited. We address this by introducing a contrastive calibration objective that regularizes responses under semantic perturbations.",
    "reason": "Square-bracketed author–year citation mixes styles; in APA-like contexts it should be (Kim et al., 2016).",
    "start": 219,
    "end": 237,
    "label": "Format"
  },
  {
    "span": "Dexterous grasping has been studied with analytic grasp metrics, learned grasp synthesis from demonstrations, and model-based planners using contact dynamics (Ferrari and Canny, 1992; Kappler et al., 2015; Pinto and Gupta, 2016; Handa et al., 2020).",
    "document": "Related Work\n\nAchieving robust dexterous manipulation requires precise contact reasoning and generalization to novel objects. Prior research spans analytical models and data-driven techniques.\n\nDexterous grasping has been studied with analytic grasp metrics, learned grasp synthesis from demonstrations, and model-based planners using contact dynamics (Ferrari and Canny, 1992; Kappler et al., 2015; Pinto and Gupta, 2016; Handa et al., 2020).\n\nReal-world execution introduces frictional uncertainties and sensing noise. We address these issues via sim-to-real adaptive control, but the above paragraph does not explain how prior work falls short.",
    "reason": "The cited sentence lists approaches without stating their limitations or how the present work differs, thus lacking synthesis (criteria a and b).",
    "start": 194,
    "end": 443,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There are many recent works exploring contrastive learning for sequential recommendation.",
    "document": "Related Work\n\nSequential recommendation models user behavior as a temporal sequence of interactions [1]. Self-attentive architectures like SASRec capture long-range dependencies and achieve strong baselines [2]. There are many recent works exploring contrastive learning for sequential recommendation. Methods differ in how they construct positive and negative pairs, sample augmentations, and enforce invariances.",
    "reason": "Vague reference to 'many recent works' without listing any citations.",
    "start": 212,
    "end": 301,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior studies have shown that annotators systematically bias toxicity labels against African American English.",
    "document": "Related Work\n\nAutomated toxicity detection is typically framed as text classification, with datasets sourced from online platforms and annotated for offensive or abusive content (Wulczyn et al., 2017; Fortuna and Nunes, 2018). Dataset construction and annotation practices have profound effects on model fairness and robustness, with known issues around imbalance and spurious lexical cues (Borkan et al., 2019). Prior studies have shown that annotators systematically bias toxicity labels against African American English. Mitigation efforts include debiasing lexica, counterfactual data augmentation, and domain-adaptive pretraining to reduce reliance on dialect markers (Zhao et al., 2018; Garg et al., 2019). Our work contributes a dialect-aware reweighting scheme and an evaluation set that isolates dialectal from semantic toxicity signals.",
    "reason": "Makes a claim about findings from prior studies on annotator bias without providing any supporting citations at the point of first mention.",
    "start": 413,
    "end": 523,
    "label": "Unsupported_claim"
  },
  {
    "span": "[Smith et al., 2015)",
    "document": "Introduction\n\nAccurate event extraction from news is crucial for situational awareness (Ahn, 2006; Nguyen and Grishman, 2015). Recent neural approaches integrate syntax and semantics for better triggers and arguments (Wadden et al., 2019; Lu et al., 2021).\n\nCross-document reasoning improves coverage but increases noise (Luan et al., 2018; Du and Cardie, 2020). Prior distant supervision settings [Smith et al., 2015) assume high-precision seed patterns, which rarely hold in multilingual streams.\n\nWe propose a calibration mechanism that downweights noisy alignments while preserving recall on emergent events.",
    "reason": "Mismatched brackets in the citation; should consistently use parentheses “(Smith et al., 2015)” or square brackets, not a mix.",
    "start": 398,
    "end": 418,
    "label": "Format"
  },
  {
    "span": "Count-based bonuses encourage visiting novel states (Bellemare et al., 2016). PPO stabilizes policy optimization with clipping (Schulman et al., 2017). Intrinsic motivation via curiosity predicts next-state features (Pathak et al., 2017).",
    "document": "Related Work\n\nExploration in Reinforcement Learning. Sparse-reward tasks require effective exploration strategies to discover informative trajectories, with approaches ranging from optimism and posterior sampling to intrinsic motivation (Auer et al., 2002; Osband et al., 2016).\n\nIntrinsic Rewards and Stability. Bonuses based on prediction error or state novelty aim to guide agents toward informative experiences (Bellemare et al., 2016; Pathak et al., 2017). Policy optimization techniques address stability and sample efficiency in high-dimensional control (Schulman et al., 2017; Haarnoja et al., 2018). Count-based bonuses encourage visiting novel states (Bellemare et al., 2016). PPO stabilizes policy optimization with clipping (Schulman et al., 2017). Intrinsic motivation via curiosity predicts next-state features (Pathak et al., 2017). We consider long-horizon exploration under function approximation.\n\nModel-Based Signals. World models provide uncertainty estimates that can be translated into exploration bonuses while improving planning (Chua et al., 2018; Sekar et al., 2020). Our approach leverages latent disagreement with return-aware normalization.",
    "reason": "The span enumerates count-based exploration, PPO, and curiosity without indicating how PPO (an optimization method) relates to intrinsic exploration methods; the lack of transitions makes the connections unclear.",
    "start": 609,
    "end": 847,
    "label": "Coherence"
  },
  {
    "span": " OpenAI GPT-3",
    "document": "Introduction\n\nA common research avenue pursued these days is to train monolithic language models with billions of parameters to solve every language understanding and reasoning challenge. In contrast, humans often tackle complex tasks by breaking them down into simpler sub-tasks, and solving these by interacting with other people or automated agents whose skillsets we are familiar with. This approach allows us to learn to solve new complex tasks quickly and effectively, by building upon what's already known. Can AI systems learn to do the same?\n\nTo facilitate research in this direction, we propose a new reasoning challenge and a benchmark called COMMAQA where, in addition to the usual end-task supervision, one has access to a set of predefined AI agents with examples of their natural language inputs. Importantly, the target end-task is designed to be too difficult for current models to learn based only on end-task supervision. The goal is instead to build models that learn to solve the target task by decomposing it into sub-tasks solvable by these agents, and interacting with these agents in natural language to do so.\n\nAs a motivating example, consider the interaction depicted in Figure 1 where a system is asked to buy a book series with a certain property. The system breaks this goal down, using agent-1 (here Google Assistant) to identify the referenced book series as well as the list of books in that series, and then using agent-2 (here Amazon Alexa) to make the purchase. While both of these agents interact with the system in natural language, they have notably different skill sets, rely on privately held knowledge sources, and have been built at an enormous cost. At the same time, neither agent by itself can accomplish the original goal.\n\nAn alternative to building such a system that interacts with existing agents is to teach all requisite sub-tasks and skills to a large black-box system, say via multi-task learning Gupta et al., 2021). This, however, is not only wasteful in terms of time and resources, but often also infeasible. For example, agents such as Google Assistant and OpenAI GPT-3 use private knowledge resources and are computationally expensive to train even once. It would thus be nearly impossible to build a single system with the capabilities of both of these agents.\n\nWe note that agents need not be sophisticated AI assistants. An agent may simply be a previously developed question-answering (QA) model, a math module, a function of textual input, an image captioning system-anything the community already knows how to build. The goal is to learn to leverage existing agents for more complex tasks.\n\nTo enable the development of general systems for this task, we identify the minimal inputs that must be assumed for the task to be learnable-training data for the complex task, existing agents that together can solve the complex task, and examples of valid questions that can be asked of these agents (capturing the agents' capabilities). We build a new synthetic benchmark dataset called COMMAQA (Communicating with agents for QA), containing three complex multihop QA tasks (involving Explicit, Implicit, and Numeric reasoning) and four input QA agents that can solve these tasks.\n\nWe demonstrate that black-box models struggle on COMMAQA even when provided with auxiliary data, such as domain-relevant agent knowledge. On the other hand, a model that leverages the agents (Khot et al., 2021) can achieve very high accuracy but relies on auxiliary supervision (decomposition annotations). While it is possible to identify valid decompositions using just the endtask labels, the search space is extremely large and naïve approaches, as we show, help only with one of the datasets. COMMAQA thus serves as a new challenge for the NLP community.\n\nContributions: We (1) propose a new challenge of learning to solve complex tasks by communicating with agents; (2) develop a synthetic multi-hop QA dataset COMMAQA with three reasoning types;\n\n(3) provide auxiliary training data and a compositional generalization test set; (4) demonstrate the challenging nature of COMMAQA for black-box models; and (5) show the promise of compositional models that learn to communicate with agents.\n\n ",
    "start": 2117,
    "end": 2130,
    "label": "Unsupported_claim"
  },
  {
    "span": "In previous work, BERT-large was fine-tuned for abstractive keyphrase generation from scientific abstracts.",
    "document": "Related Work\n\nKeyphrase extraction and generation facilitate indexing, recommendation, and search in scientific digital libraries. Early extractive methods relied on statistical features such as TF-IDF and position heuristics. Neural approaches broadened the scope to include sequence-to-sequence models capable of generating absent keyphrases not found verbatim in the document.\n\nIn previous work, BERT-large was fine-tuned for abstractive keyphrase generation from scientific abstracts. Other efforts explored hybrid extract-then-generate pipelines, leveraging graph structures to capture discourse salience. Despite these advances, domain shift across disciplines (e.g., biomedicine vs. computer science) remains a core challenge for generalization.\n\nOur approach unifies domain adaptation with contrastive document-keyphrase alignment, reducing dependence on domain-specific heuristics and improving recall of absent phrases.\n",
    "reason": "Claims a specific prior setup (BERT-large fine-tuned for a task) without citing the corresponding prior study.",
    "start": 381,
    "end": 488,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Rossi, 2019 Li, 2020)",
    "document": "Related Work\n\nFew-shot parsing leverages meta-learning and data augmentation to reach high accuracy with limited supervision (Peters and Huang, 2020; Alvarez et al., 2021). Prompt-based methods adapt pretrained encoders to structure prediction (Kim and Duarte, 2022).\n\nSeveral studies evaluate cross-lingual generalization in low-resource settings (Rossi, 2019 Li, 2020) and explore episodic training regimes (Mora and Jensen, 2021). We propose a compositional prompt template and report transfer to unseen formalisms.",
    "reason": "Missing separator between multiple citations inside the same parentheses. Different sources should be separated by a semicolon: '(Rossi, 2019; Li, 2020)'.",
    "start": 348,
    "end": 370,
    "label": "Format"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nVision-language pretraining aligns image regions with textual spans using contrastive and matching objectives (Radford et al., 2021; Li et al., 2021). Despite strong zero-shot transfer, downstream fine-tuning can suffer from catastrophic forgetting (Huang et al., 2022). Prior work proposes adapter layers and prompt tuning to preserve generality while specializing to tasks (Zhang and Wu, 2023). As shown by [12], naive fine-tuning degrades compositional reasoning on CLEVR-like diagnostics. We investigate regularizers that preserve cross-modal correspondences during specialization.",
    "reason": "Numeric bracketed citation used in an author–year context; should be converted to an author–year citation (e.g., \"as shown by Patel and Singh (2020)\") or the paper should consistently use numeric style.",
    "start": 423,
    "end": 427,
    "label": "Format"
  },
  {
    "span": "a previous study reports a 20% reduction in labeling cost",
    "document": "Introduction\n\nNamed entity recognition (NER) in specialized domains is expensive due to expert annotation requirements (Settles, 2012; Culotta and McCallum, 2005). Active learning aims to reduce labeling effort by querying the most informative instances under uncertainty or diversity criteria (Lewis and Gale, 1994; Hsu and Lin, 2015). In practical deployments, budget constraints necessitate strategies that balance informativeness with annotation throughput. For example, a previous study reports a 20% reduction in labeling cost using uncertainty sampling combined with batch-mode selection. However, little is known about how these strategies fare under distribution shift, which is common in clinical and legal corpora. We introduce a cross-domain evaluation harness to assess robustness of active learning policies for NER.",
    "reason": "The mention of a specific prior study and quantitative result lacks a citation, which is required for verifiability.",
    "start": 475,
    "end": 532,
    "label": "Unsupported_claim"
  },
  {
    "span": "Zhou et al. 2",
    "document": "Related Work\n\nGraph-based semi-supervised learning. Label propagation and graph regularization leverage manifold structure to improve generalization with limited labels (Zhu et al., 2003; Belkin et al., 2006). Neural graph models extend these ideas with message passing and attention (Kipf and Welling, 2017; Veličković et al., 2018). Zhou et al. 2 propose a transductive method that reweights edges via confidence estimates, showing improvements on citation networks and product graphs. Follow-up studies integrate uncertainty calibration and consistency training to stabilize learning under label noise (Xie et al., 2020; Wang et al., 2021).\n\nActive learning on graphs. Query strategies that exploit graph structure can reduce labeling cost by targeting high-influence nodes (Bilgic et al., 2010; Chen et al., 2017). Recent approaches co-design acquisition and propagation to avoid redundancy (Zhang et al., 2022; Citovsky et al., 2021).",
    "reason": "Improper footnote-like marker appended to an author name without year; should include a year or be formatted as a proper footnote (e.g., 'Zhou et al. (2020)' or 'Zhou et al.^2').",
    "start": 335,
    "end": 348,
    "label": "Format"
  },
  {
    "span": "(Baker et al. 2015)",
    "document": "Introduction\n\nProgram synthesis from natural language specifications has progressed via neuro-symbolic models that constrain decoding with grammar priors (Yin and Neubig, 2017; Chen et al., 2018). While execution-guided decoding improves semantic accuracy, it remains brittle to lexical variation (Wang et al., 2018). Prior work (Baker et al. 2015) introduced schema linking for compositional generalization, but its reliance on handcrafted features limits scalability.\n\nWe propose a retrieval-augmented parser that learns to ground mentions in large codebases, reducing reliance on manual features.",
    "reason": "Missing comma between author and year inside the parenthetical citation; APA-style requires '(Baker et al., 2015)'.",
    "start": 329,
    "end": 348,
    "label": "Format"
  },
  {
    "span": "In a previous study, the authors showed that depthwise separable convolutions reduce parameter count by nearly an order of magnitude without sacrificing accuracy.",
    "document": "Related Work\n\nEfficient convolutional networks aim to reduce compute and memory without degrading task performance. A prominent line of research explores architectural factorization, channel shuffling, and group convolutions to increase throughput on mobile devices. Others have examined neural architecture search under latency constraints and mixed-precision inference for further gains. In a previous study, the authors showed that depthwise separable convolutions reduce parameter count by nearly an order of magnitude without sacrificing accuracy. Beyond architectural changes, pruning and quantization techniques can compress models post hoc, though they often require careful calibration and fine-tuning. Our approach is orthogonal: we propose a training-time regularizer that encourages channel sparsity and spatial locality, enabling drop-in acceleration on commodity hardware.",
    "reason": "Mentions a prior study and a quantitative claim about its findings but provides no citation.",
    "start": 390,
    "end": 552,
    "label": "Unsupported_claim"
  },
  {
    "span": "We evaluate on a commonly used benchmark of 12 datasets introduced in the GLUE++ suite.",
    "document": "Introduction\n\nGeneral-purpose language understanding models are commonly assessed on multi-task benchmarks that aggregate diverse phenomena, from sentence similarity and entailment to paraphrase identification and commonsense reasoning. However, variability in preprocessing and training regimes complicates cross-paper comparison.\n\nWe evaluate on a commonly used benchmark of 12 datasets introduced in the GLUE++ suite. Our analysis focuses on the effect of instruction tuning and mixture composition, disentangling improvements due to better optimization from those due to cross-task transfer.\n\nTo facilitate reproducibility, we release scripts for dataset acquisition, canonical splits, and standardized metrics, along with model checkpoints for downstream fine-tuning.",
    "reason": "The sentence mentions a specific benchmark suite as prior work without providing a citation to the benchmark's introduction.",
    "start": 333,
    "end": 420,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prompting has emerged as a flexible way to steer code generation (Brown et al., 2020; Austin et al., 2021; Chen et al., 2021; Li et al., 2022).",
    "document": "Introduction\nProgram synthesis with large language models (LLMs) has shown rapid progress, enabling zero-shot and few-shot code generation across multiple languages. However, reliability and controllability remain major concerns for practical use.\n\nRelated Work\nPrompting has emerged as a flexible way to steer code generation (Brown et al., 2020; Austin et al., 2021; Chen et al., 2021; Li et al., 2022). Additional strategies include constrained decoding (Poesia et al., 2022), retrieval augmentation (Parikh et al., 2022), and execution-based reranking (Le et al., 2022). Some works study test-driven selection (Austin et al., 2021b) and self-consistency (Wang et al., 2022) to improve pass@k. We evaluate methods on benchmarks with hidden tests.",
    "reason": "The span notes a trend and cites prior work but does not connect these prompting approaches to the authors' objectives, shortcomings in prior methods, or the paper's contribution.",
    "start": 262,
    "end": 405,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT was used in an AES task trained on essays with holistic scores.",
    "document": "Introduction\n\nAutomated essay scoring (AES) seeks to predict human-assigned scores from student writing to support formative assessment. Early systems relied on surface and discourse features with linear models (Attali and Burstein, 2006), while recent neural approaches learn contextual representations and coherence signals. BERT was used in an AES task trained on essays with holistic scores. Other lines of work emphasize prompt-specific calibration and domain adaptation across prompts and grade levels (Taghipour and Ng, 2016; Dong and Zhang, 2016). We propose a multi-task framework that jointly models trait scores and holistic scores for improved generalization.",
    "reason": "Mentions a specific prior setup and model usage without providing a citation at first mention.",
    "start": 327,
    "end": 395,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Kim et al., 2020;",
    "document": "Introduction\n\nSelf-supervised pretraining for speech has achieved strong results by learning contextualized acoustic representations that transfer to downstream ASR and keyword spotting (Schneider et al., 2019; Baevski et al., 2020). Contrastive and masked prediction objectives dominate the literature, differing primarily in how they define positive samples and corruption schemes (Oord et al., 2018; Rivière et al., 2020).\n\nFor a broad overview of self-supervised acoustic models, see (Kim et al., 2020; which contrasts contrastive predictive coding with reconstruction-based methods and highlights the role of quantization. Building on these insights, we introduce a multi-granular target design that unifies frame- and segment-level prediction within a single encoder (Hsu et al., 2021; Chung et al., 2021).",
    "reason": "Missing closing parenthesis in parenthetical citation; the citation ends with a semicolon and no closing ')'.",
    "start": 488,
    "end": 506,
    "label": "Format"
  },
  {
    "span": "Chen et. al. (2022)",
    "document": "Introduction\n\nDomain adaptation aligns feature distributions between source and target corpora (Miller and Shah, 2018; Duarte et al., 2020). As shown by Chen et. al. (2022), adversarial objectives can be combined with self-training to improve robustness under label shift.\n\nRelated Work\n\nSubsequent work studies class-conditional alignment (Patel and Lin, 2021) and discrepancy minimization via optimal transport (Koenig et al., 2020). We contribute a stability analysis under curriculum schedules.",
    "reason": "Incorrect 'et. al.' punctuation. It should be 'et al.' without a period after 'et'.",
    "start": 153,
    "end": 172,
    "label": "Format"
  },
  {
    "span": "Sennrich et al. (2016) used back-translation to leverage monolingual data. Papineni et al. (2002) introduced BLEU for machine translation evaluation. van der Wees et al. (2017) proposed curriculum learning over data quality.",
    "document": "Related Work\n\nDomain adaptation for neural machine translation (NMT) has been explored through data generation, selection, and parameter-efficient tuning. Core techniques exploit monolingual data, filter noisy corpora, and schedule training.\n\nSennrich et al. (2016) used back-translation to leverage monolingual data. Papineni et al. (2002) introduced BLEU for machine translation evaluation. van der Wees et al. (2017) proposed curriculum learning over data quality. Data filtering and weighting methods aim to emphasize in-domain examples (Junczys-Dowmunt, 2018; Wang et al., 2017).\n\nRecent work studies adapters and prompt-based tuning for parameter-efficient adaptation (Bapna and Firat, 2019; Lester et al., 2021).",
    "reason": "An evaluation metric (BLEU) is inserted between two adaptation techniques, with no explanation of its relevance to domain adaptation strategies, resulting in an abrupt and incoherent sequence.",
    "start": 243,
    "end": 467,
    "label": "Coherence"
  },
  {
    "span": "The DSTC7 challenge standardized response selection evaluation for the first time.",
    "document": "Related Work on Dialogue Evaluation\n\nOpen-domain conversational agents are typically assessed with a combination of automatic metrics and human judgments. Retrieval-based formulations have popularized response selection tasks that decouple relevance from generation quality. The DSTC7 challenge standardized response selection evaluation for the first time. Subsequent shared tasks introduced adversarial negatives and multi-turn reasoning to better reflect conversational coherence.\n\nDespite these advances, correlations between automatic metrics and human ratings remain inconsistent, particularly for open-ended generation. Recent work has called for richer interaction-level protocols and longitudinal user studies. Our framework contributes a mixed-initiative evaluation that integrates turn-level utility, safety checks, and session satisfaction, aiming to reduce selection bias in small-scale human evaluations.",
    "reason": "Claims a specific historical first about a named shared task without providing a citation to the DSTC7 overview or proceedings.",
    "start": 275,
    "end": 357,
    "label": "Unsupported_claim"
  },
  {
    "span": "Knowledge-augmented QA integrates structured triples or subgraphs into neural readers using retrieval, graph attention, or path reasoning (Yang and Mitchell, 2017; Sun et al., 2019; De Cao et al., 2021).",
    "document": "Related Work\n\nKnowledge-Augmented Question Answering. External knowledge can resolve ambiguities and support compositional reasoning beyond surface text. Knowledge-augmented QA integrates structured triples or subgraphs into neural readers using retrieval, graph attention, or path reasoning (Yang and Mitchell, 2017; Sun et al., 2019; De Cao et al., 2021). Other lines of work focus on differentiable reasoning, multi-hop inference, and hybrid retrieval-generation pipelines (Dhingra et al., 2020; Xiong et al., 2021; Lewis et al., 2020).\n\nEvaluation Challenges. Calibration, faithfulness, and evidence attribution remain central evaluation concerns for knowledge-grounded systems (Jacovi and Goldberg, 2020; Kamoi et al., 2022).",
    "reason": "The span lists prior methods and citations without connecting them to the paper's objectives or clarifying what remains unresolved.",
    "start": 154,
    "end": 357,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent works have demonstrated that hybrid bandit-neural approaches consistently outperform pure neural baselines on long-horizon recommendation dialogues.",
    "document": "Introduction\n\nConversational recommender systems aim to elicit user preferences through multi-turn interactions while selectively presenting items that maximize long-term satisfaction. Unlike static recommenders, dialog-based agents must balance exploration and exploitation under sparse and delayed feedback. A central challenge is learning to ask informative questions and to adaptively refine user models across sessions.\n\nA number of strategies have been proposed, from belief-tracking methods to reinforcement learning over dialog policies. Recent works have demonstrated that hybrid bandit-neural approaches consistently outperform pure neural baselines on long-horizon recommendation dialogues. However, most reported gains come from simulated users built on simplified click models, raising concerns about robustness under realistic noise.\n\nThis paper investigates the sample efficiency and stability of offline-to-online training for conversational recommenders. We analyze how policy pretraining on logged dialogs interacts with uncertainty-aware exploration at deployment. We also introduce a modular evaluation protocol that separates candidate generation from conversation control to isolate error sources.\n\nOur contributions are threefold: (1) a unified training recipe for hybrid conversational recommenders, (2) a set of stress tests for long-horizon dialog robustness, and (3) empirical evidence on the trade-offs between exploration, user burden, and recommendation quality.",
    "reason": "Mentions unspecified 'recent works' and their findings without providing any citations (violates rule d; claim about prior work needs references).",
    "start": 546,
    "end": 701,
    "label": "Unsupported_claim"
  },
  {
    "span": "Causal representation learning has investigated independent component analysis and non-Gaussianity (Hyvarinen and Oja, 2000; Hyvarinen et al., 2019), identifiability via auxiliary variables and interventions (Peters et al., 2016; Kocaoglu et al., 2017), invariance-based objectives across environments (Schölkopf et al., 2012; Arjovsky et al., 2019), contrastive and self-supervised signals (Locatello et al., 2020; Von Kügelgen et al., 2021), and causal discovery with score-based or constraint-based methods (Chickering, 2002; Spirtes et al., 2000). Benchmarks for disentanglement and causal discovery are widely used (Eastwood and Williams, 2018; Low et al., 2022).",
    "document": "Related Work\n\nLearning causally meaningful latent factors can improve generalization and robustness to distribution shift. However, identifiability often requires strong assumptions or side information.\n\nCausal representation learning has investigated independent component analysis and non-Gaussianity (Hyvarinen and Oja, 2000; Hyvarinen et al., 2019), identifiability via auxiliary variables and interventions (Peters et al., 2016; Kocaoglu et al., 2017), invariance-based objectives across environments (Schölkopf et al., 2012; Arjovsky et al., 2019), contrastive and self-supervised signals (Locatello et al., 2020; Von Kügelgen et al., 2021), and causal discovery with score-based or constraint-based methods (Chickering, 2002; Spirtes et al., 2000). Benchmarks for disentanglement and causal discovery are widely used (Eastwood and Williams, 2018; Low et al., 2022).\n\nWe introduce interventional data augmentation via learned editors that simulate plausible counterfactuals, enabling identifiability under weaker assumptions than prior work.",
    "reason": "The span merely catalogs areas and benchmarks without relating them to the authors’ approach or specifying the precise gap, hence lacking synthesis per (a) and (c).",
    "start": 204,
    "end": 872,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT was used in an AES task trained on essays with holistic scores to provide contextual representations.",
    "document": "Related Work\n\nAutomated essay scoring (AES) has progressed from handcrafted linguistic features toward neural approaches that leverage distributed text representations. Prior neural AES models include LSTM-based regressors and convolutional architectures that predict holistic scores from essay text. BERT was used in an AES task trained on essays with holistic scores to provide contextual representations. Subsequent work explored multitask setups, combining trait-level and overall scores, and domain adaptation to new prompts. Despite these advances, explainability and robustness remain concerns, particularly when scoring essays from previously unseen prompts or demographic groups.",
    "reason": "This is a concrete claim about a previous setup and use of BERT in AES without citing the specific study, dataset, or implementation.",
    "start": 301,
    "end": 407,
    "label": "Unsupported_claim"
  },
  {
    "span": "MS-COCO has been the de facto standard for cross-modal retrieval since 2018.",
    "document": "Introduction\n\nCross-modal retrieval systems learn joint embeddings for images and text to enable bidirectional search, captioning, and content understanding (Frome et al., 2013; Faghri et al., 2018). Contrastive learning with large-scale web data has substantially improved zero-shot transfer to downstream tasks (Radford et al., 2021; Jia et al., 2021). Robustness remains challenging under compositional queries, attribute binding, and long-tail concepts (Yuksekgonul et al., 2022).\n\nBenchmark choice strongly influences perceived progress due to differences in captions, object density, and evaluation metrics. MS-COCO has been the de facto standard for cross-modal retrieval since 2018. Yet, reliance on single-domain datasets risks overfitting to stylistic biases and co-occurrence patterns.\n\nWe curate a multi-domain suite spanning product, scientific, and news imagery, and introduce attribute-centered probes that disentangle retrieval performance on compositional attributes versus category cues.",
    "reason": "This is a claim about community practice and dataset dominance without any supporting citations or survey evidence.",
    "start": 614,
    "end": 690,
    "label": "Unsupported_claim"
  },
  {
    "span": "Matrix factorization approaches remain a strong baseline for implicit feedback recommendation (Hu et al., 2008; Koren, 2009; Rendle et al., 2009).",
    "document": "Related Work\n\nCollaborative Filtering. Recommender systems frequently rely on collaborative filtering signals derived from explicit ratings or implicit interactions. Matrix factorization approaches remain a strong baseline for implicit feedback recommendation (Hu et al., 2008; Koren, 2009; Rendle et al., 2009). More recently, neural architectures model nonlinear user–item relationships and temporal dynamics (He et al., 2017; Hidasi et al., 2016). Side information, such as content and knowledge graphs, is often integrated to mitigate cold-start issues (Wang et al., 2019; Zhang et al., 2016).\n\nSelf-Supervision in Recommendation. Contrastive learning and data augmentation for user/item representations have shown promise by reducing reliance on labels and improving robustness (Wu et al., 2021; Zhou et al., 2020). Negative sampling strategies and debiasing techniques further refine representation learning (Chen et al., 2017; Wang et al., 2021).\n\nThis work explores improved user modeling with sequence-aware representations under missing-not-at-random feedback.",
    "reason": "The span cites baseline methods without clarifying how they relate to the sequence-aware approach described later or what gap persists that motivates the new method (definition a and b).",
    "start": 166,
    "end": 312,
    "label": "Lacks_synthesis"
  },
  {
    "span": "many recent works have explored cross-lingual dense retrieval using multilingual encoders",
    "document": "Introduction\n\nCross-lingual dense retrieval aims to match queries and documents written in different languages by projecting them into a shared semantic space. Despite rapid progress in multilingual representation learning, retrieval performance still lags behind monolingual systems due to vocabulary mismatch, cultural nuances, and annotation scarcity.\n\nMotivated by these challenges, we investigate negative sampling strategies that better align semantically related documents across languages. In particular, we focus on the role of hard negatives constructed through translation pairs and pivot-language mining.\n\nWhile many recent works have explored cross-lingual dense retrieval using multilingual encoders, there is limited discussion on how negative sampling choices interact with bitext quality and domain shift. We address this gap by proposing a curriculum over negatives that gradually increases difficulty during training.\n\nOur contributions are threefold: (1) a principled curriculum for multilingual negatives, (2) an analysis of noise robustness with respect to translation errors, and (3) a set of strong baselines for low-resource language pairs. We evaluate on multiple public benchmarks and show consistent improvements over contrastive learning with random negatives.\n",
    "reason": "Mentions 'many recent works' without citing any papers; per rule (d), references to recent works must be supported by citations.",
    "start": 624,
    "end": 713,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent work demonstrates that graph-based recommenders consistently outperform matrix factorization in cold-start scenarios.",
    "document": "Related Work\n\nRecommender systems have evolved from neighborhood-based methods to latent factorization and, more recently, to deep and graph-based architectures. Side information and user-item interaction graphs are increasingly exploited to improve generalization. Recent work demonstrates that graph-based recommenders consistently outperform matrix factorization in cold-start scenarios. However, the extent of this advantage depends on the quality and coverage of side signals, the sparsity pattern, and the inductive bias of the chosen aggregator. Complementary strands explore counterfactual inference for bias reduction and sequential models for intent dynamics. Our approach integrates inductive graph encoders with a generative prior to improve cold-start performance without dense side metadata.",
    "reason": "Mentions unspecified 'recent work' and a comparative claim without providing citations to those works.",
    "start": 266,
    "end": 390,
    "label": "Unsupported_claim"
  },
  {
    "span": "Alvarez et al.",
    "document": "Introduction\n\nSemi-supervised text classification has benefited from consistency training and pseudo-labeling, yet challenges remain in low-resource domains (Lopez and Choi, 2019; Patel et al., 2021). Early work on entropy minimization emphasized the role of confident predictions under unlabeled distributions (Grand and Ellis, 2018), while more recent methods integrate augmentations tailored to linguistic structure (Miller et al., 2020; Zhang and Rao, 2022). Following Alvarez et al., we treat unlabeled instances as anchors that encourage cluster-friendly decision boundaries, but we also account for domain shift via adaptive calibration.\n\nOur approach differs from prior curriculum strategies that gradually expand the trusted set (Davis et al., 2021) by jointly optimizing calibration and representation smoothness. We adopt robustness diagnostics common in domain adaptation (Kim and Huang, 2020) and evaluate on specialized datasets in clinical and legal text where annotation costs are high (Ng and Sorensen, 2021).",
    "reason": "Narrative citation missing year; should be formatted as a narrative citation with year, e.g., \"Alvarez et al. (2019)\" or converted to a parenthetical citation.",
    "start": 473,
    "end": 487,
    "label": "Format"
  },
  {
    "span": "A previous study showed that time-domain models outperform frequency-domain approaches for reverberant mixtures.",
    "document": "Related Work\n\nSingle-channel speech separation has evolved from masks in the time–frequency domain to end-to-end time-domain architectures. Classical methods estimate magnitude masks on the STFT and reconstruct signals with phase-sensitive objectives, while modern approaches learn waveform-level representations with convolutional encoders and permutation-invariant training. A previous study showed that time-domain models outperform frequency-domain approaches for reverberant mixtures.\n\nBeyond architecture, data augmentation with room impulse responses, curriculum learning on mixture SNRs, and dereverberation front-ends have been explored to tackle real-world conditions. Nevertheless, robustness to long-range echoes and overlapping speakers in dynamic scenes remains a core challenge that we address with a hybrid representation and spatial cues derived from self-supervision.",
    "reason": "References a specific prior study and its result without providing a citation to identify the study.",
    "start": 377,
    "end": 489,
    "label": "Unsupported_claim"
  },
  {
    "span": "Several recent works generate unanswerable questions to improve robustness.",
    "document": "Related Work\n\nData augmentation has been widely adopted in machine reading comprehension (MRC) to improve generalization under distribution shift. Techniques range from paraphrasing contexts and questions to synthesizing counterfactual examples that target shallow heuristics. Several recent works generate unanswerable questions to improve robustness. Other approaches perturb named entities or alter temporal expressions to break spurious correlations. Despite these efforts, most augmentations are dataset-specific and lack transferability across domains. Our method introduces controllable perturbations guided by a taxonomy of reasoning skills, enabling targeted evaluation and training across multiple benchmarks.",
    "reason": "Mentions 'recent works' without citing any of them (rule d).",
    "start": 277,
    "end": 352,
    "label": "Unsupported_claim"
  },
  {
    "span": "To the best of our knowledge, this is the first work to leverage diffusion models for table structure recognition.",
    "document": "Introduction\n\nTable structure recognition aims to recover the logical layout of rows, columns, and cells from document images. Traditional methods rely on rule-based heuristics and visual cues, while recent approaches employ deep detectors and sequence models to capture complex layouts. However, challenges persist in handling diverse templates, occlusions, and low-quality scans.\n\nTo the best of our knowledge, this is the first work to leverage diffusion models for table structure recognition. We hypothesize that generative modeling can better capture the distribution over plausible grid topologies and improve robustness to noise. Our approach formulates structure prediction as conditional generation with iterative refinement.",
    "reason": "Claims novelty with respect to prior work without citing a survey or related studies to substantiate the 'first' claim.",
    "start": 383,
    "end": 497,
    "label": "Unsupported_claim"
  },
  {
    "span": "in (Kumar et al., 2018)",
    "document": "Related Work\n\nProgram synthesis with neural architectures has progressed from sequence-to-sequence decoding to structured decoding with constraints (Li et al., 2019; Chen et al., 2020). Early systems relied on enumerative search guided by learned heuristics, while recent work integrates symbolic priors into neural models to improve generalization (Reddy et al., 2021; Song and Zhao, 2022). Following the taxonomy in (Kumar et al., 2018), we categorize existing approaches into retrieval-augmented, constraint-guided, and neuro-symbolic hybrids. Retrieval-augmented methods index code snippets and select candidates using learned similarity metrics (Park et al., 2020), whereas constraint-guided approaches impose type and grammar constraints during decoding (Zhou et al., 2020). Our method departs from these by jointly optimizing retrieval and decoding under a unified objective (Wang et al., 2023).",
    "reason": "Wrong citation style: parenthetical citation used after a preposition. Should be formatted as narrative: 'Following the taxonomy in Kumar et al. (2018)'.",
    "start": 415,
    "end": 438,
    "label": "Format"
  },
  {
    "span": "The LibriSpeech-clean-100 subset is widely used for low-resource ASR experiments",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) has shifted from hybrid HMM-DNN pipelines to attention-based encoder-decoder and CTC/Transducer models (Graves, 2012; Chan et al., 2016; Prabhavalkar et al., 2017). The LibriSpeech-clean-100 subset is widely used for low-resource ASR experiments, while larger 960-hour configurations support strong supervised baselines. Self-supervised pretraining on unlabeled speech further improves robustness (Baevski et al., 2020; Hsu et al., 2021). Our work targets streaming ASR with limited labeled data and domain mismatch.",
    "reason": "Makes a claim about common usage of a specific dataset subset without citing supporting prior work.",
    "start": 225,
    "end": 305,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Lopez et al., 2019)",
    "document": "Related Work\n\nWeak supervision for classification has leveraged programmatic labeling and distant signals to reduce annotation costs (Ratner et al., 2017; Fu et al., 2020). In (Lopez et al., 2019) the authors show that iterative reweighting can denoise weak labels. Subsequent work explores co-teaching strategies to mitigate confirmation bias in noisy regimes (Han et al., 2018). More recent methods incorporate uncertainty estimates to filter unreliable pseudo-labels before retraining (Sohn et al., 2020; Chen and Rao, 2021). Our approach differs by coupling sample selection with constraint satisfaction to enforce label consistency across tasks (Martinez and Zhou, 2022).",
    "reason": "Wrong citation style: a parenthetical citation directly after 'In' should be narrative, e.g., \"In Lopez et al. (2019)\".",
    "start": 173,
    "end": 196,
    "label": "Format"
  },
  {
    "span": "To the best of our knowledge, no prior work explores cross-modal pretraining for satellite images at continental scale.",
    "document": "Introduction\n\nSelf-supervised learning (SSL) has transformed representation learning in computer vision by leveraging massive unlabeled data (Chen et al., 2020; Grill et al., 2020; He et al., 2020). In remote sensing, SSL has been adapted to handle unique challenges such as multi-spectral inputs and geospatial biases (Ayush et al., 2021; Manas et al., 2021). Cross-modal learning, in which visual features are aligned to complementary signals like text or audio, has proven particularly effective in natural images (Radford et al., 2021; Jia et al., 2021), suggesting potential benefits for earth observation where rich side information is available (e.g., place names, land-use tags).\n\nUnlike natural images, satellite imagery exhibits strong spatial autocorrelation, seasonal patterns, and domain shift across regions and sensors. These properties complicate scaling pretraining across continents and modalities. Existing work in remote sensing cross-modal learning has typically focused on small geographic regions or narrow label vocabularies (Xie et al., 2021; Zhu et al., 2022). To the best of our knowledge, no prior work explores cross-modal pretraining for satellite images at continental scale. This gap motivates our study of aligning medium-resolution optical imagery with weak textual anchors derived from open geodatabases and volunteered geographic information.\n\nWe present GeoCLIP, a contrastive framework that pairs Sentinel-2 image tiles with noisy text derived from place categories and human-contributed tags. We curate a continental-scale dataset spanning diverse biomes and urban morphologies, and we show that learned representations transfer well to downstream land cover mapping, settlement detection, and change monitoring. Our contributions are: (1) a large-scale cross-modal corpus for satellite imagery, (2) a robust training recipe for noisy, weak textual supervision, and (3) extensive evaluation across geographies and tasks.",
    "reason": "Claims absence of prior work at continental scale without citing surveys or attempts; such 'no prior work' assertions require evidence or citations.",
    "start": 1087,
    "end": 1206,
    "label": "Unsupported_claim"
  },
  {
    "span": "A previous study reported a 20% relative improvement from persona conditioning",
    "document": "Introduction\n\nOpen-domain dialogue systems increasingly incorporate user personas to produce consistent and engaging responses. Conditioning on static persona descriptions can guide lexical choice and topic persistence, but may also introduce repetition. A previous study reported a 20% relative improvement from persona conditioning on multi-turn engagement metrics, motivating further investigation into controllable personalization. We propose a retrieval-augmented generator that adapts persona evidence over the course of a conversation to reduce drift.",
    "reason": "Claims a specific result from a prior study, including a statistic, without citation (rule b/e).",
    "start": 255,
    "end": 333,
    "label": "Unsupported_claim"
  },
  {
    "span": "in [27]",
    "document": "Related Work\n\nSelf-Supervised Representation Learning\n\nContrastive learning maximizes agreement between augmented views of the same instance (He et al., 2020; Chen et al., 2020) and benefits from large batch sizes and memory banks (Wu et al., 2018). Non-contrastive objectives avoid negative pairs via collapse-preventing constraints (Grill et al., 2020; Chen and He, 2021). Temporal extensions exploit sequential structure for videos (Patrick et al., 2021), with additional results reported in [27] for long-form content. Transfer to downstream tasks is often evaluated on linear probes and few-shot settings (Kornblith et al., 2019; Zhai et al., 2019).\n",
    "reason": "Inconsistent citation style: numeric bracket citation '[27]' appears in an author–year context; should be converted to an author–year citation.",
    "start": 492,
    "end": 499,
    "label": "Format"
  },
  {
    "span": "Knowledge tracing models include Bayesian Knowledge Tracing (Corbett and Anderson, 1995), factorization approaches such as IRT and PFA (Rasch, 1960; Pavlik et al., 2009), recurrent neural models like DKT and its variants (Piech et al., 2015; Zhang et al., 2017), dynamic key-value memory networks (Zhang et al., 2017; Abdelrahman and Wang, 2019), and graph-based KT that models relations among skills and items (Nakagawa et al., 2019; Liu et al., 2020). Public datasets such as ASSISTments, EdNet, and Statics2011 are commonly used (Feng et al., 2009; Choi et al., 2020; Koedinger et al., 2015).",
    "document": "Introduction\n\nKnowledge tracing seeks to infer a learner’s evolving mastery of skills from interaction logs, informing personalization and content sequencing. A central difficulty is handling sparsity and heterogeneity across students and items.\n\nKnowledge tracing models include Bayesian Knowledge Tracing (Corbett and Anderson, 1995), factorization approaches such as IRT and PFA (Rasch, 1960; Pavlik et al., 2009), recurrent neural models like DKT and its variants (Piech et al., 2015; Zhang et al., 2017), dynamic key-value memory networks (Zhang et al., 2017; Abdelrahman and Wang, 2019), and graph-based KT that models relations among skills and items (Nakagawa et al., 2019; Liu et al., 2020). Public datasets such as ASSISTments, EdNet, and Statics2011 are commonly used (Feng et al., 2009; Choi et al., 2020; Koedinger et al., 2015).\n\nIn this work we address cross-course generalization with a meta-learned prior over skills that adapts quickly to new curricula while preserving interpretability through calibrated skill posteriors.",
    "reason": "The span lists model families and datasets but does not connect them to the paper’s goal or articulate the gap addressed, exemplifying (a) and (b).",
    "start": 247,
    "end": 842,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Uncertainty-aware active learning selects examples that maximize expected information gain (Houlsby et al., 2011). Curriculum learning orders data to accelerate convergence (Bengio et al., 2009). Data valuation attributes utility to training points based on Shapley approximations (Ghorbani and Zou, 2019).",
    "document": "Introduction\n\nData-centric AI emphasizes acquiring, selecting, and curating data to improve model quality under budget constraints. Selection policies must consider uncertainty, diversity, and representativeness, while balancing computational overhead.\n\nRecent work explores dynamic curricula, human-in-the-loop labeling, and principled data valuation to allocate annotation effort where it matters most. However, integrating these signals into a unified framework remains challenging.\n\nUncertainty-aware active learning selects examples that maximize expected information gain (Houlsby et al., 2011). Curriculum learning orders data to accelerate convergence (Bengio et al., 2009). Data valuation attributes utility to training points based on Shapley approximations (Ghorbani and Zou, 2019).\n\nWe propose a joint objective that fuses epistemic uncertainty with marginal contribution estimates to guide acquisition and ordering. Experiments on vision and NLP benchmarks show improved label efficiency over strong baselines.",
    "reason": "The span presents three separate strands (active learning, curriculum, data valuation) with no transitions or explicit explanation of their interrelations, causing a coherence gap across sentences (a, b).",
    "start": 487,
    "end": 793,
    "label": "Coherence"
  },
  {
    "span": "([Chen et al., 2016)",
    "document": "Related Work\n\nOpen-domain passage retrieval relies on dense encoders trained with contrastive learning to map queries and documents into a shared space (Karpukhin et al., 2020; Xiong et al., 2021). Hard-negative mining improves discrimination among closely related passages (Qu et al., 2021; Zhan et al., 2021). Hybrid retrieval combines sparse and dense signals to leverage exact matching and semantic similarity (Gao et al., 2021; Formal et al., 2021). A classic dual-encoder baseline is ([Chen et al., 2016) which pairs question and paragraph embeddings with a dot-product similarity, while later methods introduce late interaction to retain token-level matches (Khattab and Zaharia, 2020).",
    "reason": "Unmatched bracket in the citation: there is an extra '[' before the opening parenthesis. It should be '(Chen et al., 2016)'.",
    "start": 490,
    "end": 510,
    "label": "Format"
  },
  {
    "span": "(Miller, 2016 Johnson, 2017)",
    "document": "Introduction\n\nEnd-to-end speech recognition models have replaced traditional HMM-GMM pipelines with neural encoders and sequence criteria such as CTC and attention-based transducers (Graves et al., 2006; Chan et al., 2016; Prabhavalkar et al., 2018). Self-supervised pretraining with masked prediction on unlabeled audio further improves data efficiency (Baevski et al., 2020; Hsu et al., 2021).\n\nAugmentations like SpecAugment and noise injection help achieve robustness to channel and environmental variability (Park et al., 2019; Ko et al., 2015). Domain adaptation remains challenging in far-field and accented speech (Li et al., 2017; Watanabe et al., 2018). Prior studies investigate curriculum learning and pronunciation lexicon constraints (Miller, 2016 Johnson, 2017) to stabilize training and reduce substitution errors.\n\nWe introduce a confidence-aware decoding strategy that calibrates beam search scores using posterior estimates derived from temperature-scaled logits (Kannan et al., 2018; Desh Raj et al., 2021).",
    "reason": "Missing separator between multiple citations; a semicolon should separate them, e.g., '(Miller, 2016; Johnson, 2017)'.",
    "start": 748,
    "end": 776,
    "label": "Format"
  },
  {
    "span": "Recent works on retrieval-augmented generation have dramatically improved open-domain QA accuracy.",
    "document": "Related Work\n\nRecent works on retrieval-augmented generation have dramatically improved open-domain QA accuracy. Classic open-domain QA pipelines retrieve passages using sparse or dense retrievers before extracting answers (Chen et al., 2017; Karpukhin et al., 2020). Generation-based approaches fuse evidence from retrieved documents at training and inference time to produce more fluent and context-grounded answers (Lewis et al., 2020). Hybrid methods further interleave retrieval and generation to iteratively refine evidence selection.",
    "reason": "Uses the phrase “Recent works” to claim progress without citing any of those works in the same statement.",
    "start": 14,
    "end": 112,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent advances in graph neural networks have largely solved the over-smoothing problem.",
    "document": "Introduction\n\nGraph neural networks (GNNs) excel at learning over relational structures but suffer from over-smoothing as layers deepen, leading to indistinguishable node representations. Proposed remedies include residual connections, normalization schemes, and regularization that preserves feature diversity. Recent advances in graph neural networks have largely solved the over-smoothing problem. Nevertheless, scalability and stability on dynamic graphs remain open issues. We address these by introducing a dual-timescale message passing scheme with adaptive residual gating.",
    "reason": "A broad claim about recent advances resolving a known issue is made without citing any supporting works.",
    "start": 312,
    "end": 400,
    "label": "Unsupported_claim"
  },
  {
    "span": "A previous study showed that node dropout is equivalent to an L2 regularizer in expectation.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a central tool for recommendation, enabling message passing over user–item interaction graphs. Regularization is critical to avoid overfitting, particularly in sparse and long-tailed regimes.\n\nA previous study showed that node dropout is equivalent to an L2 regularizer in expectation. Subsequent work has proposed alternative stochastic training tricks, such as edge masking and feature corruption, to improve generalization under distributional shift.\n\nWe investigate regularization-through-sampling for session-based recommendation and propose a sampler that targets hard negatives while preserving topology statistics.\n",
    "reason": "This sentence attributes a specific theoretical result to 'a previous study' without providing a citation to that study.",
    "start": 248,
    "end": 340,
    "label": "Unsupported_claim"
  },
  {
    "span": "Prior studies investigate secure aggregation, personalization layers, and domain adaptation across hospitals (McMahan et al., 2017; Shamir et al., 2018; Li et al., 2020; Chen et al., 2021).",
    "document": "Introduction\n\nFederated learning offers a path to train predictive models on decentralized clinical data without centralizing sensitive records. Hospitals differ in patient demographics, devices, and coding practices, creating substantial heterogeneity that challenges generalization.\n\nPrior studies investigate secure aggregation, personalization layers, and domain adaptation across hospitals (McMahan et al., 2017; Shamir et al., 2018; Li et al., 2020; Chen et al., 2021).\n\nWhile these lines of work advance privacy and utility, practical deployment still suffers from missing modalities and sporadic participation. We study robustness to partial modality availability in cross-hospital federated training.",
    "reason": "Lists categories of work with citations but does not explain how they relate to the paper’s objectives or identify a gap in that sentence (criteria a and b).",
    "start": 286,
    "end": 475,
    "label": "Lacks_synthesis"
  },
  {
    "span": "As noted in earlier work, pointer generator networks alleviate exposure bias in summarization.",
    "document": "Introduction\n\nAbstractive summarization systems have progressed from sequence-to-sequence baselines toward pretrained encoder decoder architectures with strong language modeling priors (See et al., 2017; Lewis et al., 2020). Challenges remain in factual consistency, faithfulness, and controllability of generated summaries.\n\nAs noted in earlier work, pointer generator networks alleviate exposure bias in summarization. Recent methods instead emphasize reinforcement learning with task-specific rewards, constrained decoding, and editing-based approaches to improve factuality and content selection (Paulus et al., 2018; Dong et al., 2019; Narayan et al., 2021).\n\nWe study a hybrid approach that integrates constrained decoding with lightweight fine-tuning for faithfulness, measuring both factual errors and semantic coverage across multiple news datasets.",
    "reason": "Attributes a specific effect to prior work but provides no citation to identify which work demonstrated it.",
    "start": 326,
    "end": 420,
    "label": "Unsupported_claim"
  },
  {
    "span": "It is well known that message passing architectures saturate on QM9 and fail to capture long-range effects.",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become the dominant paradigm for molecular property prediction by aggregating local neighborhood information through learned message passing (Kearnes et al., 2016; Gilmer et al., 2017). Improvements incorporate attention mechanisms, spectral filters, and geometric priors to better encode molecular structure and 3D conformations (Veličković et al., 2018; Schütt et al., 2018; Jing et al., 2021). The QM9 dataset remains a canonical benchmark for evaluating energy and property prediction models at small-molecule scale (Ramakrishnan et al., 2014).\n\nIt is well known that message passing architectures saturate on QM9 and fail to capture long-range effects. Motivated by this limitation, we propose a hybrid architecture that augments local message passing with global field transformers conditioned on learned molecular graphs, improving sensitivity to distant functional group interactions.",
    "reason": "This is a claim about established knowledge in the literature without citations; it references specific behavior on a dataset and should be supported by prior studies.",
    "start": 598,
    "end": 705,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Johnson 2019; Patel, 2020)",
    "document": "Related Work\n\nAspect-based sentiment analysis decomposes sentiment by target and context, often using syntax-aware encoders and attention (Wang et al., 2016; Tang et al., 2016; Liu et al., 2018). Pre-trained language models have improved generalization across domains and targets (Sun et al., 2019; Xu et al., 2019).\n\nDomain adaptation methods align feature distributions or exploit pivot features to transfer knowledge (Blitzer et al., 2007; Liu et al., 2019). However, label scarcity and target shift remain challenging (Ben-David et al., 2010; Azizzadenesheli et al., 2019).\n\nPrior evaluations often overlook calibration and uncertainty, leading to brittle decisions under drift (Johnson 2019; Patel, 2020). We instead report expected calibration error and assess robustness under lexicon shifts (Guo et al., 2017; Hendrycks and Gimpel, 2017).",
    "reason": "Missing comma between author and year in a parenthetical citation: 'Johnson 2019' should be 'Johnson, 2019' within '(Johnson, 2019; Patel, 2020)'.",
    "start": 682,
    "end": 709,
    "label": "Format"
  },
  {
    "span": "In (Garcia and Patel, 2021)",
    "document": "Related Work\n\nGraph neural networks (GNNs) extend deep learning to relational data by iteratively aggregating neighborhood information (Kipf and Welling, 2017; Hamilton et al., 2017). Variants differ in normalization, attention, and higher-order message passing (Velickovic et al., 2018; Xu et al., 2019). Scalability is addressed with sampling and partitioning strategies (Chen et al., 2018; Zeng et al., 2020).\n\nIn (Garcia and Patel, 2021), contrastive objectives are used to improve node representations under limited labels, complementing pretext tasks such as attribute masking and context prediction (You et al., 2020; Hassani and Khasahmadi, 2020). Self-supervised GNNs have further been studied for robustness to structure noise (Jin et al., 2021) and for transfer across graphs via meta-learning (Yao et al., 2020).\n\nWe revisit contrastive GNN training with a focus on distribution shift between training and deployment graphs, proposing an augmentation policy that preserves label-consistent motifs while discouraging shortcut correlations.",
    "reason": "Wrong citation style with preposition before a parenthetical citation; should read “In Garcia and Patel (2021)” or remove the preposition before a parenthetical citation.",
    "start": 414,
    "end": 441,
    "label": "Format"
  },
  {
    "span": "Contrastive pretraining on image–text pairs scales to billions of samples (Radford et al., 2021). Vision Transformers benefit from strong regularization and data augmentation (Dosovitskiy et al., 2021). Unsupervised domain adaptation aligns feature distributions to reduce dataset shift (Tzeng et al., 2017).",
    "document": "Related Work\n\nLarge-scale visual representation learning. Self-supervised and weakly supervised objectives have substantially improved transfer across vision tasks. Contrastive and non-contrastive frameworks learn invariances from massive unlabeled or weakly labeled datasets, often paired with careful data augmentations.\n\nArchitectures for recognition. Convolutional networks remain competitive, while transformers have shown strong results when trained on large corpora. Regularization, token sparsity, and hierarchical designs mitigate data hunger and computational cost.\n\nContrastive pretraining on image–text pairs scales to billions of samples (Radford et al., 2021). Vision Transformers benefit from strong regularization and data augmentation (Dosovitskiy et al., 2021). Unsupervised domain adaptation aligns feature distributions to reduce dataset shift (Tzeng et al., 2017).\n\nOur work studies cross-domain transfer under label scarcity with a hybrid objective that adapts contrastive embeddings while preserving discriminative performance. We benchmark on shift-prone settings, analyzing the interplay between data regime and model capacity.",
    "reason": "The sentences mention three different topics (multimodal contrastive pretraining, ViT regularization, domain adaptation) without transitions or an explicit link, making the connection between cited works unclear (a, b).",
    "start": 577,
    "end": 885,
    "label": "Coherence"
  },
  {
    "span": "Recent works demonstrate that transformer-based abstractive summarizers consistently outperform extractive methods on long biomedical reports.",
    "document": "Introduction\n\nAutomatic text summarization has seen rapid progress with the advent of large pretrained language models. In the biomedical domain, long clinical and radiology reports introduce unique challenges due to domain-specific terminology and the need to preserve critical findings. Recent works demonstrate that transformer-based abstractive summarizers consistently outperform extractive methods on long biomedical reports. However, robust evidence across institutions and report types remains limited, and external validation is still rare. In this paper, we study summarization under constrained supervision and evaluate models on multi-institutional datasets with varying report structures to assess generalizability.",
    "reason": "Mentions \"recent works\" and a comparative performance claim without providing citations at first mention (rule d and a).",
    "start": 289,
    "end": 431,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works have dramatically improved code summarization quality.",
    "document": "Related Work\n\nNeural approaches to code summarization build on sequence-to-sequence learning and structural encoders that exploit abstract syntax trees and control/data-flow graphs. Early methods adapted neural machine translation models to map code to natural language descriptions, while later work incorporated graph-based encoders and pretraining on large code corpora to improve fluency and relevance. Recent works have dramatically improved code summarization quality. Concurrently, pretrained models specialized for programming languages, such as encoder-decoder architectures initialized on source code and docstrings, have further advanced the state of the art by leveraging large-scale bimodal pretraining. Despite these advances, handling low-resource languages and project-specific terminology remains challenging, motivating our study.",
    "reason": "Mentions 'recent works' improving performance without providing citations to any specific papers (rule d).",
    "start": 407,
    "end": 474,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Singh 2019)",
    "document": "Related Work\n\nContrastive learning for sentence embeddings has been explored in supervised and unsupervised settings. Supervised contrastive losses benefit from label information to cluster classes tightly (Kohli and Raman, 2021). Unsupervised variants rely on data augmentation and dropout noise to construct positive pairs (Chen et al., 2020). In the context of document retrieval, curriculum strategies improve hard negative mining (Patel et al., 2022). Prior domain-specific evaluations include legal search (Singh 2019) and biomedical abstracts (Iyer et al., 2020), but comprehensive cross-domain comparisons remain limited.",
    "reason": "Missing comma between author and year in a parenthetical citation; should be \"(Singh, 2019)\".",
    "start": 512,
    "end": 524,
    "label": "Format"
  },
  {
    "span": "A previous study showed that mixing synthetic and natural speech at a 3:1 ratio yields the best ASR accuracy for low-resource languages.",
    "document": "Related Work\n\nData augmentation is a crucial lever for automatic speech recognition (ASR), particularly in low-resource settings. Techniques such as SpecAugment alter the time-frequency mask to improve robustness (Park et al., 2019), while multi-condition training with noise and reverberation targets domain mismatch (Ko et al., 2017). Synthetic speech from TTS has been explored to expand training corpora, with mixed findings depending on voice diversity and prosody control (Jia et al., 2019; Hayashi et al., 2020). A previous study showed that mixing synthetic and natural speech at a 3:1 ratio yields the best ASR accuracy for low-resource languages. Recent self-supervised pretraining (Baevski et al., 2020) further reduces labeled data requirements but still benefits from augmentation during fine-tuning.",
    "reason": "References a specific prior study and quantitative setup (3:1 ratio) but provides no citation, violating rule (a).",
    "start": 520,
    "end": 656,
    "label": "Unsupported_claim"
  },
  {
    "span": "Huang et. al. (2015)",
    "document": "Related Work\n\nContinual learning methods aim to prevent catastrophic forgetting when models face non-stationary data. Regularization-based techniques constrain parameter updates to preserve prior knowledge (Kirkpatrick et al., 2017). Replay buffers approximate joint training by interleaving samples from previous tasks (Rolnick et al., 2019). Dynamic architectures allocate capacity for new tasks via modular components (Rusu et al., 2016). Huang et. al. (2015) introduced a task-aware normalization scheme that stabilizes transfer across domains. More recent work employs contrastive objectives to maintain representation alignment over time (Buzzega et al., 2020). Our contribution complements replay with uncertainty-weighted rehearsal that prioritizes exemplars critical to decision boundaries (Kim and Rao, 2022).",
    "reason": "Incorrect punctuation in 'et al.'; it should be 'Huang et al. (2015)' without a period after 'et'.",
    "start": 442,
    "end": 462,
    "label": "Format"
  },
  {
    "span": "Early spatio-temporal forecasting for traffic relied on sequence models such as ARIMA and Kalman filters (Williams and Hoel, 2003; Okutani and Stephanedes, 1984), followed by deep architectures including CNNs for grid traffic maps (Zhang et al., 2016), RNNs and LSTMs for sensor streams (Ma et al., 2015; Yu et al., 2017), attention-based sequence-to-sequence models (Li et al., 2018), and graph neural networks that capture road topology (Yu et al., 2018; Li et al., 2019; Wu et al., 2019). Several benchmarks such as METR-LA, PEMS-BAY, and PEMS-D guide model evaluation (Li et al., 2018; Song et al., 2020). Recent efforts also integrate external signals like weather and events (Hoang et al., 2019; Pan et al., 2019).",
    "document": "Related Work\n\nUrban traffic forecasting. Predicting future traffic states is a canonical spatio-temporal learning problem with practical impact on routing and congestion mitigation. Methods must model both spatial dependencies induced by the road network and temporal dynamics from daily and weekly patterns. There is also increasing interest in robustness under sensor outages and topology changes.\n\nEarly spatio-temporal forecasting for traffic relied on sequence models such as ARIMA and Kalman filters (Williams and Hoel, 2003; Okutani and Stephanedes, 1984), followed by deep architectures including CNNs for grid traffic maps (Zhang et al., 2016), RNNs and LSTMs for sensor streams (Ma et al., 2015; Yu et al., 2017), attention-based sequence-to-sequence models (Li et al., 2018), and graph neural networks that capture road topology (Yu et al., 2018; Li et al., 2019; Wu et al., 2019). Several benchmarks such as METR-LA, PEMS-BAY, and PEMS-D guide model evaluation (Li et al., 2018; Song et al., 2020). Recent efforts also integrate external signals like weather and events (Hoang et al., 2019; Pan et al., 2019).\n\nDynamic graphs for traffic. A subset of work considers time-varying graphs using adaptive adjacency matrices or attention over learned node embeddings (Bai et al., 2020; Pan et al., 2021). These approaches suggest that fixed topology misses context-specific dependencies that emerge from incidents or construction.\n\nContrastive learning for spatio-temporal data. Self-supervised objectives, particularly contrastive ones, have improved representation quality in videos and multivariate time series (Tschannen et al., 2020; Yue et al., 2022). Using augmentations that preserve predictive structure appears critical.\n\nIn this paper we introduce a contrastive graph learner that constructs incident-aware dynamic neighborhoods and aligns multi-horizon representations, aiming to improve generalization to topology shifts and sparse sensors.",
    "reason": "The span lists prior models, datasets, and signals without connecting them to the paper's argument, gap, or contribution, satisfying (a) and (c).",
    "start": 401,
    "end": 1121,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Torralba and Efros (2011) analyzed dataset bias across classification benchmarks. Shortcuts in visual datasets lead to spurious correlations (Geirhos et al., 2020). Balanced sampling approaches rebalance long-tailed distributions (Cui et al., 2019). Domain adaptation methods reduce cross-dataset shift (Tzeng et al., 2017).",
    "document": "Related Work\n\nDataset bias and spurious correlations limit the external validity of visual recognition systems. Approaches to mitigating bias range from data collection and reweighting strategies to invariant representation learning and domain adaptation.\n\nTorralba and Efros (2011) analyzed dataset bias across classification benchmarks. Shortcuts in visual datasets lead to spurious correlations (Geirhos et al., 2020). Balanced sampling approaches rebalance long-tailed distributions (Cui et al., 2019). Domain adaptation methods reduce cross-dataset shift (Tzeng et al., 2017).\n\nOur contribution focuses on learning invariant predictors through counterfactual data augmentation guided by causal saliency maps.",
    "reason": "The span enumerates several lines of work without articulating how they relate or transition from analysis to mitigation strategies, making the connections implicit and abrupt.",
    "start": 257,
    "end": 581,
    "label": "Coherence"
  },
  {
    "span": "(Lee et al., 2018))",
    "document": "Introduction\n\nLarge language models for code generation leverage paired natural language–code corpora and self-supervised objectives (Alon et al., 2019; Feng et al., 2020). Encoder–decoder and decoder-only variants differ in conditioning strategies but share common pretraining setups (Chen et al., 2021; Austin et al., 2021).\n\nProgram induction remains sensitive to spurious patterns and incomplete specifications (Odena and Sutton, 2020; Nye et al., 2021). Prior work augments training with execution feedback and unit tests to improve reliability (Chen et al., 2021; Le et al., 2022). We adopt contrastive decoding with test-time retrieval to reduce hallucinations and promote semantic fidelity (Khot et al., 2022; Lee et al., 2018)). Our analyses further examine robustness to prompt perturbations and unseen APIs (Nijkamp et al., 2022; Fried et al., 2022).\n\nWe evaluate on multiple code-generation benchmarks spanning algorithmic tasks and real-world libraries to assess generalization and editability (Hendrycks et al., 2021; Austin et al., 2021).",
    "reason": "Extra closing parenthesis in the parenthetical citation; should be '(Lee et al., 2018)' with a single closing parenthesis.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "in (Klein, 2021)",
    "document": "Related Work\n\nFederated learning enables on-device training without centralizing raw data (McMahan et al., 2017). For privacy guarantees, see in (Klein, 2021) for details on the composition of noise across rounds, while recent work studies secure aggregation and compression (Bonawitz et al., 2017; Kairouz et al., 2021). Personalization techniques address client drift through meta-learning and clustering (Arivazhagan et al., 2019; Fallah et al., 2020).",
    "reason": "Improper use of a preposition with a parenthetical citation; should be narrative “see Klein (2021)” or rephrase to avoid “in (Klein, 2021)”.",
    "start": 142,
    "end": 158,
    "label": "Format"
  },
  {
    "span": "In (Lopez et al., 2019)",
    "document": "Introduction\n\nCross-lingual transfer learning has enabled effective model reuse across related languages by sharing subword vocabularies and adapter layers (Pires et al., 2019; Pfeiffer et al., 2020). In (Lopez et al., 2019) we see that unsupervised translation benefits from iterative back-translation and cross-domain language modeling.\n\nSubsequent work extended these ideas to morphologically rich languages and low-resource settings (Artetxe et al., 2020; Conneau et al., 2020). Nonetheless, performance still degrades substantially under typological divergence and script mismatch, motivating our study of robust alignment objectives.",
    "reason": "Wrong citation style with a leading preposition inside parentheses; should be 'in Lopez et al. (2019)'.",
    "start": 201,
    "end": 224,
    "label": "Format"
  },
  {
    "span": "García et al. 1",
    "document": "Related Work\n\nCross-modal retrieval has advanced through joint embedding spaces that align images and text (Frome et al., 2013; Kiros et al., 2015). García et al. 1 propose a metric-learning approach with hard negative mining, demonstrating gains on Flickr30K and MSCOCO. Subsequent research explores modality-specific encoders with late fusion to improve fine-grained grounding (Karpathy and Fei-Fei, 2015; Li et al., 2019).\n\nOur work builds on these ideas by introducing a curriculum that dynamically adjusts negative sampling based on semantic difficulty.\n",
    "reason": "Improper footnote-like usage; the superscript/footnote '1' appears without a proper citation year and should be reformatted as a standard citation.",
    "start": 149,
    "end": 164,
    "label": "Format"
  },
  {
    "span": "Multi-agent path planning addresses collision avoidance and coordination through prioritized planning, velocity obstacles, and decentralized policies (Silver, 2005; van den Berg et al., 2011; Chen et al., 2017; Everett et al., 2021).",
    "document": "Related Work\n\nCooperative navigation in crowded spaces requires anticipating interactions and resolving conflicts under partial observability and real-time constraints. Methods balance responsiveness with safety while scaling to many agents.\n\nSimulation-to-reality transfer further complicates deployment due to model mismatch and unmodeled social norms that affect human-robot interaction.\n\nMulti-agent path planning addresses collision avoidance and coordination through prioritized planning, velocity obstacles, and decentralized policies (Silver, 2005; van den Berg et al., 2011; Chen et al., 2017; Everett et al., 2021).\n\nWe develop a graph-based policy with predictive interaction fields and uncertainty-aware safety margins that generalizes across densities and preserves smoothness in tight spaces.",
    "reason": "The span lists major approach families with citations but does not relate them to the paper's objectives, identify shortcomings, or offer a synthesized perspective; it lacks synthesis.",
    "start": 392,
    "end": 625,
    "label": "Lacks_synthesis"
  },
  {
    "span": "((Miller, 2021))",
    "document": "Introduction\n\nOptimizing transformer-based models has benefited from adaptive learning rates and warmup schedules that stabilize early training (Vaswani et al., 2017; Goyal et al., 2017). Label smoothing and stochastic depth further improve generalization in deep architectures (Szegedy et al., 2016; Huang et al., 2016). Recent analyses connect sharpness-aware minimization to flat minima that correlate with robustness (Foret et al., 2021). However, empirical reports vary depending on batch size and optimizer hyperparameters ((Miller, 2021)) and lack standardized evaluation under compute constraints. We present a controlled study isolating the effects of gradient clipping and weight decay across sequence lengths.\n\nRelated Work\n\nLarge-batch training techniques enhance throughput but can degrade convergence without careful tuning (You et al., 2020). Parallelism strategies and mixed precision have broadened the feasible training regime for long-context models (Shoeybi et al., 2019; Narayanan et al., 2021).",
    "reason": "Extra parentheses around the citation; should be a single set of parentheses, \"(Miller, 2021)\".",
    "start": 529,
    "end": 545,
    "label": "Format"
  },
  {
    "span": "To reduce edge inference cost, pruning removes redundant weights or channels (Han et al., 2015; He et al., 2017), quantization lowers precision to 8/4/2 bits (Jacob et al., 2018; Banner et al., 2018), neural architecture search yields efficient backbones (Cai et al., 2019; Tan and Le, 2019), early-exit branches enable adaptive computation (Teerapittayanon et al., 2016; Scardapane et al., 2020), and knowledge distillation transfers accuracy to small students (Hinton et al., 2015; Romero et al., 2015).",
    "document": "Introduction\n\nDeploying deep models on resource-constrained edge devices demands careful trade-offs between accuracy, latency, memory footprint, and energy usage. While cloud offloading helps, network variability and privacy limit its applicability.\n\nTo reduce edge inference cost, pruning removes redundant weights or channels (Han et al., 2015; He et al., 2017), quantization lowers precision to 8/4/2 bits (Jacob et al., 2018; Banner et al., 2018), neural architecture search yields efficient backbones (Cai et al., 2019; Tan and Le, 2019), early-exit branches enable adaptive computation (Teerapittayanon et al., 2016; Scardapane et al., 2020), and knowledge distillation transfers accuracy to small students (Hinton et al., 2015; Romero et al., 2015).\n\nHowever, energy-aware optimization under realistic duty cycles and variable input difficulty remains under-explored. We propose a joint scheduler and model that co-optimizes exit policies and DVFS states to satisfy energy budgets while maintaining accuracy SLAs.",
    "reason": "The span lists categories of prior methods without linking them to the paper’s goals or articulating a specific shortcoming, violating (a) and (b).",
    "start": 251,
    "end": 756,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Transformer-based forecasters have been proposed for long-horizon multivariate series, including Informer, Autoformer, FEDformer, and PatchTST (Zhou et al., 2021; Wu et al., 2021; Zhou et al., 2022; Nie et al., 2023). Classical baselines such as ARIMA, Prophet, and exponential smoothing remain competitive in low-data regimes (Box et al., 2015; Taylor and Letham, 2018; Hyndman and Athanasopoulos, 2018).",
    "document": "Related Work\n\nTime-series forecasting spans applications from energy planning to supply chains. Models must handle seasonality, covariates, and distribution shift while producing calibrated uncertainty estimates.\n\nTransformer-based forecasters have been proposed for long-horizon multivariate series, including Informer, Autoformer, FEDformer, and PatchTST (Zhou et al., 2021; Wu et al., 2021; Zhou et al., 2022; Nie et al., 2023). Classical baselines such as ARIMA, Prophet, and exponential smoothing remain competitive in low-data regimes (Box et al., 2015; Taylor and Letham, 2018; Hyndman and Athanasopoulos, 2018).\n\nOur study explores covariate selection under covariate shift using a simple attention mechanism and evaluates across electricity and traffic datasets.",
    "reason": "The span enumerates prior transformer models and classical baselines without explaining how their design choices relate to the paper’s covariate-selection focus or where they fall short; it lacks synthesis and author perspective.",
    "start": 214,
    "end": 619,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[Johnson et al., 2015)",
    "document": "Related Work\n\nNeural IR bridges sparse and dense retrieval by learning semantic representations for queries and documents (Mitra and Craswell, 2018; Lin et al., 2021). Dual encoders scale to web corpora but may underperform on lexical matching, while cross-encoders excel at re-ranking with higher latency (Karpukhin et al., 2020; Nogueira and Cho, 2019).\n\nHybrid approaches combine BM25 signals with neural scores to capture both exact and semantic matches (Chen et al., 2017; Xiong et al., 2021). For domain adaptation, contrastive pretraining on in-domain click logs improves retrieval robustness (Lu et al., 2020; Ma et al., 2021). Traditional learning-to-rank methods remain competitive when features are well engineered [Johnson et al., 2015) but require careful feature normalization and interaction terms.\n\nOur method unifies dual-encoder retrieval with lightweight cross-attention re-ranking, balancing effectiveness and efficiency under tight latency constraints.",
    "reason": "Mismatched brackets in a citation: opening square bracket with a closing parenthesis; should use matching parentheses for author–year style, e.g., “(Johnson et al., 2015)”.",
    "start": 726,
    "end": 748,
    "label": "Format"
  },
  {
    "span": "In (Johnson and Lee, 2020)",
    "document": "Related Work\n\nGraph-based semi-supervised learning methods propagate labels through similarity graphs to exploit unlabeled data (Zhu and Ghahramani, 2002; Belkin et al., 2006). In (Johnson and Lee, 2020), the authors propose an adaptive graph reweighting scheme that updates edge strengths during training. This approach improves calibration over static k-NN graphs but introduces additional hyperparameters. Later, Chen et al. (2021) integrate consistency regularization to stabilize training under heavy label noise. Other lines of work explore graph neural networks that learn task-specific propagation rules (Kipf and Welling, 2017; Velickovic et al., 2018), while manifold regularization remains a strong baseline when feature geometry is reliable (Sindhwani and Keerthi, 2006).\n\nOur method differs by decoupling representation learning from graph construction via a curriculum over confidence thresholds, which yields benefits in both convergence and label efficiency.",
    "reason": "Wrong citation style: a narrative construction should read \"In Johnson and Lee (2020)\" without enclosing the authors in parentheses after \"In\".",
    "start": 177,
    "end": 203,
    "label": "Format"
  },
  {
    "span": "Kreutzer & Sokolov (2018)",
    "document": "Related Work\n\nBandit learning for neural machine translation seeks to optimize models with partial feedback instead of gold references. Kreutzer & Sokolov (2018) demonstrate that counterfactual risk minimization can leverage logged human feedback to improve translation quality, while Sokolov et al. (2017) study on-policy bandit updates with simulated rewards. Complementary work explores reward shaping from quality estimation models (Specia et al., 2018) and preference-based optimization (Kreutzer et al., 2018). Our work targets extractive QA and proposes an off-policy objective that reduces variance from sparse binary rewards.\n",
    "reason": "Wrong conjunction style in narrative citation; narrative form should use “and” instead of “&”.",
    "start": 136,
    "end": 161,
    "label": "Format"
  },
  {
    "span": "It is well known that ASR word error rates in kitchens exceed 40% under typical household noise conditions.",
    "document": "Introduction\n\nSmart-home speech interfaces must operate reliably in far-field, noise-prone environments. Reverberation, overlapping speech, and appliance noise severely degrade recognition accuracy, eroding user trust and impeding adoption. It is well known that ASR word error rates in kitchens exceed 40% under typical household noise conditions. This motivates research into robust acoustic modeling, noise-robust front-ends, and domain-adaptive language models tailored to household vocabularies and commands. We propose a multi-condition training regimen coupled with dynamic beamforming to mitigate performance drops in open-plan homes.",
    "reason": "Presents a precise performance statistic as common knowledge without any empirical citation; niche statistic requires evidence per rule (b).",
    "start": 241,
    "end": 348,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Vaswani et al. 2017)",
    "document": "Introduction\n\nTransformer architectures have become the de facto standard for sequence modeling in NLP and beyond due to superior parallelism and long-range dependency modeling (Vaswani et al. 2017). Subsequent refinements such as deeper networks, sparse attention, and improved pretraining objectives further enhance performance (Child et al., 2019; Clark et al., 2020). Despite these advances, training efficiency and domain robustness remain open challenges, motivating techniques like adapter modules (Houlsby et al., 2019) and parameter sharing across layers (Lan et al., 2020). We investigate a complementary direction by jointly optimizing architectural sparsity and curriculum scheduling.\n",
    "reason": "Missing comma between author and year in a parenthetical citation; should be “(Vaswani et al., 2017)”.",
    "start": 177,
    "end": 198,
    "label": "Format"
  },
  {
    "span": "Amodei et al. (2016) demonstrated that end-to-end CTC systems can scale with large datasets. Povey et al. (2011) advanced hybrid HMM-DNN pipelines with discriminative training. Chan et al. (2016) proposed Listen, Attend and Spell for attention-based sequence modeling. SpecAugment improves performance with time-frequency masking (Park et al., 2019).",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) competes with traditional hybrid systems by training a single model that maps acoustic features to text. Key dimensions include alignment modeling, language modeling integration, and data augmentation.\n\nAmodei et al. (2016) demonstrated that end-to-end CTC systems can scale with large datasets. Povey et al. (2011) advanced hybrid HMM-DNN pipelines with discriminative training. Chan et al. (2016) proposed Listen, Attend and Spell for attention-based sequence modeling. SpecAugment improves performance with time-frequency masking (Park et al., 2019).\n\nRecent work explores transducer models, internal LM control, and streaming constraints to reduce latency while maintaining accuracy (Graves, 2012; Kim et al., 2020). We focus on data-efficient training strategies for low-resource languages.",
    "reason": "The span jumps among disparate ASR paradigms without transitions or explanation of their relationships, making the connections between consecutive citations unclear.",
    "start": 263,
    "end": 613,
    "label": "Coherence"
  },
  {
    "span": "Johnson et al.",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become a central paradigm for learning on relational data. Early message-passing formulations (Kipf and Welling, 2017; Hamilton et al., 2017) demonstrated that shallow architectures can capture local structure effectively. Johnson et al. extend this line by proposing hierarchical pooling operators that aggregate multi-scale neighborhoods while preserving permutation invariance. Subsequent work has explored attention over edges (Gao and Ji, 2019) and spectral approximations (Wu et al., 2019) to improve scalability. Despite these advances, robustness to distribution shift remains underexplored in the GNN literature.\n",
    "reason": "Narrative citation missing year; should be formatted as Johnson et al. (YEAR) in author-year style.",
    "start": 271,
    "end": 285,
    "label": "Format"
  },
  {
    "span": "Multimodal brain tumor segmentation often fuses T1, T1c, T2, and FLAIR using UNet-like architectures (Ronneberger et al., 2015; Isensee et al., 2018), attention gates (Oktay et al., 2018), and cross-modal transformers (Chen et al., 2021). Self-supervised pretraining on medical images has also improved data efficiency (Zeng et al., 2021; Chaitanya et al., 2020).",
    "document": "Related Work\n\nAccurate segmentation of brain tumors across modalities supports diagnosis, treatment planning, and longitudinal monitoring. Heterogeneous scanners, limited labels, and domain shifts impose practical constraints on deployable systems.\n\nMultimodal brain tumor segmentation often fuses T1, T1c, T2, and FLAIR using UNet-like architectures (Ronneberger et al., 2015; Isensee et al., 2018), attention gates (Oktay et al., 2018), and cross-modal transformers (Chen et al., 2021). Self-supervised pretraining on medical images has also improved data efficiency (Zeng et al., 2021; Chaitanya et al., 2020).\n\nWe explore modality-robust training with missing-modality simulation and consistency regularization, assessing generalization across hospitals with differing acquisition protocols.\n",
    "reason": "The span lists architectural trends and pretraining techniques without connecting them to the authors’ modality-robust objective or clarifying what remains unsolved, thus lacking synthesis under (a) and (c).",
    "start": 250,
    "end": 613,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[Kim et al., 2021)",
    "document": "Related Work\n\nIn robot manipulation, learning visuomotor policies from demonstrations reduces sample complexity compared to pure reinforcement learning (Levine et al., 2016; Radosavovic et al., 2021). Offline pretraining on large video corpora has shown promise for generalization (Pertsch et al., 2022; Parisi et al., 2022). Prior art [Kim et al., 2021) integrates keypoint tracking with policy learning to stabilize control under occlusion, while other approaches exploit object-centric representations for compositional generalization (Goyal et al., 2020). Our method unifies temporal keypoint consistency with action-value constraints to mitigate covariate shift during deployment.",
    "reason": "Mismatched brackets in the citation; starts with '[' and ends with ')'. Should use matching parentheses or brackets consistently.",
    "start": 336,
    "end": 354,
    "label": "Format"
  },
  {
    "span": "Clinical de-identification has been approached using CRFs with hand-crafted features, BiLSTM-CRF architectures, and transformer-based NER (Santos et al., 2016; Li and Rao, 2018; Chen et al., 2020; Huang et al., 2021). Data augmentation with synthetic PHI and domain adaptation across institutions have also been explored (Patel et al., 2021; Gaddy and Noor, 2022).",
    "document": "Introduction\nRemoving protected health information (PHI) from clinical narratives is a prerequisite for data sharing and downstream modeling. Despite strong NER backbones, practical de-identification must cope with shifting annotation policies, rare entity types, and privacy-preserving deployment constraints within hospitals. We investigate methods that remain accurate under policy drift without revealing sensitive text to external services.\n\nRelated Work\nClinical de-identification has been approached using CRFs with hand-crafted features, BiLSTM-CRF architectures, and transformer-based NER (Santos et al., 2016; Li and Rao, 2018; Chen et al., 2020; Huang et al., 2021). Data augmentation with synthetic PHI and domain adaptation across institutions have also been explored (Patel et al., 2021; Gaddy and Noor, 2022). Privacy-preserving inference using enclaves and local differential privacy has been studied in clinical NLP (Rossi et al., 2020; Kim and Ahmed, 2022).\n\nWe propose a policy-conditional de-identification model that conditions on a formalized PHI schema and leverages uncertainty-triggered redaction, enabling controllable behavior across institutions while retaining high recall on rare PHI categories.",
    "reason": "The span compiles prior techniques and directions but does not explain their limitations, how they relate to policy drift, or how they inform the present work; it lacks synthesis and explicit motivation.",
    "start": 460,
    "end": 824,
    "label": "Lacks_synthesis"
  },
  {
    "span": "We build upon a prior study that first introduced anchor-free detectors for aerial images.",
    "document": "Related Work\n\nObject detection in aerial imagery presents unique challenges due to small object sizes, arbitrary orientations, and extreme aspect ratios (Zhu et al., 2020; Xia et al., 2018). Early detectors relied on anchor-based designs adapted from natural image benchmarks. Recently, anchor-free detectors have gained popularity in natural image domains due to their simplicity and strong performance. We build upon a prior study that first introduced anchor-free detectors for aerial images. Our approach differs by integrating deformable convolutions and rotation-equivariant heads tailored to overhead scenes, enabling better localization under dense object layouts.\n\nIn addition, we review rotation-aware bounding representations and discuss their trade-offs for training stability and inference speed.",
    "reason": "It claims reliance on 'a prior study' that introduced anchor-free detectors for aerial images but fails to cite that specific study.",
    "start": 405,
    "end": 495,
    "label": "Unsupported_claim"
  },
  {
    "span": "Deep models for time-series anomaly detection range from autoencoders (Sakurada and Yairi, 2014), LSTM prediction-based approaches (Malhotra et al., 2015), variational models (An and Cho, 2015), GAN-based detectors (Li et al., 2019), to graph-temporal methods (Deng and Hooi, 2021), with benchmarks such as NAB, Yahoo, UCR, and MTSAD.",
    "document": "Introduction\n\nDetecting rare deviations in multivariate time series underlies applications in manufacturing, IT operations, and healthcare monitoring. Challenges include non-stationarity, limited labels, and delayed feedback. We investigate calibrated uncertainty scoring for early detection with cost-sensitive evaluation.\n\nDeep models for time-series anomaly detection range from autoencoders (Sakurada and Yairi, 2014), LSTM prediction-based approaches (Malhotra et al., 2015), variational models (An and Cho, 2015), GAN-based detectors (Li et al., 2019), to graph-temporal methods (Deng and Hooi, 2021), with benchmarks such as NAB, Yahoo, UCR, and MTSAD.\n\nOur approach unifies probabilistic forecasting with conformal risk control to deliver threshold-stable alerts across changing operating regimes.",
    "reason": "The span lists model families and datasets but fails to connect them to the proposed uncertainty-calibrated approach or to articulate remaining gaps, corresponding to (a) and (c).",
    "start": 325,
    "end": 659,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Prompt-based paradigms—such as few-shot prompting, chain-of-thought, and self-consistency—have been widely used to elicit latent capabilities of large language models for code generation (Brown et al., 2020; Chen et al., 2021; Wei et al., 2022; Kojima et al., 2022).",
    "document": "Related Work\n\nProgram Synthesis with Large Language Models. The rise of large language models (LLMs) has led to considerable progress in code generation, repair, and synthesis from natural language specifications. Prompt-based paradigms—such as few-shot prompting, chain-of-thought, and self-consistency—have been widely used to elicit latent capabilities of large language models for code generation (Brown et al., 2020; Chen et al., 2021; Wei et al., 2022; Kojima et al., 2022). Concurrently, alignment strategies, execution feedback, and constraint enforcement have been explored to reduce hallucinations and improve correctness (Le et al., 2022; Jain et al., 2022; Wang et al., 2023).\n\nVerification and Execution Feedback. Integrating unit tests, symbolic constraints, and runtime signals can refine LLM outputs post hoc or during iterative generation (Austin et al., 2021; Shi et al., 2022; Lahiri et al., 2023).",
    "reason": "This sentence lists techniques and citations but does not connect them to the authors' research goals or articulate what aspect remains unaddressed.",
    "start": 214,
    "end": 480,
    "label": "Lacks_synthesis"
  },
  {
    "span": "in (Vatswani et al., 2019)",
    "document": "Related Work\n\nGraph representation learning has advanced rapidly with the development of neighborhood aggregation methods and scalable training regimes (Hamilton et al., 2017; Kipf and Welling, 2017). Semi-supervised node classification benchmarks have popularized message passing architectures and their variants (Wu et al., 2019). However, these models often overlook the role of heterophily and mixed-order dependencies, which recent studies attempt to address (Pei et al., 2020; Zhu et al., 2020). Similar findings were reported in (Li et al., 2020) for large-scale graphs and in (Vatswani et al., 2019) for temporal networks, though their training recipes differ from ours. In contrast, our approach targets stability under neighborhood corruption while retaining expressive power (Xu et al., 2019).\n\nBeyond convolutional designs, regularization and pretraining over subgraphs have been explored to improve generalization (Qiu et al., 2020). We also note recent interest in contrastive frameworks that exploit multi-view augmentations of graphs (You et al., 2020; Hassani and Khasahmadi, 2020).",
    "reason": "Wrong citation style: narrative use of 'in (Vatswani et al., 2019)' should be 'in Vatswani et al. (2019)'.",
    "start": 581,
    "end": 607,
    "label": "Format"
  },
  {
    "span": "BERT has been successfully used for AES when trained on large-scale essay corpora.",
    "document": "Introduction\n\nAutomated essay scoring (AES) seeks to predict human-assigned scores for student writing and has evolved from handcrafted linguistic features toward neural models. Early systems modeled surface features such as length and word usage, whereas subsequent work incorporated syntax, discourse, and coherence indicators to improve reliability. With the advent of pretrained language models, transformer-based encoders have become attractive for AES due to their strong contextual representations. BERT has been successfully used for AES when trained on large-scale essay corpora. Despite these advances, cross-prompt generalization and robustness to adversarially perturbed essays remain open issues. We address these gaps by proposing a domain-adaptive pretraining strategy and evaluating transfer across prompts and genres.\n",
    "reason": "Claims prior success of BERT in AES without citing any specific study or dataset (violates rules a and b).",
    "start": 506,
    "end": 588,
    "label": "Unsupported_claim"
  },
  {
    "span": "Industry reports estimate that roughly 22% of distribution-level outages are caused by faulty or drifting sensors.",
    "document": "Introduction\n\nAnomaly detection in power distribution systems is crucial for reliability, safety, and cost containment. Utilities deploy thousands of sensors for voltage, current, and frequency monitoring, yet noisy telemetry, missing data, and concept drift remain major pain points. Industry reports estimate that roughly 22% of distribution-level outages are caused by faulty or drifting sensors. This motivates detection methods that are robust to sensor degradation and can disentangle equipment faults from genuine grid disturbances.\n\nClassical statistical process control methods assume stationarity and independence that rarely hold in real grids. Modern deep sequence models capture temporal dependencies but often overfit to local conditions and degrade under domain shift across feeders and seasons. We propose a probabilistic multiscale forecasting framework with explicit uncertainty calibration to detect anomalies as deviations from predictive distributions, and we assess transferability across regions using few-shot adaptation.",
    "reason": "Presents a precise statistic ('22%') without any source or evidence, violating rule (b) for specific statistics.",
    "start": 285,
    "end": 399,
    "label": "Unsupported_claim"
  },
  {
    "span": "a previous study found that teacher annealing stabilizes multilingual training",
    "document": "Related Work\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student, often improving generalization and efficiency. In neural machine translation, sequence-level and token-level distillation variants have been developed, with extensions to multilingual and low-resource scenarios.\n\nWhile curriculum design and temperature scheduling have been explored, a previous study found that teacher annealing stabilizes multilingual training. Yet the interaction of annealing schedules with language imbalance and domain shift remains under-explored.\n\nWe introduce adaptive, language-conditioned annealing that adjusts the teacher influence based on per-language validation dynamics, leading to more stable convergence and better average BLEU across typologically diverse languages.",
    "reason": "References a specific prior 'previous study' without citation per rule (a).",
    "start": 394,
    "end": 472,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith & Lee (2022)",
    "document": "Related Work\n\nContrastive representation learning has transformed sentence and document embeddings (Gao et al., 2021; Giorgi et al., 2021). Hard-negative mining improves discrimination but can introduce false negatives under domain shift (Robinson et al., 2021). Following Smith & Lee (2022), we incorporate calibration-aware temperature scaling to stabilize training across batches with varying difficulty. Unlike curriculum-based schedules (Platanios et al., 2019), our approach adapts online to the observed loss landscape. We situate our work alongside recent studies on debiased contrastive objectives (Chuang et al., 2020).",
    "reason": "Wrong conjunction in narrative citation for APA-like style: should use 'and' in narrative form, i.e., 'Smith and Lee (2022)'.",
    "start": 273,
    "end": 291,
    "label": "Format"
  },
  {
    "span": "Recommender systems include collaborative and content-based methods. Matrix factorization models latent user-item interactions (Koren et al., 2009). Neural recommenders use attention and sequence encoders (Kang and McAuley, 2018). Graph-based recommenders propagate preferences over interaction graphs (Wang et al., 2019). Session-based models focus on short-term behavior (Hidasi et al., 2016). Debiasing aims to correct exposure and selection bias (Saito, 2020).",
    "document": "Related Work\n\nPersonalized recommendation is a broad field encompassing user modeling, sequence prediction, and causal inference. Beyond accuracy, modern systems must address bias, cold start, and scalability. Techniques vary in their inductive biases and data requirements, making unified evaluation challenging.\n\nRecommender systems include collaborative and content-based methods. Matrix factorization models latent user-item interactions (Koren et al., 2009). Neural recommenders use attention and sequence encoders (Kang and McAuley, 2018). Graph-based recommenders propagate preferences over interaction graphs (Wang et al., 2019). Session-based models focus on short-term behavior (Hidasi et al., 2016). Debiasing aims to correct exposure and selection bias (Saito, 2020).\n\nWe propose a counterfactual evaluation protocol that jointly measures ranking quality and de-biasing effectiveness under logged bandit feedback.",
    "reason": "The span lists multiple distinct recommender strands with no transitions or explicit linking among them, so the connection between cited works is abrupt and unclear.",
    "start": 315,
    "end": 779,
    "label": "Coherence"
  },
  {
    "span": "KBRD integrates knowledge graphs with conversation history (Chen et al., 2019). Bandit approaches optimize interactive recommendation with implicit feedback (Zhao et al., 2013; Li et al., 2016).",
    "document": "Related Work\n\nConversational recommender systems (CRS) aim to elicit user preferences through dialogue and provide personalized item suggestions. Prior research spans natural language understanding for preference extraction, knowledge-aware reasoning, and exploration-exploitation under uncertainty.\n\nNeural CRS models track user goals via slot-filling or latent intent representations (Zhang et al., 2018; Lei et al., 2020). Knowledge-aware methods leverage graphs to propagate semantics from entities mentioned in dialogue to candidate items (Zhou et al., 2020; Wang et al., 2021). KBRD integrates knowledge graphs with conversation history (Chen et al., 2019). Bandit approaches optimize interactive recommendation with implicit feedback (Zhao et al., 2013; Li et al., 2016). Mixed-initiative strategies learn when to ask clarifying questions to reduce uncertainty (Christakopoulou et al., 2016; Sun and Zhang, 2018).\n\nOur work unifies decision-making and knowledge reasoning by training a policy that adaptively queries entities when graph uncertainty is high, aligning exploration with conversational signals.",
    "reason": "The span juxtaposes a specific knowledge-graph CRS with general bandit methods without a transition or explanation of how bandits relate to KBRD or to the text’s flow, weakening coherence.",
    "start": 584,
    "end": 778,
    "label": "Coherence"
  },
  {
    "span": "The RoboSuite benchmark is commonly used for evaluating sim-to-real transfer.",
    "document": "Related Work\n\nBridging the gap between simulation and reality is a long-standing challenge in robotic manipulation. Domain randomization, representation learning, and policy adaptation methods aim to improve robustness when transferring from synthetic to real environments. The RoboSuite benchmark is commonly used for evaluating sim-to-real transfer. Researchers also rely on tabletop manipulation suites and custom lab setups to assess generalization across tasks and morphologies.\n\nWhile benchmarks facilitate standardized comparisons, variations in sensor calibration, actuation latency, and contact dynamics can obscure true performance differences. Our study proposes a reproducible protocol for measuring transfer under controlled perturbations and hardware constraints.",
    "reason": "Mentions a specific benchmark as 'commonly used' without any citation at first mention (rule a and b).",
    "start": 274,
    "end": 351,
    "label": "Unsupported_claim"
  },
  {
    "span": "Previous shared tasks on code summarization have evaluated only on Java projects.",
    "document": "Introduction\n\nAutomatic code summarization seeks to generate short natural-language descriptions of source code, supporting code search and maintenance (Allamanis et al., 2016; Ahmad et al., 2020). Benchmarks typically compile aligned function-comment pairs and evaluate sequence-to-sequence models with BLEU or ROUGE metrics (Hu et al., 2018). Previous shared tasks on code summarization have evaluated only on Java projects. This narrow focus risks overfitting modeling assumptions to a single language family and ecosystem. We introduce a multilingual benchmark spanning Java, Python, and Go, with consistent preprocessing and evaluation, and we study cross-language generalization with parameter-efficient adaptation (Houlsby et al., 2019).",
    "reason": "Mentions 'previous shared tasks' and makes a specific coverage claim without citing the shared tasks or providing evidence.",
    "start": 345,
    "end": 426,
    "label": "Unsupported_claim"
  },
  {
    "span": "Miller et al., (2015)",
    "document": "Introduction\n\nPrivacy-preserving federated learning has sparked interest in optimization with limited communication and strict client confidentiality (McMahan et al., 2017; Kairouz et al., 2021). Miller et al., (2015) demonstrate that adding calibrated noise can provide formal privacy guarantees while maintaining utility.\n\nSubsequent work integrates secure aggregation and client subsampling to further reduce privacy leakage (Bonawitz et al., 2017; Bittau et al., 2017). Building on these insights, we propose an adaptive clipping mechanism that reduces gradient bias in non-IID regimes.",
    "reason": "Incorrect narrative citation punctuation; should be 'Miller et al. (2015)' without the comma before the parenthetical year.",
    "start": 196,
    "end": 217,
    "label": "Format"
  },
  {
    "span": "SQuAD 2.0",
    "document": "Introduction\n\nExtractive question answering (QA) requires identifying an answer span within a given passage. While early reading comprehension datasets contained only answerable questions, more challenging settings include unanswerable queries that require calibrated abstention. In this work we study robust extractive QA under domain shift and adversarial perturbations. We evaluate on SQuAD 2.0 and Natural Questions and analyze performance across categories of unanswerability. Additionally, we consider data augmentation strategies that encourage the model to refrain from answering when insufficient evidence is present.",
    "reason": "References a well-known dataset but does not cite it on first mention, violating the requirement to cite datasets (rule a).",
    "start": 388,
    "end": 397,
    "label": "Unsupported_claim"
  },
  {
    "span": "There are many recent works that explore lifelong reinforcement learning in mobile robots.",
    "document": "Introduction\n\nLifelong reinforcement learning (LRL) aims to accumulate skills and knowledge across tasks while avoiding catastrophic forgetting. Embodied agents offer a natural testbed for LRL, as robots must continually adapt to novel environments, objects, and goals (Kormushev et al., 2013). Curriculum design, replay buffers, and modular policies have been proposed to balance stability and plasticity in continual learning (Parisi et al., 2019). There are many recent works that explore lifelong reinforcement learning in mobile robots. Yet, most evaluations remain siloed to simulation, highlighting the need for standardized real-world benchmarks and safety-aware protocols.\n",
    "reason": "Uses 'many recent works' without providing any citations to back the assertion.",
    "start": 451,
    "end": 541,
    "label": "Unsupported_claim"
  },
  {
    "span": "Patel et al. 3",
    "document": "Introduction\n\nFew-shot learning aims to generalize to new classes with limited labeled examples by leveraging inductive biases and metric structures. Metric-based methods learn class prototypes in an embedding space to enable nearest-neighbor classification (Snell et al., 2017). For prototypical networks, see Patel et al. 3 for a survey. Optimization-based methods adapt parameters rapidly with gradient-based meta-learning (Finn et al., 2017), while augmentation-based strategies expand support sets with synthesized examples (Zhang and Sohn, 2020).",
    "reason": "Improper use of a footnote-like number with an author name; should include a year as a proper narrative citation (e.g., \"Patel et al. (2020)\") or be formatted as an actual footnote.",
    "start": 311,
    "end": 325,
    "label": "Format"
  },
  {
    "span": "(Hendrycks and Gimpel, 2016; Jiang et al., 2020",
    "document": "Introduction\n\nOut-of-distribution detection and calibration are critical for deploying classifiers safely (Guo et al., 2017; Ovadia et al., 2019). Recent work proposes confidence regularization, energy-based models, and post-hoc temperature scaling to improve uncertainty estimates (Hinton et al., 2015; Liu et al., 2020; Wang et al., 2021). We also consider robust training techniques (Hendrycks and Gimpel, 2016; Jiang et al., 2020 that enhance resilience under covariate shift while preserving in-distribution accuracy.\n",
    "reason": "Missing closing parenthesis in a parenthetical citation list. It should be \")\" at the end: \"(Hendrycks and Gimpel, 2016; Jiang et al., 2020)\".",
    "start": 386,
    "end": 433,
    "label": "Format"
  },
  {
    "span": "Previous studies report that Swahili–English translation benefits more from subword regularization than from BPE",
    "document": "Related Work\n\nNeural machine translation (NMT) for low-resource languages faces persistent challenges, including data sparsity, domain shift, and morphological richness. Previous studies report that Swahili–English translation benefits more from subword regularization than from BPE, especially in the low-resource regime. Complementary approaches leverage multilingual transfer, back-translation, and self-training to improve quality when parallel data are scarce.",
    "reason": "Asserts a finding from prior studies without providing citations to those studies.",
    "start": 170,
    "end": 282,
    "label": "Unsupported_claim"
  },
  {
    "span": "LibriSpeech",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) has seen rapid progress with the advent of sequence-to-sequence architectures and self-supervised pretraining. Robust evaluation remains essential to measure gains across acoustic conditions and speaker variability. We evaluate on LibriSpeech to ensure comparability with prior work and to assess performance at different training data scales. Our approach integrates a conformer encoder with shallow fusion decoding and a multi-resolution loss to improve stability on rare words and numerical expressions.",
    "reason": "First mention of a specific dataset appears without a citation (rule a).",
    "start": 291,
    "end": 302,
    "label": "Unsupported_claim"
  },
  {
    "span": "Nguyen et al.",
    "document": "Related Work\n\nContinual learning has progressed from early regularization-based methods to memory-augmented architectures. Elastic Weight Consolidation constrains important parameters to resist forgetting (Kirkpatrick et al., 2017), while memory replay mixes stored exemplars with new data to stabilize updates (Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2019). Nguyen et al. propose a class-balanced reservoir strategy that selects exemplars to maintain representation diversity. Prototype-based objectives further reduce interference by aligning features across tasks (Wang et al., 2021; Liu and Hoi, 2022). Orthogonal gradient projection reduces negative transfer by decoupling task-specific updates (Yu et al., 2020). In domain-incremental scenarios, contrastive rehearsal has shown strong performance (Zhou and Liang, 2022), and test-time adaptation complements replay by adjusting batch statistics on the fly (Nado et al., 2020). Our work combines exemplar selection with adaptive consolidation to improve stability under strict memory budgets.",
    "reason": "Narrative citation is missing the publication year; it should be formatted as 'Nguyen et al. (YEAR)'.",
    "start": 365,
    "end": 378,
    "label": "Format"
  },
  {
    "span": "Patel et al.",
    "document": "Introduction\n\nGraph neural networks (GNNs) have emerged as a dominant paradigm for learning over relational data, showing strong performance in chemistry, recommender systems, and knowledge graphs (Scarselli et al., 2009; Kipf & Welling, 2017; Hamilton et al., 2017). Attention mechanisms and residual connections improve message passing and stability (Veličković et al., 2018; Xu et al., 2018). As argued by Patel et al., message-passing depth interacts with over-smoothing in ways that depend on graph homophily (Zhu et al., 2020; Oono & Suzuki, 2020). We build on hierarchical pooling methods to better capture multi-scale structure (Ying et al., 2018; Lee et al., 2019), and we evaluate against strong baselines using transductive and inductive splits (Yang et al., 2016; Hu et al., 2020). Our contribution is a training recipe that balances expressivity with robustness to distribution shift (Sagawa et al., 2020; Hendrycks et al., 2021).",
    "reason": "Narrative citation missing year; should be formatted as 'Patel et al. (YEAR)'.",
    "start": 409,
    "end": 421,
    "label": "Format"
  },
  {
    "span": "Shermis and Burstein (2013) surveyed automated essay scoring systems and their validity. Stab and Gurevych (2017) created corpora for argument mining in persuasive essays. Zhai et al. (2020) explored neural approaches for feedback generation in classrooms. Xu and Meurers (2015) examined task-specific formative feedback in intelligent tutoring systems.",
    "document": "Introduction\n\nProviding timely, high-quality feedback is critical for learning outcomes, yet manual feedback is labor-intensive and inconsistent across instructors and contexts (Shute, 2008; Carless and Boud, 2018). Natural language processing has opened opportunities to scale feedback generation and align comments with rubric criteria (Ramachandran et al., 2017; Mathias and Bhattacharyya, 2018).\n\nShermis and Burstein (2013) surveyed automated essay scoring systems and their validity. Stab and Gurevych (2017) created corpora for argument mining in persuasive essays. Zhai et al. (2020) explored neural approaches for feedback generation in classrooms. Xu and Meurers (2015) examined task-specific formative feedback in intelligent tutoring systems.\n\nDespite progress, prior approaches often struggle with domain transfer and the pedagogical specificity required for actionable feedback (Wambsganss et al., 2021; Nguyen et al., 2022). We introduce a rubric-aligned generation framework with uncertainty-aware critique selection to improve both pedagogical utility and cross-course generalization.",
    "reason": "The listed citations span essay scoring, argument mining, feedback generation, and tutoring but there is no explicit connective tissue or transitions clarifying how each work relates to the previous one or to a shared problem.",
    "start": 401,
    "end": 754,
    "label": "Coherence"
  },
  {
    "span": "In (Garcia and Lee, 2020)",
    "document": "Related Work\n\nGraph neural networks (GNNs) generalize convolution to non-Euclidean domains by iteratively aggregating neighbor features (Kipf and Welling, 2017; Hamilton et al., 2017). Attention-based variants learn importances over edges to improve expressivity (Velickovic et al., 2018). In (Garcia and Lee, 2020) a hierarchical pooling mechanism is introduced to coarsen graphs while preserving task-relevant structure, achieving competitive results on molecular benchmarks. Subsequent studies refine pooling with differentiable clustering (Bianchi et al., 2020) and spectral criteria (Diehl, 2019). Our method departs from these by enforcing global consistency with a Markov diffusion prior, akin to label propagation (Zhu et al., 2003) but integrated into end-to-end training.",
    "reason": "Wrong citation style; preposition placed before a parenthetical citation should use narrative form 'In Garcia and Lee (2020)' rather than 'In (Garcia and Lee, 2020)'.",
    "start": 290,
    "end": 315,
    "label": "Format"
  },
  {
    "span": "Dosovitskiy et al. (2021) introduced Vision Transformers for image classification. Caron et al. (2021) studied self-distillation with no labels for representation learning. He et al. (2020) proposed momentum contrast for unsupervised pretraining. Touvron et al. (2021) optimized training recipes for data-efficient transformers.",
    "document": "Related Work\n\nModern visual recognition systems leverage both architectural advances and representation learning without labels. Convolutional networks dominated for years due to their inductive biases and efficient training procedures (He et al., 2016; Huang et al., 2017). Recently, transformer-based models have displaced CNNs on many benchmarks by scaling data and compute, while self-supervised learning has reduced reliance on annotations (Chen et al., 2020; Grill et al., 2020).\n\nDosovitskiy et al. (2021) introduced Vision Transformers for image classification. Caron et al. (2021) studied self-distillation with no labels for representation learning. He et al. (2020) proposed momentum contrast for unsupervised pretraining. Touvron et al. (2021) optimized training recipes for data-efficient transformers.\n\nWhile these strands are often combined in practice, the mechanisms by which pretraining recipes interact with architectural capacity remain underexplored. Prior comparative studies typically vary one factor at a time, confounding capacity, augmentation, or optimization (Wightman et al., 2021). We explicitly factorize these elements in a unified framework to assess transfer efficiency under limited labels.",
    "reason": "The span presents four papers in a sequence with no connective tissue explaining their relationships. It is unclear how self-distillation, momentum contrast, and training recipes connect to the introduction of Vision Transformers, leading to abrupt topic shifts.",
    "start": 487,
    "end": 815,
    "label": "Coherence"
  },
  {
    "span": "(Hansen et. al., 2016)",
    "document": "Introduction\n\nTime series anomaly detection has benefited from advances in representation learning, including autoencoders and probabilistic forecasting (Zong et al., 2018; Salinas et al., 2020). Hybrid detectors that combine reconstruction error with predictive uncertainty show promise in nonstationary settings (Audibert et al., 2020; Xu et al., 2018). Yet, evaluation protocols vary widely across benchmarks (Hansen et. al., 2016), making cross-study comparisons difficult and potentially misleading (Tatbul et al., 2018; Ren et al., 2019).\n\nWe present a standardized evaluation suite with consistent preprocessing, metrics, and significance testing across diverse datasets.",
    "reason": "Incorrect formatting of “et al.” with an extra dot after “et”; should be “(Hansen et al., 2016)”.",
    "start": 412,
    "end": 434,
    "label": "Format"
  },
  {
    "span": "Smith et al., 2019[3]",
    "document": "Related Work\n\nCross-modal retrieval aligns vision and text in a shared space to enable image-to-text and text-to-image search (Frome et al., 2013; Kiros et al., 2014). Dual-encoder architectures scale training with in-batch negatives and large corpora (Wu et al., 2018; Radford et al., 2021). Empirically, strong negative mining and temperature scaling are crucial for performance, as shown by Smith et al., 2019[3], who study curriculum negatives in large-scale training.\n\nCLIP-style contrastive pretraining further improves zero-shot transfer (Radford et al., 2021), but suffers from dataset biases and prompt sensitivity (Goh et al., 2021). Our approach addresses retrieval robustness under distribution shift by adaptive temperature control.",
    "reason": "Mixing numeric footnote notation with author-year style in the same citation; should be a consistent author-year citation (e.g., \"Smith et al., 2019\") or a properly formatted numbered reference.",
    "start": 394,
    "end": 415,
    "label": "Format"
  },
  {
    "span": "Model-free and model-based reinforcement learning have been extensively studied for navigation (Mnih et al., 2015; Levine et al., 2016; Kalashnikov et al., 2018).",
    "document": "Introduction\n\nLearning for robot navigation. Reinforcement learning provides a framework for learning navigation policies from interaction. Model-free and model-based reinforcement learning have been extensively studied for navigation (Mnih et al., 2015; Levine et al., 2016; Kalashnikov et al., 2018). Sim-to-real transfer and domain randomization address the gap between training and deployment (Tobin et al., 2017; Peng et al., 2018).\n\nOur setting. We focus on long-horizon, partially observable indoor navigation with dynamic obstacles and sparse rewards.\n\nOur contribution. We present a hierarchical latent-world model with uncertainty-aware planning that improves sample efficiency and safety.",
    "reason": "The span cites broad RL paradigms for navigation without articulating their limitations in the targeted setting or connecting them to the proposed approach, failing to synthesize.",
    "start": 140,
    "end": 302,
    "label": "Lacks_synthesis"
  },
  {
    "span": "We follow the standard setup used in prior work on BERT-based AES models.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) has shifted from hand-crafted features and linear models to pretrained language models that capture richer discourse and style signals. Recent neural approaches jointly model prompt, content, and coherence while enforcing score calibration across prompts.\n\nWe follow the standard setup used in prior work on BERT-based AES models. Specifically, we fine-tune with holistic scores, apply prompt-aware segment truncation, and evaluate using quadratic weighted kappa. Our contribution lies in a contrastive objective that aligns sentence-level evidence with overall rubric dimensions.",
    "reason": "References an unspecified 'standard setup' and 'prior work' without citing any papers establishing it (rules a, d).",
    "start": 301,
    "end": 374,
    "label": "Unsupported_claim"
  },
  {
    "span": "Our baseline follows the standard setup where UCF101 is used for pre-training and HMDB51 for fine-tuning.",
    "document": "Related Work\n\nVideo action recognition has progressed from hand-crafted descriptors to deep 3D convolutional networks and two-stream architectures (Simonyan and Zisserman, 2014; Carreira and Zisserman, 2017; Tran et al., 2018). Large-scale datasets such as Kinetics have enabled strong pretraining that transfers well to smaller benchmarks. Our baseline follows the standard setup where UCF101 is used for pre-training and HMDB51 for fine-tuning. Recent transformer-based approaches further improve long-range temporal modeling but require careful regularization to avoid overfitting in low-data regimes (Bertasius et al., 2021; Arnab et al., 2021).",
    "reason": "Asserts a 'standard setup' involving specific datasets without citing the source or precedent establishing this protocol, violating rule (a)/(b).",
    "start": 341,
    "end": 446,
    "label": "Unsupported_claim"
  },
  {
    "span": "Brown and al. (2020)",
    "document": "Related Work\n\nFew-shot learning with large language models relies on in-context examples to elicit task behaviors without gradient updates (Radford et al., 2019; Brown and al. (2020)). Prompt design and calibration have been shown to impact performance across tasks (Zhao et al., 2021; Holtzman et al., 2021). Subsequent research explores instruction tuning and alignment to improve robustness and followability (Sanh et al., 2022; Ouyang et al., 2022).\n\nWhile in-context learning reduces annotation burden, it remains sensitive to prompt selection and domain shift (Min et al., 2022). Our work complements these findings by proposing a retrieval-augmented prompting strategy that stabilizes performance.",
    "reason": "Incorrect author listing in citation; \"and al.\" should be \"et al.\" (APA style), e.g., \"Brown et al. (2020)\".",
    "start": 162,
    "end": 182,
    "label": "Format"
  },
  {
    "span": "In contrast, while\ncross-encoder QA models are more accurate, they depend on a specific question when encoding a passage; thus, they are less suited to downstream use cases that require contextualized representations of passages in isolation.",
    "document": "Introduction\n\nAlthough masked language models build contextualized word representations, they are pre-trained with losses that minimize distance to uncontextualized word embeddings (Peters et al., 2018;Devlin et al., 2019;Liu et al., 2019). In this paper, we introduce Question Answering Infused Pre-training (QUIP), a new pre-training loss based on question answering (QA) that depends much more directly on context, and learns improved token-level representations for a range of zero-and few-shot tasks.\n\nOur intuition for QUIP is that the contextualized representation for a phrase in a passage should contain enough information to identify all the questions that the phrase could answer in context. For example, in Figure 1, the representation for Johannes Brahms should be similar to the representation of all questions it can answer, such as \"Who wrote the violin concerto?\" We anticipate that optimizing passage representations for QA should benefit many downstream tasks, as question-answer pairs have been used as broad-coverage meaning representations (He et al., 2015; Michael et al., 2018), and a wide range of NLP tasks can be cast as QA problems (Levy et al., 2017; McCann et al., 2018; Gardner et al., 2019), For instance, our learned representations should encode whether a phrase answers a question like “Why was the movie considered good?”, which corresponds to identifying rationales for sentiment analysis.\n\nWe train QUIP with a bi-encoder extractive QA objective. The bi-encoder model independently encodes passages and questions such that the representation of each phrase in a passage is similar to the representation of reading comprehension questions answered by that phrase. To train this model, we use a question generation model to synthesize 80 million QA examples, then train the bi-encoder to match the predictions of a cross-encoder QA\nmodel, which processes the passage and question together, on these examples. Bi-encoder QA has been used before for efficient open-domain QA via phrase retrieval (Seo et al., 2018, 2019; Lee et al., 2020, 2021), but its lower accuracy compared to cross-encoder QA has previously been viewed as a drawback. We instead view the relative weakness of bi-encoder QA as an opportunity to improve contextual representations via knowledge distillation, as self-training can be effective when the student model must solve a harder problem than the teacher (Xie et al., 2020). In particular, since the bi-encoder does not know the question when encoding the passage, it must produce a single passage representation that simultaneously encodes the answers to all possible questions. In contrast, while\ncross-encoder QA models are more accurate, they depend on a specific question when encoding a passage; thus, they are less suited to downstream use cases that require contextualized representations of passages in isolation.\n\nWe show that QUIP token-level representations are useful in a variety of zero-shot and few-shot learning settings, both because the representations directly encode useful contextual information, and because we can often reduce downstream tasks to QA. For few-shot paraphrase detection, QUIP with BERTScore-based features (Zhang et al., 2020)\noutperforms prior work by 9 F1 points across four datasets. For few-shot named entity recognition (NER), QUIP combined with an initialization scheme that uses question embeddings improves over RoBERTa-large by 14 F1 across two\ndatasets. Finally, for zero-shot sentiment analysis, QUIP with question prompts improves over RoBERTa-large with MLM-style prompts by 5 accuracy points across three datasets, and extracts interpretable rationales as a side effect. Through ablations, we show that using real questions, a strong teacher model, and the bi-encoder architecture are all crucial to the success of QUIP. We will release code to reproduce all results upon publication.\n\n ",
    "start": 2640,
    "end": 2882,
    "label": "Unsupported_claim"
  },
  {
    "span": "Hardt et al. (2016) define equalized odds to measure discrimination. Kleinberg et al. (2017) prove impossibility results for calibration and fairness constraints. Preprocessing can learn fair representations (Zemel et al., 2013). Adversarial debiasing trains models against sensitive attributes (Zhang et al., 2018).",
    "document": "Related Work\n\nAlgorithmic fairness literature proposes criteria, learning objectives, and evaluation practices to mitigate disparate impact. We focus on group fairness in classification.\n\nHardt et al. (2016) define equalized odds to measure discrimination. Kleinberg et al. (2017) prove impossibility results for calibration and fairness constraints. Preprocessing can learn fair representations (Zemel et al., 2013). Adversarial debiasing trains models against sensitive attributes (Zhang et al., 2018).\n\nDespite extensive work, the interaction between calibration under distribution shift and enforcement of group metrics in online settings is not well understood.",
    "reason": "The paragraph lists definitions, impossibility results, and learning approaches without transitions or explaining how they relate to each other; coherence across the multiple sentences is lacking.",
    "start": 188,
    "end": 504,
    "label": "Coherence"
  },
  {
    "span": "Chen and Manning (2014",
    "document": "Introduction\n\nNeural dependency parsing has rapidly progressed with advancements in structured prediction and representation learning. Transition-based parsers benefit from compositional features and global training objectives (Goldberg and Nivre, 2013; Andor et al., 2016). As argued by Chen and Manning (2014 neural networks can replace manual feature engineering while maintaining linear-time decoding, inspiring subsequent architectures that exploit biaffine scoring and self-attention (Dozat and Manning, 2017; Kitaev and Klein, 2018). Cross-lingual transfer extends these gains to low-resource languages via multilingual embeddings and projection (Ammar et al., 2016; de Lhoneux et al., 2018).\n\nOpen challenges include handling non-projective structures and long-distance dependencies, where graph-based methods often excel (McDonald et al., 2005; Kiperwasser and Goldberg, 2016).",
    "reason": "Missing closing parenthesis in a narrative citation; should be 'Chen and Manning (2014)'.",
    "start": 288,
    "end": 310,
    "label": "Format"
  },
  {
    "span": "(Smith and Jones, 2017)",
    "document": "Related Work\n\nLearning to rank methods in information retrieval traditionally rely on pairwise and listwise losses (Burges, 2010; Cao et al., 2007). Neural ranking models integrate contextual encoders with interaction-focused architectures to capture fine-grained term dependencies. A comprehensive survey (Smith and Jones, 2017) contrasts representation- and interaction-based paradigms and highlights the role of pretraining in dense retrieval. More recent work explores retrieval-augmented generation for open-domain QA (Karpukhin et al., 2020; Lewis et al., 2021).\n",
    "reason": "Wrong citation style for parenthetical APA-like format; within parentheses, '&' should be used instead of 'and' (i.e., '(Smith & Jones, 2017)') in author-year style.",
    "start": 306,
    "end": 329,
    "label": "Format"
  },
  {
    "span": "(Chen et al., 2018;Kumar and Lee, 2020;",
    "document": "Introduction\n\nRecent advances in multimodal learning have demonstrated the importance of shared representations across text and vision. Many approaches leverage weak supervision (Chen et al., 2018;Kumar and Lee, 2020; to align features across domains, while others rely on explicit pairing signals to constrain the embedding space. However, these methods can be brittle when supervision is sparse and distributions shift between training and deployment.\n\nIn this work, we propose a contrastive pretraining strategy that emphasizes consistency under augmentation and robust alignment under weak semantic cues. Our approach builds on prior contrastive objectives (van den Oord et al., 2019) and adapts them for multimodal signals with minimal architectural assumptions.",
    "reason": "Parenthetical citation is missing the closing parenthesis and lacks a space after the semicolon between two citations.",
    "start": 178,
    "end": 217,
    "label": "Format"
  },
  {
    "span": "Nguyen et al.",
    "document": "Related Work\n\nLow-resource machine translation has leveraged transfer from high-resource languages via multilingual pretraining (Conneau and Lample, 2019; Aharoni et al., 2019) and vocabulary sharing strategies (Kudo, 2018). Data augmentation through back-translation remains a cornerstone for boosting performance when parallel data is scarce (Sennrich et al., 2016; Edunov et al., 2018).\n\nFollowing Nguyen et al., we adopt a curriculum that orders training examples by estimated difficulty, integrating monolingual signals to stabilize early training. Prior studies show that curriculum design can be complementary to noise-robust objectives (Wang et al., 2018; Zhang et al., 2021), but its interaction with multilingual encoders is underexplored. We close this gap by analyzing curriculum schedules across typologically diverse language pairs.",
    "reason": "Narrative citation missing year; should be 'Nguyen et al. (YEAR)'.",
    "start": 401,
    "end": 414,
    "label": "Format"
  },
  {
    "span": "Vaswani et al. (2017))",
    "document": "Introduction\n\nAttention mechanisms enable models to focus on relevant context and have become ubiquitous across modalities. The canonical Transformer architecture streamlines sequence transduction with parallel self-attention layers and residual connections. Transformer Vaswani et al. (2017)) sparked a wave of research into scaling laws and transfer (Kaplan et al., 2020; Brown et al., 2020). Subsequent variants improve efficiency with sparse patterns (Child et al., 2019) and linear attention (Katharopoulos et al., 2020). In this work, we analyze the trade-offs between capacity and context length for long-form generation.",
    "reason": "Extraneous closing parenthesis in a narrative citation; should be Vaswani et al. (2017) or Transformer (Vaswani et al., 2017).",
    "start": 271,
    "end": 293,
    "label": "Format"
  },
  {
    "span": "Approaches span enumerative search (Solar-Lezama, 2008), neural sequence-to-code models (Yin and Neubig, 2017), and neuro-symbolic methods with constraint solvers (Devlin et al., 2017; Nye et al., 2021).",
    "document": "Related Work\n\nProgram synthesis from natural language aims to translate task descriptions into executable code or domain-specific programs. The challenge lies in grounding ambiguous instructions to precise, verifiable semantics.\n\nApproaches span enumerative search (Solar-Lezama, 2008), neural sequence-to-code models (Yin and Neubig, 2017), and neuro-symbolic methods with constraint solvers (Devlin et al., 2017; Nye et al., 2021). Pretraining on code corpora and retrieval-augmented generation have improved in-context learning for synthesis tasks (Chen et al., 2021; Parvez et al., 2021). Verification-guided decoding uses test-time feedback to prune incorrect candidates (Le et al., 2015; Austin et al., 2021).\n\nWe present a spec-grounded planner that constructs intermediate structured intents to steer decoding toward semantically faithful programs.",
    "reason": "The span lists categories of methods and citations without integrating them into an argument about what is missing or how the current work builds upon them.",
    "start": 230,
    "end": 433,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent works show that audio features consistently outperform visual cues on in-the-wild benchmarks.",
    "document": "Introduction\n\nMultimodal sentiment analysis (MSA) seeks to infer opinions from synchronized text, audio, and visual streams (Poria et al., 2017; Tsai et al., 2019). Progress has been driven by representation learning across modalities and benchmarks such as CMU-MOSI and CMU-MOSEI (Zadeh et al., 2016; Zadeh et al., 2018). Despite advances in fusion, modality reliability varies across conditions such as studio versus in-the-wild recordings.\n\nRecent works show that audio features consistently outperform visual cues on in-the-wild benchmarks. However, the relative importance of modalities remains unclear when strong language models are used for the textual channel (Devlin et al., 2019; Liu et al., 2019). We revisit this question with a systematic evaluation that controls for backbone capacity and training data size.\n\nRelated Work\n\nEarly approaches relied on feature concatenation (Zadeh et al., 2017), while later methods adopted cross-modal attention (Tsai et al., 2019) and transformers (Sun et al., 2020). Robustness to covariate shift in MSA remains underexplored, particularly when modalities degrade at test time.",
    "reason": "Claims findings of 'recent works' without providing citations to the works, violating the requirement to cite prior studies.",
    "start": 444,
    "end": 544,
    "label": "Unsupported_claim"
  },
  {
    "span": "Formal verification proves safety properties under model assumptions (Bartos and Li, 2017). Coverage-guided fuzzing uncovers crashes through input mutation (Singhal et al., 2018). Symbolic execution explores program paths with constraint solvers (Ahmed and Noor, 2016).",
    "document": "Related Work\n\nSoftware security testing leverages both formal methods and heuristic search to find vulnerabilities. Recent advances combine static and dynamic analyses to increase coverage and reduce false positives.\n\nFormal verification proves safety properties under model assumptions (Bartos and Li, 2017). Coverage-guided fuzzing uncovers crashes through input mutation (Singhal et al., 2018). Symbolic execution explores program paths with constraint solvers (Ahmed and Noor, 2016). Hybrid frameworks attempt to blend these strengths (Reed et al., 2020), though their scalability on large codebases remains contested.\n\nWe focus on compositional fuzzing that reuses summaries of verified components to guide input generation for unverified modules.",
    "reason": "Three consecutive sentences describe distinct techniques with citations but lack transitions or explanation of their interrelations; the connections are implied rather than made explicit, harming coherence across multiple sentences.",
    "start": 218,
    "end": 487,
    "label": "Coherence"
  },
  {
    "span": "Smith et al.",
    "document": "Introduction\n\nActive learning aims to reduce labeling effort by querying informative samples (Settles, 2009). Pool-based strategies often rely on uncertainty estimation with modern neural architectures (Gal and Ghahramani, 2016; Lewis and Gale, 1994). Smith et al. demonstrate that selection bias can degrade downstream generalization in sequence tagging tasks, motivating debiasing methods (Sener and Savarese, 2018). We revisit this problem in cross-domain settings where annotation budgets are constrained and class priors shift substantially.",
    "reason": "Narrative citation missing the year. It should include the year in parentheses: \"Smith et al. (YEAR)\".",
    "start": 252,
    "end": 264,
    "label": "Format"
  },
  {
    "span": "Classical statistical models and deep autoencoders, VAEs, and LSTMs have been used for time-series anomaly detection (Box and Jenkins, 1976; Malhotra et al., 2016; An and Cho, 2015; Hundman et al., 2018).",
    "document": "Introduction\n\nTime-series anomaly detection. Detecting rare deviations in sensor data is critical for reliability in industrial and cloud systems. Classical statistical models and deep autoencoders, VAEs, and LSTMs have been used for time-series anomaly detection (Box and Jenkins, 1976; Malhotra et al., 2016; An and Cho, 2015; Hundman et al., 2018). More recent transformer-based methods model long-range dependencies (Zhou et al., 2021; Wu et al., 2021).\n\nOur challenge. We consider rapidly shifting regimes and limited anomaly labels, where fixed thresholds and static embeddings struggle.\n\nOur contributions. We propose an adaptive contrastive detector with online calibration that maintains precision-recall under distribution shifts.",
    "reason": "The span is a broad citation list of prior methods without explaining their assumptions or how they fall short for the paper’s focus, so it lacks synthesis.",
    "start": 147,
    "end": 351,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Patel et al. 2",
    "document": "Related Work\n\nActive learning strategies for sequence labeling prioritize uncertain tokens or informative spans to reduce annotation cost (Settles, 2010; Shen et al., 2018). Batch selection under representation shift has been studied with diversity-promoting objectives (Ash et al., 2019; Coleman et al., 2020). In domain adaptation for NER, Patel et al. 2 showed that annotator disagreement can be leveraged as a signal, but their method requires dense confidence estimates at the token level. We instead propose a calibration-aware query function that balances uncertainty and coverage (Gal and Ghahramani, 2016).",
    "reason": "Improper use of a footnote-like number; should include year or be formatted as a proper footnote/citation.",
    "start": 342,
    "end": 356,
    "label": "Format"
  },
  {
    "span": "Causal representation learning studies disentanglement and identifiability under various assumptions (Locatello et al., 2019; Schölkopf et al., 2021; Khemakhem et al., 2020; Hyvärinen et al., 2019; Lachapelle et al., 2022). We study identifiability under interventions.",
    "document": "Introduction\n\nLearning structured latent variables that reflect causal factors of variation can improve robustness and sample efficiency. A central question is when such latent factors are identifiable from observations.\n\nCausal representation learning studies disentanglement and identifiability under various assumptions (Locatello et al., 2019; Schölkopf et al., 2021; Khemakhem et al., 2020; Hyvärinen et al., 2019; Lachapelle et al., 2022). We study identifiability under interventions.\n\nOur analysis focuses on finite interventions with sparse targets and provides sufficient conditions for recovering latent modules. We also give constructive algorithms and evaluate on synthetic benchmarks.\n",
    "reason": "Violates (b): after summarizing prior literature, it states the paper's focus without clarifying what gap remains or how existing assumptions are insufficient, thus lacking synthesis.",
    "start": 222,
    "end": 491,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Bose, 2021,; Malik et al., 2022)",
    "document": "Related Work\n\nSelf-training has been used to exploit large pools of unlabeled data in low-resource settings. Recent semi-supervised systems (Bose, 2021,; Malik et al., 2022) achieve strong results by filtering pseudo-labels with uncertainty estimates (Xie et al., 2020) and consistency regularization (Sohn et al., 2020). Complementary approaches leverage teacher–student frameworks (Tarvainen and Valpola, 2017) and data augmentation in latent space (Chen et al., 2020). However, stability depends heavily on calibration and class imbalance.",
    "reason": "Erroneous punctuation ',;' inside the citation list; should separate citations with a semicolon only.",
    "start": 140,
    "end": 173,
    "label": "Format"
  },
  {
    "span": "Statistical downscaling maps coarse fields to local scales using learned transfer functions (Oliveira and Park, 2017). Data assimilation fuses models with observations to reduce forecast error (Gonzalez et al., 2016). Parameterization schemes approximate unresolved processes in global models (Huang and Peters, 2015).",
    "document": "Related Work\n\nBridging scales in climate modeling requires complementary techniques that address structural bias, observational gaps, and computational limits. Methods range from empirical mappings to physics-aware machine learning.\n\nStatistical downscaling maps coarse fields to local scales using learned transfer functions (Oliveira and Park, 2017). Data assimilation fuses models with observations to reduce forecast error (Gonzalez et al., 2016). Parameterization schemes approximate unresolved processes in global models (Huang and Peters, 2015). Hybrid models embed physical constraints within data-driven surrogates (Kaur and Malik, 2020). Despite progress, consistent evaluation under nonstationary forcings remains challenging.\n\nWe propose a benchmark that couples scenario-conditioned downscaling with temporal covariate shifts to evaluate robustness of learned surrogates.",
    "reason": "Three consecutive sentences cite disparate subfields (downscaling, assimilation, parameterization) without explaining their connections or providing transitions; the relationships are only implied, causing coherence issues across multiple sentences.",
    "start": 234,
    "end": 552,
    "label": "Coherence"
  },
  {
    "span": "prior work has shown that gradient inversion can exactly reconstruct training examples",
    "document": "Related Work\n\nPrivacy and Security in Federated Learning\n\nFederated learning enables on-device training with centralized aggregation, reducing the need to collect raw user data. However, recent studies have demonstrated leakage channels through gradients and model updates that can reveal sensitive information about training records.\n\nIn particular, prior work has shown that gradient inversion can exactly reconstruct training examples, highlighting severe privacy risks under realistic threat models. Complementary defenses propose gradient clipping, noise injection, and secure aggregation to reduce attack efficacy while preserving utility.\n\nWe extend this line by introducing a certified defense that bounds reconstruction risk at the example level without prohibitive degradation in model accuracy.",
    "reason": "Claims a concrete, strong result attributed to prior work (“exactly reconstruct training examples”) without citing the studies that demonstrate it.",
    "start": 351,
    "end": 437,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT has been used in AES trained on the ASAP dataset with a pairwise ranking loss.",
    "document": "Related Work: Automated Essay Scoring\n\nAutomated Essay Scoring (AES) systems map free-form writing to holistic or trait-specific scores under constrained prompts. Early approaches used hand-engineered features and regression, while neural models improved robustness by learning hierarchical representations of discourse and syntax. BERT has been used in AES trained on the ASAP dataset with a pairwise ranking loss. Subsequent studies have explored prompt-agnostic transfer and domain adaptation to new writing genres, but the scarcity of long-context supervision and the risk of prompt leakage limit generalization. Recent transformer variants that support longer input windows have enabled full-essay modeling without aggressive truncation, yet calibration across prompts remains challenging.\n\nOur work differs by introducing a contrastive objective tailored to rubric descriptors and by decoupling content relevance from writing quality signals. We also analyze cross-prompt fairness and sensitivity to adversarial paraphrase, providing a thorough error taxonomy.",
    "reason": "Specific claim about the use of BERT, a particular dataset (ASAP), and a training setup (pairwise ranking loss) requires a citation to the study that introduced this configuration.",
    "start": 332,
    "end": 415,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Chen et al., 2020)",
    "document": "Introduction\n\nModel calibration has emerged as a key factor in safe deployment, with temperature scaling proving effective for post-hoc adjustment (Guo et al., 2017). According to (Chen et al., 2020), combining confidence estimation with selective prediction yields better risk control under dataset shift. We extend this line by integrating conformal risk control with active data collection (Angelopoulos et al., 2021; Romano et al., 2020).\n",
    "reason": "Wrong citation style in narrative text: after \"According to\", the citation should be narrative (\"According to Chen et al. (2020)\") rather than parenthetical.",
    "start": 180,
    "end": 199,
    "label": "Format"
  },
  {
    "span": "Garcia et al. 1",
    "document": "Related Work\n\nMulti-task learning shares representations across tasks to improve generalization (Caruana, 1997; Ruder, 2017). Parameter-efficient adapters allow specialization without full fine-tuning (Houlsby et al., 2019; Pfeiffer et al., 2021). Recent methods route examples dynamically to experts to balance capacity and interference (Shazeer et al., 2017; Fedus et al., 2022).\n\nHard parameter sharing can induce negative transfer when tasks conflict (Ruder, 2017; Bingel and Søgaard, 2017). To address this, soft-sharing via regularization and task-specific heads has been explored (Duong et al., 2015; Standley et al., 2020). Garcia et al. 1 propose uncertainty-guided weighting, but their evaluation focuses on computer vision benchmarks and limited NLP tasks.\n\nWe instead introduce a gradient surgery approach that enforces task agreement at the representation level, demonstrating robust gains across classification and sequence labeling.",
    "reason": "Improper footnote-like usage without year; should be a proper author–year citation or a formatted footnote, e.g., “Garcia et al. (2020)” or a superscripted footnote.",
    "start": 632,
    "end": 647,
    "label": "Format"
  },
  {
    "span": "Existing KGC evaluations typically report MRR@10 exclusively.",
    "document": "Related Work\n\nKnowledge graph completion (KGC) predicts missing links by scoring candidate triples (h, r, t) [1]. Embedding-based methods such as translational and bilinear models remain popular for large graphs [2]. Existing KGC evaluations typically report MRR@10 exclusively. While convenient, this practice obscures performance on the full ranking and may misrepresent gains across relation types.",
    "reason": "Generalization about evaluation practices in prior work without citations to surveys or benchmark papers.",
    "start": 217,
    "end": 278,
    "label": "Unsupported_claim"
  },
  {
    "span": "Knowledge tracing models estimate latent mastery over time (Wang and Collins, 2016). Automated feedback generation can scaffold problem solving (Ibrahim et al., 2019). Fairness-aware grading addresses demographic disparities (Ortiz and Vega, 2021).",
    "document": "Related Work\n\nIntelligent tutoring systems (ITS) combine student modeling with instructional policies to personalize practice. Prior art separates progress estimation from intervention design, often evaluating on narrow task families.\n\nKnowledge tracing models estimate latent mastery over time (Wang and Collins, 2016). Automated feedback generation can scaffold problem solving (Ibrahim et al., 2019). Fairness-aware grading addresses demographic disparities (Ortiz and Vega, 2021). Beyond these strands, policy learning has focused on bandit feedback (Chen and Foster, 2020), but cross-domain generalization remains unresolved.\n\nWe investigate how mastery estimates calibrated with uncertainty inform adaptive hinting under fairness constraints.",
    "reason": "The three sentences reference distinct areas without connecting them or using transitions; the relevance of fairness-aware grading to knowledge tracing and feedback is not made explicit, reducing coherence across multiple sentences.",
    "start": 236,
    "end": 484,
    "label": "Coherence"
  },
  {
    "span": "In (Smith et al., 2020)",
    "document": "Introduction\n\nActive learning aims to reduce annotation cost by selecting the most informative instances (Settles, 2009; Lewis and Gale, 1994). Modern pool-based strategies integrate uncertainty, diversity, and representativeness to balance exploration and exploitation (Sener and Savarese, 2018; Ash et al., 2020).\n\nIn (Smith et al., 2020), the authors propose combining gradient embeddings with core-set selection to stabilize performance across acquisition rounds. Follow-up work adapts this idea to pretrained Transformers (Ein-Dor et al., 2020) and vision-language models (Yuan et al., 2021). We position our method within this line by introducing a calibration-aware objective that complements BADGE (Ash et al., 2020) under distribution shift.\n",
    "reason": "Wrong citation style: preposition \"In\" should not precede a parenthetical citation; it should be narrative as \"In Smith et al. (2020)\".",
    "start": 317,
    "end": 340,
    "label": "Format"
  },
  {
    "span": "Vision-language pretraining has rapidly progressed with models like CLIP and ALIGN that learn from web-scale image–text pairs.",
    "document": "Related Work\n\nMultimodal learning methods align visual and textual signals to enable zero-shot transfer, retrieval, and captioning tasks (Anderson et al., 2018; Lu et al., 2019). Scaling both data and model capacity has been a key driver for improvements in open-vocabulary recognition and cross-modal retrieval. Vision-language pretraining has rapidly progressed with models like CLIP and ALIGN that learn from web-scale image–text pairs. While these approaches demonstrate remarkable zero-shot capabilities, downstream adaptation remains sensitive to domain shift and prompt design. Our work studies sample-efficient adapters that stabilize finetuning without sacrificing zero-shot performance.",
    "reason": "Cites no sources when naming specific prior models (CLIP, ALIGN) and claiming rapid progress; first mentions of such works require citations.",
    "start": 313,
    "end": 439,
    "label": "Unsupported_claim"
  },
  {
    "span": "WMT19 News Translation shared task",
    "document": "Introduction\n\nData augmentation remains a cornerstone of neural machine translation (NMT), with synthetic parallel data from back-translation often leading to substantial quality improvements (Sennrich et al., 2016; Edunov et al., 2018). In the low-resource regime, iterative training with mined pseudo-parallel corpora and domain-adaptive pretraining can compensate for scarce supervision (Lample et al., 2018; Conneau and Lample, 2019). We follow evaluation protocols from the WMT19 News Translation shared task to ensure comparability across language pairs and test sets, focusing on constrained conditions.\n\nWhile prior studies emphasize the benefits of scaling model size and synthetic data quantity, our analysis reveals diminishing returns beyond a threshold determined by the monolingual data domain. We provide ablations over back-translation quality, sampling temperature, and filtering strategies, and we discuss practical trade-offs in computational cost vs. BLEU/COMET gains.",
    "reason": "First mention of a shared task must include a supporting citation to the task or official overview, per rule (a); none is provided.",
    "start": 479,
    "end": 513,
    "label": "Unsupported_claim"
  },
  {
    "span": "(2021, Patel et al.)",
    "document": "Related Work\n\nProbabilistic programming systems offer composable inference (Goodman and Stuhlmüller, 2014; Gordon et al., 2014). Advances in black-box variational inference (Ranganath et al., 2014; Kingma and Welling, 2014) and Monte Carlo gradients (Paisley et al., 2012) enable scalability. New runtimes compile probabilistic code to accelerators (Bingham et al., 2019). An overview can be found in (2021, Patel et al.), along with benchmarks contrasting MCMC and VI. Our contribution is a typed abstraction for effect handlers (Heunen et al., 2018).",
    "reason": "Year is placed before authors; should be '(Patel et al., 2021)'.",
    "start": 401,
    "end": 421,
    "label": "Format"
  },
  {
    "span": "Autoencoders reconstruct normal patterns to flag deviations (Zong et al., 2018). Matrix profile detects discords in large series (Yeh et al., 2016). Prophet decomposes trends and seasonality for forecasting (Taylor and Letham, 2018). Change point detection locates regime shifts using Bayesian models (Fearnhead, 2006).",
    "document": "Introduction\n\nTime-Series Anomaly Detection\n\nDetecting anomalies in time series is critical in monitoring, finance, and industrial IoT. Methods range from reconstruction-based deep models to statistical techniques and distance profiles. Despite progress, generalization across domains remains challenging due to varying seasonalities and noise levels.\n\nApproaches and Tools\n\nAutoencoders reconstruct normal patterns to flag deviations (Zong et al., 2018). Matrix profile detects discords in large series (Yeh et al., 2016). Prophet decomposes trends and seasonality for forecasting (Taylor and Letham, 2018). Change point detection locates regime shifts using Bayesian models (Fearnhead, 2006).\n\nOur Contribution\n\nWe introduce a unified probabilistic framework that integrates seasonal decomposition with representation learning and provides calibrated anomaly scores with uncertainty estimates.",
    "reason": "The span lists four methods spanning deep learning, distance-based profiles, forecasting, and change point detection with no transitions or explicit linkage. The abrupt inclusion of a forecasting tool (Prophet) among anomaly detectors is not explained, leading to unclear relationships between sentences.",
    "start": 375,
    "end": 694,
    "label": "Coherence"
  },
  {
    "span": "Zhao (2017]",
    "document": "Related Work\n\nTime Series Forecasting with Probabilistic Models\n\nClassical statistical models such as ARIMA and ETS remain strong baselines for short-horizon forecasting (Hyndman and Athanasopoulos, 2018). Deep models capture nonlinearities and seasonality with greater capacity (Salinas et al., 2020; Oreshkin et al., 2019). Zhao (2017] introduced a hybrid residual architecture to combine local autoregressive patterns with global neural trends, while subsequent work proposed decomposition-based Transformers for long sequences (Wu et al., 2021; Zeng et al., 2023). Uncertainty quantification has been addressed via quantile losses and Bayesian formulations (Lakshminarayanan et al., 2017; Salinas et al., 2019).",
    "reason": "Mismatched delimiter around year in narrative citation: closing ']' instead of ')'.",
    "start": 326,
    "end": 337,
    "label": "Format"
  },
  {
    "span": "Garcia et al.",
    "document": "Related Work\n\nUnsupervised domain adaptation aligns feature distributions between source and target domains to enable transfer without labels. Adversarial learning has been widely adopted to minimize a discrepancy between latent distributions by confusing a domain discriminator. As Garcia et al. demonstrate, adversarial alignment alone may fail when class-conditional distributions remain mismatched, prompting conditional or class-aware objectives (Saito et al., 2018; Zhang et al., 2019). Alternative strategies regularize models via consistency across augmentations or pseudo-labeling (French et al., 2018; Berthelot et al., 2019), though these often require careful thresholding to avoid confirmation bias.\n\nOur approach combines conditional alignment with curriculum pseudo-labeling to progressively refine class boundaries in the target domain.",
    "reason": "Narrative citation is missing the publication year; it should appear as 'Garcia et al. (YEAR)'.",
    "start": 283,
    "end": 296,
    "label": "Format"
  },
  {
    "span": "(Smith et al., 2021))",
    "document": "Related Work\n\nNeural abstractive summarization has advanced rapidly with sequence-to-sequence pretraining, enabling strong performance with limited task-specific data (Lewis et al., 2020; Zhang et al., 2020). Early approaches relied on pointer-generator networks to balance copying and generation (See et al., 2017), while later work introduced reinforcement learning to optimize sequence-level metrics (Paulus et al., 2018). Recent extensions (Smith et al., 2021)) explore factuality constraints via entity-aware decoding, complementing methods that leverage external knowledge bases for grounding (Cao et al., 2018; Dong et al., 2020). We study contrastive training objectives that penalize faithfulness errors by constructing minimal counterfactuals, improving both ROUGE and factual consistency.",
    "reason": "Extra closing parenthesis in the citation.",
    "start": 444,
    "end": 465,
    "label": "Format"
  },
  {
    "span": "Schuller et al. (2009) used spectral and prosodic features for emotion recognition. Busso et al. (2008) created the IEMOCAP corpus. Tzirakis et al. (2017) proposed end-to-end deep models from raw audio.",
    "document": "Related Work\n\nSpeech emotion recognition (SER) has evolved from classical feature engineering to end-to-end deep architectures. Progress has been driven by robust features, curated corpora, and advances in representation learning.\n\nSchuller et al. (2009) used spectral and prosodic features for emotion recognition. Busso et al. (2008) created the IEMOCAP corpus. Tzirakis et al. (2017) proposed end-to-end deep models from raw audio. Multimodal extensions incorporate facial cues and physiological signals (Zadeh et al., 2016; Tripathi et al., 2018).\n\nClass imbalance and cross-corpus generalization remain open challenges addressed by reweighting and domain adversarial training (Zhang et al., 2018; Gideon et al., 2019).",
    "reason": "A dataset citation is sandwiched between two methodological advances without any explicit statement of its role in those methods, causing an abrupt and unclear connection.",
    "start": 232,
    "end": 434,
    "label": "Coherence"
  },
  {
    "span": "(Garcia and Lin, 2016)",
    "document": "Related Work. Vision Transformers (ViTs) reframe image recognition as sequence modeling over patch embeddings (Dosovitskiy et al., 2021). Data-efficient variants use distillation and strong augmentations to reduce pretraining requirements (Touvron et al., 2021). Hybrid CNN–Transformer models retain local inductive biases while gaining global context (Graham et al., 2021). In detection and segmentation, hierarchical ViTs improve multi-scale reasoning (Swin; Liu et al., 2021). For robustness, adversarial training and stochastic depth have been explored (Xie et al., 2020; Huang et al., 2016). Prior work (Garcia and Lin, 2016) studies regularization for attention modules, which we revisit in large-scale settings with stronger data augmentation.",
    "reason": "Wrong conjunction in parenthetical citation for APA style: within parentheses, 'and' should be '&'; should be '(Garcia & Lin, 2016)'.",
    "start": 608,
    "end": 630,
    "label": "Format"
  },
  {
    "span": "It is well-known that demographic parity is unsuitable for credit scoring",
    "document": "Related Work\n\nFairness in lending requires balancing predictive performance with regulatory and ethical considerations. Multiple formal criteria exist, including demographic parity, equalized odds, and calibration within groups.\n\nIt is well-known that demographic parity is unsuitable for credit scoring. As a result, practitioners often favor constraints that align with risk-based pricing and default prediction.\n\nWe build on this literature by proposing a constrained optimization framework that trades off group-wise calibration error against expected profit, enabling transparent policy selection under operational constraints.\n",
    "reason": "Asserts domain-specific consensus ('well-known') without citation; per rule (b), such claims in a specialized area must be referenced.",
    "start": 230,
    "end": 303,
    "label": "Unsupported_claim"
  },
  {
    "span": "[23]",
    "document": "Related Work\n\nEvent extraction from text has advanced from pipeline systems to end-to-end neural models (Nguyen et al., 2016; Wadden et al., 2019). Span-based formulations capture flexible argument structures and improve cross-domain generalization (Zhang et al., 2019; Lin et al., 2020). As shown in [23], pretraining on weakly annotated corpora can further boost argument role coverage. Distant supervision aligns triggers with knowledge bases but introduces label noise, which recent approaches mitigate through denoising objectives and instance selection (Zeng et al., 2015; Liu et al., 2017; Chen et al., 2021).\n\nCross-lingual transfer leverages multilingual encoders and projection techniques to handle low-resource languages (Ahmad et al., 2021; Subburathinam et al., 2019; Conneau et al., 2020). However, typological differences and domain mismatch still cause substantial performance drops.",
    "reason": "Wrong citation style: numeric bracket '[23]' used within an author–year citation context; should be an author–year citation (e.g., 'as shown in Zhang et al. (2020)').",
    "start": 301,
    "end": 305,
    "label": "Format"
  },
  {
    "span": "Recently, several studies have explored prompting techniques with pre-trained language models to steer model outputs or elicit latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Lester et al., 2021; Wei et al., 2022).",
    "document": "Related Work\n\nPrompting and instruction following. Prompt-based learning has emerged as a powerful paradigm for adapting large language models without extensive task-specific fine-tuning. Discrete prompts guide models with natural-language patterns, while soft prompts introduce trainable embeddings in the input space. Recently, several studies have explored prompting techniques with pre-trained language models to steer model outputs or elicit latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Lester et al., 2021; Wei et al., 2022). Parallel lines of work investigate instruction tuning, where models are fine-tuned on collections of tasks written as instructions to improve generalization to unseen tasks.\n\nMultilingual and cross-lingual transfer. Cross-lingual transfer with multilingual encoders has been studied for classification, sequence labeling, and generation. Prior work considers aligning representations across languages using joint subword vocabularies, parallel data, or constraints derived from dictionaries and typology (Conneau et al., 2020; Hu et al., 2020). For sequence generation, recent efforts examine multilingual pre-training objectives that encourage sharing across languages (Xue et al., 2021).\n\nCalibration and control. Another thread focuses on controlling model behavior through calibrated decoding, verbalizer selection, and consistency regularization (Zhao et al., 2021; Holtzman et al., 2020). These methods aim to reduce prompt sensitivity and improve stability across tasks and domains.\n",
    "reason": "The sentence lists prior prompting works without explaining their relevance to the paper, the specific gap in the literature, or how the current study builds on or differs from them (criteria a and c).",
    "start": 320,
    "end": 560,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Message-passing neural networks aggregate neighbor information to compute node embeddings (Gilmer et al., 2017). Spectral methods leverage graph Laplacian properties (Defferrard et al., 2016). Positional encodings enhance structural expressivity (Dwivedi et al., 2021).",
    "document": "Related Work\n\nGraph representation learning methods differ in how they encode topology and attribute interactions. Recent efforts aim to overcome limitations in distinguishing non-isomorphic structures while maintaining scalability.\n\nMessage-passing neural networks aggregate neighbor information to compute node embeddings (Gilmer et al., 2017). Spectral methods leverage graph Laplacian properties (Defferrard et al., 2016). Positional encodings enhance structural expressivity (Dwivedi et al., 2021). Subgraph- and motif-based architectures further improve expressivity beyond 1-WL tests (Bouritsas et al., 2022). Despite these advances, the interplay between positional encodings and graph sparsity remains underexplored.\n\nOur work characterizes when positional features complement message passing and introduces a sparsity-aware encoding that improves OOD generalization.",
    "reason": "Three distinct families of GNN techniques are listed consecutively without explanation of their relationships or transitions. The relevance of each cited work to the previous sentence is not stated, creating a coherence problem.",
    "start": 234,
    "end": 503,
    "label": "Coherence"
  },
  {
    "span": "Reinforcement learning from human feedback optimizes a reward model to prefer helpful outputs (Ziegler et al., 2019; Ouyang et al., 2022). Detoxification datasets contain human-annotated toxicity spans (Smith et al., 2021). Constitutional AI replaces humans with a rule set during preference optimization (Bai et al., 2022).",
    "document": "Related Work\n\nAligning Large Language Models\n\nAlignment methods aim to steer large language models (LLMs) toward helpful, honest, and harmless behavior. Post-training with preference data and policy optimization has emerged as a dominant paradigm (Stiennon et al., 2020; Ouyang et al., 2022). Data curation and safety filtering reduce exposure to undesirable content (Gehman et al., 2020; Welbl et al., 2021). Reinforcement learning from human feedback optimizes a reward model to prefer helpful outputs (Ziegler et al., 2019; Ouyang et al., 2022). Detoxification datasets contain human-annotated toxicity spans (Smith et al., 2021). Constitutional AI replaces humans with a rule set during preference optimization (Bai et al., 2022). Alternatives to RLHF include direct preference optimization and rejection sampling fine-tuning (Rafailov et al., 2023; Bai et al., 2022).",
    "reason": "The span juxtaposes RLHF, toxicity datasets, and Constitutional AI without clarifying how the dataset relates to the training strategies or why these three points are presented together.",
    "start": 410,
    "end": 734,
    "label": "Coherence"
  },
  {
    "span": "Müller, 2017",
    "document": "Related Work\n\nModel-based reinforcement learning (MBRL) aims to leverage learned dynamics for sample-efficient policy improvement (Deisenroth and Rasmussen, 2011; Janner et al., 2019). Uncertainty-aware models are crucial to mitigate compounding errors during rollouts (Chua et al., 2018; Kurutach et al., 2018). Recent studies integrate latent variable models with planning to balance exploration and exploitation (Hafner et al., 2020; Schrittwieser et al., 2020). For a comprehensive taxonomy of model classes, see Müller, 2017 for details. We address the challenge of multi-step rollout bias by proposing a consistency-regularized dynamics prior, extending findings in (Lambert et al., 2020).",
    "reason": "Citation appears as author, year without parentheses in running text; it should be Müller (2017) in narrative form or (Müller, 2017) as a parenthetical citation.",
    "start": 517,
    "end": 529,
    "label": "Format"
  },
  {
    "span": "Klein et al. 2",
    "document": "Introduction\n\nNeural ranking models have advanced retrieval quality through deep interaction architectures (Santos and Zhang, 2018; Ortega et al., 2020). A classic taxonomy of matching signals was proposed by Klein et al. 2 and has influenced the design of many subsequent models. Later studies refined this view by separating exact and soft matching via kernel pooling (Gao and Xu, 2019) and improved training by using listwise losses (Huang, 2021).\n\nRelated Work\n\nPretraining on weakly supervised clicks boosts ad-hoc retrieval performance (Diaz et al., 2022), while knowledge distillation compresses rankers for efficiency (Park and Cho, 2020). Our work revisits the taxonomy to measure which signals transfer under domain shift.",
    "reason": "Wrong use of a footnote/dangling number in place of a year. Should be a proper citation like 'Klein et al. (2017)' or a correctly formatted footnote.",
    "start": 209,
    "end": 223,
    "label": "Format"
  },
  {
    "span": "Graves et al. (2014) presented end-to-end CTC models. Chan et al. (2016) proposed Listen, Attend and Spell. Park et al. (2019) introduced SpecAugment. Gulati et al. (2020) developed Conformer.",
    "document": "Introduction\n\nAutomatic speech recognition (ASR) has moved from hybrid HMM-DNN systems to end-to-end models that jointly learn acoustic and language representations. Advances include attention mechanisms, data augmentation, and architectures tailored to long-range dependencies.\n\nGraves et al. (2014) presented end-to-end CTC models. Chan et al. (2016) proposed Listen, Attend and Spell. Park et al. (2019) introduced SpecAugment. Gulati et al. (2020) developed Conformer.\n\nWe investigate a streaming-friendly architecture that combines lightweight attention with masked augmentation to improve robustness under latency constraints.",
    "reason": "The four sentences list contributions without any connective phrases or explanation, leaving the relationships among CTC, attention-based models, augmentation, and architecture design implicit.",
    "start": 280,
    "end": 472,
    "label": "Coherence"
  },
  {
    "span": "Pearl (2009) articulated causal graphical models and do-calculus. Rubin (1974) formalized potential outcomes for causal inference. Arjovsky et al. (2020) proposed invariant risk minimization to capture stable predictors across environments. Kusner et al. (2017) introduced counterfactual fairness for decision-making systems.",
    "document": "Related Work\n\nCausal inference for machine learning aims to move beyond associations toward actionable interventions and robust generalization. Approaches include graphical models, counterfactual reasoning, and invariance principles under environment shifts.\n\nPearl (2009) articulated causal graphical models and do-calculus. Rubin (1974) formalized potential outcomes for causal inference. Arjovsky et al. (2020) proposed invariant risk minimization to capture stable predictors across environments. Kusner et al. (2017) introduced counterfactual fairness for decision-making systems.\n\nSubsequent research explores representation learning that respects causal structure and enables counterfactual queries (Schölkopf et al., 2021; Pawlowski et al., 2020). Our contribution focuses on environment-invariant representations with identifiability guarantees under weak supervision.",
    "reason": "Although all citations are about causality, the text gives no transitions showing how they relate or evolve. It is unclear how IRM or counterfactual fairness connect to the two foundational frameworks introduced just prior.",
    "start": 260,
    "end": 585,
    "label": "Coherence"
  },
  {
    "span": "Many recent works propose hybrid neural–symbolic models that combine execution with pre-trained transformers.",
    "document": "Related Work\n\nQuestion answering over tables and databases has evolved from semantic parsers with hand-crafted grammars to neural models that generate executable programs. Pre-trained language models have improved schema linking and contextual reasoning by leveraging large-scale text corpora.\n\nMany recent works propose hybrid neural–symbolic models that combine execution with pre-trained transformers. These approaches vary in how they supervise intermediate programs and handle execution errors, but a consistent challenge is ensuring compositional generalization across schemas.\n\nOur approach integrates constrained decoding with schema-augmented representations, aiming to reduce spurious programs while maintaining end-to-end differentiability.",
    "reason": "Invokes 'many recent works' without providing any citations to back up the statement (rule d).",
    "start": 295,
    "end": 404,
    "label": "Unsupported_claim"
  },
  {
    "span": "It is well known in educational measurement that essay length is the strongest predictor of holistic score.",
    "document": "Related Work\n\nPredictive factors in writing assessment have been investigated through both linguistic feature analysis and rater behavior studies. Features such as lexical diversity, syntactic complexity, discourse coherence, and topical relevance have each been correlated with holistic scores to varying degrees.\n\nIt is well known in educational measurement that essay length is the strongest predictor of holistic score. While length correlates with score in many datasets, its prominence raises concerns about construct validity and the potential for superficial heuristics to dominate predictions. Recent AES systems therefore incorporate length normalization and content relevance checks to mitigate gaming.\n\nOur study quantifies the marginal contribution of length relative to discourse and content features under cross-prompt transfer, and proposes a rubric-aligned regularizer that penalizes over-reliance on superficial cues.",
    "reason": "This sweeping claim about a field-specific finding requires citations to empirical studies supporting it (violates rule b).",
    "start": 316,
    "end": 423,
    "label": "Unsupported_claim"
  },
  {
    "span": "there have been many recent works that explore this topic",
    "document": "Introduction\n\nTask-oriented dialogue (TOD) systems aim to assist users in completing goals such as booking flights or finding restaurants by tracking dialogue state and issuing API calls (Young et al., 2013; Williams et al., 2014). Early approaches relied on hand-crafted ontologies and generative belief trackers (Mrkšić et al., 2017; Ramadan et al., 2018). With the advent of large-scale pretraining, neural state trackers have improved robustness and reduced annotation requirements (Rastogi et al., 2020; Hosseini-Asl et al., 2020). In the last three years, there have been many recent works that explore this topic, proposing schema-guided trackers and pretraining-based methods. However, the interplay between schema variability and slot value normalization remains underexplored. We address this gap by introducing a retrieval-augmented state tracker that conditions on dynamically induced slot schemas derived from API documentation.",
    "reason": "The phrase claims the existence of many recent works but provides no citations to support it, violating the requirement to cite when mentioning recent literature.",
    "start": 562,
    "end": 619,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Gupta et al., 2019)",
    "document": "Related Work\n\nFederated learning reduces privacy risks by training on-device (Kairouz et al., 2021). In (Gupta et al., 2019) we adopt proximal regularization to mitigate client drift, whereas others consider personalization layers (Arivazhagan et al., 2019) and meta-learning (Fallah et al., 2020). Our study compares optimization stability with non-IID partitions (Zhao et al., 2018) and analyzes communication efficiency under heterogeneous client participation.",
    "reason": "Wrong citation style after a preposition; should be 'in Gupta et al. (2019)'.",
    "start": 101,
    "end": 124,
    "label": "Format"
  },
  {
    "span": "the widely used MIND dataset",
    "document": "Introduction\n\nPersonalized news recommendation requires modeling user interests from both short-term and long-term behaviors. Transformer-based user encoders and semantic item representations have improved performance by capturing contextualized signals from titles and entities. Nevertheless, challenges persist in handling cold-start items and ensuring diversity in recommendations.\n\nTo facilitate a fair comparison with existing systems, we evaluate on the widely used MIND dataset and report results under the same split and metrics as prior work. We also include ablations on negative sampling strategies and embedding sharing to assess their impact on generalization.\n\nOur approach combines hierarchical attention over sessions with a disentangled representation of topical and stylistic factors, enabling better generalization to unseen news items.",
    "reason": "First mention of a specific dataset lacks a citation to its source or introduction (definition a).",
    "start": 456,
    "end": 484,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith et al.",
    "document": "Introduction\n\nSemi-supervised text classification leverages unlabeled corpora to improve generalization when labeled data is scarce (Zhang et al., 2020; Miyato et al., 2017). Recent advances in pretraining have further boosted performance on low-resource settings (Devlin et al., 2019; Liu et al., 2019). As shown by Smith et al., transformer encoders benefit from large-scale pseudo-labeling and consistency regularization, especially when domain shift is moderate. We build on this line of work and investigate curriculum-aware augmentation strategies (Xie et al., 2020) for noisy web data.\n\nOur contributions are threefold: (1) we propose a confidence-calibrated schedule for label refinement; (2) we introduce a noise-robust consistency loss; and (3) we present a comprehensive analysis across domains.\n\nWe evaluate on five benchmarks spanning sentiment, topic, and news categorization (McAuley and Leskovec, 2013; Zhang et al., 2015), demonstrating strong gains over competitive baselines.",
    "reason": "Narrative citation missing year; should be formatted as “Smith et al. (YEAR)” or converted to a parenthetical citation.",
    "start": 317,
    "end": 329,
    "label": "Format"
  },
  {
    "span": "Graph neural networks for molecular property prediction have been studied extensively, including message-passing architectures (Gilmer et al., 2017; Kearnes et al., 2016), graph attention and transformer variants (Veličković et al., 2018; Ying et al., 2021; Rong et al., 2020), spectral and spatial methods (Defferrard et al., 2016; Kipf and Welling, 2017), and equivariant models tailored to 3D geometry (Schütt et al., 2018; Anderson et al., 2019; Satorras et al., 2021). Recent benchmarks compare pooling and readout strategies (Xu et al., 2019; Li et al., 2020) and explore uncertainty estimation for molecular graphs (Gustafsson et al., 2020; Hirschfeld et al., 2020).",
    "document": "Introduction\n\nPredicting molecular properties from structure is fundamental in cheminformatics and drug discovery. Recent advances in geometric deep learning have brought graph-based methods to the forefront due to their ability to encode local and global structural signals in molecules.\n\nGraph neural networks for molecular property prediction have been studied extensively, including message-passing architectures (Gilmer et al., 2017; Kearnes et al., 2016), graph attention and transformer variants (Veličković et al., 2018; Ying et al., 2021; Rong et al., 2020), spectral and spatial methods (Defferrard et al., 2016; Kipf and Welling, 2017), and equivariant models tailored to 3D geometry (Schütt et al., 2018; Anderson et al., 2019; Satorras et al., 2021). Recent benchmarks compare pooling and readout strategies (Xu et al., 2019; Li et al., 2020) and explore uncertainty estimation for molecular graphs (Gustafsson et al., 2020; Hirschfeld et al., 2020).\n\nIn this paper, we investigate whether task-conditional representations can improve generalization across scaffold splits by dynamically modulating message passing based on property type. We evaluate our approach on common molecular benchmarks and analyze how conditional signals alter the information flow over graph neighborhoods.",
    "reason": "The span lists prior work without linking it to the paper's approach, gap, or viewpoint, satisfying (a) and (c) in the definition.",
    "start": 290,
    "end": 963,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Brown et al. 1",
    "document": "Related Work\n\nFederated learning enables collaborative model training without centralizing raw data, mitigating privacy risks (McMahan et al., 2017; Konečný et al., 2016). Robust aggregation rules defend against adversarial updates and byzantine clients (Blanchard et al., 2017; Yin et al., 2018). Additional privacy threats were noted by Brown et al. 1 in the context of gradient leakage and reconstruction attacks, prompting the use of secure aggregation and differential privacy (Bonawitz et al., 2017; Geyer et al., 2017; Truex et al., 2019). Despite progress, heterogeneity in data and systems remains a major challenge (Li et al., 2020; Karimireddy et al., 2020).",
    "reason": "Wrong use of footnotes: includes a superscript-like numeral without a year; should be Brown et al. (YEAR) or a properly formatted footnote.",
    "start": 339,
    "end": 353,
    "label": "Format"
  },
  {
    "span": "Johnson et al. 2020)",
    "document": "Introduction\n\nMultilingual neural machine translation (MNMT) aims to share parameters across languages to improve low-resource performance (Firat et al., 2016; Aharoni et al., 2019). For practical deployment, zero-shot and zero-resource transfer are crucial to minimize labeling costs (Gu et al., 2019). For translation direction control and shared subword vocabularies, see Johnson et al. 2020) for a seminal approach to massively multilingual NMT.\n\nDespite gains from parameter sharing, language imbalance and domain drift remain central challenges (Arivazhagan et al., 2019; Conneau et al., 2020). We investigate curriculum-based balancing to mitigate negative interference.",
    "reason": "Missing opening parenthesis in the parenthetical author-year citation; should be \"Johnson et al. (2020)\" or \"(Johnson et al., 2020)\" depending on context.",
    "start": 375,
    "end": 395,
    "label": "Format"
  },
  {
    "span": "In (Park et al., 2018)",
    "document": "Introduction\n\nSequential recommendation frames user interaction as a dynamic process, where next-item prediction benefits from modeling order effects and intent drift (Zhou et al., 2019; Liang and Chao, 2020). In (Park et al., 2018) a reinforcement learning formulation was proposed to optimize long-term engagement, while self-attentive architectures improved short-horizon accuracy (Quinn et al., 2021).\n\nHowever, pure RL policies can overfit to short-term click proxies and ignore diversity (Singla and Raman, 2022). Recent work integrates contrastive objectives and slate-wise rewards to counteract exposure bias (Huang et al., 2023; Patel et al., 2023). Our method unifies sequence modeling with counterfactual evaluation under a shared objective.\n",
    "reason": "Wrong citation style; should be narrative 'In Park et al. (2018)' rather than 'In (Park et al., 2018)'.",
    "start": 210,
    "end": 232,
    "label": "Format"
  },
  {
    "span": "according to (Miller, 2020)",
    "document": "Introduction\n\nProgram repair systems aim to automatically fix bugs by searching over candidate patches (Monperrus, 2018). Learning-based approaches leverage large corpora of code changes to model fix patterns (Chen et al., 2019). However, overfitting to common patterns reduces generalization to rare defects. We introduce abstract edit plans with slot filling that regularize the search space, which, according to (Miller, 2020), can improve transfer when combined with invariance objectives.\n\nRelated Work\n\nSemantic-based repair using test oracles (Le Goues et al., 2012) and synthesis-guided repair (Rolim et al., 2018) address complementary aspects; our method focuses on structure-aware generalization.",
    "reason": "Wrong narrative citation style. When used after “according to”, the citation should be narrative: “according to Miller (2020)”, not parenthetical.",
    "start": 402,
    "end": 429,
    "label": "Format"
  },
  {
    "span": "[12]",
    "document": "Related Work\n\nTopic modeling captures latent themes in text collections (Blei et al., 2003; Griffiths and Steyvers, 2004). Neural variants improve coherence and scalability (Miao et al., 2017; Srivastava and Sutton, 2017).\n\nMost prior work evaluates on English newswire corpora (Boyd-Graber et al., 2017). For multilingual social media, several preprocessing pipelines have been proposed [12], but results are difficult to compare due to inconsistent tokenization and stopword lists.\n\nWe focus on cross-lingual robustness by aligning topics through shared subword vocabularies and contrastive regularization.",
    "reason": "Numeric citation style is inconsistent with the author–year style used elsewhere; should cite with authors and year.",
    "start": 388,
    "end": 392,
    "label": "Format"
  },
  {
    "span": "Prompt-based learning turns downstream tasks into masked language modeling problems (Gao et al., 2021). Instruction tuning aligns models with task descriptions (Sanh et al., 2022). Chain-of-thought prompting elicits intermediate reasoning steps (Wei et al., 2022).",
    "document": "Introduction\n\nLarge language models have shifted NLP paradigms from task-specific architectures to flexible prompting and adaptation. Despite strong performance, systematic evaluation under limited supervision remains challenging.\n\nPrompt-based learning turns downstream tasks into masked language modeling problems (Gao et al., 2021). Instruction tuning aligns models with task descriptions (Sanh et al., 2022). Chain-of-thought prompting elicits intermediate reasoning steps (Wei et al., 2022). We investigate calibration under prompt distribution shift.\n\nOur method estimates prompt sensitivity using an uncertainty-aware selection mechanism.",
    "reason": "The three sentences list different prompting strategies without explicit transitions or explanation of how they relate to each other or to the stated challenge, resulting in weak coherence.",
    "start": 232,
    "end": 496,
    "label": "Coherence"
  },
  {
    "span": "Recent works have demonstrated that attention-based GNNs outperform convolutional baselines on all standard traffic datasets.",
    "document": "Introduction\n\nUrban traffic forecasting aims to predict future traffic states (e.g., speed, flow) from historical sensor readings and road network structure. Accurate forecasts support congestion mitigation, routing, and signal control. Deep learning models that exploit spatial-temporal correlations have become dominant due to their capacity to model complex dependencies. Graph neural networks have gained particular traction by leveraging road topology, while attention mechanisms can model dynamic and long-range interactions.\n\nRecent works have demonstrated that attention-based GNNs outperform convolutional baselines on all standard traffic datasets.\n\nDespite this progress, most prior architectures assume static connectivity and struggle with non-recurrent disruptions such as incidents and weather. In this paper, we propose a dynamic graph forecasting framework that adapts connectivity based on exogenous context, improving robustness under distribution shift.",
    "reason": "Mentions 'recent works' and a comparative performance claim without citing any studies (violates rule d and b).",
    "start": 533,
    "end": 658,
    "label": "Unsupported_claim"
  },
  {
    "span": "In a previous study, the authors claimed that multilingual BERT achieved human parity on named entity recognition for several low-resource languages.",
    "document": "Related Work\n\nMultilingual named entity recognition (NER) has benefited from cross-lingual transfer using shared subword vocabularies and multilingual pretraining. A growing body of research explores how to adapt multilingual encoders to low-resource languages through fine-tuning, adapters, or prompting. In a previous study, the authors claimed that multilingual BERT achieved human parity on named entity recognition for several low-resource languages. Subsequent work has investigated the role of annotation noise, label projection, and domain shift in cross-lingual generalization, highlighting the need for careful evaluation protocols and robust metrics.",
    "reason": "Refers to a specific prior study and its finding without citing the study (rule a; example ii).",
    "start": 306,
    "end": 455,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Kumar et al., 2019)",
    "document": "Related Work\n\nFederated learning (FL) enables training across distributed clients without centralizing raw data. Since the original FedAvg algorithm by McMahan et al. (2017), researchers have explored communication compression (Sattler et al., 2019), personalization (Smith et al., 2017), and robustness to drift (Li et al., 2020). In (Kumar et al., 2019), the authors survey privacy-preserving primitives that complement FL protocols. Kairouz et al. (2021) outline open problems in FL systems and emphasize heterogeneity. Recent works also analyze client sampling and fairness (Mohri et al., 2019; Li et al., 2021). Our paper builds on these strands by proposing an adaptive optimizer for cross-device FL, extending insights from Reddi et al. (2021) to non-IID regimes.",
    "reason": "Wrong citation style: a preposition directly before a parenthetical citation; should be narrative (e.g., 'In Kumar et al. (2019)') or purely parenthetical without 'In'.",
    "start": 332,
    "end": 355,
    "label": "Format"
  },
  {
    "span": "MiniImageNet has become the de facto standard benchmark for few-shot classification.",
    "document": "Related Work\n\nFew-shot learning seeks to generalize to novel classes with only a handful of labeled examples. Meta-learning methods optimize for rapid adaptation, while metric-learning approaches learn transferable similarity functions (Vinyals et al., 2016; Finn et al., 2017; Snell et al., 2017).\n\nMiniImageNet has become the de facto standard benchmark for few-shot classification. More recently, researchers have questioned overfitting to narrow domains and proposed cross-domain evaluations and larger, more diverse datasets to assess robustness (Triantafillou et al., 2019). Augmentation strategies and self-supervised pretraining have further improved performance in the low-data regime (Chen et al., 2020).\n\nOur work introduces a domain-shifted benchmark with controlled visual factors and a unified evaluation protocol that separates class novelty from distribution shift. We release code and standardized splits to facilitate fair comparison.",
    "reason": "Introduces and characterizes a dataset as a standard benchmark without citing its source at first mention.",
    "start": 300,
    "end": 384,
    "label": "Unsupported_claim"
  },
  {
    "span": "Zhang et al. (2019) introduced graph-based retrieval for open-domain QA. Guu et al. (2020) trained retrieval-augmented generators end-to-end. Yasunaga et al. (2021) constructed knowledge-augmented QA benchmarks. Karpukhin et al. (2020) presented DPR for dense passage retrieval.",
    "document": "Related Work\n\nOpen-domain question answering (ODQA) systems frequently combine retrieval with generation or extractive readers. The choice of retriever, indexing strategy, and integration with parametric knowledge affects accuracy, latency, and robustness to domain shift. Recent trends emphasize differentiable retrieval and joint training to align retriever and reader components under weak supervision.\n\nZhang et al. (2019) introduced graph-based retrieval for open-domain QA. Guu et al. (2020) trained retrieval-augmented generators end-to-end. Yasunaga et al. (2021) constructed knowledge-augmented QA benchmarks. Karpukhin et al. (2020) presented DPR for dense passage retrieval.\n\nOur approach unifies dense retrieval with structured reasoning by constructing latent relational graphs over top-k passages, enabling the generator to attend over entities and relations while preserving token-level evidence.",
    "reason": "The span lists four works in succession with no explicit relational statements or transitions, making it unclear how graph-based retrieval, RAG, benchmarks, and DPR tie together.",
    "start": 407,
    "end": 685,
    "label": "Coherence"
  },
  {
    "span": "Grad-CAM highlights convolutional saliency via class-specific gradients (Selvaraju et al., 2017). LIME perturbs inputs to fit local surrogate models (Ribeiro et al., 2016). SHAP attributes features using Shapley values (Lundberg and Lee, 2017). TCAV estimates concept sensitivity with linear probes (Kim et al., 2018). The CheXpert dataset is widely used for chest X-ray benchmarking (Irvin et al., 2019).",
    "document": "Introduction\n\nExplainability in Medical Imaging\n\nClinical adoption of deep learning relies on faithful and interpretable predictions. In radiology, stakeholders seek spatial rationales consistent with pathophysiology and textual explanations aligning with reporting conventions. A wide spectrum of post-hoc and concept-based methods has been explored to provide explanations that clinicians can audit.\n\nGrad-CAM highlights convolutional saliency via class-specific gradients (Selvaraju et al., 2017). LIME perturbs inputs to fit local surrogate models (Ribeiro et al., 2016). SHAP attributes features using Shapley values (Lundberg and Lee, 2017). TCAV estimates concept sensitivity with linear probes (Kim et al., 2018). The CheXpert dataset is widely used for chest X-ray benchmarking (Irvin et al., 2019).\n\nLimitations of Current XAI\n\nDespite widespread use, saliency maps can be unstable under slight perturbations (Adebayo et al., 2018), and post-hoc rationales may not reflect model decision pathways (Rudin, 2019). There is a growing interest in counterfactual and prototype-based explanations that connect imaging patterns to human-understandable semantics (Chen et al., 2019; Van Looveren and Klaise, 2021).\n\nOur Contribution\n\nWe present a report-aligned, pathology-grounded explanation framework that combines calibrated concept bottlenecks with uncertainty-aware saliency refinement, improving trust and utility for radiologists.",
    "reason": "The span abruptly shifts from listing heterogeneous XAI methods to a dataset citation without clarifying the connection; there are no transitions or explicit relationships among the cited works, making the flow incoherent.",
    "start": 403,
    "end": 808,
    "label": "Coherence"
  },
  {
    "span": "In (Kumar et al., 2019)",
    "document": "Introduction\n\nMeta-reinforcement learning (meta-RL) aims to enable agents to rapidly adapt to new tasks using prior experience (Finn et al., 2017; Nichol et al., 2018). Context-based methods learn task embeddings that condition a policy for fast adaptation (Rakelly et al., 2019), while gradient-based methods adapt via inner-loop updates. However, both approaches often overfit to meta-training task distributions, leading to brittle adaptation on out-of-distribution tasks (Xu et al., 2020).\n\nIn (Kumar et al., 2019) a regularization scheme is proposed to stabilize adaptation by constraining the policy in representation space. Complementary to this, exploration-aware meta-learners encourage information-gathering behaviors during the adaptation phase (Gupta et al., 2018; Sekar et al., 2020). We build upon these ideas and introduce a task-uncertainty aware objective that learns when to explore vs. exploit based on posterior task uncertainty.\n\nWe benchmark on continuous control suites and procedurally generated tasks, demonstrating improved sample efficiency and robustness compared to strong baselines (Lang et al., 2022; Zhang and Pineau, 2021).",
    "reason": "Wrong citation style in narrative context; should be 'In Kumar et al. (2019)' instead of placing the citation fully in parentheses after 'In'.",
    "start": 495,
    "end": 518,
    "label": "Format"
  },
  {
    "span": "Most existing datasets for sarcasm detection are scraped from Twitter and Reddit.",
    "document": "Introduction\n\nSarcasm detection requires modeling pragmatic cues, contextual incongruity, and world knowledge. Despite advances in representation learning, performance is highly sensitive to domain and platform-specific linguistic patterns. Dataset construction often relies on distant supervision, such as self-annotated hashtags or community labels, which can introduce selection biases.\n\nMost existing datasets for sarcasm detection are scraped from Twitter and Reddit. This narrow sourcing risks overfitting to platform idiosyncrasies and limits generalization to other domains. We introduce a multi-domain sarcasm corpus with balanced sampling across platforms and examine cross-domain transfer.",
    "reason": "Asserts a summary of prior datasets and their sources without citing any dataset papers.",
    "start": 391,
    "end": 472,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Chen et al., 2021",
    "document": "Introduction\n\nGraph neural networks (GNNs) learn node and graph representations by iteratively aggregating information from local neighborhoods (Kipf and Welling, 2017; Hamilton et al., 2017). While effective, standard message passing can suffer from oversmoothing when stacked deeply, causing node embeddings to become indistinguishable (Li et al., 2018; Oono and Suzuki, 2020). Recent work introduces residual connections and normalization to alleviate these issues (Zhou et al., 2020; Xu et al., 2021).\n\nAnother line of research tackles heterophily, where connected nodes do not share labels, by decoupling propagation and transformation or by designing filters that emphasize non-local cues (Pei et al., 2020; Zhang et al., 2022). Techniques such as adaptive propagation and attention over higher-order neighborhoods further improve robustness (Chen et al., 2021 across diverse benchmarks, but their behavior under distribution shift remains underexplored.",
    "reason": "Missing closing parenthesis in a parenthetical citation; it should be \")\" after the year.",
    "start": 848,
    "end": 866,
    "label": "Format"
  },
  {
    "span": "It is widely known that Chinese word segmentation errors severely degrade downstream parsing performance.",
    "document": "Introduction\n\nProcessing Chinese text typically involves a segmentation step prior to higher-level tasks such as parsing and information extraction. Errors introduced early in the pipeline can propagate, complicating the evaluation of syntactic and semantic models. Joint learning approaches attempt to mitigate this by sharing representations across tasks.\n\nIt is widely known that Chinese word segmentation errors severely degrade downstream parsing performance. In this work, we quantify the impact under controlled perturbations and propose a segmentation-aware parser that marginalizes over multiple tokenization hypotheses, improving stability without incurring large computational overhead.",
    "reason": "Makes a specific claim about segmentation errors affecting parsing that should be supported by citations in this specialized area.",
    "start": 359,
    "end": 464,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent recommender systems literature includes fairness-aware ranking, exposure constraints, and calibration techniques (Singh and Joachims, 2018; Biega et al., 2018; Diaz et al., 2020; Ekstrand et al., 2018). Work on provider-side fairness focuses on marketplace dynamics and platform interventions (Liu et al., 2020; Mehrotra et al., 2018; Mansoury et al., 2020).",
    "document": "Introduction\n\nPersonalized recommendation systems can inadvertently propagate or amplify societal and marketplace inequities. Ensuring fairness across users and content providers has thus become an important design objective alongside accuracy and engagement.\n\nRecent recommender systems literature includes fairness-aware ranking, exposure constraints, and calibration techniques (Singh and Joachims, 2018; Biega et al., 2018; Diaz et al., 2020; Ekstrand et al., 2018). Work on provider-side fairness focuses on marketplace dynamics and platform interventions (Liu et al., 2020; Mehrotra et al., 2018; Mansoury et al., 2020).\n\nWe investigate dynamic exposure allocation under drifting relevance and propose an online correction mechanism that enforces long-term exposure targets without sacrificing short-term utility. Our analysis characterizes regret-fairness trade-offs and our experiments demonstrate improved fairness stability on public benchmarks.",
    "reason": "The span catalogs prior strands of work but does not articulate how they interrelate, where they fall short, or why the present study is needed, thus lacking synthesis and author perspective.",
    "start": 261,
    "end": 626,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Temporal fusion networks combine static covariates with attention mechanisms (Lim et al., 2021). Classical ARIMA uses differencing and autoregression (Box and Jenkins, 1970). Seasonal-trend decomposition informs exogenous regressors (Cleveland et al., 1990).",
    "document": "Related Work\n\nNeural Time Series Forecasting. Deep architectures leverage recurrence, convolution, and attention to model long-range dependencies and heterogeneous covariates, often outperforming classical baselines on complex datasets.\n\nClassical Baselines and Hybrids. Traditional models provide strong inductive biases and interpretability, and hybrids aim to capture both linear components and nonlinear residuals.\n\nMethods Overview. Temporal fusion networks combine static covariates with attention mechanisms (Lim et al., 2021). Classical ARIMA uses differencing and autoregression (Box and Jenkins, 1970). Seasonal-trend decomposition informs exogenous regressors (Cleveland et al., 1990).\n\nProbabilistic Forecasting. Distributional outputs and quantile losses enable uncertainty estimation and decision-making under risk, complementing point forecasts.",
    "reason": "The span juxtaposes a neural architecture, a classical ARIMA model, and seasonal decomposition without explaining their relationships or providing transitions, resulting in an abrupt and incoherent sequence.",
    "start": 438,
    "end": 696,
    "label": "Coherence"
  },
  {
    "span": "Carlini et al. (2021) extract training data from language models via prompt-based attacks. Knowledge editing algorithms modify model outputs with localized changes (Meng et al., 2022). De-identification replaces personal names in text corpora (Neamatullah et al., 2008).",
    "document": "Related Work\n\nPrivacy Risks in Language Models. Large language models can memorize and regurgitate rare phrases from training data, raising concerns about leakage of sensitive information (Carlini et al., 2019; Brown et al., 2020).\n\nMitigation and Control. Auditing methods attempt to quantify memorization and attribute outputs to specific training sources, while filtering pipelines aim to remove personally identifiable information prior to training (Thakkar et al., 2021; Lee et al., 2022). Carlini et al. (2021) extract training data from language models via prompt-based attacks. Knowledge editing algorithms modify model outputs with localized changes (Meng et al., 2022). De-identification replaces personal names in text corpora (Neamatullah et al., 2008). We focus on post-hoc controls under constrained retraining budgets.\n\nAccess Control and Differential Privacy. Differentially private training bounds worst-case leakage at the cost of utility, and API-level safeguards reduce exposure by detecting unsafe prompts (Abadi et al., 2016; Perez et al., 2022). Our method combines black-box auditing with targeted output filtering to limit memorization at inference time.",
    "reason": "The span juxtaposes extraction attacks, knowledge editing, and de-identification without explaining their relationships or providing transitions; it is unclear how each cited work connects to the others.",
    "start": 495,
    "end": 765,
    "label": "Coherence"
  },
  {
    "span": "[23]",
    "document": "Introduction\n\nProgram analysis with neural models has leveraged graph encoders to capture control and data flow (Li et al., 2019; Allamanis et al., 2021). In the software mining community, code clone detection and defect prediction benefit from pretraining on large repositories (Chen and Monperrus, 2020). We adopt an author–year citation style throughout this paper; for example, we refer to prior defect localization work by Park et al. (2020) and Sui et al. (2021). However, some studies claim that simple token models are competitive as shown in [23], which motivates our ablation on lexical-only features. Our approach integrates static analysis signals with contrastive objectives (Khosla et al., 2020).",
    "reason": "Inconsistent citation style: numeric bracket '[23]' used in a context that otherwise follows author–year style.",
    "start": 551,
    "end": 555,
    "label": "Format"
  },
  {
    "span": "Statistical downscaling uses bias correction and spatial disaggregation (Wood et al., 2004; Maraun et al., 2010), quantile mapping (Themeßl et al., 2012), and weather typing (Boé et al., 2006), while dynamical downscaling employs nested regional climate models (Giorgi and Mearns, 1999; Jacob et al., 2014). Recently, deep learning methods like CNNs, GANs, and diffusion models have also been investigated (Vandal et al., 2017; Pan et al., 2019; Harris et al., 2022).",
    "document": "Related Work\n\nProducing actionable local projections from coarse global climate models (GCMs) is crucial for adaptation planning. Downscaling approaches attempt to bridge spatial resolution gaps while preserving physical consistency and correcting biases. Our work targets precipitation downscaling with explicit calibration objectives and distributional fidelity.\n\nStatistical downscaling uses bias correction and spatial disaggregation (Wood et al., 2004; Maraun et al., 2010), quantile mapping (Themeßl et al., 2012), and weather typing (Boé et al., 2006), while dynamical downscaling employs nested regional climate models (Giorgi and Mearns, 1999; Jacob et al., 2014). Recently, deep learning methods like CNNs, GANs, and diffusion models have also been investigated (Vandal et al., 2017; Pan et al., 2019; Harris et al., 2022).\n\nWe emphasize distribution-aware training with explicit calibration metrics and examine physical plausibility via water balance diagnostics, enabling a more interpretable assessment than pointwise RMSE alone.",
    "reason": "The span lists families of downscaling methods without articulating their limitations or linking them to the paper’s calibration-focused objective, thus lacking synthesis per (a) and (c).",
    "start": 366,
    "end": 833,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Zhang et.al. (2014)",
    "document": "Introduction\n\nTime-series forecasting under covariate shift requires models that handle regime changes and long-range dependencies (Hyndman and Athanasopoulos, 2018; Sen et al., 2019). Deep architectures such as temporal convolutional networks and transformers have shown strong performance on multivariate datasets (Bai et al., 2018; Wu et al., 2021; Zhou et al., 2021).\n\nClassical statistical baselines remain competitive when properly tuned and regularized (Hyndman et al., 2008; Taylor and Letham, 2018). Zhang et.al. (2014) explore hybrid ARIMA–ANN models to capture linear and nonlinear components, a direction we revisit with modern sequence encoders and kernel-based residuals.\n",
    "reason": "Incorrect formatting of \"et al.\" with an extra period and missing space; should be \"Zhang et al. (2014)\".",
    "start": 509,
    "end": 528,
    "label": "Format"
  },
  {
    "span": "Fairness in recommender systems has been studied via exposure parity, calibration consistency, and disparate impact constraints. Supplier-side fairness targets equitable exposure across item providers, while consumer-side fairness considers utility gaps across demographic groups. Regularization-based approaches, post-processing re-ranking, and constrained optimization are common strategies.",
    "document": "Related Work\n\nRecommender systems can propagate or exacerbate societal biases due to feedback loops, skewed training data, and popularity bias. Fairness-aware recommendation aims to mitigate harms across stakeholders while maintaining utility.\n\nFairness in recommender systems has been studied via exposure parity, calibration consistency, and disparate impact constraints. Supplier-side fairness targets equitable exposure across item providers, while consumer-side fairness considers utility gaps across demographic groups. Regularization-based approaches, post-processing re-ranking, and constrained optimization are common strategies.\n\nRecent evaluations report trade-offs between relevance and fairness under different traffic patterns and catalog dynamics.\n\nWe consider both short-head and long-tail items in an industrial-scale setting with changing supply, focusing on stability under traffic shocks.",
    "reason": "This span enumerates prior concepts and methods without synthesizing them into the authors' perspective or clarifying the specific problem or gap they tackle.",
    "start": 245,
    "end": 638,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Contrastive multimodal alignment has achieved strong performance using InfoNCE objectives (Radford et al., 2021; Jia et al., 2021; Wang et al., 2021).",
    "document": "Introduction\n\nLearning aligned representations across modalities enables zero-shot transfer and flexible downstream use. Recent progress in vision-language models demonstrates the effectiveness of large-scale contrastive pretraining.\n\nContrastive multimodal alignment has achieved strong performance using InfoNCE objectives (Radford et al., 2021; Jia et al., 2021; Wang et al., 2021). Extensions incorporate hard negative mining, momentum encoders, and temperature scaling to refine alignment quality.\n\nDespite advances, reproducibility remains challenging due to dataset curation, filtering strategies, and training heuristics that are often underreported.",
    "reason": "The sentence summarizes prior success without connecting it to the authors' problem or clarifying a specific gap or motivation, thus lacking synthesis (criterion a and c).",
    "start": 235,
    "end": 385,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Contrastive pretraining of image-text pairs has been explored at scale with dual-encoder architectures that align visual and textual embeddings. ALIGN extends this approach using massive noisy web data, and CLIP demonstrates strong zero-shot transfer via prompt engineering. Unified models such as FLAVA and BLIP integrate generative objectives and cross-attention to improve downstream fine-tuning. LiT improves transfer by freezing vision backbones and training text adapters. In this paper we train a contrastive-capable encoder-decoder with a simple multimodal curriculum.",
    "document": "Introduction\n\nVision-language pretraining has rapidly advanced zero-shot and few-shot performance across retrieval, captioning, and classification. A central theme is learning general-purpose representations that align modalities while remaining adaptable to diverse tasks.\n\nContrastive pretraining of image-text pairs has been explored at scale with dual-encoder architectures that align visual and textual embeddings. ALIGN extends this approach using massive noisy web data, and CLIP demonstrates strong zero-shot transfer via prompt engineering. Unified models such as FLAVA and BLIP integrate generative objectives and cross-attention to improve downstream fine-tuning. LiT improves transfer by freezing vision backbones and training text adapters. In this paper we train a contrastive-capable encoder-decoder with a simple multimodal curriculum.\n\nWe benchmark on retrieval, VQA, and open-vocabulary classification and analyze the role of data scales and text normalization.",
    "reason": "The passage summarizes influential vision-language models but does not connect them to a specific shortcoming or motivation for the proposed curriculum; the transition to the authors' method lacks explicit synthesis or gap articulation.",
    "start": 275,
    "end": 851,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The xView dataset has become the de facto benchmark for satellite object detection.",
    "document": "Introduction\n\nObject detection in overhead imagery poses unique challenges due to extreme scale variation, dense object distributions, and viewpoint changes. Remote sensing applications such as disaster response and infrastructure monitoring require detectors that are both accurate and efficient. While convolutional architectures dominated early progress, vision transformers have shown potential benefits in modeling long-range context and handling small objects when paired with multi-scale features.\n\nBenchmarking is complicated by annotation quality, sensor diversity, and class imbalance across datasets. The xView dataset has become the de facto benchmark for satellite object detection. Nevertheless, many real-world deployments operate under different imaging conditions, leading to a generalization gap between research benchmarks and operational data. In this work, we study domain-robust detection by combining multi-resolution token merging with weakly supervised adaptation.\n\nRelated Work\n\nPrior efforts on overhead detection span single-stage and two-stage detectors, adaptive anchors, and scale-aware feature pyramids. Domain adaptation techniques include adversarial alignment, style transfer, and self-training with pseudo labels. However, trading off performance with computational cost remains an open problem for resource-constrained platforms.",
    "reason": "Asserts the status of a specific dataset as a benchmark without citing the dataset or supporting evidence; per rule a and b, such claims require a citation.",
    "start": 612,
    "end": 695,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith et al.",
    "document": "Related Work\n\nGraph neural networks (GNNs) extend deep learning to relational data. Early spectral methods built convolutional filters in the Fourier domain (Bruna et al., 2014; Defferrard et al., 2016), while spatial approaches defined localized message passing (Kipf and Welling, 2017; Hamilton et al., 2017).\n\nAttention mechanisms have been explored to improve expressivity and robustness. Following Smith et al., many works introduced multi-head attention over edges to weight neighborhood influence (Velickovic et al., 2018; Xu et al., 2019). Recent studies also consider positional encodings for graphs (Dwivedi and Bresson, 2021) and global context aggregation (You et al., 2020).",
    "reason": "Narrative citation missing year; should be formatted as “Smith et al. (YEAR)”.",
    "start": 403,
    "end": 415,
    "label": "Format"
  },
  {
    "span": "Independent Q-learning treats other agents as part of the environment (Tan, 1993). COMA uses a centralized critic to compute counterfactual advantages (Foerster et al., 2018). Emergent communication protocols arise from differentiable channels (Mordatch and Abbeel, 2018). Parameter sharing reduces sample complexity (Gupta et al., 2017).",
    "document": "Related Work\n\nMulti-agent reinforcement learning (MARL) studies how multiple learners coordinate, compete, or communicate to achieve joint objectives. Key themes include credit assignment, nonstationarity, partial observability, and communication protocols.\n\nIndependent Q-learning treats other agents as part of the environment (Tan, 1993). COMA uses a centralized critic to compute counterfactual advantages (Foerster et al., 2018). Emergent communication protocols arise from differentiable channels (Mordatch and Abbeel, 2018). Parameter sharing reduces sample complexity (Gupta et al., 2017).\n\nLater work introduced value factorization for scalable cooperation (Sunehag et al., 2018; Rashid et al., 2018) and transformer-based policies for implicit coordination (Mahajan et al., 2021). Our study focuses on communication efficiency under bandwidth constraints, proposing a learned sparsification scheme with task-aware compression.",
    "reason": "The span strings together distinct MARL topics (independent learners, counterfactual critics, communication, parameter sharing) without transitions or explicit links, making the inter-paper connections unclear.",
    "start": 259,
    "end": 597,
    "label": "Coherence"
  },
  {
    "span": "Personalization in federated learning has been investigated through multi-task objectives, model interpolation, clustered aggregation, and meta-learning strategies (Smith et al., 2017; Fallah et al., 2020; Collins et al., 2021; Arivazhagan et al., 2019; Sattler et al., 2020).",
    "document": "Introduction\n\nFederated learning enables on-device training without centralizing raw data, mitigating privacy risks while leveraging cross-silo or cross-device data at scale. Heterogeneity in client data distributions and compute budgets introduces optimization and generalization challenges, often manifesting as unstable convergence, client drift, or degraded accuracy on long-tail users.\n\nPersonalization in federated learning has been investigated through multi-task objectives, model interpolation, clustered aggregation, and meta-learning strategies (Smith et al., 2017; Fallah et al., 2020; Collins et al., 2021; Arivazhagan et al., 2019; Sattler et al., 2020). Communication efficiency has also motivated compressed updates, partial participation, and adaptive client sampling to balance utility and resource constraints.\n\nWe propose an adaptive regularization framework that stabilizes client updates by aligning local curvature with a global surrogate, achieving robust personalization under severe non-IID conditions and limited rounds.",
    "reason": "The span enumerates approaches and citations without articulating how they inform the present work or what specific shortcoming persists, thus lacking synthesis.",
    "start": 392,
    "end": 668,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Previous studies show that syntax-only models cannot generalize beyond trivial bugs.",
    "document": "Related Work\n\nAutomated program repair (APR) methods span search-based patch generation, semantic constraint solving, and learning-based approaches that predict edits from code context (Monperrus, 2018; Le Goues et al., 2012). Neural repair models leverage code representations such as token sequences, AST paths, and graph-based flows to capture syntax and semantics (Alon et al., 2019; Dinella et al., 2020).\n\nGeneralization to out-of-project bugs requires models to understand dataflow, control dependencies, and type constraints rather than memorizing surface patterns (Tufano et al., 2019). Previous studies show that syntax-only models cannot generalize beyond trivial bugs. This motivates hybrid repair architectures that fuse semantic analyses with learned priors.\n\nWe adopt a constraint-aware decoder that conditions on dynamic invariants inferred from concrete executions, yielding patches that better satisfy semantic specifications.",
    "reason": "The statement attributes a conclusion to 'previous studies' but does not cite any specific papers demonstrating the limitation.",
    "start": 596,
    "end": 680,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT was used in a biomedical NER shared task with macro-F1 above 90.",
    "document": "Introduction\n\nBiomedical named entity recognition (NER) supports downstream applications such as literature curation and clinical decision support. BERT was used in a biomedical NER shared task with macro-F1 above 90. Despite impressive headline numbers, performance often varies across entity types and document genres, motivating analyses of generalization and domain adaptation.\n\nIn this study, we investigate pretraining domain effects and span boundary calibration under limited supervision, providing a systematic breakdown by entity family and source corpus.",
    "reason": "Refers to a specific shared task outcome and performance statistic without providing a citation (rules a and e/iii).",
    "start": 148,
    "end": 217,
    "label": "Unsupported_claim"
  },
  {
    "span": "Rafferty et al., 2020)",
    "document": "Related Work\n\nPre-trained encoder-decoders have transformed abstractive summarization, with early work focusing on encoder-only models (Devlin et al., 2019) and later approaches embracing sequence-to-sequence architectures like BART (Lewis et al., 2019). More recent systems combine retrieval with generation and rely on large-scale pretraining, including T5 Rafferty et al., 2020) for multi-task objectives.",
    "reason": "Missing opening parenthesis before the citation; it should read '(Rafferty et al., 2020)'.",
    "start": 359,
    "end": 381,
    "label": "Format"
  },
  {
    "span": "Perez et al. (2021",
    "document": "Introduction\n\nEdge computing pushes computation to the network edge to reduce latency and bandwidth usage. Early systems focus on offloading mobile workloads to nearby servers (Satyanarayanan, 2017). Perez et al. (2021 propose a hierarchical orchestration layer that coordinates micro-datacenters across regions. Subsequent studies examine model compression for edge inference (Han et al., 2016) and adaptive placement under resource constraints (Mao et al., 2017). Our work complements these directions with a scheduling algorithm that balances energy and accuracy across heterogeneous devices.",
    "reason": "Missing closing parenthesis in a narrative citation.",
    "start": 200,
    "end": 218,
    "label": "Format"
  },
  {
    "span": "BERT has been used for rumor classification on Twitter with weak supervision.",
    "document": "Introduction\n\nMisinformation and rumor propagation on social media pose significant societal risks, motivating automated detection approaches. Traditional methods rely on content-based features, propagation structures, and user-level signals (Castillo et al., 2011; Shu et al., 2017). With the advent of pretrained language models, transformer-based encoders have become prominent for text classification tasks.\n\nBERT has been used for rumor classification on Twitter with weak supervision. However, most existing systems are brittle to domain shift and temporal drift, and they frequently ignore conversation context. We propose a context-aware temporal adapter that leverages conversation threads and time-aware contrastive learning to improve robustness under evolving topics.",
    "reason": "Makes a specific claim about prior usage of BERT and training regime without any citation; mentions of prior studies and setups must be referenced.",
    "start": 413,
    "end": 490,
    "label": "Unsupported_claim"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nContinual learning (CL) faces catastrophic forgetting when models are trained sequentially on evolving data streams (Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017). Replay-based methods store exemplars to stabilize performance (Rebuffi et al., 2017), whereas regularization approaches constrain updates in parameter space (Zenke et al., 2017). Dynamic architectures grow capacity over time to accommodate new tasks (Yoon et al., 2018). A comprehensive survey is provided in [12], but our focus is on realistic resource budgets and privacy constraints that preclude raw exemplar storage (Chaudhry et al., 2019; Aljundi et al., 2019).",
    "reason": "Numeric bracket citation used in an author–year context; wrong citation style for the document.",
    "start": 496,
    "end": 500,
    "label": "Format"
  },
  {
    "span": "ClinicalBERT has been widely adopted as the default encoder in radiology report classification.",
    "document": "Introduction\n\nAutomated analysis of radiology reports supports triage, quality assurance, and cohort discovery [1]. Domain-specific language models have improved performance on clinical NLP tasks by capturing medical terminology and report style [2,3]. ClinicalBERT has been widely adopted as the default encoder in radiology report classification. However, most evaluations focus on single-institution datasets, raising concerns about generalization.",
    "reason": "Claims widespread adoption of a specific model in a niche application area but provides no supporting citations.",
    "start": 253,
    "end": 348,
    "label": "Unsupported_claim"
  },
  {
    "span": "Garcia et al. 1",
    "document": "Introduction\n\nCounterfactual data augmentation aims to expose models to minimally edited examples that challenge spurious correlations. In natural language inference, token-level perturbations can mitigate hypothesis-only biases (Gururangan et al., 2018), and syntactic transformations stress-test compositionality (McCoy et al., 2019).\n\nGarcia et al. 1 introduce an editing framework that learns to propose candidate counterfactuals conditioned on model gradients, while Wu et al. (2021) study human-in-the-loop rewriting to ensure semantic plausibility. We extend these ideas to multi-turn dialogue safety by targeting pragmatic markers that often trigger misclassification.",
    "reason": "Improper footnote-style marker used in place of a year. Should include the publication year as 'Garcia et al. (YEAR)' or be formatted as a proper footnote reference.",
    "start": 338,
    "end": 353,
    "label": "Format"
  },
  {
    "span": "Scalability in graph neural networks has been addressed from multiple angles. Hamilton et al. (2017) introduced neighbor sampling to reduce computation. Chen et al. (2018) approximated minibatch training with control variates. Cluster-GCN partitions large graphs into subgraphs (Chiang et al., 2019). Heterogeneous graphs require relation-specific transformations (Schlichtkrull et al., 2018). Caching and precomputation can accelerate inference (Zeng et al., 2020).",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a standard tool for learning on relational data. However, applying GNNs at web-scale or for dynamic graphs remains a challenge due to memory, computation, and communication bottlenecks. A variety of techniques have been proposed to improve training and inference efficiency, as well as to adapt to richer graph schemas.\n\nScalability in graph neural networks has been addressed from multiple angles. Hamilton et al. (2017) introduced neighbor sampling to reduce computation. Chen et al. (2018) approximated minibatch training with control variates. Cluster-GCN partitions large graphs into subgraphs (Chiang et al., 2019). Heterogeneous graphs require relation-specific transformations (Schlichtkrull et al., 2018). Caching and precomputation can accelerate inference (Zeng et al., 2020).\n\nOur study unifies sampling, partitioning, and caching into a single framework and provides empirical trade-offs under strict latency constraints.",
    "reason": "The span jumps between sampling, variance reduction, clustering, heterogeneous modeling, and caching without clarifying their relationships or providing transitions, leaving the connections between cited works implicit and incoherent.",
    "start": 376,
    "end": 842,
    "label": "Coherence"
  },
  {
    "span": "Early methods optimize a global objective across clients with periodic averaging (McMahan et al., 2017; Li et al., 2020). Subsequent approaches explore personalization via fine-tuning, meta-learning, and mixture models (Arivazhagan et al., 2019; Fallah et al., 2020; Dinh et al., 2020). Recent studies examine client sampling, adaptive optimizers, and proximal regularization to stabilize training (Karimireddy et al., 2020; Reddi et al., 2021; Li et al., 2020b).",
    "document": "Introduction\n\nFederated learning (FL) enables on-device training without centralizing raw data, mitigating privacy risks while leveraging distributed data diversity. Yet, heterogeneity in client data distributions and resource constraints often limit model performance and stability. A central challenge is balancing global generalization with local personalization under communication and compute budgets.\n\nEarly methods optimize a global objective across clients with periodic averaging (McMahan et al., 2017; Li et al., 2020). Subsequent approaches explore personalization via fine-tuning, meta-learning, and mixture models (Arivazhagan et al., 2019; Fallah et al., 2020; Dinh et al., 2020). Recent studies examine client sampling, adaptive optimizers, and proximal regularization to stabilize training (Karimireddy et al., 2020; Reddi et al., 2021; Li et al., 2020b).\n\nIn this paper, we study communication-efficient personalization for heterogeneous clients by decoupling shared and client-specific subspaces and compressing updates with structure-aware sketching. We provide convergence guarantees under partial participation and show improvements on vision and language benchmarks with 2–4× fewer communication rounds.",
    "reason": "The span lists prior lines of work without explaining how they relate to the present study, what limitations remain, or how the cited methods inform the proposed approach; it lacks an articulated gap or author perspective.",
    "start": 408,
    "end": 871,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Most recent works adopt a reinforcement learning fine-tuning stage to directly optimize ROUGE.",
    "document": "Introduction\n\nNeural abstractive summarization has advanced rapidly with sequence-to-sequence models augmented by attention and copy mechanisms (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). Most recent works adopt a reinforcement learning fine-tuning stage to directly optimize ROUGE. However, these approaches often suffer from exposure bias and reward sparsity, motivating alternative training objectives that better align with human preferences (Ranzato et al., 2016; Stiennon et al., 2020). In parallel, research has explored content planning and factual consistency to mitigate hallucinations (Dong et al., 2020; Kryscinski et al., 2020). Our work revisits training objectives for summarization with a focus on stable optimization and faithfulness.",
    "reason": "Claims about 'most recent works' and specific methodology require citations at first mention; none are provided for this sentence.",
    "start": 205,
    "end": 299,
    "label": "Unsupported_claim"
  },
  {
    "span": "End-to-end ASR unifies acoustic and language modeling. CTC enables monotonic alignment via path marginalization (Graves et al., 2006). Attention-based encoders decode without explicit alignments (Chan et al., 2016). RNN-T jointly optimizes encoder, prediction, and joiner networks (Graves, 2012). External language models can be fused during decoding (Toshniwal et al., 2018). Multilingual pretraining improves low-resource recognition (Pratap et al., 2020).",
    "document": "Related Work\n\nAutomatic speech recognition (ASR) has shifted from hybrid HMM-DNN pipelines to end-to-end architectures that streamline training and inference. The literature spans alignment mechanisms, decoding strategies, and leveraging external resources to improve robustness across domains and languages.\n\nEnd-to-end ASR unifies acoustic and language modeling. CTC enables monotonic alignment via path marginalization (Graves et al., 2006). Attention-based encoders decode without explicit alignments (Chan et al., 2016). RNN-T jointly optimizes encoder, prediction, and joiner networks (Graves, 2012). External language models can be fused during decoding (Toshniwal et al., 2018). Multilingual pretraining improves low-resource recognition (Pratap et al., 2020).\n\nWe investigate streaming constraints and propose a latency-aware training objective compatible with both CTC and RNN-T.",
    "reason": "The span presents several ASR approaches and add-ons sequentially without transitions or explanation of their relationships, resulting in abrupt topic shifts and implied, not explicit, connections.",
    "start": 310,
    "end": 768,
    "label": "Coherence"
  },
  {
    "span": "Recent works have demonstrated that lightweight Vision Transformers surpass convolutional baselines on multi-spectral semantic segmentation.",
    "document": "Introduction\n\nRemote sensing imagery presents unique challenges for semantic segmentation due to its multi-spectral channels, extreme class imbalance, and large spatial context requirements. Convolutional neural networks (CNNs) have been the de facto backbone for these tasks, with encoder–decoder architectures and pyramid pooling modules achieving strong performance on aerial and satellite benchmarks. However, the growing interest in self-attention has led to Vision Transformers (ViTs) and hybrid CNN–ViT models that better capture long-range dependencies across spectral bands. Recent works have demonstrated that lightweight Vision Transformers surpass convolutional baselines on multi-spectral semantic segmentation. Despite these advances, most methods are tailored to specific sensors and require extensive pretraining, hindering deployment in data-scarce regions.\n\nIn this paper, we investigate cross-sensor generalization with a spectral-aware transformer that shares parameters across bands while preserving channel-specific inductive biases. We analyze robustness to spectral shifts and assess data efficiency through few-shot adaptation. Our contributions include: a band-conditioned transformer module, a spectral augmentation strategy, and a cross-sensor evaluation protocol covering scenes from agricultural, urban, and coastal environments.",
    "reason": "Mentions 'Recent works' and a comparative performance claim without any citations, violating rule (d).",
    "start": 584,
    "end": 724,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Vaswani et al., 2017)",
    "document": "Introduction\n\nTransformer architectures have become the de facto standard for sequence modeling across NLP and vision. In (Vaswani et al., 2017) the self-attention mechanism was introduced as a means to model long-range dependencies without recurrence. Building on this foundation, pretrained language models such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2019; Brown et al., 2020) demonstrated strong transfer to a variety of downstream tasks. Subsequent work explored scaling laws (Kaplan et al., 2020) and instruction tuning (Sanh et al., 2022; Ouyang et al., 2022), while multimodal variants extended Transformers to handle images and audio (Dosovitskiy et al., 2021; Radford et al., 2021). In contrast to these efforts, our study focuses on efficient fine-tuning strategies for domain adaptation under limited compute (Houlsby et al., 2019; Lester et al., 2021).",
    "reason": "Wrong citation style: the preposition 'In' precedes a parenthetical citation. It should be narrative, e.g., 'In Vaswani et al. (2017)' or the preposition removed: '(Vaswani et al., 2017)'.",
    "start": 119,
    "end": 144,
    "label": "Format"
  },
  {
    "span": "Modern object detectors include two-stage frameworks like Faster R-CNN (Ren et al., 2015) and Mask R-CNN (He et al., 2017), single-stage models such as SSD (Liu et al., 2016) and YOLO variants (Redmon et al., 2016; Bochkovskiy et al., 2020), and end-to-end set prediction with DETR and its successors (Carion et al., 2020; Zhu et al., 2021). Advances in feature pyramids (Lin et al., 2017), normalization (Ioffe and Szegedy, 2015; Wu and He, 2018), and loss design (Lin et al., 2017b) further improve accuracy.",
    "document": "Related Work\n\nObject detection has evolved rapidly with deep convolutional and transformer-based architectures. Benchmarks such as COCO and PASCAL VOC have driven progress with standardized evaluation and large-scale annotations (Lin et al., 2014; Everingham et al., 2015). The community has also explored robustness to shifts such as occlusion, scale, and illumination.\n\nModern object detectors include two-stage frameworks like Faster R-CNN (Ren et al., 2015) and Mask R-CNN (He et al., 2017), single-stage models such as SSD (Liu et al., 2016) and YOLO variants (Redmon et al., 2016; Bochkovskiy et al., 2020), and end-to-end set prediction with DETR and its successors (Carion et al., 2020; Zhu et al., 2021). Advances in feature pyramids (Lin et al., 2017), normalization (Ioffe and Szegedy, 2015; Wu and He, 2018), and loss design (Lin et al., 2017b) further improve accuracy.\n\nLow-light detection has attracted attention through specialized datasets and enhancement-then-detect pipelines (Chen et al., 2018; Liu et al., 2021). We study a detection-first approach that integrates noise-aware representations with exposure-invariant training on raw and sRGB images.\n",
    "reason": "The span enumerates detection models and components without connecting them to the paper’s focus on low-light settings or explaining what is missing, exhibiting lack of synthesis per (a) and (c).",
    "start": 372,
    "end": 882,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[23]",
    "document": "Related Work\n\nRobust optimization aims to improve worst-case performance under bounded perturbations (Madry et al., 2018; Sinha et al., 2018). While PGD-based training dominates image robustness, text models demand discrete perturbation strategies and semantic constraints (Jin et al., 2020; Morris et al., 2020). Several defenses rely on certified bounds via interval bound propagation or randomized smoothing (Cohen et al., 2019; Huang et al., 2021). As demonstrated in [23], evaluation protocols must account for attack adaptivity and gradient obfuscation.\n\nWe adopt a standardized evaluation harness and contribute transfer-based attacks tailored to transformer tokenization.",
    "reason": "Numeric bracket citation used in an author–year context; should cite with author and year (e.g., 'as demonstrated in Author (Year)' or '(Author, Year)').",
    "start": 472,
    "end": 476,
    "label": "Format"
  },
  {
    "span": "Fairness in recommender systems has been studied through metrics for exposure, calibration, and disparity across users and items (Singh and Joachims, 2018; Biega et al., 2018; Beutel et al., 2019; Yao and Huang, 2017). Mitigation strategies include pre-processing reweighting, in-processing constraints, and post-processing re-ranking (Kamishima et al., 2012; Celis et al., 2018; Diaz et al., 2020).",
    "document": "Related Work\n\nRecommender systems can amplify biases present in data or system design, affecting both consumer and provider stakeholders. Measuring and mitigating such biases requires clear objectives and careful system integration.\n\nFairness in recommender systems has been studied through metrics for exposure, calibration, and disparity across users and items (Singh and Joachims, 2018; Biega et al., 2018; Beutel et al., 2019; Yao and Huang, 2017). Mitigation strategies include pre-processing reweighting, in-processing constraints, and post-processing re-ranking (Kamishima et al., 2012; Celis et al., 2018; Diaz et al., 2020).\n\nOur work evaluates fairness–accuracy trade-offs under sparse provider feedback using a simple constrained training objective.",
    "reason": "The span summarizes metrics and mitigation strategies but does not connect them to the paper’s setting or identify which limitations motivate the proposed evaluation; it lacks synthesis.",
    "start": 234,
    "end": 633,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Santos et al.",
    "document": "Related Work\n\nSemi-supervised speech recognition leverages unlabeled audio to reduce the dependence on costly human transcriptions. Early self-training systems refined pseudo-labels with confidence filters (Lee et al., 2019) and iterative re-training (Park and Kim, 2020). Prior work by Santos et al. examined entropy minimization for acoustic models under domain shift, showing promising gains on noisy speech. More recently, consistency-regularization across augmentations has become popular (Xie et al., 2020; Ahn and Lee, 2021). In contrast to these methods, we explore uncertainty-aware pseudo-labeling for streaming ASR and provide a thorough ablation across augmentation strengths.",
    "reason": "Narrative citation missing year; should be formatted like Santos et al. (2019).",
    "start": 287,
    "end": 300,
    "label": "Format"
  },
  {
    "span": "Prior work shows that cross-attention is essential for long-context modeling.",
    "document": "Introduction\n\nLong-context transformers enable processing of inputs far beyond 512 tokens by modifying attention patterns or memory (Beltagy et al., 2020; Zaheer et al., 2020). Retrieval-augmented models externalize memory to scale to millions of tokens (Borgeaud et al., 2022; Lewis et al., 2020). Prior work shows that cross-attention is essential for long-context modeling. We revisit this assumption with lightweight recurrent adapters that maintain global state without quadratic cost.",
    "reason": "Asserts a conclusion attributed to prior work without citing any specific studies.",
    "start": 299,
    "end": 376,
    "label": "Unsupported_claim"
  },
  {
    "span": "Human-in-the-loop reinforcement learning has consistently improved safety scores by 15–20 points across benchmarks.",
    "document": "Introduction Aligning language models with human preferences improves helpfulness and safety by optimizing towards human-in-the-loop feedback rather than static proxies (Christiano et al., 2017; Stiennon et al., 2020). Preference modeling and reinforcement learning from AI feedback further scale supervision when expert annotation is scarce (Bai et al., 2022). Human-in-the-loop reinforcement learning has consistently improved safety scores by 15–20 points across benchmarks. We examine whether mixture-of-preference-experts training can reduce over-optimization pathologies.",
    "reason": "Claims a quantitative effect size across benchmarks without any citations, violating rule (b).",
    "start": 362,
    "end": 477,
    "label": "Unsupported_claim"
  },
  {
    "span": "Garcia 2018",
    "document": "Related Work\n\nTime series anomaly detection methods include reconstruction-based autoencoders (Zong et al., 2018), prediction-based models (Hundman et al., 2018), and probabilistic approaches (Ren et al., 2019). Benchmarks emphasize point- and range-level metrics (Tatbul et al., 2018). Recent transformer models capture long-range dependencies (Zerveas et al., 2021; Wu et al., 2021). In industrial settings, change-point detection remains crucial (Truong et al., 2020; Aminikhanghahi and Cook, 2017; Garcia 2018). We unify scoring and calibration via conformal risk control (Angelopoulos et al., 2021).",
    "reason": "Missing comma between author and year in a parenthetical citation; should be 'Garcia, 2018'.",
    "start": 502,
    "end": 513,
    "label": "Format"
  },
  {
    "span": "(Ahmed, 2020,)",
    "document": "Related Work\n\nPolicy gradient methods form a foundational class of reinforcement learning algorithms. Variance reduction techniques such as baselines and advantage estimation stabilize training across environments (Sutton et al., 2000; Schulman et al., 2015). Some tutorials (Ahmed, 2020,) overview practical implementation details, but often omit discussion of off-policy corrections that arise in asynchronous settings (Espeholt et al., 2018). Trust region and proximal methods enforce conservative updates to prevent catastrophic policy collapse (Schulman et al., 2017).\n\nWe extend generalized advantage estimation with adaptive clipping to further reduce variance under sparse rewards.",
    "reason": "Trailing comma appears before the closing parenthesis in the citation; it should be '(Ahmed, 2020)'.",
    "start": 275,
    "end": 289,
    "label": "Format"
  },
  {
    "span": "Fairness metrics assess disparities in exposure or utility for users and items (Beutel et al., 2019; Ekstrand et al., 2021). Debiasing through propensity modeling corrects selection bias in implicit feedback (Schnabel et al., 2016; Saito, 2020). Exposure modeling in auctions analyzes ranking with budget and pacing constraints (Balseiro et al., 2019; Lahaie et al., 2021).",
    "document": "Related Work\n\nFairness in Recommender Systems\n\nPersonalized recommendation has raised concerns about disparate treatment and unequal exposure of content across providers and user groups (Ekstrand et al., 2021; Burke, 2017). Responses include measurement, algorithmic mitigation, and stakeholder-aware evaluation.\n\nFairness metrics assess disparities in exposure or utility for users and items (Beutel et al., 2019; Ekstrand et al., 2021). Debiasing through propensity modeling corrects selection bias in implicit feedback (Schnabel et al., 2016; Saito, 2020). Exposure modeling in auctions analyzes ranking with budget and pacing constraints (Balseiro et al., 2019; Lahaie et al., 2021).\n\nOther works propose constrained learning-to-rank, counterfactual evaluation, and multi-objective optimization to balance relevance and fairness (Agarwal et al., 2019; Singh and Joachims, 2019; Diaz et al., 2020). Still, a unified view that links exposure dynamics, feedback loops, and fairness constraints remains open.",
    "reason": "The three sentences juxtapose fairness metrics, propensity-based debiasing, and auction exposure modeling without transitions or an explicit explanation of how they relate, causing an abrupt shift between cited areas.",
    "start": 314,
    "end": 687,
    "label": "Coherence"
  },
  {
    "span": "(Garcia et al. 2019)",
    "document": "Introduction\n\nLearning-to-rank with human feedback has gained momentum as user interactions provide implicit relevance judgments (Joachims, 2002; Radlinski and Joachims, 2007). Counterfactual evaluation corrects for presentation bias by modeling click propensities (Wang et al., 2016; Joachims et al., 2017). Recent advances apply inverse propensity scoring within off-policy learning frameworks for large-scale retrieval (Ai et al., 2019). Prior work has also explored pairwise preference elicitation for efficient data collection (Chen et al., 2013) and multi-fidelity feedback aggregation (Garcia et al. 2019) to reduce annotation cost.",
    "reason": "Missing comma before the year in an author–date parenthetical citation; should be \"(Garcia et al., 2019)\".",
    "start": 592,
    "end": 612,
    "label": "Format"
  },
  {
    "span": "[see Johnson et al., 2015)",
    "document": "Related Work\n\nTime-series forecasting has benefited from advances in deep sequence models, including temporal convolutions (Lea et al., 2017), recurrent networks (Hochreiter and Schmidhuber, 1997), and transformers (Lim et al., 2021). Exogenous variables and dynamic covariates are crucial for accurate predictions in real-world systems (Salinas et al., 2020).\n\nClassical decomposition techniques remain competitive for certain seasonal patterns (Hyndman and Athanasopoulos, 2018), but hybrid models often outperform purely statistical baselines [see Johnson et al., 2015). Our work introduces a calibration layer that corrects for regime shifts and improves multi-horizon accuracy across benchmarks.\n\nWe evaluate on energy, traffic, and retail datasets, reporting significant gains over strong neural and statistical baselines.",
    "reason": "Mismatched brackets in the citation; it starts with a square bracket and ends with a parenthesis. It should be consistent (e.g., “(see Johnson et al., 2015)”).",
    "start": 546,
    "end": 572,
    "label": "Format"
  },
  {
    "span": "mBERT has been fine-tuned for low-resource machine translation by masking target-side tokens during decoding.",
    "document": "Introduction\n\nLow-resource machine translation (MT) faces data scarcity, domain shift, and limited lexical coverage. Cross-lingual pretraining has become a key ingredient to bridge these gaps by sharing representations across languages. mBERT has been fine-tuned for low-resource machine translation by masking target-side tokens during decoding. While such adaptations report gains on typologically diverse pairs, the mechanisms that drive transfer remain poorly understood. We analyze parameter sharing patterns and propose a modular adapter that yields consistent improvements without task-specific pretraining.",
    "reason": "Describes a specific training setup for a model in prior work but does not cite any source.",
    "start": 237,
    "end": 346,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent work leverages instrumental variables in machine learning through two-stage predictors, deep IV estimators, and kernel-based approaches (Hartford et al., 2017; Singh et al., 2019; Syrgkanis et al., 2019; Lewis and Syrgkanis, 2018). Extensions address weak instruments and heterogeneity with various regularizers (Bai and Ng, 2010; Andrews et al., 2019).",
    "document": "Introduction\n\nEstimating causal effects from observational data is central to decision-making in economics, healthcare, and platforms. Instrumental variables (IV) provide identification under specific assumptions, but finite-sample estimation in high dimensions is challenging.\n\nRecent work leverages instrumental variables in machine learning through two-stage predictors, deep IV estimators, and kernel-based approaches (Hartford et al., 2017; Singh et al., 2019; Syrgkanis et al., 2019; Lewis and Syrgkanis, 2018). Extensions address weak instruments and heterogeneity with various regularizers (Bai and Ng, 2010; Andrews et al., 2019).\n\nPractical deployments often contend with partially observed confounding and shifting instrument strength across contexts. Estimators can be sensitive to misspecification and overfitting in the first stage, which propagates bias to the second stage.\n\nWe propose a robust IV pipeline with adaptive shrinkage and representation learning guided by instrument strength diagnostics, offering improved stability under weak and heterogeneous instruments.",
    "reason": "The span lists prior IV methods and extensions but does not synthesize how these methods relate to the authors’ goals or define the specific gap addressed.",
    "start": 279,
    "end": 639,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Ekstrand et al. (2018) examine user-side fairness measures in collaborative filtering. Burke (2017) defines the concept of multi-sided fairness. Singh and Joachims (2018) develop exposure-based ranking fairness constraints. Steck (2018) analyzes popularity bias in recommendation lists.",
    "document": "Related Work\n\nFairness in Recommender Systems\n\nFairness notions in recommendation span users, items, and providers, and interact with optimization and ranking objectives. Early studies adapted classification fairness to user-level outcomes, while later work considered platform and provider perspectives. Ekstrand et al. (2018) examine user-side fairness measures in collaborative filtering. Burke (2017) defines the concept of multi-sided fairness. Singh and Joachims (2018) develop exposure-based ranking fairness constraints. Steck (2018) analyzes popularity bias in recommendation lists. Practical deployments must reconcile these objectives with relevance and calibration.\n\nMitigation techniques include post-processing re-ranking, in-processing regularizers, and counterfactual evaluation. We build on exposure-based formulations but target long-tail creator fairness under evolving catalogs and position bias.",
    "reason": "Multiple sentences enumerate disparate papers with no connective tissue or explanation of how each builds on or contrasts with the previous one; the relationships between user fairness, multi-sided fairness, exposure constraints, and popularity bias are left implicit.",
    "start": 305,
    "end": 591,
    "label": "Coherence"
  },
  {
    "span": "Common practice is to report BLEU scores on newstest sets without significance testing.",
    "document": "Introduction\n\nAutomatic evaluation remains central to machine translation research, with metrics such as BLEU, chrF, and COMET used to assess incremental progress (Papineni et al., 2002; Popovic, 2015; Rei et al., 2020). As models improve and performance gaps narrow, robust comparison protocols become increasingly important.\n\nCommon practice is to report BLEU scores on newstest sets without significance testing. While this simplifies reporting, it can obscure whether small absolute differences reflect systematic quality improvements or random variation due to sampling. Moreover, reliance on a single test set risks overfitting evaluation to idiosyncrasies of a particular year or domain.\n\nWe propose a multi-testbed evaluation framework combining stratified resampling, effect-size reporting, and metric triangulation. Our experiments across four language pairs show that nominal improvements under single-set BLEU often vanish when subjected to paired bootstrap resampling and cross-year aggregation.",
    "reason": "It generalizes about community practice without providing evidence or references to surveys, guidelines, or studies supporting the claim.",
    "start": 328,
    "end": 415,
    "label": "Unsupported_claim"
  },
  {
    "span": "Exploration remains a central challenge. Epsilon-greedy is a simple baseline (Sutton and Barto, 2018). Upper confidence bounds guide selection by optimism (Auer, 2002). Count-based bonuses encourage visiting novel states (Bellemare et al., 2016). Intrinsic curiosity uses prediction error as reward (Pathak et al., 2017). Entropy regularization prevents premature convergence (Mnih et al., 2016).",
    "document": "Related Work\n\nReinforcement learning (RL) algorithms must balance exploration and exploitation to discover high-reward behaviors efficiently. Classical approaches emphasize statistical guarantees in bandits, while deep RL introduces auxiliary signals that scale to high-dimensional observations. Despite numerous strategies, understanding their relative performance remains nontrivial.\n\nExploration remains a central challenge. Epsilon-greedy is a simple baseline (Sutton and Barto, 2018). Upper confidence bounds guide selection by optimism (Auer, 2002). Count-based bonuses encourage visiting novel states (Bellemare et al., 2016). Intrinsic curiosity uses prediction error as reward (Pathak et al., 2017). Entropy regularization prevents premature convergence (Mnih et al., 2016).\n\nOur work provides a unified benchmark and diagnostic metrics to disentangle optimism, novelty, and stochasticity effects.",
    "reason": "The span presents a list of exploration methods without transitions or explanations tying them together, so the relationship between each cited approach is left implicit and coherence across sentences is weak.",
    "start": 387,
    "end": 783,
    "label": "Coherence"
  },
  {
    "span": "Bolukbasi et al. (2016) demonstrated gender bias in word embeddings. Hardt et al. (2016) proposed equalized odds as a fairness criterion. Zhao et al. (2018) examined coreference resolution bias in contextual models.",
    "document": "Related Work\n\nFairness in NLP spans bias measurement, mitigation, and evaluation across tasks and languages. Approaches include data balancing, representation learning, and constraint-based optimization.\n\nBolukbasi et al. (2016) demonstrated gender bias in word embeddings. Hardt et al. (2016) proposed equalized odds as a fairness criterion. Zhao et al. (2018) examined coreference resolution bias in contextual models. Sun et al. (2019) surveyed debiasing methods for word embeddings.\n\nOur contribution is a task-agnostic calibration layer that enforces group-aware monotonicity while preserving utility under distribution shift.",
    "reason": "The span juxtaposes embedding bias, a classification fairness metric, and model-specific bias without clarifying how these concepts relate or transition, leaving the connections between cited works implicit.",
    "start": 205,
    "end": 420,
    "label": "Coherence"
  },
  {
    "span": "Gal et al. (2017) estimated uncertainty with Bayesian deep learning to select informative samples. Sener and Savarese (2018) formulated core-set selection as a k-center problem. Yang et al. (2017) used adversarial learning to query maximally confusing examples. Casanova et al. (2020) designed region-level acquisition for dense prediction.",
    "document": "Related Work\n\nActive Learning for Semantic Segmentation\n\nLabeling dense masks is expensive, motivating acquisition functions that maximize utility per annotation. Prior work spans uncertainty estimation, diversity-based core-sets, and structure-aware selection for dense outputs. Evaluation protocols vary in batch size, augmentation, and pretraining.\n\nGal et al. (2017) estimated uncertainty with Bayesian deep learning to select informative samples. Sener and Savarese (2018) formulated core-set selection as a k-center problem. Yang et al. (2017) used adversarial learning to query maximally confusing examples. Casanova et al. (2020) designed region-level acquisition for dense prediction.\n\nWe propose a topology-aware acquisition strategy that aligns sample selection with expected improvements in boundary accuracy, reducing redundancy across queried regions.",
    "reason": "The span lists four acquisition strategies in separate sentences with no transitions; their relationships and comparative roles in active learning are left implicit.",
    "start": 353,
    "end": 693,
    "label": "Coherence"
  },
  {
    "span": "Knowledge distillation has been applied to ASR using teacher–student frameworks, including logit matching, feature mimicry, sequence-level KD, and intermediate-layer alignment (Hinton et al., 2015; Watanabe et al., 2017; Kurata and Audhkhasi, 2018; Kim and Rush, 2016; Park et al., 2019). Related techniques incorporate temperature scaling, multi-teacher ensembling, and curriculum schedules to stabilize training (Furlanello et al., 2018; You et al., 2019; Tian et al., 2020).",
    "document": "Related Work\n\nEnd-to-end automatic speech recognition (ASR) has advanced through encoder–decoder architectures, CTC variants, and attention-based decoders. Improving accuracy under resource constraints has driven interest in model compression and distillation.\n\nKnowledge distillation has been applied to ASR using teacher–student frameworks, including logit matching, feature mimicry, sequence-level KD, and intermediate-layer alignment (Hinton et al., 2015; Watanabe et al., 2017; Kurata and Audhkhasi, 2018; Kim and Rush, 2016; Park et al., 2019). Related techniques incorporate temperature scaling, multi-teacher ensembling, and curriculum schedules to stabilize training (Furlanello et al., 2018; You et al., 2019; Tian et al., 2020).\n\nWe focus on compressing conformer-based ASR models into low-latency students. Our experiments compare several KD objectives under limited-label and noisy-label regimes.",
    "reason": "The span summarizes multiple KD techniques and citations but does not explain how they relate to the proposed approach or what specific deficiency motivates the new method; it lacks explicit synthesis.",
    "start": 262,
    "end": 739,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The Kaggle 2015 RecSys Challenge defined the standard split for this problem.",
    "document": "Introduction\n\nSession-based recommendation models user interactions within short windows to predict the next item. Recurrent and attention-based architectures have shown strong performance by capturing the evolving intent within a session.\n\nThe Kaggle 2015 RecSys Challenge defined the standard split for this problem.\n\nWhile this convention facilitates comparisons across methods, it also encourages overfitting to a single benchmark. We therefore evaluate on multiple public datasets and report robustness under different temporal splits.",
    "reason": "References a specific competition and a protocol it introduced without providing a citation to the challenge overview or dataset (rule a).",
    "start": 241,
    "end": 318,
    "label": "Unsupported_claim"
  },
  {
    "span": "Unsupervised domain adaptation aligns feature distributions across domains (Ganin and Lempitsky, 2015). Self-supervised pretraining supplies generalizable invariances (He et al., 2020). Test-time adaptation updates batch-norm statistics to track drift (Sun et al., 2020). Robust learning addresses label noise in large-scale datasets (Song et al., 2022).",
    "document": "Related Work\n\nModels deployed in the wild face performance degradation under distribution shift. Research on domain adaptation and robustness proposes techniques that aim to maintain accuracy as data characteristics evolve over time.\n\nUnsupervised domain adaptation aligns feature distributions across domains (Ganin and Lempitsky, 2015). Self-supervised pretraining supplies generalizable invariances (He et al., 2020). Test-time adaptation updates batch-norm statistics to track drift (Sun et al., 2020). Robust learning addresses label noise in large-scale datasets (Song et al., 2022).\n\nOur approach combines adaptation with explicit shift detection to decide when and how to update models.",
    "reason": "The works are listed sequentially without transitions or an explicit rationale linking domain adaptation, self-supervision, test-time adaptation, and label-noise robustness, leaving their relationships implied rather than stated.",
    "start": 235,
    "end": 589,
    "label": "Coherence"
  },
  {
    "span": "Alvarez et al. 1",
    "document": "Related Work\n\nActive learning for document OCR aims to reduce labeling costs by prioritizing informative instances. Classic uncertainty sampling and margin-based criteria have been adapted for sequence transduction (Lewis and Gale, 1994; Settles, 2009). Diversity-aware acquisition mitigates redundancy in highly similar page images (Roy and McCallum, 2001). As noted by Alvarez et al. 1, most pool-based strategies overlook layout uncertainty, leading to suboptimal region proposals. Recent work integrates detection confidence with recognition entropy to focus on ambiguous text lines (Chen and Rao, 2020). Semi-supervised approaches combine pseudo-labels with consistency regularization to leverage large unlabeled corpora (Sohn et al., 2020). Our method complements these directions by jointly modeling acquisition for detection and recognition, accounting for cross-task dependencies on challenging scripts and degradations (Kumar and Patel, 2022).",
    "reason": "Improper footnote-like usage after authors; should include the year in proper narrative form (e.g., 'Alvarez et al. (YEAR)') or be formatted as a proper footnote.",
    "start": 371,
    "end": 387,
    "label": "Format"
  },
  {
    "span": "previous studies have conclusively shown that oversmoothing is inevitable as depth grows",
    "document": "Introduction\n\nGraph neural networks (GNNs) generalize message passing to learn over relational structures, achieving strong results in node classification, link prediction, and graph-level tasks. However, stacking many layers often leads to optimization difficulties and degraded performance on homophilous as well as heterophilous graphs.\n\nIn this context, previous studies have conclusively shown that oversmoothing is inevitable as depth grows, leading node embeddings to become indistinguishable after only a few propagation steps. Architectural modifications like residual connections, normalization, and decoupled propagation have been proposed to mitigate this effect.\n\nOur contribution departs from propagation-centric fixes by reparameterizing neighborhood aggregation as frequency-aware filtering that preserves inter-class variation in deeper stacks.",
    "reason": "Asserts a strong prior finding (“conclusively shown” and inevitability of oversmoothing) without any supporting citations to the studies making this claim.",
    "start": 358,
    "end": 446,
    "label": "Unsupported_claim"
  },
  {
    "span": "Safety evaluations for large language models increasingly rely on red-teaming and jailbreak benchmarks. Adversarial prompt collections probe refusal boundaries, automated agents search for exploit chains, and multi-turn attacks escalate model compliance. Concurrently, mitigation studies examine system prompts, content filters, and reinforcement learning from human feedback to reduce harmful outputs.",
    "document": "Introduction\nLarge language models can generate harmful or unsafe content, prompting the need for systematic evaluation and mitigation. Assessing real-world risk requires scalable, diverse tests that reflect evolving adversarial behavior.\n\nRelated Work\nStandard toxicity metrics and keyword lists give coarse measures, whereas targeted adversarial testing reveals model failure modes.\nSafety evaluations for large language models increasingly rely on red-teaming and jailbreak benchmarks. Adversarial prompt collections probe refusal boundaries, automated agents search for exploit chains, and multi-turn attacks escalate model compliance. Concurrently, mitigation studies examine system prompts, content filters, and reinforcement learning from human feedback to reduce harmful outputs.\nEvaluation protocols span single-turn and dialog settings, with varying policies and safety taxonomies, complicating comparisons across studies.\n\nOur Study\nWe present an evaluation suite centered on policy-grounded dialog tasks with dynamic adversaries.",
    "reason": "The span inventories red-teaming and mitigation approaches but does not connect them to the proposed evaluation suite or explain unresolved limitations, thus lacking synthesis per (a) and (c).",
    "start": 385,
    "end": 787,
    "label": "Lacks_synthesis"
  },
  {
    "span": "There are many recent works that explore temporal knowledge graph completion.",
    "document": "Related Work\n\nKnowledge graph completion (KGC) methods traditionally assume static relations and time-invariant entities. However, many real-world facts evolve, motivating temporal KGC models that encode timestamped events and relation dynamics. There are many recent works that explore temporal knowledge graph completion. These methods vary in how they represent time, from discrete snapshots to continuous-time processes, and in whether they model relation evolution explicitly. Our approach unifies these views via a contrastive temporal encoder that captures both short- and long-range temporal dependencies.",
    "reason": "Vague reference to 'many recent works' without any supporting citations.",
    "start": 246,
    "end": 323,
    "label": "Unsupported_claim"
  },
  {
    "span": "Rendle et al. (2009) present factorization machines for sparse features. Hidasi et al. (2015) use RNNs for session-based recommendation. Wu et al. (2019) develop graph convolutional networks for collaborative filtering. Kang and McAuley (2018) propose SASRec for sequential recommendation.",
    "document": "Related Work\n\nSequential and Graph-based Recommendation\n\nWe summarize methods that exploit temporal signals and graph structure. Rendle et al. (2009) present factorization machines for sparse features. Hidasi et al. (2015) use RNNs for session-based recommendation. Wu et al. (2019) develop graph convolutional networks for collaborative filtering. Kang and McAuley (2018) propose SASRec for sequential recommendation. Cold-start and debiasing have also been studied in complementary lines (Schein et al., 2002; Bonner and Vasile, 2018).\n\nOur study investigates contrastive pretraining that unifies sequence dependencies and user-item graph topology.",
    "reason": "Citations are enumerated without stating how they connect or contrast, resulting in abrupt topic shifts between factorization, RNN, GNN, and Transformer approaches.",
    "start": 129,
    "end": 418,
    "label": "Coherence"
  },
  {
    "span": "many previous coherence models are evaluated on AES and AWQ",
    "document": "Introduction\n\nCoherence describes the semantic relation between elements of a text. It recognizes how well a text is organized to convey the information to the reader effectively. Modeling coherence can be beneficial to any system which needs to process a text.\n\nRecent neural coherence models (Mesgar and Strube, 2018;Moon et al., 2019) encode the input document using large-scale pretrained language models (Peters et al., 2018). These neural models compute local coherence, semantic relations between items in adjacent sentences, on the basis of words and even sub-words.\n\nHowever, it has been unclear on which basis these models compute local coherence. Jeon and Strube (2020) present a neural coherence model, which allows to interpret focus information for the first time. Their investigation reveals that neural models, adopting large-scale pretrained language models, frequently compute coherence on the basis of connections between any (sub-)words or function words. In these cases, the model might capture the focus based on spurious information. While such a model might reach or set the state of the art in some end applications, it will do so for the wrong reasons from a linguistic perspective.\n\nThis problem did not appear with pre-neural models of coherence, since they compute coherence on the basis of entities. Early work about pronoun and anaphora resolution by Sidner (1981Sidner ( , 1983 assumes that there is one single salient entity in a sentence, its focus, which serves as a preferred antecedent for anaphoric expressions. Centering theory (Joshi and Weinstein, 1981;Grosz et al., 1995) builds on these insights and introduces an algorithm for tracking changes in focus. Centering theory serves as basis for many researchers to develop systems computing local coherence based on the approximations of entities (Barzilay and Lapata 2008;Feng and Hirst 2012;Guinaudeau and Strube 2013, inter alia).\n\nIn this paper, we propose a neural coherence model which is linguistically more sound than previously proposed neural coherence models. We compute coherence on the basis of entities by constraining our model to capture focus on noun phrases and proper names. This provides us with an explicit representation of the most important items in sentences, leading to the notion of focus. This brings our model linguistically in line with pre-neural models of coherence.\n\nOur approach is not only linguistically more sound but also is in accord with the recent empirical study by O'Connor and Andreas (2021) who investigate what contextual information contributes to accurate predictions in transformer-based language models. Their experiments show that most usable information is captured by nouns and verbs. Their findings suggest that we can design better neural models by focusing on specific context words. Our work follows their findings by modeling entitybased coherence in an end-to-end framework to improve a neural coherence model.\n\nOur model integrates a local coherence module with a component which takes context into account. Our model first encodes a document using a pretrained language model and identifies entities using an linguistic parser. The local coherence module captures the most related representations of entities between adjacent sentences, the local focus. Then it tracks the changes of local foci. The second component captures the context of a text by averaging sentence representations.\n\nWe evaluate our model on three downstream tasks: automated essay scoring (AES), assessing writing quality (AWQ), and assessing discourse coherence (ADC). AES and AWQ determine text quality for a given text, aiming to replicate human scoring results. Since coherence is an essential factor in assessing text quality, many previous coherence models are evaluated on AES and AWQ. ADC evaluates coherence models on informal texts such as emails and online reviews. In our evaluation, our model achieves state-of-the-art performance.\n\nWe also perform a series of analyses to investigate how our model works. Our analyses show that capturing focus on entities gives us better insight into the behaviour of the model, leading to better explainability. Using this information, we examine the statistical differences of texts assigned to different qualities. From the perspective of local coherence, we find that texts of higher quality are neither semantically too consistent nor too variant. Finally, we inspect error cases to investigate how the models achieve their performance differently.\n\n ",
    "start": 3755,
    "end": 3814,
    "label": "Unsupported_claim"
  },
  {
    "span": "Downs et al. (2010) used gold questions to screen workers. Waggoner and Chen (2014) developed peer prediction to elicit truthful reporting without ground truth. Whitehill et al. (2009) modeled annotator expertise with GLAD. Yin et al. (2016) tested real-time interventions to improve effort.",
    "document": "Related Work\n\nQuality control in crowdsourcing encompasses screening, modeling annotator behavior, and incentivizing effort and honesty. Various mechanisms exist, spanning ex-ante and ex-post strategies.\n\nDowns et al. (2010) used gold questions to screen workers. Waggoner and Chen (2014) developed peer prediction to elicit truthful reporting without ground truth. Whitehill et al. (2009) modeled annotator expertise with GLAD. Yin et al. (2016) tested real-time interventions to improve effort.\n\nWe target a complementary angle by combining skill estimation with adaptive task routing in real time.",
    "reason": "The cited works are presented as isolated statements with no transitions or explanation of how they relate to each other, producing an abrupt, incoherent sequence.",
    "start": 205,
    "end": 496,
    "label": "Coherence"
  },
  {
    "span": "(Miller, Jones and Clark, 2021)",
    "document": "Introduction\n\nExplainable recommendation systems seek to justify predictions with human-interpretable evidence. Early models used feature attribution and rule lists, while more recent approaches generate textual rationales or highlight user–item aspects inferred from reviews. Aspect-based explanations can improve user trust and perceived usefulness, but there is tension between fidelity and simplicity. Comparative evaluations indicate that joint reasoning over reviews and graphs yields better coverage of fine-grained preferences (Miller, Jones and Clark, 2021), though standardization of human evaluation protocols remains an open problem (Zhang and Chen, 2020).",
    "reason": "In parenthetical citations, common styles use “&” before the last author. It should be “(Miller, Jones & Clark, 2021)” or use narrative form with “and” outside parentheses.",
    "start": 535,
    "end": 566,
    "label": "Format"
  },
  {
    "span": "Graph convolutional networks tailored to road topology, diffusion convolution, and attention-based temporal encoders form the basis of modern traffic predictors and achieve state-of-the-art on METR-LA and PEMS datasets (Yu et al., 2018; Li et al., 2018; Wu et al., 2019; Guo et al., 2019). Hybrid models integrate external factors like weather and events using embedding modules (Zheng et al., 2020; Cui et al., 2020).",
    "document": "Related Work\n\nShort-term traffic forecasting relies on modeling both spatial dependencies across the road network and temporal dynamics in sensor streams. A variety of graph and sequence models have been proposed.\n\nGraph convolutional networks tailored to road topology, diffusion convolution, and attention-based temporal encoders form the basis of modern traffic predictors and achieve state-of-the-art on METR-LA and PEMS datasets (Yu et al., 2018; Li et al., 2018; Wu et al., 2019; Guo et al., 2019). Hybrid models integrate external factors like weather and events using embedding modules (Zheng et al., 2020; Cui et al., 2020).\n\nOur work addresses deployment-time sparsity when sensors intermittently fail. We propose an imputation-aware training scheme that couples masked graph modeling with uncertainty-weighted loss.",
    "reason": "The span only catalogues prior architectures and benchmarks, without linking them to the problem of deployment-time sparsity or articulating a gap, matching (a) and (b).",
    "start": 215,
    "end": 633,
    "label": "Lacks_synthesis"
  },
  {
    "span": "safety fine-tuning degrades task performance on knowledge grounding",
    "document": "Introduction\n\nConversational agents must balance helpfulness with safety to avoid generating harmful content. Alignment techniques often impose behavioral constraints that may conflict with task utility, raising concerns about trade-offs.\n\nRelated Work\n\nInstruction tuning and reinforcement learning from human feedback improve coherence and adherence to norms, while safety-specific objectives reduce toxic outputs. Prior studies suggest that safety fine-tuning degrades task performance on knowledge grounding, prompting research on multi-objective optimization and selective application of safety policies. Yet, there is little consensus on standardized evaluation protocols that reflect real-world use.",
    "reason": "Claims a specific prior finding without citation, violating rule (b) and (d) for 'prior studies' lacking references.",
    "start": 444,
    "end": 511,
    "label": "Unsupported_claim"
  },
  {
    "span": "Harris et al. 2",
    "document": "Related Work\n\nAffective computing leverages multimodal cues from speech, text, and physiology to infer human emotions (Picard, 1997; Poria et al., 2017). Self-supervised pretraining on large speech corpora has advanced paralinguistic tasks by improving representations of prosody and timbre (Schneider et al., 2019; Shor et al., 2020). Lexicon-based features can complement embeddings by providing interpretable signals (Mohammad & Turney, 2013; Hutto & Gilbert, 2014). Clinical annotation guidelines derived by Harris et al. 2 also inform our protocol, though we adapt them for conversational settings with multi-label dynamics (Böck et al., 2020; Kim et al., 2021). We further consider temporal modeling with hierarchical attention to capture context shifts in long dialogues (Yang et al., 2016; Zhang et al., 2018).",
    "reason": "Wrong use of footnotes/numbering in an author–year citation; should include the year (e.g., 'Harris et al. (YEAR)') or be formatted as a proper footnote.",
    "start": 512,
    "end": 527,
    "label": "Format"
  },
  {
    "span": "Smith et al., 2019)",
    "document": "Introduction\n\nDistant supervision for relation extraction reduces annotation costs by aligning knowledge base triples with text (Mintz et al., 2009; Zeng et al., 2015). However, noise from heuristic alignment leads to label uncertainty that degrades generalization (Feng et al., 2018). Recent work explores curriculum schedules and posterior regularization to denoise training signals as demonstrated by Smith et al., 2019) in large-scale biomedical corpora. We extend these techniques by incorporating instance-dependent noise models estimated via small clean subsets (Varma and Ré, 2018; Karamanolakis et al., 2021).\n\nOur experiments on TACRED and ChemProt show consistent improvements over strong baselines while reducing variance across random seeds.",
    "reason": "Missing opening parenthesis for a parenthetical citation; should be '(Smith et al., 2019)'.",
    "start": 404,
    "end": 423,
    "label": "Format"
  },
  {
    "span": "Bolukbasi et al. (2016) showed gender bias in word embeddings. Caliskan et al. (2017) measured implicit associations with WEAT. Zhao et al. (2017) documented gender bias in coreference resolution. Sun et al. (2019) surveyed mitigation strategies.",
    "document": "Introduction and Related Work\n\nBias and Fairness in NLP\nBias in language technologies can propagate societal harms and disparities (Blodgett et al., 2020). Measuring and mitigating these behaviors has become a central concern across tasks and model families.\n\nEvidence of Bias and Early Diagnostics\nWork has identified representational harms in static embeddings and downstream errors in structured prediction. Bolukbasi et al. (2016) showed gender bias in word embeddings. Caliskan et al. (2017) measured implicit associations with WEAT. Zhao et al. (2017) documented gender bias in coreference resolution. Sun et al. (2019) surveyed mitigation strategies.\n\nMitigation in Contextual Models\nSubsequent approaches target debiasing for contextual representations via counterfactual data augmentation, parameter regularization, and adversarial objectives (Zhao et al., 2018; Zhao and Chang, 2020; Ravfogel et al., 2020). Our work provides task-level calibration complementary to representation-level interventions.",
    "reason": "The span lists multiple works without explicitly connecting them, such as how embedding bias findings relate to coreference bias or how a survey synthesizes these findings. The transitions are minimal and the relationships are only implied.",
    "start": 411,
    "end": 657,
    "label": "Coherence"
  },
  {
    "span": "Domain adaptation for neural machine translation includes fine-tuning on in-domain parallel data to specialize general models. Multi-domain training mixes corpora with sampling strategies and domain tags. Adapters provide lightweight specialization layers that can be toggled per domain. Data augmentation leverages back-translation and self-training to expand in-domain coverage. In our study we train domain-specific adapters with a simple sampling scheme.",
    "document": "Introduction\n\nNeural machine translation performance can degrade sharply when test data diverge from the training distribution. Domain adaptation techniques aim to recover performance with minimal in-domain supervision and limited computational overhead.\n\nDomain adaptation for neural machine translation includes fine-tuning on in-domain parallel data to specialize general models. Multi-domain training mixes corpora with sampling strategies and domain tags. Adapters provide lightweight specialization layers that can be toggled per domain. Data augmentation leverages back-translation and self-training to expand in-domain coverage. In our study we train domain-specific adapters with a simple sampling scheme.\n\nWe evaluate on legal, medical, and IT domains and report both BLEU and domain-specific terminology accuracy.",
    "reason": "The passage catalogs prior adaptation strategies and then states the authors' plan without articulating a concrete shortcoming in existing methods or a motivating hypothesis that connects prior work to the proposed approach.",
    "start": 256,
    "end": 714,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Robust features and speech enhancement reduce the impact of additive and reverberant noise on recognition (Sainath et al., 2013; Xu et al., 2015). Domain-adversarial training mitigates mismatches between train and test distributions (Ganin et al., 2016; Shinohara, 2016). Few-shot speaker adaptation updates small parameter subsets to specialize models (Huang et al., 2020; Winata et al., 2020).",
    "document": "Related Work\n\nNoise-Robust Automatic Speech Recognition\n\nEnd-to-end ASR models have achieved strong performance under clean conditions but remain vulnerable to environmental noise and channel variability (Amodei et al., 2016; Hannun et al., 2014). Research spans acoustic modeling, front-end enhancement, and domain adaptation to improve robustness.\n\nRobust features and speech enhancement reduce the impact of additive and reverberant noise on recognition (Sainath et al., 2013; Xu et al., 2015). Domain-adversarial training mitigates mismatches between train and test distributions (Ganin et al., 2016; Shinohara, 2016). Few-shot speaker adaptation updates small parameter subsets to specialize models (Huang et al., 2020; Winata et al., 2020).\n\nCurriculum learning and multi-condition training further improve generalization across SNRs, while self-supervised pretraining yields noise-robust representations (Park et al., 2019; Baevski et al., 2020). Yet, systematic evaluation across realistic, time-varying noise profiles is still limited.",
    "reason": "The sequence introduces enhancement, domain adaptation, and speaker adaptation as isolated points without transitions or explicit links, leaving the relationships between cited works implied rather than stated.",
    "start": 351,
    "end": 746,
    "label": "Coherence"
  },
  {
    "span": "(Johnson et al.)",
    "document": "Related Work\n\nContrastive learning objectives have shown strong transfer by maximizing agreement across views (Chen et al., 2020; He et al., 2020). Extensions incorporate hard negatives and supervised signals to improve alignment (Khosla et al., 2020; Robinson et al., 2021). A concurrent line studies data augmentations and invariances critical for representation quality (Tian et al., 2020; Grill et al., 2020). Early work (Johnson et al.) explored pretext tasks for image-text alignment, but did not address cross-modal retrieval at scale.\n\nWe build upon recent multimodal transformers and analyze text-conditioned negatives under domain shift.",
    "reason": "Parenthetical citation missing the year; APA-style requires author and year inside parentheses.",
    "start": 425,
    "end": 441,
    "label": "Format"
  },
  {
    "span": "(Johnson, 2022",
    "document": "Related Work\n\nSelf-supervised learning has advanced automatic speech recognition (ASR) by pretraining encoders on large unlabeled audio corpora. Contrastive objectives and masked prediction losses enable robust acoustic representations transferable to low-resource languages. Recent surveys (Johnson, 2022 discuss pretraining strategies and downstream adaptation via lightweight adapters (Chen and Wang, 2021; Ali et al., 2022). Building on these insights, we introduce a multi-granular masking scheme that aligns phonetic and prosodic cues during pretraining.",
    "reason": "Missing closing parenthesis in the citation. It should be closed as \"(Johnson, 2022)\".",
    "start": 291,
    "end": 305,
    "label": "Format"
  },
  {
    "span": "Competitions on bias detection have standardized evaluation protocols for gender and race.",
    "document": "Introduction\n\nFairness in natural language processing examines how models amplify or mitigate social biases across demographic attributes. Research spans dataset curation, bias measurement, debiasing algorithms, and fairness-aware training objectives. As models scale, subtle forms of stereotyping and representation harm become more prominent.\n\nBenchmarks and shared competitions provide common testbeds for measuring progress and reproducibility. Competitions on bias detection have standardized evaluation protocols for gender and race. Despite this, gaps persist for other attributes, and cross-linguistic fairness remains underexplored.\n\nIn this work, we benchmark debiasing methods across multiple languages and attributes under a unified protocol. We analyze trade-offs between fairness and utility, and we release tools for auditing bias in multilingual settings.",
    "reason": "Asserts the role of competitions in standardizing protocols without citing any specific competitions (violates rule a/d; first mention of competitions and claim about their impact needs references).",
    "start": 449,
    "end": 539,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Clark and Lewis, 2018)",
    "document": "Introduction\n\nSyntactic parsing has progressed from classical probabilistic models to neural architectures that integrate context and subword information (Klein & Manning, 2003; Dozat & Manning, 2017; Kitaev & Klein, 2018). Pretrained language models further improve parsing accuracy via contextualized representations (Devlin et al., 2019; Kitaev et al., 2019). Despite these advances, domain robustness remains a challenge, motivating semi-supervised and self-training approaches (McClosky et al., 2006; Yu et al., 2020). Our approach revisits grammar induction signals inspired by classical work (Clark and Lewis, 2018) and integrates them with modern encoders for improved generalization (He et al., 2018; Smith et al., 2020).",
    "reason": "Wrong conjunction inside a parenthetical citation for APA style; should use an ampersand: '(Clark & Lewis, 2018)'.",
    "start": 599,
    "end": 622,
    "label": "Format"
  },
  {
    "span": "(Nguyen et al., 2020.",
    "document": "Introduction\n\nBootstrapped training schemes iteratively expand labeled sets using high-confidence predictions. While effective, this can entrench spurious correlations unless calibration and diversity constraints are enforced. As shown in (Nguyen et al., 2020. subsequent work introduces temperature scaling and ensemble agreement to temper overconfident predictions (Gupta et al., 2021). Parallel advances in representation learning emphasize domain-invariant features to generalize across shifts (Arora et al., 2019; Kim & Lee, 2020).\n\nOur study builds on these ideas by combining calibrated pseudo-labeling with diversity-aware sampling, yielding improved stability in low-data regimes.",
    "reason": "Missing closing parenthesis in the parenthetical citation; should be \"(Nguyen et al., 2020).\"",
    "start": 239,
    "end": 260,
    "label": "Format"
  },
  {
    "span": "Existing approaches reduce communication via quantization and sparsification, and protect privacy with secure aggregation and differential privacy (Konečný et al., 2016; Suresh et al., 2017; Bonawitz et al., 2017; McMahan et al., 2018). In this paper, we propose a federated optimizer that combines secure aggregation with adaptive compression to improve efficiency and privacy.",
    "document": "Related Work\n\nFederated learning enables collaborative training without centralizing raw data, but communication efficiency and privacy protection remain key deployment bottlenecks. Prior art has pursued compression of model updates and cryptographic protocols to mitigate these issues.\n\nExisting approaches reduce communication via quantization and sparsification, and protect privacy with secure aggregation and differential privacy (Konečný et al., 2016; Suresh et al., 2017; Bonawitz et al., 2017; McMahan et al., 2018). In this paper, we propose a federated optimizer that combines secure aggregation with adaptive compression to improve efficiency and privacy.\n\nWe evaluate our method across heterogeneous client scenarios and show that it maintains accuracy under tight bandwidth budgets while meeting strict privacy accounting constraints.",
    "reason": "The span jumps from listing prior work directly to the authors’ contribution without explicitly identifying a gap or shortcoming the new method addresses, failing to synthesize literature into a motivating argument.",
    "start": 288,
    "end": 666,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Prior work has released sentiment lexicons for more than 60 low-resource languages.",
    "document": "Related Work\n\nMultilingual sentiment analysis has benefited from cross-lingual transfer, adversarial alignment, and multilingual pretrained encoders. Resources remain scarce for truly low-resource languages, where labeled data and sentiment lexicons are limited or noisy. Prior work has released sentiment lexicons for more than 60 low-resource languages. Parallel corpora and bilingual dictionaries have been leveraged to project annotations and bootstrap classifiers. We focus on lexicon induction from monolingual web text via few-shot prompting and evaluate downstream on document-level sentiment in unseen languages.",
    "reason": "Asserts the existence and scale of prior resources without citing the works that released them; per rule (a) this prior work must be cited.",
    "start": 272,
    "end": 355,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT has been successfully applied to automated essay scoring on TOEFL prompts.",
    "document": "Related Work\n\nAutomated essay scoring (AES) methods have transitioned from handcrafted features and classical regression to deep neural architectures. Recent approaches leverage contextual embeddings to model coherence, argument structure, and prompt adherence. BERT has been successfully applied to automated essay scoring on TOEFL prompts. Nevertheless, performance often degrades when prompts change, highlighting sensitivity to topic distribution and stylistic factors. To address cross-prompt generalization, researchers have explored domain adaptation and multi-task learning with auxiliary linguistic signals such as discourse markers and error annotations.\n",
    "reason": "Claims prior application and success of a model on a specific dataset (TOEFL prompts) without any supporting citation.",
    "start": 262,
    "end": 341,
    "label": "Unsupported_claim"
  },
  {
    "span": "Graph neural networks for molecular property prediction encompass message passing (Gilmer et al., 2017), spectral methods (Defferrard et al., 2016), and attention mechanisms (Velickovic et al., 2018).",
    "document": "Related Work\n\nGNNs for Molecules\nLearning molecular representations directly from graphs has led to substantial gains over hand-crafted fingerprints. Graph neural networks for molecular property prediction encompass message passing (Gilmer et al., 2017), spectral methods (Defferrard et al., 2016), and attention mechanisms (Velickovic et al., 2018). Pretraining strategies on large unlabeled corpora of molecules have also emerged, including contrastive and generative objectives.\n\nPhysics-Informed Models\nHybrid approaches integrate domain priors such as 3D geometry, quantum-derived features, or partial charges to improve generalization in low-data regimes.\n\nData Splits and Evaluation\nScaffold splits and time-based splits have been recommended to better measure out-of-distribution performance, but practice remains inconsistent across studies.",
    "reason": "The sentence catalogs prior approaches without indicating how they relate to the authors' problem setting or what deficiency motivates the present work (criterion a and c).",
    "start": 150,
    "end": 350,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent works overwhelmingly focus on urban scenes captured in North America and Europe.",
    "document": "Introduction\n\nVision Transformers (ViTs) have demonstrated strong performance in dense prediction tasks by leveraging global receptive fields and large-scale pre-training (Dosovitskiy et al., 2021; Liu et al., 2021). In aerial and satellite imagery, ViT-based segmentation has been explored for building extraction, road mapping, and land-cover classification, often in conjunction with multi-spectral inputs (Ronneberger et al., 2015; Marmanis et al., 2018; Wang et al., 2022). Benchmarks such as DOTA and xView provide large annotated corpora for object detection from overhead imagery (Xia et al., 2018; Lam et al., 2018).\n\nRecent works overwhelmingly focus on urban scenes captured in North America and Europe. This geographic skew can limit model generalization to rural and informal settlements that exhibit different morphology and material signatures. We introduce a cross-region benchmark and a domain-adaptive ViT to mitigate this bias by aligning representations across diverse geographies.",
    "reason": "The phrase 'Recent works' claims a trend in the literature but provides no citations; per the guideline, such mentions require references to support the assertion.",
    "start": 627,
    "end": 714,
    "label": "Unsupported_claim"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nAbstractive summarization has progressed rapidly with encoder–decoder transformers pre-trained on massive corpora (Lewis et al., 2020; Raffel et al., 2020). Content selection remains challenging, especially under domain shift and factuality constraints (Maynez et al., 2020; Goyal and Durrett, 2021). Several approaches enforce faithfulness via constrained decoding and post-editing with entailment models (Kryscinski et al., 2020; Honovich et al., 2021). Unlike prior extract-and-edit pipelines, our method jointly plans and realizes summaries using discourse signals learned from weak supervision, improving both coverage and faithfulness as demonstrated in [12].\n",
    "reason": "Wrong citation style: numeric bracket “[12]” is inconsistent with the surrounding author–year style used elsewhere in the document.",
    "start": 674,
    "end": 678,
    "label": "Format"
  },
  {
    "span": "Statistical downscaling spans bias correction and spatial disaggregation (Wood et al., 2004; Michelangeli et al., 2009), quantile mapping (Themeßl et al., 2012), and weather generators (Kilsby et al., 2007). Dynamical downscaling leverages regional climate models nested within GCMs (Giorgi, 2019; Samuelsson et al., 2011). Deep learning variants include CNNs and GANs for super-resolution (Vandal et al., 2018; Stengel et al., 2020).",
    "document": "Introduction\n\nHigh-resolution climate projections are needed for local risk assessment and adaptation planning. General circulation models operate at coarse scales, motivating downscaling methods that translate large-scale fields into local variables.\n\nStatistical downscaling spans bias correction and spatial disaggregation (Wood et al., 2004; Michelangeli et al., 2009), quantile mapping (Themeßl et al., 2012), and weather generators (Kilsby et al., 2007). Dynamical downscaling leverages regional climate models nested within GCMs (Giorgi, 2019; Samuelsson et al., 2011). Deep learning variants include CNNs and GANs for super-resolution (Vandal et al., 2018; Stengel et al., 2020).\n\nWe study uncertainty-aware deep downscaling for precipitation extremes with physics-informed constraints and evaluate out-of-sample generalization across basins and emission scenarios.\n",
    "reason": "The span catalogs approaches without clarifying their limitations relative to extremes, nor how the proposed method addresses those, indicating a lack of synthesis as in (a) and (c).",
    "start": 253,
    "end": 687,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Abadi et al. (2016) introduced differentially private stochastic gradient descent. Papernot et al. (2017) proposed PATE for private aggregation of teacher ensembles. McMahan et al. (2018) studied federated averaging at scale. Kairouz et al. (2021) surveyed advances in federated learning.",
    "document": "Related Work\n\nPrivacy in Machine Learning\n\nProtecting user data during model training has led to techniques grounded in differential privacy (DP) and federated learning (FL). While these paradigms can be combined, most prior studies evaluate them in isolation under incomparable budgets and data heterogeneity.\n\nAbadi et al. (2016) introduced differentially private stochastic gradient descent. Papernot et al. (2017) proposed PATE for private aggregation of teacher ensembles. McMahan et al. (2018) studied federated averaging at scale. Kairouz et al. (2021) surveyed advances in federated learning.\n\nAccounting and Composition\n\nRecent work refines privacy accounting via tighter composition and subsampling amplification, yet practical deployments must reconcile accounting assumptions with non-iid participation and stragglers.\n\nOur Focus\n\nWe develop a unified accounting framework for cross-device FL with DP that aligns sampling assumptions with real participation traces, yielding end-to-end privacy guarantees.",
    "reason": "The cited works are listed as separate sentences without transitions or explicit explanation of their relationships, causing abrupt shifts between DP and FL with unclear linkage.",
    "start": 312,
    "end": 600,
    "label": "Coherence"
  },
  {
    "span": "Most recent works on task-oriented dialogue rely on synthetic conversations rather than real human-human interactions.",
    "document": "Introduction\n\nTask-oriented dialogue (TOD) systems aim to help users complete goals such as booking flights or making restaurant reservations. Recent neural approaches combine pretrained language models with symbolic planners to handle multi-domain interactions [1,2]. Despite progress, data scarcity and domain shift remain challenges [3]. Most recent works on task-oriented dialogue rely on synthetic conversations rather than real human-human interactions. This gap can lead to over-optimistic estimates of robustness and poor transfer to production settings.",
    "reason": "Claims about 'recent works' without providing citations; first mention of this body of work should be supported.",
    "start": 341,
    "end": 459,
    "label": "Unsupported_claim"
  },
  {
    "span": "Ribeiro et al. (2016) proposed LIME to produce local linear explanations around predictions. Lundberg and Lee (2017) introduced SHAP values. Clinical decision support systems integrate risk scores into workflows (Escobar et al., 2017). Counterfactual explanations have been explored to suggest minimal changes (Wachter et al., 2018).",
    "document": "Introduction\n\nExplainable AI (XAI) is increasingly vital in clinical decision support, where model transparency and reliability affect trust, safety, and regulatory compliance. Techniques for local and global interpretability are being evaluated for their ability to communicate risks and limitations to clinicians.\n\nRibeiro et al. (2016) proposed LIME to produce local linear explanations around predictions. Lundberg and Lee (2017) introduced SHAP values. Clinical decision support systems integrate risk scores into workflows (Escobar et al., 2017). Counterfactual explanations have been explored to suggest minimal changes (Wachter et al., 2018).\n\nIn this work, we examine how explanation stability interacts with triage thresholds in sepsis prediction and introduce a calibration-aware explainer selection framework.",
    "reason": "The span lists methods (LIME, SHAP), then jumps to clinical workflow integration and back to counterfactuals, without transitions or clarifying how these items relate to one another.",
    "start": 317,
    "end": 650,
    "label": "Coherence"
  },
  {
    "span": "There are many recent works that leverage transformer-based encoders for fake news detection.",
    "document": "Introduction\n\nOnline misinformation has emerged as a global concern, motivating automated methods for detecting fake news across platforms and languages. Early approaches focused on surface linguistic cues and source reliability, while subsequent models incorporated discourse structure and propagation patterns. There are many recent works that leverage transformer-based encoders for fake news detection. Despite progress, generalization across topics and events remains limited, and cross-domain robustness is poorly understood. In this paper, we study domain-invariant representations for stance-informed deception cues in news articles.",
    "reason": "Mentions 'many recent works' without providing any citations to support the claim.",
    "start": 313,
    "end": 406,
    "label": "Unsupported_claim"
  },
  {
    "span": "Shen et al. (2023) introduce a planner-executor agent that queries external tools to decompose complex tasks. Toolformer (Schick et al., 2023) fine-tunes a model to call APIs during generation by self-labeling tool-use opportunities. RETRO retrieves chunks from a large datastore to condition generation (Borgeaud et al., 2022).",
    "document": "Related Work\n\nTool-Augmented Language Models\n\nLarge language models increasingly integrate external tools and retrieval to extend their capabilities beyond next-token prediction. Early retrieval-augmented generation connects parametric knowledge with nonparametric sources to improve factuality and grounding. Shen et al. (2023) introduce a planner-executor agent that queries external tools to decompose complex tasks. Toolformer (Schick et al., 2023) fine-tunes a model to call APIs during generation by self-labeling tool-use opportunities. RETRO retrieves chunks from a large datastore to condition generation (Borgeaud et al., 2022). Despite the proliferation of techniques, a unified view on when to plan, retrieve, or call tools remains limited.\n\nEvaluation of Tool Use\n\nBenchmarks for tool use emphasize compositional reasoning and access to updated knowledge. However, differences in interface assumptions and latency constraints complicate direct comparison across methods. Our work contributes a controlled evaluation of planning depth and retrieval frequency under resource limits.",
    "reason": "Multiple sentences list distinct works without transitions or explicit relationships, making it unclear how each approach relates to the others (abrupt connection, implied relation only).",
    "start": 310,
    "end": 638,
    "label": "Coherence"
  },
  {
    "span": "State-of-the-art clinical NER models already surpass 90% F1 on standard i2b2 problems.",
    "document": "Introduction\n\nClinical named entity recognition (NER) is foundational for structuring information in electronic health records, enabling downstream phenotyping and cohort discovery. Advances in domain-adaptive language modeling have further improved extraction quality.\n\nState-of-the-art clinical NER models already surpass 90% F1 on standard i2b2 problems. Despite high aggregate scores, portability across institutions remains challenging due to documentation practices and annotation drift.",
    "reason": "Provides a concrete performance statistic on a specific benchmark without citation; per rule (b), such quantitative claims require evidence.",
    "start": 271,
    "end": 357,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works have widely adopted contrastive pre-training for tabular representation learning.",
    "document": "Related Work\n\nRepresentation learning for tabular data has historically lagged behind vision and language, with many methods relying on gradient-boosted decision trees and ensembling (Ke et al., 2017; Prokhorenkova et al., 2018). Neural approaches have explored entity embeddings, attention over features, and self-supervision through missing-value reconstruction (Guo and Berkhahn, 2016; Arik and Pfister, 2021). Recent works have widely adopted contrastive pre-training for tabular representation learning. At the same time, augmentation strategies for tables remain underexplored relative to image or text domains (Chen et al., 2020). Our method unifies masking-based and contrastive objectives and introduces semantically grounded perturbations for numerical and categorical fields.",
    "reason": "Mentions 'recent works' adopting a method without citing any of them, which requires references per the definition.",
    "start": 414,
    "end": 508,
    "label": "Unsupported_claim"
  },
  {
    "span": "[Basu et al., 2016]",
    "document": "Related Work\n\nActive learning selects informative instances to reduce labeling cost (Settles, 2010). Uncertainty sampling and query-by-committee are classical strategies that remain competitive in modern neural settings (Lewis and Gale, 1994; Freund et al., 1997). Recent research investigates batch selection under diversity constraints to avoid redundancy (Ash et al., 2020). Some studies adopt Bayesian approximations for uncertainty estimation [Basu et al., 2016] while others leverage gradient embeddings for core-set selection (Sener and Savarese, 2018). Our method combines epistemic uncertainty with coverage guarantees in a unified acquisition objective.",
    "reason": "Wrong bracket style for APA-like in-text citation: uses square brackets instead of parentheses; should be '(Basu et al., 2016)'.",
    "start": 448,
    "end": 467,
    "label": "Format"
  },
  {
    "span": "[12]",
    "document": "Related Work\n\nNeural code search aligns natural language queries with code snippets using joint embeddings and interaction models (Gu et al., 2018; Cambronero et al., 2019). Early dual-encoder approaches scale well but miss fine-grained token interactions, motivating cross-encoders with late interaction (Nogueira and Cho, 2019; Khattab and Zaharia, 2020). Pretraining on code corpora with identifier-aware masking further improves retrieval (Feng et al., 2020; Guo et al., 2022).\n\nHard-negative mining and in-batch contrast improve top-k recall, while reranking leverages structural signals from ASTs (Sachan et al., 2021; Zhu et al., 2022). Context-aware methods incorporate repository history and dependency graphs (Yao et al., 2019; Wang et al., 2021). Some systems report gains from hybrid lexical–neural ensembles [12], though reproducibility varies across languages and frameworks (Huang et al., 2021).\n\nOur contribution is a compositional retriever with code-aware tokenization and hierarchical pooling, achieving strong performance under zero-shot language transfer and long-context scenarios.",
    "reason": "Numeric bracket citation used in an author–year context; should be an author–year citation or consistently numeric with a bibliography.",
    "start": 821,
    "end": 825,
    "label": "Format"
  },
  {
    "span": "Lopez et al.",
    "document": "Introduction\n\nNeural text generation has rapidly improved due to advances in large-scale pretraining and fine-tuning regimes. Foundational studies demonstrate that scaling both data and parameters leads to emergent capabilities in language understanding and generation, while careful regularization prevents overfitting in low-resource scenarios. Brown et al. (2021) argue that instruction-tuning can bridge the gap between pretraining objectives and downstream tasks. Others focus on efficiency and adaptation (Ahmed & Sun, 2019; Lee et al., 2020), reporting strong performance with task-specific adapters.\n\nFollowing Lopez et al., we adopt an uncertainty-driven sampling criterion to reduce annotation costs in iterative model refinement. Our approach is complementary to curriculum learning strategies that reweight training examples by estimated difficulty (Wang & Zhao, 2020) and to data augmentation techniques that improve generalization under distribution shift (Rivera et al., 2020).",
    "reason": "Narrative citation is missing the publication year; should be formatted as a narrative citation with year, e.g., \"Lopez et al. (YEAR)\".",
    "start": 619,
    "end": 631,
    "label": "Format"
  },
  {
    "span": "The SemEval stance detection shared task established the standard evaluation protocol for this problem.",
    "document": "Related Work\n\nStance detection aims to determine whether a text expresses support, opposition, or neutrality toward a given target, and has applications in misinformation analysis and public opinion mining (Mohammad et al., 2016; Küçük and Can, 2020). Early approaches used feature-rich linear models, while recent transformer-based architectures leverage target-aware encoding and domain adaptation for social media (Augenstein et al., 2016; Ghosh et al., 2019; Sun et al., 2021).\n\nThe SemEval stance detection shared task established the standard evaluation protocol for this problem. Our work complements prior methods by introducing contrastively learned target representations that generalize across topics with minimal labeled data.",
    "reason": "References a specific shared task and claims it set the standard without citing the task description or results paper.",
    "start": 483,
    "end": 586,
    "label": "Unsupported_claim"
  },
  {
    "span": "Across benchmarks, more than 60% of models dramatically overfit and fail to generalize to new domains.",
    "document": "Introduction\n\nGeneralization beyond the training distribution is a central challenge in modern NLP. While pretraining and data augmentation improve robustness, models can still latch onto spurious correlations and shortcut features. Across benchmarks, more than 60% of models dramatically overfit and fail to generalize to new domains. This motivates methods that explicitly encourage invariance to domain-specific artifacts. We explore representation-level regularization to improve out-of-domain performance on classification and QA tasks.\n",
    "reason": "Presents a quantitative statistic about model performance ('more than 60%') without any citation or evidence.",
    "start": 233,
    "end": 335,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Chen et al., 2021;,",
    "document": "Introduction\n\nNeural machine translation (NMT) models have achieved strong performance with Transformer architectures (Vaswani et al., 2017; Chen et al., 2018). Domain shift remains a central challenge, leading to degraded quality on specialized corpora (Chu and Wang, 2018; Koehn and Knowles, 2017).\n\nTo improve robustness, researchers leverage data selection, multi-domain training, and adapters (Wang et al., 2020; Bapna and Firat, 2019). Prior work suggests that lexical diversity is crucial for generalization (Ott et al., 2018; Freitag et al., 2022; (Chen et al., 2021;, which we revisit with new diagnostics for subword vocabularies.\n\nWe introduce a frequency-aware objective that encourages broader coverage without sacrificing adequacy on rare constructions.",
    "reason": "Malformed parenthetical citation with extra semicolon/comma and a missing closing parenthesis; should be “(Chen et al., 2021)” within the list.",
    "start": 556,
    "end": 576,
    "label": "Format"
  },
  {
    "span": "Brown et al. 1",
    "document": "Introduction\n\nPrivacy-preserving machine learning seeks to protect sensitive data while maintaining predictive performance (Shokri and Shmatikov, 2015; Papernot et al., 2016). Differential privacy (Dwork et al., 2006) provides formal guarantees but can degrade accuracy when naively applied (Abadi et al., 2016).\n\nRecent approaches combine cryptographic techniques with noise injection to balance utility and privacy (Truex et al., 2019; Bonawitz et al., 2019). Brown et al. 1 propose a hybrid method, yet their evaluation focuses on small-scale datasets. In contrast, we examine large-scale vision and language tasks under realistic privacy budgets.\n\nOur results indicate that carefully tuned clipping and adaptive noise schedules recover much of the non-private baseline performance.",
    "reason": "Improper use of a footnote-style superscript/marker without a year or proper reference formatting; should be formatted as a standard citation or as a properly linked footnote.",
    "start": 462,
    "end": 476,
    "label": "Format"
  },
  {
    "span": "CLIP learns joint vision–language representations from image–text pairs at scale (Radford et al., 2021). AudioCLIP extends contrastive learning to the audio modality (Guzhov et al., 2021). Recent works explore instruction-tuned vision-language models (Liu et al., 2023).",
    "document": "Related Work\n\nMultimodal Representation Learning\n\nLearning aligned representations across modalities enables zero-shot transfer and cross-modal retrieval. Scaling data and objectives has been central to recent advances. CLIP learns joint vision–language representations from image–text pairs at scale (Radford et al., 2021). AudioCLIP extends contrastive learning to the audio modality (Guzhov et al., 2021). Recent works explore instruction-tuned vision-language models (Liu et al., 2023). However, how instruction tuning interacts with modality alignment and robustness to distribution shift remains underexplored.\n\nEvaluation Protocols\n\nZero-shot, few-shot, and retrieval metrics capture complementary capabilities. We standardize training data and evaluation splits to enable fair comparison.",
    "reason": "The span strings together separate lines of work without articulating their relationships or providing transitions, making the connections between them unclear.",
    "start": 220,
    "end": 490,
    "label": "Coherence"
  },
  {
    "span": "(Garcia et al., 2020;)",
    "document": "Introduction\n\nSelf-supervised visual representation learning reduces reliance on expensive annotations by leveraging pretext tasks (He et al., 2020; Chen et al., 2020). Contrastive objectives encourage invariance across views while preserving discriminative features (Grill et al., 2020; Caron et al., 2021). However, many methods still underperform supervised pretraining in data-scarce transfer settings.\n\nTo address this gap, multi-granularity pretext tasks combine instance- and cluster-level objectives to capture both fine and coarse semantics (Caron et al., 2020). Additionally, strong data augmentations and projector designs improve invariance but may discard task-relevant details if not balanced (Tian et al., 2020). Prior work also explores curriculum schedules to gradually increase augmentation difficulty (Garcia et al., 2020;).\n\nWe propose a curriculum-aware contrastive learner that adaptively tunes augmentation strength based on uncertainty, delivering superior linear and few-shot transfer on diverse target datasets.",
    "reason": "Trailing semicolon inside the parenthetical citation is a formatting error; should be '(Garcia et al., 2020)'.",
    "start": 820,
    "end": 842,
    "label": "Format"
  },
  {
    "span": "Numerous studies have demonstrated that GNNs consistently outperform CNNs on molecular property prediction.",
    "document": "Introduction\n\nDeep learning has become a cornerstone of modern chemoinformatics, enabling accurate prediction of molecular and material properties directly from structural representations. Early approaches relied on hand-crafted descriptors, but neural models now learn features end-to-end from graphs or sequences.\n\nNumerous studies have demonstrated that GNNs consistently outperform CNNs on molecular property prediction. Despite this trend, most existing methods still struggle to capture long-range interactions across molecular graphs. Recent work on message passing neural networks attempts to address these issues by aggregating local neighborhoods across multiple hops (e.g., virtual nodes, residual connections), while equivariant models incorporate geometric inductive biases to improve performance on 3D tasks.\n\nIn this paper, we propose a hierarchical graph transformer that couples local message passing with global attention to model multi-scale interactions. We evaluate on multiple public benchmarks and analyze robustness to distribution shift.",
    "reason": "Claims a broad trend ('Numerous studies...') about prior work without providing citations at first mention (rule a, d).",
    "start": 317,
    "end": 424,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works have shown that prompt tuning dramatically reduces the need for labeled data.",
    "document": "Related Work\n\nPrompt-based learning. Prompting reformulates downstream problems as conditional text completion, aligning training with the objectives of large language models. Soft and hard prompting strategies aim to elicit task-relevant behavior while minimizing additional parameters.\n\nRecent works have shown that prompt tuning dramatically reduces the need for labeled data. Approaches vary from discrete templates with verbalizers to continuous prompt vectors optimized jointly with the model. Despite promising results, sensitivity to prompt design and calibration remains a key limitation, particularly in low-resource and cross-domain settings.\n\nParameter-efficient adaptation. Alternatives such as adapters and low-rank updates target a similar goal of reducing fine-tuning overhead. These methods can be composed with prompting but impose different trade-offs in latency and memory.",
    "reason": "Mentions 'recent works' and makes a performance claim without providing citations to specific studies.",
    "start": 289,
    "end": 379,
    "label": "Unsupported_claim"
  },
  {
    "span": "the largest available corpus for Xhosa-English has only 500k sentence pairs",
    "document": "Introduction\n\nLow-resource machine translation remains challenging due to data scarcity, domain mismatch, and morphological complexity. Recent progress with multilingual pretrained encoders and back-translation has narrowed the gap for some languages, yet truly low-resource pairs still suffer from brittle performance and limited generalization.\n\nFor Bantu languages, the largest available corpus for Xhosa-English has only 500k sentence pairs, which constrains both supervised training and the effectiveness of synthetic data generation. To address these limitations, we propose a multilingual adaptation framework that leverages typological similarity and character-level transfer, combined with constrained decoding to maintain agreement with gloss-based dictionaries.",
    "reason": "This is a specific quantitative claim about dataset size without any citation or evidence (rule b).",
    "start": 369,
    "end": 444,
    "label": "Unsupported_claim"
  },
  {
    "span": "We follow the protocol introduced in the SemEval 2018 Task 7 for relation classification.",
    "document": "Introduction\n\nRelation classification has long served as a testbed for evaluating sentence-level reasoning under limited supervision. Earlier approaches relied on feature engineering and dependency paths, while recent neural methods leverage pretrained encoders and attention mechanisms to capture long-range context.\n\nWe follow the protocol introduced in the SemEval 2018 Task 7 for relation classification.\n\nThis setup enables comparisons with distant supervision and few-shot methods, and it provides a consistent evaluation across heterogeneous corpora. However, the standard does not prescribe negative sampling strategies, which can lead to instability across runs.",
    "reason": "References a specific shared task ('SemEval 2018 Task 7') without citing its official description paper at first mention (rule a).",
    "start": 319,
    "end": 408,
    "label": "Unsupported_claim"
  },
  {
    "span": "The HumanEval dataset is known to contain leakage from training corpora.",
    "document": "Introduction\n\nEvaluating code generation systems requires careful design of problems, tests, and contamination controls. Popular benchmarks emphasize functional correctness via unit tests and measure pass@k under various sampling strategies (Austin et al., 2021; Chen et al., 2021). Recent models leverage instruction tuning and reinforcement learning from human feedback to improve adherence to problem statements (Ouyang et al., 2022; Le et al., 2022).\n\nThe HumanEval dataset is known to contain leakage from training corpora. However, the extent and impact of potential contamination remain debated, and practices for deduplication and measurement vary across studies. To address these concerns, we introduce CleanEval, a benchmark constructed with strict provenance tracking, adversarial paraphrasing, and mutation-based test expansion. We further provide a contamination audit protocol and release tools to reproduce our checks.",
    "reason": "Makes a specific claim about a known dataset without providing evidence or a citation.",
    "start": 456,
    "end": 528,
    "label": "Unsupported_claim"
  },
  {
    "span": "In previous studies, adversarial training consistently improved robustness on sentiment analysis.",
    "document": "Related Work\n\nRobust text classification. Adversarial examples expose fragility in models trained on clean data. For sentiment analysis, character- and word-level perturbations can alter predictions while preserving meaning. Defense strategies span data augmentation, regularization, and certified robustness.\n\nIn previous studies, adversarial training consistently improved robustness on sentiment analysis. However, gains often come at the cost of clean accuracy and increased training time. Beyond perturbation-level defenses, recent research considers distribution shifts such as topic and platform changes, highlighting the need for comprehensive evaluation.\n\nContrastive learning. Representation-level approaches encourage invariance to semantically consistent edits, complementing token-level perturbation training.",
    "reason": "References 'previous studies' with a general performance claim but provides no citations to those works.",
    "start": 311,
    "end": 408,
    "label": "Unsupported_claim"
  },
  {
    "span": "In a previous study, the authors claim that back-translation accounts for most of the improvements in low-resource settings.",
    "document": "Introduction\n\nData augmentation via synthetic parallel data has become a cornerstone of neural machine translation, especially when bitext is scarce. Back-translation leverages target-side monolingual corpora to generate pseudo-source sentences that augment training and improve fluency and adequacy.\n\nIn a previous study, the authors claim that back-translation accounts for most of the improvements in low-resource settings. However, the extent to which the gains arise from noise robustness, domain coverage, or language modeling effects remains debated, and the interaction with scaling and regularization is not fully understood.\n\nWe investigate the sources of improvement by decoupling noise profiles from data quantity and propose a controlled experimental design that isolates each factor. Our analysis spans multiple language pairs and domains, with a focus on transferability across varying monolingual corpora.",
    "reason": "This sentence refers to a specific prior study and its claim without citing it, violating rule b and a (mention of previous study requires citation).",
    "start": 302,
    "end": 426,
    "label": "Unsupported_claim"
  },
  {
    "span": "Federated learning has been applied to cross-silo medical imaging with synchronous optimization, personalized heads, and secure aggregation (Li et al., 2020; Sheller et al., 2020; Chen et al., 2021). Studies report performance comparable to centrally trained baselines on MRI and CT tasks (Rieke et al., 2020; Kaissis et al., 2021).",
    "document": "Introduction\n\nDistributed training across hospitals promises privacy-preserving learning for medical imaging. However, non-iid distributions, scanner variability, and annotation sparsity complicate optimization and evaluation.\n\nFederated learning has been applied to cross-silo medical imaging with synchronous optimization, personalized heads, and secure aggregation (Li et al., 2020; Sheller et al., 2020; Chen et al., 2021). Studies report performance comparable to centrally trained baselines on MRI and CT tasks (Rieke et al., 2020; Kaissis et al., 2021).\n\nIn this paper, we explore federated robustness under extreme site imbalance and temporal drift. We present DriftFed, which leverages site-conditioned normalization and agreement-regularized aggregation to stabilize learning in the presence of distribution shifts.",
    "reason": "The span summarizes prior federated learning applications and results without linking them to the paper's focus on robustness or clarifying unmet needs, thus lacking synthesis under (a) and (c).",
    "start": 228,
    "end": 560,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Text augmentation techniques such as back-translation, synonym replacement, masked language model infilling, and embedding-space mixup have been used to improve generalization (Fadaee et al., 2017; Wei and Zou, 2019; Kobayashi, 2018; Guo et al., 2020).",
    "document": "Introduction\n\nGeneralization under distribution shift remains a core challenge for NLP models, particularly in low-resource and few-shot regimes.\n\nText augmentation techniques such as back-translation, synonym replacement, masked language model infilling, and embedding-space mixup have been used to improve generalization (Fadaee et al., 2017; Wei and Zou, 2019; Kobayashi, 2018; Guo et al., 2020). While these strategies increase data diversity, their effect on calibration and minority-class recall is less understood.\n\nWe propose TargetMix, a label-aware augmentation policy that conditions token-level perturbations on class-specific confusion patterns, yielding consistent gains in minority-class F1 and expected calibration error.",
    "reason": "The span lists prior augmentation methods but does not explain how they relate to the paper’s calibration-oriented objective or what motivates the new policy.",
    "start": 147,
    "end": 399,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In a previous study, the authors claim that batch normalization completely eliminates internal covariate shift in deep networks.",
    "document": "Related Work\n\nNormalization techniques have become a cornerstone of deep vision systems by stabilizing gradients and accelerating convergence. Variants such as batch, layer, instance, and group normalization modify intermediate statistics to control activation distributions during training. These methods have been integrated into architectures from residual networks to vision transformers.\n\nIn a previous study, the authors claim that batch normalization completely eliminates internal covariate shift in deep networks. Subsequent analyses have debated the exact mechanism by which normalization improves optimization, suggesting alternative explanations based on smoothing of the loss landscape and better conditioning. Nevertheless, normalization layers continue to be standard components in high-performing image classification and detection pipelines.\n\nOur work revisits normalization from the lens of scale-invariance and reparameterization, evaluating its impact on data-efficient training and robustness.",
    "reason": "Refers to a prior study and its claim without citing the study.",
    "start": 394,
    "end": 522,
    "label": "Unsupported_claim"
  },
  {
    "span": "A growing body of recent work explores debiasing GNNs under various definitions of fairness.",
    "document": "Related Work\n\nGraph neural networks (GNNs) achieve strong performance on node classification, link prediction, and graph-level tasks but can inherit and amplify societal biases present in the data. Fairness in graphs spans multiple axes, including demographic parity, equal opportunity, and counterfactual invariance. A growing body of recent work explores debiasing GNNs under various definitions of fairness. Techniques include adversarial regularization, reweighting of sensitive neighborhoods, and fairness-aware message passing. Yet, evaluation remains fragmented, with heterogeneous metrics, sensitive attribute availability, and task settings that hinder direct comparison.\n",
    "reason": "The mention of 'recent work' without citing any of the studies violates the requirement to support such claims with references (rule d).",
    "start": 318,
    "end": 410,
    "label": "Unsupported_claim"
  },
  {
    "span": "later refined by several studies",
    "document": "Related Work\n\nDistant supervision for relation extraction aligns textual mentions with facts in a knowledge base to generate training labels automatically. Our problem setting follows the weakly supervised relation extraction setup introduced by Mintz et al., and later refined by several studies. Subsequent advancements have addressed noise via multi-instance learning, bag-level attention, and denoising objectives. Despite these developments, long-tail relations and cross-sentence reasoning remain difficult, motivating hybrid approaches that combine structure prediction with pretrained language models.\n",
    "reason": "Vague reference to 'several studies' refining the setup without citing any of them.",
    "start": 264,
    "end": 296,
    "label": "Unsupported_claim"
  },
  {
    "span": "Privacy in federated learning has been approached using secure aggregation (Bonawitz et al., 2017), local and central differential privacy (McMahan et al., 2018; Kairouz et al., 2021), homomorphic encryption (Ateniese et al., 2015), and trusted execution environments (Hunt et al., 2018).",
    "document": "Introduction\n\nFederated learning enables on-device training while keeping raw data local, but gradient sharing can still leak sensitive information. Achieving end-to-end privacy and robustness in realistic deployments remains challenging due to heterogeneous devices and intermittent connectivity.\n\nPrivacy in federated learning has been approached using secure aggregation (Bonawitz et al., 2017), local and central differential privacy (McMahan et al., 2018; Kairouz et al., 2021), homomorphic encryption (Ateniese et al., 2015), and trusted execution environments (Hunt et al., 2018).\n\nWe propose an adaptive privacy controller that tunes clipping and noise parameters per-client based on learned sensitivity proxies, coupled with variance-reduced aggregation. The method aims to preserve utility under tight privacy budgets in non-IID settings.",
    "reason": "Lists privacy techniques without synthesizing their trade-offs or explaining how they motivate the adaptive controller; no explicit gap is articulated.",
    "start": 299,
    "end": 587,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Nguyen and Boydell 2021)",
    "document": "Introduction\n\nTime-series forecasting under distribution shift requires models that can adapt to evolving dynamics (Box and Jenkins, 1970; Lim and Zohren, 2021). Deep sequence models, including temporal convolutions and transformers, achieve state-of-the-art accuracy but can be brittle when regimes change (Bai et al., 2018; Zhou et al., 2021).\n\nRecent works introduce meta-learning and online adaptation to maintain performance in non-stationary environments (Finn et al., 2017; Oreshkin et al., 2020). Others regularize temporal representations to preserve seasonality and trend (Wen et al., 2017; Wu et al., 2021). A probabilistic approach to regime shifts is proposed by Bayesian changepoint models (Adams and MacKay, 2007; Turner, 2011) and extended to deep learners by amortized inference methods (Rangapuram et al., 2018). We compare against a dynamic factor model baseline (Nguyen and Boydell 2021) and show improved calibration under abrupt changes.\n\nOur contributions include an adaptive likelihood with uncertainty-aware ensembling, evaluated across electricity, traffic, and retail datasets.",
    "reason": "Missing comma between author list and year in a parenthetical author–year citation; should be “(Nguyen and Boydell, 2021)”.",
    "start": 882,
    "end": 907,
    "label": "Format"
  },
  {
    "span": "(Smith and Doe, 2018)",
    "document": "Related Work\n\nBilingual lexicon induction. Early distributional approaches relied on seed dictionaries and linear alignment of monolingual embeddings (Mikolov et al., 2013; Artetxe et al., 2018). Unsupervised adversarial mapping reduces dependency on parallel data but can be unstable for distant languages (Zhang et al., 2017; Conneau et al., 2018). Hybrid methods incorporate orthographic cues and subword structure to improve rare word coverage (Kementchedjhieva et al., 2018; (Smith and Doe, 2018) introduce a joint training objective over character n-grams and word embeddings. Recent transformer-based encoders further enhance cross-lingual alignment with multilingual pretraining (Conneau and Lample, 2019).",
    "reason": "Incorrect conjunction inside a parenthetical citation. In APA style, \"and\" should be \"&\" within parentheses: \"(Smith & Doe, 2018)\".",
    "start": 480,
    "end": 501,
    "label": "Format"
  },
  {
    "span": "(Miller et al., 2020))",
    "document": "Related Work\n\nTransformer-based vision models have challenged convolutional inductive biases. Vision Transformers process images as token sequences and achieve strong accuracy when pre-trained at scale (Dosovitskiy et al., 2021; Touvron et al., 2021). This effect was later replicated (Miller et al., 2020)) on medical imaging benchmarks, and hybrid CNN-Transformer models further improved data efficiency (Graham et al., 2021).",
    "reason": "Extra closing parenthesis in a parenthetical citation; should be “(Miller et al., 2020)”.",
    "start": 285,
    "end": 307,
    "label": "Format"
  },
  {
    "span": "Miller et al. 2",
    "document": "Related Work\n\nNeural code summarization leverages sequence models to translate source code into natural language (Iyer et al., 2016; LeClair and McMillan, 2019). Structural encoders incorporate AST and control-flow signals to improve content selection (Alon et al., 2019). A classic survey by Miller et al. 2 synthesizes earlier efforts on program comprehension, while more recent studies explore prompt-based adaptation for unseen libraries (Ahmad et al., 2021).\n",
    "reason": "Improper footnote-style numeral used with an author-year citation; should include a year (e.g., “Miller et al. (YEAR)”) or be formatted as a proper footnote.",
    "start": 293,
    "end": 308,
    "label": "Format"
  },
  {
    "span": "Settles (2012) surveys active learning strategies. Annotator disagreement impacts label reliability (Snow et al., 2008). Crowdsourcing platforms provide scalable annotation pipelines (Buhrmester et al., 2011). Weak supervision frameworks can replace some labels with programmatic sources (Ratner et al., 2017).",
    "document": "Related Work\n\nHuman-in-the-loop learning leverages selective querying, interpretability tools, and interactive interfaces to improve model quality under limited labeling budgets. A key challenge is balancing annotation cost, label quality, and bias amplification from imperfect human feedback.\n\nSettles (2012) surveys active learning strategies. Annotator disagreement impacts label reliability (Snow et al., 2008). Crowdsourcing platforms provide scalable annotation pipelines (Buhrmester et al., 2011). Weak supervision frameworks can replace some labels with programmatic sources (Ratner et al., 2017).\n\nIn contrast to prior work focusing on any single component (query policy, labeling pipeline, or weak supervision), we propose a unified objective that couples selection with disagreement-aware aggregation and consistency regularization.",
    "reason": "The sentences form a list of independent observations about active learning, disagreement, crowdsourcing, and weak supervision without transitions or an explicit narrative linking them, resulting in poor coherence across multiple sentences.",
    "start": 295,
    "end": 605,
    "label": "Coherence"
  },
  {
    "span": "Mnih et al. (2015) achieved human-level control on Atari with DQN. Schulman et al. (2017) presented PPO. Haarnoja et al. (2018) proposed SAC for continuous control.",
    "document": "Introduction\n\nDeep reinforcement learning (RL) combines function approximation with trial-and-error learning to solve sequential decision-making problems. Despite impressive results in games and robotics, stability, sample efficiency, and safe exploration remain persistent bottlenecks that limit adoption in real-world domains.\n\nPolicy and value learning\n\nMnih et al. (2015) achieved human-level control on Atari with DQN. Schulman et al. (2017) presented PPO. Haarnoja et al. (2018) proposed SAC for continuous control. Espeholt et al. (2018) investigated distributed RL for scalability.\n\nExploration and safety\n\nComplementary lines explore intrinsic motivation, count-based bonuses, and risk-sensitive objectives to avoid catastrophic states. In safety-critical settings, constraint satisfaction and shielding mechanisms provide additional safeguards but may hinder exploration.\n\nWe build on these insights by introducing an offline-to-online pipeline that bootstraps from logged trajectories with conservative updates, then safely refines the policy with constrained optimization under uncertainty-aware value estimates.",
    "reason": "The three sentences are adjacent statements about different RL algorithms without transitions or explicit relationships, leading to an abrupt, unconnected listing. This meets criteria (a) and (b) and spans multiple sentences as per (c).",
    "start": 357,
    "end": 521,
    "label": "Coherence"
  },
  {
    "span": "Prior datasets for multimodal sarcasm are limited to English and Chinese.",
    "document": "Introduction\n\nSarcasm detection seeks to recognize utterances where literal meaning diverges from intended sentiment, a phenomenon that is often multimodal in social media posts combining text, audio, and video. Unimodal approaches relying on text alone frequently misinterpret sarcasm due to missing prosodic cues, facial expressions, or scene context. Multimodal models promise improved detection by integrating acoustic, visual, and linguistic signals but face challenges in annotation consistency and cross-cultural variation.\n\nExisting resources for multimodal sarcasm focus primarily on specific platforms and languages. Prior datasets for multimodal sarcasm are limited to English and Chinese. This gap hinders research on multilingual and cross-cultural generalization and makes it difficult to study how cultural norms shape sarcastic expression.\n\nWe introduce MuSa-ES, a multimodal sarcasm corpus for Spanish featuring synchronized transcripts, audio, and video clips with utterance-level sarcasm labels. We design a culturally grounded annotation protocol and report inter-annotator agreement on both sarcasm polarity and intensity. Baselines with late fusion, cross-modal attention, and co-training are benchmarked to facilitate future work.",
    "reason": "Claims a landscape limitation about existing datasets across languages without providing citations to those datasets or surveys (b).",
    "start": 627,
    "end": 700,
    "label": "Unsupported_claim"
  },
  {
    "span": "Arora et al. 1",
    "document": "Related Work\n\nLow-resource machine translation leverages auxiliary data and inductive biases to compensate for scarce parallel corpora (Miller and Zhao, 2020; Karthik et al., 2021). Multilingual pretraining and vocabulary sharing enable parameter transfer across related languages (Dutta and Rao, 2022). Back-translation and dual learning further exploit monolingual text (Nguyen et al., 2019; Silva and Costa, 2020).\n\nArora et al. 1 demonstrated that lexicon constraints can substantially improve adequacy for morphologically rich targets, while pivot-based strategies reduce cascading errors (Bose and Li, 2022). Building on these insights, we propose a lexicon-guided decoder with uncertainty-aware reranking for better faithfulness.\n",
    "reason": "Improper footnote-style marker; should include the year as a citation '(Arora et al., YEAR)' or be formatted as a proper footnote.",
    "start": 419,
    "end": 433,
    "label": "Format"
  },
  {
    "span": "Many recent works have proposed graph-based approaches for fake news detection on social media.",
    "document": "Related Work\n\nMisinformation detection leverages signals from content, user behavior, and propagation patterns. Beyond text-only classifiers, modeling the relational structure of users and posts can capture community-level cues indicative of coordinated manipulation. Many recent works have proposed graph-based approaches for fake news detection on social media. These methods integrate textual features with user-post interaction graphs and diffusion trees, but challenges remain in handling temporal drift and adversarial evasion.",
    "reason": "Uses the phrase \"recent works\" to summarize prior literature without citing any representative studies (rule d).",
    "start": 268,
    "end": 363,
    "label": "Unsupported_claim"
  },
  {
    "span": "Differential privacy for NLP includes DP-SGD training, PATE-style aggregation, and DP fine-tuning of pre-trained models (Abadi et al., 2016; Papernot et al., 2016; Yu et al., 2022; Li et al., 2022). Accounting methods such as RDP and privacy amplification by subsampling are commonly used (Mironov, 2017; Feldman et al., 2018). We develop a private adaptation method for sequence labeling.",
    "document": "Introduction\n\nPrivacy-preserving language technologies are increasingly necessary as models ingest sensitive user data. Ensuring rigorous privacy guarantees while retaining utility presents algorithmic and optimization challenges.\n\nDifferential privacy for NLP includes DP-SGD training, PATE-style aggregation, and DP fine-tuning of pre-trained models (Abadi et al., 2016; Papernot et al., 2016; Yu et al., 2022; Li et al., 2022). Accounting methods such as RDP and privacy amplification by subsampling are commonly used (Mironov, 2017; Feldman et al., 2018). We develop a private adaptation method for sequence labeling.\n\nWe benchmark our approach on NER datasets and report utility–privacy trade-offs across epsilon values.",
    "reason": "The span lists DP techniques and states the contribution but does not specify the shortcoming in existing methods or how the new adaptation addresses it; this lacks synthesis and clear motivation.",
    "start": 232,
    "end": 621,
    "label": "Lacks_synthesis"
  },
  {
    "span": "In a previous study, the authors claim that decoupling dynamics and vision leads to sample efficiency gains of 3–5x.",
    "document": "Related Work\n\nSample efficiency in visuomotor control can be improved through model-based RL, representation learning, or data augmentation (Finn and Levine, 2017; Janner et al., 2019; Laskin et al., 2020). World-model approaches learn latent dynamics from pixels, then plan or improve policies using imagined rollouts (Hafner et al., 2019; Schrittwieser et al., 2020). Separating perception from control is a common heuristic to reduce complexity and stabilize training (Yarats et al., 2021).\n\nIn a previous study, the authors claim that decoupling dynamics and vision leads to sample efficiency gains of 3–5x. While this intuition is compelling, the extent to which decoupling helps remains sensitive to task diversity, encoder capacity, and data augmentation.\n\nOur method, DecoupleRL, formalizes this separation via an information bottleneck and task-adaptive latent augmentation, yielding consistent gains across diverse robotic manipulation benchmarks.",
    "reason": "Refers to an unspecified 'previous study' and a quantitative claim without citing the source.",
    "start": 495,
    "end": 611,
    "label": "Unsupported_claim"
  },
  {
    "span": "McMahan et al. (2017) proposed FedAvg for on-device training. Differential privacy bounds disclosure risk by adding noise (Dwork et al., 2014). Secure aggregation hides individual updates from the server (Bonawitz et al., 2017). Personalization methods adapt global models to clients (Arivazhagan et al., 2019).",
    "document": "Introduction\n\nFederated learning (FL) enables collaborative model training without centralizing raw data. Practical deployments must address heterogeneity, communication efficiency, and privacy, often requiring a combination of algorithmic and systems solutions.\n\nMcMahan et al. (2017) proposed FedAvg for on-device training. Differential privacy bounds disclosure risk by adding noise (Dwork et al., 2014). Secure aggregation hides individual updates from the server (Bonawitz et al., 2017). Personalization methods adapt global models to clients (Arivazhagan et al., 2019).\n\nAlthough these techniques are frequently combined in practice, their interactions are under-theorized. We provide a unifying objective that quantifies the accuracy–privacy–heterogeneity trade-offs and guides principled composition.",
    "reason": "The span lists FL, DP, secure aggregation, and personalization without transitions or an explicit articulation of how they connect, causing abrupt shifts and unclear relationships.",
    "start": 264,
    "end": 575,
    "label": "Coherence"
  },
  {
    "span": "Recent works have demonstrated state-of-the-art results on COCO",
    "document": "Related Work\n\nRecent works have demonstrated state-of-the-art results on COCO by leveraging multi-scale features and transformer decoders. Two-stage detectors derived from Faster R-CNN emphasize region-level refinement (Ren et al., 2015; He et al., 2017), whereas one-stage variants like RetinaNet focus on dense prediction with focal loss (Lin et al., 2017). The DETR family further simplifies pipelines using bipartite matching (Carion et al., 2020; Zhu et al., 2021). Our method builds on deformable attention while introducing long-range context priors.",
    "reason": "Uses the phrase “recent works” and a performance claim without citing any of the works.",
    "start": 14,
    "end": 77,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works have shown that instruction tuning dramatically improves code generation.",
    "document": "Introduction\n\nLarge language models for code have rapidly advanced, enabling tools that assist with synthesis, refactoring, and documentation. However, aligning these models to follow task directives and adhere to developer style guides remains an open challenge.\n\nRecent works have shown that instruction tuning dramatically improves code generation. Motivated by this, we introduce a hybrid tuning strategy that combines repository-level demonstrations with unit-test feedback to better align models with real-world development workflows.\n\nWe evaluate across tasks requiring multi-file reasoning, API compliance, and safety constraints, and present ablations on the effect of instruction specificity and test coverage.",
    "reason": "The phrase \"Recent works\" makes a claim about prior literature without providing any citations (guideline d).",
    "start": 265,
    "end": 351,
    "label": "Unsupported_claim"
  },
  {
    "span": "There is growing evidence that chain-of-thought prompting improves factuality across tasks.",
    "document": "Introduction\n\nLarge language models can produce fluent but ungrounded answers. Prompting strategies that elicit intermediate reasoning steps aim to improve reliability and reduce hallucinations. Chain-of-thought prompting encourages models to verbalize reasoning, potentially enabling self-correction and better use of external tools.\n\nThere is growing evidence that chain-of-thought prompting improves factuality across tasks. However, observed gains may conflate longer outputs with increased chance of hitting correct facts, and can vary widely with prompt phrasing and decoding settings. We study factuality under controlled length and calibration, isolating the effect of reasoning steps from verbosity.\n\nRelated Work\n\nPrior work explores rationale generation, self-consistency decoding, verification via retrieval, and process supervision. Evaluations have spanned arithmetic, commonsense, and open-domain QA, yet standardized factuality protocols remain underdeveloped.",
    "reason": "Mentions 'growing evidence' without providing citations to the works; per rule d, such claims need references.",
    "start": 336,
    "end": 427,
    "label": "Unsupported_claim"
  },
  {
    "span": "state-of-the-art adversarial defenses",
    "document": "Related Work\n\nAdversarial robustness has been a central focus in computer vision, with extensive research on both attack algorithms and defenses. While adversarial training remains the most reliable approach, it is computationally demanding and can degrade clean accuracy. Alternative strategies include randomized smoothing, certified defenses, and feature denoising, each with different trade-offs between robustness and efficiency.\n\nOur approach relates to preprocessing-based defenses that aim to remove adversarial perturbations before classification. However, unlike prior schemes that rely on handcrafted filters or heavy diffusion models, we introduce a lightweight, learnable transformation with provable Lipschitz bounds. We compare against state-of-the-art adversarial defenses under strong white-box and adaptive attacks to quantify improvements in robustness and runtime.",
    "reason": "The reference to \"state-of-the-art adversarial defenses\" implies specific prior methods but does not cite any, leaving the claim unsupported (rules a and d).",
    "start": 751,
    "end": 788,
    "label": "Unsupported_claim"
  },
  {
    "span": "Gao et al. 1",
    "document": "Related Work\n\nFairness and Reliability in Decision Support\n\nAlgorithmic fairness has been framed through statistical parity, equalized odds, and calibration (Hardt et al., 2016; Kleinberg et al., 2017). Counterfactual fairness formalizes invariance to sensitive attributes (Kusner et al., 2017), enabling causal analyses. In healthcare triage, calibration has been emphasized by Gao et al. 1 and revisited by Kleinberg et al. (2017), while recent audits explore subgroup robustness and distribution shift (Subbaswamy and Saria, 2020; Wang et al., 2021).\n",
    "reason": "Wrong use of footnote/marker in place of a proper citation: should include the year as an author-year citation (e.g., 'Gao et al. (2019)') or be formatted as a proper footnote.",
    "start": 379,
    "end": 391,
    "label": "Format"
  },
  {
    "span": "Henderson et al.",
    "document": "Related Work\n\nTask-oriented dialogue systems typically comprise natural language understanding, belief tracking, policy learning, and natural language generation (Young et al., 2013; Wen et al., 2017). Henderson et al. propose robust dialogue state tracking with neural architectures, inspiring later refinements that leverage pretraining (Mrkšić et al., 2017; Rastogi et al., 2020). Recent approaches integrate retrieval-augmented memory (Chen et al., 2020) and leverage end-to-end training to reduce annotation costs (Budzianowski and Vulić, 2019; Zhang et al., 2020). Our work differs by emphasizing generalization to unseen domains under sparse supervision (Shah et al., 2018; Peng et al., 2021).",
    "reason": "Narrative citation missing year: 'Henderson et al.' should include the publication year as 'Henderson et al. (YEAR)'.",
    "start": 202,
    "end": 218,
    "label": "Format"
  },
  {
    "span": "Progress on ImageNet has plateaued, with only marginal top-1 gains reported since 2021.",
    "document": "Related Work\n\nImage classification benchmarks have historically driven advances in representation learning, with ImageNet serving as a central testbed for model scaling, augmentation, and optimization strategies. As architectures have matured, incremental improvements have required careful tuning and larger computational budgets.\n\nProgress on ImageNet has plateaued, with only marginal top-1 gains reported since 2021. This trend has motivated a shift toward transfer and robustness benchmarks that better capture real-world variability, including distribution shifts and long-tail phenomena. In this paper, we study whether improvements on synthetic corruption robustness translate to downstream detection and segmentation tasks.\n\nWe provide a comprehensive suite of evaluations that measure trade-offs between clean accuracy, robustness to common corruptions, and calibration under shift.",
    "reason": "The sentence makes a time-bound, quantitative field trend claim without citing sources such as leaderboards, surveys, or papers documenting these gains.",
    "start": 333,
    "end": 420,
    "label": "Unsupported_claim"
  },
  {
    "span": "The CoNLL-2003 dataset provides a clean, noise-free benchmark for named entity recognition.",
    "document": "Related Work\n\nNamed entity recognition has transitioned from linear statistical models to neural architectures with contextual embeddings and pretrained language models (Collobert et al., 2011; Lample et al., 2016; Devlin et al., 2019). Evaluation benchmarks have historically focused on newswire but have expanded to include social media and biomedical domains.\n\nThe CoNLL-2003 dataset provides a clean, noise-free benchmark for named entity recognition. Cross-domain evaluation has revealed substantial performance degradation when models are applied to noisy or user-generated text, motivating domain adaptation and robust training techniques (Augenstein et al., 2017; Ritter et al., 2011).\n\nWe propose a corruption-aware training scheme that simulates realistic noise processes and calibrates token-level confidence to improve out-of-domain generalization without hurting in-domain accuracy.",
    "reason": "Mentions a specific dataset and asserts properties about it without providing a citation at first mention.",
    "start": 364,
    "end": 455,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Johnson 2020)",
    "document": "Related Work\n\nIn reinforcement learning, model-free methods such as Q-learning and policy gradients have achieved strong performance across control benchmarks (Mnih et al., 2015; Schulman et al., 2017). Distributional critics and entropy regularization further stabilize training and improve exploration (Bellemare et al., 2017; Haarnoja et al., 2018). Off-policy evaluation remains difficult due to extrapolation error and coverage issues (Fujimoto et al., 2019; Munos et al., 2016). Several studies analyze the interaction between function approximation and bootstrapping (Johnson 2020), but results depend on the expressivity of the underlying value network. Our work introduces a conservative target network update that reduces overestimation without sacrificing sample efficiency.\n\nIntroduction\n\nWe benchmark across continuous-control tasks with varying reward scales, reporting both median and interquartile ranges to account for instability.",
    "reason": "Missing comma in author–year citation per common author–date styles (e.g., APA). Should be \"(Johnson, 2020)\".",
    "start": 574,
    "end": 588,
    "label": "Format"
  },
  {
    "span": "Nguyen et al. 3",
    "document": "Introduction\n\nEvent extraction requires accurate schemas, consistent triggers, and robust argument typing. Prior datasets collected by Nguyen et al. 3 emphasize political events, whereas subsequent corpora expand the scope to finance (Zhang and Chen, 2020) and biomedicine (Mehryary et al., 2018). We revisit schema generalization and propose a benchmark that controls for domain drift across time.",
    "reason": "Improper footnote-like numeral used with an author name. Should include the year in a proper citation (e.g., “Nguyen et al. (2016)”) or be formatted as an actual footnote, not “Nguyen et al. 3”.",
    "start": 135,
    "end": 150,
    "label": "Format"
  },
  {
    "span": "There are many recent works that explore hallucination mitigation in news summarization.",
    "document": "Introduction\n\nAbstractive summarization systems have achieved impressive fluency but often generate content that is not supported by the source article. This issue, commonly termed hallucination or factual inconsistency, undermines trust in automated summarization for news and other high-stakes domains. Earlier neural approaches emphasized sequence-to-sequence modeling with attention and copying mechanisms to improve content selection and lexical control. More recently, pretrained language models fine-tuned for summarization have further improved ROUGE, yet factual errors persist and can even increase under aggressive compression.\n\nA growing body of work studies faithfulness metrics, constrained decoding, and post-hoc verification, as well as training-time interventions such as contrastive learning and reinforcement learning from human feedback. However, evaluation remains challenging because factuality involves both entity-level correctness and discourse-level coherence. There are many recent works that explore hallucination mitigation in news summarization. In this paper, we present a lightweight approach that augments the decoder with a retrieval-guided verifier trained to reject unsupported continuations while preserving fluency.\n\nRelated Work\n\nFaithfulness metrics include entailment-based measures and question-answering derived probes. Constrained decoding methods aim to force alignment with extracted facts. Verification-based approaches rerank candidate summaries using external evidence. Despite these advances, reliable generalization across outlets and styles is limited, in part due to distributional shift and noisy reference summaries.",
    "reason": "Mentions 'many recent works' without providing citations; per rule d, references are required for such claims.",
    "start": 987,
    "end": 1075,
    "label": "Unsupported_claim"
  },
  {
    "span": "The SIGNS-3D dataset includes 200k motion-capture sequences.",
    "document": "Introduction\n\nAnimating intelligible sign language avatars requires high-fidelity motion data and linguistically informed control signals. While video-based corpora are plentiful, precise kinematic supervision for handshape and facial articulators remains limited. The SIGNS-3D dataset includes 200k motion-capture sequences. Despite its scale, prior work has focused largely on single-signer models, leaving questions about generalization across morphology and signing styles. We address these gaps by proposing a signer-normalized diffusion model trained to reconstruct articulatory trajectories, with an evaluation protocol targeting phonological contrast and coarticulation.",
    "reason": "States a specific dataset and size without citing a source that defines or releases the dataset.",
    "start": 265,
    "end": 325,
    "label": "Unsupported_claim"
  },
  {
    "span": "[12]",
    "document": "Related Work\n\nNeural program synthesis has benefited from large-scale code corpora and self-supervision (Dias and Pereira, 2020; Kwon et al., 2021). Several unsupervised approaches [12] rely on identifier masking to learn structural regularities, whereas others use contrastive objectives over execution traces (Amari and Holt, 2022).\n\nDespite gains in benchmark accuracy, generalization to out-of-distribution specifications remains limited (Klein and Omar, 2023). We explore compositional decoders that factorize control flow and data dependencies (Rossi et al., 2022).",
    "reason": "Numeric bracketed citation is inconsistent with the surrounding author–year style. Should be replaced with an author–year citation like '(Smith, 2019)'.",
    "start": 181,
    "end": 185,
    "label": "Format"
  },
  {
    "span": "Garcia et al. 1",
    "document": "Introduction\n\nAuditing machine learning systems for fairness requires both dataset diagnostics and post-deployment monitoring (Barocas et al., 2019; Mitchell et al., 2019). Bias can enter through historical imbalances, measurement errors, and model misspecification (Friedler et al., 2019). Approaches to mitigation include pre-processing reweighting, in-processing regularization, and post-processing threshold adjustments (Kamiran and Calders, 2012; Hardt et al., 2016; Agarwal et al., 2018).\n\nCausal perspectives emphasize counterfactual reasoning and path-specific effects (Pearl, 2009; Kilbertus et al., 2017), while marketplace settings motivate group- and individual-level guarantees (Dwork et al., 2012; Kearns et al., 2018). Recent toolkits standardize reporting practices and metrics (Mitchell et al., 2019; Saleiro et al., 2018). As shown by Garcia et al. 1, practitioner uptake depends on actionable guidance and stakeholder alignment.\n\nWe propose a risk register for fairness auditing that tracks data collection, labeling, and model monitoring decisions. We validate the register across three product teams and report reductions in incidences of untracked data drift.",
    "reason": "Improper footnote-like usage; the narrative reference ends with a superscript-style number \"1\" instead of including the publication year or using a proper footnote format.",
    "start": 853,
    "end": 868,
    "label": "Format"
  },
  {
    "span": "Vatswani et al.",
    "document": "Introduction\n\nAbstractive multimodal summarization has emerged as a promising direction for distilling information from video and audio alongside text (Li et al., 2019; Zhang et al., 2021; Chen and Wu, 2020). Despite substantial progress in encoder-decoder architectures, existing systems often fail to align visual evidence with generated claims, leading to hallucinations and factual drift (Maynez et al., 2020; Kryscinski et al., 2020).\n\nFollowing Vatswani et al., we adopt a content-planning stage that structures salient events before surface realization, but we extend this idea by incorporating cross-modal entailment checks during decoding. Our approach is complementary to retrieval-augmented summarization methods (Lewis et al., 2020; Gao et al., 2023), yet differs by grounding plan selection in both audio cues and frame-level semantics.\n\nWe evaluate on news and instructional video datasets and find consistent gains in factuality and coherence over strong baselines (Narayan et al., 2018; Ladhak et al., 2020).",
    "reason": "Narrative citation missing year; should be formatted as Vatswani et al. (YEAR) rather than Vatswani et al.",
    "start": 451,
    "end": 466,
    "label": "Format"
  },
  {
    "span": "In (Park and Lee, 2020)",
    "document": "Introduction\n\nTask-oriented dialogue systems increasingly leverage pre-trained language models to improve intent detection and slot filling (Chen et al., 2019; Peng et al., 2021). Data scarcity and domain shifts pose practical obstacles, motivating semi-supervised and continual learning strategies (Jia et al., 2020; Madotto et al., 2021).\n\nIn (Park and Lee, 2020) the authors argue for schema-guided modeling, but their method assumes complete slot catalogs at training time. We instead propose a compositional approach that generalizes to unseen slots by grounding spans to latent schemas, building on contextual encoders (Devlin et al., 2019) and prompt-based constraints (Liu et al., 2021).\n\nOur experiments on multiple domains demonstrate strong zero-shot and few-shot gains over schema-guided baselines.",
    "reason": "Wrong citation style: using “In (Author, Year)” is incorrect; should be narrative (e.g., “In Park and Lee (2020)”) or purely parenthetical without the preposition.",
    "start": 342,
    "end": 365,
    "label": "Format"
  },
  {
    "span": "Sutskever et al. 1",
    "document": "Related Work\n\nSequence-to-sequence modeling with neural networks unified a broad class of tasks under a single encoder-decoder framework. Early work demonstrated the feasibility of mapping variable-length input sequences to outputs with learned alignments (Bahdanau et al., 2014). The use of subword tokenization and deeper architectures further improved robustness (Sennrich et al., 2016; Kaiser et al., 2018). Sutskever et al. 1 showed that reversing input sequences and leveraging large LSTMs yield strong baselines for translation. We extend this line by introducing a constrained decoding objective that enforces schema consistency in structured prediction.",
    "reason": "Wrong use of a footnote-like marker appended to an author name instead of providing a proper year or a formatted footnote; should be 'Sutskever et al. (2014)' or a correctly formatted footnote.",
    "start": 412,
    "end": 430,
    "label": "Format"
  },
  {
    "span": "There are numerous recent works that demonstrate that adapters outperform full fine-tuning on low-resource tasks.",
    "document": "Introduction\n\nPretrained language models have become the foundation for a wide range of NLP applications, but adapting them to downstream tasks can be expensive in terms of computation and memory. Parameter-efficient tuning methods aim to reduce the cost of adaptation by training a small number of additional parameters while keeping the base model frozen. Among these methods, adapters, prompt tuning, and low-rank reparameterizations have received significant attention for their ability to retain performance while minimizing resource usage.\n\nThere are numerous recent works that demonstrate that adapters outperform full fine-tuning on low-resource tasks. However, the conditions under which these approaches consistently excel remain unclear, especially when domain shifts and label scarcity interact. In this paper, we provide a systematic analysis of several parameter-efficient methods across diverse data regimes and domains, and we propose a simple selection criterion that predicts which method to deploy for a given task.",
    "reason": "Mentions unspecified 'recent works' and a comparative claim without providing any citations.",
    "start": 547,
    "end": 660,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works have shown significant gains from instruction tuning for code generation.",
    "document": "Introduction Large language models pretrained on source code have rapidly advanced the state of automated program synthesis, with models leveraging both natural and programming languages to generalize across tasks (Chen et al., 2021; Austin et al., 2021). Despite strong zero-shot capabilities, aligning models to developer intent remains challenging due to ambiguity in specifications and evaluation (Liu et al., 2023). Recent works have shown significant gains from instruction tuning for code generation. In this paper, we investigate whether combining instruction signals with unit-test based feedback further improves controllability and robustness.",
    "reason": "Mentions 'recent works' without citing any specific papers, violating rule (d).",
    "start": 421,
    "end": 507,
    "label": "Unsupported_claim"
  },
  {
    "span": "Early fusion concatenates features across modalities before classification (Ngiam et al., 2011). Late fusion aggregates unimodal predictions with learned weights (Atrey et al., 2010). Text encoders based on transformers capture contextual sentiment cues (Devlin et al., 2019). Acoustic prosody correlates with affect (Eyben et al., 2013). Visual facial action units signal emotions (Ekman and Friesen, 1978).",
    "document": "Introduction\n\nMultimodal Sentiment Analysis (MMSA)\n\nMMSA seeks to infer subjective states by integrating linguistic, acoustic, and visual cues. The principal challenge is modeling cross-modal interactions while handling modality-specific noise and missing data. Solutions vary from simple fusion strategies to structured alignment and attention mechanisms.\n\nEarly fusion concatenates features across modalities before classification (Ngiam et al., 2011). Late fusion aggregates unimodal predictions with learned weights (Atrey et al., 2010). Text encoders based on transformers capture contextual sentiment cues (Devlin et al., 2019). Acoustic prosody correlates with affect (Eyben et al., 2013). Visual facial action units signal emotions (Ekman and Friesen, 1978).\n\nCross-Modal Alignment\n\nTemporal alignment techniques synchronize modalities at the frame or word level (Tsai et al., 2019). Co-attention models encourage information sharing across modalities (Zadeh et al., 2018). Robust MMSA further leverages uncertainty modeling to mitigate missing streams (Hazarika et al., 2020).\n\nOur Approach\n\nWe introduce a token-level multimodal alignment with variational gating that adaptively selects informative modalities, reducing over-reliance on spurious correlations.",
    "reason": "The span mixes fusion taxonomies with unimodal feature claims in a sentence-by-sentence list, offering no transitions or explicit connections, leading to abrupt topic shifts and unclear relationships.",
    "start": 358,
    "end": 766,
    "label": "Coherence"
  },
  {
    "span": "Dathathri et al. (2020) steer generation with attribute models. Gehman et al. (2020) curate a toxicity benchmark for language models. Dinan et al. (2019) collect adversarial dialogue to probe safety. Bai et al. (2022) apply reinforcement learning from human feedback to align assistants.",
    "document": "Related Work\n\nSafety in open-domain dialogue requires controlling toxic content, avoiding sensitive topics, and aligning responses with human values. Prior work spans data curation, controllable generation, detection, and post-hoc filtering.\n\nDathathri et al. (2020) steer generation with attribute models. Gehman et al. (2020) curate a toxicity benchmark for language models. Dinan et al. (2019) collect adversarial dialogue to probe safety. Bai et al. (2022) apply reinforcement learning from human feedback to align assistants.\n\nDespite progress, safe generation often trades off fluency and helpfulness. We propose a retrieval-augmented controller that balances factuality and safety through constraint-aware decoding.\n",
    "reason": "The span lists four distinct efforts (control, benchmarking, data collection, RLHF) with no transitions or explicit connections, causing abrupt topic shifts and unclear relationships between cited works.",
    "start": 243,
    "end": 530,
    "label": "Coherence"
  },
  {
    "span": "(2020, Zhao et al.)",
    "document": "Introduction\n\nSelf-supervised visual pretraining learns transferrable features without manual labels (He et al., 2020; Chen et al., 2020). Contrastive objectives and clustering-based methods dominate recent advances (Caron et al., 2020; Grill et al., 2020).\n\nHowever, for dense prediction tasks, pretraining signals must preserve spatial information (Xie et al., 2021). Evidence from medical imaging (2020, Zhao et al.) suggests that patch-level consistency aids segmentation under limited supervision.\n\nWe incorporate multi-scale alignment to maintain local structure while improving global discrimination.",
    "reason": "Author–year order reversed inside parentheses; should be “(Zhao et al., 2020)”.",
    "start": 400,
    "end": 419,
    "label": "Format"
  },
  {
    "span": "There are many datasets for code search released in the past two years.",
    "document": "Related Work\n\nCode search aims to retrieve relevant code snippets given natural language queries. There are many datasets for code search released in the past two years. While early benchmarks emphasized short, templated queries, recent resources target realistic, multi-sentence problem descriptions and cross-repository retrieval.\n\nEmbedding-based dual encoders and cross-encoders have both been explored, with trade-offs in latency and interaction modeling. Despite progress, reproducibility remains challenging due to differences in preprocessing, negative sampling, and evaluation granularity.",
    "reason": "Vague claim about numerous recent datasets is made without any citations to the datasets (rules b and d).",
    "start": 98,
    "end": 169,
    "label": "Unsupported_claim"
  },
  {
    "span": "The Common Voice 17.0 corpus now covers 90 languages.",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) in low-resource settings depends critically on the availability of diverse, community-sourced speech. Crowdsourced corpora have broadened access, but data quality, accent coverage, and label noise remain key obstacles.\n\nThe Common Voice 17.0 corpus now covers 90 languages. Despite this expansion, many languages still lack sufficient hours for competitive ASR, and the imbalance across speakers and domains complicates transfer.\n\nWe present a multilingual pretraining strategy that emphasizes robust alignment for noisy labels and demonstrate improvements across a suite of low-resource benchmarks.\n",
    "reason": "This sentence states a specific dataset version and a numerical coverage claim without citing the dataset or providing evidence, which should be supported by a citation.",
    "start": 280,
    "end": 333,
    "label": "Unsupported_claim"
  },
  {
    "span": "[12]",
    "document": "Related Work\n\nDocument-level relation extraction methods range from pipeline approaches to end-to-end graph reasoning models (Christopoulou et al., 2019; Yao et al., 2019). Early pipelines relied on sentence-level predictions aggregated across mentions, whereas recent models propagate evidence via entity graphs (Zhang et al., 2018; Nan et al., 2020). As shown in [12], training with curriculum schedules can improve stability on long documents, but the reported gains are inconsistent when evaluated with alternative metrics (Alt et al., 2020; Tang et al., 2020). Our work focuses on robust graph construction that avoids spurious shortcuts.",
    "reason": "Numeric bracket citation used amid author–year style; should be replaced with an author–year citation (e.g., 'as shown in Author (Year)' or '(Author, Year)') for consistency.",
    "start": 365,
    "end": 369,
    "label": "Format"
  },
  {
    "span": "Recent works leverage contrastive pretraining to align images and text for captioning.",
    "document": "Introduction\n\nImage captioning systems have evolved from encoder-decoder architectures with attention (Vinyals et al., 2015; Xu et al., 2015) to bottom-up attention and object-centric encoders (Anderson et al., 2018). Vision-language pretraining has further advanced transfer to downstream tasks (Lu et al., 2019; Chen et al., 2020).\n\nRecent works leverage contrastive pretraining to align images and text for captioning. Despite improved representations, exposure bias and length control remain open challenges for fluent, diverse captions.\n\nWe present a non-autoregressive captioner initialized from a vision-language contrastive model and introduce a calibration loss to mitigate length bias.",
    "reason": "Mentions unspecified recent works without providing citations to them (rule d).",
    "start": 335,
    "end": 421,
    "label": "Unsupported_claim"
  },
  {
    "span": "Graph convolution improves neighborhood aggregation for recommendation (Ying et al., 2018). Session-based models employ gated recurrent units to capture short-term intent (Hidasi et al., 2016). Negative sampling strategies affect representation quality (Mikolov et al., 2013).",
    "document": "Related Work\n\nCollaborative Filtering. Matrix factorization and neural CF leverage user–item interaction matrices to learn latent representations. Side information such as content and social links can regularize or augment these embeddings.\n\nGraph-based and Sequential Models. Graph neural recommenders propagate preferences along user–item bipartite graphs, while sequential recommenders capture temporal dynamics at the session or user level. Hybrid models attempt to blend structural and temporal signals.\n\nModeling Choices. Graph convolution improves neighborhood aggregation for recommendation (Ying et al., 2018). Session-based models employ gated recurrent units to capture short-term intent (Hidasi et al., 2016). Negative sampling strategies affect representation quality (Mikolov et al., 2013).\n\nContrastive Learning for Recommendation. Recent work introduces self-supervised objectives to denoise interactions and improve cold-start performance, often via augmentations on graphs or sequences.",
    "reason": "The span lists graph convolution, GRU-based session models, and negative sampling without articulating how these ideas interact or transition, leaving the relationships between methods implicit and incoherent.",
    "start": 528,
    "end": 804,
    "label": "Coherence"
  },
  {
    "span": "Prior work has proven that data augmentation with back-translation always yields significant gains for low-resource machine translation.",
    "document": "Related Work\n\nData augmentation is a central strategy for low-resource machine translation, with methods ranging from synthetic parallel generation to noise injection and constrained paraphrasing. Back-translation, in particular, creates synthetic source sentences from monolingual target data to improve coverage and fluency on the source side.\n\nPrior work has proven that data augmentation with back-translation always yields significant gains for low-resource machine translation. Nevertheless, the magnitude of improvement depends on language typology, domain match, and the quality of the reverse model. We systematically vary these factors and provide guidance on when back-translation is most beneficial.",
    "reason": "Overgeneralizes prior findings and references 'prior work' without any supporting citations.",
    "start": 347,
    "end": 483,
    "label": "Unsupported_claim"
  },
  {
    "span": "Smith et al., (2019)",
    "document": "Introduction\n\nTime-series anomaly detection methods include reconstruction-based autoencoders, probabilistic forecasting, and change-point detection (Gomez and Li, 2020; Iqbal and Sun, 2021). Smith et al., (2019) proposed a hybrid model that combines variational encoders with attention over recent windows, but later studies reported instability under sparse anomalies (Patel and Kim, 2020). Recent benchmarks emphasize realistic evaluation, including variable sampling rates and missing values (Ortega and Rao, 2022).\n\nOur contribution is a calibration-aware detector that reweights anomaly scores using interval uncertainty derived from conformal prediction. This yields more reliable alerts at fixed false positive budgets across diverse sensor suites (Hernandez and Wu, 2022).",
    "reason": "Incorrect punctuation in narrative citation with a comma before the parenthetical year; should be 'Smith et al. (2019)'.",
    "start": 192,
    "end": 212,
    "label": "Format"
  },
  {
    "span": "A variety of spatial-temporal GNNs have been proposed for traffic forecasting, including diffusion convolutional networks (Li et al., 2018), graph wavelet models (Wu et al., 2019), attention-based graphs (Guo et al., 2019), and adaptive adjacency learning (Bai et al., 2020).",
    "document": "Introduction\n\nAccurate traffic forecasting enables proactive congestion management and reliable route planning in modern cities. Recent research emphasizes modeling both spatial dependencies across road networks and temporal dynamics of traffic flows.\n\nRelated Work\n\nGraph neural networks (GNNs) have emerged as a compelling paradigm for learning spatial-temporal dependencies directly on road graphs. A variety of spatial-temporal GNNs have been proposed for traffic forecasting, including diffusion convolutional networks (Li et al., 2018), graph wavelet models (Wu et al., 2019), attention-based graphs (Guo et al., 2019), and adaptive adjacency learning (Bai et al., 2020). Transformer-based decoders have also been integrated on top of graph encoders to better capture long-range temporal patterns (Cao et al., 2021; He et al., 2022). External knowledge such as weather and events has been incorporated via side channels (Liu et al., 2020; Zhang et al., 2021).\n\nOur Approach\n\nWe build on spatial-temporal graph modeling and propose a dynamic context router that conditions message passing on exogenous signals while preserving computational efficiency for deployment on edge devices.",
    "reason": "The sentence lists prior GNN approaches without articulating how they relate to the current work, what gap remains, or the author's perspective.",
    "start": 402,
    "end": 677,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Garcia et al. 2022)",
    "document": "Related Work\n\nDomain adaptation methods include discrepancy minimization, adversarial alignment, and self-training (Ganin et al., 2016; Long et al., 2018; Zhang et al., 2019). A concurrent study (Garcia et al. 2022) demonstrates that curriculum-based pseudo-labeling can stabilize training under severe shift.\n\nOther works propose target-specific augmentations and confidence calibration to control error propagation (Liang et al., 2020; Nado et al., 2021). We complement these strategies with an uncertainty-aware teacher that filters hard examples online.",
    "reason": "Missing comma before the year in a parenthetical citation; should be '(Garcia et al., 2022)'.",
    "start": 195,
    "end": 215,
    "label": "Format"
  },
  {
    "span": "The SemEval 2017 Task 3 dataset contains 150k question–comment pairs.",
    "document": "Introduction\n\nCommunity Question Answering (CQA) aims to rank or select relevant answers for a given user question, often under noisy conditions and informal language (Nakov et al., 2016). Neural ranking models with attention and pre-trained encoders have set strong baselines in answer selection by learning fine-grained interactions between questions and candidate posts (Tan et al., 2015; Bian et al., 2017). The SemEval 2017 Task 3 dataset contains 150k question–comment pairs. While large-scale benchmarks help drive progress, domain mismatch and annotation ambiguity continue to limit generalization to new forums and topics.\n",
    "reason": "Claims a specific dataset statistic without citing the dataset or a source.",
    "start": 412,
    "end": 481,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Zhou et al., 2020",
    "document": "Related Work\n\nSequential recommendation. Early approaches relied on Markov chains and factorization methods to model user dynamics (Rendle et al., 2010; He and McAuley, 2016). With the advent of deep learning, recurrent and attention-based models became standard, capturing long-term dependencies and item co-occurrence patterns (Kang and McAuley, 2018; Sun et al., 2019). Context gating and temporal modulation have been proposed to better encode session signals (Zhu et al., 2021; Park and Lee, 2022); see (Zhou et al., 2020 for a comprehensive benchmark and analysis of training protocols.",
    "reason": "Missing closing parenthesis in the citation. It should be \"(Zhou et al., 2020)\".",
    "start": 508,
    "end": 526,
    "label": "Format"
  },
  {
    "span": "Earlier papers claimed that keyword blacklists reduce toxic outputs by 60%.",
    "document": "Introduction\n\nSafety in open-domain dialogue systems has emerged as a critical concern as generative models scale (Gehman et al., 2020). Mitigation strategies range from data filtering and toxicity-controlled decoding to post-hoc classifiers that reject unsafe responses (Dathathri et al., 2020; Dinan et al., 2019). However, simple pattern-matching approaches can create false security by censoring benign content and being easily evaded.\n\nEarlier papers claimed that keyword blacklists reduce toxic outputs by 60%. Despite such claims, the generality and reproducibility of blacklist-based filtering across domains and languages remain unclear. We present a counterfactual evaluation framework that avoids exposure bias and measures true safety gains under adversarial prompting.\n\nRelated Work\n\nRecent studies evaluate safety with human-in-the-loop red teaming (Perez et al., 2022) and train detoxification objectives that trade off fluency and safety (Krause et al., 2021).",
    "reason": "Reports a specific quantitative claim ('reduce toxic outputs by 60%') attributed to earlier papers without citing any source.",
    "start": 441,
    "end": 516,
    "label": "Unsupported_claim"
  },
  {
    "span": "Transfer learning from ImageNet has become the de facto standard for fine-grained visual classification.",
    "document": "Related Work\n\nFine-grained visual classification (FGVC) requires distinguishing subtle inter-class differences under significant intra-class variation. Early approaches emphasized part localization and attribute modeling, whereas modern pipelines rely on powerful backbones and data augmentation. Transfer learning from ImageNet has become the de facto standard for fine-grained visual classification. Nevertheless, domain shift between ImageNet and fine-grained targets motivates strategies that adapt representations to texture, pose, and scale biases specific to the target domain. Our method builds on this line by introducing a curriculum of class-aware augmentations that gradually narrows the domain gap.",
    "reason": "Asserts a community-wide practice ('de facto standard') about prior work without citing foundational or survey papers supporting the claim (criteria a and b).",
    "start": 297,
    "end": 401,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Kumar et. al., 2018)",
    "document": "Introduction\n\nCalibration of probabilistic classifiers is crucial for decision-making under uncertainty (Guo et al., 2017; Kuleshov et al., 2018). Deep ensembles and temperature scaling are popular post-hoc methods, but they can degrade under dataset shift (Ovadia et al., 2019; Minderer et al., 2021). In generative modeling, likelihoods can be misaligned with human perception, complicating risk assessment (Nalisnick et al., 2019; Kirichenko et al., 2020).\n\nRecent work explores distributionally robust optimization and conformal prediction to provide finite-sample guarantees (Sagawa et al., 2020; Vovk et al., 2005; Romano et al., 2020). For multi-label settings, dependencies between outputs necessitate structured calibration (Zhang and Zhou, 2014; Vaicenavicius et al., 2019). We build on prior approaches to multi-class calibration (Kumar et. al., 2018) and extend them with a label-conditional objective that remains stable under covariate shift.\n\nOur experiments evaluate calibration under synthetic corruptions and real-world drifts, reporting ECE, ACE, and coverage metrics alongside accuracy.",
    "reason": "Incorrect abbreviation 'et. al.'; should be 'et al.' as in '(Kumar et al., 2018)'.",
    "start": 841,
    "end": 862,
    "label": "Format"
  },
  {
    "span": "Exposure ranking can amplify bias (Singh and Joachims, 2018). Personalization can mitigate perceived unfairness (Biega et al., 2020). Calibration aligns relevance with user intent (Steck, 2018). Causal approaches model interventions on feedback loops (Schnabel et al., 2016).",
    "document": "Related Work\n\nFairness in recommender systems has been examined from user-centric, item-producer, and platform perspectives. A central challenge is disentangling relevance from systemic exposure inequities while maintaining user satisfaction. Existing literature proposes algorithmic and evaluation adjustments across multiple axes.\n\nExposure ranking can amplify bias (Singh and Joachims, 2018). Personalization can mitigate perceived unfairness (Biega et al., 2020). Calibration aligns relevance with user intent (Steck, 2018). Causal approaches model interventions on feedback loops (Schnabel et al., 2016).\n\nHowever, few studies provide a unifying objective that jointly constrains exposure, utility, and long-term dynamics. Our work introduces a constrained optimization view that bridges short-term utility and fairness-aware exposure under counterfactual logging.",
    "reason": "The cited statements are presented as isolated facts without clarifying their relationships or building a cohesive argument; the abrupt shifts create coherence problems.",
    "start": 334,
    "end": 609,
    "label": "Coherence"
  },
  {
    "span": "Zhou et al.",
    "document": "Related Work\n\nContrastive objectives. Zhou et al. propose a sentence-level contrastive framework that pulls together paraphrases while pushing apart distractors, demonstrating improvements on STS benchmarks. Subsequent studies incorporate hard negatives from retrieval (Tan and Li, 2021) and multi-view augmentations (Gao and Liu, 2021). Nevertheless, these objectives can overfit to surface similarity when supervision is scarce.\n",
    "reason": "Narrative citation missing the year. Should be \"Zhou et al. (YEAR)\" with the publication year in parentheses.",
    "start": 38,
    "end": 49,
    "label": "Format"
  },
  {
    "span": "Kumar et al., 2021)",
    "document": "Introduction\n\nRecent parameter-efficient tuning techniques, such as adapters and prefix layers, have reduced the cost of domain customization (Houlsby et al., 2019; Li and Liang, 2021). However, these methods often overlook acquisition bias introduced by heuristic selection (Ash et al., 2020). As argued by Kumar et al., 2021) the choice of acquisition metric can entrench spurious correlations learned during pretraining. We mitigate this effect with a reweighting scheme that matches feature marginals across the labeled pool and the target distribution, related to importance weighting in covariate shift (Sugiyama et al., 2007).",
    "reason": "Unbalanced/mismatched parentheses: closing parenthesis present without a corresponding opening parenthesis for the citation.",
    "start": 308,
    "end": 327,
    "label": "Format"
  },
  {
    "span": "the widely used WMT14 English–German benchmark",
    "document": "Introduction\n\nNeural machine translation (NMT) continues to benefit from larger models and datasets, but scaling alone does not address domain shift and adequacy errors. Recent strategies include curriculum learning, uncertainty modeling, and external memory for rare words. We evaluate on the widely used WMT14 English–German benchmark and perform ablations to understand the contribution of each component under both constrained and unconstrained settings. Additionally, we test cross-domain robustness on news and parliamentary proceedings to assess generalization.",
    "reason": "First mention of a specific benchmark dataset should include a citation to the WMT14 En–De task or dataset release; none is given.",
    "start": 290,
    "end": 336,
    "label": "Unsupported_claim"
  },
  {
    "span": "Graph-based recommender systems include neighborhood aggregation, graph convolution with light weighting, and random-walk-based embedding for large-scale item graphs (Wang et al., 2019; He et al., 2020; Ying et al., 2018; Qiu et al., 2020).",
    "document": "Introduction\n\nRecommender systems with graph structure\n\nUser–item interactions form sparse, high-dimensional graphs where inductive biases for connectivity and homophily can be exploited. Graph-based models have become standard due to their ability to propagate signals along interaction edges and higher-order neighborhoods.\n\nGraph-based recommender systems include neighborhood aggregation, graph convolution with light weighting, and random-walk-based embedding for large-scale item graphs (Wang et al., 2019; He et al., 2020; Ying et al., 2018; Qiu et al., 2020).\n\nContext and dynamics\n\nPractical recommenders must handle temporal dynamics, exposure bias, and cold-start issues, which are not fully captured by static graph encoders. There is increasing interest in causal reasoning and counterfactual evaluation frameworks.\n\nFocus of this work\n\nWe propose a temporally aware graph encoder that disentangles preference shifts from exposure effects.",
    "reason": "The span lists categories and citations but does not articulate how these methods relate to the authors’ problem setting or what gap remains, exhibiting lack of synthesis (criteria a and b).",
    "start": 327,
    "end": 567,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent work demonstrates that masked language modeling on proteins captures tertiary structure.",
    "document": "Related Work\n\nProtein language models leverage vast unlabeled sequence databases to learn contextual representations that transfer to structure and function prediction. Early models focused on residue-level embeddings, while more recent architectures introduce long-context attention and multi-task objectives. Recent work demonstrates that masked language modeling on proteins captures tertiary structure. Nevertheless, performance degrades on rare folds and disordered regions, suggesting that sequence-only pretraining may miss critical biophysical constraints. Our study augments masked modeling with geometric self-distillation signals derived from coarse contact maps.",
    "reason": "Uses 'recent work demonstrates' to assert a specific scientific claim without citing any studies.",
    "start": 311,
    "end": 406,
    "label": "Unsupported_claim"
  },
  {
    "span": "McMahan et al. (2017) introduce federated learning to train models over decentralized data. Secure aggregation protects model updates by cryptographic protocols (Bonawitz et al., 2017). DP-SGD clips gradients and adds noise to bound privacy loss (Abadi et al., 2016).",
    "document": "Related Work\n\nPrivacy-Preserving Machine Learning\n\nPractical privacy requires minimizing leakage while maintaining utility in distributed settings. Techniques span data minimization, cryptography, and randomized training. McMahan et al. (2017) introduce federated learning to train models over decentralized data. Secure aggregation protects model updates by cryptographic protocols (Bonawitz et al., 2017). DP-SGD clips gradients and adds noise to bound privacy loss (Abadi et al., 2016). Yet, their combined impact on convergence speed and personalization remains insufficiently characterized.\n\nEvaluation Settings\n\nHeterogeneous client data and partial participation complicate learning dynamics. We benchmark personalization under strict privacy and communication budgets.",
    "reason": "The three sentences list different techniques without transitions or description of how they relate, resulting in abrupt, low-coherence connections.",
    "start": 222,
    "end": 489,
    "label": "Coherence"
  },
  {
    "span": "A recent meta-analysis shows that reinforcement learning with human feedback improves factuality by 20–30% across datasets.",
    "document": "Introduction\n\nLarge language models often generate fluent yet factually incorrect content, motivating methods that align models with human preferences and external knowledge. Reinforcement learning with human feedback (RLHF) fine-tunes models to better reflect desired behaviors during generation. A recent meta-analysis shows that reinforcement learning with human feedback improves factuality by 20–30% across datasets. Despite these gains, measurement remains challenging due to evaluator bias, prompt sensitivity, and the scarcity of domain-specific human feedback.",
    "reason": "Reports a quantitative finding from \"a recent meta-analysis\" without providing a citation (rule d; statistical claim without evidence).",
    "start": 298,
    "end": 421,
    "label": "Unsupported_claim"
  },
  {
    "span": "Secure multiparty computation (MPC) and homomorphic encryption (HE) enable private inference and training by operating on encrypted or secret-shared data (Acar et al., 2018; Mohassel and Zhang, 2017). Protocols have been proposed for linear layers, activation functions, and optimization steps (Riazi et al., 2018; Mishra et al., 2020; Cho et al., 2021). In this work, we propose an efficient protocol for transformer inference under MPC.",
    "document": "Introduction\n\nPrivacy-preserving machine learning allows parties to collaborate without exposing raw data. Achieving practical efficiency while preserving formal guarantees remains challenging, especially for deep architectures with non-linearities and attention mechanisms. Recent advances in cryptography and systems have narrowed the gap but have not eliminated it.\n\nSecure multiparty computation (MPC) and homomorphic encryption (HE) enable private inference and training by operating on encrypted or secret-shared data (Acar et al., 2018; Mohassel and Zhang, 2017). Protocols have been proposed for linear layers, activation functions, and optimization steps (Riazi et al., 2018; Mishra et al., 2020; Cho et al., 2021). In this work, we propose an efficient protocol for transformer inference under MPC.\n\nOur protocol uses approximate attention via low-degree polynomial kernels and pre-shared randomness to reduce interaction, yielding latency reductions on WAN settings while maintaining accuracy within 0.5 BLEU on translation benchmarks.",
    "reason": "The span transitions from a list of prior cryptographic techniques directly to the authors’ contribution without making the gap explicit or explaining how prior methods fall short for transformer inference.",
    "start": 370,
    "end": 808,
    "label": "Lacks_synthesis"
  },
  {
    "span": "To our knowledge, this is the first work to apply transformers to ancient Greek lemmatization.",
    "document": "Related Work\n\nMorphological analysis and lemmatization for historical languages have relied on character models and finite-state analyzers due to limited annotated data (Cotterell et al., 2016; Malouf, 2019). Neural sequence-to-sequence methods with attention improved inflection generation under low-resource constraints (Kann and Schütze, 2016; Peters and Martins, 2020). For ancient Greek, prior studies focus on lexicon-driven analyzers and CRF-based taggers trained on treebanks (Bamman and Crane, 2011; Celano et al., 2016). To our knowledge, this is the first work to apply transformers to ancient Greek lemmatization. We combine multilingual pretraining with adapter-based finetuning to leverage related Indo-European languages.",
    "reason": "A novelty claim about being the first requires contextual evidence or citations to prior attempts; none are provided.",
    "start": 531,
    "end": 625,
    "label": "Unsupported_claim"
  },
  {
    "span": "the MIMIC-IV EHR dataset",
    "document": "Introduction\n\nClinical outcome prediction from electronic health records (EHRs) requires models that can handle irregular sampling, missingness, and heterogeneous feature types. Transformer-based architectures have recently shown promise in modeling long-range temporal dependencies in clinical sequences while capturing complex feature interactions.\n\nTo evaluate our method, we conduct experiments on the MIMIC-IV EHR dataset and compare against non-sequential baselines, temporal convolutional networks, and recurrent models under consistent preprocessing. We report results on standard metrics for in-hospital mortality and 30-day readmission prediction and provide per-subgroup analyses to assess fairness across demographic cohorts.\n\nOur contributions include a simple yet effective imputation-aware positional encoding, a calibration-focused loss, and a suite of robustness tests under synthetic missingness.\n",
    "reason": "Introduces a specific, well-known dataset without providing a citation at first mention.",
    "start": 402,
    "end": 426,
    "label": "Unsupported_claim"
  },
  {
    "span": "RoBERTa has been successfully applied to code summarization using the CodeSearchNet corpus.",
    "document": "Related Work\n\nNeural code summarization maps source code to concise natural-language descriptions, aiding code comprehension and maintenance (Allamanis et al., 2018). Early sequence-to-sequence models relied on RNNs and copy mechanisms tailored to identifier distributions (Iyer et al., 2016; Hu et al., 2018). Transformer-based encoders that leverage subtokenization and structural signals from abstract syntax trees have improved generation quality (LeClair et al., 2019; Ahmad et al., 2020). Pretrained models on large code corpora further boost performance through masked language modeling and span denoising (Feng et al., 2020; Guo et al., 2021).\n\nRoBERTa has been successfully applied to code summarization using the CodeSearchNet corpus. Building on this line, we introduce retrieval-augmented decoding that conditions on semantically similar code snippets to improve faithfulness and specificity of summaries.",
    "reason": "Mentions a specific prior application on a named dataset without providing a supporting citation.",
    "start": 653,
    "end": 744,
    "label": "Unsupported_claim"
  },
  {
    "span": "Personalization in federated learning has been studied via local fine-tuning, model interpolation, meta-learning, and multi-task formulations (Arivazhagan et al., 2019; Dinh et al., 2020; Fallah et al., 2020; Smith et al., 2017). Clustered FL partitions clients into groups based on learned representations or update similarity (Sattler et al., 2020; Ghosh et al., 2020).",
    "document": "Related Work\n\nFederated learning (FL) enables decentralized training across heterogeneous clients while preserving data locality. Heterogeneity in data distributions and resource constraints motivates methods that adapt global models to client-specific needs.\n\nPersonalization in federated learning has been studied via local fine-tuning, model interpolation, meta-learning, and multi-task formulations (Arivazhagan et al., 2019; Dinh et al., 2020; Fallah et al., 2020; Smith et al., 2017). Clustered FL partitions clients into groups based on learned representations or update similarity (Sattler et al., 2020; Ghosh et al., 2020).\n\nThis paper proposes a calibration-aware personalization layer that aligns client logits to a shared temperature-normalized geometry, improving transfer under severe label skew. We provide convergence guarantees under bounded drift and evaluate on vision and NLP tasks with extreme heterogeneity.",
    "reason": "The span enumerates categories of FL personalization and clustering with citations but does not connect these summaries to the paper’s method, gap, or motivation, thus lacking synthesis (a, c).",
    "start": 261,
    "end": 632,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Garcia et al. 2017)",
    "document": "Related Work\n\nSafe reinforcement learning (RL) introduces mechanisms to satisfy constraints during exploration and deployment. Shielding and constrained policy optimization have been studied extensively (Achiam et al., 2017; Ray et al., 2019). Another line regularizes policies using risk-sensitive criteria (Tamar et al., 2015). Techniques for safe model-based RL aim to propagate uncertainty through dynamics models (Berkenkamp et al., 2017; Chua et al., 2018). For a comprehensive survey, see (Garcia et al. 2017).",
    "reason": "Missing comma between authors and year in a parenthetical citation; should be (Garcia et al., 2017).",
    "start": 496,
    "end": 516,
    "label": "Format"
  },
  {
    "span": "(Miller 2017)",
    "document": "Related Work\n\nSyntactic parsing has evolved from probabilistic context-free grammars to neural span-based models. Transition systems enabled efficient dependency parsing with learned feature representations (Nivre, 2008). Neural chart parsers combine biaffine scoring with global inference for competitive accuracy (Dozat and Manning, 2017). Recent approaches integrate pre-trained language models to capture long-range dependencies (Zhou and Ma, 2020). Earlier chart-based models (Miller 2017) struggled with domain transfer due to brittle feature templates, motivating robust pretraining and domain adaptation (Clark and Smith, 2019). We propose a hybrid parser that merges span scoring with latent structural priors to improve generalization on low-resource domains (Kramer and Liu, 2022).",
    "reason": "Missing comma between author and year in the parenthetical citation; should be '(Miller, 2017)'.",
    "start": 481,
    "end": 494,
    "label": "Format"
  },
  {
    "span": "For time-series causal discovery, constraint-based methods (Spirtes et al., 2000), score-based search (Chickering, 2002), Granger causality (Granger, 1969), and deep learning approaches (Tank et al., 2018; Pamfil et al., 2020) have been widely explored.",
    "document": "Introduction\n\nCausal discovery in time series. Identifying causal relations from temporal data underpins scientific understanding and reliable forecasting. For time-series causal discovery, constraint-based methods (Spirtes et al., 2000), score-based search (Chickering, 2002), Granger causality (Granger, 1969), and deep learning approaches (Tank et al., 2018; Pamfil et al., 2020) have been widely explored. Extensions consider latent confounding, nonlinearity, and interventions, as well as structure learning in vector autoregressive and nonparametric settings.\n\nScalability and identifiability. Practical deployment requires algorithms that scale to hundreds of variables and provide identifiable causal graphs under reasonable assumptions. Recent work proposes continuous relaxations and sparsity-inducing objectives to enable gradient-based optimization (Zheng et al., 2018; Pamfil et al., 2020).\n\nEvaluation. Synthetic benchmarks and limited real-world case studies are commonly used, though standardized, domain-relevant evaluations remain challenging due to missing ground truth (Runge et al., 2019).\n",
    "reason": "The sentence lists broad families of approaches without explaining their trade-offs or the precise problem the paper addresses, thus failing to synthesize prior work with the paper’s aims (criteria a and c).",
    "start": 156,
    "end": 409,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Chen and Rao, 2017; Kim et al., 2019;;",
    "document": "Related Work\n\nSequence modeling transitioned from recurrent architectures to attention-based models that scale with context length (Bahdanau et al., 2015; Luong et al., 2015). Early transformer variants improved training stability and data efficiency (Popel and Bojar, 2018; Press et al., 2020). In pretraining for language understanding, encoder-only models leverage masked objectives for broad coverage, while decoder-only models focus on next-token prediction for fluent generation. Our comparison emphasizes architectural choices and optimization tricks relative to prior sequence models (Chen and Rao, 2017; Kim et al., 2019;; Vaswani et al., 2017) and concurrent scaling laws (Kaplan et al., 2020).",
    "reason": "Extra semicolon within a multi-citation parenthetical.",
    "start": 592,
    "end": 631,
    "label": "Format"
  },
  {
    "span": "Johnson and Li, 2017)",
    "document": "Related Work\n\nPolicy gradient methods optimize parameterized policies directly via sampled returns (Sutton et al., 2000). Trust-region techniques stabilize updates by constraining divergence (Schulman et al., 2015; Achiam, 2017). Actor-critic algorithms reduce variance by learning a baseline value function (Konda and Tsitsiklis, 2000). We build on Johnson and Li, 2017) to introduce a natural gradient preconditioner tailored for partially observable environments, and we compare to modern proximal variants (Schulman et al., 2017).",
    "reason": "Missing opening parenthesis for a parenthetical citation; should be (Johnson and Li, 2017).",
    "start": 350,
    "end": 371,
    "label": "Format"
  },
  {
    "span": "Model compression techniques such as pruning, quantization, and knowledge distillation have been extensively explored for edge deployment (Han et al., 2016; Jacob et al., 2018; Hinton et al., 2015).",
    "document": "Related Work\n\nDeploying deep models on edge devices requires balancing accuracy with energy, latency, and memory constraints under dynamic workloads. System-level schedulers and runtime adaptors complement model-level compression by exploiting hardware heterogeneity.\n\nCompression methods. Model compression techniques such as pruning, quantization, and knowledge distillation have been extensively explored for edge deployment (Han et al., 2016; Jacob et al., 2018; Hinton et al., 2015). Recent works propose mixed-precision strategies and neural architecture search to trade accuracy for cost along Pareto fronts (Cai et al., 2020; Wu et al., 2019; Tan and Le, 2019).\n\nOur study targets energy-proportional inference under bursty traffic, introducing a budget-aware controller that coordinates early exiting with on-device DVFS policies across varying input difficulties.",
    "reason": "The span enumerates techniques without indicating which limitations persist for energy-proportionality or how they motivate the proposed controller, thus failing to synthesize prior art with the paper’s objective.",
    "start": 290,
    "end": 488,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Brown et al., 2020.",
    "document": "Introduction\n\nLarge-scale pretrained language models have redefined transfer learning in NLP by enabling strong zero- and few-shot performance (Radford et al., 2019; Raffel et al., 2020). Prompt-based conditioning further reduces task-specific parameters and improves data efficiency, as shown in (Brown et al., 2020. However, prompt sensitivity and distribution shift can degrade robustness, motivating approaches that combine prompts with lightweight adapters (Houlsby et al., 2019) or retrieval augmentation (Lewis et al., 2020). We extend this line by studying calibration-aware prompting strategies under low-resource settings.\n",
    "reason": "Missing closing parenthesis in a parenthetical citation.",
    "start": 297,
    "end": 317,
    "label": "Format"
  },
  {
    "span": "Vision Transformers and their hierarchical variants have been applied to medical image analysis, including classification, segmentation, and detection (Dosovitskiy et al., 2021; Touvron et al., 2021; Liu et al., 2021; Chen et al., 2021).",
    "document": "Related Work\n\nTransformers in medical imaging. Transformers have shown promise in modeling long-range dependencies in 2D and 3D scans. Vision Transformers and their hierarchical variants have been applied to medical image analysis, including classification, segmentation, and detection (Dosovitskiy et al., 2021; Touvron et al., 2021; Liu et al., 2021; Chen et al., 2021). Hybrid CNN-Transformer backbones and self-distillation have also been explored.\n\nData efficiency. Scarcity of labeled scans often limits performance. Strategies include self-supervision, synthetic augmentation, and multi-site pretraining.\n\nThis work presents a token-sparse pretraining scheme designed to reduce compute while preserving diagnostic fidelity on small datasets.\n",
    "reason": "The span only inventories applications and citations without linking them to the paper’s data efficiency focus or identifying what is missing, thus failing to synthesize prior work with the authors’ aims.",
    "start": 135,
    "end": 372,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Neural-guided search prunes program candidates using learned heuristics (Balog et al., 2017). Large code models enable few-shot program generation (Chen et al., 2021). Type-aware decoders enforce syntactic constraints (Maddison and Tarlow, 2014).",
    "document": "Introduction\n\nProgram synthesis from specifications spans enumerative, constraint-based, and neural methods. Recent work explores hybrid approaches that combine symbolic guarantees with learned components for scalability.\n\nNeural-guided search prunes program candidates using learned heuristics (Balog et al., 2017). Large code models enable few-shot program generation (Chen et al., 2021). Type-aware decoders enforce syntactic constraints (Maddison and Tarlow, 2014). Verification-aided decoding can further filter candidates against specifications (Shi et al., 2022). Despite these advances, understanding trade-offs between generalization and correctness is still challenging.\n\nWe introduce a benchmark with variable specification ambiguity and measure the interplay between guidance strength, decoding constraints, and verification depth.",
    "reason": "The span enumerates three different strands of program synthesis with no connective explanation of how they relate or differ. This creates abrupt shifts and leaves relationships implied rather than explicit.",
    "start": 223,
    "end": 469,
    "label": "Coherence"
  },
  {
    "span": "Mnih et al. (2015) introduced DQN for value-based control with experience replay. Koren et al. (2009) surveyed collaborative filtering algorithms. Zhao et al. (2017) applied deep RL to recommendation with slate optimization.",
    "document": "Related Work\n\nReinforcement learning (RL) has been increasingly applied to recommender systems to model long-term user engagement. Prior work spans value-based, policy-based, and bandit formulations, often integrating user modeling and exposure constraints.\n\nMnih et al. (2015) introduced DQN for value-based control with experience replay. Koren et al. (2009) surveyed collaborative filtering algorithms. Zhao et al. (2017) applied deep RL to recommendation with slate optimization. Chen et al. (2019) studied constrained bandits for safe exploration.\n\nOur method leverages off-policy actor-critic with counterfactual corrections to mitigate exposure bias in logged implicit feedback.",
    "reason": "The sequence abruptly mixes general collaborative filtering with deep RL papers without clarifying how collaborative filtering connects to RL or transitions between problem settings.",
    "start": 259,
    "end": 483,
    "label": "Coherence"
  },
  {
    "span": "In (Zhang et al., 2019)",
    "document": "Related Work\n\nTransfer learning aims to leverage knowledge from source domains to improve learning in a target domain with limited labels (Pan and Yang, 2010; Ruder, 2019). In computer vision, pretraining on large-scale datasets followed by fine-tuning has become the de facto standard (He et al., 2016; Kolesnikov et al., 2020). In NLP, contextual language models have similarly transformed downstream performance (Peters et al., 2018; Devlin et al., 2019).\n\nIn (Zhang et al., 2019), the authors investigate negative transfer when source and target distributions diverge substantially. They propose discrepancy-aware regularization to attenuate spurious correlations introduced during source pretraining. Subsequent work extends these ideas to multi-source settings and domain generalization by aligning feature spaces across tasks (Ganin et al., 2016; Gulrajani and Lopez-Paz, 2021).\n\nOur work complements this line by diagnosing transfer failure modes at the layer level and introducing a simple adapter that gates source-specific features during fine-tuning.",
    "reason": "Wrong citation style: placing the citation in parentheses after 'In' is ungrammatical; it should read 'In Zhang et al. (2019), ...' or be rephrased to an in-text style.",
    "start": 460,
    "end": 483,
    "label": "Format"
  },
  {
    "span": "Johnson et al. 3",
    "document": "Introduction\n\nInterpretable machine learning seeks models whose predictions can be explained to end users and regulators. Classical linear models and decision trees provide inherent interpretability (Rudin, 2019), while post hoc methods approximate explanations for black-box models (Ribeiro et al., 2016; Lundberg and Lee, 2017). Johnson et al. 3 argue that faithful explanations must be consistent under input perturbations, inspiring stability metrics for attribution methods. Building on these ideas, we propose a training objective that balances task loss and explanation stability.",
    "reason": "Improper footnote-style marker in a citation; should include a year or be formatted as a proper footnote or standard citation (e.g., Johnson et al. (YYYY)).",
    "start": 331,
    "end": 347,
    "label": "Format"
  },
  {
    "span": "Recent advances demonstrate that off-policy RL dramatically improves long-term coherence.",
    "document": "Related Work\n\nReinforcement learning (RL) has been explored to move beyond myopic next-utterance prediction in both task-oriented and open-domain dialogue. Reward shaping with dialog success, user satisfaction proxies, and semantic coverage has shown promise but remains brittle under sparse and delayed signals. Recent advances demonstrate that off-policy RL dramatically improves long-term coherence. Complementary work studies offline RL from logged conversations, contrastive value learning, and constrained decoding to mitigate safe but dull responses. Our method unifies model-based planning with controllable generation, targeting sample efficiency and safety via explicit uncertainty estimates.",
    "reason": "Uses the phrase 'Recent advances demonstrate' to claim performance improvements without citing any works (rule d).",
    "start": 313,
    "end": 402,
    "label": "Unsupported_claim"
  },
  {
    "span": "Batch-constrained RL restricts policy support to the dataset distribution (Fujimoto et al., 2019). Advantage-weighted regression fits a policy to filtered actions (Peng et al., 2019). Behavior cloning minimizes negative log-likelihood of actions (Pomerleau, 1989).",
    "document": "Related Work\n\nOffline Reinforcement Learning. Learning policies from fixed datasets avoids unsafe exploration but risks extrapolation error when policies query unsupported states or actions. Algorithms impose constraints or regularization to remain within the data manifold.\n\nPolicy Learning and Regularization. A variety of methods modify targets, constrain policies, or reweight samples to stabilize training under distributional shift.\n\nRepresentative Approaches. Batch-constrained RL restricts policy support to the dataset distribution (Fujimoto et al., 2019). Advantage-weighted regression fits a policy to filtered actions (Peng et al., 2019). Behavior cloning minimizes negative log-likelihood of actions (Pomerleau, 1989).\n\nValue Regularization. Other lines adjust Q-function estimation or apply conservative backups to reduce overestimation on out-of-distribution actions.",
    "reason": "The span mentions three methods sequentially without articulating how they compare or relate, leaving the reader to infer connections and creating an abrupt, incoherent flow.",
    "start": 467,
    "end": 731,
    "label": "Coherence"
  },
  {
    "span": "COCO",
    "document": "Introduction\n\nObject detection has progressed rapidly with anchor-based and anchor-free paradigms, benefiting from stronger backbones and multi-scale feature aggregation. To compare fairly with prior detectors, we report results on COCO and analyze performance across object sizes and crowding conditions. Our contribution is a decoupled head with dynamic label assignment that reduces classification–localization conflict and improves stability during training.",
    "reason": "First mention of a major benchmark dataset lacks a citation (rule a).",
    "start": 232,
    "end": 236,
    "label": "Unsupported_claim"
  },
  {
    "span": "Patel et al., (2017)",
    "document": "Related Work\n\nDomain adaptation in speech recognition. Unsupervised adaptation techniques align feature distributions between source and target domains (Sun et al., 2017; Shinohara, 2016). Patel et al., (2017) introduce adversarial objectives to encourage domain-invariant acoustic features, while later work combines adversarial losses with self-training (Karita et al., 2018; Park et al., 2020). Semi-supervised approaches leverage pseudo-labels and confidence filtering to reduce error propagation (Xie et al., 2020; Xu et al., 2021).\n\nIn parallel, data augmentation strategies such as SpecAugment have become standard for robustness (Park et al., 2019).",
    "reason": "Incorrect punctuation in the narrative citation: there should be no comma before the year; it should read “Patel et al. (2017)”.",
    "start": 189,
    "end": 209,
    "label": "Format"
  },
  {
    "span": "The MS-COCO 2017 split is universally adopted for weakly supervised object detection.",
    "document": "Related Work\n\nWeakly supervised object detection (WSOD) aims to learn detectors using only image-level labels rather than bounding boxes. Early approaches relied on multiple-instance learning and heuristics to bridge the gap between image-level supervision and instance-level localization. Recent progress leverages region proposals, attention mechanisms, and self-training to progressively refine pseudo boxes.\n\nThe MS-COCO 2017 split is universally adopted for weakly supervised object detection. While WSOD methods are also studied on VOC and OpenImages, reporting conventions, evaluation metrics, and training pipelines vary widely, complicating comparison across papers. In this work, we standardize the training schedule and augment proposal mining with consistency constraints to isolate gains attributable to modeling rather than dataset or protocol discrepancies.",
    "reason": "Asserts a universal community practice about dataset adoption without any citation or evidence; such a community-wide claim requires references.",
    "start": 413,
    "end": 498,
    "label": "Unsupported_claim"
  },
  {
    "span": "Most recent works adopt contrastive pre-training with large batches of in-batch negatives to learn sentence embeddings.",
    "document": "Related Work\n\nSentence representation learning has progressed from unsupervised objectives such as next-sentence prediction and context prediction (Kiros et al., 2015; Logeswaran and Lee, 2018) to supervised training on natural language inference and paraphrase datasets (Conneau et al., 2017; Cer et al., 2018; Reimers and Gurevych, 2019). Recent advances emphasize instance discrimination signals that align semantically similar sentences and repel dissimilar ones (Chen et al., 2020; Gao et al., 2021). Most recent works adopt contrastive pre-training with large batches of in-batch negatives to learn sentence embeddings. In parallel, encoder-decoder pretraining has been adapted for retrieval and question answering by aggregating evidence across documents (Lewis et al., 2020; Izacard and Grave, 2021), but adapting these methods for universal sentence embeddings remains an open question.",
    "reason": "Claims that 'most recent works' use a specific method without providing any citations, violating rule (d) about mentioning recent works without references.",
    "start": 506,
    "end": 625,
    "label": "Unsupported_claim"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nGraph neural networks (GNNs) have transformed learning over relational data by enabling message passing among nodes (Hamilton et al., 2017; Kipf and Welling, 2017). However, over-smoothing and limited expressivity hinder depth and generalization (Xu et al., 2019; Oono and Suzuki, 2020).\n\nMost prior work in our setting adopts author–year citations, e.g., (Bianchi et al., 2021; Frasca and Bronstein, 2022). Nevertheless, some studies report scalability improvements via sampling-based training as noted in [12], while others exploit spectral filters to stabilize deep stacks (Chen et al., 2020). Our method integrates sampling with residual propagation to mitigate oversquashing (Topping et al., 2022).",
    "reason": "Numeric citation '[12]' is inconsistent with the surrounding author–year style; should be replaced with an author–year citation (e.g., Smith et al., YEAR).",
    "start": 521,
    "end": 525,
    "label": "Format"
  },
  {
    "span": "in (Larsen et al., 2022)",
    "document": "Related Work\n\nDomain Adaptation and Robust Transfer\n\nDomain adaptation addresses distribution shifts between labeled source and unlabeled target data (Ben-David et al., 2010; Pan and Yang, 2010). Deep approaches leverage adversarial objectives to align feature distributions (Ganin and Lempitsky, 2015) and confidence-based self-training to refine pseudo-labels (Zou et al., 2019). Recent work highlights target-robust representations in (Larsen et al., 2022), while others study class-conditional alignment (Saito et al., 2018) and batch-normalization statistics for adaptation (Chang et al., 2019). In semi-supervised shifts, entropy minimization remains common (Grandvalet and Bengio, 2005), although consistency regularization further improves stability (French et al., 2018).\n",
    "reason": "Wrong citation style: a narrative construction uses a parenthetical citation after the preposition 'in'; it should be 'in Larsen et al. (2022)'.",
    "start": 435,
    "end": 459,
    "label": "Format"
  },
  {
    "span": "In (Lopez et al., 2018)",
    "document": "Introduction\n\nSemi-supervised learning has become a central approach for leveraging large pools of unlabeled data in natural language processing. Early work explored consistency regularization and virtual adversarial training to stabilize decision boundaries (Miyato et al., 2018; Sajjadi et al., 2016). More recent advances combine pseudo-labeling with strong data augmentation to improve robustness (Xie et al., 2020; Chen et al., 2020). In (Lopez et al., 2018) we observe that curriculum strategies can further mitigate confirmation bias in pseudo-labeling, though their sensitivity to class imbalance remains under-explored. Complementary lines of research integrate self-training with pretraining objectives to narrow the gap between pretext and downstream tasks (Gururangan et al., 2020; Phang et al., 2019). Our work builds on these trends by unifying uncertainty-aware pseudo-label selection with augmentation policies tuned to domain shift, demonstrating gains on low-resource sequence labeling benchmarks.\n\nRelated Work\n\nConsistency-based objectives enforce output invariance under perturbations (Sohn et al., 2020) while contrastive pretraining improves representation separability (Gao et al., 2021). Data augmentation tailored for text—such as back-translation and word replacement—has proven especially effective when combined with pseudo-label refinement (Edunov et al., 2018; Wei and Zou, 2019). However, domain mismatch can degrade pseudo-label quality, necessitating adaptive thresholds or calibration (Zhang et al., 2021).",
    "reason": "Wrong citation style: preposition placed before a parenthetical citation. Should be narrative style, e.g., \"In Lopez et al. (2018) we ...\"",
    "start": 440,
    "end": 463,
    "label": "Format"
  },
  {
    "span": "Most task-oriented dialogue systems now adopt an end-to-end Transformer backbone.",
    "document": "Related Work\n\nTask-oriented dialogue (TOD) traditionally decomposes into modules for state tracking, policy learning, and natural language generation (Young et al., 2013; Williams et al., 2017). The rise of pretrained language models has enabled unified modeling where a single network handles multiple TOD sub-tasks (Budzianowski and Vulić, 2019; Hosseini-Asl et al., 2020). Data augmentation and synthetic user simulators have further improved robustness (Campagna et al., 2020; Rastogi et al., 2020).\n\nMost task-oriented dialogue systems now adopt an end-to-end Transformer backbone. Recent studies also explore retrieval augmentation, schema-guided generalization, and multi-domain adaptation (Kale and Rastogi, 2020; Peng et al., 2021). Our work builds on unified modeling but introduces compositional constraints to enhance generalization to unseen slot-value combinations.",
    "reason": "Claims the prevailing approach in the field without citing supporting surveys or representative studies.",
    "start": 505,
    "end": 586,
    "label": "Unsupported_claim"
  },
  {
    "span": "KGCN propagates user preferences over entity neighbors to enrich item representations (Wang et al., 2019). RippleNet performs preference diffusion over knowledge paths (Wang et al., 2018). KGAT learns attentive user–item–entity interactions (Wang et al., 2019b). BPR remains a strong baseline for implicit feedback ranking (Rendle et al., 2009).",
    "document": "Related Work\n\nKnowledge Graphs for Recommendation\n\nIncorporating structured knowledge can mitigate sparsity and improve explainability in recommender systems. Methods vary in how they traverse or attend to multi-hop relations, how they regularize entity embeddings, and how they integrate side information with user–item interactions.\n\nKGCN propagates user preferences over entity neighbors to enrich item representations (Wang et al., 2019). RippleNet performs preference diffusion over knowledge paths (Wang et al., 2018). KGAT learns attentive user–item–entity interactions (Wang et al., 2019b). BPR remains a strong baseline for implicit feedback ranking (Rendle et al., 2009).\n\nExplainability and Path Reasoning\n\nPath-based reasoning methods generate human-interpretable trails (Xian et al., 2019), while reinforcement learning explores relation sequences (Cai and Wang, 2018). Contrastive training aligns items and entities to reduce spurious correlations (Zhu et al., 2021).\n\nOur Contribution\n\nWe introduce a counterfactual path sampler that contrasts causal and spurious trails, improving both ranking accuracy and explanation fidelity.",
    "reason": "The span lists methods with no transitions or explicit statements about how they relate; it also abruptly introduces BPR without clarifying its relevance to KG-based methods, resulting in weak coherence.",
    "start": 336,
    "end": 681,
    "label": "Coherence"
  },
  {
    "span": "DP-SGD and privacy accounting techniques have been adopted for text models (Abadi et al., 2016; Mironov, 2017; McMahan et al., 2018).",
    "document": "Related Work\n\nDifferential privacy in NLP. Privacy-preserving training seeks to protect sensitive text data while retaining utility. DP-SGD and privacy accounting techniques have been adopted for text models (Abadi et al., 2016; Mironov, 2017; McMahan et al., 2018). Recent lines explore private fine-tuning of large language models and private aggregation for federated NLP (Yu et al., 2021; Li et al., 2022).\n\nProblem considered. We aim to maintain competitive downstream accuracy under tight privacy budgets when labeled data are scarce and class-imbalanced.\n\nOur proposal. We introduce a label-aware noise scheduling scheme and representation stabilization that reduce privacy-utility degradation on low-resource text classification.",
    "reason": "The sentence lists DP methods and citations without relating them to the specific low-resource, imbalanced NLP scenario or motivating the new technique, thus lacking synthesis.",
    "start": 133,
    "end": 266,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent works adopt propensity scoring, inverse probability weighting, and doubly robust estimators to mitigate selection bias in recommenders (Schnabel et al., 2016; Wang et al., 2019; Saito et al., 2020).",
    "document": "Introduction\n\nCausality in recommender systems. Observational logs in recommender systems suffer from exposure bias and user self-selection, challenging unbiased evaluation and learning. Recent works adopt propensity scoring, inverse probability weighting, and doubly robust estimators to mitigate selection bias in recommenders (Schnabel et al., 2016; Wang et al., 2019; Saito et al., 2020). Other studies explore instrumental variables and front-door adjustments when suitable proxies are available (Bonner and Vasile, 2018; Zhang et al., 2021).\n\nOur problem. We consider extreme sparsity and cold-start items where propensities are poorly specified and unstable.\n\nContributions. We introduce a calibration-aware, representation-sharing estimator that jointly learns exposures and preferences, improving robustness under sparse feedback.",
    "reason": "The sentence lists causal debiasing techniques and citations without explaining their limitations in the addressed setting or how the new method relates, hence lacking synthesis.",
    "start": 187,
    "end": 392,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Several recent competitions have standardized the Defects4J evaluation protocol.",
    "document": "Related Work\n\nAutomated program repair (APR) methods range from generate-and-validate approaches to semantics-guided synthesis (Monperrus, 2018; Gazzola et al., 2019). Benchmarks such as Defects4J, Bugs.jar, and QuixBugs are frequently used to compare patch correctness and generalization (Just et al., 2014; Lin et al., 2017). Recent efforts emphasize overfitting detection, patch plausibility, and test suite adequacy (Smith et al., 2015; Liu et al., 2019).\n\nSeveral recent competitions have standardized the Defects4J evaluation protocol. Parallel to these developments, studies propose unified taxonomies for repair operators and learning-based priors over edits (Le et al., 2016; Jiang et al., 2021). Our work contributes a search-space factorization that improves fix localization and reduces invalid patch generation.",
    "reason": "References 'recent competitions' and a specific standardization outcome without citing the competitions.",
    "start": 461,
    "end": 541,
    "label": "Unsupported_claim"
  },
  {
    "span": "Post-hoc saliency techniques like Grad-CAM and Integrated Gradients have been widely adopted (Selvaraju et al., 2017; Sundararajan et al., 2017). Propagation-based approaches such as LRP attribute relevance across layers (Bach et al., 2015). Prototype and concept-based methods aim for human-aligned explanations (Chen et al., 2019; Kim et al., 2018). Clinical studies have explored interpretability for trust and auditing (Tonekaboni et al., 2019; Rudin, 2019).",
    "document": "Related Work\n\nExplaining medical imaging models is essential for clinician trust, regulatory approval, and error analysis. A variety of techniques aim to reveal model rationales or provide interpretable evidence.\n\nPost-hoc saliency techniques like Grad-CAM and Integrated Gradients have been widely adopted (Selvaraju et al., 2017; Sundararajan et al., 2017). Propagation-based approaches such as LRP attribute relevance across layers (Bach et al., 2015). Prototype and concept-based methods aim for human-aligned explanations (Chen et al., 2019; Kim et al., 2018). Clinical studies have explored interpretability for trust and auditing (Tonekaboni et al., 2019; Rudin, 2019).\n\nConcurrently, evaluation protocols range from human-in-the-loop assessments to counterfactual faithfulness tests. We adopt standard datasets and expert-annotated regions in our experiments.",
    "reason": "The span aggregates prior explanation methods and clinical studies but does not clarify the authors' stance, the specific limitation being targeted, or how their work synthesizes or advances these lines.",
    "start": 214,
    "end": 676,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT has been widely adopted for automatic essay scoring",
    "document": "Introduction\n\nAutomatic essay scoring (AES) seeks to predict holistic or analytic writing quality using computational models (Shermis and Burstein, 2013). Traditional AES systems rely on engineered features capturing grammar, cohesion, and discourse structure (Attali and Burstein, 2006; Crossley and McNamara, 2012). With the rise of pretrained language models, BERT has been widely adopted for automatic essay scoring, yielding strong performance with limited feature engineering. Yet, the extent to which these models generalize across prompts and genres remains unclear. We study cross-prompt transfer and propose a prompt-adaptive calibration layer to mitigate prompt bias.",
    "reason": "This is a claim about prior work adoption and impact but provides no supporting citations to studies using BERT for AES.",
    "start": 363,
    "end": 419,
    "label": "Unsupported_claim"
  },
  {
    "span": "Brown et al. (2020) introduced in-context learning using large language models without gradient updates. Gao et al. (2021) proposed prompt-based fine-tuning for few-shot classification. Zhao et al. (2021) studied calibration methods for prompt predictions. Schick and Schütze (2021) presented pattern-exploiting training to leverage cloze-style prompts.",
    "document": "Related Work\n\nPrompting and Few-Shot Learning\n\nRecent advances in language modeling have shown that task performance can be improved by conditioning on carefully designed prompts rather than learning full task-specific heads. Methods vary in how they construct prompts, how they calibrate label spaces, and how they exploit unlabeled data for adaptation. Despite rapid progress, the relationships among these approaches remain difficult to compare when evaluation settings differ.\n\nBrown et al. (2020) introduced in-context learning using large language models without gradient updates. Gao et al. (2021) proposed prompt-based fine-tuning for few-shot classification. Zhao et al. (2021) studied calibration methods for prompt predictions. Schick and Schütze (2021) presented pattern-exploiting training to leverage cloze-style prompts.\n\nUnlike prior prompt-only approaches, our method couples automatic prompt search with stability-driven selection under distribution shift and evaluates under a consistent protocol across datasets. We further analyze sensitivity to verbalizers and provide an ablation on calibration components.",
    "reason": "The span lists four cited works in succession without transitions or explicit relationships among them; it is unclear how each work builds on or contrasts with the others.",
    "start": 482,
    "end": 835,
    "label": "Coherence"
  },
  {
    "span": "the DIV2K benchmark remains the de facto standard for evaluating single-image super-resolution models",
    "document": "Related Work\n\nSingle-image super-resolution (SISR) aims to reconstruct high-resolution images from low-resolution inputs by recovering missing details. Early interpolation-based methods are fast but struggle to capture complex textures. Deep convolutional networks substantially improved the state of the art by learning hierarchical features, and subsequent work explored residual connections, attention mechanisms, and frequency-domain priors.\n\nBenchmarking in SISR has coalesced around a few datasets and protocols. In particular, the DIV2K benchmark remains the de facto standard for evaluating single-image super-resolution models, with widely adopted scale factors and train/validation splits. Community challenges have further standardized metrics, pre-processing, and degradation models to enable reproducible comparisons across architectures.\n\nRecent trends include lightweight models for on-device inference and blind super-resolution where the degradation kernel is unknown. Knowledge distillation, dynamic convolution, and implicit neural representations have each shown promise in balancing quality and efficiency. Nevertheless, most approaches are tuned for synthetic bicubic downsampling and can struggle under real-world degradations.\n\nOur work contributes a content-aware degradation adapter that conditions the reconstruction network on an estimated blur-noise code. We show that this plug-in module can be attached to prior SISR backbones and improves robustness under realistic degradations without retraining the entire model.",
    "reason": "Asserts a dataset's de facto status but provides no citation or evidence supporting this community-wide claim.",
    "start": 534,
    "end": 635,
    "label": "Unsupported_claim"
  },
  {
    "span": "Common text augmentation strategies include token-level edits such as synonym replacement, insertion, and deletion (Wei and Zou, 2019), paraphrasing via back-translation (Sennrich et al., 2016; Edunov et al., 2018), and mixup or manifold interpolation techniques on hidden representations (Guo et al., 2019; Chen et al., 2020). Recently, pretrained language models have been used to generate task-conditioned synthetic examples (Anaby-Tavor et al., 2020; Kumar et al., 2020).",
    "document": "Related Work\n\nData augmentation seeks to expand the effective training distribution to improve generalization, especially in low-resource and distribution-shifted regimes. In natural language processing, augmentation must preserve semantic labels while varying surface forms.\n\nCommon text augmentation strategies include token-level edits such as synonym replacement, insertion, and deletion (Wei and Zou, 2019), paraphrasing via back-translation (Sennrich et al., 2016; Edunov et al., 2018), and mixup or manifold interpolation techniques on hidden representations (Guo et al., 2019; Chen et al., 2020). Recently, pretrained language models have been used to generate task-conditioned synthetic examples (Anaby-Tavor et al., 2020; Kumar et al., 2020).\n\nSemi-supervised methods further exploit unlabeled corpora through consistency training and pseudo-labeling (Xie et al., 2020; Clark et al., 2018). Nevertheless, existing techniques often struggle to maintain label fidelity under large perturbations.\n\nWe present SemAR, a semantics-aware augmentation framework that constrains generated variants using entailment-based validators and prototype-guided edits. Our approach yields consistent gains across sentiment, NLI, and intent classification under few-shot and cross-domain transfer.",
    "reason": "The span inventories augmentation techniques and cites works but does not explain how they relate to the paper's approach, what limitations motivate the new method, or any author viewpoint. It lacks synthesis as per (a) and (c).",
    "start": 277,
    "end": 752,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Khandelwal et al., 2020)",
    "document": "Introduction\n\nRecent progress in text generation has been driven by large encoder-decoder architectures pre-trained on massive corpora. Transformer variants such as BART Khandelwal et al., 2020) and T5 (Raffel et al., 2020) provide strong baselines for conditional generation tasks. Despite their success, out-of-domain generalization remains challenging (Park et al., 2021). We study domain-robust pretraining schemes that minimize reliance on spurious lexical cues while preserving factuality.\n",
    "reason": "Missing opening parenthesis in a parenthetical citation. It should be \"(Khandelwal et al., 2020)\" to balance the parentheses.",
    "start": 170,
    "end": 194,
    "label": "Format"
  },
  {
    "span": "[Green and Blue, 2017]",
    "document": "Introduction\n\nUnsupervised domain adaptation seeks to transfer knowledge from labeled source data to unlabeled target data by reducing distribution shift (Ben-David et al., 2010). Adversarial methods align feature distributions through domain-confusion objectives (Ganin and Lempitsky, 2015), while discrepancy-based approaches minimize statistical distances between domains (Long et al., 2015; Sun and Saenko, 2016). Previous studies [Green and Blue, 2017] demonstrate the benefit of aligning conditional distributions, whereas instance reweighting techniques account for label shift and selection bias (Chen et al., 2019). Despite progress, negative transfer and partial adaptation remain challenging (Cao et al., 2018).",
    "reason": "Wrong citation style: author–year citations should use parentheses in this context; square brackets are inconsistent with the surrounding style.",
    "start": 435,
    "end": 457,
    "label": "Format"
  },
  {
    "span": "[Garcia, 2021)",
    "document": "Introduction\n\nCausal inference for recommendation requires accounting for exposure bias and confounding (Zhang and Duan, 2020; Ortega and Mills, 2021). Bandit debiasing uses inverse propensity scoring to correct feedback loops (Koh and Patel, 2019), while front-door adjustments have been explored in session-based settings (Mora et al., 2022).\n\nRelated Work\n\nWe build on graph counterfactual models that simulate interventions on user-item edges [Garcia, 2021) and combine them with variational estimators (Cho and Rivera, 2020). Prior evaluations typically report uplift under randomized buckets (Ng and Yu, 2021), but few analyze long-term welfare.",
    "reason": "Mismatched brackets. The citation opens with a square bracket and closes with a parenthesis; it should consistently use parentheses '(Garcia, 2021)'.",
    "start": 447,
    "end": 461,
    "label": "Format"
  },
  {
    "span": "(2021, Johnson et al.)",
    "document": "Introduction\n\nTopic modeling remains a fundamental tool for discovering latent structure in large text corpora. Neural variational models improve flexibility over classical LDA by learning amortized inference networks, enabling richer priors and document-level supervision. Despite these advances, stability and interpretability trade-offs persist, especially under short documents and domain shift. We build on recent neural topic models to incorporate lexical constraints and anchor words that enhance semantic coherence (2021, Johnson et al.).\n\nOur contributions are: (1) a constraint-aware encoder that preserves sparsity, (2) a calibration objective to align topic proportions with lexical priors, and (3) an evaluation suite focused on coherence, diversity, and downstream classification.",
    "reason": "Incorrect ordering inside a parenthetical citation: year appears before authors. It should be '(Johnson et al., 2021)' or narrative 'Johnson et al. (2021)'.",
    "start": 523,
    "end": 545,
    "label": "Format"
  },
  {
    "span": "[Lopez et al., 2021)",
    "document": "Introduction\n\nDomain adaptation methods aim to transfer knowledge from a labeled source domain to an unlabeled or sparsely labeled target domain (Ben-David and Urner, 2014; Zhao and Zhang, 2019). A concurrent line of work [Lopez et al., 2021) explores meta-learning strategies that quickly adapt encoders to new domains with minimal fine-tuning. Complementary approaches minimize divergence between source and target representations through adversarial training (Chen and Liu, 2020) or moment matching (Gretton et al., 2012). We propose a hybrid objective that schedules alignment strength based on target-domain uncertainty (Xu and Park, 2022).",
    "reason": "Mismatched brackets in the citation; use parentheses consistently in author–year style: '(Lopez et al., 2021)'.",
    "start": 222,
    "end": 242,
    "label": "Format"
  },
  {
    "span": "Clinical NLI has seen a surge of datasets in the past year.",
    "document": "Introduction\nNatural language inference (NLI) in the clinical domain supports information extraction, cohort selection, and decision support by assessing entailment relationships among medical statements. Domain-specific challenges include negation, temporality, and abbreviation expansion.\n\nRelated Work\nEarly efforts adapted general-domain NLI models using clinical corpora, while more recent work has explored specialized pretraining objectives and ontology-aware architectures. Clinical NLI has seen a surge of datasets in the past year. Nevertheless, inconsistent annotation guidelines and label distributions complicate cross-dataset transfer. We examine label mapping strategies and calibration to improve robustness across clinical NLI sources.",
    "reason": "The phrase claims growth in 'datasets' and implies 'recent works' without any citations to datasets or studies (violates d and a).",
    "start": 482,
    "end": 541,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Kim et al., 2018,)",
    "document": "Introduction\n\nTask-oriented dialogue systems must track user goals and produce grounded responses. Neural approaches integrate belief tracking and policy learning in an end-to-end manner (Wen et al., 2017; Lei et al., 2018). Prior work (Kim et al., 2018,) demonstrates that incorporating schema constraints improves slot consistency across turns. We build upon this line by introducing a constrained decoding objective compatible with large language models.",
    "reason": "Extraneous comma before the closing parenthesis in the citation; should be “(Kim et al., 2018)”.",
    "start": 236,
    "end": 255,
    "label": "Format"
  },
  {
    "span": "Abadi et al. (2016) introduced differentially private SGD for deep learning. Papernot et al. (2018) proposed PATE for private semi-supervised learning. McMahan et al. (2017) developed federated averaging for on-device training. Bassily et al. (2014) studied sample complexity bounds under differential privacy.",
    "document": "Related Work\n\nDifferential privacy (DP) provides a principled notion of privacy for statistical learning and has been adapted to modern deep models (Dwork et al., 2014; Dwork and Roth, 2014). Research spans private optimization, accounting, and deployment in federated and centralized settings (Kairouz et al., 2021; Mironov, 2017; Gopi et al., 2021).\n\nAbadi et al. (2016) introduced differentially private SGD for deep learning. Papernot et al. (2018) proposed PATE for private semi-supervised learning. McMahan et al. (2017) developed federated averaging for on-device training. Bassily et al. (2014) studied sample complexity bounds under differential privacy.\n\nWhile these contributions address complementary aspects of privacy-preserving learning, significant gaps remain in practical utility, particularly for high-dimensional vision tasks (Tramèr and Boneh, 2021). Our method complements DP-SGD with task-adaptive clipping and per-layer privacy allocation to improve accuracy under fixed privacy budgets.",
    "reason": "The sequence abruptly shifts between methods and settings without clarifying relationships (e.g., federated averaging is presented alongside DP methods without explaining the connection). No transitions or explicit ties are provided, reducing coherence.",
    "start": 353,
    "end": 663,
    "label": "Coherence"
  },
  {
    "span": "In (Kumar et al., 2018)",
    "document": "Related Work\n\nMultimodal representation learning aims to align information from text, audio, and vision. In (Kumar et al., 2018) the authors introduce a co-attention mechanism for aligning image regions with textual phrases, improving retrieval performance. Later, cross-modal Transformers (Tsai et al., 2019; Chen et al., 2020) unified attention across modalities, and contrastive pretraining (Radford et al., 2021) further boosted zero-shot transfer. However, most methods assume perfectly synchronized modalities, an assumption often violated in real-world settings.\n",
    "reason": "Wrong citation style; narrative context should use 'in Kumar et al. (2018)' rather than a parenthetical 'in (Kumar et al., 2018)'.",
    "start": 105,
    "end": 128,
    "label": "Format"
  },
  {
    "span": "Early dialogue state tracking relied on slot-filling pipelines with delexicalization and handcrafted features. Neural approaches introduced recurrent encoders for belief state estimation, followed by attention mechanisms over dialogue context. Pointer networks and span-based models treat slot values as spans in the utterance history. Pretrained transformers have been adapted to multi-domain datasets with schema-based generalization and contrastive objectives. Our method fine-tunes a unified encoder with a simple contrastive belief loss.",
    "document": "Introduction\n\nTask-oriented dialogue systems maintain a representation of user goals and constraints to guide policy and response generation. Robust dialogue state tracking (DST) is particularly challenging in multi-domain, noisy settings with open-vocabulary slot values.\n\nEarly dialogue state tracking relied on slot-filling pipelines with delexicalization and handcrafted features. Neural approaches introduced recurrent encoders for belief state estimation, followed by attention mechanisms over dialogue context. Pointer networks and span-based models treat slot values as spans in the utterance history. Pretrained transformers have been adapted to multi-domain datasets with schema-based generalization and contrastive objectives. Our method fine-tunes a unified encoder with a simple contrastive belief loss.\n\nWe evaluate on standard DST benchmarks and analyze calibration and out-of-domain robustness.",
    "reason": "The span recounts DST methods and then immediately mentions the authors' approach without stating what is missing from existing techniques or articulating a perspective that motivates the new loss.",
    "start": 274,
    "end": 816,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Industry reports indicate that over 80% of deployed ML systems undergo formal bias audits.",
    "document": "Introduction\n\nAuditing machine learning systems for fairness has become an essential component of responsible AI practice. Methodological advances span pre-processing, in-processing, and post-processing mitigations, as well as causal and counterfactual analyses of disparate impact. Industry reports indicate that over 80% of deployed ML systems undergo formal bias audits. Yet, practices remain heterogeneous across sectors, and there is limited understanding of how audit outcomes translate into model changes and downstream social impact. In this work, we survey audit pipelines in high-stakes domains and introduce a standardized audit card with measurable criteria for transparency and accountability.",
    "reason": "Reports a specific statistic purportedly from industry sources without citing any report or dataset.",
    "start": 283,
    "end": 373,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Smith and Jones, 2015)",
    "document": "Related Work\n\nProgram synthesis from natural language has benefited from neural architectures that align textual intents with executable representations (Yin and Neubig, 2017; Cheng et al., 2019). Sequence-to-sequence models augmented with copy mechanisms and constrained decoding help ensure syntactic validity (Dong and Lapata, 2016; Krishnamurthy et al., 2017). Hybrid methods incorporate symbolic inductive biases through sketches or type systems to reduce search complexity (Polosukhin and Skidanov, 2018; Nye et al., 2019). Evaluation protocols often consider exact match and execution accuracy, with recent work emphasizing robustness to spurious programs (Shi et al., 2020). In low-resource settings, data augmentation and meta-learning have been explored to improve generalization (Xie et al., 2018; Yu et al., 2018). As observed by several studies (Smith and Jones, 2015), compositional generalization remains a central challenge motivating modular decoding strategies. Our approach introduces a verifier-guided planner that composes subprograms under semantic constraints derived from the specification.\n",
    "reason": "Wrong style for authors within a parenthetical citation; APA-style parenthetical citations should use '&' instead of 'and', i.e., '(Smith & Jones, 2015)'.",
    "start": 858,
    "end": 881,
    "label": "Format"
  },
  {
    "span": "To address privacy in federated learning, differential privacy mechanisms clip and perturb updates (Abadi et al., 2016; McMahan et al., 2018). Secure aggregation protects updates from the server (Bonawitz et al., 2017). Personalization methods adapt global models to clients (Smith et al., 2017; Arivazhagan et al., 2019). Hybrid approaches combine these techniques (Truex et al., 2019).",
    "document": "Introduction\n\nFederated learning enables on-device training across a population of clients while keeping raw data local. Practical deployments must balance privacy guarantees, communication efficiency, and heterogeneous client capabilities. The literature explores a range of algorithmic and systems-level solutions to these challenges.\n\nTo address privacy in federated learning, differential privacy mechanisms clip and perturb updates (Abadi et al., 2016; McMahan et al., 2018). Secure aggregation protects updates from the server (Bonawitz et al., 2017). Personalization methods adapt global models to clients (Smith et al., 2017; Arivazhagan et al., 2019). Hybrid approaches combine these techniques (Truex et al., 2019).\n\nRecent work also targets stragglers, partial participation, and skewed data distributions with adaptive optimization and client selection policies. In the following sections we outline our protocol and experimental results on real-world benchmarks.",
    "reason": "The span enumerates prior privacy and personalization techniques but does not connect them to the authors' problem setting, identify limitations, or state how the new method interacts with or differs from them.",
    "start": 338,
    "end": 725,
    "label": "Lacks_synthesis"
  },
  {
    "span": "FedAvg aggregates local updates by weighted averaging (McMahan et al., 2017). Differential privacy adds calibrated noise to gradients (Abadi et al., 2016). Meta-learning adapts quickly to new clients (Finn et al., 2017).",
    "document": "Related Work\n\nFederated Optimization. Communication-efficient algorithms aggregate client updates on a central server to train global models without centralized data (Kairouz et al., 2021). Personalization and robustness are key challenges due to system and statistical heterogeneity.\n\nPrivacy and Personalization. Secure aggregation and cryptographic protocols protect update confidentiality, while privacy mechanisms provide formal guarantees against inference attacks (Bonawitz et al., 2017; Abadi et al., 2016). Personalization aims to adapt the global model to client-specific distributions via multi-task learning, clustering, or local fine-tuning (Smith et al., 2017; Arivazhagan et al., 2019). FedAvg aggregates local updates by weighted averaging (McMahan et al., 2017). Differential privacy adds calibrated noise to gradients (Abadi et al., 2016). Meta-learning adapts quickly to new clients (Finn et al., 2017). We build on these directions to address non-IID drift.\n\nModel Heterogeneity. Heterogeneous architectures across clients motivate knowledge transfer via distillation and feature sharing (Li and Wang, 2019; Horvath et al., 2021). Our approach decouples representation learning from personalization through a bilevel objective.",
    "reason": "The span lists FedAvg, differential privacy, and meta-learning in succession without articulating how these concepts are connected to one another or to the stated goal; transitions and explicit relations are missing.",
    "start": 702,
    "end": 922,
    "label": "Coherence"
  },
  {
    "span": "Earlier shared tasks standardized evaluation with chrF++ and COMET.",
    "document": "Related Work\n\nMachine translation (MT) evaluation has progressed from simple lexical overlap to semantic and quality estimation metrics. While BLEU remains widely reported, sentence-level robustness and correlation with human judgments have motivated alternatives that emphasize character n-grams, syntax, and learned semantic similarity. Earlier shared tasks standardized evaluation with chrF++ and COMET. However, cross-domain reliability and multilingual calibration still vary widely, complicating meta-analysis. We investigate composite metrics that combine reference-based and reference-free signals, improving stability under domain shift and low-resource settings.",
    "reason": "References 'shared tasks' and specific metrics as standardized without any citations to the tasks or publications (rule a and d).",
    "start": 339,
    "end": 406,
    "label": "Unsupported_claim"
  },
  {
    "span": "(2017 Smith et al.)",
    "document": "Related Work\n\nPolicy gradient methods have been studied extensively for continuous control (Schulman et al., 2015; Lillicrap et al., 2016). Early results (2017 Smith et al.) suggested that baseline variance reduction is crucial, and later works formalized trust-region updates to stabilize training (Schulman et al., 2017). More recent algorithms couple entropy regularization with distributional critics to improve exploration (Haarnoja et al., 2018).\n",
    "reason": "Author-year order is reversed in the parenthetical citation; should be “(Smith et al., 2017)”.",
    "start": 154,
    "end": 173,
    "label": "Format"
  },
  {
    "span": "(Brown et al., 2018.",
    "document": "Related Work\n\nModern object detectors balance accuracy and efficiency through architectural design and training strategies. Two-stage detectors like Faster R-CNN emphasize region proposal quality and precise localization (Ren et al., 2015), while single-stage approaches such as YOLO and RetinaNet aim for speed with dense predictions and appropriate loss functions (Redmon and Farhadi, 2018; Lin et al., 2017). Multi-scale feature pyramids and focal losses have been particularly effective for small-object detection and class imbalance. Data augmentation and longer training schedules can further improve robustness, as shown by (Brown et al., 2018. More recent work integrates self-supervised pretraining to boost performance with limited annotations (He et al., 2020).",
    "reason": "Missing closing parenthesis in the parenthetical citation; it should be “(Brown et al., 2018).”",
    "start": 631,
    "end": 651,
    "label": "Format"
  },
  {
    "span": "Human performance on SQuAD 2.0 is around 89 F1.",
    "document": "Introduction\n\nReading comprehension benchmarks have catalyzed progress in machine understanding of text. Unanswerable-question datasets introduced a stronger test of robustness by requiring models not only to locate relevant spans but also to abstain when evidence is absent. Despite advances, models still struggle to calibrate confidence under distribution shift.\n\nHuman performance on SQuAD 2.0 is around 89 F1. In this work, we examine whether lightweight calibration heads trained on small amounts of development data can close the abstention gap without sacrificing span selection accuracy. We also study how negative sampling and thresholding strategies interact with pretraining objectives.\n\nOur contributions include a systematic evaluation of calibration methods, an analysis of error modes for abstention decisions, and recommendations for reporting confidence metrics alongside exact match and F1.",
    "reason": "The sentence states a specific performance statistic about a benchmark without providing a citation to the source of that number.",
    "start": 367,
    "end": 414,
    "label": "Unsupported_claim"
  },
  {
    "span": "Zero-shot cross-lingual learning becomes an emerging research topic as it eliminates the requirement of labeled data for training models in low-resource languages.",
    "document": "Related Work\n\nZero-shot cross-lingual structured prediction. Zero-shot cross-lingual learning becomes an emerging research topic as it eliminates the requirement of labeled data for training models in low-resource languages. Various structured prediction tasks have be studied, including named entity recognition (Pan et al., 2017;Hu et al., 2020), dependency parsing (Ahmad et al., 2019, relation extraction (Zou et al., 2018;Ni and Florian, 2019), and event argument extraction (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). Most of them are classification-based models that build classifiers on top of a multilingual pre-trained masked language models. To further deal with the discrepancy between languages, some of them require additional information, such as bilingual dictionaries (Liu et al., 2019; Ni and Florian, 2019), translation pairs (Zou et al., 2018), and dependency parse trees (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). However, as pointed out by previous literature (Li et al., 2021;Hsu et al., 2021), classification-based models are less powerful to model dependencies between entities compared to generation-based models.\n\nGeneration-based structured prediction. Several works have demonstrated the great success of generation-based models on monolingual structured prediction tasks, including named entity recognition (Yan et al., 2021), relation extraction (Huang et al., 2021;Paolini et al., 2021), and event extraction (Du et al., 2021;Hsu et al., 2021;Lu et al., 2021). Yet, as mentioned in Section 1, their designed generating targets are language-dependent. Accordingly, directly applying their methods to the zero-shot cross-lingual setting would result in less-preferred performance.\n\nPrompting methods. There are growing interests recently to incorporate prompts on pre-trained language models in order to guide the models' behavior or elicit knowledge (Shin et al., 2020;Schick and Schütze, 2021;Qin and Eisner, 2021;Scao and Rush, 2021). Following the taxonomy in (Liu et al., 2021), these methods can be classified depending on whether the language models' parameters are tuned and on whether trainable prompts are introduced. Our method belongs to the category that fixes the prompts and tune the language models' parameters. Despite the flourish of the research in prompting methods, there is only limited attention being put on multilingual tasks (Winata et al., 2021).\n\n ",
    "start": 62,
    "end": 224,
    "label": "Unsupported_claim"
  },
  {
    "span": "Accordingly, directly applying their methods to the zero-shot cross-lingual setting would result in less-preferred performance.",
    "document": "Related Work\n\nZero-shot cross-lingual structured prediction. Zero-shot cross-lingual learning becomes an emerging research topic as it eliminates the requirement of labeled data for training models in low-resource languages. Various structured prediction tasks have be studied, including named entity recognition (Pan et al., 2017;Hu et al., 2020), dependency parsing (Ahmad et al., 2019, relation extraction (Zou et al., 2018;Ni and Florian, 2019), and event argument extraction (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). Most of them are classification-based models that build classifiers on top of a multilingual pre-trained masked language models. To further deal with the discrepancy between languages, some of them require additional information, such as bilingual dictionaries (Liu et al., 2019; Ni and Florian, 2019), translation pairs (Zou et al., 2018), and dependency parse trees (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). However, as pointed out by previous literature (Li et al., 2021;Hsu et al., 2021), classification-based models are less powerful to model dependencies between entities compared to generation-based models.\n\nGeneration-based structured prediction. Several works have demonstrated the great success of generation-based models on monolingual structured prediction tasks, including named entity recognition (Yan et al., 2021), relation extraction (Huang et al., 2021;Paolini et al., 2021), and event extraction (Du et al., 2021;Hsu et al., 2021;Lu et al., 2021). Yet, as mentioned in Section 1, their designed generating targets are language-dependent. Accordingly, directly applying their methods to the zero-shot cross-lingual setting would result in less-preferred performance.\n\nPrompting methods. There are growing interests recently to incorporate prompts on pre-trained language models in order to guide the models' behavior or elicit knowledge (Shin et al., 2020;Schick and Schütze, 2021;Qin and Eisner, 2021;Scao and Rush, 2021). Following the taxonomy in (Liu et al., 2021), these methods can be classified depending on whether the language models' parameters are tuned and on whether trainable prompts are introduced. Our method belongs to the category that fixes the prompts and tune the language models' parameters. Despite the flourish of the research in prompting methods, there is only limited attention being put on multilingual tasks (Winata et al., 2021).\n\n ",
    "start": 1643,
    "end": 1769,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Ahmad et al., 2019,",
    "document": "Related Work\n\nZero-shot cross-lingual structured prediction. Zero-shot cross-lingual learning becomes an emerging research topic as it eliminates the requirement of labeled data for training models in low-resource languages. Various structured prediction tasks have be studied, including named entity recognition (Pan et al., 2017;Hu et al., 2020), dependency parsing (Ahmad et al., 2019, relation extraction (Zou et al., 2018;Ni and Florian, 2019), and event argument extraction (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). Most of them are classification-based models that build classifiers on top of a multilingual pre-trained masked language models. To further deal with the discrepancy between languages, some of them require additional information, such as bilingual dictionaries (Liu et al., 2019; Ni and Florian, 2019), translation pairs (Zou et al., 2018), and dependency parse trees (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). However, as pointed out by previous literature (Li et al., 2021;Hsu et al., 2021), classification-based models are less powerful to model dependencies between entities compared to generation-based models.\n\nGeneration-based structured prediction. Several works have demonstrated the great success of generation-based models on monolingual structured prediction tasks, including named entity recognition (Yan et al., 2021), relation extraction (Huang et al., 2021;Paolini et al., 2021), and event extraction (Du et al., 2021;Hsu et al., 2021;Lu et al., 2021). Yet, as mentioned in Section 1, their designed generating targets are language-dependent. Accordingly, directly applying their methods to the zero-shot cross-lingual setting would result in less-preferred performance.\n\nPrompting methods. There are growing interests recently to incorporate prompts on pre-trained language models in order to guide the models' behavior or elicit knowledge (Shin et al., 2020;Schick and Schütze, 2021;Qin and Eisner, 2021;Scao and Rush, 2021). Following the taxonomy in (Liu et al., 2021), these methods can be classified depending on whether the language models' parameters are tuned and on whether trainable prompts are introduced. Our method belongs to the category that fixes the prompts and tune the language models' parameters. Despite the flourish of the research in prompting methods, there is only limited attention being put on multilingual tasks (Winata et al., 2021).\n\n ",
    "start": 369,
    "end": 388,
    "label": "Format"
  },
  {
    "span": "in (Liu et al., 2021)",
    "document": "Related Work\n\nZero-shot cross-lingual structured prediction. Zero-shot cross-lingual learning becomes an emerging research topic as it eliminates the requirement of labeled data for training models in low-resource languages. Various structured prediction tasks have be studied, including named entity recognition (Pan et al., 2017;Hu et al., 2020), dependency parsing (Ahmad et al., 2019, relation extraction (Zou et al., 2018;Ni and Florian, 2019), and event argument extraction (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). Most of them are classification-based models that build classifiers on top of a multilingual pre-trained masked language models. To further deal with the discrepancy between languages, some of them require additional information, such as bilingual dictionaries (Liu et al., 2019; Ni and Florian, 2019), translation pairs (Zou et al., 2018), and dependency parse trees (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). However, as pointed out by previous literature (Li et al., 2021;Hsu et al., 2021), classification-based models are less powerful to model dependencies between entities compared to generation-based models.\n\nGeneration-based structured prediction. Several works have demonstrated the great success of generation-based models on monolingual structured prediction tasks, including named entity recognition (Yan et al., 2021), relation extraction (Huang et al., 2021;Paolini et al., 2021), and event extraction (Du et al., 2021;Hsu et al., 2021;Lu et al., 2021). Yet, as mentioned in Section 1, their designed generating targets are language-dependent. Accordingly, directly applying their methods to the zero-shot cross-lingual setting would result in less-preferred performance.\n\nPrompting methods. There are growing interests recently to incorporate prompts on pre-trained language models in order to guide the models' behavior or elicit knowledge (Shin et al., 2020;Schick and Schütze, 2021;Qin and Eisner, 2021;Scao and Rush, 2021). Following the taxonomy in (Liu et al., 2021), these methods can be classified depending on whether the language models' parameters are tuned and on whether trainable prompts are introduced. Our method belongs to the category that fixes the prompts and tune the language models' parameters. Despite the flourish of the research in prompting methods, there is only limited attention being put on multilingual tasks (Winata et al., 2021).\n\n ",
    "start": 2051,
    "end": 2071,
    "label": "Format"
  },
  {
    "span": "Following the taxonomy in (Liu et al., 2021), these methods can be classified depending on whether the language models' parameters are tuned and on whether trainable prompts are introduced. Our method belongs to the category that fixes the prompts and tune the language models' parameters. Despite the flourish of the research in prompting methods, there is only limited attention being put on multilingual tasks (Winata et al., 2021).",
    "document": "Related Work\n\nZero-shot cross-lingual structured prediction. Zero-shot cross-lingual learning becomes an emerging research topic as it eliminates the requirement of labeled data for training models in low-resource languages. Various structured prediction tasks have be studied, including named entity recognition (Pan et al., 2017;Hu et al., 2020), dependency parsing (Ahmad et al., 2019, relation extraction (Zou et al., 2018;Ni and Florian, 2019), and event argument extraction (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). Most of them are classification-based models that build classifiers on top of a multilingual pre-trained masked language models. To further deal with the discrepancy between languages, some of them require additional information, such as bilingual dictionaries (Liu et al., 2019; Ni and Florian, 2019), translation pairs (Zou et al., 2018), and dependency parse trees (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). However, as pointed out by previous literature (Li et al., 2021;Hsu et al., 2021), classification-based models are less powerful to model dependencies between entities compared to generation-based models.\n\nGeneration-based structured prediction. Several works have demonstrated the great success of generation-based models on monolingual structured prediction tasks, including named entity recognition (Yan et al., 2021), relation extraction (Huang et al., 2021;Paolini et al., 2021), and event extraction (Du et al., 2021;Hsu et al., 2021;Lu et al., 2021). Yet, as mentioned in Section 1, their designed generating targets are language-dependent. Accordingly, directly applying their methods to the zero-shot cross-lingual setting would result in less-preferred performance.\n\nPrompting methods. There are growing interests recently to incorporate prompts on pre-trained language models in order to guide the models' behavior or elicit knowledge (Shin et al., 2020;Schick and Schütze, 2021;Qin and Eisner, 2021;Scao and Rush, 2021). Following the taxonomy in (Liu et al., 2021), these methods can be classified depending on whether the language models' parameters are tuned and on whether trainable prompts are introduced. Our method belongs to the category that fixes the prompts and tune the language models' parameters. Despite the flourish of the research in prompting methods, there is only limited attention being put on multilingual tasks (Winata et al., 2021).\n\n ",
    "start": 2028,
    "end": 2462,
    "label": "Coherence"
  },
  {
    "span": "There are growing interests recently to incorporate prompts on pre-trained language models in order to guide the models' behavior or elicit knowledge (Shin et al., 2020;Schick and Schütze, 2021;Qin and Eisner, 2021;Scao and Rush, 2021).",
    "document": "Related Work\n\nZero-shot cross-lingual structured prediction. Zero-shot cross-lingual learning becomes an emerging research topic as it eliminates the requirement of labeled data for training models in low-resource languages. Various structured prediction tasks have be studied, including named entity recognition (Pan et al., 2017;Hu et al., 2020), dependency parsing (Ahmad et al., 2019, relation extraction (Zou et al., 2018;Ni and Florian, 2019), and event argument extraction (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). Most of them are classification-based models that build classifiers on top of a multilingual pre-trained masked language models. To further deal with the discrepancy between languages, some of them require additional information, such as bilingual dictionaries (Liu et al., 2019; Ni and Florian, 2019), translation pairs (Zou et al., 2018), and dependency parse trees (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). However, as pointed out by previous literature (Li et al., 2021;Hsu et al., 2021), classification-based models are less powerful to model dependencies between entities compared to generation-based models.\n\nGeneration-based structured prediction. Several works have demonstrated the great success of generation-based models on monolingual structured prediction tasks, including named entity recognition (Yan et al., 2021), relation extraction (Huang et al., 2021;Paolini et al., 2021), and event extraction (Du et al., 2021;Hsu et al., 2021;Lu et al., 2021). Yet, as mentioned in Section 1, their designed generating targets are language-dependent. Accordingly, directly applying their methods to the zero-shot cross-lingual setting would result in less-preferred performance.\n\nPrompting methods. There are growing interests recently to incorporate prompts on pre-trained language models in order to guide the models' behavior or elicit knowledge (Shin et al., 2020;Schick and Schütze, 2021;Qin and Eisner, 2021;Scao and Rush, 2021). Following the taxonomy in (Liu et al., 2021), these methods can be classified depending on whether the language models' parameters are tuned and on whether trainable prompts are introduced. Our method belongs to the category that fixes the prompts and tune the language models' parameters. Despite the flourish of the research in prompting methods, there is only limited attention being put on multilingual tasks (Winata et al., 2021).\n\n ",
    "start": 1791,
    "end": 2026,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Smith et al.",
    "document": "Introduction\n\nNeural graph summarization aims to distill large-scale graphs into compact representations for downstream tasks such as classification and question answering. Early approaches compress structure with heuristics (Navlakha et al., 2008) or matrix factorization (Tong et al., 2008), while recent methods leverage neural encoders to capture higher-order dependencies (Kipf and Welling, 2017;Hamilton et al., 2017).\n\nFollowing Smith et al., we adopt a hierarchical encoder that pools neighborhoods into super-nodes before generating summaries for evaluation. Unlike extractive techniques that depend on hand-crafted scoring (Mihalcea and Tarau, 2004), our model learns content selection jointly with structure preservation.\n\nOur contribution complements prior work on scalable training (Ying et al., 2018) and subgraph pooling (Lee et al., 2019) by targeting summarization quality under budget constraints.",
    "reason": "Narrative citation is missing the publication year; should be formatted as 'Smith et al. (YEAR)' or converted to a parenthetical form '(Smith et al., YEAR)'.",
    "start": 436,
    "end": 448,
    "label": "Format"
  },
  {
    "span": "(Davis and Moore, 2020)",
    "document": "Related Work\n\nCommunity detection in social networks has leveraged modularity maximization, stochastic block models, and embedding-based clustering (Newman, 2006; Fortunato and Hric, 2016). Overlapping community structures are often modeled with mixed-membership assumptions and nonparametric priors (Airoldi et al., 2008; Gopalan and Blei, 2013). Role discovery complements community assignment by capturing structural equivalence (Henderson et al., 2012; Rossi and Ahmed, 2015). Recent dynamic models incorporate temporal smoothness and event excitation (Ho et al., 2011; Matias and Miele, 2017). Attribute-aware methods integrate node metadata to improve interpretability (Yang et al., 2013; Davis and Moore, 2020). Our method unifies dynamic role discovery with attribute guidance through a variational formulation.",
    "reason": "In APA-style parenthetical citations with two authors, '&' should be used instead of 'and'; should be (Davis & Moore, 2020).",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "Deng et al. 1",
    "document": "Introduction\n\nLarge-scale visual recognition has been propelled by curated datasets and architectures that exploit hierarchical features (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; He et al., 2016). The ImageNet dataset Deng et al. 1 has been central to benchmarking progress and pretraining backbones for transfer learning across tasks (Russakovsky et al., 2015). Despite this success, long-tailed class distributions remain a major challenge (Zhang et al., 2021). We propose a reweighting scheme that adapts margins by class rarity, improving minority accuracy without degrading head classes.\n\nRelated Work\n\nRebalancing strategies include loss reweighting, resampling, and margin adjustments (Cui et al., 2019; Menon et al., 2021). Our approach is orthogonal and compatible with recent decoupled training pipelines (Kang et al., 2020).",
    "reason": "Wrong use of footnote-like number; 'Deng et al. 1' should include a publication year or be formatted as a proper citation (e.g., 'Deng et al. (2009)' or '(Deng et al., 2009)').",
    "start": 232,
    "end": 245,
    "label": "Format"
  },
  {
    "span": "For biomedical NER, Huang et al. (2020) propose prototypical networks for few-shot tagging. Lee and Chen (2019) adapt meta-learning to slot filling with episodic training. Peters et al. (2018) introduce contextualized embeddings that improve sequence tagging. Liu et al. (2020) curate distant supervision for entity labels.",
    "document": "Related Work\n\nLow-resource biomedical named entity recognition (NER) has attracted considerable interest due to the scarcity of annotated data and the domain shift from general language. Early approaches relied on feature engineering and CRFs with domain lexicons, but recent neural methods leverage pretraining and meta-learning to generalize from limited supervision.\n\nFor biomedical NER, Huang et al. (2020) propose prototypical networks for few-shot tagging. Lee and Chen (2019) adapt meta-learning to slot filling with episodic training. Peters et al. (2018) introduce contextualized embeddings that improve sequence tagging. Liu et al. (2020) curate distant supervision for entity labels.\n\nBeyond few-shot methods, domain-adaptive pretraining of language models has been effective for biomedical tasks. Models such as BioBERT and SciBERT adapt general-purpose transformers to scientific and clinical corpora, improving token-level predictions. Our work builds on domain-adapted encoders but focuses on structured support sets and calibration under extreme label sparsity.\n",
    "reason": "The span lists four works in consecutive sentences without transitions or an explicit explanation of how they relate to one another (few-shot NER, slot filling, contextual embeddings, and distant supervision). The connection between cited works is abrupt and the relationships are only implied.",
    "start": 371,
    "end": 694,
    "label": "Coherence"
  },
  {
    "span": "In (Lopez et al., 2018)",
    "document": "Related Work\n\nNeural approaches to structured prediction have evolved rapidly over the past decade. Early sequence labeling systems relied on handcrafted features and CRF-based decoding (Miller and Gupta, 2016), while recent models leverage transformer backbones (Rao et al., 2020) and task-specific pretraining (Chen et al., 2021). In (Lopez et al., 2018) we see one of the first attempts to couple uncertainty estimation with active learning for slot filling, but their approach assumes access to dense annotations. Subsequent research has explored pool-based strategies with margin sampling (Kwok et al., 2019) and diversity-aware acquisition (Singh et al., 2022). Our work differs by emphasizing budget-aware selection under domain shift, similar in spirit to Johnson et al. (2019) but adapted to multilingual settings.",
    "reason": "Wrong citation style: narrative use should be formatted as 'In Lopez et al. (2018)' rather than 'In (Lopez et al., 2018)'.",
    "start": 333,
    "end": 356,
    "label": "Format"
  },
  {
    "span": "Dinan et al. (2019) collect adversarial dialogue to reveal unsafe behaviors. Xu et al. (2021) apply detoxification via prefix-tuning. Bai et al. (2022) align models with human feedback to reduce harmful outputs. Roller et al. (2021) scale open-domain chatbots with safety classifiers.",
    "document": "Related Work\n\nSafety and Toxicity Mitigation in Conversational Agents\n\nOpen-domain systems can generate unsafe content due to data biases and poorly aligned objectives. Mitigation spans data curation, decoding constraints, and alignment techniques. Dinan et al. (2019) collect adversarial dialogue to reveal unsafe behaviors. Xu et al. (2021) apply detoxification via prefix-tuning. Bai et al. (2022) align models with human feedback to reduce harmful outputs. Roller et al. (2021) scale open-domain chatbots with safety classifiers. The trade-offs between safety, utility, and fluency remain an active area of study.\n\nWe propose retrieval-augmented refusal with context-sensitive policy shaping that preserves informativeness while reducing harmful generations.",
    "reason": "Several works are mentioned back-to-back without transitions or explicit explanation of how adversarial data collection, detoxification, RLHF, and safety classifiers interrelate, resulting in abrupt topic shifts.",
    "start": 249,
    "end": 533,
    "label": "Coherence"
  },
  {
    "span": "(Li, 2019, Zhao, 2020)",
    "document": "Related Work\n\nExploration in reinforcement learning (RL) has been addressed through intrinsic motivation, optimism, and posterior sampling. Count-based bonuses extend to high-dimensional observations via density models (Bellemare et al., 2016). Optimistic value estimates drive exploration in linear and nonlinear settings (Jin et al., 2020). Posterior sampling approximates Thompson sampling for RL with scalable inference (Osband et al., 2016). Curiosity-driven objectives use prediction error as an intrinsic reward (Pathak et al., 2017). Our approach builds on uncertainty-aware policies that separate epistemic from aleatoric noise, consistent with (Li, 2019, Zhao, 2020), while incorporating risk constraints to avoid unsafe behaviors (Thomas et al., 2015).",
    "reason": "Multiple citations inside a single set of parentheses are separated by a comma rather than a semicolon; should be '(Li, 2019; Zhao, 2020)'.",
    "start": 654,
    "end": 676,
    "label": "Format"
  },
  {
    "span": "Kipf and Welling (2017) proposed graph convolutional networks for semi-supervised classification. Hamilton et al. (2017) introduced GraphSAGE with neighborhood sampling. Xu et al. (2019) analyzed the expressive power of GNNs. Ying et al. (2018) studied hierarchical pooling.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have become a central paradigm for learning on relational data, enabling message passing over nodes and edges with task-specific objectives. Despite strong performance, open questions remain about scalability, expressivity, and robustness to heterophily and distribution shift.\n\nArchitectures and representation learning\n\nKipf and Welling (2017) proposed graph convolutional networks for semi-supervised classification. Hamilton et al. (2017) introduced GraphSAGE with neighborhood sampling. Xu et al. (2019) analyzed the expressive power of GNNs. Ying et al. (2018) studied hierarchical pooling. Velickovic et al. (2018) developed attention mechanisms on graphs.\n\nScalability and sampling\n\nLarge-scale training has spurred interest in neighborhood sampling, subgraph batching, and memory-efficient training. Systems such as Cluster-GCN and GraphSAINT report notable throughput gains but may alter the distribution of training signals across graph regions.\n\nRobustness and heterophily\n\nRecent work examines robustness to adversarial perturbations and performance under heterophily, proposing modified aggregations that downweight incompatible neighbors. These advances motivate our approach, which decouples propagation depth from feature transformation and uses learned relation-specific mixing to adapt to local homophily levels.",
    "reason": "The sentences enumerate separate works without indicating how they relate to each other (e.g., progression, contrast, or shared limitations), creating abrupt connections. This is a multi-sentence coherence problem per (a) and (b).",
    "start": 365,
    "end": 639,
    "label": "Coherence"
  },
  {
    "span": "The SemEval 2018 Task on irony detection introduced the standard Twitter corpus used in most benchmarks.",
    "document": "Introduction\n\nDetecting irony and sarcasm in social media has gained attention due to their impact on sentiment analysis and misinformation tracking. The SemEval 2018 Task on irony detection introduced the standard Twitter corpus used in most benchmarks. Subsequent studies have proposed context-aware models leveraging user history and conversation threads. However, inconsistent annotation guidelines across datasets complicate model comparison.",
    "reason": "First mention of a shared task and its dataset lacks a citation; per rule (a), the originating task/dataset must be cited.",
    "start": 150,
    "end": 254,
    "label": "Unsupported_claim"
  },
  {
    "span": "Johnson & Patel (2017)",
    "document": "Related Work\n\nCollaborative filtering methods decompose user–item interactions into latent factors (Koren et al., 2009). Johnson & Patel (2017) introduce side-information fusion through context-aware regularization, showing gains on implicit feedback data. Neural recommenders leverage deep encoders for content and sequence signals (He et al., 2017; Kang and McAuley, 2018). Our work extends these by incorporating counterfactual risk minimization to debias exposure effects (Schnabel et al., 2016; Bonner and Vasile, 2018). We benchmark on MovieLens and Amazon, following standard splits and metrics (Wang et al., 2019).",
    "reason": "Incorrect conjunction in narrative citation; APA narrative form uses 'and' not '&' outside parentheses, so it should be 'Johnson and Patel (2017)'.",
    "start": 121,
    "end": 143,
    "label": "Format"
  },
  {
    "span": "Short-term load forecasting has seen classical ARIMA and exponential smoothing (Hyndman and Fan, 2010), kernel and tree ensembles (Taieb and Hyndman, 2014; Lago et al., 2018), and deep sequence models including LSTM, TCN, and Transformers (Hochreiter and Schmidhuber, 1997; Bai et al., 2018; Lim et al., 2021). Weather covariates, calendar effects, and holiday features are standard (Hong et al., 2016; Dudek, 2016).",
    "document": "Related Work\n\nElectric load forecasting underpins grid planning, demand response, and renewable integration. Accuracy and reliability are challenged by non-stationarity, changing tariffs, and evolving consumption behavior at multiple aggregation levels.\n\nShort-term load forecasting has seen classical ARIMA and exponential smoothing (Hyndman and Fan, 2010), kernel and tree ensembles (Taieb and Hyndman, 2014; Lago et al., 2018), and deep sequence models including LSTM, TCN, and Transformers (Hochreiter and Schmidhuber, 1997; Bai et al., 2018; Lim et al., 2021). Weather covariates, calendar effects, and holiday features are standard (Hong et al., 2016; Dudek, 2016).\n\nFew studies examine distribution shifts driven by electrification and rooftop PV at feeder scale. We introduce shift-robust multi-horizon forecasting with adaptive covariate selection and conformal prediction for calibrated uncertainty.\n",
    "reason": "The span summarizes families of methods and common features without relating them to the specific challenges targeted by the paper or stating any gap, aligning with (a) and (c).",
    "start": 255,
    "end": 671,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Zhao et al., 2016; Li and Wang, 2017;; Chen, 2019)",
    "document": "Related Work\n\nEntity linking systems typically decompose the task into mention detection, candidate generation, and disambiguation (Cucerzan, 2007; Ratinov et al., 2011; Shen et al., 2015). Neural approaches integrate these stages with context-aware encoders and global coherence objectives (Ganea and Hofmann, 2017; Le and Titov, 2018; Yamada et al., 2020).\n\nEarly neural methods emphasize local context windows, while recent models leverage pre-trained language models for richer semantics and joint reasoning (Logeswaran et al., 2019; Wu et al., 2020). Knowledge-enhanced encoders incorporate entity descriptions and graph structure to improve robustness (Zhang et al., 2019; Xiong et al., 2020).\n\nPrior work on candidate pruning examines lexical overlap and semantic similarity to reduce search space (Zhao et al., 2016; Li and Wang, 2017;; Chen, 2019), but these strategies can discard rare yet correct candidates under distribution shift. Our approach couples uncertainty-aware pruning with coherence constraints to retain plausible long-tail entities.",
    "reason": "Extra punctuation in a citation list: there is a double semicolon before 'Chen, 2019'; should be a single semicolon separating items.",
    "start": 805,
    "end": 856,
    "label": "Format"
  },
  {
    "span": "Nguyen et al. 1",
    "document": "Related Work\n\nReinforcement learning from human feedback (RLHF) aligns agent behavior with human preferences by learning a reward model from comparisons. Early systems combined policy optimization with preference modeling (Christiano et al., 2017), and later work scaled to language models with iterative data collection (Ziegler et al., 2019). Following Nguyen et al. 1, we collect pairwise rankings in batches and periodically refit the reward model to mitigate drift. Alternative strategies optimize from implicit signals such as clicks or dwell time in recommendation settings (Ie et al., 2019).\n\nOur method extends offline RLHF with conservative policy updates and counterfactual data augmentation. We evaluate robustness to annotator disagreement and explore calibration of the learned reward via temperature scaling.",
    "reason": "Improper use of a footnote-like number without proper citation formatting; it should include the year as an author–year citation (e.g., 'Nguyen et al. (2020)') or be formatted as an actual footnote.",
    "start": 355,
    "end": 370,
    "label": "Format"
  },
  {
    "span": "Nguyen et al. 2",
    "document": "Introduction\n\nMultimodal summarization integrates textual and visual cues to produce concise abstracts of rich media content. Comprehensive surveys by Nguyen et al. 2 categorize methods by the alignment strategy between modalities and highlight the trade-offs between late fusion and cross-attention fusion. More recent work emphasizes contrastive pretraining to unify vision–language representations (Singh et al., 2021; Duarte and Kim, 2022).",
    "reason": "Wrong use of footnote-style numbering without a proper citation format: should include the year (e.g., “Nguyen et al. (2020)”) or be formatted as a proper footnote.",
    "start": 151,
    "end": 166,
    "label": "Format"
  },
  {
    "span": "BERT has been used in AES with prompt-specific adapters.",
    "document": "Introduction\n\nAutomated Essay Scoring (AES) seeks to predict holistic or trait scores from student writing. Traditional regression over hand-crafted features has given way to pretrained language models that better capture coherence, style, and content relevance. BERT has been used in AES with prompt-specific adapters. Yet, data scarcity per prompt and rubric variability limit generalization, motivating methods that transfer knowledge across prompts. We investigate multi-task learning with calibration objectives to improve fairness and robustness across diverse prompts.",
    "reason": "This mentions a specific modeling setup in prior work (BERT with prompt-specific adapters) without citing any supporting paper, violating rule (a) and example (e-iii).",
    "start": 263,
    "end": 319,
    "label": "Unsupported_claim"
  },
  {
    "span": "((Mikolov et al., 2013)",
    "document": "Related Work\n\nDistributed word representations underlie many advances in NLP. Predictive models trained on large corpora capture syntactic and semantic regularities ((Mikolov et al., 2013; Pennington et al., 2014). Contextualized encoders further adapt token meaning to usage, significantly improving transfer (Peters et al., 2018; Devlin et al., 2019). Subword modeling mitigates OOV issues in morphologically rich languages (Sennrich et al., 2016; Kudo, 2018).\n\nDespite strong performance, embeddings can encode societal biases and spurious correlations (Bolukbasi et al., 2016; Caliskan et al., 2017). Debiasing and counterfactual data augmentation partially alleviate these problems but may distort useful signal (Zhao et al., 2018; Ravfogel et al., 2020).",
    "reason": "Double opening parenthesis before a parenthetical citation; should be a single '(' as in '(Mikolov et al., 2013)'.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "Chen et al. (2021) evaluate code generation at scale with a human-designed benchmark. AlphaCode achieves competition-level programming through sampled search (Li et al., 2022). Program repair techniques propose edits from learned models (Gupta et al., 2017). Static analysis detects classes of bugs automatically (Johnson et al., 2013).",
    "document": "Related Work\n\nProgram synthesis with large language models (LLMs) requires balancing correctness, efficiency, and security under limited test feedback. Recent research has explored dataset curation, search strategies, and verification to improve pass rates while limiting spurious generalization.\n\nChen et al. (2021) evaluate code generation at scale with a human-designed benchmark. AlphaCode achieves competition-level programming through sampled search (Li et al., 2022). Program repair techniques propose edits from learned models (Gupta et al., 2017). Static analysis detects classes of bugs automatically (Johnson et al., 2013).\n\nWe differ by integrating static and dynamic checks into the sampling loop through learned verifiers, guiding search toward semantically consistent candidates.",
    "reason": "The span lists separate works—benchmarking, competition-level search, learned repair, and static analysis—without transitions or explicit explanation of how they relate to each other, creating abrupt, unconnected sentences.",
    "start": 298,
    "end": 634,
    "label": "Coherence"
  },
  {
    "span": "The widely used SQuAD 2.0 dataset contains over 50,000 unanswerable questions curated by experts.",
    "document": "Introduction\n\nExtractive question answering evaluates a system's ability to locate answer spans in a given passage. While early benchmarks focused on answerable questions with direct span evidence, later datasets introduced unanswerable questions to stress test calibration and reasoning under uncertainty. These benchmarks have driven progress in both model architecture and training methodology, as systems must balance recall with precise abstention.\n\nThe widely used SQuAD 2.0 dataset contains over 50,000 unanswerable questions curated by experts. Despite widespread adoption of this benchmark, many models still struggle to confidently abstain when appropriate. In this work, we examine the role of contrastive training signals and confidence-aware decoding for improving abstention without sacrificing answer accuracy.",
    "reason": "Presents a specific statistic about a named dataset without citing a source.",
    "start": 455,
    "end": 552,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Baker et al. 2018)",
    "document": "Related Work\n\nData augmentation for text classification includes synonym replacement, back-translation, and mixup variants (Wu and Agarwal, 2019; Nair et al., 2020). For low-resource settings, paraphrase generation can reduce overfitting (Ibrahim and Choi, 2021).\n\nOur method composes label-preserving transformations with confidence-aware filtering (Sosa and Vega, 2022). Prior studies report improvements on sentiment and topic datasets (Baker et al. 2018) and (Diaz and Ryu, 2019), but results on hierarchical labels remain mixed.",
    "reason": "Missing comma between author and year in parenthetical citation. Should be '(Baker et al., 2018)'.",
    "start": 439,
    "end": 458,
    "label": "Format"
  },
  {
    "span": "Self-supervised pretraining on unlabeled audio has emerged as a promising direction (Schneider et al., 2019; Baevski et al., 2020; Hsu et al., 2021).",
    "document": "Introduction\n\nLow-resource automatic speech recognition (ASR) remains challenging due to limited transcribed data, dialectal variation, and domain mismatch between training and deployment conditions. Approaches to close this gap include transfer learning, multilingual training, and semi-supervised learning with pseudo-labels.\n\nPretraining and adaptation. Self-supervised pretraining on unlabeled audio has emerged as a promising direction (Schneider et al., 2019; Baevski et al., 2020; Hsu et al., 2021). Other techniques leverage cross-lingual representations, pronunciation modeling, and lexicon-free decoding for data efficiency (Conneau et al., 2020; Pratap et al., 2020; Zhang et al., 2021).\n\nIn this work, we examine adaptation under extreme label scarcity (<1 hour labeled), introducing an alignment-consistent objective that couples acoustic units with subword targets to stabilize fine-tuning across languages.",
    "reason": "The span states that self-supervised pretraining is promising but does not explain why, what specific gaps remain, or how this motivates the authors’ method, thus lacking synthesis to the paper’s argument.",
    "start": 357,
    "end": 506,
    "label": "Lacks_synthesis"
  },
  {
    "span": "The MetaSet dataset has become the de facto benchmark for few-shot evaluation in computer vision.",
    "document": "Introduction\n\nFew-shot learning aims to generalize to novel categories with only a handful of labeled examples. Metric-based methods and gradient-based meta-learning have shown promise by learning transferable inductive biases from episodic training regimes. Despite progress, evaluation remains fragmented across tasks and domains.\n\nThe MetaSet dataset has become the de facto benchmark for few-shot evaluation in computer vision. However, inconsistencies in episode sampling, backbone capacity, and pretraining objectives hinder fair comparisons across methods. We introduce a standardized protocol and a suite of diagnostics to assess sensitivity to class imbalance and domain shift.\n\nOur contributions include a unified evaluation harness and an analysis of how episodic sampling strategies influence generalization under varying support sizes.",
    "reason": "Claims benchmark status of a specific dataset without providing a supporting citation (rule a).",
    "start": 334,
    "end": 431,
    "label": "Unsupported_claim"
  },
  {
    "span": "Brown & Lee (2020)",
    "document": "Related Work\n\nAspect-based sentiment analysis (ABSA) decomposes opinions into targets and associated polarities (Pontiki et al., 2014). Neural approaches exploit syntactic structure and attention mechanisms to better link opinion expressions with their aspects (Wang et al., 2016; Ma et al., 2017).\n\nContextual encoders and span-level models further improve extraction and classification, especially in cross-domain settings (He et al., 2018; Xu et al., 2019). Brown & Lee (2020) demonstrate that jointly modeling aspect terms and categories reduces error propagation in pipeline systems.\n\nOur method extends joint modeling with a constrained decoding layer that enforces cross-sentence consistency.",
    "reason": "Wrong conjunction in narrative citation for APA-like style: '&' should not be used in running text; use 'and' -> 'Brown and Lee (2020)'.",
    "start": 461,
    "end": 479,
    "label": "Format"
  },
  {
    "span": "Client-level and record-level differential privacy mechanisms inject calibrated noise to gradients or updates (McMahan et al., 2018; Kairouz et al., 2021; Geyer et al., 2017).",
    "document": "Introduction\n\nFederated learning (FL) enables on-device training without centralizing raw data, but it raises privacy concerns due to potential information leakage from model updates. Differential privacy (DP) provides formal guarantees by bounding the influence of any individual client or record.\n\nClient-level and record-level differential privacy mechanisms inject calibrated noise to gradients or updates (McMahan et al., 2018; Kairouz et al., 2021; Geyer et al., 2017). Secure aggregation further prevents the server from observing individual client updates in plaintext (Bonawitz et al., 2017). Personalization techniques adapt global models to heterogeneous client distributions through mixture models and meta-learning (Fallah et al., 2020; Arivazhagan et al., 2019).\n\nCommunication efficiency. Multiple works compress updates via sparsification, quantization, and error feedback to reduce bandwidth, often trading off convergence speed and accuracy (Stich et al., 2018; Alistarh et al., 2017; Karimireddy et al., 2019).",
    "reason": "The span reports categories of DP mechanisms without explaining the particular limitations relevant to the authors' setting, the trade-offs motivating their method, or how prior work informs their approach.",
    "start": 300,
    "end": 475,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Classical methods such as ARIMA model linear temporal dependencies (Box and Jenkins, 1970). Transformers leverage self-attention for long-range forecasting (Zhou et al., 2021). Probabilistic forecasts quantify uncertainty via distributional outputs (Salinas et al., 2020).",
    "document": "Related Work\n\nTime Series Forecasting\n\nForecasting methods span from classical statistical models to deep learning architectures and probabilistic predictors. Classical methods such as ARIMA model linear temporal dependencies (Box and Jenkins, 1970). Transformers leverage self-attention for long-range forecasting (Zhou et al., 2021). Probabilistic forecasts quantify uncertainty via distributional outputs (Salinas et al., 2020). Decomposition techniques separate trend, seasonality, and residual components (Cleveland et al., 1990; Yu et al., 2016). Recent benchmarks highlight scale variability and evaluation under multiple horizons (Montero-Manso et al., 2020; Woo et al., 2022).",
    "reason": "The span abruptly enumerates distinct lines of work—classical models, transformer-based models, and probabilistic forecasting—without transitions or explicit links between them.",
    "start": 159,
    "end": 431,
    "label": "Coherence"
  },
  {
    "span": "Miller et al. 1",
    "document": "Introduction\n\nProgram synthesis from natural language has progressed with neural decoders trained on paired text-code datasets (Yin and Neubig, 2017; Rabinovich et al., 2017). Early error analyses, reported by Miller et al. 1, emphasized compositional generalization failures when variable bindings were reused. Follow-up studies proposed grammar-constrained decoding to improve well-formedness and reduce search space (Krishnamurthy et al., 2017; Dong and Lapata, 2018). We extend this line of work by incorporating structure-aware contrastive objectives that penalize near-miss programs differing only in argument order.",
    "reason": "Improper footnote-style usage appended to an author name; should include a publication year (e.g., \"Miller et al. (YEAR)\") or be formatted as a proper footnote (e.g., \"Miller et al. [1]\") with a corresponding note.",
    "start": 210,
    "end": 225,
    "label": "Format"
  },
  {
    "span": "Several competitions on bug detection have standardized benchmarks for evaluating static analysis tools.",
    "document": "Related Work\n\nSoftware defect prediction spans static analysis, dynamic testing, and learning-based approaches that mine code and change histories. Public corpora of labeled bugs and patches have enabled comparative evaluation of classification models that predict defect-prone files or functions. Beyond datasets, community challenges have galvanized progress by defining common tasks and metrics for source-level vulnerability discovery and repair. Several competitions on bug detection have standardized benchmarks for evaluating static analysis tools. Neural models have recently leveraged code structure through graph and sequence encoders, but overfitting to project-specific idioms remains a concern. We propose cross-project pretraining with contrastive objectives to enhance transfer to unseen repositories.",
    "reason": "References competitions and standardized benchmarks without citing any specific events or datasets.",
    "start": 451,
    "end": 555,
    "label": "Unsupported_claim"
  },
  {
    "span": "Gilmer et al. (2017) formalized message passing neural networks for molecular property prediction. Quantum mechanical targets require accurate electronic structure approximations (Perdew et al., 1996). Xu et al. (2019) analyzed the expressive power of GNNs via the Weisfeiler–Leman test. Data augmentation using multiple conformers can improve generalization (Ramakrishnan et al., 2014).",
    "document": "Related Work\n\nGraph Neural Networks for Molecular Modeling. Graph neural networks (GNNs) have become the dominant paradigm for molecular property prediction by operating directly on molecular graphs and learning task-specific representations (Duvenaud et al., 2015; Gilmer et al., 2017). Architectural advances include attention mechanisms, spectral filters, and equivariant layers that encode geometric information (Velickovic et al., 2018; Defferrard et al., 2016; Satorras et al., 2021).\n\nExpressivity and Targets. Gilmer et al. (2017) formalized message passing neural networks for molecular property prediction. Quantum mechanical targets require accurate electronic structure approximations (Perdew et al., 1996). Xu et al. (2019) analyzed the expressive power of GNNs via the Weisfeiler–Leman test. Data augmentation using multiple conformers can improve generalization (Ramakrishnan et al., 2014). In contrast, our approach studies how incorporating 3D geometry and electronic features jointly affects performance on out-of-distribution molecules.\n\nDatasets and Evaluation Protocols. Common benchmarks include QM9 for small molecules and MoleculeNet’s suite (ZINC, ESOL, FreeSolv) for physical and biochemical tasks (Ramakrishnan et al., 2014; Wu et al., 2018). We adopt scaffold splits to better reflect generalization to novel chemotypes (Hu et al., 2020).",
    "reason": "The sentences jump between message passing, quantum approximations, expressive power theory, and data augmentation without linking how each relates to the previous one or to a coherent subtopic, leading to poor sentence-to-sentence transitions.",
    "start": 518,
    "end": 905,
    "label": "Coherence"
  },
  {
    "span": "In molecular property prediction, message passing neural networks, graph attention mechanisms, and spectral graph convolutions have been widely adopted (Alvarez et al., 2019; Kim and Cho, 2020; Duarte et al., 2018; Singh et al., 2021). Self-supervised pretraining on molecular graphs using context prediction or contrastive losses has also been explored (Rao et al., 2020; Lin et al., 2021; Ortega and Ma, 2022).",
    "document": "Related Work\nGraph neural networks for molecules. In molecular property prediction, message passing neural networks, graph attention mechanisms, and spectral graph convolutions have been widely adopted (Alvarez et al., 2019; Kim and Cho, 2020; Duarte et al., 2018; Singh et al., 2021). Self-supervised pretraining on molecular graphs using context prediction or contrastive losses has also been explored (Rao et al., 2020; Lin et al., 2021; Ortega and Ma, 2022). Several works exploit scaffold splits and domain-specific augmentations (Teo et al., 2021; Yuan and Patel, 2022).\n\nUncertainty and calibration. Predictive uncertainty in cheminformatics has been addressed via ensembles, Bayesian message passing, and Monte Carlo dropout (Costa et al., 2020; Wu and Huang, 2021; Bell and Shah, 2022). Calibration under distribution shift has gained attention through temperature scaling and post-hoc adjustments (Nash and Cole, 2021; Ren et al., 2022).\n\nIn this paper, we target robust OOD generalization by coupling substructure-aware pretraining with an adaptive risk minimization objective tailored to scaffold shifts, and we evaluate on curated OOD benchmarks with stringent chemical dissimilarity criteria.",
    "reason": "The span lists categories of methods and cites multiple papers but provides no explanation of how these approaches compare, what gaps persist, or how they motivate the authors' method; it merely catalogs prior work without synthesis.",
    "start": 50,
    "end": 462,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Chen et al. 2017)",
    "document": "Introduction\n\nEnd-to-end automatic speech recognition (ASR) models replace modular pipelines with neural architectures trained directly from audio-text pairs (Goyal and Tan, 2019). Attention-based encoder-decoder models and CTC/attention hybrids have achieved strong results across languages and domains (Park and Li, 2020). Streaming constraints and latency-aware decoding remain active areas of research (Rao and Byrne, 2021). Prior work has explored joint language model integration during training to improve prefix stability (Chen et al. 2017), while others propose rescoring with external LMs at inference (Ahmed and Silva, 2020).\n\nWe introduce an alignment-regularized transducer that balances blank probabilities with coverage to mitigate early emission. Our experiments on multilingual corpora show consistent WER reductions and improved calibration under streaming settings (Mendes and Ortiz, 2022).",
    "reason": "Missing comma between author and year in a parenthetical citation; should be '(Chen et al., 2017)'.",
    "start": 530,
    "end": 548,
    "label": "Format"
  },
  {
    "span": "BERT has been used in automatic essay scoring trained on student essays to capture semantic coherence.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) traditionally relied on surface features such as length, vocabulary richness, and error counts. With pretrained language models, semantic and discourse properties can be modeled more effectively. BERT has been used in automatic essay scoring trained on student essays to capture semantic coherence. Other approaches incorporate syntactic parse features or coherence graphs to improve robustness across prompts. Our contribution differs by aligning discourse representations with rubric dimensions via multi-task learning.",
    "reason": "Describes a specific prior use of BERT for AES without providing a citation to the work demonstrating this setup (criterion a).",
    "start": 240,
    "end": 342,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from multiple prompts.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) seeks to assign reliable scores that align with human judgment across prompts and genres. Traditional AES pipelines relied on hand-crafted features such as syntactic variety and discourse coherence; neural encoders later improved robustness by modeling local and global context. BERT was used in an AES task trained on essays from multiple prompts. Cross-prompt generalization remains challenging due to prompt-specific lexical cues and topical leakage. To address this, multi-task and domain-adaptive strategies have been proposed, along with content-preserving data augmentation. Our approach complements these by disentangling prompt-specific semantics from quality indicators through contrastive regularization.",
    "reason": "Describes a specific setup ('BERT... in an AES task... multiple prompts') without any citation (matches example iii; rule a).",
    "start": 323,
    "end": 392,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT was used in an AES task trained on essays from the ASAP corpus to obtain state-of-the-art results.",
    "document": "Introduction\n\nAutomatic essay scoring (AES) aims to predict holistic or trait-specific scores for student essays to support large-scale assessment and formative feedback. Traditional AES systems rely on hand-crafted features capturing grammar, cohesion, and discourse structure, often combined with linear or tree-based models. With the advent of pre-trained language models, contextualized representations have become the dominant approach.\n\nBERT was used in an AES task trained on essays from the ASAP corpus to obtain state-of-the-art results. However, the extent to which domain adaptation and prompt-specific calibration affect model generalization remains unclear. We investigate prompt-agnostic fine-tuning strategies and propose an uncertainty-aware calibration layer that reduces overfitting to prompt-specific artifacts.\n\nWe evaluate our approach across multiple prompts and cross-prompt transfer settings, reporting results in terms of quadratic weighted kappa and mean absolute error.",
    "reason": "Mentions a specific prior setup and dataset without providing a citation per rule (a)/(b).",
    "start": 443,
    "end": 546,
    "label": "Unsupported_claim"
  },
  {
    "span": "The Netflix Prize established collaborative filtering as the dominant paradigm for recommendation.",
    "document": "Introduction\n\nRecommender systems have evolved from memory-based neighborhood methods to model-based techniques that capture user–item interactions. The Netflix Prize established collaborative filtering as the dominant paradigm for recommendation. Subsequent advances introduced factorization machines and deep neural networks to learn high-capacity interaction functions. Recently, graph neural networks (GNNs) have been explored to exploit higher-order connectivity in user–item bipartite graphs. Despite promising results, most studies optimize top-k ranking on static snapshots and underexplore temporal and cold-start dynamics.",
    "reason": "Mentions a well-known competition at first mention without citing it and asserts its historical impact without evidence.",
    "start": 149,
    "end": 247,
    "label": "Unsupported_claim"
  },
  {
    "span": "(He et al, 2016)",
    "document": "Related Work\n\nDeep convolutional networks have steadily improved through architectural innovations such as inception modules (Szegedy et al., 2015), residual connections (He et al., 2016), and squeeze-and-excitation blocks (Hu et al., 2018). These advances enable very deep models while mitigating vanishing gradients and optimization difficulties (Ioffe and Szegedy, 2015; Mishkin and Matas, 2016).\n\nIn image recognition, residual learning remains a foundational design that eases optimization by learning perturbations around identity mappings, as originally shown in (He et al, 2016) and later generalized to wider and pre-activation variants (Zagoruyko and Komodakis, 2016; He et al., 2016b). Our backbone adopts a pre-activation formulation to stabilize training at scale.",
    "reason": "Incorrect punctuation in citation: missing period after 'al.' and missing comma before the year; should be '(He et al., 2016)'.",
    "start": 570,
    "end": 586,
    "label": "Format"
  },
  {
    "span": "(Sato et. al., 2020)",
    "document": "Introduction\n\nMultimodal dialogue systems integrate language understanding with visual and acoustic cues to ground responses in context. Pre-trained encoders adapted to multimodal inputs have improved slot filling and response generation (Tsai et al., 2019; Radford et al., 2021). For retrieval-augmented dialogue, hybrid encoders combining text and image features have shown promise (Karpukhin et al., 2020; (Sato et. al., 2020). Nonetheless, robustness to missing modalities remains an open question.\n\nWe present a modality-robust objective that encourages graceful degradation when one or more inputs are absent.",
    "reason": "Incorrect abbreviation 'et. al.' with a period after 'et'; the correct form is 'et al.' so the citation should be \"(Sato et al., 2020)\".",
    "start": 409,
    "end": 429,
    "label": "Format"
  },
  {
    "span": "[27]",
    "document": "Related Work\n\nFew-shot learning tackles classification with extremely limited labeled data by exploiting meta-learning and metric-learning strategies (Vinyals et al., 2016; Snell et al., 2017). Data augmentation and self-supervision are often used to enrich representations and reduce overfitting (Gidaris et al., 2019; Tian et al., 2020). Prototype refinement through transductive inference has also proven effective when query distributions are accessible at test time (Liu et al., 2019; Dhillon et al., 2020).\n\nWhile prior work focuses on Euclidean metrics, hyperbolic and manifold-aware approaches capture hierarchical relations between classes (Khrulkov et al., 2020; Zhang et al., 2021). Recent results [27] suggest that optimizing for distributional calibration across episodes can further improve robustness under domain shift. We extend these insights with a class-conditional prior that calibrates episodic prototypes via Bayesian adaptation.",
    "reason": "Numeric bracket citation used in an author–year styled section; should provide an author–year citation instead of \"[27]\".",
    "start": 709,
    "end": 713,
    "label": "Format"
  },
  {
    "span": "Recent works show that transformer decoders outperform CNN baselines on long-horizon load forecasting.",
    "document": "Related Work\n\nTime series forecasting for energy systems requires models that capture seasonality, exogenous drivers, and rare events. Traditional ARIMA-family models struggle with nonstationarity, while deep architectures can leverage larger context windows and learned representations. Recent works show that transformer decoders outperform CNN baselines on long-horizon load forecasting. Nevertheless, transformers often incur high computational costs and can overfit on small utilities with limited historical data.\n\nWe contribute a lightweight hybrid architecture that couples frequency-domain filtering with sparse attention to retain performance gains while reducing compute and memory.",
    "reason": "Mentions 'recent works' with a performance claim but provides no citations to those works (definition d).",
    "start": 288,
    "end": 390,
    "label": "Unsupported_claim"
  },
  {
    "span": "Knowledge distillation for speech recognition encompasses teacher–student training at the frame, sequence, and posterior levels. Teachers include hybrid HMM-DNNs, CTC models, and transducers, and students benefit from softened targets, intermediate feature matching, and data augmentation such as SpecAugment. Distillation has also been applied in multilingual and noisy environments.",
    "document": "Introduction\nEnd-to-end speech recognizers have achieved state-of-the-art results but remain expensive to deploy on-device. Knowledge distillation compresses models by transferring knowledge from a large teacher to a compact student.\n\nRelated Work\nDistillation in ASR spans multiple granularities and architectures.\nKnowledge distillation for speech recognition encompasses teacher–student training at the frame, sequence, and posterior levels. Teachers include hybrid HMM-DNNs, CTC models, and transducers, and students benefit from softened targets, intermediate feature matching, and data augmentation such as SpecAugment. Distillation has also been applied in multilingual and noisy environments.\nAdditional efforts integrate language models into distillation or perform sequence-level KD with minimum Bayes risk objectives.\n\nThis Work\nWe study streaming-capable students with low latency under constrained compute budgets.",
    "reason": "The span catalogs distillation techniques and settings but does not connect them to the focus on streaming-capable students or state deficiencies of prior work, hence lacking synthesis per (a) and (c).",
    "start": 316,
    "end": 700,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Wang et al., 2020;; Li, 2019)",
    "document": "Introduction\n\nAccurate time-series forecasting under limited supervision is critical for applications such as demand planning and energy management (De Gooijer and Hyndman, 2006; Makridakis et al., 2018). Global models that leverage cross-series patterns often outperform local models on sparse data (Bandara et al., 2020; Montero-Manso and Hyndman, 2020). Hybrid architectures further combine statistical components with neural networks to capture both short- and long-term dependencies (Salinas et al., 2020; Rangapuram et al., 2018).\n\nRegularization and data augmentation have been shown to improve generalization in low-data regimes (Wen et al., 2017; Oreshkin et al., 2020). Recent advances explore frequency-domain augmentation and seasonality-aware embeddings to stabilize training across heterogeneous series (Wang et al., 2020;; Li, 2019). We extend these ideas with a probabilistic calibration layer that aligns predictive quantiles with empirical coverage.",
    "reason": "Punctuation error in citation: double semicolon inside the same parenthetical citation; should be a single separator (e.g., \";\").",
    "start": 817,
    "end": 847,
    "label": "Format"
  },
  {
    "span": "There are many recent works that explore alignment with minimal supervision.",
    "document": "Related Work\n\nVision-language pretraining has rapidly advanced due to large-scale paired image–text corpora. CLIP-style models align images and natural language by contrasting matched and mismatched pairs, enabling strong zero-shot transfer across tasks. Extensions incorporate region-level grounding and cross-modal transformers to improve fine-grained understanding. There are many recent works that explore alignment with minimal supervision. However, most approaches still depend on web-scale datasets and heuristics for negative sampling, which can introduce spurious correlations and biases. Our method differs by leveraging structure-aware augmentation to reduce reliance on negatives while maintaining competitive performance.",
    "reason": "Mentions 'recent works' without providing any citations to those works, violating rule (d) for unsupported claims.",
    "start": 369,
    "end": 445,
    "label": "Unsupported_claim"
  },
  {
    "span": "To reduce computation on edge devices, pruning removes redundant weights (Han et al., 2015), quantization compresses parameters to low-bit formats (Jacob et al., 2018; Nahshan et al., 2020), and knowledge distillation transfers behavior from large to compact models (Hinton et al., 2015; Tian et al., 2020). Neural architecture search discovers efficient topologies (Tan et al., 2019; Cai et al., 2019), and hardware-aware training co-optimizes models with device constraints (Cai et al., 2020).",
    "document": "Introduction\n\nEdge deployment of deep neural networks is constrained by limited compute, memory, and energy budgets. A variety of compression and efficiency techniques have been proposed to meet these constraints without severely compromising accuracy.\n\nTo reduce computation on edge devices, pruning removes redundant weights (Han et al., 2015), quantization compresses parameters to low-bit formats (Jacob et al., 2018; Nahshan et al., 2020), and knowledge distillation transfers behavior from large to compact models (Hinton et al., 2015; Tian et al., 2020). Neural architecture search discovers efficient topologies (Tan et al., 2019; Cai et al., 2019), and hardware-aware training co-optimizes models with device constraints (Cai et al., 2020).\n\nSystem-level approaches complement algorithmic compression through on-device schedulers and operator fusion (Zhang et al., 2020).\n\nWe investigate efficiency-accuracy trade-offs under strict latency constraints, but details follow in later sections.",
    "reason": "The span is a catalog of techniques unconnected to the authors' objective or gap; it does not articulate how these methods inform or contrast with the proposed work (criteria a and c).",
    "start": 254,
    "end": 749,
    "label": "Lacks_synthesis"
  },
  {
    "span": "It is well known that temperature scaling of 4 yields the best results across NLP benchmarks.",
    "document": "Introduction\n\nKnowledge distillation transfers information from a large teacher model to a compact student via softened targets and auxiliary objectives (Hinton et al., 2015; Jiao et al., 2020). The temperature parameter in the softmax controls the smoothness of teacher probabilities and influences student generalization (Yuan et al., 2020). It is well known that temperature scaling of 4 yields the best results across NLP benchmarks. While prior work tunes temperature per task, a principled understanding of its interaction with calibration and label smoothing remains limited (Müller et al., 2019).\n\nWe present a theory-driven schedule that adapts temperature during training based on gradient variance, leading to consistent gains without task-specific sweeps.",
    "reason": "States a universal best temperature value without any supporting citation or evidence (violates rule b and e).",
    "start": 344,
    "end": 437,
    "label": "Unsupported_claim"
  },
  {
    "span": "Vatswani et al.",
    "document": "Introduction\n\nGraph neural networks (GNNs) have become a standard tool for learning on non-Euclidean data by iteratively aggregating neighborhood information to produce expressive node and graph representations (Kipf and Welling, 2017; Hamilton et al., 2017). Despite rapid progress, challenges remain in effectively handling heterophily, scalability, and dynamic structures (Pei et al., 2020; Rossi et al., 2020).\n\nEarly efforts to stabilize message passing under sparse supervision were introduced by Vatswani et al., who proposed constraining the propagation depth to mitigate over-smoothing effects. Subsequent work formalized the phenomenon and proposed architectural remedies, including residual and jumping knowledge connections (Xu et al., 2018; Li et al., 2019). In complementary directions, scalable sampling, subgraph training, and quantization techniques have broadened the applicability of GNNs to web-scale graphs (Chen et al., 2018; Zeng et al., 2019).\n\nIn this paper, we revisit the trade-offs between expressivity and stability in multi-hop aggregation and propose an adaptive propagation schedule driven by node-level uncertainty estimates. We evaluate on eight benchmarks and show consistent improvements in both homophilous and heterophilous regimes.",
    "reason": "Narrative citation is missing the publication year; it should appear as 'Vatswani et al. (YEAR)'.",
    "start": 503,
    "end": 518,
    "label": "Format"
  },
  {
    "span": "Basu (2015;",
    "document": "Introduction\n\nTime-series anomaly detection often combines forecasting with residual thresholding (Laptev et al., 2015). Classical density estimators struggle with non-stationarity, leading to renewed interest in deep probabilistic models. Basu (2015; provides a unifying view of change-point detection under Bayesian frameworks, which we adapt to multivariate settings with temporal covariates.",
    "reason": "Narrative citation has an opening parenthesis but uses a semicolon and lacks the closing parenthesis. It should be “Basu (2015) provides …”.",
    "start": 240,
    "end": 251,
    "label": "Format"
  },
  {
    "span": "Yin and Neubig (2017) used a syntax-constrained decoder for semantic parsing. Li et al. (2018) applied neural program synthesis with reinforcement learning. Chen et al. (2021) leveraged few-shot prompting for code generation.",
    "document": "Related Work\n\nProgram synthesis from natural language spans semantic parsing, code generation, and interactive repair. Methods vary in how they incorporate syntax, search, and learning signals to improve correctness and generalization.\n\nYin and Neubig (2017) used a syntax-constrained decoder for semantic parsing. Li et al. (2018) applied neural program synthesis with reinforcement learning. Chen et al. (2021) leveraged few-shot prompting for code generation. Austin et al. (2021) explored program synthesis with large language models and self-consistency.\n\nOur approach unifies constrained decoding with execution-guided reranking under a retrieval-augmented encoder.",
    "reason": "The span presents three different directions (semantic parsing, RL-based synthesis, few-shot LLM prompting) without linking them or providing transitions that clarify their relationships.",
    "start": 237,
    "end": 462,
    "label": "Coherence"
  },
  {
    "span": "dark themes reduce eye strain by 40% during prolonged reading",
    "document": "Introduction\n\nDark mode has become a default option across mobile and desktop systems, motivated by potential benefits for comfort, battery life, and accessibility. Despite broad adoption, empirical evidence for when and how dark palettes help is mixed and context-dependent.\n\nIndustry guidelines often recommend dark backgrounds for low-light environments, yet there is limited agreement on perceptual and performance impacts across tasks such as reading and visual search. Common claims suggest that dark themes reduce eye strain by 40% during prolonged reading, but user studies vary in methodology and measurement.\n\nIn this work, we present a controlled experiment that isolates luminance contrast, ambient illumination, and font rendering to quantify the effects of theme choice on comfort, performance, and preference.",
    "reason": "Presents a specific quantitative statistic ('40%') without any supporting citation or evidence (rule b).",
    "start": 502,
    "end": 563,
    "label": "Unsupported_claim"
  },
  {
    "span": "Late fusion combines independent branch predictions (Snoek et al., 2005; Zhou et al., 2019). Self-supervised pretraining on radiology reports aligns text and image embeddings (Zhang et al., 2020; Boecking et al., 2022).",
    "document": "Related Work\n\nMultimodal learning in medical imaging seeks to integrate signals from pixel-level scans and associated clinical text to improve diagnostic accuracy and calibration. Prior work explores various fusion strategies and representation learning methods to address limited labeled data and domain shift across institutions.\n\nEarly fusion concatenates token and visual features at intermediate layers to encourage cross-attention between modalities (Tsai et al., 2019; Baltrusaitis et al., 2019). Cross-modal contrastive objectives align views from images and reports to improve retrieval and zero-shot transfer (Radford et al., 2021; Zhang et al., 2020). Late fusion combines independent branch predictions (Snoek et al., 2005; Zhou et al., 2019). Self-supervised pretraining on radiology reports aligns text and image embeddings (Zhang et al., 2020; Boecking et al., 2022). Domain adaptation techniques mitigate scanner and protocol differences via adversarial or distance-based regularizers (Ganin et al., 2016; Hoffman et al., 2018).\n\nWe focus on calibration-aware fusion that jointly models uncertainty across modalities and enforces consistency under missing-view scenarios common in practice.",
    "reason": "The span abruptly moves from late fusion to self-supervised pretraining without clarifying the connection or transition, leaving the relationship between these cited lines of work implicit.",
    "start": 663,
    "end": 882,
    "label": "Coherence"
  },
  {
    "span": "Prior shared tasks have consistently shown that rule-based systems lag far behind neural approaches on aspect based sentiment analysis.",
    "document": "Introduction\n\nAspect based sentiment analysis decomposes opinion mining into aspect extraction and sentiment classification, enabling fine-grained analysis for products and services (Pontiki et al., 2014). Neural architectures with attention and pretrained language models have advanced the state of the art across multiple benchmarks.\n\nPrior shared tasks have consistently shown that rule-based systems lag far behind neural approaches on aspect based sentiment analysis. Concurrent efforts explore data augmentation, domain adaptation, and multi-task learning to improve performance in low-resource domains (Ma et al., 2018; He et al., 2018).\n\nWe revisit model design under annotation scarcity by introducing a constrained adapter framework that leverages task structure while maintaining parameter efficiency. Experiments cover restaurant, laptop, and service domains with cross-domain transfer.",
    "reason": "References shared tasks and a comparative outcome but provides no citations to the specific tasks or results.",
    "start": 337,
    "end": 472,
    "label": "Unsupported_claim"
  },
  {
    "span": "Kumar et al., (2016)",
    "document": "Related Work\n\nSemi-supervised learning reduces label dependence by exploiting unlabeled data through consistency, entropy minimization, or pseudo-labeling (Grandvalet and Bengio, 2005; Sajjadi et al., 2016; Laine and Aila, 2017). Consistency regularization enforces invariance under stochastic perturbations, a principle realized by VAT and Mean Teacher (Miyato et al., 2018; Tarvainen and Valpola, 2017). Kumar et al., (2016) explore graph-based methods that propagate labels along manifold structures, while later approaches combine MixUp-style interpolation with consistency to close the gap to fully supervised baselines (Berthelot et al., 2019; Sohn et al., 2020).",
    "reason": "Extraneous comma before the parenthetical year in a narrative citation; should be Kumar et al. (2016).",
    "start": 406,
    "end": 426,
    "label": "Format"
  },
  {
    "span": "Prototypical Networks are the de facto baseline for few-shot relation extraction.",
    "document": "Related Work\n\nFew-shot relation extraction (FSRE) aims to generalize to novel relations with limited labeled instances. Metric-based methods such as Matching Networks and Prototypical Networks have been adapted to FSRE (Vinyals et al., 2016; Snell et al., 2017; Gao et al., 2019). Large pretrained encoders and prompt-based tuning further improved data efficiency (Baldini Soares et al., 2019; Han et al., 2018; Gao et al., 2021).\n\nPrototypical Networks are the de facto baseline for few-shot relation extraction. However, prototype collapse and sensitivity to support set noise hinder robust generalization.\n\nWe introduce a noise-robust prototype refinement procedure and demonstrate gains on FewRel and TACRED-Rev splits.",
    "reason": "Labels a specific method as the de facto baseline without citing shared tasks, leaderboards, or survey evidence (rule b).",
    "start": 432,
    "end": 513,
    "label": "Unsupported_claim"
  },
  {
    "span": "Text data augmentation methods include lexical substitution, synonym-based edits, random insertion/deletion, back-translation, paraphrasing via pretrained models, and mixup-style interpolation in embedding space. Constraint-driven augmentations maintain label semantics by filtering edits with entailment or semantic similarity. We follow this line of work to generate additional training instances.",
    "document": "Related Work\n\nData augmentation for text classification improves generalization under label scarcity and domain shift. Methods vary in how they perturb surface forms versus latent representations and in the degree to which they preserve semantics.\n\nText data augmentation methods include lexical substitution, synonym-based edits, random insertion/deletion, back-translation, paraphrasing via pretrained models, and mixup-style interpolation in embedding space. Constraint-driven augmentations maintain label semantics by filtering edits with entailment or semantic similarity. We follow this line of work to generate additional training instances.\n\nEvaluation protocols typically compare augmentation policies across balanced and imbalanced label distributions with fixed model capacity.\n\nOur study targets cross-domain transfer with limited labeled target data.",
    "reason": "The paragraph lists prior methods and then states the authors' adoption of them without articulating a gap or synthesizing how these methods motivate their specific contribution.",
    "start": 249,
    "end": 648,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BERT has been used in AES tasks trained on hand-scored essays.",
    "document": "Related Work\n\nAutomatic essay scoring (AES) aims to predict human-assigned scores for student writing to support scalable assessment and feedback. Traditional AES systems rely on hand-engineered features such as lexical diversity, syntactic complexity, and discourse indicators.\n\nBERT has been used in AES tasks trained on hand-scored essays. More recent neural approaches incorporate prompt-aware encoders and multi-trait objectives to capture content relevance and writing quality.\n\nOur approach introduces content-salience supervision from reference outlines and jointly models prompt adherence and coherence.",
    "reason": "This claim references a specific application of BERT to AES and should cite the first works demonstrating this setup (guideline a and b).",
    "start": 280,
    "end": 342,
    "label": "Unsupported_claim"
  },
  {
    "span": "the BraTS challenge introduced a standardized split in 2019",
    "document": "Related Work\n\nMedical Image Segmentation\n\nBrain tumor segmentation has been a core benchmark for assessing progress in clinical image analysis. Public challenges have catalyzed advances by providing curated datasets, evaluation servers, and common metrics. Methods based on U-Net backbones, multi-scale context aggregation, and uncertainty modeling have steadily improved reported Dice scores on test benchmarks.\n\nWithin this ecosystem, the BraTS challenge introduced a standardized split in 2019 that many subsequent studies adopted for comparability across institutions and scanners. Concurrently, semi-supervised techniques emerged to exploit unlabeled volumes, reducing annotation burdens while maintaining performance.\n\nWe build on this progress by proposing a topology-aware loss that emphasizes boundary fidelity and reduces false positives in peritumoral regions.",
    "reason": "First mention of a specific shared task/dataset policy (“BraTS challenge” and a particular split year) without citation or evidence.",
    "start": 437,
    "end": 496,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent works demonstrate that contrastive pretraining eliminates the need for any task-specific finetuning.",
    "document": "Introduction\n\nVision–language models pretrain on large-scale image–text pairs to learn aligned representations for downstream tasks such as retrieval, captioning, and VQA. Contrastive objectives have proven effective in aligning modalities, and generative objectives enable conditional text production. Recent works demonstrate that contrastive pretraining eliminates the need for any task-specific finetuning. However, zero-shot performance can be brittle under distribution shift, and captioning requires compositional grounding that purely contrastive training may not provide. We explore hybrid objectives to bridge this gap.",
    "reason": "Asserts a broad claim about 'recent works' and their conclusions without any references.",
    "start": 303,
    "end": 410,
    "label": "Unsupported_claim"
  },
  {
    "span": "Tobin et al. (2017) proposed domain randomization for visual sim-to-real transfer. Peng et al. (2018) introduced dynamics randomization to bridge control discrepancies. Rusu et al. (2017) developed progressive networks for transfer across tasks. Hardware imperfections can be modeled via actuator noise (Tan et al., 2018).",
    "document": "Related Work\n\nRobotic sim-to-real transfer aims to close the gap between simulated training and real-world deployment by addressing differences in perception, dynamics, and hardware. Techniques include randomization, adaptation, and modular architectures.\n\nTobin et al. (2017) proposed domain randomization for visual sim-to-real transfer. Peng et al. (2018) introduced dynamics randomization to bridge control discrepancies. Rusu et al. (2017) developed progressive networks for transfer across tasks. Hardware imperfections can be modeled via actuator noise (Tan et al., 2018).\n\nRecent approaches combine representation learning with online adaptation and uncertainty-aware policies to remain robust under unforeseen shifts (Xie et al., 2022; James et al., 2019). We examine how structured perturbations interact with policy regularization in contact-rich manipulation.",
    "reason": "The cited works are enumerated without explaining how they relate to each other, moving abruptly across vision, dynamics, architecture, and noise modeling with no transitions.",
    "start": 257,
    "end": 579,
    "label": "Coherence"
  },
  {
    "span": "Recent benchmarks indicate that transformer architectures outperform recurrent networks on long-range dependency tests.",
    "document": "Related Work\n\nSequence modeling architectures have progressed from recurrent and convolutional designs to attention-based transformers. Recurrent networks capture temporal order but can struggle with vanishing gradients and limited context. Recent benchmarks indicate that transformer architectures outperform recurrent networks on long-range dependency tests. Nevertheless, transformers incur quadratic complexity with sequence length, motivating research into sparse attention, memory mechanisms, and hierarchical encoders. We contribute an analysis of lightweight attention variants that retain accuracy while reducing computational overhead.\n",
    "reason": "Claims results from “recent benchmarks” without providing any citations to those benchmarks.",
    "start": 241,
    "end": 360,
    "label": "Unsupported_claim"
  },
  {
    "span": "BERT was used in a clinical NER task trained on discharge summaries from a large academic hospital.",
    "document": "Related Work\n\nClinical natural language processing faces unique challenges due to domain-specific terminology, abbreviations, and privacy constraints that limit data sharing. Early systems for named entity recognition (NER) in clinical text relied on dictionary matching and conditional random fields augmented with handcrafted features. With the advent of contextualized language models, transfer learning from general-domain corpora enabled large gains on concept extraction and assertion status classification. BERT was used in a clinical NER task trained on discharge summaries from a large academic hospital. Subsequent efforts explored domain-adaptive pretraining on de-identified notes to bridge vocabulary gaps and improve temporal relation extraction. We build on this line by introducing a hierarchical span selector that leverages section headers and note templates common in electronic health records.",
    "reason": "Refers to a specific prior setup and dataset provenance without providing a citation.",
    "start": 514,
    "end": 613,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recommender systems with privacy use DP-SGD (Abadi et al., 2016), randomized response (Warner, 1965), local differential privacy for implicit feedback (Erlingsson et al., 2014; Wang et al., 2017), and matrix factorization with noise injection (Hua et al., 2015; Berlioz et al., 2015).",
    "document": "Introduction\n\nPersonalized recommendation often relies on sensitive behavioral logs. Differential privacy (DP) provides formal protections, but utility degradation and deployment complexity remain key barriers. We study private training for top-k recommendation under tight epsilon budgets with skewed popularity distributions.\n\nRecommender systems with privacy use DP-SGD (Abadi et al., 2016), randomized response (Warner, 1965), local differential privacy for implicit feedback (Erlingsson et al., 2014; Wang et al., 2017), and matrix factorization with noise injection (Hua et al., 2015; Berlioz et al., 2015).\n\nOur method combines calibrated clipping with exposure-aware noise allocation to protect infrequent items while minimizing accuracy loss on head and tail.",
    "reason": "The span enumerates techniques without explaining trade-offs or how they motivate the proposed exposure-aware approach, demonstrating lack of synthesis as in (a) and (c).",
    "start": 329,
    "end": 613,
    "label": "Lacks_synthesis"
  },
  {
    "span": "BLAST has been widely used for protein function prediction",
    "document": "Introduction\n\nPredicting protein function from sequence is fundamental for annotating newly discovered proteins and guiding experimental studies. Similarity-based methods compare a query sequence against annotated databases and transfer labels from close homologs, while machine learning approaches learn mappings from sequence-derived features to function labels. BLAST has been widely used for protein function prediction due to its speed and effectiveness in identifying homologous sequences. However, homology-based transfer suffers when close matches are unavailable, motivating the development of deep learning models that capture remote evolutionary signals. We propose a transformer-based architecture that integrates multiple sequence alignments with subcellular localization cues to improve Gene Ontology annotation.\n",
    "reason": "Makes a claim about prior tool usage in a specific application area without providing a citation.",
    "start": 365,
    "end": 423,
    "label": "Unsupported_claim"
  },
  {
    "span": "In (Krizhevsky et al., 2012)",
    "document": "Related Work\n\nImage classification performance improved dramatically with the advent of deep convolutional neural networks trained on large datasets. In (Krizhevsky et al., 2012) the success of AlexNet on ImageNet catalyzed a wave of architectural innovations including deeper networks and residual connections (Szegedy et al., 2015; He et al., 2016). Subsequent efforts emphasized efficiency and normalization techniques to stabilize training and reduce computational costs (Ioffe and Szegedy, 2015; Howard et al., 2017).\n\nModern vision transformers (Dosovitskiy et al., 2021) further reframe image recognition as sequence modeling, often relying on large-scale pretraining. Yet, data efficiency and robustness remain concerns. We focus on curriculum-based pretraining schedules that improve data efficiency while retaining strong generalization across distribution shifts (Hendrycks and Dietterich, 2019; Taori et al., 2020).",
    "reason": "Wrong citation style: the preposition 'In' should not precede a parenthetical citation; it should be 'In Krizhevsky et al. (2012)...' for narrative use.",
    "start": 150,
    "end": 178,
    "label": "Format"
  },
  {
    "span": "Recent works propose doubly robust estimators for continuous control.",
    "document": "Related Work\n\nOff-policy evaluation (OPE) in reinforcement learning estimates the value of a target policy using logged data from a different behavior policy. Classical approaches include importance sampling, model-based estimation, and their hybrids. Importance sampling provides unbiasedness but suffers from high variance, while model-based estimators risk bias under model misspecification.\n\nRecent works propose doubly robust estimators for continuous control. These approaches aim to combine the low variance of model-based methods with the unbiasedness of importance sampling by correcting residual model error. Parallel research studies representation learning to reduce covariate shift between behavior and target policies and develops confidence intervals for safety-critical deployment. Our method extends this line by incorporating uncertainty-aware critics with partial identification.\n\nIntroduction\n\nWe present a practical OPE method for continuous-action tasks leveraging distributional critics and action-dependent feature reweighting. The approach yields tighter bounds under limited overlap and mitigates extrapolation error.",
    "reason": "Uses 'Recent works' to refer to prior literature without citing any sources; per rule d, such mentions require citations.",
    "start": 396,
    "end": 465,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent work measures group fairness in top-k recommendation (Ekstrand et al., 2018; Mehrotra et al., 2018), exposure-aware objectives (Singh and Joachims, 2018; Biega et al., 2018), counterfactual evaluation (Schnabel et al., 2016; Bonner and Vasile, 2018), and user-side personalization for fairness (Patro et al., 2020; Burke, 2017). In this paper, we present a ranking framework that balances utility and fairness constraints for multiple stakeholders.",
    "document": "Introduction\n\nRecommender systems shape how users and providers interact on digital platforms. Balancing utility with fairness has become a critical design goal to ensure equitable exposure and user satisfaction. Stakeholders include consumers, producers, and the platform, each with potentially conflicting objectives.\n\nRecent work measures group fairness in top-k recommendation (Ekstrand et al., 2018; Mehrotra et al., 2018), exposure-aware objectives (Singh and Joachims, 2018; Biega et al., 2018), counterfactual evaluation (Schnabel et al., 2016; Bonner and Vasile, 2018), and user-side personalization for fairness (Patro et al., 2020; Burke, 2017). In this paper, we present a ranking framework that balances utility and fairness constraints for multiple stakeholders.\n\nWe empirically evaluate on two marketplaces with both offline and simulated online metrics and provide ablations on constraint strength and calibration. We also analyze the sensitivity of trade-offs under covariate shift.\n",
    "reason": "The span summarizes prior fairness directions and immediately states the authors’ contribution without identifying a specific gap or explicitly relating how the contribution differs, matching definition (b).",
    "start": 321,
    "end": 776,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Large language models trained on source code demonstrate strong performance on code completion and synthesis (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Li et al., 2022). Prompting strategies and few-shot exemplars further improve results across benchmarks (Brown et al., 2020; Liu et al., 2021; Zhao et al., 2021).",
    "document": "Introduction\n\nProgram synthesis with natural language interfaces promises to lower barriers to software creation and maintenance. Recent advances in large language models have made it possible to generate nontrivial code from textual intent, but robustness and controllability remain open challenges.\n\nLarge language models trained on source code demonstrate strong performance on code completion and synthesis (Chen et al., 2021; Austin et al., 2021; Nijkamp et al., 2022; Li et al., 2022). Prompting strategies and few-shot exemplars further improve results across benchmarks (Brown et al., 2020; Liu et al., 2021; Zhao et al., 2021).\n\nWe investigate structured decoding with execution feedback and propose a constraint-guided sampler that incorporates type information and unit test signals at inference time. Our method reduces syntactic and semantic errors without retraining.",
    "reason": "The span summarizes prior achievements and techniques as isolated facts and does not tie them to the authors' perspective, gap, or proposed method, exhibiting lack of synthesis.",
    "start": 302,
    "end": 636,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Miller, 2020,)",
    "document": "Introduction\n\nOpen-domain question answering (QA) relies on effective retrieval and robust readers to extract or generate answers from large corpora (Chen et al., 2017; Karpukhin et al., 2020). Dense retrieval with dual encoders has become a dominant paradigm due to its scalability (Xiong et al., 2021), though it still lags on compositional queries (Min et al., 2021).\n\nRecent work explores iterative retrieval–reading pipelines and knowledge distillation to bridge this gap (Izacard and Grave, 2021; Sachan et al., 2021). Improvements in calibration and uncertainty estimation can further enhance user trust (Miller, 2020,) while reducing spurious patterns via contrastive training (Roberts et al., 2020).\n\nWe propose a modular retriever–reader with answer-aware reranking that improves both accuracy and calibration on standard QA benchmarks.",
    "reason": "Trailing comma inside the parenthetical citation; a comma before the closing parenthesis is a formatting error.",
    "start": 611,
    "end": 626,
    "label": "Format"
  },
  {
    "span": "A variety of reinforcement learning methods have been proposed for traffic signal control, including value-based, actor-critic, and multi-agent coordination strategies (Li et al., 2016; Wei et al., 2019; Chu et al., 2019). Graph-based representations and attention mechanisms capture network-level interactions (Zang et al., 2020; Chen et al., 2020).",
    "document": "Introduction\n\nAdaptive traffic signal control is a canonical application of reinforcement learning (RL), where agents optimize throughput and reduce congestion under stochastic demand. Realistic deployments require robustness to partial observability and scalability across large networks.\n\nA variety of reinforcement learning methods have been proposed for traffic signal control, including value-based, actor-critic, and multi-agent coordination strategies (Li et al., 2016; Wei et al., 2019; Chu et al., 2019). Graph-based representations and attention mechanisms capture network-level interactions (Zang et al., 2020; Chen et al., 2020).\n\nWe introduce MARL-FlowMatch, which aligns local policies via global flow conservation constraints and uses a primal-dual scheme to coordinate intersections without centralized training. A topology-aware representation stabilizes learning under varying network structures.\n\nOn synthetic and real-world networks, MARL-FlowMatch reduces average delay and queue lengths versus strong baselines. We analyze sensitivity to demand surges and ablate coordination penalties.",
    "reason": "The span summarizes prior RL approaches without connecting them to the authors’ method or identifying the specific limitations they address, thus lacking synthesis (criteria a and c).",
    "start": 291,
    "end": 641,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Transformer Vaswani et al. (2017))",
    "document": "Related Work\n\nPretrained language models have transformed sequence modeling by scaling self-attention and data (Devlin et al., 2019; Liu et al., 2019). While LSTMs were once dominant (Hochreiter and Schmidhuber, 1997), the community has largely converged on the self-attention architecture (Transformer Vaswani et al. (2017)) due to its parallelism and long-range capability. Later variants such as RoBERTa (Liu et al., 2019) and DeBERTa (He et al., 2021) improved pretraining objectives and representations.",
    "reason": "Mixed narrative and parenthetical forms with doubled parentheses around the year. Should be “Transformer (Vaswani et al., 2017)” or “the Transformer by Vaswani et al. (2017)”.",
    "start": 290,
    "end": 325,
    "label": "Format"
  },
  {
    "span": "[14]",
    "document": "Introduction\n\nNeural machine translation (NMT) has shifted from recurrent architectures to Transformer models, enabling efficient parallel training and improved accuracy (Vaswani et al., 2017; Ott et al., 2018). Subword tokenization mitigates the open-vocabulary problem and supports multilingual transfer (Sennrich et al., 2016; Conneau et al., 2020). As shown in [14], domain adaptation via continued pretraining can substantially improve in-domain BLEU, but brittle performance persists under severe domain shift (Chu and Wang, 2018; Ramponi and Plank, 2020).\n\nWe study robust adaptation through contrastive pretraining and targeted regularization, evaluating on extreme low-resource and cross-domain benchmarks.",
    "reason": "Wrong citation style: numeric bracket “[14]” used in an author–year context; should cite by author and year (e.g., “as shown in Smith and Lee (2019)”).",
    "start": 365,
    "end": 369,
    "label": "Format"
  },
  {
    "span": "A wide array of methods for explaining NLP predictions has been proposed, including perturbation-based attributions, gradient saliency, and extractive rationales (Smith et al., 2018; Rao and Lin, 2019; Chen et al., 2020; Gupta et al., 2021).",
    "document": "Introduction\n\nLarge language models deployed in high-stakes settings require explanations that are faithful and understandable to human stakeholders. Prior research has framed explanations as post-hoc attributions over input tokens or as rationales that justify a model’s decision.\n\nA wide array of methods for explaining NLP predictions has been proposed, including perturbation-based attributions, gradient saliency, and extractive rationales (Smith et al., 2018; Rao and Lin, 2019; Chen et al., 2020; Gupta et al., 2021). Despite this diversity, evaluations often focus on proxy metrics that correlate weakly with faithfulness.\n\nIn this paper, we introduce RITE, a lightweight explanation protocol that couples counterfactual data augmentation with stability checks to assess whether an explanation is behaviorally faithful. We evaluate RITE on sentiment and entailment benchmarks and release a small suite of diagnostic datasets.",
    "reason": "The sentence lists prior methods and citations without explaining how they relate to the paper’s approach or argument.",
    "start": 283,
    "end": 524,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Chen et al. (2021) introduced a large language model for code completion trained on public repositories. Le et al. (2020) studied automated program repair with neural sequence models. Hendrycks et al. (2021) released the APPS benchmark for code generation and problem solving. Nye et al. (2021) proposed execution-guided decoding to improve program synthesis.",
    "document": "Related Work\n\nProgram synthesis and code generation have seen rapid progress with the advent of large language models (Austin et al., 2021; Wang et al., 2021). Prior efforts explored grammar-constrained decoding (Yin and Neubig, 2017) and neural execution-guided search (Chen et al., 2018), while recent work focuses on scaling model capacity and leveraging curated datasets.\n\nChen et al. (2021) introduced a large language model for code completion trained on public repositories. Le et al. (2020) studied automated program repair with neural sequence models. Hendrycks et al. (2021) released the APPS benchmark for code generation and problem solving. Nye et al. (2021) proposed execution-guided decoding to improve program synthesis.\n\nA parallel track investigates evaluation protocols and security concerns, including data contamination (Magar and Schwartz, 2022), license compliance (Biderman and Raffel, 2022), and robustness to adversarial prompts (Carlini et al., 2023). In contrast, our work targets controllable synthesis with structured constraints and unit-test feedback, building on execution-aware decoding but emphasizing constraint satisfaction under limited hints.",
    "reason": "The sentences list four distinct works with no explicit explanation of how each relates to the others or to a common theme; transitions and relational cues are missing, creating abrupt shifts between cited works.",
    "start": 377,
    "end": 736,
    "label": "Coherence"
  },
  {
    "span": "Brown et al., 2018",
    "document": "Introduction\n\nDomain adaptation for speech recognition has focused on bridging acoustic and lexical shifts between source and target corpora. Following Brown et al., 2018, we adopt feature-space normalization to reduce speaker variability while preserving phonetic distinctions. Prior work has examined adversarial learning to learn domain-invariant representations (Shinohara, 2016; Tzeng et al., 2017) and meta-learning to quickly adapt to new accents (Finn et al., 2017). More recent approaches leverage self-supervised pretraining on massive unlabeled audio to improve downstream robustness (Baevski et al., 2020; Hsu et al., 2021).\n\nWe build on these findings by introducing a transductive alignment objective that couples CTC loss with an unsupervised target-domain clustering criterion, yielding consistent gains across three benchmarks.",
    "reason": "Narrative citation missing proper formatting: should be \"Following Brown et al. (2018),\" not \"Following Brown et al., 2018,\".",
    "start": 152,
    "end": 170,
    "label": "Format"
  },
  {
    "span": "(Garcia et. al., 2016)",
    "document": "Introduction\n\nTask-oriented dialogue systems have progressed from pipeline architectures to end-to-end neural models that jointly learn state tracking and policy optimization. Pretrained language models adapted with supervised signals can reduce annotation cost (Henderson et al., 2019). Memory-augmented approaches (Garcia et. al., 2016) introduce external knowledge slots to improve factual grounding, while reinforcement learning fine-tunes policies for long-term success (Su et al., 2016). Still, robustness to ASR errors and out-of-domain entities remains limited.\n",
    "reason": "Incorrect abbreviation 'et. al.'; should be 'et al.' without a period after 'et'.",
    "start": 316,
    "end": 338,
    "label": "Format"
  },
  {
    "span": "(Nguyen, 2019",
    "document": "Introduction\n\nGraph-based semi-supervised learning propagates labels through the topology of similarity graphs constructed from features (Zhu, 2005). Later variants improved robustness via regularization and random walk formulations (Belkin et al., 2006; Zhou et al., 2004). Recent advances show that consistency regularization can be combined with graph augmentations (Nguyen, 2019 to further reduce label complexity). Complementary approaches incorporate pseudolabeling with confidence thresholds (Lee, 2013) and exploit feature smoothing via graph convolutional networks (Kipf and Welling, 2017).",
    "reason": "Missing closing parenthesis in the parenthetical citation; should be \"(Nguyen, 2019)\".",
    "start": 369,
    "end": 382,
    "label": "Format"
  },
  {
    "span": "Previous work on adversarial attacks has conclusively shown that JPEG compression neutralizes perturbations.",
    "document": "Introduction\nDefending image classifiers from adversarial perturbations remains a pressing challenge. Empirical defenses often rely on input transformations, detection modules, or robust training, yet many methods fail under adaptive threat models.\n\nRelated Work\nImage-space transformations such as bit-depth reduction, denoising, and compression have been proposed to disrupt adversarial patterns. Previous work on adversarial attacks has conclusively shown that JPEG compression neutralizes perturbations. However, compression may introduce artifacts that degrade clean accuracy and can be circumvented by attackers aware of the defense. This paper evaluates compression-based defenses under adaptive attacks and distribution shifts.",
    "reason": "The sentence asserts a definitive result from 'previous work' without citing any sources; it overgeneralizes and lacks references (violates d and b).",
    "start": 399,
    "end": 507,
    "label": "Unsupported_claim"
  },
  {
    "span": "Zhang et al.",
    "document": "Introduction\n\nFew-shot image classification aims to generalize to novel classes from only a handful of labeled examples. Early metric-based approaches established the importance of embedding spaces and inductive biases (Snell et al., 2017; Vinyals et al., 2016). Optimization-based meta-learning further improved adaptation speed by learning initialization points that are amenable to fast fine-tuning (Finn et al., 2017; Nichol et al., 2018). Unlike the episodic training setup proposed by Zhang et al., we focus on transductive adaptation where unlabeled query instances inform the inner-loop updates. Concurrent work has also explored semi-supervised variants that leverage auxiliary unlabeled data (Ren et al., 2018; Bertinetto et al., 2019), but our study isolates the effect of batch composition on stability.",
    "reason": "Narrative citation missing year; should be formatted as \"Zhang et al. (YEAR)\" instead of just the author string without the year.",
    "start": 491,
    "end": 503,
    "label": "Format"
  },
  {
    "span": "Differential privacy mechanisms bound information leakage in model updates via per-step noise and clipping (Abadi et al., 2016; McMahan et al., 2018). Secure aggregation hides individual client updates from the server (Bonawitz et al., 2017). Model pruning and quantization reduce communication costs (Han et al., 2015; Suresh et al., 2017).",
    "document": "Related Work\n\nFederated Learning and Privacy\n\nFederated learning (FL) enables on-device training across a population of clients without centralized raw data collection (McMahan et al., 2017). This paradigm raises challenges in privacy, communication efficiency, personalization, and robustness (Kairouz et al., 2021; Li et al., 2020).\n\nDifferential privacy mechanisms bound information leakage in model updates via per-step noise and clipping (Abadi et al., 2016; McMahan et al., 2018). Secure aggregation hides individual client updates from the server (Bonawitz et al., 2017). Model pruning and quantization reduce communication costs (Han et al., 2015; Suresh et al., 2017).\n\nPersonalization methods incorporate client heterogeneity through multi-task formulations, clustering, or parameter decoupling (Smith et al., 2017; Arivazhagan et al., 2019; Dinh et al., 2020). Robust aggregation mitigates the impact of adversarial or corrupted clients (Blanchard et al., 2017; Yin et al., 2018). While these directions address orthogonal constraints in FL, integrating privacy guarantees with efficient and personalized training remains an open research area.",
    "reason": "There is no transition or explicit link between the privacy-focused sentences and the sudden mention of pruning and quantization; the connection between privacy mechanisms and compression techniques is left unstated, harming coherence across sentences.",
    "start": 336,
    "end": 677,
    "label": "Coherence"
  },
  {
    "span": "(Kumar, 2019, Li, 2020)",
    "document": "Introduction\n\nMulti-task learning (MTL) improves generalization by sharing inductive biases across related tasks (Caruana, 1997; Ruder, 2017). Parameter sharing schemes range from hard sharing to soft, task-specific adapters (Houlsby et al., 2019; Pfeiffer et al., 2021). Negative transfer remains a persistent issue when task gradients conflict (Yu et al., 2020; Chen et al., 2020). Gradient surgery and task reweighting approaches aim to reconcile objectives during joint training (Sener and Koltun, 2018; Yu et al., 2020b). Recent transformer-based MTL frameworks demonstrate strong transfer with carefully designed routing (Tay et al., 2020). We benchmark against dynamic weighting and gradient projection baselines (Kumar, 2019, Li, 2020) and show that our uncertainty-calibrated scheduler yields consistent gains under distribution shifts.\n",
    "reason": "Multiple citations inside one set of parentheses are separated by a comma; they should be separated by a semicolon: \"(Kumar, 2019; Li, 2020)\".",
    "start": 720,
    "end": 743,
    "label": "Format"
  },
  {
    "span": "Propensity score matching balances covariates between treated and control groups (Rosenbaum and Rubin, 1983). Equalized odds constrains disparities in false positive and false negative rates (Hardt et al., 2016). Instrumental variable methods identify causal effects with exogenous variation (Angrist and Imbens, 1995).",
    "document": "Related Work\n\nCausal Inference and Fairness. Understanding and mitigating algorithmic bias benefits from causal frameworks that distinguish correlation from causation and support counterfactual reasoning about interventions (Pearl, 2009; Kusner et al., 2017).\n\nEstimation and Constraints. Estimators for average treatment effects rely on unconfoundedness, overlap, and correct model specification, while fairness interventions impose constraints on predictive behavior to reduce disparities (Imbens and Rubin, 2015; Hardt et al., 2016). Propensity score matching balances covariates between treated and control groups (Rosenbaum and Rubin, 1983). Equalized odds constrains disparities in false positive and false negative rates (Hardt et al., 2016). Instrumental variable methods identify causal effects with exogenous variation (Angrist and Imbens, 1995). We consider policy learning under fairness and identification constraints.\n\nPolicy Learning under Constraints. Recent work optimizes decision policies subject to fairness or welfare constraints using bandit and off-policy evaluation techniques (Joseph et al., 2016; Dimitrakakis et al., 2017). Our approach integrates identification-robust estimators with constraint-aware optimization.",
    "reason": "The span strings together propensity score matching, equalized odds, and instrumental variables without transitions or explanation of how they relate, causing abrupt shifts and unclear connections.",
    "start": 537,
    "end": 856,
    "label": "Coherence"
  },
  {
    "span": "It is well known that gradient inversion attacks can fully reconstruct private images from a single batch.",
    "document": "Related Work\n\nFederated learning enables collaborative model training without centralized data collection, offering potential privacy benefits for sensitive domains such as healthcare and finance (Kairouz et al., 2021). However, sharing model updates can leak information about the underlying data through inference attacks.\n\nIt is well known that gradient inversion attacks can fully reconstruct private images from a single batch. While mitigation strategies such as gradient clipping, noise addition, and secure aggregation have been proposed, their effectiveness depends on the threat model, batch size, and the availability of auxiliary information.\n\nWe study the interplay between optimizer dynamics and attack success in practical training regimes. Our analysis reveals that certain momentum schedules amplify leakage, and we introduce an adaptive regularizer that preserves utility while substantially reducing reconstructability under constrained adversaries.",
    "reason": "The sweeping claim about gradient inversion capabilities lacks citations to the original attack papers or empirical studies.",
    "start": 326,
    "end": 432,
    "label": "Unsupported_claim"
  },
  {
    "span": "Zhao et al.",
    "document": "Related Work\n\nGraph neural networks (GNNs) generalize deep learning to relational structures. Early message-passing approaches formalize neighborhood aggregation and permutation invariance (Gilmer et al., 2017; Kipf and Welling, 2017). Zhao et al. introduce temporal extensions that capture evolving topologies. Subsequent work proposes scalable sampling (Hamilton et al., 2017) and attention mechanisms (Velickovic et al., 2018). For heterogeneous graphs, metapath-based aggregation has proven effective (Hu et al., 2020). Our approach differs by integrating structural sparsity priors with self-supervision, akin to methods surveyed by Wu et al. (2020).",
    "reason": "Narrative citation missing year; should be 'Zhao et al. (YEAR)'.",
    "start": 236,
    "end": 247,
    "label": "Format"
  },
  {
    "span": "The 2021 shared task on code-switching adopted a macro-F1 leaderboard and prohibited external resources.",
    "document": "Related Work\n\nCode-switching presents unique challenges to tokenization, tagging, and language identification due to lexical borrowing and rapid alternation between languages. Shared tasks have played a crucial role in standardizing evaluation and catalyzing progress by releasing curated datasets and baselines.\n\nThe 2021 shared task on code-switching adopted a macro-F1 leaderboard and prohibited external resources. Subsequent editions introduced multilingual settings and examined domain-shift robustness through held-out social platforms. Beyond classification, recent efforts emphasize token-level and span-level evaluation to capture fine-grained errors.\n\nOur work builds on these initiatives by providing a unified evaluation suite across levels of granularity and analyzing the impact of lexicon normalization and subword segmentation on performance.",
    "reason": "This sentence references a specific shared task and its policies without citing the task report or website (violates rule a).",
    "start": 314,
    "end": 418,
    "label": "Unsupported_claim"
  },
  {
    "span": "(ResNet He et al. (2016))",
    "document": "Related Work\n\nVision backbones such as ResNet and EfficientNet have become standard for image understanding (Tan and Le, 2019). Transfer learning from large-scale pretraining provides strong initial features for downstream tasks (Kornblith et al., 2019). Recently, vision transformers rival convolutional networks by leveraging global self-attention (Dosovitskiy et al., 2021; Liu et al., 2021).\n\nDetection frameworks evolve from two-stage proposals to single-shot and anchor-free designs (Ren et al., 2015; Redmon and Farhadi, 2018; Tian et al., 2019). For fine-grained recognition, metric learning and part discovery improve robustness to intra-class variability (Wang et al., 2019; Sun et al., 2018). We initialize our model with a standard residual network (ResNet He et al. (2016)) to enable fair comparison with prior detectors.\n\nOur approach augments backbone features with localized attention maps that emphasize discriminative regions, improving both accuracy and calibration.",
    "reason": "Nested parentheses around a narrative mention; model name and citation should be formatted as “ResNet (He et al., 2016)” rather than “(ResNet He et al. (2016))”.",
    "start": 761,
    "end": 786,
    "label": "Format"
  },
  {
    "span": "In industry, more than 70% of streaming recommendations are generated by bandit algorithms.",
    "document": "Introduction\n\nLarge-scale recommender systems power content discovery in e-commerce, video, and music platforms (Covington et al., 2016; Zhou et al., 2018). Exploration-exploitation trade-offs are critical when user preferences evolve and items rapidly decay, motivating contextual and combinatorial bandit methods (Li et al., 2010; Chen et al., 2013). Recent deep bandit approaches leverage representation learning to generalize across users and items (Riquelme et al., 2018; Zhou et al., 2020).\n\nIn industry, more than 70% of streaming recommendations are generated by bandit algorithms. Despite their ubiquity, deployment remains challenging due to delayed feedback, slate effects, and non-stationarity. To address these issues, we propose a slate-aware, distributionally robust bandit that explicitly models position bias and temporal drift using adversarial risk minimization.\n\nWe evaluate on a large-scale public benchmark and a production A/B test proxy, showing improved long-horizon engagement and better new-item exposure compared to strong baselines.",
    "reason": "Presents a specific industry statistic without any supporting citation or evidence.",
    "start": 498,
    "end": 589,
    "label": "Unsupported_claim"
  },
  {
    "span": "All prior TOD pretraining frameworks rely on fully annotated belief states and delexicalized responses.",
    "document": "Related Work\n\nTask-Oriented Dialogue (TOD) systems traditionally decompose the pipeline into belief state tracking, policy learning, and response generation, with neural models increasingly unifying these steps. Prior studies have explored curriculum learning, slot carryover mechanisms, and schema generalization for zero-shot domains (e.g., schema linking and promptable interfaces). All prior TOD pretraining frameworks rely on fully annotated belief states and delexicalized responses. This constraint limits the scale of usable corpora and hampers adaptation to domains where such labels are scarce. Recent advances in semi-supervised learning and weak supervision suggest promising avenues to alleviate annotation bottlenecks, but their integration into unified TOD modeling remains limited. Our work proposes a multi-task pretraining objective that avoids explicit belief state supervision while leveraging abundant unlabeled dialogues.",
    "reason": "Asserts a blanket characterization of all prior frameworks without citing any specific studies to substantiate the claim.",
    "start": 386,
    "end": 489,
    "label": "Unsupported_claim"
  },
  {
    "span": "TRADES balances robustness and accuracy with a regularization term (Zhang et al., 2019). Vision Transformers rely on patch embeddings and global attention (Dosovitskiy et al., 2021).",
    "document": "Introduction\n\nAdversarial robustness in image classification has largely been studied in convolutional architectures, yet transformer-based models are now state of the art across many vision tasks. Understanding how training objectives and architectures interact is essential for reliable deployment.\n\nAdversarial training methods such as PGD and TRADES improve worst-case performance by including adversarially perturbed examples during learning (Madry et al., 2018; Zhang et al., 2019). Certified defenses offer robustness guarantees under bounded perturbations but often compromise clean accuracy (Cohen et al., 2019; Salman et al., 2019). TRADES balances robustness and accuracy with a regularization term (Zhang et al., 2019). Vision Transformers rely on patch embeddings and global attention (Dosovitskiy et al., 2021). Recent analyses suggest attention heads can localize salient regions but remain brittle under distribution shifts (Bhojanapalli et al., 2021; Naseer et al., 2021).\n\nWe propose a robustness-aware token mixing scheme that aligns intermediate representations of clean and adversarial views to stabilize attention maps while preserving clean accuracy.",
    "reason": "The two sentences place an objective (TRADES) next to a description of ViT architecture without linking why the architectural detail matters to TRADES or to robustness, producing an unexplained jump.",
    "start": 643,
    "end": 825,
    "label": "Coherence"
  },
  {
    "span": "(Lee and Kim, 2017",
    "document": "Introduction\n\nPolicy optimization methods underpin modern reinforcement learning. REINFORCE provided an early stochastic gradient estimator (Williams, 1992), and actor–critic methods improved sample efficiency (Konda and Tsitsiklis, 2000). Several works explored trust-region optimization (Schulman et al., 2015) and proximal updates (Schulman et al., 2017). Off-policy corrections for recurrent policies were studied in (Lee and Kim, 2017 to stabilize credit assignment. Recent algorithms combine entropy regularization (Haarnoja et al., 2018) with distributional critics (Bellemare et al., 2017).",
    "reason": "Missing closing parenthesis in a parenthetical citation; it should be '(Lee and Kim, 2017)'.",
    "start": 421,
    "end": 439,
    "label": "Format"
  },
  {
    "span": "state-of-the-art extractive QA models now exceed 90 F1 on SQuAD.",
    "document": "Introduction\n\nMachine reading comprehension has rapidly advanced with neural architectures and large-scale supervision. Despite these advances, building systems that generalize across domains and question types remains challenging. Work on span-extractive models has particularly benefited from large annotated corpora and transfer learning from pretrained language models. However, state-of-the-art extractive QA models now exceed 90 F1 on SQuAD. This impressive score obscures persistent weaknesses in multi-hop reasoning, numerical understanding, and robustness to adversarial perturbations. In education-oriented settings, moreover, inference over narrative structure and implicit causal relations is still underexplored. To address these gaps, we introduce a dataset targeting narrative inference and fine-grained question types, and we benchmark a suite of encoder-decoder and instruction-tuned baselines. Our results highlight a marked drop in performance on questions requiring causal and temporal reasoning, motivating future work on structured knowledge integration.",
    "reason": "Claims a performance milestone on a specific dataset and references a widely known dataset at first mention without citation or evidence.",
    "start": 383,
    "end": 447,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent efforts adapt Vision Transformers to medical imaging (Chen et al., 2021; He et al., 2021; Wang et al., 2022). In this work, we fine-tune a Vision Transformer on histopathology patches.",
    "document": "Introduction\n\nVision Transformers (ViTs) have demonstrated strong performance on large-scale image classification due to their ability to capture long-range dependencies via self-attention. Medical imaging analysis, particularly in histopathology, presents unique challenges such as extreme resolution, label scarcity, and strong inter-patient variability.\n\nRecent efforts adapt Vision Transformers to medical imaging (Chen et al., 2021; He et al., 2021; Wang et al., 2022). In this work, we fine-tune a Vision Transformer on histopathology patches. We further evaluate on standard public datasets and report accuracy and AUC under various magnifications.\n\nWhile our experiments focus on slide-level diagnostic prediction from patch embeddings, we also examine the effect of pretraining duration and data augmentations on downstream metrics.",
    "reason": "The span summarizes prior work and immediately states the authors' contribution without identifying any specific gap or limitation that motivates their method (criterion b and c).",
    "start": 358,
    "end": 549,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Differentially private learning for text employs gradient perturbation, clipping, and noise calibration in centralized and federated setups, with applications to classification and generation (Abadi et al., 2016; McMahan et al., 2018; Yu et al., 2021; Li et al., 2022). Auditing and membership inference studies probe privacy leakage in language models (Shokri et al., 2017; Carlini et al., 2021; Thakkar et al., 2020).",
    "document": "Related Work\n\nProtecting user text data requires training and deployment protocols that mitigate leakage while preserving utility. In practical systems, token frequency skew and long-tail vocabulary complicate privacy accounting and degrade model quality. We analyze privacy–utility trade-offs under heavy-tailed distributions.\n\nDifferentially private learning for text employs gradient perturbation, clipping, and noise calibration in centralized and federated setups, with applications to classification and generation (Abadi et al., 2016; McMahan et al., 2018; Yu et al., 2021; Li et al., 2022). Auditing and membership inference studies probe privacy leakage in language models (Shokri et al., 2017; Carlini et al., 2021; Thakkar et al., 2020).\n\nOur contribution is a frequency-aware clipping and noise allocation scheme that improves performance on rare tokens while maintaining formal privacy guarantees.",
    "reason": "The span lists DP techniques and auditing works without relating them to the heavy-tailed vocabulary setting or clarifying what gap remains, thus lacking synthesis with the paper’s motivation (criteria a and c).",
    "start": 329,
    "end": 748,
    "label": "Lacks_synthesis"
  },
  {
    "span": "[see Smith et al., 2020)",
    "document": "Introduction\n\nDomain adaptation for dialogue systems remains challenging due to shifting intents and slot distributions. Prior work emphasizes the role of meta-learning and regularization to prevent catastrophic forgetting [see Smith et al., 2020) and demonstrates that adapter-based tuning can preserve general conversational skills while specializing to new domains (Lin and Perez, 2021; Zhao et al., 2022). Building on these insights, we propose a retrieval-augmented adaptation framework that conditions generation on domain exemplars.",
    "reason": "Mismatched bracket/parenthesis in a citation: opening with '[' and closing with ')' is a formatting error. It should be consistently bracketed, e.g., \"(see Smith et al., 2020)\" or \"[see Smith et al., 2020]\" depending on the chosen style.",
    "start": 223,
    "end": 247,
    "label": "Format"
  },
  {
    "span": "Classical anomaly detection for time series includes ARIMA residual checks and change-point methods, while deep approaches leverage autoencoders, temporal CNNs, and Transformers (Box et al., 2015; Aminikhanghahi and Cook, 2017; Malhotra et al., 2016; Zhou et al., 2021; Wu et al., 2021).",
    "document": "Related Work\n\nDetecting anomalies in multivariate time series is critical for monitoring complex systems. Challenges include nonstationarity, correlations across channels, and scarce labeled anomalies.\n\nClassical anomaly detection for time series includes ARIMA residual checks and change-point methods, while deep approaches leverage autoencoders, temporal CNNs, and Transformers (Box et al., 2015; Aminikhanghahi and Cook, 2017; Malhotra et al., 2016; Zhou et al., 2021; Wu et al., 2021).\n\nDespite impressive accuracy on curated benchmarks, many methods underperform under covariate shift and missing data. We propose a probabilistic imputation-aware detector with uncertainty calibration.",
    "reason": "Summarizes prior methods without indicating how they compare to or motivate the proposed method in that passage (criteria a and c).",
    "start": 203,
    "end": 490,
    "label": "Lacks_synthesis"
  },
  {
    "span": "2019, Zhang et al.",
    "document": "Related Work\n\nDomain generalization methods seek invariances that transfer across unseen environments (Gulrajani and Lopez-Paz, 2021). 2019, Zhang et al. introduced a meta-learning approach that simulates domain shift during training, and follow-up work studies feature disentanglement to separate causal from spurious factors (Arjovsky et al., 2019; Sagawa et al., 2020). Recent benchmarks emphasize robustness under subpopulation shift (Koh et al., 2021). We contribute a lightweight, augmentation-agnostic regularizer that improves worst-group accuracy.",
    "reason": "Narrative citation has year preceding authors; should be formatted as 'Zhang et al. (2019)'.",
    "start": 135,
    "end": 153,
    "label": "Format"
  },
  {
    "span": "(Chen et al., 2019,; Park, 2020)",
    "document": "Introduction\n\nData augmentation has become central to improving robustness and sample efficiency across vision and language tasks (Shorten and Khoshgoftaar, 2019; Feng et al., 2021). In NLP, token- and span-level perturbations, as well as back-translation, have been widely adopted to increase lexical diversity (Sennrich et al., 2016; Wei and Zou, 2019). More recent work uses generative models to synthesize contextually coherent examples tailored to target classes (Kumar et al., 2020). While simple random edits can harm semantics, task-aware policies improve label preservation (Morris et al., 2020). Prior studies report gains on text classification and NER (Chen et al., 2019,; Park, 2020), yet systematic evaluations under distribution shift remain scarce. We propose a controllable augmentation framework that conditions on both label and domain attributes, yielding diverse but faithful variants that enhance out-of-domain generalization.\n",
    "reason": "Incorrect punctuation inside a parenthetical citation list; stray comma before semicolon. Should be '(Chen et al., 2019; Park, 2020)'.",
    "start": 664,
    "end": 696,
    "label": "Format"
  },
  {
    "span": "(Li et al., 2020",
    "document": "Introduction\n\nTransformers have rapidly advanced visual recognition by replacing convolutions with global self-attention (Dosovitskiy et al., 2021; Touvron et al., 2021). While early CNNs dominated image classification (Krizhevsky et al., 2012), recent works explore hierarchical self-attention and token aggregation (Vaswani et al., 2017; Li et al., 2020 across large-scale benchmarks to improve data efficiency and inductive bias. However, scaling alone does not fully address robustness to distribution shift (Taori et al., 2020), motivating architectural priors that balance global context with locality.",
    "reason": "Missing closing parenthesis in the parenthetical citation.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "The 2019 TUM RGB-D benchmark introduced dynamic-scene sequences to stress-test SLAM.",
    "document": "Related Work\n\nSimultaneous Localization and Mapping (SLAM) has progressed from feature-based pipelines to tightly coupled visual-inertial and learning-augmented methods. Public benchmarks have been crucial for measuring progress and diagnosing failure modes under viewpoint, illumination, and texture variation. The 2019 TUM RGB-D benchmark introduced dynamic-scene sequences to stress-test SLAM. Subsequent datasets expanded to outdoor and adverse-weather settings, while differentiable rendering has opened new avenues for end-to-end optimization. Our evaluation emphasizes dynamic objects and occlusions, comparing classical keypoint pipelines with dense and neural field methods.",
    "reason": "References a specific benchmark release and its contribution without providing a citation to the benchmark or associated paper.",
    "start": 312,
    "end": 396,
    "label": "Unsupported_claim"
  },
  {
    "span": "Knowledge distillation has been extensively studied in NLP for compressing large pre-trained models (Hinton et al., 2015; Sanh et al., 2019; Jiao et al., 2020; Sun et al., 2020; Wang et al., 2021).",
    "document": "Related Work\n\nModern language models deliver strong accuracy but are costly at inference time, prompting research on model compression to reduce latency and energy consumption while preserving quality.\n\nKnowledge distillation has been extensively studied in NLP for compressing large pre-trained models (Hinton et al., 2015; Sanh et al., 2019; Jiao et al., 2020; Sun et al., 2020; Wang et al., 2021).\n\nOther compression techniques include pruning, quantization, and low-rank factorization (Han et al., 2016; Shen et al., 2020; Dettmers et al., 2022). We later describe a curriculum that aligns intermediate representations for multi-task transfer.\n",
    "reason": "Violates (a) and (ii): cites prior work without any explanation of their approaches or their relation to the current work, offering no synthesis or connection.",
    "start": 203,
    "end": 400,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Miller et. al. (2018)",
    "document": "Related Work\n\nForecasting methods range from classical statistical models to deep architectures that capture seasonality and long-range dependencies (Hyndman and Athanasopoulos, 2018; Salinas et al., 2020). Miller et. al. (2018) propose a sequence-to-sequence forecaster with attention to improve multi-horizon accuracy under covariate shift. Recent advances explore probabilistic forecasting with normalizing flows and quantile losses (Oreshkin et al., 2019; Rangapuram et al., 2018). We contribute a distributionally robust objective that adapts to temporal nonstationarity using adversarial perturbations of exogenous features.",
    "reason": "Misspelled 'et al.'; correct form is 'et al.' without an extra period after 'et', so it should be 'Miller et al. (2018)'.",
    "start": 207,
    "end": 228,
    "label": "Format"
  },
  {
    "span": "(Jones et al. 2016)",
    "document": "Introduction\n\nKnowledge distillation transfers inductive biases from a high-capacity teacher to a compact student (Hinton et al., 2015). Sequence-level distillation improves translation quality while reducing latency (Kim and Rush, 2016). For speech, temperature scaling and feature matching have been explored (Park et al., 2019). We adopt a consistency loss across augmentations, following the agreement objective in (Jones et al. 2016), to reduce overconfidence on noisy inputs.\n\nRelated Work\n\nRecent work studies teacher-free distillation via self-training (Zhang et al., 2021), which we compare against in our experiments.",
    "reason": "Missing comma before the year in an author–year parenthetical citation. It should be “(Jones et al., 2016)”.",
    "start": 419,
    "end": 438,
    "label": "Format"
  },
  {
    "span": "Adversarial approaches align feature distributions across domains using discriminators at the pixel or feature level (Tsai et al., 2018; Hoffman et al., 2018). Self-training strategies generate pseudo-labels to refine the target model iteratively (Zou et al., 2019; Zhang et al., 2021). In this work, we propose a novel cross-view consistency loss for urban-scene adaptation.",
    "document": "Introduction\n\nUnsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain with distributional shift. Urban-scene datasets present substantial challenges due to changes in weather, sensors, and geographic regions, leading to label noise and domain mismatch.\n\nAdversarial approaches align feature distributions across domains using discriminators at the pixel or feature level (Tsai et al., 2018; Hoffman et al., 2018). Self-training strategies generate pseudo-labels to refine the target model iteratively (Zou et al., 2019; Zhang et al., 2021). In this work, we propose a novel cross-view consistency loss for urban-scene adaptation.\n\nRecent works also explore style transfer and frequency-space alignment to reduce low-level appearance gaps (Yang and Soatto, 2020; Yue et al., 2019). Transformer-based segmenters demonstrate improved global context modeling but remain sensitive to domain shift (Strudel et al., 2021; Pan et al., 2022).",
    "reason": "The span summarizes two major UDA paradigms and immediately presents the authors' method without articulating what specific gap in these paradigms motivates the proposed consistency loss, failing to synthesize prior work with their contribution.",
    "start": 347,
    "end": 722,
    "label": "Lacks_synthesis"
  },
  {
    "span": "(Johnson and Zhang 2015)",
    "document": "Introduction\n\nConvolutional architectures for text classification offer competitive speed and accuracy with minimal feature engineering. Early models rely on n-gram embeddings and max-pooling (Kim, 2014), while deeper stacks improve capacity (Conneau et al., 2017). Convolutional baselines (Johnson and Zhang 2015) highlight the benefits of region embeddings for document categorization. We revisit these designs under modern pretraining and regularization schemes to assess their continued relevance.",
    "reason": "Missing comma between author names and the year in the parenthetical citation; should be “(Johnson and Zhang, 2015)”.",
    "start": 290,
    "end": 314,
    "label": "Format"
  },
  {
    "span": "MIMIC-III",
    "document": "Related Work\n\nClinical natural language processing (NLP) focuses on extracting structured information from free-text clinical notes, including problems, medications, and procedures. Many studies rely on MIMIC-III for development and evaluation of entity recognition, relation extraction, and phenotyping systems due to its scale and diversity. Prior work has explored dictionary-based taggers, feature-rich CRFs, and more recently contextual encoders adapted to clinical terminology. Our method builds on these directions by integrating structured knowledge with contextualized representations.",
    "reason": "First mention of a specific clinical dataset appears without citation (rule a).",
    "start": 203,
    "end": 212,
    "label": "Unsupported_claim"
  },
  {
    "span": "Gilmer et al. (2017) proposed message passing neural networks for learning molecular representations. Ramakrishnan et al. (2014) released the QM9 dataset for small molecules. Schwaller et al. (2019) trained sequence-to-sequence transformers over SMILES to model reactions.",
    "document": "Related Work\n\nMolecular property prediction has benefited from graph-based learning and sequence modeling. A line of studies focuses on learning expressive representations from molecular graphs, while another explores SMILES strings as sequences.\n\nGilmer et al. (2017) proposed message passing neural networks for learning molecular representations. Ramakrishnan et al. (2014) released the QM9 dataset for small molecules. Schwaller et al. (2019) trained sequence-to-sequence transformers over SMILES to model reactions. Other variants investigate attention over bonds and atoms (Velickovic et al., 2018; Ying et al., 2021) and pretraining on large compound libraries (Hu et al., 2020).\n\nActive learning and uncertainty estimation have also been studied for molecular design (Jin et al., 2018; Swann et al., 2020), but these lines remain orthogonal to data-efficient fine-tuning considered in this work.",
    "reason": "The span lists a graph-based method, then abruptly mentions a dataset, then a sequence model without explaining how the dataset connects to either method or why these works are juxtaposed. There are no transitions clarifying relationships.",
    "start": 248,
    "end": 520,
    "label": "Coherence"
  },
  {
    "span": "Baker and Inventado (2014) surveyed educational data mining. Kizilcec et al. (2013) analyzed MOOC dropout patterns. Piech et al. (2015) proposed deep knowledge tracing for mastery modeling. Waddington et al. (2021) studied fairness in automated grading. A/B testing is used to evaluate interventions at scale (Kohavi et al., 2014).",
    "document": "Related Work\n\nLearning Analytics and Student Modeling\n\nResearch in learning analytics spans predictive modeling of engagement, personalization of content, and evaluation of interventions. Our contribution is a calibration method for early-warning systems that accounts for cohort shift.\n\nBaker and Inventado (2014) surveyed educational data mining. Kizilcec et al. (2013) analyzed MOOC dropout patterns. Piech et al. (2015) proposed deep knowledge tracing for mastery modeling. Waddington et al. (2021) studied fairness in automated grading. A/B testing is used to evaluate interventions at scale (Kohavi et al., 2014).\n\nWe apply conformal risk control to ensure stable alert thresholds across courses and semesters.",
    "reason": "The sequence moves from surveys to MOOC dropout, knowledge tracing, grading fairness, and A/B testing without transitions; relationships among these strands are not clarified.",
    "start": 288,
    "end": 619,
    "label": "Coherence"
  },
  {
    "span": "Nguyen et al., (2019)",
    "document": "Introduction\n\nBias in language models manifests across demographic axes and can be amplified by pretraining corpora (Bolukbasi et al., 2016; Caliskan et al., 2017; Bender et al., 2021). Mitigation approaches include counterfactual data augmentation, debiasing objectives, and constrained decoding (Zhao et al., 2018; Ma et al., 2020; Liu et al., 2021). Nguyen et al., (2019) introduce a bias calibration framework that reweights predictions based on protected-attribute proxies, yet their method relies on sensitive attribute inference at test time.\n\nWe propose a representation-level perturbation that enforces invariance without explicit attribute access, combining causal probing with adversarial alignment. Our evaluation spans toxicity classification, coreference, and occupation prediction, measuring both utility and fairness with group and individual metrics.",
    "reason": "Incorrect comma before the parenthetical year in a narrative citation; it should be \"Nguyen et al. (2019)\" without the comma.",
    "start": 353,
    "end": 374,
    "label": "Format"
  },
  {
    "span": "(Garcia et al. 2020)",
    "document": "Related Work\n\nDynamic malware analysis enriches static signatures with behavioral traces to improve detection robustness (Rizzo et al., 2019). Hybrid systems combine API call sequences with control-flow graphs to capture evasive patterns (Kolbitsch et al., 2009; Pascanu et al., 2015). Recent benchmarks emphasize generalization to novel families (Anderson and Roth, 2018). Performance improves with representation learning (Garcia et al. 2020) but remains sensitive to concept drift in the wild.",
    "reason": "Missing comma before the year; should be '(Garcia et al., 2020)'.",
    "start": 424,
    "end": 444,
    "label": "Format"
  },
  {
    "span": "Garcia et al. 3",
    "document": "Introduction\n\nIntelligent tutoring systems (ITS) personalize instruction by modeling learner knowledge and selecting appropriate pedagogical actions (Koedinger et al., 2013). Knowledge tracing has evolved from Bayesian models to deep sequence architectures capturing complex temporal patterns (Corbett and Anderson, 1995; Piech et al., 2015). Garcia et al. 3 demonstrate that incorporating item semantics into the student model can reduce cold-start errors when new questions are introduced. Concurrent work explores counterfactual evaluation of tutoring policies using logged interaction data (Thomas and Brunskill, 2016; Mandel et al., 2017). We extend semantic-aware tracing with explainable feedback designed to surface concept-level misconceptions.",
    "reason": "Wrong use of footnote-style numbering with author-year in-text citation; should include a year or be formatted as a proper footnote, e.g., “Garcia et al. (2019)” or a numbered note.",
    "start": 343,
    "end": 358,
    "label": "Format"
  },
  {
    "span": "(Smith et al., 2018; Lee, 2020",
    "document": "Introduction\n\nGraph representation learning has advanced rapidly with message-passing neural networks enabling end-to-end training on relational data (Kipf and Welling, 2017; Hamilton et al., 2017). Despite strong results on benchmarks, concerns about calibration and robustness remain (Guo et al., 2017; Ovadia et al., 2019). Prior studies on semi-supervised node classification emphasize efficient propagation schemes (Zhu et al., 2003) and regularization with topology-aware losses (You et al., 2020). Our work departs from these by introducing an adaptive temperature that modulates neighborhood aggregation across layers. Similar ideas have been explored in metric learning for graphs (Smith et al., 2018; Lee, 2020 where contrastive losses regularize embeddings, but existing approaches do not consider distribution shift between training and inference.",
    "reason": "Missing closing parenthesis in a parenthetical citation.",
    "start": 690,
    "end": 720,
    "label": "Format"
  },
  {
    "span": "The MovieLens 1M dataset is widely used for implicit feedback evaluation.",
    "document": "Introduction\n\nRecommender systems for implicit feedback rely on learning from positive-only interactions such as clicks and watch events, typically through negative sampling or pairwise ranking objectives (Rendle et al., 2009; He et al., 2017). Matrix factorization remains a strong baseline, while deep models capture complex user–item patterns and side information (Koren et al., 2009; Covington et al., 2016).\n\nThe MovieLens 1M dataset is widely used for implicit feedback evaluation. However, evaluation protocols vary substantially across works, complicating fair comparison of top-N recommenders. We revisit negative sampling strategies and propose a calibrated evaluation pipeline that standardizes candidate pools and temporal splits.\n\nOur contributions include a principled sampling scheme, a temporal holdout protocol, and a comprehensive benchmark across classical and deep models.",
    "reason": "First mention of a specific dataset and its typical usage lacks a citation to the dataset or supporting studies (violates rule a).",
    "start": 414,
    "end": 487,
    "label": "Unsupported_claim"
  },
  {
    "span": "Klein et al. 1",
    "document": "Related Work\n\nAlgorithmic fairness research spans pre-, in-, and post-processing interventions addressing demographic parity, equalized odds, and calibration trade-offs (Hardt et al., 2016; Kleinberg et al., 2017). Klein et al. 1 argue that metric selection should be context-dependent, yet their analysis assumes stable base rates across subpopulations. Subsequent studies formalize the incompatibilities among fairness criteria and highlight data-driven limitations under label noise (Chouldechova, 2017; Corbett-Davies and Goel, 2018). Recent methods incorporate causal structure to avoid enforcing spurious constraints (Kusner et al., 2017; Kilbertus et al., 2020). We extend this line by proposing counterfactual data augmentation that preserves identifiability while improving subgroup robustness.\n\nIntroduction\n\nPractical deployment necessitates auditing pipelines that include data collection, model training, and monitoring (Raji et al., 2020). Our framework integrates uncertainty estimates with fairness diagnostics to flag distribution shift early in production.",
    "reason": "Wrong use of footnotes in a citation; includes a footnote marker without a year. Should be a proper citation like \"Klein et al. (2019)\" or a correctly formatted footnote.",
    "start": 215,
    "end": 229,
    "label": "Format"
  },
  {
    "span": "(Baker et al.) 2015",
    "document": "Related Work\n\nAnomaly detection algorithms span density estimation, reconstruction error modeling, and one-class classification (Breunig et al., 2000; Schölkopf et al., 2001; Zong et al., 2018). Deep generative models and self-supervised pretext tasks have recently advanced performance on image and time-series data (An and Cho, 2015; Golan and El-Yaniv, 2018; Bergman and Hoshen, 2020).\n\nContextual anomalies that depend on auxiliary covariates pose additional challenges, often requiring conditional models or attention mechanisms (Li et al., 2020; Ruff et al., 2021). According to (Baker et al.) 2015, incorporating domain constraints during training improves interpretability and reduces false alarms, but may restrict flexibility in nonstationary settings. Our method injects constraints only at inference via a learned verifier, avoiding training-time brittleness (Pang et al., 2021; Tack et al., 2020).\n\nWe evaluate across benchmarks with controlled context shifts and provide ablations on verifier strength and coverage guarantees (Angelopoulos and Bates, 2022; Romano et al., 2020).",
    "reason": "Year placed outside the parenthetical citation; should be 'According to Baker et al. (2015), ...' or '(Baker et al., 2015)'.",
    "start": 585,
    "end": 604,
    "label": "Format"
  },
  {
    "span": "Graph convolutional networks have been widely applied to molecular property prediction (Duvenaud et al., 2015). Social recommendation leverages user-item graphs to model interactions (Fan et al., 2019). Message passing neural networks capture long-range dependencies through iterative updates (Gilmer et al., 2017). Heterogeneous graph transformers introduce relation-specific attention (Hu et al., 2020).",
    "document": "Related Work\n\nGraph-based learning has become a cornerstone for modeling structured domains, including chemistry and materials, where atoms and bonds can be naturally represented as nodes and edges. Predicting molecular properties benefits from architectures that respect graph symmetries and support message passing.\n\nGraph convolutional networks have been widely applied to molecular property prediction (Duvenaud et al., 2015). Social recommendation leverages user-item graphs to model interactions (Fan et al., 2019). Message passing neural networks capture long-range dependencies through iterative updates (Gilmer et al., 2017). Heterogeneous graph transformers introduce relation-specific attention (Hu et al., 2020).\n\nWe focus on chemically informed message passing and uncertainty estimation for low-data molecular regimes.",
    "reason": "The sentences jump between molecular prediction and social recommendation without articulating a relationship, and they list additional GNN variants without explaining how they connect to each other or to the molecular context.",
    "start": 319,
    "end": 724,
    "label": "Coherence"
  },
  {
    "span": "the FiD model remains the de facto standard for generative readers.",
    "document": "Related Work\n\nOpen-Domain Question Answering\n\nRetriever-reader pipelines have become the dominant approach for ODQA, where a retriever first surfaces candidate passages and a reader produces an answer conditioned on those passages. Early neural readers were based on bidirectional RNNs with attention, while later work leveraged large pre-trained encoders and decoders to improve answer quality. Generative readers, in particular, have shown strong performance by synthesizing answers from multiple retrieved sources and by better aggregating evidence across passages.\n\nDespite the diversity of recent sequence-to-sequence architectures, the FiD model remains the de facto standard for generative readers. Extensions that distill knowledge from readers into retrievers, as well as variants that improve passage selection, have further popularized this family of methods.\n\nOur work differs from prior ODQA systems by focusing on controllable attribution during generation, aiming to ensure that each generated token is explicitly supported by one or more retrieved passages.",
    "reason": "Claims a prevailing standard (“de facto standard”) and references a specific prior model family without providing any citation or evidence at first mention.",
    "start": 638,
    "end": 705,
    "label": "Unsupported_claim"
  },
  {
    "span": "Multimodal fake news detection leverages textual cues with transformers, visual features from CNNs, and fusion modules to capture cross-modal inconsistencies (Wang et al., 2018; Khattar et al., 2019; Jin et al., 2020; Zhou et al., 2020). Knowledge graphs and social context have also been incorporated to improve robustness (Shu et al., 2019; Cui et al., 2020).",
    "document": "Related Work\n\nText-only detection. Early misinformation detection models focused on linguistic signals such as style, stance, and factual consistency, often using traditional machine learning or pretrained language models to flag deceptive content based on textual evidence alone.\n\nMultimodal approaches. Multimodal fake news detection leverages textual cues with transformers, visual features from CNNs, and fusion modules to capture cross-modal inconsistencies (Wang et al., 2018; Khattar et al., 2019; Jin et al., 2020; Zhou et al., 2020). Knowledge graphs and social context have also been incorporated to improve robustness (Shu et al., 2019; Cui et al., 2020).\n\nExplainability and bias. Recent efforts seek to explain model decisions and mitigate demographic or topical biases by grounding predictions in evidence and calibrating confidence, enabling safer deployment in real-world settings.\n\nOur work introduces a causality-aware fusion mechanism that disentangles stylistic correlations from causal content cues, yielding better out-of-domain detection and more faithful rationales.",
    "reason": "The span lists modalities, models, and enhancements without explaining how they compare to or motivate the proposed causality-aware approach, leaving the gap unstated.",
    "start": 305,
    "end": 666,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Chen et al. 1",
    "document": "Related Work\n\nCrowdsourced argument mining has leveraged weak supervision and distant labels to mitigate annotation costs (Liu et al., 2019; Schouten and Frasincar, 2020). Following Chen et al. 1, we introduce a hierarchical labeling scheme to capture claim–premise relations across topics. Unlike prior datasets that focus on news editorials (Stab and Gurevych, 2017), our corpus covers social media discussions with high linguistic variability.",
    "reason": "Improper use of a footnote-like '1' after the author; should include a publication year or be formatted as a proper footnote/endnote.",
    "start": 182,
    "end": 195,
    "label": "Format"
  },
  {
    "span": "(Smith et al., 2018",
    "document": "Introduction\n\nSemi-supervised semantic segmentation leverages unlabeled images to reduce annotation costs while maintaining accuracy. Early consistency-based methods showed promise (Garcia and Patel, 2016; Jones and Li, 2017; Smith et al., 2018 but struggled with domain shifts across datasets. Recent approaches incorporate pseudo-label refinement and uncertainty estimation to mitigate confirmation bias (Huang and Zhao, 2019; Kumar et al., 2020). However, cross-domain generalization remains limited when source and target distributions diverge significantly (Rao and Singh, 2021).\n\nIn this paper, we introduce a curriculum-style self-training strategy that gradually expands the trusted region of predictions, improving robustness to distribution drift. We conduct comprehensive evaluations across four benchmarks and provide ablations on confidence calibration and augmentation schedules.",
    "reason": "Missing closing parenthesis in a parenthetical citation; the citation formatting is incomplete.",
    "start": -1,
    "end": -1,
    "label": "Format"
  },
  {
    "span": "Prior work has established that execution-based evaluation is more reliable than BLEU for code generation.",
    "document": "Introduction\n\nNeural code generation models map natural language specifications to executable programs. While n-gram overlap metrics such as BLEU and ROUGE offer quick proxies, they often misalign with functional correctness. Prior work has established that execution-based evaluation is more reliable than BLEU for code generation. Nonetheless, execution metrics can be brittle under flaky tests and environment drift, and they rarely capture readability or security properties.\n\nWe propose a hybrid metric that combines specification coverage through unit tests, dynamic trace similarity, and static analysis warnings. We also introduce a contamination-aware protocol that detects template leakage across train and test, reporting both pass@k and calibrated confidence intervals under varying test suite strengths.",
    "reason": "Asserts a conclusion attributed to 'prior work' without citing any specific studies that demonstrate this reliability difference.",
    "start": 226,
    "end": 332,
    "label": "Unsupported_claim"
  },
  {
    "span": "A previous study showed that pruning at 30% sparsity improves robustness to label noise in image classifiers.",
    "document": "Related Work\n\nModel compression techniques such as pruning and quantization reduce inference cost with minimal loss in accuracy for a variety of vision tasks. Beyond efficiency, recent investigations consider whether sparsity can also confer benefits in robustness and generalization. The interplay between pruning schedules, training dynamics, and noise resilience remains an open question.\n\nA previous study showed that pruning at 30% sparsity improves robustness to label noise in image classifiers. Other reports suggest that extreme sparsity can degrade calibration unless combined with retraining and temperature scaling. In this work, we disentangle these effects by varying sparsity levels and noise rates under a unified experimental protocol.",
    "reason": "Refers to a 'previous study' with a concrete result but does not cite it (rule a).",
    "start": 393,
    "end": 502,
    "label": "Unsupported_claim"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nGraph neural networks (GNNs) generalize convolutional architectures to relational data by propagating messages along edges while respecting permutation invariance (Sato et al., 2019; Romero and Xu, 2020). Despite strong empirical results on node classification and link prediction, oversmoothing and oversquashing remain central limitations (Ilyas and Morgan, 2021; Chen and Huang, 2022).\n\nRegularization strategies include residual connections, positional encodings, and subgraph sampling to expand effective receptive fields [12]. In contrast, spectral methods stabilize training by constraining filters to localized bands (Vega and Tran, 2021). Our work introduces a curvature-aware sampler that alleviates bottlenecks in long-range dependency propagation.\n",
    "reason": "Numeric bracket citation '[12]' conflicts with the surrounding author–year style.",
    "start": 541,
    "end": 545,
    "label": "Format"
  },
  {
    "span": "(Howard and Ruder 2018)",
    "document": "Related Work\n\nTransfer learning with language model pretraining has changed the landscape of low-resource NLP. Universal fine-tuning recipes, such as gradual unfreezing, were initially established for text classification (Howard and Ruder 2018) and then extended to sequence tagging (Peters et al., 2018). In parallel, domain-specific pretraining continues to show benefits (Beltagy et al., 2019). We focus on the implications for sample-efficient QA.",
    "reason": "Missing comma before the year in the parenthetical author–year citation; should be '(Howard and Ruder, 2018)'.",
    "start": 221,
    "end": 244,
    "label": "Format"
  },
  {
    "span": "Active learning selects informative points to query annotators (Settles, 2012). Crowdsourcing scales up annotation via microtasks (Snow et al., 2008). Weak supervision leverages noisy labeling functions (Ratner et al., 2017). Interface design affects labeling speed and consistency (Krishna et al., 2016).",
    "document": "Related Work\n\nHuman-in-the-loop data labeling connects model training with iterative supervision to improve sample efficiency and label quality. Approaches differ in how they acquire labels, manage annotator variability, and integrate weak or noisy signals into training pipelines.\n\nActive learning selects informative points to query annotators (Settles, 2012). Crowdsourcing scales up annotation via microtasks (Snow et al., 2008). Weak supervision leverages noisy labeling functions (Ratner et al., 2017). Interface design affects labeling speed and consistency (Krishna et al., 2016).\n\nOur method combines active selection with programmatic labeling and a calibration module that models annotator bias, aimed at minimizing total labeling cost under accuracy constraints.",
    "reason": "The span enumerates different strands of work with no transitions or explanation of their interrelations, reducing coherence between consecutive sentences.",
    "start": 283,
    "end": 588,
    "label": "Coherence"
  },
  {
    "span": "[12]",
    "document": "Introduction\n\nGraph neural networks (GNNs) propagate and transform node features along edges to capture local and global structure (Kipf and Welling, 2017; Hamilton et al., 2017). Despite strong performance, oversmoothing and oversquashing limit depth (Li et al., 2018; Alon and Yahav, 2020). A growing body of work proposes spectral regularization and positional encodings to address these issues (Dwivedi et al., 2021; Kreuzer et al., 2021). For practical deployment, we summarize common recipes for large-scale training, see [12], and provide a benchmark suite with standardized splits (Hu et al., 2020).",
    "reason": "Inconsistent citation style; numeric bracketed citation conflicts with author–year style used elsewhere. Should be an author–year citation.",
    "start": 528,
    "end": 532,
    "label": "Format"
  },
  {
    "span": "Johnson (2013))",
    "document": "Introduction\n\nNeural program synthesis blends search with learned priors to generate executable code from specifications (Balog et al., 2016; Devlin et al., 2017). Inductive biases that favor short, compositional programs can improve generalization, as argued by Johnson (2013)) in the context of structured prediction. We build on this perspective by introducing a length-aware decoding constraint that regularizes exploration.",
    "reason": "Extraneous closing parenthesis produces an unmatched parenthesis in the narrative citation.",
    "start": 263,
    "end": 278,
    "label": "Format"
  },
  {
    "span": "(O'Neil et. al., 2014)",
    "document": "Related Work\n\nFairness in machine learning has been studied through statistical, individual, and causal lenses. Early work formalized parity-based metrics for classification (Dwork et al., 2012). Subsequent studies examined trade-offs between calibration and equalized odds (Kleinberg et al., 2016) and domain-informed constraints in lending and hiring (O'Neil et. al., 2014). Recent surveys emphasize auditing pipelines and dataset documentation.",
    "reason": "Typographical/style error in 'et al.': it should be 'et al.' (without the extra period after 'et'). Correct form: '(O'Neil et al., 2014)'.",
    "start": 353,
    "end": 375,
    "label": "Format"
  },
  {
    "span": "Prior work has conclusively shown that residual connections eliminate oversmoothing in deep GNNs.",
    "document": "Related Work\n\nGraph neural networks (GNNs) have achieved strong results across node classification, link prediction, and graph-level tasks. A well-documented challenge is oversmoothing, where repeated message passing causes node representations to become indistinguishable. Various architectural modifications, normalization schemes, and training protocols have been proposed to mitigate this effect.\n\nPrior work has conclusively shown that residual connections eliminate oversmoothing in deep GNNs. While residual pathways can stabilize training and preserve gradient flow, their impact may interact with normalization and aggregation choices. In this paper, we revisit the relationship between residual design, normalization, and neighborhood expansion, and provide a controlled comparison across depth, sparsity, and homophily levels.\n\nOur results suggest that residuals alone are insufficient at high depths without appropriate normalization and sampling, motivating a holistic design framework for deep GNNs.",
    "reason": "The sentence attributes a conclusive result to 'prior work' without citing any specific papers, which is an unsupported claim about the literature.",
    "start": 402,
    "end": 499,
    "label": "Unsupported_claim"
  },
  {
    "span": "Previous work typically tunes hyperparameters on the test set in energy forecasting benchmarks.",
    "document": "Introduction\n\nShort-term energy load forecasting underpins grid reliability, demand response, and market operations. Classical approaches include ARIMA and exponential smoothing (Hyndman and Athanasopoulos, 2018), while recent advances in deep learning exploit temporal dependencies and exogenous drivers (Salinas et al., 2020; Lim and Zohren, 2021). Public benchmarks and competitions have spurred progress but also revealed pitfalls in evaluation protocols (Makridakis et al., 2020; Gasthaus et al., 2019).\n\nPrevious work typically tunes hyperparameters on the test set in energy forecasting benchmarks. To isolate this source of leakage, we propose a rolling-origin validation framework with nested cross-validation that strictly separates model selection from performance estimation.",
    "reason": "Accuses prior work of a specific methodological flaw without any citation or survey evidence (rule b).",
    "start": 510,
    "end": 605,
    "label": "Unsupported_claim"
  },
  {
    "span": "The CoNLL-03 shared task remains the de facto standard benchmark for English NER.",
    "document": "Introduction\n\nNamed entity recognition (NER) underpins numerous information extraction pipelines, from relation extraction to knowledge base population. Progress in pre-trained encoders has led to substantial gains on in-domain benchmarks, yet robustness to domain shift remains limited.\n\nThe CoNLL-03 shared task remains the de facto standard benchmark for English NER. However, its newswire focus and relatively small size can obscure weaknesses that appear in noisy or specialized domains.\n\nWe study cross-domain generalization by introducing a synthetic corruption suite and evaluating transfer from newswire to biomedical and social media text.",
    "reason": "A shared task and benchmark must be cited upon first mention to credit the dataset and task organizers (guideline a).",
    "start": 289,
    "end": 370,
    "label": "Unsupported_claim"
  },
  {
    "span": "(Brown et al., 2015))",
    "document": "Introduction\n\nFirst-order optimization remains the backbone of large-scale machine learning, with adaptive methods such as Adam and RMSProp widely adopted for their fast convergence in practice (Kingma and Ba, 2015; Tieleman and Hinton, 2012). Despite empirical success, their generalization properties lag behind simpler methods like SGD with momentum in certain regimes (Wilson et al., 2017; Keskar and Socher, 2017). Recent work studies the interplay between step-size schedules, implicit regularization, and curvature adaptation (Zhang et al., 2019; Liu et al., 2020).\n\nVariance reduction techniques, including SVRG and SAGA, improve convergence on finite-sum problems (Johnson and Zhang, 2013; Defazio et al., 2014), while quasi-Newton methods approximate second-order information at manageable cost (Nocedal and Wright, 2006; Byrd et al., 2016). Normalization layers and scale-invariant architectures further complicate the effective learning rate dynamics (Ioffe and Szegedy, 2015; Santurkar et al., 2018).\n\nOur contribution is a schedule-free adaptive optimizer that modulates parameter-wise curvature estimates using predicted loss decreases. We theoretically characterize stability under heterogeneous curvature and demonstrate improved sharpness-aware generalization. We compare against AdamW, Adafactor, and Shampoo on vision and language benchmarks (Loshchilov and Hutter, 2019; Shazeer and Stern, 2018; Gupta et al., 2018). Building on findings from (Brown et al., 2015)), we isolate the role of momentum buffering in mitigating noisy updates and show how our modulation complements decoupled weight decay.",
    "reason": "Extra closing parenthesis in the parenthetical citation; it should be '(Brown et al., 2015)'.",
    "start": 1464,
    "end": 1485,
    "label": "Format"
  },
  {
    "span": "Recent works demonstrate that retrieval-augmented models outperform parametric language models on factual QA.",
    "document": "Introduction\n\nParametric language models store substantial world knowledge in their weights, yet struggle with up-to-date facts and attribution (Petroni et al., 2019; Roberts et al., 2020). Retrieval-augmented generation (RAG) explicitly incorporates external documents at inference time, offering improved verifiability and latency-accuracy trade-offs (Guu et al., 2020; Lewis et al., 2020). Dense passage retrieval and hybrid indexing have further enhanced recall over large corpora (Karpukhin et al., 2020; Gao et al., 2021).\n\nRecent works demonstrate that retrieval-augmented models outperform parametric language models on factual QA. We revisit this comparison under strict retrieval budget constraints and propose a re-ranking module that optimizes for answerability and citation coverage.",
    "reason": "Uses the phrase 'recent works' to claim superiority without citing any specific studies, which requires supporting references.",
    "start": 530,
    "end": 639,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent audits show large gender disparities in occupation classification",
    "document": "Related Work\n\nAlgorithmic fairness research studies group disparities across demographic attributes. Recent audits show large gender disparities in occupation classification, raising concerns about deployment of web-scale classifiers. Mitigation strategies include adversarial debiasing (Zhang et al., 2018), reweighting (Kamiran and Calders, 2012), and calibrated equalized odds (Pleiss et al., 2017). In text classification, counterfactual data augmentation has reduced spurious correlations (Kaushik et al., 2020). We investigate post-hoc calibration under distribution shift.",
    "reason": "Claims evidence from unspecified “recent audits” without citing the audits.",
    "start": 101,
    "end": 173,
    "label": "Unsupported_claim"
  },
  {
    "span": "Recent models leverage message passing over user–item graphs to capture higher-order connectivity (Ying et al., 2018; He et al., 2020; Wu et al., 2021; Sun et al., 2020).",
    "document": "Related Work\n\nNeural recommendation. Traditional collaborative filtering methods model user–item interactions through matrix factorization and latent factors (Koren et al., 2009). With the adoption of deep learning, neural architectures have been introduced to capture non-linearities and side information, including textual reviews, images, and implicit feedback (He et al., 2017; Zhang et al., 2019).\n\nGraph-based models. Recent models leverage message passing over user–item graphs to capture higher-order connectivity (Ying et al., 2018; He et al., 2020; Wu et al., 2021; Sun et al., 2020). Self-supervised extensions further introduce auxiliary objectives such as contrastive learning on augmented graphs to improve robustness (Sun et al., 2019; Yu et al., 2022). Several works explore disentangling user intents or item facets within graph embeddings (Ma et al., 2019; Wang et al., 2020).\n\nCold-start and sparsity. To address data sparsity, some approaches incorporate content features, meta-learning, or knowledge graphs to enrich representations (Vartak et al., 2017; Hu et al., 2018; Wang et al., 2019). Others propose curriculum learning and denoising strategies that mitigate popularity bias and unreliable interactions (Zhu et al., 2021; Wei et al., 2021).\n\nSequential recommendation. A parallel line of research models temporal user behavior through Transformers or recurrent networks, integrating positional dynamics with attention over historical actions (Kang and McAuley, 2018; Sun et al., 2019; Chen et al., 2022).",
    "reason": "The span lists prior work on GNN-based recommendation without explaining how those methods relate to the authors' problem, what limitations remain, or how their approach differs, thereby lacking synthesis.",
    "start": 424,
    "end": 594,
    "label": "Lacks_synthesis"
  },
  {
    "span": "Recent works have achieved impressive gains by integrating factuality constraints into sequence-to-sequence models",
    "document": "Introduction\n\nAbstractive text summarization aims to produce concise, coherent summaries that capture the salient content of a document while allowing for novel phrasing. Despite strong advancements from pretrained encoder–decoder models, these systems often hallucinate unsupported statements that undermine factual reliability in downstream applications such as news aggregation and biomedical evidence synthesis.\n\nRecent works have achieved impressive gains by integrating factuality constraints into sequence-to-sequence models. Nevertheless, aligning generation with document-grounded evidence remains challenging when supervision for factual errors is scarce or noisy. Moreover, purely likelihood-based training does not directly penalize faithfulness violations, and post-hoc reranking or filtering introduces latency at inference time.\n\nIn this paper, we revisit fact-sensitive learning and propose a lightweight training-time regularizer that rewards extractive grounding without sacrificing abstractive capacity. We evaluate on multiple summarization datasets and analyze error types to understand when the method reduces hallucinations. Our contributions focus on simple mechanisms that are easy to integrate into existing pipelines and efficient to train.",
    "reason": "The phrase claims results by unspecified 'recent works' without providing citations, violating the requirement to cite prior work at first mention (rule d and a).",
    "start": 417,
    "end": 531,
    "label": "Unsupported_claim"
  },
  {
    "span": "REALM jointly learns retrieval and language modeling (Guu et al., 2020). RAG combines retriever and generator end-to-end (Lewis et al., 2020). DPR learns dense passage encoders (Karpukhin et al., 2020). Atlas scales parametric and non-parametric memory (Izacard et al., 2022).",
    "document": "Related Work\n\nRetrieval-Augmented Question Answering\n\nLarge language models benefit from augmenting parametric knowledge with non-parametric memory through retrieval. Such methods mitigate hallucinations and enable up-to-date information access.\n\nREALM jointly learns retrieval and language modeling (Guu et al., 2020). RAG combines retriever and generator end-to-end (Lewis et al., 2020). DPR learns dense passage encoders (Karpukhin et al., 2020). Atlas scales parametric and non-parametric memory (Izacard et al., 2022).\n\nWhile these systems differ in supervision and negative mining strategies, few address retrieval drift under distribution shift. We propose an entropy-regularized reranker that adapts to evolving corpora without full retraining.",
    "reason": "The span lists four systems but does not explain how they differ or build upon one another, nor are there transitions between sentences. The connection among the cited works is abrupt and only implied (criteria a and b).",
    "start": 247,
    "end": 523,
    "label": "Coherence"
  },
  {
    "span": "Williams et al., 2014]",
    "document": "Related Work\n\nAdversarial training improves robustness by optimizing worst-case losses under norm-bounded perturbations (Madry et al., 2018). However, early studies noted that empirical risk minimization can overspecialize to the training distribution (Williams et al., 2014] and Goodfellow et al., 2015), prompting interest in distributionally robust optimization (Duchi and Namkoong, 2018). Recent work scales robust objectives with curvature-aware updates (Andriushchenko and Flammarion, 2020).\n\nOur method complements these by decoupling feature alignment from classifier robustness, yielding improved transfer under unseen corruptions.",
    "reason": "Unmatched closing bracket in a citation; should be (Williams et al., 2014) without the trailing ']'",
    "start": 253,
    "end": 275,
    "label": "Format"
  }
]