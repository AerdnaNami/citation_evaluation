[
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\n",
        "completion": "The widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\n",
        "completion": "a previous study showed that temporal attention outperforms graph convolutions on rush-hour data"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\n",
        "completion": "The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\n",
        "completion": "To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\n",
        "completion": "In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\n",
        "completion": "SQuAD v3.0 includes unanswerable multi-hop questions."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\n",
        "completion": "To the best of our knowledge, this is the first work to align brain signals and code representations end-to-end."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\n",
        "completion": "The ImageNet-21k pretraining has become the de facto standard for strong ViT baselines."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\n",
        "completion": "using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\n",
        "completion": "The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\n",
        "completion": "Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\n",
        "completion": "there are numerous recent benchmarks for long-context language models"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\n",
        "completion": "SQuAD and HotpotQA datasets"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\n",
        "completion": "in a previous study, the authors showed that SpecAugment reduces WER by 20%"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\n",
        "completion": "In prior competitions, agents routinely exploited reward shaping to overfit the leaderboard"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\n",
        "completion": "Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\n",
        "completion": "The CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\n",
        "completion": "Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\n",
        "completion": "Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\n",
        "completion": "As reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\n",
        "completion": "Industry deployments have consistently achieved a 15% reduction in disparity after post-processing."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\n",
        "completion": "Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\n",
        "completion": "Over 90% of the world's languages lack transcribed speech corpora"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\n",
        "completion": "The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\n",
        "completion": "HotpotQA"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\n",
        "completion": "The 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\n",
        "completion": "BERT was used in an AES task trained on essays from Grade 7 to 10."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\n",
        "completion": "In a prior study, the authors claim that graph attention outperforms CNNs on citation networks"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\n",
        "completion": "BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\n",
        "completion": "A prior study demonstrated a 30% reduction in gender bias using causal prompts."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\n",
        "completion": "BERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\n",
        "completion": "It is widely accepted that toxicity classifiers underperform on African-American English."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\n",
        "completion": "BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\n",
        "completion": "In a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\n",
        "completion": "Most prior studies conclude that message passing saturates at 3\u20135 layers for QM9."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\n",
        "completion": "The DSTC8 track introduced multi-domain state tracking with schema-based labels."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\n",
        "completion": "the CoNLL-2003 shared task on NER established the standard splits used today"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\n",
        "completion": "BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\n",
        "completion": "there are many recent works that explore this topic"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\n",
        "completion": "The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\n",
        "completion": "There has been a surge of recent work on counterfactual fairness in ranking."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\n",
        "completion": "CLIP-style models have become the default backbone for multimodal evaluation."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\n",
        "completion": "several works have explored self-supervised graph augmentation for recommendation"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\n",
        "completion": "Earlier work established that anchor-free detectors are more robust to scale variance."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\n",
        "completion": "Recent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\n",
        "completion": "Early work on graph out-of-distribution detection introduced the leave-subgraph-out protocol"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\n",
        "completion": "Most recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\n",
        "completion": "Recently, a growing number of works have explored multimodal sarcasm detection in social media."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\n",
        "completion": "It is well known that word-level attacks transfer across models and datasets."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\n",
        "completion": "HIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\n",
        "completion": "The HumanEval dataset contains 164 Python problems"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\n",
        "completion": "BERT was used in an AES task trained on essays from multiple prompts and achieved strong results."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\n",
        "completion": "Vision-language models have recently been applied to medical report generation"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\n",
        "completion": "BERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\n",
        "completion": "On Atari, our baseline DQN reproduces state-of-the-art scores."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\n",
        "completion": "Most hospitals now deploy federated learning for chest X-ray analysis"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\nIntroduction\n\nLearning dexterous manipulation policies has advanced through model-free RL with demonstration guidance and structured state abstractions (Rajeswaran et al., 2018; Kalashnikov et al., 2018). Residual reinforcement learning combines analytical controllers with learned residuals to improve sample efficiency (Johannink et al., 2019). Curriculum learning has been used to stage task difficulty and improve exploration (Narvekar et al., 2020). We are the first to propose curriculum-aware residual RL for bimanual manipulation. Our method decomposes skill acquisition across coordinated arms while adapting residual capacity to curriculum stages.\n\nRelated Work\n\nPrior works tackle bimanual tasks with centralized policies (Levine et al., 2016) or hierarchical controllers (Zhang et al., 2020), but do not jointly optimize residuals and curricula. We evaluate on assembly and cloth manipulation benchmarks under partial observability.\n",
        "completion": "We are the first to propose curriculum-aware residual RL for bimanual manipulation."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\nIntroduction\n\nLearning dexterous manipulation policies has advanced through model-free RL with demonstration guidance and structured state abstractions (Rajeswaran et al., 2018; Kalashnikov et al., 2018). Residual reinforcement learning combines analytical controllers with learned residuals to improve sample efficiency (Johannink et al., 2019). Curriculum learning has been used to stage task difficulty and improve exploration (Narvekar et al., 2020). We are the first to propose curriculum-aware residual RL for bimanual manipulation. Our method decomposes skill acquisition across coordinated arms while adapting residual capacity to curriculum stages.\n\nRelated Work\n\nPrior works tackle bimanual tasks with centralized policies (Levine et al., 2016) or hierarchical controllers (Zhang et al., 2020), but do not jointly optimize residuals and curricula. We evaluate on assembly and cloth manipulation benchmarks under partial observability.\nFairness in Natural Language Understanding\n\nConcerns about bias in language technologies have prompted audits of datasets and models across tasks such as sentiment analysis, coreference, and natural language inference (NLI). Bias can arise from imbalanced label distributions, lexical artifacts, and annotation guidelines that inadvertently encode stereotypes. Recent benchmarks reveal systematic gender bias in widely used natural language inference datasets. These findings underscore the need for evaluation protocols that disentangle true reasoning ability from spurious correlations.\n\nWe propose a counterfactual data augmentation strategy for NLI that swaps gendered entities while preserving semantic roles and plausibility. Our approach is model-agnostic and integrates seamlessly with standard training pipelines.\n\nExperiments demonstrate improved robustness to artifact-triggered shortcuts and reduced disparities across gendered slices without sacrificing overall accuracy. We also provide an error taxonomy to guide dataset curation and model debugging.\n",
        "completion": "Recent benchmarks reveal systematic gender bias in widely used natural language inference datasets."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\nIntroduction\n\nLearning dexterous manipulation policies has advanced through model-free RL with demonstration guidance and structured state abstractions (Rajeswaran et al., 2018; Kalashnikov et al., 2018). Residual reinforcement learning combines analytical controllers with learned residuals to improve sample efficiency (Johannink et al., 2019). Curriculum learning has been used to stage task difficulty and improve exploration (Narvekar et al., 2020). We are the first to propose curriculum-aware residual RL for bimanual manipulation. Our method decomposes skill acquisition across coordinated arms while adapting residual capacity to curriculum stages.\n\nRelated Work\n\nPrior works tackle bimanual tasks with centralized policies (Levine et al., 2016) or hierarchical controllers (Zhang et al., 2020), but do not jointly optimize residuals and curricula. We evaluate on assembly and cloth manipulation benchmarks under partial observability.\nFairness in Natural Language Understanding\n\nConcerns about bias in language technologies have prompted audits of datasets and models across tasks such as sentiment analysis, coreference, and natural language inference (NLI). Bias can arise from imbalanced label distributions, lexical artifacts, and annotation guidelines that inadvertently encode stereotypes. Recent benchmarks reveal systematic gender bias in widely used natural language inference datasets. These findings underscore the need for evaluation protocols that disentangle true reasoning ability from spurious correlations.\n\nWe propose a counterfactual data augmentation strategy for NLI that swaps gendered entities while preserving semantic roles and plausibility. Our approach is model-agnostic and integrates seamlessly with standard training pipelines.\n\nExperiments demonstrate improved robustness to artifact-triggered shortcuts and reduced disparities across gendered slices without sacrificing overall accuracy. We also provide an error taxonomy to guide dataset curation and model debugging.\nIntroduction\n\nTokenization is a foundational step in text processing pipelines, especially for models that operate on subword units. While static tokenizers are efficient, they struggle with domain-specific idiosyncrasies such as noise, abbreviations, and code-switching.\n\nIn a previous study, the authors claim that context-aware tokenization yields a 5% absolute gain on noisy SMS data. Motivated by this observation, we propose a dynamic tokenization framework that adapts to local context signals using a lightweight controller. Our approach integrates seamlessly with standard encoders and introduces minimal computational overhead.\n\nWe evaluate our method under varying noise regimes and demonstrate improved robustness over static baselines. We further analyze segment boundary stability and its downstream impact on classification performance.\n",
        "completion": "In a previous study, the authors claim that context-aware tokenization yields a 5% absolute gain on noisy SMS data."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\nIntroduction\n\nLearning dexterous manipulation policies has advanced through model-free RL with demonstration guidance and structured state abstractions (Rajeswaran et al., 2018; Kalashnikov et al., 2018). Residual reinforcement learning combines analytical controllers with learned residuals to improve sample efficiency (Johannink et al., 2019). Curriculum learning has been used to stage task difficulty and improve exploration (Narvekar et al., 2020). We are the first to propose curriculum-aware residual RL for bimanual manipulation. Our method decomposes skill acquisition across coordinated arms while adapting residual capacity to curriculum stages.\n\nRelated Work\n\nPrior works tackle bimanual tasks with centralized policies (Levine et al., 2016) or hierarchical controllers (Zhang et al., 2020), but do not jointly optimize residuals and curricula. We evaluate on assembly and cloth manipulation benchmarks under partial observability.\nFairness in Natural Language Understanding\n\nConcerns about bias in language technologies have prompted audits of datasets and models across tasks such as sentiment analysis, coreference, and natural language inference (NLI). Bias can arise from imbalanced label distributions, lexical artifacts, and annotation guidelines that inadvertently encode stereotypes. Recent benchmarks reveal systematic gender bias in widely used natural language inference datasets. These findings underscore the need for evaluation protocols that disentangle true reasoning ability from spurious correlations.\n\nWe propose a counterfactual data augmentation strategy for NLI that swaps gendered entities while preserving semantic roles and plausibility. Our approach is model-agnostic and integrates seamlessly with standard training pipelines.\n\nExperiments demonstrate improved robustness to artifact-triggered shortcuts and reduced disparities across gendered slices without sacrificing overall accuracy. We also provide an error taxonomy to guide dataset curation and model debugging.\nIntroduction\n\nTokenization is a foundational step in text processing pipelines, especially for models that operate on subword units. While static tokenizers are efficient, they struggle with domain-specific idiosyncrasies such as noise, abbreviations, and code-switching.\n\nIn a previous study, the authors claim that context-aware tokenization yields a 5% absolute gain on noisy SMS data. Motivated by this observation, we propose a dynamic tokenization framework that adapts to local context signals using a lightweight controller. Our approach integrates seamlessly with standard encoders and introduces minimal computational overhead.\n\nWe evaluate our method under varying noise regimes and demonstrate improved robustness over static baselines. We further analyze segment boundary stability and its downstream impact on classification performance.\nIntroduction\n\nQuestion answering (QA) benchmarks have catalyzed rapid progress in reading comprehension and open-domain retrieval. Span-extractive datasets emphasize precise localization of evidence within a passage (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), while multi-hop tasks require reasoning across multiple documents (Yang et al., 2018). Transfer to low-resource domains often demands distant supervision or weak labeling strategies (Clark and Gardner, 2018). The SQuAD dataset contains over 150,000 question\u2013answer pairs curated from Wikipedia. In this paper, we revisit domain adaptation for QA and study robustness under controlled distribution shifts.\n",
        "completion": "The SQuAD dataset contains over 150,000 question\u2013answer pairs curated from Wikipedia."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\nIntroduction\n\nLearning dexterous manipulation policies has advanced through model-free RL with demonstration guidance and structured state abstractions (Rajeswaran et al., 2018; Kalashnikov et al., 2018). Residual reinforcement learning combines analytical controllers with learned residuals to improve sample efficiency (Johannink et al., 2019). Curriculum learning has been used to stage task difficulty and improve exploration (Narvekar et al., 2020). We are the first to propose curriculum-aware residual RL for bimanual manipulation. Our method decomposes skill acquisition across coordinated arms while adapting residual capacity to curriculum stages.\n\nRelated Work\n\nPrior works tackle bimanual tasks with centralized policies (Levine et al., 2016) or hierarchical controllers (Zhang et al., 2020), but do not jointly optimize residuals and curricula. We evaluate on assembly and cloth manipulation benchmarks under partial observability.\nFairness in Natural Language Understanding\n\nConcerns about bias in language technologies have prompted audits of datasets and models across tasks such as sentiment analysis, coreference, and natural language inference (NLI). Bias can arise from imbalanced label distributions, lexical artifacts, and annotation guidelines that inadvertently encode stereotypes. Recent benchmarks reveal systematic gender bias in widely used natural language inference datasets. These findings underscore the need for evaluation protocols that disentangle true reasoning ability from spurious correlations.\n\nWe propose a counterfactual data augmentation strategy for NLI that swaps gendered entities while preserving semantic roles and plausibility. Our approach is model-agnostic and integrates seamlessly with standard training pipelines.\n\nExperiments demonstrate improved robustness to artifact-triggered shortcuts and reduced disparities across gendered slices without sacrificing overall accuracy. We also provide an error taxonomy to guide dataset curation and model debugging.\nIntroduction\n\nTokenization is a foundational step in text processing pipelines, especially for models that operate on subword units. While static tokenizers are efficient, they struggle with domain-specific idiosyncrasies such as noise, abbreviations, and code-switching.\n\nIn a previous study, the authors claim that context-aware tokenization yields a 5% absolute gain on noisy SMS data. Motivated by this observation, we propose a dynamic tokenization framework that adapts to local context signals using a lightweight controller. Our approach integrates seamlessly with standard encoders and introduces minimal computational overhead.\n\nWe evaluate our method under varying noise regimes and demonstrate improved robustness over static baselines. We further analyze segment boundary stability and its downstream impact on classification performance.\nIntroduction\n\nQuestion answering (QA) benchmarks have catalyzed rapid progress in reading comprehension and open-domain retrieval. Span-extractive datasets emphasize precise localization of evidence within a passage (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), while multi-hop tasks require reasoning across multiple documents (Yang et al., 2018). Transfer to low-resource domains often demands distant supervision or weak labeling strategies (Clark and Gardner, 2018). The SQuAD dataset contains over 150,000 question\u2013answer pairs curated from Wikipedia. In this paper, we revisit domain adaptation for QA and study robustness under controlled distribution shifts.\nRelated Work\n\nRecommender systems have evolved from memory-based collaborative filtering to latent factor models and, more recently, graph-based neural architectures (Koren et al., 2009; He et al., 2017). Modeling higher-order connectivity and heterogeneous relations promises to capture richer user\u2013item interactions. Recent works show that graph neural networks consistently outperform matrix factorization on top-N recommendation. At the same time, issues such as oversmoothing, popularity bias, and cold-start remain active areas of research (Chen et al., 2020; Wu et al., 2022).\n",
        "completion": "Recent works show that graph neural networks consistently outperform matrix factorization on top-N recommendation."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\nIntroduction\n\nLearning dexterous manipulation policies has advanced through model-free RL with demonstration guidance and structured state abstractions (Rajeswaran et al., 2018; Kalashnikov et al., 2018). Residual reinforcement learning combines analytical controllers with learned residuals to improve sample efficiency (Johannink et al., 2019). Curriculum learning has been used to stage task difficulty and improve exploration (Narvekar et al., 2020). We are the first to propose curriculum-aware residual RL for bimanual manipulation. Our method decomposes skill acquisition across coordinated arms while adapting residual capacity to curriculum stages.\n\nRelated Work\n\nPrior works tackle bimanual tasks with centralized policies (Levine et al., 2016) or hierarchical controllers (Zhang et al., 2020), but do not jointly optimize residuals and curricula. We evaluate on assembly and cloth manipulation benchmarks under partial observability.\nFairness in Natural Language Understanding\n\nConcerns about bias in language technologies have prompted audits of datasets and models across tasks such as sentiment analysis, coreference, and natural language inference (NLI). Bias can arise from imbalanced label distributions, lexical artifacts, and annotation guidelines that inadvertently encode stereotypes. Recent benchmarks reveal systematic gender bias in widely used natural language inference datasets. These findings underscore the need for evaluation protocols that disentangle true reasoning ability from spurious correlations.\n\nWe propose a counterfactual data augmentation strategy for NLI that swaps gendered entities while preserving semantic roles and plausibility. Our approach is model-agnostic and integrates seamlessly with standard training pipelines.\n\nExperiments demonstrate improved robustness to artifact-triggered shortcuts and reduced disparities across gendered slices without sacrificing overall accuracy. We also provide an error taxonomy to guide dataset curation and model debugging.\nIntroduction\n\nTokenization is a foundational step in text processing pipelines, especially for models that operate on subword units. While static tokenizers are efficient, they struggle with domain-specific idiosyncrasies such as noise, abbreviations, and code-switching.\n\nIn a previous study, the authors claim that context-aware tokenization yields a 5% absolute gain on noisy SMS data. Motivated by this observation, we propose a dynamic tokenization framework that adapts to local context signals using a lightweight controller. Our approach integrates seamlessly with standard encoders and introduces minimal computational overhead.\n\nWe evaluate our method under varying noise regimes and demonstrate improved robustness over static baselines. We further analyze segment boundary stability and its downstream impact on classification performance.\nIntroduction\n\nQuestion answering (QA) benchmarks have catalyzed rapid progress in reading comprehension and open-domain retrieval. Span-extractive datasets emphasize precise localization of evidence within a passage (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), while multi-hop tasks require reasoning across multiple documents (Yang et al., 2018). Transfer to low-resource domains often demands distant supervision or weak labeling strategies (Clark and Gardner, 2018). The SQuAD dataset contains over 150,000 question\u2013answer pairs curated from Wikipedia. In this paper, we revisit domain adaptation for QA and study robustness under controlled distribution shifts.\nRelated Work\n\nRecommender systems have evolved from memory-based collaborative filtering to latent factor models and, more recently, graph-based neural architectures (Koren et al., 2009; He et al., 2017). Modeling higher-order connectivity and heterogeneous relations promises to capture richer user\u2013item interactions. Recent works show that graph neural networks consistently outperform matrix factorization on top-N recommendation. At the same time, issues such as oversmoothing, popularity bias, and cold-start remain active areas of research (Chen et al., 2020; Wu et al., 2022).\nRelated Work\n\nNeural program synthesis benchmarks range from domain-specific languages to general-purpose settings (Devlin et al., 2017; Ellis et al., 2018). Popular datasets include Robotic Puzzles, AlgoTasks, and diversified code corpora constructed from online judges (Sun et al., 2019; Chen et al., 2021b). The standard dataset for general-purpose program synthesis is PSB2. Evaluation protocols often consider test-time generalization to unseen tasks and execution-based correctness (Koul et al., 2020). We complement these efforts with a compositional benchmark emphasizing type-directed search.\n",
        "completion": "The standard dataset for general-purpose program synthesis is PSB2."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\nIntroduction\n\nLearning dexterous manipulation policies has advanced through model-free RL with demonstration guidance and structured state abstractions (Rajeswaran et al., 2018; Kalashnikov et al., 2018). Residual reinforcement learning combines analytical controllers with learned residuals to improve sample efficiency (Johannink et al., 2019). Curriculum learning has been used to stage task difficulty and improve exploration (Narvekar et al., 2020). We are the first to propose curriculum-aware residual RL for bimanual manipulation. Our method decomposes skill acquisition across coordinated arms while adapting residual capacity to curriculum stages.\n\nRelated Work\n\nPrior works tackle bimanual tasks with centralized policies (Levine et al., 2016) or hierarchical controllers (Zhang et al., 2020), but do not jointly optimize residuals and curricula. We evaluate on assembly and cloth manipulation benchmarks under partial observability.\nFairness in Natural Language Understanding\n\nConcerns about bias in language technologies have prompted audits of datasets and models across tasks such as sentiment analysis, coreference, and natural language inference (NLI). Bias can arise from imbalanced label distributions, lexical artifacts, and annotation guidelines that inadvertently encode stereotypes. Recent benchmarks reveal systematic gender bias in widely used natural language inference datasets. These findings underscore the need for evaluation protocols that disentangle true reasoning ability from spurious correlations.\n\nWe propose a counterfactual data augmentation strategy for NLI that swaps gendered entities while preserving semantic roles and plausibility. Our approach is model-agnostic and integrates seamlessly with standard training pipelines.\n\nExperiments demonstrate improved robustness to artifact-triggered shortcuts and reduced disparities across gendered slices without sacrificing overall accuracy. We also provide an error taxonomy to guide dataset curation and model debugging.\nIntroduction\n\nTokenization is a foundational step in text processing pipelines, especially for models that operate on subword units. While static tokenizers are efficient, they struggle with domain-specific idiosyncrasies such as noise, abbreviations, and code-switching.\n\nIn a previous study, the authors claim that context-aware tokenization yields a 5% absolute gain on noisy SMS data. Motivated by this observation, we propose a dynamic tokenization framework that adapts to local context signals using a lightweight controller. Our approach integrates seamlessly with standard encoders and introduces minimal computational overhead.\n\nWe evaluate our method under varying noise regimes and demonstrate improved robustness over static baselines. We further analyze segment boundary stability and its downstream impact on classification performance.\nIntroduction\n\nQuestion answering (QA) benchmarks have catalyzed rapid progress in reading comprehension and open-domain retrieval. Span-extractive datasets emphasize precise localization of evidence within a passage (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), while multi-hop tasks require reasoning across multiple documents (Yang et al., 2018). Transfer to low-resource domains often demands distant supervision or weak labeling strategies (Clark and Gardner, 2018). The SQuAD dataset contains over 150,000 question\u2013answer pairs curated from Wikipedia. In this paper, we revisit domain adaptation for QA and study robustness under controlled distribution shifts.\nRelated Work\n\nRecommender systems have evolved from memory-based collaborative filtering to latent factor models and, more recently, graph-based neural architectures (Koren et al., 2009; He et al., 2017). Modeling higher-order connectivity and heterogeneous relations promises to capture richer user\u2013item interactions. Recent works show that graph neural networks consistently outperform matrix factorization on top-N recommendation. At the same time, issues such as oversmoothing, popularity bias, and cold-start remain active areas of research (Chen et al., 2020; Wu et al., 2022).\nRelated Work\n\nNeural program synthesis benchmarks range from domain-specific languages to general-purpose settings (Devlin et al., 2017; Ellis et al., 2018). Popular datasets include Robotic Puzzles, AlgoTasks, and diversified code corpora constructed from online judges (Sun et al., 2019; Chen et al., 2021b). The standard dataset for general-purpose program synthesis is PSB2. Evaluation protocols often consider test-time generalization to unseen tasks and execution-based correctness (Koul et al., 2020). We complement these efforts with a compositional benchmark emphasizing type-directed search.\nRelated Work\n\nBiomedical named entity recognition (BioNER) underpins downstream tasks such as relation extraction and knowledge base construction. Early approaches adapted conditional random fields with domain-specific features, while neural models now dominate with contextualized embeddings and span classification strategies. Cross-corpus generalization remains challenging due to annotation differences and domain shifts across journals and time periods.\n\nThe BC5CDR dataset has been widely used for chemical and disease recognition and contains 1,500 annotated abstracts. Complementary corpora such as NCBI-Disease and JNLPBA target different entity types and labeling conventions, motivating multi-corpus training and normalization. Our approach leverages dictionary-augmented pretraining and constrained decoding to improve precision without sacrificing recall.\n",
        "completion": "The BC5CDR dataset has been widely used for chemical and disease recognition and contains 1,500 annotated abstracts."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\nIntroduction\n\nLearning dexterous manipulation policies has advanced through model-free RL with demonstration guidance and structured state abstractions (Rajeswaran et al., 2018; Kalashnikov et al., 2018). Residual reinforcement learning combines analytical controllers with learned residuals to improve sample efficiency (Johannink et al., 2019). Curriculum learning has been used to stage task difficulty and improve exploration (Narvekar et al., 2020). We are the first to propose curriculum-aware residual RL for bimanual manipulation. Our method decomposes skill acquisition across coordinated arms while adapting residual capacity to curriculum stages.\n\nRelated Work\n\nPrior works tackle bimanual tasks with centralized policies (Levine et al., 2016) or hierarchical controllers (Zhang et al., 2020), but do not jointly optimize residuals and curricula. We evaluate on assembly and cloth manipulation benchmarks under partial observability.\nFairness in Natural Language Understanding\n\nConcerns about bias in language technologies have prompted audits of datasets and models across tasks such as sentiment analysis, coreference, and natural language inference (NLI). Bias can arise from imbalanced label distributions, lexical artifacts, and annotation guidelines that inadvertently encode stereotypes. Recent benchmarks reveal systematic gender bias in widely used natural language inference datasets. These findings underscore the need for evaluation protocols that disentangle true reasoning ability from spurious correlations.\n\nWe propose a counterfactual data augmentation strategy for NLI that swaps gendered entities while preserving semantic roles and plausibility. Our approach is model-agnostic and integrates seamlessly with standard training pipelines.\n\nExperiments demonstrate improved robustness to artifact-triggered shortcuts and reduced disparities across gendered slices without sacrificing overall accuracy. We also provide an error taxonomy to guide dataset curation and model debugging.\nIntroduction\n\nTokenization is a foundational step in text processing pipelines, especially for models that operate on subword units. While static tokenizers are efficient, they struggle with domain-specific idiosyncrasies such as noise, abbreviations, and code-switching.\n\nIn a previous study, the authors claim that context-aware tokenization yields a 5% absolute gain on noisy SMS data. Motivated by this observation, we propose a dynamic tokenization framework that adapts to local context signals using a lightweight controller. Our approach integrates seamlessly with standard encoders and introduces minimal computational overhead.\n\nWe evaluate our method under varying noise regimes and demonstrate improved robustness over static baselines. We further analyze segment boundary stability and its downstream impact on classification performance.\nIntroduction\n\nQuestion answering (QA) benchmarks have catalyzed rapid progress in reading comprehension and open-domain retrieval. Span-extractive datasets emphasize precise localization of evidence within a passage (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), while multi-hop tasks require reasoning across multiple documents (Yang et al., 2018). Transfer to low-resource domains often demands distant supervision or weak labeling strategies (Clark and Gardner, 2018). The SQuAD dataset contains over 150,000 question\u2013answer pairs curated from Wikipedia. In this paper, we revisit domain adaptation for QA and study robustness under controlled distribution shifts.\nRelated Work\n\nRecommender systems have evolved from memory-based collaborative filtering to latent factor models and, more recently, graph-based neural architectures (Koren et al., 2009; He et al., 2017). Modeling higher-order connectivity and heterogeneous relations promises to capture richer user\u2013item interactions. Recent works show that graph neural networks consistently outperform matrix factorization on top-N recommendation. At the same time, issues such as oversmoothing, popularity bias, and cold-start remain active areas of research (Chen et al., 2020; Wu et al., 2022).\nRelated Work\n\nNeural program synthesis benchmarks range from domain-specific languages to general-purpose settings (Devlin et al., 2017; Ellis et al., 2018). Popular datasets include Robotic Puzzles, AlgoTasks, and diversified code corpora constructed from online judges (Sun et al., 2019; Chen et al., 2021b). The standard dataset for general-purpose program synthesis is PSB2. Evaluation protocols often consider test-time generalization to unseen tasks and execution-based correctness (Koul et al., 2020). We complement these efforts with a compositional benchmark emphasizing type-directed search.\nRelated Work\n\nBiomedical named entity recognition (BioNER) underpins downstream tasks such as relation extraction and knowledge base construction. Early approaches adapted conditional random fields with domain-specific features, while neural models now dominate with contextualized embeddings and span classification strategies. Cross-corpus generalization remains challenging due to annotation differences and domain shifts across journals and time periods.\n\nThe BC5CDR dataset has been widely used for chemical and disease recognition and contains 1,500 annotated abstracts. Complementary corpora such as NCBI-Disease and JNLPBA target different entity types and labeling conventions, motivating multi-corpus training and normalization. Our approach leverages dictionary-augmented pretraining and constrained decoding to improve precision without sacrificing recall.\nRelated Work\n\nNatural language inference (NLI) evaluates whether a premise entails, contradicts, or is neutral to a hypothesis and is a cornerstone of language understanding (Bowman et al., 2015). Large-scale resources such as SNLI and MultiNLI facilitated learning transferable sentence representations (Williams et al., 2018). The SNLI dataset is widely used for evaluating commonsense reasoning. However, subsequent analyses revealed annotation artifacts and shallow heuristics that models can exploit, motivating adversarial and stress-test evaluations (Gururangan et al., 2018; Naik et al., 2018).\n\nOur focus is on compositional generalization, where we test whether models reason consistently under controlled perturbations of lexical and syntactic cues. We provide a suite of counterfactual pairs designed to isolate world knowledge from linguistic form.\n",
        "completion": "The SNLI dataset is widely used for evaluating commonsense reasoning"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\nIntroduction\n\nLearning dexterous manipulation policies has advanced through model-free RL with demonstration guidance and structured state abstractions (Rajeswaran et al., 2018; Kalashnikov et al., 2018). Residual reinforcement learning combines analytical controllers with learned residuals to improve sample efficiency (Johannink et al., 2019). Curriculum learning has been used to stage task difficulty and improve exploration (Narvekar et al., 2020). We are the first to propose curriculum-aware residual RL for bimanual manipulation. Our method decomposes skill acquisition across coordinated arms while adapting residual capacity to curriculum stages.\n\nRelated Work\n\nPrior works tackle bimanual tasks with centralized policies (Levine et al., 2016) or hierarchical controllers (Zhang et al., 2020), but do not jointly optimize residuals and curricula. We evaluate on assembly and cloth manipulation benchmarks under partial observability.\nFairness in Natural Language Understanding\n\nConcerns about bias in language technologies have prompted audits of datasets and models across tasks such as sentiment analysis, coreference, and natural language inference (NLI). Bias can arise from imbalanced label distributions, lexical artifacts, and annotation guidelines that inadvertently encode stereotypes. Recent benchmarks reveal systematic gender bias in widely used natural language inference datasets. These findings underscore the need for evaluation protocols that disentangle true reasoning ability from spurious correlations.\n\nWe propose a counterfactual data augmentation strategy for NLI that swaps gendered entities while preserving semantic roles and plausibility. Our approach is model-agnostic and integrates seamlessly with standard training pipelines.\n\nExperiments demonstrate improved robustness to artifact-triggered shortcuts and reduced disparities across gendered slices without sacrificing overall accuracy. We also provide an error taxonomy to guide dataset curation and model debugging.\nIntroduction\n\nTokenization is a foundational step in text processing pipelines, especially for models that operate on subword units. While static tokenizers are efficient, they struggle with domain-specific idiosyncrasies such as noise, abbreviations, and code-switching.\n\nIn a previous study, the authors claim that context-aware tokenization yields a 5% absolute gain on noisy SMS data. Motivated by this observation, we propose a dynamic tokenization framework that adapts to local context signals using a lightweight controller. Our approach integrates seamlessly with standard encoders and introduces minimal computational overhead.\n\nWe evaluate our method under varying noise regimes and demonstrate improved robustness over static baselines. We further analyze segment boundary stability and its downstream impact on classification performance.\nIntroduction\n\nQuestion answering (QA) benchmarks have catalyzed rapid progress in reading comprehension and open-domain retrieval. Span-extractive datasets emphasize precise localization of evidence within a passage (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), while multi-hop tasks require reasoning across multiple documents (Yang et al., 2018). Transfer to low-resource domains often demands distant supervision or weak labeling strategies (Clark and Gardner, 2018). The SQuAD dataset contains over 150,000 question\u2013answer pairs curated from Wikipedia. In this paper, we revisit domain adaptation for QA and study robustness under controlled distribution shifts.\nRelated Work\n\nRecommender systems have evolved from memory-based collaborative filtering to latent factor models and, more recently, graph-based neural architectures (Koren et al., 2009; He et al., 2017). Modeling higher-order connectivity and heterogeneous relations promises to capture richer user\u2013item interactions. Recent works show that graph neural networks consistently outperform matrix factorization on top-N recommendation. At the same time, issues such as oversmoothing, popularity bias, and cold-start remain active areas of research (Chen et al., 2020; Wu et al., 2022).\nRelated Work\n\nNeural program synthesis benchmarks range from domain-specific languages to general-purpose settings (Devlin et al., 2017; Ellis et al., 2018). Popular datasets include Robotic Puzzles, AlgoTasks, and diversified code corpora constructed from online judges (Sun et al., 2019; Chen et al., 2021b). The standard dataset for general-purpose program synthesis is PSB2. Evaluation protocols often consider test-time generalization to unseen tasks and execution-based correctness (Koul et al., 2020). We complement these efforts with a compositional benchmark emphasizing type-directed search.\nRelated Work\n\nBiomedical named entity recognition (BioNER) underpins downstream tasks such as relation extraction and knowledge base construction. Early approaches adapted conditional random fields with domain-specific features, while neural models now dominate with contextualized embeddings and span classification strategies. Cross-corpus generalization remains challenging due to annotation differences and domain shifts across journals and time periods.\n\nThe BC5CDR dataset has been widely used for chemical and disease recognition and contains 1,500 annotated abstracts. Complementary corpora such as NCBI-Disease and JNLPBA target different entity types and labeling conventions, motivating multi-corpus training and normalization. Our approach leverages dictionary-augmented pretraining and constrained decoding to improve precision without sacrificing recall.\nRelated Work\n\nNatural language inference (NLI) evaluates whether a premise entails, contradicts, or is neutral to a hypothesis and is a cornerstone of language understanding (Bowman et al., 2015). Large-scale resources such as SNLI and MultiNLI facilitated learning transferable sentence representations (Williams et al., 2018). The SNLI dataset is widely used for evaluating commonsense reasoning. However, subsequent analyses revealed annotation artifacts and shallow heuristics that models can exploit, motivating adversarial and stress-test evaluations (Gururangan et al., 2018; Naik et al., 2018).\n\nOur focus is on compositional generalization, where we test whether models reason consistently under controlled perturbations of lexical and syntactic cues. We provide a suite of counterfactual pairs designed to isolate world knowledge from linguistic form.\nRelated Work\n\nEarth system modeling has progressed through community intercomparison projects that unify experimental protocols and facilitate multi-model synthesis (Eyring et al., 2016). Downstream applications in impacts and adaptation increasingly rely on standardized access patterns and data cubes for scalable analytics (Pangeo Consortium, 2019).\n\nThe CMIP6 archive contains over 20 PB of data accessible via OPeNDAP. Despite this scale, practitioners lack lightweight baselines for spatiotemporal subsampling that preserve climate signals of interest. We address this by proposing a streaming framework with provable error controls for common diagnostics.\n",
        "completion": "The CMIP6 archive contains over 20 PB of data accessible via OPeNDAP."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\nIntroduction\n\nLearning dexterous manipulation policies has advanced through model-free RL with demonstration guidance and structured state abstractions (Rajeswaran et al., 2018; Kalashnikov et al., 2018). Residual reinforcement learning combines analytical controllers with learned residuals to improve sample efficiency (Johannink et al., 2019). Curriculum learning has been used to stage task difficulty and improve exploration (Narvekar et al., 2020). We are the first to propose curriculum-aware residual RL for bimanual manipulation. Our method decomposes skill acquisition across coordinated arms while adapting residual capacity to curriculum stages.\n\nRelated Work\n\nPrior works tackle bimanual tasks with centralized policies (Levine et al., 2016) or hierarchical controllers (Zhang et al., 2020), but do not jointly optimize residuals and curricula. We evaluate on assembly and cloth manipulation benchmarks under partial observability.\nFairness in Natural Language Understanding\n\nConcerns about bias in language technologies have prompted audits of datasets and models across tasks such as sentiment analysis, coreference, and natural language inference (NLI). Bias can arise from imbalanced label distributions, lexical artifacts, and annotation guidelines that inadvertently encode stereotypes. Recent benchmarks reveal systematic gender bias in widely used natural language inference datasets. These findings underscore the need for evaluation protocols that disentangle true reasoning ability from spurious correlations.\n\nWe propose a counterfactual data augmentation strategy for NLI that swaps gendered entities while preserving semantic roles and plausibility. Our approach is model-agnostic and integrates seamlessly with standard training pipelines.\n\nExperiments demonstrate improved robustness to artifact-triggered shortcuts and reduced disparities across gendered slices without sacrificing overall accuracy. We also provide an error taxonomy to guide dataset curation and model debugging.\nIntroduction\n\nTokenization is a foundational step in text processing pipelines, especially for models that operate on subword units. While static tokenizers are efficient, they struggle with domain-specific idiosyncrasies such as noise, abbreviations, and code-switching.\n\nIn a previous study, the authors claim that context-aware tokenization yields a 5% absolute gain on noisy SMS data. Motivated by this observation, we propose a dynamic tokenization framework that adapts to local context signals using a lightweight controller. Our approach integrates seamlessly with standard encoders and introduces minimal computational overhead.\n\nWe evaluate our method under varying noise regimes and demonstrate improved robustness over static baselines. We further analyze segment boundary stability and its downstream impact on classification performance.\nIntroduction\n\nQuestion answering (QA) benchmarks have catalyzed rapid progress in reading comprehension and open-domain retrieval. Span-extractive datasets emphasize precise localization of evidence within a passage (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), while multi-hop tasks require reasoning across multiple documents (Yang et al., 2018). Transfer to low-resource domains often demands distant supervision or weak labeling strategies (Clark and Gardner, 2018). The SQuAD dataset contains over 150,000 question\u2013answer pairs curated from Wikipedia. In this paper, we revisit domain adaptation for QA and study robustness under controlled distribution shifts.\nRelated Work\n\nRecommender systems have evolved from memory-based collaborative filtering to latent factor models and, more recently, graph-based neural architectures (Koren et al., 2009; He et al., 2017). Modeling higher-order connectivity and heterogeneous relations promises to capture richer user\u2013item interactions. Recent works show that graph neural networks consistently outperform matrix factorization on top-N recommendation. At the same time, issues such as oversmoothing, popularity bias, and cold-start remain active areas of research (Chen et al., 2020; Wu et al., 2022).\nRelated Work\n\nNeural program synthesis benchmarks range from domain-specific languages to general-purpose settings (Devlin et al., 2017; Ellis et al., 2018). Popular datasets include Robotic Puzzles, AlgoTasks, and diversified code corpora constructed from online judges (Sun et al., 2019; Chen et al., 2021b). The standard dataset for general-purpose program synthesis is PSB2. Evaluation protocols often consider test-time generalization to unseen tasks and execution-based correctness (Koul et al., 2020). We complement these efforts with a compositional benchmark emphasizing type-directed search.\nRelated Work\n\nBiomedical named entity recognition (BioNER) underpins downstream tasks such as relation extraction and knowledge base construction. Early approaches adapted conditional random fields with domain-specific features, while neural models now dominate with contextualized embeddings and span classification strategies. Cross-corpus generalization remains challenging due to annotation differences and domain shifts across journals and time periods.\n\nThe BC5CDR dataset has been widely used for chemical and disease recognition and contains 1,500 annotated abstracts. Complementary corpora such as NCBI-Disease and JNLPBA target different entity types and labeling conventions, motivating multi-corpus training and normalization. Our approach leverages dictionary-augmented pretraining and constrained decoding to improve precision without sacrificing recall.\nRelated Work\n\nNatural language inference (NLI) evaluates whether a premise entails, contradicts, or is neutral to a hypothesis and is a cornerstone of language understanding (Bowman et al., 2015). Large-scale resources such as SNLI and MultiNLI facilitated learning transferable sentence representations (Williams et al., 2018). The SNLI dataset is widely used for evaluating commonsense reasoning. However, subsequent analyses revealed annotation artifacts and shallow heuristics that models can exploit, motivating adversarial and stress-test evaluations (Gururangan et al., 2018; Naik et al., 2018).\n\nOur focus is on compositional generalization, where we test whether models reason consistently under controlled perturbations of lexical and syntactic cues. We provide a suite of counterfactual pairs designed to isolate world knowledge from linguistic form.\nRelated Work\n\nEarth system modeling has progressed through community intercomparison projects that unify experimental protocols and facilitate multi-model synthesis (Eyring et al., 2016). Downstream applications in impacts and adaptation increasingly rely on standardized access patterns and data cubes for scalable analytics (Pangeo Consortium, 2019).\n\nThe CMIP6 archive contains over 20 PB of data accessible via OPeNDAP. Despite this scale, practitioners lack lightweight baselines for spatiotemporal subsampling that preserve climate signals of interest. We address this by proposing a streaming framework with provable error controls for common diagnostics.\nIntroduction\n\nSentiment Analysis on Social Media\n\nWe investigate sentence-level sentiment classification on Twitter. We follow the setup of the SemEval-2017 Task 4 on Twitter sentiment to ensure comparability across systems, including the standard train-dev-test splits and three-way polarity labels. Prior benchmarks underscore the challenges posed by informal language, sarcasm, and domain drift. Our contribution focuses on robust pretraining and test-time augmentation for handling noisy, rapidly evolving slang in microtexts.\n",
        "completion": "We follow the setup of the SemEval-2017 Task 4 on Twitter sentiment"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\nIntroduction\n\nLearning dexterous manipulation policies has advanced through model-free RL with demonstration guidance and structured state abstractions (Rajeswaran et al., 2018; Kalashnikov et al., 2018). Residual reinforcement learning combines analytical controllers with learned residuals to improve sample efficiency (Johannink et al., 2019). Curriculum learning has been used to stage task difficulty and improve exploration (Narvekar et al., 2020). We are the first to propose curriculum-aware residual RL for bimanual manipulation. Our method decomposes skill acquisition across coordinated arms while adapting residual capacity to curriculum stages.\n\nRelated Work\n\nPrior works tackle bimanual tasks with centralized policies (Levine et al., 2016) or hierarchical controllers (Zhang et al., 2020), but do not jointly optimize residuals and curricula. We evaluate on assembly and cloth manipulation benchmarks under partial observability.\nFairness in Natural Language Understanding\n\nConcerns about bias in language technologies have prompted audits of datasets and models across tasks such as sentiment analysis, coreference, and natural language inference (NLI). Bias can arise from imbalanced label distributions, lexical artifacts, and annotation guidelines that inadvertently encode stereotypes. Recent benchmarks reveal systematic gender bias in widely used natural language inference datasets. These findings underscore the need for evaluation protocols that disentangle true reasoning ability from spurious correlations.\n\nWe propose a counterfactual data augmentation strategy for NLI that swaps gendered entities while preserving semantic roles and plausibility. Our approach is model-agnostic and integrates seamlessly with standard training pipelines.\n\nExperiments demonstrate improved robustness to artifact-triggered shortcuts and reduced disparities across gendered slices without sacrificing overall accuracy. We also provide an error taxonomy to guide dataset curation and model debugging.\nIntroduction\n\nTokenization is a foundational step in text processing pipelines, especially for models that operate on subword units. While static tokenizers are efficient, they struggle with domain-specific idiosyncrasies such as noise, abbreviations, and code-switching.\n\nIn a previous study, the authors claim that context-aware tokenization yields a 5% absolute gain on noisy SMS data. Motivated by this observation, we propose a dynamic tokenization framework that adapts to local context signals using a lightweight controller. Our approach integrates seamlessly with standard encoders and introduces minimal computational overhead.\n\nWe evaluate our method under varying noise regimes and demonstrate improved robustness over static baselines. We further analyze segment boundary stability and its downstream impact on classification performance.\nIntroduction\n\nQuestion answering (QA) benchmarks have catalyzed rapid progress in reading comprehension and open-domain retrieval. Span-extractive datasets emphasize precise localization of evidence within a passage (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), while multi-hop tasks require reasoning across multiple documents (Yang et al., 2018). Transfer to low-resource domains often demands distant supervision or weak labeling strategies (Clark and Gardner, 2018). The SQuAD dataset contains over 150,000 question\u2013answer pairs curated from Wikipedia. In this paper, we revisit domain adaptation for QA and study robustness under controlled distribution shifts.\nRelated Work\n\nRecommender systems have evolved from memory-based collaborative filtering to latent factor models and, more recently, graph-based neural architectures (Koren et al., 2009; He et al., 2017). Modeling higher-order connectivity and heterogeneous relations promises to capture richer user\u2013item interactions. Recent works show that graph neural networks consistently outperform matrix factorization on top-N recommendation. At the same time, issues such as oversmoothing, popularity bias, and cold-start remain active areas of research (Chen et al., 2020; Wu et al., 2022).\nRelated Work\n\nNeural program synthesis benchmarks range from domain-specific languages to general-purpose settings (Devlin et al., 2017; Ellis et al., 2018). Popular datasets include Robotic Puzzles, AlgoTasks, and diversified code corpora constructed from online judges (Sun et al., 2019; Chen et al., 2021b). The standard dataset for general-purpose program synthesis is PSB2. Evaluation protocols often consider test-time generalization to unseen tasks and execution-based correctness (Koul et al., 2020). We complement these efforts with a compositional benchmark emphasizing type-directed search.\nRelated Work\n\nBiomedical named entity recognition (BioNER) underpins downstream tasks such as relation extraction and knowledge base construction. Early approaches adapted conditional random fields with domain-specific features, while neural models now dominate with contextualized embeddings and span classification strategies. Cross-corpus generalization remains challenging due to annotation differences and domain shifts across journals and time periods.\n\nThe BC5CDR dataset has been widely used for chemical and disease recognition and contains 1,500 annotated abstracts. Complementary corpora such as NCBI-Disease and JNLPBA target different entity types and labeling conventions, motivating multi-corpus training and normalization. Our approach leverages dictionary-augmented pretraining and constrained decoding to improve precision without sacrificing recall.\nRelated Work\n\nNatural language inference (NLI) evaluates whether a premise entails, contradicts, or is neutral to a hypothesis and is a cornerstone of language understanding (Bowman et al., 2015). Large-scale resources such as SNLI and MultiNLI facilitated learning transferable sentence representations (Williams et al., 2018). The SNLI dataset is widely used for evaluating commonsense reasoning. However, subsequent analyses revealed annotation artifacts and shallow heuristics that models can exploit, motivating adversarial and stress-test evaluations (Gururangan et al., 2018; Naik et al., 2018).\n\nOur focus is on compositional generalization, where we test whether models reason consistently under controlled perturbations of lexical and syntactic cues. We provide a suite of counterfactual pairs designed to isolate world knowledge from linguistic form.\nRelated Work\n\nEarth system modeling has progressed through community intercomparison projects that unify experimental protocols and facilitate multi-model synthesis (Eyring et al., 2016). Downstream applications in impacts and adaptation increasingly rely on standardized access patterns and data cubes for scalable analytics (Pangeo Consortium, 2019).\n\nThe CMIP6 archive contains over 20 PB of data accessible via OPeNDAP. Despite this scale, practitioners lack lightweight baselines for spatiotemporal subsampling that preserve climate signals of interest. We address this by proposing a streaming framework with provable error controls for common diagnostics.\nIntroduction\n\nSentiment Analysis on Social Media\n\nWe investigate sentence-level sentiment classification on Twitter. We follow the setup of the SemEval-2017 Task 4 on Twitter sentiment to ensure comparability across systems, including the standard train-dev-test splits and three-way polarity labels. Prior benchmarks underscore the challenges posed by informal language, sarcasm, and domain drift. Our contribution focuses on robust pretraining and test-time augmentation for handling noisy, rapidly evolving slang in microtexts.\nIntroduction\n\nOffensive language identification aims to detect abusive, profane, or threatening content across social media platforms. Early research framed the problem as binary classification over tweets and forum posts with handcrafted features (Waseem and Hovy, 2016; Davidson et al., 2017). More recent advances rely on pretrained encoders fine-tuned with class reweighting or focal losses to mitigate skew (Zampieri et al., 2020). The task was first formalized in the 2019 SemEval shared task, which catalyzed standardized evaluation and spurred the release of multilingual resources. Building on this, our work examines cross-lingual transfer under limited annotation.\n\n",
        "completion": "The task was first formalized in the 2019 SemEval shared task"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\nIntroduction\n\nLearning dexterous manipulation policies has advanced through model-free RL with demonstration guidance and structured state abstractions (Rajeswaran et al., 2018; Kalashnikov et al., 2018). Residual reinforcement learning combines analytical controllers with learned residuals to improve sample efficiency (Johannink et al., 2019). Curriculum learning has been used to stage task difficulty and improve exploration (Narvekar et al., 2020). We are the first to propose curriculum-aware residual RL for bimanual manipulation. Our method decomposes skill acquisition across coordinated arms while adapting residual capacity to curriculum stages.\n\nRelated Work\n\nPrior works tackle bimanual tasks with centralized policies (Levine et al., 2016) or hierarchical controllers (Zhang et al., 2020), but do not jointly optimize residuals and curricula. We evaluate on assembly and cloth manipulation benchmarks under partial observability.\nFairness in Natural Language Understanding\n\nConcerns about bias in language technologies have prompted audits of datasets and models across tasks such as sentiment analysis, coreference, and natural language inference (NLI). Bias can arise from imbalanced label distributions, lexical artifacts, and annotation guidelines that inadvertently encode stereotypes. Recent benchmarks reveal systematic gender bias in widely used natural language inference datasets. These findings underscore the need for evaluation protocols that disentangle true reasoning ability from spurious correlations.\n\nWe propose a counterfactual data augmentation strategy for NLI that swaps gendered entities while preserving semantic roles and plausibility. Our approach is model-agnostic and integrates seamlessly with standard training pipelines.\n\nExperiments demonstrate improved robustness to artifact-triggered shortcuts and reduced disparities across gendered slices without sacrificing overall accuracy. We also provide an error taxonomy to guide dataset curation and model debugging.\nIntroduction\n\nTokenization is a foundational step in text processing pipelines, especially for models that operate on subword units. While static tokenizers are efficient, they struggle with domain-specific idiosyncrasies such as noise, abbreviations, and code-switching.\n\nIn a previous study, the authors claim that context-aware tokenization yields a 5% absolute gain on noisy SMS data. Motivated by this observation, we propose a dynamic tokenization framework that adapts to local context signals using a lightweight controller. Our approach integrates seamlessly with standard encoders and introduces minimal computational overhead.\n\nWe evaluate our method under varying noise regimes and demonstrate improved robustness over static baselines. We further analyze segment boundary stability and its downstream impact on classification performance.\nIntroduction\n\nQuestion answering (QA) benchmarks have catalyzed rapid progress in reading comprehension and open-domain retrieval. Span-extractive datasets emphasize precise localization of evidence within a passage (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), while multi-hop tasks require reasoning across multiple documents (Yang et al., 2018). Transfer to low-resource domains often demands distant supervision or weak labeling strategies (Clark and Gardner, 2018). The SQuAD dataset contains over 150,000 question\u2013answer pairs curated from Wikipedia. In this paper, we revisit domain adaptation for QA and study robustness under controlled distribution shifts.\nRelated Work\n\nRecommender systems have evolved from memory-based collaborative filtering to latent factor models and, more recently, graph-based neural architectures (Koren et al., 2009; He et al., 2017). Modeling higher-order connectivity and heterogeneous relations promises to capture richer user\u2013item interactions. Recent works show that graph neural networks consistently outperform matrix factorization on top-N recommendation. At the same time, issues such as oversmoothing, popularity bias, and cold-start remain active areas of research (Chen et al., 2020; Wu et al., 2022).\nRelated Work\n\nNeural program synthesis benchmarks range from domain-specific languages to general-purpose settings (Devlin et al., 2017; Ellis et al., 2018). Popular datasets include Robotic Puzzles, AlgoTasks, and diversified code corpora constructed from online judges (Sun et al., 2019; Chen et al., 2021b). The standard dataset for general-purpose program synthesis is PSB2. Evaluation protocols often consider test-time generalization to unseen tasks and execution-based correctness (Koul et al., 2020). We complement these efforts with a compositional benchmark emphasizing type-directed search.\nRelated Work\n\nBiomedical named entity recognition (BioNER) underpins downstream tasks such as relation extraction and knowledge base construction. Early approaches adapted conditional random fields with domain-specific features, while neural models now dominate with contextualized embeddings and span classification strategies. Cross-corpus generalization remains challenging due to annotation differences and domain shifts across journals and time periods.\n\nThe BC5CDR dataset has been widely used for chemical and disease recognition and contains 1,500 annotated abstracts. Complementary corpora such as NCBI-Disease and JNLPBA target different entity types and labeling conventions, motivating multi-corpus training and normalization. Our approach leverages dictionary-augmented pretraining and constrained decoding to improve precision without sacrificing recall.\nRelated Work\n\nNatural language inference (NLI) evaluates whether a premise entails, contradicts, or is neutral to a hypothesis and is a cornerstone of language understanding (Bowman et al., 2015). Large-scale resources such as SNLI and MultiNLI facilitated learning transferable sentence representations (Williams et al., 2018). The SNLI dataset is widely used for evaluating commonsense reasoning. However, subsequent analyses revealed annotation artifacts and shallow heuristics that models can exploit, motivating adversarial and stress-test evaluations (Gururangan et al., 2018; Naik et al., 2018).\n\nOur focus is on compositional generalization, where we test whether models reason consistently under controlled perturbations of lexical and syntactic cues. We provide a suite of counterfactual pairs designed to isolate world knowledge from linguistic form.\nRelated Work\n\nEarth system modeling has progressed through community intercomparison projects that unify experimental protocols and facilitate multi-model synthesis (Eyring et al., 2016). Downstream applications in impacts and adaptation increasingly rely on standardized access patterns and data cubes for scalable analytics (Pangeo Consortium, 2019).\n\nThe CMIP6 archive contains over 20 PB of data accessible via OPeNDAP. Despite this scale, practitioners lack lightweight baselines for spatiotemporal subsampling that preserve climate signals of interest. We address this by proposing a streaming framework with provable error controls for common diagnostics.\nIntroduction\n\nSentiment Analysis on Social Media\n\nWe investigate sentence-level sentiment classification on Twitter. We follow the setup of the SemEval-2017 Task 4 on Twitter sentiment to ensure comparability across systems, including the standard train-dev-test splits and three-way polarity labels. Prior benchmarks underscore the challenges posed by informal language, sarcasm, and domain drift. Our contribution focuses on robust pretraining and test-time augmentation for handling noisy, rapidly evolving slang in microtexts.\nIntroduction\n\nOffensive language identification aims to detect abusive, profane, or threatening content across social media platforms. Early research framed the problem as binary classification over tweets and forum posts with handcrafted features (Waseem and Hovy, 2016; Davidson et al., 2017). More recent advances rely on pretrained encoders fine-tuned with class reweighting or focal losses to mitigate skew (Zampieri et al., 2020). The task was first formalized in the 2019 SemEval shared task, which catalyzed standardized evaluation and spurred the release of multilingual resources. Building on this, our work examines cross-lingual transfer under limited annotation.\n\nRelated Work\n\nObject detection for urban traffic analytics has evolved alongside increasingly large-scale datasets and benchmarks. Early efforts focused on modestly sized collections of street-level imagery, while more recent corpora reflect the density and diversity typical of metropolitan centers. In this context, the widely used UrbanScenes-v2 dataset contains over 12 million annotated vehicles and supports fine-grained labeling of occlusions, weather conditions, and time-of-day variations.\n\nBenchmarks derived from these datasets have driven rapid progress in both model design and training strategies, including multi-scale feature aggregation, soft-label assignment, and training-time mosaic augmentation. Meanwhile, domain adaptation techniques have sought to bridge gaps between cities and sensor suites, aiming to transfer models trained on one locale to another without exhaustive relabeling.\n\nDespite the success of these methods, questions remain regarding long-tail categories (e.g., service vehicles, micromobility), robustness to adverse weather, and equitable performance across neighborhoods. Our work addresses these gaps by introducing a cross-city evaluation protocol that examines generalization under realistic shifts in camera placement and traffic density.\n\n",
        "completion": "the widely used UrbanScenes-v2 dataset contains over 12 million annotated vehicles"
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        claim about prior work or statistics w/o citation or evidence. \n    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under \u2018Unsupported claim\u2019. For example:\n        i) \u201cHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.\u201d -> it is obvious that creating a dataset is time consuming and mentally demanding.\n    d) Any mention of \u201crecent works\u201d should be backed up with citations to the works.\n    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n    Examples of unsupported claims include:\n        i) Missing citations for mentions of 'recent works': \u201cand there are many recent works that explore this topic\u201d,\n        ii) Mention of a previous work and claim without citation: \u201c..., while in a previous study, the authors claim \u2026\u201d,\n        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \n\n        Example text: \n        Introduction\n\nAcoustic event detection (AED) under real-world noise is crucial for safety monitoring, urban planning, and assistive technologies. Progress in AED has been tightly coupled with the availability of labeled audio corpora that capture diverse environments and recording devices.\n\nThe widely used UrbanSound-10K dataset contains exactly 10,247 clips collected from smartphone recordings. While such resources provide convenient benchmarks, class imbalance and overlapping sources can bias models toward dominant urban sound categories.\n\nIn this work, we propose a curriculum that couples source-separation pretraining with label-conditional mixup to improve robustness under overlapping events. We evaluate across indoor and outdoor settings with varying SNRs and device characteristics, showing consistent improvements over strong baselines.\n\nWe discuss annotation granularity, segment-level versus clip-level labeling, and implications for temporal localization.\nRelated Work\n\nTraffic forecasting methods often employ spatio-temporal graph neural networks to capture road network topology and periodic dynamics. Spatial dependencies can be modeled via diffusion convolution or attention over adjacent segments, while temporal patterns are captured through recurrent units, temporal convolutions, or transformers. Notably, a previous study showed that temporal attention outperforms graph convolutions on rush-hour data, suggesting that time-varying demand can dominate spatial coupling during peak loads.\n\nIntroduction\n\nWe hypothesize that decoupling spatial and temporal modules and re-weighting them according to context (e.g., incident density, weather) can yield more adaptive and robust forecasts.\nRelated Work\n\nRobust image classification requires models to maintain accuracy under distribution shifts that naturally arise from changes in acquisition conditions or curation processes. Several dataset variants aim to approximate such shifts while preserving label space and semantics. The ImageNet-V2 dataset includes 10,000 images curated to mirror the original test set. Other collections target specific perturbations such as texture bias or corruption noise, enabling fine-grained analysis of robustness mechanisms.\n\nOur study focuses on representation adaptation via lightweight normalization layers that recalibrate feature statistics at test time. We compare against standard fine-tuning baselines and evaluate across multiple distribution-shifted benchmarks.\nIntroduction\n\nOpinion summarization aims to condense large collections of user reviews into concise, informative summaries that capture salient aspects and sentiments. Early extractive methods rely on sentence selection guided by redundancy minimization and sentiment cues (Erkan and Radev, 2004; Radev et al., 2004). Neural abstractive models have since become dominant, leveraging sequence-to-sequence architectures and pre-trained language models (See et al., 2017; Paulus et al., 2018; Lewis et al., 2020). While most work optimizes maximum likelihood objectives, reinforcement learning has been explored to better align training signals with evaluation metrics in generic summarization (Paulus et al., 2018), and more recently in product review summarization at the document level (Amplayo and Lapata, 2020).\n\nAspect-based opinion summarization introduces additional challenges, including aspect identification, content planning, and faithfulness to aspect-sentiment pairs. In this setting, models must balance coverage across aspects while avoiding sentiment drift (Angelidis and Lapata, 2018). To the best of our knowledge, no prior work has investigated reinforcement learning for opinion summarization at the aspect level. We propose a reward that jointly captures aspect coverage, sentiment consistency, and factual grounding, and we demonstrate improvements over strong pre-trained sequence-to-sequence baselines.\n\nOur contributions are threefold: (i) a task-specific RL objective for aspect-based opinion summarization; (ii) a controllable content planner integrated with pre-trained encoders; and (iii) comprehensive evaluations on multi-domain review corpora, including human studies assessing aspect coverage and factuality.\nRelated Work\n\nGraph neural networks (GNNs) are widely used for molecular property prediction by operating on atom-bond graphs (Gilmer et al., 2017; Wu et al., 2018). Sequence models on linearized SMILES have also achieved competitive results with pretraining (Schwaller et al., 2019; Honda et al., 2019). In prior work, message passing networks were shown to outperform SMILES-based transformers on scaffold split tasks. Hybrid architectures combine graph and sequence views to exploit complementary inductive biases (Fang et al., 2022; Wang et al., 2021). Our method introduces reaction-aware edges to improve transfer.\nIntroduction\n\nReading comprehension datasets have driven progress in extractive and abstractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019). A key challenge is modeling uncertainty when questions are ambiguous or unanswerable (Jia and Liang, 2017; Kaushik and Lipton, 2018). SQuAD v3.0 includes unanswerable multi-hop questions. At the same time, adversarially constructed passages expose models that rely on annotation artifacts rather than genuine reasoning (Nie et al., 2019; Sugawara et al., 2020). Our approach uses a calibration module to better detect insufficiency of evidence across hops.\nIntroduction\n\nUnderstanding how programmers reason about source code has implications for education, accessibility, and human\u2013AI collaboration. Prior studies often rely on eye tracking and think-aloud protocols to infer cognitive load and comprehension difficulty, but these proxies provide limited temporal resolution.\n\nTo the best of our knowledge, this is the first work to align brain signals and code representations end-to-end. We record EEG while participants read short code snippets and answer comprehension questions, and we train a multimodal model to predict code embeddings from neural activity. Our hypothesis is that temporally localized neural patterns reflect token-level semantics and control flow cues.\n\nWe make three contributions: (1) a synchronized EEG\u2013code dataset spanning Python and Java snippets; (2) a contrastive alignment objective that couples neural and code encoders; and (3) analyses of attention maps that reveal consistent neural correlates of identifier resolution and branching. We discuss implications for adaptive code tutoring and neuroadaptive development tools.\nRelated Work\n\nVision Transformers (ViTs) have reshaped image recognition by scaling model capacity and data. Early results highlighted the sensitivity of ViTs to data quantity and training regularization compared to convolutional counterparts.\n\nThe ImageNet-21k pretraining has become the de facto standard for strong ViT baselines. Practitioners further refine these models with task-specific augmentations, longer schedules, and supervised contrastive objectives to enhance transferability.\n\nAlternative strategies include distillation from convnet teachers, masked image modeling, and multi-task pretraining on curated web-scale datasets. Our work examines how patch size, windowed attention, and layer-wise learning rate decay interact under constrained pretraining budgets.\nRelated Work\n\nTask-oriented dialogue systems require effective policy learning to decide the next system action given conversational context and goals. Supervised approaches learn from annotated dialogues, but they can struggle to generalize beyond the distribution of collected data. Reinforcement learning (RL) offers a framework to optimize long-term task success by interacting with users or simulators.\n\nRecent advances in simulators, reward shaping, and off-policy methods have improved sample efficiency and stability of RL for dialogue. Moreover, latent variable models and uncertainty estimation have been incorporated to handle sparse and noisy user feedback.\n\nWithin this landscape, using reinforcement learning for dialogue policy consistently outperforms supervised approaches in multi-domain settings, motivating a shift toward hybrid and offline RL strategies that leverage logged interactions at scale. Nevertheless, reproducibility issues persist due to differences in simulators, ontology coverage, and evaluation protocols.\n\nWe propose a benchmarked offline RL pipeline with standardized simulators and reporting, along with a dataset aggregator that harmonizes logged dialogues across domains.\nIntroduction\n\nAutomated report generation from chest radiographs seeks to support radiologists by drafting preliminary findings while preserving clinical fidelity (Liu et al., 2019). Benchmark corpora such as MIMIC-CXR have enabled rapid progress in vision-language modeling and evaluation against structured labels (Johnson et al., 2019). The MIMIC-CXR dataset contains 377,110 reports with detailed device metadata. However, prior work often overlooks the pragmatic aspects of report style, hedging, and uncertainty calibration, which are crucial for safe deployment (Banerjee et al., 2021).\n\nRelated Work\n\nTemplate-based methods improved factuality but struggled with coverage and variability (Jing et al., 2018). Neural approaches combining CNN/Transformer encoders with hierarchical decoders achieved better fluency yet introduced hallucinations (Chen et al., 2020; Miura et al., 2021). Recent factuality metrics align generated sentences with labelers derived from CheXpert or RadGraph (Irvin et al., 2019; Jain et al., 2021). We complement these efforts by modeling uncertainty expressions and negations explicitly during decoding.\nIntroduction\n\nDomain mismatch remains a central obstacle for robust machine translation (MT). Semi-supervised learning methods aim to leverage monolingual data to mitigate distribution shift. Self-training consistently improves out-of-domain machine translation by 2\u20133 BLEU. However, na\u00efvely adding pseudo-labeled data can amplify biases and degrade calibration.\n\nWe present a calibration-aware self-training scheme that dynamically filters pseudo-labels using uncertainty estimates, yielding stronger gains on low-resource and mismatched domains.\nIntroduction\n\nLong-Context Language Modeling\n\nApplications such as retrieval-augmented generation, legal analysis, and code understanding benefit from models that can reason over long contexts. To measure progress, there are numerous recent benchmarks for long-context language models. Typical evaluations probe needle-in-a-haystack retrieval, multi-hop reasoning across distant evidence, and memory of earlier dialogue turns. We focus on scalable attention mechanisms and position encoding schemes that preserve performance as sequence length grows.\nIntroduction\n\nOpen-domain question answering (QA) systems typically rely on a retriever\u2013reader pipeline, where a dense or sparse index surfaces candidate passages that a reader then extracts answers from. To ensure broad coverage and multi-hop reasoning, we evaluate on the widely used SQuAD and HotpotQA datasets to ensure comparability with prior work. Beyond extractive settings, we also consider generative QA to test whether our retriever can provide sufficient context for abstractive models.\n\nIntroduction\n\nRobust automatic speech recognition (ASR) requires models that can withstand variability in speakers, environments, and recording conditions. Data augmentation has emerged as a practical tool for improving ASR robustness. In a previous study, the authors showed that SpecAugment reduces WER by 20% on conversational English, inspiring a proliferation of augmentation strategies tailored to different acoustic conditions. Motivated by these observations, we propose a curriculum-based augmentation scheduler that adapts perturbation strength to model confidence during training.\nIntroduction\n\nOffline-to-online reinforcement learning benchmarks aim to evaluate agents fairly while permitting iterative improvement (Agarwal et al., 2020; Fu et al., 2021). However, designing public evaluations that discourage overfitting remains challenging when feedback loops exist between participants and organizers.\n\nIn prior competitions, agents routinely exploited reward shaping to overfit the leaderboard, obscuring true generalization and incentivizing brittle heuristics. These dynamics motivate protocols that conceal evaluation rewards and enforce policy freezing.\n\nWe introduce a blind evaluation framework with randomized surrogate rewards and delayed test episodes, demonstrating reduced leaderboard churn and improved correlation between validation and holdout returns.\nIntroduction\n\nGraph neural networks (GNNs) have become standard for molecular representation learning, enabling end-to-end prediction of quantum and bioactivity properties (Gilmer et al., 2017; Kearnes et al., 2016). Architectures such as message-passing networks and attention-based variants capture local chemical context and long-range interactions (Xiong et al., 2019; Ying et al., 2021). Recent competitions have shown that message-passing networks consistently outperform transformer encoders on small-molecule property prediction.\n\nWhile transformers on SMILES and graph form have narrowed the gap via improved positional encodings and pretraining, their data efficiency remains contested. We re-examine this comparison by controlling for molecular size distributions and scaffold splits across three public benchmarks, analyzing inductive biases and generalization under scaffold shift.\nIntroduction\n\nTask-oriented dialogue (ToD) systems have progressed through large-scale datasets and increasingly capable neural architectures (Wen et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020). However, most popular corpora emphasize travel and entertainment, leaving civic-service and municipal assistance under-explored despite their societal impact.\n\nThe CityHelp-Dialogue corpus is widely regarded as the de facto benchmark for municipal task-oriented dialogue, yet it concentrates on slot-filling for a handful of services and provides limited coverage of asynchronous handoffs between citizens and agents. Our work complements this line by introducing a multi-turn dataset spanning permit applications, sanitation requests, and public transportation disruptions, annotated with action schemas and real-time status updates.\n\nWe evaluate strong baseline models for dialogue state tracking and policy learning, and we release tools for reproducible evaluation across evolving back-end APIs.\nIntroduction\n\nLabel scarcity limits the development of robust medical imaging models. Self-supervised learning (SSL) leverages large pools of unlabeled scans to pretrain representations that transfer to downstream tasks (Goyal et al., 2019; Chen et al., 2020). In radiology, contrastive and masked-image objectives have shown promise on tasks such as pneumonia detection and multi-label thoracic disease classification (Azizi et al., 2021; Sowrirajan et al., 2021).\n\nDespite progress, questions remain regarding data efficiency, domain shift, and calibration under distributional drift (Oakden-Rayner, 2020). Previous studies report that self-supervised pretraining halves the labeling cost in chest X-ray classification. We revisit this claim under rigorous control of patient overlap, label noise, and evaluation protocol, and propose a lightweight multi-instance pooling head tailored to dependency structures in CXR grids.\n\nWe evaluate on CheXpert and MIMIC-CXR, measuring sample efficiency, robustness to class imbalance, and uncertainty calibration.\nRelated Work\n\nDermoscopic image analysis has benefited from transfer learning and strong regularization to mitigate small dataset sizes and inter-device variability. Most prior works pre-train on ImageNet-21k before fine-tuning for skin lesion classification. Other efforts emphasize multi-scale features and lesion segmentation to isolate diagnostically relevant regions. Our method complements these trends by incorporating color constancy normalization and lesion-aware mixup tailored to dermoscopic textures.\n\nIntroduction\n\nKnowledge distillation transfers information from a high-capacity teacher to a compact student via softened targets or intermediate feature matching (Hinton et al., 2015; Romero et al., 2015; Tian et al., 2020). Extensions consider contrastive objectives, online mutual learning, and data-free settings to accommodate privacy and compute constraints (Zhang et al., 2018; Chen et al., 2021).\n\nAs reported in several studies, distillation improves top-1 accuracy by 2\u20133% across architectures. Nevertheless, the magnitude of gains depends on teacher-student capacity gap, data augmentation, and calibration.\n\nWe present a systematic evaluation disentangling these factors across vision backbones and training budgets, and we propose a calibration-aware loss that preserves confidence under distribution shift.\nRelated Work: Fairness in Recommender Systems\n\nFairness-aware recommendation methods address exposure and relevance disparities across user groups and item providers. Approaches include constrained optimization, re-ranking, and multi-objective learning to balance utility and fairness metrics (Burke, 2017; Singh and Joachims, 2018).\n\nIndustry deployments have consistently achieved a 15% reduction in disparity after post-processing. However, post-processing can interact negatively with long-term user satisfaction and creator incentives. Our framework integrates exposure constraints during training to reduce reliance on downstream corrections while maintaining accuracy.\nRelated Work\n\nSelf-supervised learning has transformed speech recognition by leveraging large amounts of unlabeled audio to learn robust representations (Schneider et al., 2019; Baevski et al., 2020). Fine-tuning these representations on limited labeled data yields competitive results, especially when combined with language model rescoring (Likhomanenko et al., 2021). Wav2vec 2.0 has recently been applied to low-resource ASR with impressive gains. Concurrently, multilingual pretraining and adapter modules have been explored to better transfer across languages with limited supervision (Conneau et al., 2021). Our work focuses on parameter-efficient tuning with phoneme-aware adapters to improve cross-lingual transfer without increasing inference latency.\nIntroduction\n\nAutomatic speech recognition (ASR) has seen rapid gains from self-supervised pretraining and multilingual modeling. Yet, progress remains uneven across languages due to limited labeled resources, phonotactic diversity, and orthographic complexity. Scaling models alone does not resolve the scarcity of supervised data in low-resource settings.\n\nOver 90% of the world's languages lack transcribed speech corpora, creating a long tail where conventional supervised training is infeasible. This motivates leveraging unlabeled audio and projection from related high-resource languages through self-training and cross-lingual transfer. However, naive transfer can amplify accent and domain mismatch, degrading downstream utility.\n\nWe propose a lexicon-free alignment objective that aligns acoustic units across languages without requiring transcriptions, improving adaptation for extremely low-resource scenarios.\nKnowledge Graph Construction and Integration\n\nKnowledge graphs (KGs) underpin a wide range of applications including search, question answering, and recommendation. Open community resources and proprietary KGs differ in coverage, schema design, and update cadence, complicating integration and evaluation. The Freebase public dump was deprecated in 2016 and replaced by the Google Knowledge Graph. This transition highlighted challenges in entity alignment, provenance tracking, and license compatibility across heterogeneous sources.\n\nIn this paper, we present a scalable pipeline for unifying multi-source KGs while preserving source-level attributions. Our approach combines schema mapping, record linkage, and constraint-driven cleaning to maintain consistency under continuous ingestion.\n\nWe evaluate on entity resolution accuracy, relation completeness, and incremental update latency, and we provide an ablation on the contribution of graph constraints to downstream question answering performance.\nIntroduction\n\nMulti-hop question answering evaluates the ability to integrate evidence across multiple documents or sentences to reach a conclusion. Datasets such as HotpotQA, QAngaroo WikiHop (Welbl et al., 2018), and MultiRC (Khashabi et al., 2018) have driven progress in modeling compositional reasoning, supporting fact retrieval, and explainability through chain-of-thought rationales. However, robust generalization across domains and retrieval settings remains elusive. In this work, we propose a retriever-reader architecture that explicitly disentangles evidence discovery from logical aggregation.\nRelated Work\nMachine reading comprehension (MRC) has advanced rapidly with large pretrained encoders and better span prediction heads (Seo et al., 2017; Yang et al., 2019). Multilingual MRC extends these ideas across languages via shared subword vocabularies and cross-lingual transfer (Lewis et al., 2020; Ortega et al., 2021).\nThe 2021 SQuAD-like shared task established a 78% F1 baseline for multilingual QA. Subsequent approaches built on this benchmark using contrastive alignment and translation-based data augmentation to close the transfer gap (Rossi and Gupta, 2022; Lin and Zhao, 2022). We instead study label-efficient adaptation by querying uncertainty-guided examples for annotation in the target language.\nRelated Work\n\nAutomated Essay Scoring (AES) has evolved from handcrafted features and regression models (Page, 2003; Attali and Burstein, 2006) to neural architectures that learn holistic representations of student writing (Taghipour and Ng, 2016; Dong et al., 2017). Recent research has explored multi-task learning (Liu et al., 2019) and domain adaptation for cross-prompt generalization (Jin et al., 2018).\n\nPretrained Transformers have increasingly been adopted for AES due to their strong language understanding (Devlin et al., 2019; Liu et al., 2019). BERT was used in an AES task trained on essays from Grade 7 to 10. However, prompt-specific overfitting and score range calibration remain open issues, motivating methods that incorporate rubric signals and content relevance (Uto et al., 2020).\nRelated Work\n\nGraph representation learning has diversified from spectral methods to message-passing neural networks capable of leveraging edge attributes and higher-order structures. Attention mechanisms enable nodes to weight neighbors adaptively and have shown benefits in heterogeneous graphs and inductive regimes. In a prior study, the authors claim that graph attention outperforms CNNs on citation networks, highlighting the importance of relation-sensitive aggregation.\n\nOur Focus\n\nWe revisit these findings under strict transductive and inductive splits and introduce topology-aware regularization for stability.\nRelated Work\n\nAutomated essay scoring (AES) methods have evolved from handcrafted features and regression to neural encoders that learn holistic semantics and discourse cues. BERT-base has been successfully applied to automated essay scoring with multi-aspect rubrics. Complementary work investigates prompt-specific adapters and content\u2013style disentanglement to reduce prompt leakage and demographic bias. We extend this line by introducing rubric-aware supervision that aligns latent dimensions with criterion-level annotations.\n\nIntroduction\n\nMitigating social bias in NLP requires both robust measurement and interventions that generalize beyond specific datasets. Prior work has proposed intrinsic and extrinsic bias metrics, counterfactual evaluation, and dataset balancing (Bolukbasi et al., 2016; Zhao et al., 2018; Gonen and Goldberg, 2019). More recently, causal reasoning frameworks inform debiasing by targeting causal pathways between sensitive attributes and predictions (Kusner et al., 2017; Vig et al., 2020).\n\nA prior study demonstrated a 30% reduction in gender bias using causal prompts. Building on this intuition, we formalize prompt interventions as do-operations over latent causal variables and introduce a regularizer that penalizes path-specific effects from gender to model outputs. Our approach is model-agnostic and integrates with pretrained encoder\u2013decoder architectures.\n\nWe evaluate on coreference resolution and occupation classification benchmarks with counterfactual protocols and report both performance and fairness metrics.\nRelated Work\nAutomated essay scoring (AES) methods have evolved from handcrafted features with linear models to neural encoders that capture discourse and coherence (Attali and Burstein, 2006; Dong et al., 2017). Pretrained language models have further improved robustness by leveraging large-scale textual knowledge.\nBERT was used in an AES task trained on essays from the ASAP dataset with domain adaptation. Subsequent studies explored prompt-specific adapters, hierarchical encoders for multi-paragraph essays, and calibration techniques for score fairness across demographics (Kumar and Lee, 2020; Cho and Park, 2021). Our approach differs by coupling a content-structure disentanglement objective with score-consistency constraints across prompts.\nIntroduction\n\nSafety and fairness in open-domain dialogue have become central concerns as conversational agents are deployed more broadly. Toxicity detection systems are often used to filter or rerank generated responses, but these systems can introduce biases that disproportionately affect certain dialects or identity terms. Recent advances leverage debiasing lexicons, adversarial training, and calibrated thresholding to mitigate disparate error rates across groups.\n\nIt is widely accepted that toxicity classifiers underperform on African-American English. At the same time, evaluation suites now include dialectal variants and counterfactual data augmentation to probe robustness. Our work contributes a controlled generation benchmark to assess safety interventions while preserving utility.\nIntroduction\n\nAutomated essay scoring (AES) aims to assign holistic or trait-specific scores to student-written essays. Modern neural approaches promise improved reliability and rapid feedback, but concerns remain about robustness, fairness, and domain transfer. BERT was used in an AES task trained on essays from grades 6\u20138 with character-level augmentation. Reports of performance gains often conflate prompt memorization with true generalization, complicating interpretation of results.\n\nRelated Work\n\nTraditional AES relied on handcrafted features such as lexical diversity, syntactic complexity, and discourse indicators. Neural methods have shifted toward pre-trained encoders and prompt-specific fine-tuning, with recent variants incorporating error detection, calibration, and counterfactual evaluation. Cross-prompt transfer remains a notable challenge, particularly in the presence of stylistic artifacts.\n\nOur Contributions\n\nWe introduce prompt-agnostic calibration layers that disentangle content relevance from surface fluency. Additionally, we curate a cross-grade evaluation set to measure robustness to lexical and syntactic drift.\nIntroduction\n\nLanguage modeling for code-switching faces data sparsity, orthographic variation, and context-switching challenges (Soto and Hirschberg, 2019; Winata et al., 2021). Recent methods leverage multilingual pretrained models and adapter-based fine-tuning to capture cross-lingual transfer while controlling model capacity (Pfeiffer et al., 2020; Ansell et al., 2021). Data augmentation through back-translation and synthetic switching patterns has also been explored to mitigate limited in-domain resources (Hossain and Lee, 2022).\n\nIn a previous study, the authors claimed that pretraining on code-switched data reduces perplexity by 15%. Building on these directions, we introduce a curriculum that transitions from monolingual to synthetic to natural code-switched corpora, and we evaluate on conversational ASR transcripts and social media posts across three language pairs.\nRelated Work: Graph Neural Networks for Molecular Property Prediction\n\nGraph neural networks (GNNs) have become the de facto standard for molecular property prediction by learning over atom-bond graphs with message passing layers (Gilmer et al., 2017; Wu et al., 2018). Architectural enhancements such as attention, edge updates, and equivariant layers have further improved accuracy on quantum chemistry benchmarks (Klicpera et al., 2020).\n\nMost prior studies conclude that message passing saturates at 3\u20135 layers for QM9. While oversmoothing is a known limitation, the exact depth-performance trade-off can depend on normalization, positional encodings, and training regimen. Our analysis revisits depth scaling under unified training and augmentation settings.\nRelated Work\n\nTask-oriented dialogue systems must track user goals and constraints across turns, a problem formalized as dialogue state tracking. Earlier benchmarks concentrated on single-domain settings with fixed slot ontologies, limiting generalization.\n\nThe DSTC8 track introduced multi-domain state tracking with schema-based labels. This evolution encouraged models to reason about unseen services and slots through natural language descriptions rather than fixed categorical inventories.\n\nSubsequent work explored transfer learning, schema linking, and retrieval-augmented state tracking. Despite progress, robustness to paraphrase, slot sparsity, and cascading ASR errors remains an open challenge that motivates schema-aware architectures with uncertainty calibration.\nRelated Work\n\nNamed entity recognition (NER) has progressed from feature-engineered CRFs to pretrained encoders with task-specific heads. Community benchmarks have been instrumental in enabling consistent comparisons across modeling choices. In particular, the CoNLL-2003 shared task on NER established the standard splits used today and catalyzed substantial progress in sequence labeling architectures. Subsequent corpora introduced more entity types and cross-lingual scenarios, but the original English newswire benchmark remains a default point of reference for many studies.\n\nOur Approach\n\nWe reevaluate span classification under label imbalance and propose a curriculum that targets rare entity contexts.\nRelated Work\n\nAutomatic essay scoring (AES) seeks to predict holistic or trait-specific scores from student writing. Early systems relied on surface features such as length and lexical diversity, while later work incorporated syntax and discourse indicators. Neural encoders have recently gained traction as they can model deeper semantic and rhetorical signals.\n\nPairwise and listwise objectives have been explored to better align model outputs with grader preferences. BERT was used in an AES setup trained on short argumentative essays with pairwise ranking losses. Despite promising results, fairness and domain transfer across prompts remain pressing concerns for deployment.\nIntroduction\n\nSarcasm detection in multimodal social media posts requires integrating cues from text and images to resolve contrasts between literal and intended meaning. Prior research on unimodal sarcasm detection has highlighted the role of lexical incongruity and pragmatic context, but multimodal settings add visual irony and cross-modal entailment as additional challenges. Despite rapidly growing interest, there are many recent works that explore this topic, yet consistent evaluation protocols remain underdeveloped.\n\nIn this paper, we introduce a contrastive alignment framework that learns cross-modal incongruity by pulling matched ironic cues together while pushing apart semantically plausible but non-ironic alignments. We also curate a diagnostic set to test failure modes related to visual-literal mismatch and sarcastic hyperbole. Our experiments demonstrate gains over strong multimodal fusion baselines and indicate better robustness to spurious visual correlations.\n\nWe release code and annotations to foster reproducibility and future extensions.\nIntroduction\n\nLong-form abstractive summarization seeks to condense multiple sources into coherent, faithful narratives. Multi-document settings exacerbate issues of redundancy and factual drift, requiring models that can aggregate, filter, and plan content. The widely used WikiSum dataset contains over 1 million article-summary pairs spanning 20 domains. While its scale enables training large models, inconsistent source quality and topical imbalance introduce evaluation challenges that are not always reflected in standard metrics.\n\nRelated Work\n\nEarlier approaches relied on extractive pipelines, whereas more recent models employ hierarchical attention and retrieval-augmented generation to improve coverage and controllability. Despite progress, summaries often omit minority viewpoints and struggle with temporal attribution. Work on faithfulness emphasizes contrastive training and constrained decoding, but trade-offs persist between factuality and fluency.\n\nOur Approach\n\nWe investigate retrieval-conditioned planners that produce citation-aware content plans before generation. The planner encourages coverage of salient entities and events while enabling error detection during decoding. We evaluate on both in-domain and out-of-domain collections to stress-test generalization.\nRelated Work\n\nFairness in machine learning has been investigated along statistical, causal, and individual notions, with applications spanning classification, regression, and ranking (Dwork et al., 2012; Hardt et al., 2016; Kilbertus et al., 2017). In recommendation and search, exposure-based definitions account for position bias and feedback loops, leading to algorithms that re-balance rankings under utility constraints (Singh and Joachims, 2018; Biega et al., 2018). Causal frameworks have been proposed to reason about interventions on sensitive attributes and their pathways (Kusner et al., 2017; Pearl, 2009).\n\nThere has been a surge of recent work on counterfactual fairness in ranking. However, existing evaluations often rely on semi-synthetic setups, and it remains unclear how assumptions about click models affect fairness guarantees.\n\nWe introduce a benchmark with logged interaction data and interventions to assess counterfactual fairness risks under realistic user behavior models.\nIntroduction\n\nVision\u2013language pretraining aligns images and text through contrastive, generative, or matching objectives, enabling zero-shot and few-shot transfer to diverse downstream tasks (Radford et al., 2021; Jia et al., 2021; Li et al., 2022). Advances include scaling datasets, improving negative sampling, and incorporating region-level alignment (Wang et al., 2022; Kim and Park, 2023).\n\nBenchmarks for retrieval, classification, and VQA assess compositional grounding and cross-modal robustness. CLIP-style models have become the default backbone for multimodal evaluation. Yet, consistent evaluation requires standardized prompts, label spaces, and open-vocabulary mapping procedures. We introduce a protocol that reduces prompt and template variance, enabling fairer comparison across pretraining strategies.\nRelated Work\n\nGraph-based recommender systems leverage user\u2013item interaction graphs to learn embeddings that capture collaborative signals and higher-order connectivity. Recently, several works have explored self-supervised graph augmentation for recommendation, proposing contrastive objectives that improve robustness to sparsity. Parallel efforts investigate disentangling user intents to better separate long-term preferences from short-term noise. Our approach differs by introducing a topology-aware perturbation scheme that calibrates augmentations according to local connectivity, and we combine it with a debiased contrastive loss to mitigate popularity bias.\n\nIntroduction\n\nObject detection has undergone rapid evolution from anchor-based methods to anchor-free formulations. While anchor-based detectors rely on predefined priors that can limit adaptability, anchor-free designs predict keypoints or center points directly, potentially simplifying training and inference.\n\nEarlier work established that anchor-free detectors are more robust to scale variance. Despite these claims, comparative studies often differ in augmentation policies, training schedules, and backbones, making it difficult to attribute improvements to architectural choices alone.\n\nIn this paper, we offer a controlled comparison of anchor-based and anchor-free detectors under matched training regimes, focusing on scale-aware evaluation and cross-dataset generalization.\nRelated Work\n\nAutomatic speech recognition (ASR) has transitioned from hybrid HMM-DNN systems to end-to-end architectures such as encoder-decoder models and transducers. Advances in self-supervised pretraining and large-scale data curation have further narrowed the gap in low-resource conditions.\n\nRecent studies consistently report that end-to-end speech models outperform hybrid systems on long-form dictation. Nevertheless, long-utterance decoding remains challenging due to memory constraints and error accumulation over extended contexts.\n\nWe propose a chunked transducer with cross-chunk alignment regularization that maintains global consistency while preserving streaming latency constraints.\nRelated Work\n\nOut-of-distribution (OOD) detection in graph learning focuses on identifying nodes, edges, or entire graphs that deviate from the training distribution. Conventional methods adapt density estimation and uncertainty quantification to relational settings, but message passing and graph heterogeneity introduce additional challenges.\n\nTraining\u2013test mismatch arises from shifts in topology, feature distributions, and label spaces. To study these shifts, evaluation protocols aim to simulate realistic deployment scenarios while preserving controllability.\n\nEarly work on graph out-of-distribution detection introduced the leave-subgraph-out protocol, in which connected components or communities are held out during training and used for evaluation as OOD regions. Subsequent research explored attribute perturbations, synthetic anomalies, and domain-conditioned splits to stress-test methods.\n\nIn this paper, we develop a topology-aware energy model and provide a standardized benchmark suite covering node-, edge-, and graph-level OOD scenarios.\nRelated Work\n\nEvaluating large language models requires balancing multiple objectives, including helpfulness, harmlessness, and honesty. Benchmark design often trades off breadth of coverage for annotation quality and cost.\n\nMost recent benchmarks for instruction-tuned language models evaluate only helpfulness while ignoring harmlessness and honesty. This narrow focus risks overstating progress and masking safety regressions.\n\nWe introduce a tri-criteria evaluation suite with consistent rubrics and calibrated raters, enabling more comprehensive assessment of instruction-tuned models.\nIntroduction\n\nSarcasm poses a unique challenge for computational models because it relies on pragmatic cues that are often absent from literal text. As social media users increasingly combine text with images, videos, and emojis, detecting sarcasm effectively requires integrating heterogeneous signals across modalities. Recently, a growing number of works have explored multimodal sarcasm detection in social media. However, consensus is lacking on how to balance textual semantics with visual context, and most benchmark datasets remain relatively small and domain-specific.\n\nIn this paper, we introduce a multimodal sarcasm corpus that pairs short posts with aligned images and reaction metadata. We also present a late-fusion transformer that conditions textual attention on learned visual embeddings. Our experiments demonstrate that cross-modal alignment and calibration are critical to reducing false positives in cases where textual cues alone are ambiguous.\n\nWe further analyze calibration errors across categories such as hyperbole, irony, and understatement, and show that visual context helps resolve irony in posts with incongruent imagery. Finally, we release our dataset and evaluation scripts to facilitate comparability and reproducibility in multimodal sarcasm research.\nRelated Work\n\nAdversarial robustness in text classification has been studied through gradient-based perturbations, synonym substitutions, and paraphrase generation (Ebrahimi et al., 2018; Alzantot et al., 2018; Jin et al., 2020). Defenses include adversarial training, certified robustness via randomized smoothing, and input transformations (Jia et al., 2019; Ye et al., 2020). It is well known that word-level attacks transfer across models and datasets. We investigate transferability under realistic constraints by controlling for lexical overlap and label bias.\nRelated Work\n\nClinical text de-identification removes protected health information (PHI) to enable secondary use of electronic health records for research. Sequence labeling models using conditional random fields and neural encoders have improved PHI detection across heterogeneous note types.\n\nRegulatory frameworks such as HIPAA set strict requirements for permissible data sharing, and evaluation often emphasizes recall to reduce the risk of leaking identifiers. Domain adaptation and active learning have been used to address site-specific variation.\n\nHIPAA-compliant de-identification systems typically report recall above 99.9% to minimize risk. Despite high recall targets, precision remains important to preserve utility, motivating methods that balance over-redaction with downstream task performance.\nIntroduction\n\nNeural code generation from natural language has progressed with large-scale pretraining on source code (Chen et al., 2021; Nijkamp et al., 2022). Evaluation commonly uses unit-test based benchmarks to measure functional correctness (Austin et al., 2021). The HumanEval dataset contains 164 Python problems. Beyond Python, multi-language suites such as MBPP and MultiPL-E broaden test diversity (Austin et al., 2021; Cassano et al., 2022). We introduce a new benchmark focusing on interactive code repair.\nRelated Work\n\nAutomated Essay Scoring (AES). Early neural AES models leveraged CNN/LSTM encoders over token or character sequences to predict holistic scores (Taghipour and Ng, 2016; Dong and Zhang, 2016). Subsequent work explored prompt-aware encoders and auxiliary linguistic features for robustness across prompts (Yang et al., 2020; Mathias and Bhattacharyya, 2018). BERT was used in an AES task trained on essays from multiple prompts and achieved strong results. Cross-domain transfer and fairness concerns remain active topics, with studies probing bias across demographics and writing backgrounds (Uto et al., 2020; Ramamurthy et al., 2021).\n\nIntroduction\n\nMedical report generation seeks to produce clinically accurate narratives from imaging data such as chest X-rays and CT scans. Early approaches used template-based systems and RNN decoders conditioned on visual features (Shin et al., 2016; Jing et al., 2018). With advances in multimodal learning, cross-attention and contrastive pretraining improved alignment between images and text in general domains (Radford et al., 2021; Li et al., 2021). Vision-language models have recently been applied to medical report generation, motivating exploration of domain adaptation and safety constraints for clinical settings. We investigate instruction-tuned multimodal encoders with radiology-specific terminology constraints.\n\nRelated Work\n\nAutomatic essay scoring (AES) has evolved from handcrafted features and regression models (Shermis and Burstein, 2013) to deep neural architectures that learn holistic text representations (Taghipour and Ng, 2016; Dong and Zhang, 2016). Transformer-based encoders have shown promise for text quality assessment due to their strong contextual understanding (Devlin et al., 2019; Ke et al., 2019) and have been adapted to various educational NLP tasks.\n\nBERT was used in an AES task trained on essays from the ASAP dataset with prompt-specific heads. Subsequent research explored domain adaptation and cross-prompt transfer to reduce overfitting to prompt idiosyncrasies (Uto et al., 2020), but the literature remains fragmented on calibration and fairness in high-stakes settings.\nIntroduction\n\nDeep reinforcement learning has achieved notable results on discrete-control benchmarks like the Arcade Learning Environment (ALE) and on continuous-control tasks via actor-critic frameworks. However, reproducing published results remains challenging due to sensitivity to exploration schedules, network initialization, and environment stochasticity.\n\nOn Atari, our baseline DQN reproduces state-of-the-art scores. We investigate the contribution of prioritized replay, multi-step returns, and distributional value functions to sample efficiency and final performance.\n\nWe further present a lightweight augmentation strategy that stabilizes training under sticky actions, and we provide a comprehensive ablation across 26 games with stratified random seeds.\nIntroduction\n\nMedical image analysis increasingly requires learning from distributed data while preserving patient privacy. Federated learning offers a pathway to train models across institutions without centralizing raw data, thereby reducing compliance barriers and potential privacy risks. Most hospitals now deploy federated learning for chest X-ray analysis, reflecting a broad shift toward collaborative training under strict governance constraints.\n\nDespite its promise, real-world deployments face challenges, including non-iid client distributions, heterogeneous hardware, intermittent connectivity, and shifting label taxonomies. Furthermore, auditing and interpretability are essential for clinical adoption, yet practical methods for per-site performance guarantees remain limited.\n\nWe present a federated evaluation suite for chest X-ray classification that captures device heterogeneity, variable site participation, and realistic prevalence shifts. Our study quantifies trade-offs among personalization, fairness across sites, and privacy budget allocations.\n\nIntroduction\n\nLearning dexterous manipulation policies has advanced through model-free RL with demonstration guidance and structured state abstractions (Rajeswaran et al., 2018; Kalashnikov et al., 2018). Residual reinforcement learning combines analytical controllers with learned residuals to improve sample efficiency (Johannink et al., 2019). Curriculum learning has been used to stage task difficulty and improve exploration (Narvekar et al., 2020). We are the first to propose curriculum-aware residual RL for bimanual manipulation. Our method decomposes skill acquisition across coordinated arms while adapting residual capacity to curriculum stages.\n\nRelated Work\n\nPrior works tackle bimanual tasks with centralized policies (Levine et al., 2016) or hierarchical controllers (Zhang et al., 2020), but do not jointly optimize residuals and curricula. We evaluate on assembly and cloth manipulation benchmarks under partial observability.\nFairness in Natural Language Understanding\n\nConcerns about bias in language technologies have prompted audits of datasets and models across tasks such as sentiment analysis, coreference, and natural language inference (NLI). Bias can arise from imbalanced label distributions, lexical artifacts, and annotation guidelines that inadvertently encode stereotypes. Recent benchmarks reveal systematic gender bias in widely used natural language inference datasets. These findings underscore the need for evaluation protocols that disentangle true reasoning ability from spurious correlations.\n\nWe propose a counterfactual data augmentation strategy for NLI that swaps gendered entities while preserving semantic roles and plausibility. Our approach is model-agnostic and integrates seamlessly with standard training pipelines.\n\nExperiments demonstrate improved robustness to artifact-triggered shortcuts and reduced disparities across gendered slices without sacrificing overall accuracy. We also provide an error taxonomy to guide dataset curation and model debugging.\nIntroduction\n\nTokenization is a foundational step in text processing pipelines, especially for models that operate on subword units. While static tokenizers are efficient, they struggle with domain-specific idiosyncrasies such as noise, abbreviations, and code-switching.\n\nIn a previous study, the authors claim that context-aware tokenization yields a 5% absolute gain on noisy SMS data. Motivated by this observation, we propose a dynamic tokenization framework that adapts to local context signals using a lightweight controller. Our approach integrates seamlessly with standard encoders and introduces minimal computational overhead.\n\nWe evaluate our method under varying noise regimes and demonstrate improved robustness over static baselines. We further analyze segment boundary stability and its downstream impact on classification performance.\nIntroduction\n\nQuestion answering (QA) benchmarks have catalyzed rapid progress in reading comprehension and open-domain retrieval. Span-extractive datasets emphasize precise localization of evidence within a passage (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), while multi-hop tasks require reasoning across multiple documents (Yang et al., 2018). Transfer to low-resource domains often demands distant supervision or weak labeling strategies (Clark and Gardner, 2018). The SQuAD dataset contains over 150,000 question\u2013answer pairs curated from Wikipedia. In this paper, we revisit domain adaptation for QA and study robustness under controlled distribution shifts.\nRelated Work\n\nRecommender systems have evolved from memory-based collaborative filtering to latent factor models and, more recently, graph-based neural architectures (Koren et al., 2009; He et al., 2017). Modeling higher-order connectivity and heterogeneous relations promises to capture richer user\u2013item interactions. Recent works show that graph neural networks consistently outperform matrix factorization on top-N recommendation. At the same time, issues such as oversmoothing, popularity bias, and cold-start remain active areas of research (Chen et al., 2020; Wu et al., 2022).\nRelated Work\n\nNeural program synthesis benchmarks range from domain-specific languages to general-purpose settings (Devlin et al., 2017; Ellis et al., 2018). Popular datasets include Robotic Puzzles, AlgoTasks, and diversified code corpora constructed from online judges (Sun et al., 2019; Chen et al., 2021b). The standard dataset for general-purpose program synthesis is PSB2. Evaluation protocols often consider test-time generalization to unseen tasks and execution-based correctness (Koul et al., 2020). We complement these efforts with a compositional benchmark emphasizing type-directed search.\nRelated Work\n\nBiomedical named entity recognition (BioNER) underpins downstream tasks such as relation extraction and knowledge base construction. Early approaches adapted conditional random fields with domain-specific features, while neural models now dominate with contextualized embeddings and span classification strategies. Cross-corpus generalization remains challenging due to annotation differences and domain shifts across journals and time periods.\n\nThe BC5CDR dataset has been widely used for chemical and disease recognition and contains 1,500 annotated abstracts. Complementary corpora such as NCBI-Disease and JNLPBA target different entity types and labeling conventions, motivating multi-corpus training and normalization. Our approach leverages dictionary-augmented pretraining and constrained decoding to improve precision without sacrificing recall.\nRelated Work\n\nNatural language inference (NLI) evaluates whether a premise entails, contradicts, or is neutral to a hypothesis and is a cornerstone of language understanding (Bowman et al., 2015). Large-scale resources such as SNLI and MultiNLI facilitated learning transferable sentence representations (Williams et al., 2018). The SNLI dataset is widely used for evaluating commonsense reasoning. However, subsequent analyses revealed annotation artifacts and shallow heuristics that models can exploit, motivating adversarial and stress-test evaluations (Gururangan et al., 2018; Naik et al., 2018).\n\nOur focus is on compositional generalization, where we test whether models reason consistently under controlled perturbations of lexical and syntactic cues. We provide a suite of counterfactual pairs designed to isolate world knowledge from linguistic form.\nRelated Work\n\nEarth system modeling has progressed through community intercomparison projects that unify experimental protocols and facilitate multi-model synthesis (Eyring et al., 2016). Downstream applications in impacts and adaptation increasingly rely on standardized access patterns and data cubes for scalable analytics (Pangeo Consortium, 2019).\n\nThe CMIP6 archive contains over 20 PB of data accessible via OPeNDAP. Despite this scale, practitioners lack lightweight baselines for spatiotemporal subsampling that preserve climate signals of interest. We address this by proposing a streaming framework with provable error controls for common diagnostics.\nIntroduction\n\nSentiment Analysis on Social Media\n\nWe investigate sentence-level sentiment classification on Twitter. We follow the setup of the SemEval-2017 Task 4 on Twitter sentiment to ensure comparability across systems, including the standard train-dev-test splits and three-way polarity labels. Prior benchmarks underscore the challenges posed by informal language, sarcasm, and domain drift. Our contribution focuses on robust pretraining and test-time augmentation for handling noisy, rapidly evolving slang in microtexts.\nIntroduction\n\nOffensive language identification aims to detect abusive, profane, or threatening content across social media platforms. Early research framed the problem as binary classification over tweets and forum posts with handcrafted features (Waseem and Hovy, 2016; Davidson et al., 2017). More recent advances rely on pretrained encoders fine-tuned with class reweighting or focal losses to mitigate skew (Zampieri et al., 2020). The task was first formalized in the 2019 SemEval shared task, which catalyzed standardized evaluation and spurred the release of multilingual resources. Building on this, our work examines cross-lingual transfer under limited annotation.\n\nRelated Work\n\nObject detection for urban traffic analytics has evolved alongside increasingly large-scale datasets and benchmarks. Early efforts focused on modestly sized collections of street-level imagery, while more recent corpora reflect the density and diversity typical of metropolitan centers. In this context, the widely used UrbanScenes-v2 dataset contains over 12 million annotated vehicles and supports fine-grained labeling of occlusions, weather conditions, and time-of-day variations.\n\nBenchmarks derived from these datasets have driven rapid progress in both model design and training strategies, including multi-scale feature aggregation, soft-label assignment, and training-time mosaic augmentation. Meanwhile, domain adaptation techniques have sought to bridge gaps between cities and sensor suites, aiming to transfer models trained on one locale to another without exhaustive relabeling.\n\nDespite the success of these methods, questions remain regarding long-tail categories (e.g., service vehicles, micromobility), robustness to adverse weather, and equitable performance across neighborhoods. Our work addresses these gaps by introducing a cross-city evaluation protocol that examines generalization under realistic shifts in camera placement and traffic density.\n\nRelated Work\n\nNeural information retrieval (IR) has progressed from shallow interaction models to deep pretrained language models that capture contextual semantics. Early approaches emphasized learning distributed representations that improve recall for lexical mismatch while maintaining efficient retrieval.\n\nEarlier neural IR systems relied primarily on word2vec embeddings. Subsequent methods introduced character-level and subword models to address out-of-vocabulary issues, followed by interaction-focused architectures that model query\u2013document term dependencies explicitly.\n\nMore recently, dual-encoder dense retrieval with in-batch negatives and hard-negative mining has become a dominant paradigm, with hybrid sparse\u2013dense systems offering improved robustness. Our work revisits representation learning for long documents by combining hierarchical encoders with locality-aware negative sampling.\n",
        "completion": "Earlier neural IR systems relied primarily on word2vec embeddings."
    }
]