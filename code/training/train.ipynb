{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b36f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_def = f\"\"\"Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n",
    "    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n",
    "    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n",
    "    c) Formatting issues appear as either citations or parts of a citation.\n",
    "    Examples of formatting issues include:\n",
    "        i) Narrative citation missing year: “Vatswani et al.” -> should be “Vatswani et al. (2020)”\n",
    "        ii) Wrong citation style: “In (Vatswani et al., 2019)” -> should be “in Vatswani et al. (2019)”\n",
    "        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\"\"\"\n",
    "\n",
    "unsupp_def = f\"\"\"claim about prior work or statistics w/o citation or evidence. \n",
    "    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n",
    "    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n",
    "    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under ‘Unsupported claim’. For example:\n",
    "        i) “However, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.” -> it is obvious that creating a dataset is time consuming and mentally demanding.\n",
    "    d) Any mention of “recent works” should be backed up with citations to the works.\n",
    "    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n",
    "    Examples of unsupported claims include:\n",
    "        i) Missing citations for mentions of 'recent works': “and there are many recent works that explore this topic”,\n",
    "        ii) Mention of a previous work and claim without citation: “..., while in a previous study, the authors claim …”,\n",
    "        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \"\"\"\n",
    "\n",
    "lacksynth_def = f\"\"\"occurs when either:\n",
    "    a) The author describes or cites papers without connecting them to their own work/argument \n",
    "    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n",
    "    c) It does not articulate the author's perspective or motivation.\n",
    "    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n",
    "    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n",
    "    Examples of lack of synthesis include:\n",
    "        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n",
    "        ii)  No explanation of the cited works and relation to their own work: “Recently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).” \"\"\"\n",
    "\n",
    "coherence_def = f\"\"\"connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n",
    "    a) Sentences are not transitioned from one to another.\n",
    "    b) The relationship between sentences describing papers is implied but not explicitly stated.\n",
    "    c) Coherence issues appear only as multiple sentences.\n",
    "    Examples of coherence issues include:\n",
    "        i) Relation between mentioned works is not explicit: “Smith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).”\n",
    "        ii) Lack of transitions between sentences: “Recent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.” \n",
    "        iii) No explanation of the cited works and relation to their own work: “Recently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).” \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b1c1a2",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python sft_train_decoder_prompt_completion.py --config config.json --category \"$CAT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os \n",
    "import json \n",
    "from pathlib import Path\n",
    "\n",
    "def load_config(config_path: str) -> dict:\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def merge_dicts(base: dict, override: dict) -> dict:\n",
    "    out = dict(base)\n",
    "    for k, v in (override or {}).items():\n",
    "        out[k] = v\n",
    "    return out\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", required=True, help=\"Path to config.json\")\n",
    "parser.add_argument(\"--category\", required=True, help=\"Category name from config.json\")\n",
    "args_cli = parser.parse_args()\n",
    "\n",
    "cfg = load_config(args_cli.config)\n",
    "\n",
    "defaults = cfg.get(\"defaults\", {})\n",
    "base_model = cfg.get(\"base_model\", \"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "defaults = cfg.get(\"defaults\", {})\n",
    "cat_cfg = {...}  # the config entry for the selected category\n",
    "\n",
    "def merge_dicts(base, override):\n",
    "    out = dict(base)\n",
    "    out.update(override)\n",
    "    return out\n",
    "\n",
    "run_cfg = merge_dicts(defaults, cat_cfg)\n",
    "\n",
    "# If set in config, use it; otherwise derive from the category's data folder\n",
    "# Save into: <data_folder>/models/<category>\n",
    "train_path = run_cfg[\"train_path\"]\n",
    "data_dir = Path(train_path).resolve().parent\n",
    "\n",
    "output_subdir = run_cfg.get(\"output_subdir\", \"models\")  # configurable if you want\n",
    "run_name = run_cfg.get(\"run_name\", args_cli.category)  # configurable run folder name\n",
    "\n",
    "out_dir = str(data_dir / run_name)\n",
    "\n",
    "# Find the category block\n",
    "cat_cfg = None\n",
    "for c in cfg.get(\"categories\", []):\n",
    "    if c.get(\"name\") == args_cli.category:\n",
    "        cat_cfg = c\n",
    "        break\n",
    "if cat_cfg is None:\n",
    "    raise ValueError(f\"Category '{args_cli.category}' not found in config.json\")\n",
    "\n",
    "# Allow per-category overrides of defaults\n",
    "run_cfg = merge_dicts(defaults, cat_cfg)\n",
    "\n",
    "# Required per-category dataset paths\n",
    "train_path = run_cfg[\"train_path\"]\n",
    "dev_path = run_cfg.get(\"dev_path\")\n",
    "eval_path = run_cfg.get(\"eval_path\")\n",
    "\n",
    "eval_split = run_cfg.get(\"eval_split\", \"dev\")  # dev or eval\n",
    "eval_path_for_training = dev_path if eval_split == \"dev\" else eval_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eb8cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List, Any\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PromptCompletionDataset(Dataset):\n",
    "    def __init__(self, path: str, prompt_key: str = \"prompt\", completion_key: str = \"completion\"):\n",
    "        self.rows: List[Dict[str, Any]] = []\n",
    "        self.prompt_key = prompt_key\n",
    "        self.completion_key = completion_key\n",
    "\n",
    "        # Read entire file once; decide JSON vs JSONL\n",
    "        with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            text = f.read().strip()\n",
    "\n",
    "        if not text:\n",
    "            raise ValueError(f\"{path} is empty\")\n",
    "\n",
    "        # Heuristic: if it starts with '[' or '{', try JSON first (covers .json and pretty-printed arrays)\n",
    "        if text[0] in \"[{\":\n",
    "            try:\n",
    "                obj = json.loads(text)\n",
    "                self.rows = self._normalize_json(obj, path)\n",
    "                self._validate_rows(path)\n",
    "                return\n",
    "            except json.JSONDecodeError:\n",
    "                # Fall back to JSONL parsing below\n",
    "                pass\n",
    "\n",
    "        # JSONL fallback (robust: skips empty lines, reports bad lines clearly)\n",
    "        self.rows = []\n",
    "        with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            for lineno, line in enumerate(f, start=1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    self.rows.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    preview = line[:200].replace(\"\\n\", \"\\\\n\")\n",
    "                    raise ValueError(\n",
    "                        f\"Failed to parse JSON on line {lineno} in {path}: {e}\\n\"\n",
    "                        f\"Line preview: {preview}\"\n",
    "                    ) from e\n",
    "\n",
    "        self._validate_rows(path)\n",
    "\n",
    "    def _normalize_json(self, obj: Any, path: str) -> List[Dict[str, Any]]:\n",
    "        # Accept: list of dicts\n",
    "        if isinstance(obj, list):\n",
    "            return obj\n",
    "        # Accept: {\"data\": [...]} or {\"rows\": [...]} (common variants)\n",
    "        if isinstance(obj, dict):\n",
    "            for k in (\"data\", \"rows\", \"examples\", \"items\"):\n",
    "                if k in obj and isinstance(obj[k], list):\n",
    "                    return obj[k]\n",
    "        raise ValueError(\n",
    "            f\"{path} parsed as JSON but is not a list of examples or a dict containing a list \"\n",
    "            f\"(expected e.g. [{{...}}, ...] or {{'data': [...]}}). Got: {type(obj)}\"\n",
    "        )\n",
    "\n",
    "    def _validate_rows(self, path: str) -> None:\n",
    "        if not isinstance(self.rows, list):\n",
    "            raise ValueError(f\"{path}: expected a list of examples, got {type(self.rows)}\")\n",
    "\n",
    "        for i, r in enumerate(self.rows):\n",
    "            if not isinstance(r, dict):\n",
    "                raise ValueError(f\"{path}: example {i} is not a dict (got {type(r)})\")\n",
    "            if self.prompt_key not in r or self.completion_key not in r:\n",
    "                raise ValueError(\n",
    "                    f\"{path}: example {i} missing keys \"\n",
    "                    f\"'{self.prompt_key}' and/or '{self.completion_key}'. Keys: {list(r.keys())}\"\n",
    "                )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, str]:\n",
    "        r = self.rows[idx]\n",
    "        return {\"prompt\": r[self.prompt_key], \"completion\": r[self.completion_key]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae756215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "import torch\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PromptCompletionCollator:\n",
    "    tokenizer: Any\n",
    "    max_length: int = 2048\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, str]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Batch is a list of:\n",
    "          {\"prompt\": str, \"completion\": str}\n",
    "\n",
    "        We build:\n",
    "          input_ids      = tokenizer(prompt + completion)\n",
    "          attention_mask = usual\n",
    "          labels         = input_ids, but prompt tokens masked to IGNORE_INDEX\n",
    "        \"\"\"\n",
    "        prompts = [ex[\"prompt\"] for ex in batch]\n",
    "        completions = [ex[\"completion\"] for ex in batch]\n",
    "\n",
    "        # Important: ensure completion starts immediately after prompt\n",
    "        # (your data should already include leading space/newline if needed)\n",
    "        full_texts = [p + c for p, c in zip(prompts, completions)]\n",
    "\n",
    "        enc_full = self.tokenizer(\n",
    "            full_texts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Tokenize prompts alone to know prompt token lengths\n",
    "        enc_prompt = self.tokenizer(\n",
    "            prompts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = enc_full[\"input_ids\"]\n",
    "        attention_mask = enc_full[\"attention_mask\"]\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Mask out prompt tokens from loss\n",
    "        for i in range(len(batch)):\n",
    "            # Count non-padding tokens in prompt\n",
    "            prompt_len = int(enc_prompt[\"attention_mask\"][i].sum().item())\n",
    "            labels[i, :prompt_len] = IGNORE_INDEX\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, socket, platform\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# =========================\n",
    "# A100-tuned defaults (safe)\n",
    "# =========================\n",
    "# A100 supports bf16 very well; prefer bf16 over fp16.\n",
    "os.environ.setdefault(\"BF16\", \"1\")\n",
    "os.environ.setdefault(\"FP16\", \"0\")\n",
    "\n",
    "# ---- Settings from config ----\n",
    "model_name = run_cfg.get(\"model_name\", base_model)\n",
    "eval_path = eval_path_for_training\n",
    "\n",
    "print(\"Settings (from config):\")\n",
    "print(f\"Category: {args_cli.category}\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Train data: {train_path}\")\n",
    "print(f\"Eval data: {eval_path}\")\n",
    "print(f\"Output dir: {out_dir}\")\n",
    "\n",
    "max_length = int(run_cfg.get(\"max_length\", 2048))\n",
    "per_device_train_batch_size = int(run_cfg.get(\"per_device_train_batch_size\", 1))\n",
    "per_device_eval_batch_size = int(run_cfg.get(\"per_device_eval_batch_size\", per_device_train_batch_size))\n",
    "gradient_accumulation_steps = int(run_cfg.get(\"gradient_accumulation_steps\", 16))\n",
    "\n",
    "learning_rate = float(run_cfg.get(\"learning_rate\", 2e-4))\n",
    "num_train_epochs = float(run_cfg.get(\"num_train_epochs\", 1))\n",
    "warmup_ratio = float(run_cfg.get(\"warmup_ratio\", 0.03))\n",
    "\n",
    "logging_steps = int(run_cfg.get(\"logging_steps\", 25))\n",
    "save_steps = int(run_cfg.get(\"save_steps\", 500))\n",
    "save_reason = str(run_cfg.get(\"save_reason\", \"config-driven run\"))\n",
    "\n",
    "bf16 = bool(run_cfg.get(\"bf16\", True))\n",
    "fp16 = bool(run_cfg.get(\"fp16\", False))\n",
    "\n",
    "lora_r = int(run_cfg.get(\"lora_r\", 16))\n",
    "lora_alpha = int(run_cfg.get(\"lora_alpha\", 32))\n",
    "lora_dropout = float(run_cfg.get(\"lora_dropout\", 0.05))\n",
    "\n",
    "torch_compile = bool(run_cfg.get(\"torch_compile\", False))\n",
    "use_flash_attn = bool(run_cfg.get(\"flash_attn\", False))\n",
    "\n",
    "num_workers = int(run_cfg.get(\"dataloader_num_workers\", 4))\n",
    "max_grad_norm = float(run_cfg.get(\"max_grad_norm\", 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab21440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load tokenizer ----\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# ---- Load model with 4-bit quantization (QLoRA) ----\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\" if bf16 else \"float16\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"bfloat16\" if bf16 else (\"float16\" if fp16 else None),\n",
    ")\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f35e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: flash-attn v2 (if installed and supported)\n",
    "if use_flash_attn:\n",
    "    try:\n",
    "        model.config.attn_implementation = \"flash_attention_2\"\n",
    "        print(\"Enabled flash_attention_2\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not enable flash_attention_2:\", e)\n",
    "\n",
    "print(f\"Model loaded with {sum(p.numel() for p in model.parameters())} parameters.\")\n",
    "\n",
    "print(\"Prepping for kbit training and adding LoRA adapters...\")\n",
    "# Prepare for k-bit training + add LoRA adapters\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Optional: torch.compile (PyTorch 2.x). Can help throughput; sometimes finicky.\n",
    "if torch_compile:\n",
    "    try:\n",
    "        import torch\n",
    "        model = torch.compile(model)\n",
    "        print(\"Enabled torch.compile\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not enable torch.compile:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e65764e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "train.json: example 0 missing keys 'prompt' and/or 'completion'. Keys: ['span', 'document', 'reason', 'start', 'end', 'label']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading datasets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mPromptCompletionDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m eval_ds \u001b[38;5;241m=\u001b[39m PromptCompletionDataset(eval_path) \u001b[38;5;28;01mif\u001b[39;00m eval_path \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(eval_path) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      5\u001b[0m collator \u001b[38;5;241m=\u001b[39m PromptCompletionCollator(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, max_length\u001b[38;5;241m=\u001b[39mmax_length)\n",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m, in \u001b[0;36mPromptCompletionDataset.__init__\u001b[0;34m(self, path, prompt_key, completion_key)\u001b[0m\n\u001b[1;32m     21\u001b[0m     obj \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(text)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_json(obj, path)\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Fall back to JSONL parsing below\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 69\u001b[0m, in \u001b[0;36mPromptCompletionDataset._validate_rows\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: example \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a dict (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(r)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m r \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m r:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: example \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m missing keys \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and/or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(r\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: train.json: example 0 missing keys 'prompt' and/or 'completion'. Keys: ['span', 'document', 'reason', 'start', 'end', 'label']"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "\n",
    "train_ds = PromptCompletionDataset(train_path)\n",
    "eval_ds = PromptCompletionDataset(eval_path) if eval_path and os.path.exists(eval_path) else None\n",
    "collator = PromptCompletionCollator(tokenizer=tokenizer, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Training args (A100-tuned) ----\n",
    "args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    overwrite_output_dir=False,  # keep checkpoints for resume\n",
    "\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "\n",
    "    # A100: bf16 is preferred\n",
    "    bf16=bf16,\n",
    "    fp16=fp16,\n",
    "\n",
    "    # Optim + scheduler\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    "\n",
    "    # Throughput improvements\n",
    "    dataloader_num_workers=int(os.environ.get(\"NUM_WORKERS\", \"4\")),\n",
    "    max_grad_norm=float(os.environ.get(\"MAX_GRAD_NORM\", \"1.0\")),\n",
    "\n",
    "    gradient_checkpointing=True,  # reduces activation memory; helps longer context\n",
    "    tf32=True,                    # A100 supports TF32; can improve matmul speed\n",
    "    # Stability\n",
    "\n",
    "    # Logging/saving\n",
    "    logging_steps=logging_steps,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=2,\n",
    "    save_safetensors=True,\n",
    "    logging_dir=os.path.join(out_dir, \"logs\"),\n",
    "\n",
    "    # Eval\n",
    "    evaluation_strategy=\"steps\" if eval_ds is not None else \"no\",\n",
    "    eval_steps=save_steps if eval_ds is not None else None,\n",
    "\n",
    "    # Optional: save a bit of overhead with no external reporters\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # Optional: speed by grouping similar lengths (needs dataset to return lengths or use HF datasets)\n",
    "    group_by_length=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "def _latest_checkpoint(output_dir: str):\n",
    "    if not os.path.isdir(output_dir):\n",
    "        return None\n",
    "    ckpts = []\n",
    "    for name in os.listdir(output_dir):\n",
    "        if name.startswith(\"checkpoint-\"):\n",
    "            p = os.path.join(output_dir, name)\n",
    "            if os.path.isdir(p):\n",
    "                try:\n",
    "                    step = int(name.split(\"-\")[-1])\n",
    "                except Exception:\n",
    "                    step = -1\n",
    "                ckpts.append((step, p))\n",
    "    if not ckpts:\n",
    "        return None\n",
    "    ckpts.sort(key=lambda x: x[0])\n",
    "    return ckpts[-1][1]\n",
    "\n",
    "def write_save_reason_json(output_dir: str, reason: str, extra: dict = None):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    path = os.path.join(output_dir, \"save_reasons.json\")\n",
    "\n",
    "    entry = {\n",
    "        \"timestamp_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"reason\": reason,\n",
    "        \"model_name\": model_name,\n",
    "        \"train_path\": train_path,\n",
    "        \"eval_path\": eval_path if eval_ds is not None else None,\n",
    "        \"out_dir\": output_dir,\n",
    "        \"host\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"a100_tuned\": True,\n",
    "    }\n",
    "    if extra:\n",
    "        entry.update(extra)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "    else:\n",
    "        data = []\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "\n",
    "    data.append(entry)\n",
    "\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ---- Resume if possible ----\n",
    "resume_ckpt = _latest_checkpoint(out_dir)\n",
    "if resume_ckpt:\n",
    "    print(f\"Resuming from latest checkpoint: {resume_ckpt}\")\n",
    "    train_output = trainer.train(resume_from_checkpoint=resume_ckpt)\n",
    "else:\n",
    "    print(\"No checkpoint found; starting fresh.\")\n",
    "    train_output = trainer.train()\n",
    "\n",
    "# ---- Final save (LoRA adapters + tokenizer) ----\n",
    "trainer.save_model(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "write_save_reason_json(\n",
    "    out_dir,\n",
    "    reason=save_reason,\n",
    "    extra={\n",
    "        \"final_save\": True,\n",
    "        \"global_step\": int(getattr(trainer.state, \"global_step\", -1)),\n",
    "        \"train_runtime_sec\": float(getattr(train_output, \"metrics\", {}).get(\"train_runtime\", -1)),\n",
    "        \"lora\": {\"r\": lora_r, \"alpha\": lora_alpha, \"dropout\": lora_dropout},\n",
    "        \"quantization\": \"4bit_nf4\",\n",
    "        \"max_length\": max_length,\n",
    "        \"per_device_train_batch_size\": per_device_train_batch_size,\n",
    "        \"grad_accum\": gradient_accumulation_steps,\n",
    "        \"lr\": learning_rate,\n",
    "        \"bf16\": bf16,\n",
    "        \"fp16\": fp16,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"tf32\": True,\n",
    "        \"flash_attn\": use_flash_attn,\n",
    "        \"torch_compile\": torch_compile,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Saved (LoRA adapters + tokenizer) to: {out_dir}\")\n",
    "print(f\"Wrote save reasons to: {os.path.join(out_dir, 'save_reasons.json')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
