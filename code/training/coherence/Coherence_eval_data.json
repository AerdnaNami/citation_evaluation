[
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nEnsuring safety and robustness in NLP involves both data-centric and model-centric defenses. Augmentation, adversarial training, and controllable generation are frequently combined to mitigate harmful or biased outputs.\n\nParaphrase generation expands lexical coverage for robust classification (Zhang et al., 2019). Adversarial rewriting frameworks craft semantically similar yet toxic variants (Mahloujifar et al., 2019). Detoxification techniques reduce harmful content during decoding (Gehman et al., 2020). We propose a safety-centric augmentation pipeline that balances coverage with semantic fidelity through constrained editing.\n\n",
        "completion": "Paraphrase generation expands lexical coverage for robust classification (Zhang et al., 2019). Adversarial rewriting frameworks craft semantically similar yet toxic variants (Mahloujifar et al., 2019). Detoxification techniques reduce harmful content during decoding (Gehman et al., 2020)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nEnsuring safety and robustness in NLP involves both data-centric and model-centric defenses. Augmentation, adversarial training, and controllable generation are frequently combined to mitigate harmful or biased outputs.\n\nParaphrase generation expands lexical coverage for robust classification (Zhang et al., 2019). Adversarial rewriting frameworks craft semantically similar yet toxic variants (Mahloujifar et al., 2019). Detoxification techniques reduce harmful content during decoding (Gehman et al., 2020). We propose a safety-centric augmentation pipeline that balances coverage with semantic fidelity through constrained editing.\n\nRelated Work\n\nHuman feedback has emerged as a powerful signal for training agents when rewards are sparse or hard to specify. Preference-based reinforcement learning collects pairwise trajectory comparisons to shape rewards (Christiano et al., 2017). Generative adversarial imitation learning matches expert occupancy measures (Ho and Ermon, 2016). Off-policy evaluation methods estimate returns from logged data (Thomas and Brunskill, 2016). Recent work integrates language feedback for policy refinement (Sumers et al., 2021). We study aligning learned rewards with natural-language critiques to reduce labeling burden.\n",
        "completion": "Preference-based reinforcement learning collects pairwise trajectory comparisons to shape rewards (Christiano et al., 2017). Generative adversarial imitation learning matches expert occupancy measures (Ho and Ermon, 2016). Off-policy evaluation methods estimate returns from logged data (Thomas and Brunskill, 2016)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nEnsuring safety and robustness in NLP involves both data-centric and model-centric defenses. Augmentation, adversarial training, and controllable generation are frequently combined to mitigate harmful or biased outputs.\n\nParaphrase generation expands lexical coverage for robust classification (Zhang et al., 2019). Adversarial rewriting frameworks craft semantically similar yet toxic variants (Mahloujifar et al., 2019). Detoxification techniques reduce harmful content during decoding (Gehman et al., 2020). We propose a safety-centric augmentation pipeline that balances coverage with semantic fidelity through constrained editing.\n\nRelated Work\n\nHuman feedback has emerged as a powerful signal for training agents when rewards are sparse or hard to specify. Preference-based reinforcement learning collects pairwise trajectory comparisons to shape rewards (Christiano et al., 2017). Generative adversarial imitation learning matches expert occupancy measures (Ho and Ermon, 2016). Off-policy evaluation methods estimate returns from logged data (Thomas and Brunskill, 2016). Recent work integrates language feedback for policy refinement (Sumers et al., 2021). We study aligning learned rewards with natural-language critiques to reduce labeling burden.\nRelated Work\n\nPrompting and in-context learning\nLarge language models can be steered via natural language prompts to perform downstream tasks without task-specific fine-tuning, a phenomenon often called in-context learning (Brown et al., 2020). Subsequent work studies automatic prompt construction and selection to stabilize performance across tasks (Gao et al., 2021; Zhou et al., 2022). Instruction induction has also been explored for more controllable generation behaviors (Sanh et al., 2021). Calibration methods aim to correct confidence estimates without changing predictions (Guo et al., 2017). Retrieval-augmented prompting pairs prompts with external evidence (Lewis et al., 2020). Parameter-efficient tuning freezes most weights while adding small adapters (Houlsby et al., 2019; Lester et al., 2021).\n\nMultilingual adaptation\nCross-lingual transfer uses shared subword vocabularies and aligned pretraining objectives to generalize prompts across languages (Conneau et al., 2020). However, prompt templates designed in English may not transfer directly due to morphological and word-order differences (Winata et al., 2021). Data augmentation through machine translation can reduce variance but introduces translationese artifacts (Artetxe et al., 2020).\n\nOur work studies prompt selection under distribution shift, focusing on stable choices that maintain accuracy across domains and languages while minimizing reliance on labeled validation sets.\n",
        "completion": "Instruction induction has also been explored for more controllable generation behaviors (Sanh et al., 2021). Calibration methods aim to correct confidence estimates without changing predictions (Guo et al., 2017). Retrieval-augmented prompting pairs prompts with external evidence (Lewis et al., 2020). Parameter-efficient tuning freezes most weights while adding small adapters (Houlsby et al., 2019; Lester et al., 2021)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nEnsuring safety and robustness in NLP involves both data-centric and model-centric defenses. Augmentation, adversarial training, and controllable generation are frequently combined to mitigate harmful or biased outputs.\n\nParaphrase generation expands lexical coverage for robust classification (Zhang et al., 2019). Adversarial rewriting frameworks craft semantically similar yet toxic variants (Mahloujifar et al., 2019). Detoxification techniques reduce harmful content during decoding (Gehman et al., 2020). We propose a safety-centric augmentation pipeline that balances coverage with semantic fidelity through constrained editing.\n\nRelated Work\n\nHuman feedback has emerged as a powerful signal for training agents when rewards are sparse or hard to specify. Preference-based reinforcement learning collects pairwise trajectory comparisons to shape rewards (Christiano et al., 2017). Generative adversarial imitation learning matches expert occupancy measures (Ho and Ermon, 2016). Off-policy evaluation methods estimate returns from logged data (Thomas and Brunskill, 2016). Recent work integrates language feedback for policy refinement (Sumers et al., 2021). We study aligning learned rewards with natural-language critiques to reduce labeling burden.\nRelated Work\n\nPrompting and in-context learning\nLarge language models can be steered via natural language prompts to perform downstream tasks without task-specific fine-tuning, a phenomenon often called in-context learning (Brown et al., 2020). Subsequent work studies automatic prompt construction and selection to stabilize performance across tasks (Gao et al., 2021; Zhou et al., 2022). Instruction induction has also been explored for more controllable generation behaviors (Sanh et al., 2021). Calibration methods aim to correct confidence estimates without changing predictions (Guo et al., 2017). Retrieval-augmented prompting pairs prompts with external evidence (Lewis et al., 2020). Parameter-efficient tuning freezes most weights while adding small adapters (Houlsby et al., 2019; Lester et al., 2021).\n\nMultilingual adaptation\nCross-lingual transfer uses shared subword vocabularies and aligned pretraining objectives to generalize prompts across languages (Conneau et al., 2020). However, prompt templates designed in English may not transfer directly due to morphological and word-order differences (Winata et al., 2021). Data augmentation through machine translation can reduce variance but introduces translationese artifacts (Artetxe et al., 2020).\n\nOur work studies prompt selection under distribution shift, focusing on stable choices that maintain accuracy across domains and languages while minimizing reliance on labeled validation sets.\nRelated Work\n\nAlgorithmic fairness literature spans definitions, measurement, and mitigation techniques across supervised learning settings (Barocas et al., 2019; Mitchell et al., 2019). Metrics frequently trade off with one another under distribution shift and label bias.\n\nDemographic parity enforces independence between predictions and sensitive attributes (Dwork et al., 2012). Disparate impact quantifies relative selection rates (Feldman et al., 2015). Counterfactual fairness uses structural causal models (Kusner et al., 2017). Toolkits implement auditing at scale (Saleiro et al., 2018).\n\nOur work investigates metric compatibility when sensitive attributes are partially observed.\n",
        "completion": "Demographic parity enforces independence between predictions and sensitive attributes (Dwork et al., 2012). Disparate impact quantifies relative selection rates (Feldman et al., 2015). Counterfactual fairness uses structural causal models (Kusner et al., 2017). Toolkits implement auditing at scale (Saleiro et al., 2018)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nEnsuring safety and robustness in NLP involves both data-centric and model-centric defenses. Augmentation, adversarial training, and controllable generation are frequently combined to mitigate harmful or biased outputs.\n\nParaphrase generation expands lexical coverage for robust classification (Zhang et al., 2019). Adversarial rewriting frameworks craft semantically similar yet toxic variants (Mahloujifar et al., 2019). Detoxification techniques reduce harmful content during decoding (Gehman et al., 2020). We propose a safety-centric augmentation pipeline that balances coverage with semantic fidelity through constrained editing.\n\nRelated Work\n\nHuman feedback has emerged as a powerful signal for training agents when rewards are sparse or hard to specify. Preference-based reinforcement learning collects pairwise trajectory comparisons to shape rewards (Christiano et al., 2017). Generative adversarial imitation learning matches expert occupancy measures (Ho and Ermon, 2016). Off-policy evaluation methods estimate returns from logged data (Thomas and Brunskill, 2016). Recent work integrates language feedback for policy refinement (Sumers et al., 2021). We study aligning learned rewards with natural-language critiques to reduce labeling burden.\nRelated Work\n\nPrompting and in-context learning\nLarge language models can be steered via natural language prompts to perform downstream tasks without task-specific fine-tuning, a phenomenon often called in-context learning (Brown et al., 2020). Subsequent work studies automatic prompt construction and selection to stabilize performance across tasks (Gao et al., 2021; Zhou et al., 2022). Instruction induction has also been explored for more controllable generation behaviors (Sanh et al., 2021). Calibration methods aim to correct confidence estimates without changing predictions (Guo et al., 2017). Retrieval-augmented prompting pairs prompts with external evidence (Lewis et al., 2020). Parameter-efficient tuning freezes most weights while adding small adapters (Houlsby et al., 2019; Lester et al., 2021).\n\nMultilingual adaptation\nCross-lingual transfer uses shared subword vocabularies and aligned pretraining objectives to generalize prompts across languages (Conneau et al., 2020). However, prompt templates designed in English may not transfer directly due to morphological and word-order differences (Winata et al., 2021). Data augmentation through machine translation can reduce variance but introduces translationese artifacts (Artetxe et al., 2020).\n\nOur work studies prompt selection under distribution shift, focusing on stable choices that maintain accuracy across domains and languages while minimizing reliance on labeled validation sets.\nRelated Work\n\nAlgorithmic fairness literature spans definitions, measurement, and mitigation techniques across supervised learning settings (Barocas et al., 2019; Mitchell et al., 2019). Metrics frequently trade off with one another under distribution shift and label bias.\n\nDemographic parity enforces independence between predictions and sensitive attributes (Dwork et al., 2012). Disparate impact quantifies relative selection rates (Feldman et al., 2015). Counterfactual fairness uses structural causal models (Kusner et al., 2017). Toolkits implement auditing at scale (Saleiro et al., 2018).\n\nOur work investigates metric compatibility when sensitive attributes are partially observed.\nRelated Work: Knowledge Resources for Question Answering\n\nQuestion answering (QA) systems rely on external resources to supply facts and context. Two dominant paradigms are text-based retrieval over unstructured corpora and symbolic reasoning over structured knowledge graphs (KGs), with hybrids combining both.\n\nOpen-domain question answering retrieves evidential passages from large corpora (Karpukhin et al., 2020). Knowledge graph completion methods predict missing triples using embedding models (Bordes et al., 2013). Memory networks store and access supporting facts for reasoning (Weston et al., 2015). Dense passage retrieval leverages bi-encoders to scale retrieval (Karpukhin et al., 2020).\n\nHybrid QA integrates KG constraints with neural retrievers to reduce ambiguity and improve compositional reasoning (Sun et al., 2019). Recent work aligns textual spans with KG entities using joint encoders to facilitate multi-hop answering (Xiong et al., 2020). Our approach builds a bridge module that converts retrieved text into structured edges, enabling controlled reasoning paths.\n",
        "completion": "Open-domain question answering retrieves evidential passages from large corpora (Karpukhin et al., 2020). Knowledge graph completion methods predict missing triples using embedding models (Bordes et al., 2013). Memory networks store and access supporting facts for reasoning (Weston et al., 2015). Dense passage retrieval leverages bi-encoders to scale retrieval (Karpukhin et al., 2020)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nEnsuring safety and robustness in NLP involves both data-centric and model-centric defenses. Augmentation, adversarial training, and controllable generation are frequently combined to mitigate harmful or biased outputs.\n\nParaphrase generation expands lexical coverage for robust classification (Zhang et al., 2019). Adversarial rewriting frameworks craft semantically similar yet toxic variants (Mahloujifar et al., 2019). Detoxification techniques reduce harmful content during decoding (Gehman et al., 2020). We propose a safety-centric augmentation pipeline that balances coverage with semantic fidelity through constrained editing.\n\nRelated Work\n\nHuman feedback has emerged as a powerful signal for training agents when rewards are sparse or hard to specify. Preference-based reinforcement learning collects pairwise trajectory comparisons to shape rewards (Christiano et al., 2017). Generative adversarial imitation learning matches expert occupancy measures (Ho and Ermon, 2016). Off-policy evaluation methods estimate returns from logged data (Thomas and Brunskill, 2016). Recent work integrates language feedback for policy refinement (Sumers et al., 2021). We study aligning learned rewards with natural-language critiques to reduce labeling burden.\nRelated Work\n\nPrompting and in-context learning\nLarge language models can be steered via natural language prompts to perform downstream tasks without task-specific fine-tuning, a phenomenon often called in-context learning (Brown et al., 2020). Subsequent work studies automatic prompt construction and selection to stabilize performance across tasks (Gao et al., 2021; Zhou et al., 2022). Instruction induction has also been explored for more controllable generation behaviors (Sanh et al., 2021). Calibration methods aim to correct confidence estimates without changing predictions (Guo et al., 2017). Retrieval-augmented prompting pairs prompts with external evidence (Lewis et al., 2020). Parameter-efficient tuning freezes most weights while adding small adapters (Houlsby et al., 2019; Lester et al., 2021).\n\nMultilingual adaptation\nCross-lingual transfer uses shared subword vocabularies and aligned pretraining objectives to generalize prompts across languages (Conneau et al., 2020). However, prompt templates designed in English may not transfer directly due to morphological and word-order differences (Winata et al., 2021). Data augmentation through machine translation can reduce variance but introduces translationese artifacts (Artetxe et al., 2020).\n\nOur work studies prompt selection under distribution shift, focusing on stable choices that maintain accuracy across domains and languages while minimizing reliance on labeled validation sets.\nRelated Work\n\nAlgorithmic fairness literature spans definitions, measurement, and mitigation techniques across supervised learning settings (Barocas et al., 2019; Mitchell et al., 2019). Metrics frequently trade off with one another under distribution shift and label bias.\n\nDemographic parity enforces independence between predictions and sensitive attributes (Dwork et al., 2012). Disparate impact quantifies relative selection rates (Feldman et al., 2015). Counterfactual fairness uses structural causal models (Kusner et al., 2017). Toolkits implement auditing at scale (Saleiro et al., 2018).\n\nOur work investigates metric compatibility when sensitive attributes are partially observed.\nRelated Work: Knowledge Resources for Question Answering\n\nQuestion answering (QA) systems rely on external resources to supply facts and context. Two dominant paradigms are text-based retrieval over unstructured corpora and symbolic reasoning over structured knowledge graphs (KGs), with hybrids combining both.\n\nOpen-domain question answering retrieves evidential passages from large corpora (Karpukhin et al., 2020). Knowledge graph completion methods predict missing triples using embedding models (Bordes et al., 2013). Memory networks store and access supporting facts for reasoning (Weston et al., 2015). Dense passage retrieval leverages bi-encoders to scale retrieval (Karpukhin et al., 2020).\n\nHybrid QA integrates KG constraints with neural retrievers to reduce ambiguity and improve compositional reasoning (Sun et al., 2019). Recent work aligns textual spans with KG entities using joint encoders to facilitate multi-hop answering (Xiong et al., 2020). Our approach builds a bridge module that converts retrieved text into structured edges, enabling controlled reasoning paths.\nRelated Work\n\nFairness in recommender systems spans algorithmic, stakeholder, and experiential dimensions. Prior studies investigate selection bias, exposure allocation, and feedback loops.\n\nYao and Huang (2017) defined fairness-aware learning objectives for recommender systems. Burke et al. (2018) discussed multi-stakeholder fairness in recommendation settings. Ekstrand et al. (2022) examined user-perceived fairness and exposure disparities.\n\nOur method optimizes exposure parity under constrained relevance.\n",
        "completion": "Yao and Huang (2017) defined fairness-aware learning objectives for recommender systems. Burke et al. (2018) discussed multi-stakeholder fairness in recommendation settings. Ekstrand et al. (2022) examined user-perceived fairness and exposure disparities."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nEnsuring safety and robustness in NLP involves both data-centric and model-centric defenses. Augmentation, adversarial training, and controllable generation are frequently combined to mitigate harmful or biased outputs.\n\nParaphrase generation expands lexical coverage for robust classification (Zhang et al., 2019). Adversarial rewriting frameworks craft semantically similar yet toxic variants (Mahloujifar et al., 2019). Detoxification techniques reduce harmful content during decoding (Gehman et al., 2020). We propose a safety-centric augmentation pipeline that balances coverage with semantic fidelity through constrained editing.\n\nRelated Work\n\nHuman feedback has emerged as a powerful signal for training agents when rewards are sparse or hard to specify. Preference-based reinforcement learning collects pairwise trajectory comparisons to shape rewards (Christiano et al., 2017). Generative adversarial imitation learning matches expert occupancy measures (Ho and Ermon, 2016). Off-policy evaluation methods estimate returns from logged data (Thomas and Brunskill, 2016). Recent work integrates language feedback for policy refinement (Sumers et al., 2021). We study aligning learned rewards with natural-language critiques to reduce labeling burden.\nRelated Work\n\nPrompting and in-context learning\nLarge language models can be steered via natural language prompts to perform downstream tasks without task-specific fine-tuning, a phenomenon often called in-context learning (Brown et al., 2020). Subsequent work studies automatic prompt construction and selection to stabilize performance across tasks (Gao et al., 2021; Zhou et al., 2022). Instruction induction has also been explored for more controllable generation behaviors (Sanh et al., 2021). Calibration methods aim to correct confidence estimates without changing predictions (Guo et al., 2017). Retrieval-augmented prompting pairs prompts with external evidence (Lewis et al., 2020). Parameter-efficient tuning freezes most weights while adding small adapters (Houlsby et al., 2019; Lester et al., 2021).\n\nMultilingual adaptation\nCross-lingual transfer uses shared subword vocabularies and aligned pretraining objectives to generalize prompts across languages (Conneau et al., 2020). However, prompt templates designed in English may not transfer directly due to morphological and word-order differences (Winata et al., 2021). Data augmentation through machine translation can reduce variance but introduces translationese artifacts (Artetxe et al., 2020).\n\nOur work studies prompt selection under distribution shift, focusing on stable choices that maintain accuracy across domains and languages while minimizing reliance on labeled validation sets.\nRelated Work\n\nAlgorithmic fairness literature spans definitions, measurement, and mitigation techniques across supervised learning settings (Barocas et al., 2019; Mitchell et al., 2019). Metrics frequently trade off with one another under distribution shift and label bias.\n\nDemographic parity enforces independence between predictions and sensitive attributes (Dwork et al., 2012). Disparate impact quantifies relative selection rates (Feldman et al., 2015). Counterfactual fairness uses structural causal models (Kusner et al., 2017). Toolkits implement auditing at scale (Saleiro et al., 2018).\n\nOur work investigates metric compatibility when sensitive attributes are partially observed.\nRelated Work: Knowledge Resources for Question Answering\n\nQuestion answering (QA) systems rely on external resources to supply facts and context. Two dominant paradigms are text-based retrieval over unstructured corpora and symbolic reasoning over structured knowledge graphs (KGs), with hybrids combining both.\n\nOpen-domain question answering retrieves evidential passages from large corpora (Karpukhin et al., 2020). Knowledge graph completion methods predict missing triples using embedding models (Bordes et al., 2013). Memory networks store and access supporting facts for reasoning (Weston et al., 2015). Dense passage retrieval leverages bi-encoders to scale retrieval (Karpukhin et al., 2020).\n\nHybrid QA integrates KG constraints with neural retrievers to reduce ambiguity and improve compositional reasoning (Sun et al., 2019). Recent work aligns textual spans with KG entities using joint encoders to facilitate multi-hop answering (Xiong et al., 2020). Our approach builds a bridge module that converts retrieved text into structured edges, enabling controlled reasoning paths.\nRelated Work\n\nFairness in recommender systems spans algorithmic, stakeholder, and experiential dimensions. Prior studies investigate selection bias, exposure allocation, and feedback loops.\n\nYao and Huang (2017) defined fairness-aware learning objectives for recommender systems. Burke et al. (2018) discussed multi-stakeholder fairness in recommendation settings. Ekstrand et al. (2022) examined user-perceived fairness and exposure disparities.\n\nOur method optimizes exposure parity under constrained relevance.\nIntroduction\n\nExplainable AI (XAI) in healthcare seeks to provide transparent model rationales that clinicians can trust (Doshi-Velez and Kim, 2017). Beyond accuracy, accountability and robustness require explanations that reflect clinical mechanisms and data quality.\n\nSHAP assigns feature attributions under a game-theoretic framework (Lundberg and Lee, 2017). Counterfactual explanations specify minimal changes to alter predictions (Wachter et al., 2018). Missing value imputation in EHRs can bias downstream models (Beaulieu-Jones and Moore, 2017). We focus on aligning attribution methods with data missingness patterns by accounting for informative absence in EHR features.\n\n",
        "completion": "SHAP assigns feature attributions under a game-theoretic framework (Lundberg and Lee, 2017). Counterfactual explanations specify minimal changes to alter predictions (Wachter et al., 2018). Missing value imputation in EHRs can bias downstream models (Beaulieu-Jones and Moore, 2017)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nEnsuring safety and robustness in NLP involves both data-centric and model-centric defenses. Augmentation, adversarial training, and controllable generation are frequently combined to mitigate harmful or biased outputs.\n\nParaphrase generation expands lexical coverage for robust classification (Zhang et al., 2019). Adversarial rewriting frameworks craft semantically similar yet toxic variants (Mahloujifar et al., 2019). Detoxification techniques reduce harmful content during decoding (Gehman et al., 2020). We propose a safety-centric augmentation pipeline that balances coverage with semantic fidelity through constrained editing.\n\nRelated Work\n\nHuman feedback has emerged as a powerful signal for training agents when rewards are sparse or hard to specify. Preference-based reinforcement learning collects pairwise trajectory comparisons to shape rewards (Christiano et al., 2017). Generative adversarial imitation learning matches expert occupancy measures (Ho and Ermon, 2016). Off-policy evaluation methods estimate returns from logged data (Thomas and Brunskill, 2016). Recent work integrates language feedback for policy refinement (Sumers et al., 2021). We study aligning learned rewards with natural-language critiques to reduce labeling burden.\nRelated Work\n\nPrompting and in-context learning\nLarge language models can be steered via natural language prompts to perform downstream tasks without task-specific fine-tuning, a phenomenon often called in-context learning (Brown et al., 2020). Subsequent work studies automatic prompt construction and selection to stabilize performance across tasks (Gao et al., 2021; Zhou et al., 2022). Instruction induction has also been explored for more controllable generation behaviors (Sanh et al., 2021). Calibration methods aim to correct confidence estimates without changing predictions (Guo et al., 2017). Retrieval-augmented prompting pairs prompts with external evidence (Lewis et al., 2020). Parameter-efficient tuning freezes most weights while adding small adapters (Houlsby et al., 2019; Lester et al., 2021).\n\nMultilingual adaptation\nCross-lingual transfer uses shared subword vocabularies and aligned pretraining objectives to generalize prompts across languages (Conneau et al., 2020). However, prompt templates designed in English may not transfer directly due to morphological and word-order differences (Winata et al., 2021). Data augmentation through machine translation can reduce variance but introduces translationese artifacts (Artetxe et al., 2020).\n\nOur work studies prompt selection under distribution shift, focusing on stable choices that maintain accuracy across domains and languages while minimizing reliance on labeled validation sets.\nRelated Work\n\nAlgorithmic fairness literature spans definitions, measurement, and mitigation techniques across supervised learning settings (Barocas et al., 2019; Mitchell et al., 2019). Metrics frequently trade off with one another under distribution shift and label bias.\n\nDemographic parity enforces independence between predictions and sensitive attributes (Dwork et al., 2012). Disparate impact quantifies relative selection rates (Feldman et al., 2015). Counterfactual fairness uses structural causal models (Kusner et al., 2017). Toolkits implement auditing at scale (Saleiro et al., 2018).\n\nOur work investigates metric compatibility when sensitive attributes are partially observed.\nRelated Work: Knowledge Resources for Question Answering\n\nQuestion answering (QA) systems rely on external resources to supply facts and context. Two dominant paradigms are text-based retrieval over unstructured corpora and symbolic reasoning over structured knowledge graphs (KGs), with hybrids combining both.\n\nOpen-domain question answering retrieves evidential passages from large corpora (Karpukhin et al., 2020). Knowledge graph completion methods predict missing triples using embedding models (Bordes et al., 2013). Memory networks store and access supporting facts for reasoning (Weston et al., 2015). Dense passage retrieval leverages bi-encoders to scale retrieval (Karpukhin et al., 2020).\n\nHybrid QA integrates KG constraints with neural retrievers to reduce ambiguity and improve compositional reasoning (Sun et al., 2019). Recent work aligns textual spans with KG entities using joint encoders to facilitate multi-hop answering (Xiong et al., 2020). Our approach builds a bridge module that converts retrieved text into structured edges, enabling controlled reasoning paths.\nRelated Work\n\nFairness in recommender systems spans algorithmic, stakeholder, and experiential dimensions. Prior studies investigate selection bias, exposure allocation, and feedback loops.\n\nYao and Huang (2017) defined fairness-aware learning objectives for recommender systems. Burke et al. (2018) discussed multi-stakeholder fairness in recommendation settings. Ekstrand et al. (2022) examined user-perceived fairness and exposure disparities.\n\nOur method optimizes exposure parity under constrained relevance.\nIntroduction\n\nExplainable AI (XAI) in healthcare seeks to provide transparent model rationales that clinicians can trust (Doshi-Velez and Kim, 2017). Beyond accuracy, accountability and robustness require explanations that reflect clinical mechanisms and data quality.\n\nSHAP assigns feature attributions under a game-theoretic framework (Lundberg and Lee, 2017). Counterfactual explanations specify minimal changes to alter predictions (Wachter et al., 2018). Missing value imputation in EHRs can bias downstream models (Beaulieu-Jones and Moore, 2017). We focus on aligning attribution methods with data missingness patterns by accounting for informative absence in EHR features.\n\nRelated Work\n\nVision Transformers (ViTs) extend self-attention to images and challenge convolutional inductive biases with global context modeling (Dosovitskiy et al., 2021). Follow-up studies examine data efficiency, positional encodings, and training regularization to close gaps with CNNs in limited-data regimes.\n\nScaling laws reveal predictable accuracy gains with model and data size (Kaplan et al., 2020). Rotary position embeddings improve extrapolation of attention (Su et al., 2021). Distillation from CNN teachers stabilizes training of ViTs on small datasets (Touvron et al., 2021). We study a unified recipe that couples positional design with token mixing priors under small-scale training budgets.\n\n",
        "completion": "Scaling laws reveal predictable accuracy gains with model and data size (Kaplan et al., 2020). Rotary position embeddings improve extrapolation of attention (Su et al., 2021). Distillation from CNN teachers stabilizes training of ViTs on small datasets (Touvron et al., 2021)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nEnsuring safety and robustness in NLP involves both data-centric and model-centric defenses. Augmentation, adversarial training, and controllable generation are frequently combined to mitigate harmful or biased outputs.\n\nParaphrase generation expands lexical coverage for robust classification (Zhang et al., 2019). Adversarial rewriting frameworks craft semantically similar yet toxic variants (Mahloujifar et al., 2019). Detoxification techniques reduce harmful content during decoding (Gehman et al., 2020). We propose a safety-centric augmentation pipeline that balances coverage with semantic fidelity through constrained editing.\n\nRelated Work\n\nHuman feedback has emerged as a powerful signal for training agents when rewards are sparse or hard to specify. Preference-based reinforcement learning collects pairwise trajectory comparisons to shape rewards (Christiano et al., 2017). Generative adversarial imitation learning matches expert occupancy measures (Ho and Ermon, 2016). Off-policy evaluation methods estimate returns from logged data (Thomas and Brunskill, 2016). Recent work integrates language feedback for policy refinement (Sumers et al., 2021). We study aligning learned rewards with natural-language critiques to reduce labeling burden.\nRelated Work\n\nPrompting and in-context learning\nLarge language models can be steered via natural language prompts to perform downstream tasks without task-specific fine-tuning, a phenomenon often called in-context learning (Brown et al., 2020). Subsequent work studies automatic prompt construction and selection to stabilize performance across tasks (Gao et al., 2021; Zhou et al., 2022). Instruction induction has also been explored for more controllable generation behaviors (Sanh et al., 2021). Calibration methods aim to correct confidence estimates without changing predictions (Guo et al., 2017). Retrieval-augmented prompting pairs prompts with external evidence (Lewis et al., 2020). Parameter-efficient tuning freezes most weights while adding small adapters (Houlsby et al., 2019; Lester et al., 2021).\n\nMultilingual adaptation\nCross-lingual transfer uses shared subword vocabularies and aligned pretraining objectives to generalize prompts across languages (Conneau et al., 2020). However, prompt templates designed in English may not transfer directly due to morphological and word-order differences (Winata et al., 2021). Data augmentation through machine translation can reduce variance but introduces translationese artifacts (Artetxe et al., 2020).\n\nOur work studies prompt selection under distribution shift, focusing on stable choices that maintain accuracy across domains and languages while minimizing reliance on labeled validation sets.\nRelated Work\n\nAlgorithmic fairness literature spans definitions, measurement, and mitigation techniques across supervised learning settings (Barocas et al., 2019; Mitchell et al., 2019). Metrics frequently trade off with one another under distribution shift and label bias.\n\nDemographic parity enforces independence between predictions and sensitive attributes (Dwork et al., 2012). Disparate impact quantifies relative selection rates (Feldman et al., 2015). Counterfactual fairness uses structural causal models (Kusner et al., 2017). Toolkits implement auditing at scale (Saleiro et al., 2018).\n\nOur work investigates metric compatibility when sensitive attributes are partially observed.\nRelated Work: Knowledge Resources for Question Answering\n\nQuestion answering (QA) systems rely on external resources to supply facts and context. Two dominant paradigms are text-based retrieval over unstructured corpora and symbolic reasoning over structured knowledge graphs (KGs), with hybrids combining both.\n\nOpen-domain question answering retrieves evidential passages from large corpora (Karpukhin et al., 2020). Knowledge graph completion methods predict missing triples using embedding models (Bordes et al., 2013). Memory networks store and access supporting facts for reasoning (Weston et al., 2015). Dense passage retrieval leverages bi-encoders to scale retrieval (Karpukhin et al., 2020).\n\nHybrid QA integrates KG constraints with neural retrievers to reduce ambiguity and improve compositional reasoning (Sun et al., 2019). Recent work aligns textual spans with KG entities using joint encoders to facilitate multi-hop answering (Xiong et al., 2020). Our approach builds a bridge module that converts retrieved text into structured edges, enabling controlled reasoning paths.\nRelated Work\n\nFairness in recommender systems spans algorithmic, stakeholder, and experiential dimensions. Prior studies investigate selection bias, exposure allocation, and feedback loops.\n\nYao and Huang (2017) defined fairness-aware learning objectives for recommender systems. Burke et al. (2018) discussed multi-stakeholder fairness in recommendation settings. Ekstrand et al. (2022) examined user-perceived fairness and exposure disparities.\n\nOur method optimizes exposure parity under constrained relevance.\nIntroduction\n\nExplainable AI (XAI) in healthcare seeks to provide transparent model rationales that clinicians can trust (Doshi-Velez and Kim, 2017). Beyond accuracy, accountability and robustness require explanations that reflect clinical mechanisms and data quality.\n\nSHAP assigns feature attributions under a game-theoretic framework (Lundberg and Lee, 2017). Counterfactual explanations specify minimal changes to alter predictions (Wachter et al., 2018). Missing value imputation in EHRs can bias downstream models (Beaulieu-Jones and Moore, 2017). We focus on aligning attribution methods with data missingness patterns by accounting for informative absence in EHR features.\n\nRelated Work\n\nVision Transformers (ViTs) extend self-attention to images and challenge convolutional inductive biases with global context modeling (Dosovitskiy et al., 2021). Follow-up studies examine data efficiency, positional encodings, and training regularization to close gaps with CNNs in limited-data regimes.\n\nScaling laws reveal predictable accuracy gains with model and data size (Kaplan et al., 2020). Rotary position embeddings improve extrapolation of attention (Su et al., 2021). Distillation from CNN teachers stabilizes training of ViTs on small datasets (Touvron et al., 2021). We study a unified recipe that couples positional design with token mixing priors under small-scale training budgets.\n\nIntroduction\n\nMultimodal representation learning for medical imaging seeks to link visual content with clinical text, enabling retrieval, decision support, and automated reporting. However, label scarcity and domain shift across institutions remain practical obstacles.\n\nCross-modal contrastive learning aligns image and text embeddings (Radford et al., 2021). Self-supervised pretraining on radiographs improves downstream detection (Azizi et al., 2021). Transformers aggregate long-range features (Dosovitskiy et al., 2021). Report generation systems summarize clinical findings (Liu et al., 2019).\n\nWe propose a domain-adaptive contrastive pretraining scheme that incorporates structured report cues, bridging representation alignment with clinically grounded supervision.\n",
        "completion": "Cross-modal contrastive learning aligns image and text embeddings (Radford et al., 2021). Self-supervised pretraining on radiographs improves downstream detection (Azizi et al., 2021). Transformers aggregate long-range features (Dosovitskiy et al., 2021). Report generation systems summarize clinical findings (Liu et al., 2019)."
    }
]