[
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\n",
        "completion": "Screen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\n",
        "completion": "Policy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\nIntroduction\n\nQuestion Answering over Knowledge Graphs\n\nKnowledge graph question answering (KGQA) aims to answer natural language questions by reasoning over entities and relations. Methods range from symbolic semantic parsing to neural approaches that operate over learned representations, often combining retrieval and reasoning components (Lan and Jiang, 2020; Chen et al., 2021). Benchmarks such as WebQuestionsSP and ComplexWebQuestions have spurred progress on multi-hop reasoning and compositional generalization.\n\nSemantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018).\n\nWe present a retrieval-augmented semantic parser that conditions on entity-aware encoder states and uses constrained decoding to ensure executable outputs, yielding gains on compositional and conversational KGQA tasks.\n",
        "completion": "Semantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\nIntroduction\n\nQuestion Answering over Knowledge Graphs\n\nKnowledge graph question answering (KGQA) aims to answer natural language questions by reasoning over entities and relations. Methods range from symbolic semantic parsing to neural approaches that operate over learned representations, often combining retrieval and reasoning components (Lan and Jiang, 2020; Chen et al., 2021). Benchmarks such as WebQuestionsSP and ComplexWebQuestions have spurred progress on multi-hop reasoning and compositional generalization.\n\nSemantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018).\n\nWe present a retrieval-augmented semantic parser that conditions on entity-aware encoder states and uses constrained decoding to ensure executable outputs, yielding gains on compositional and conversational KGQA tasks.\nRelated Work\n\nSelf-supervised learning (SSL) in computer vision has progressed rapidly through pretext tasks and instance discrimination, yielding strong transfer to downstream tasks (He et al., 2020; Grill et al., 2020). Architectural choices and training recipes both strongly affect representation quality, with recent efforts exploring data augmentations, projector heads, and multi-crop strategies (Chen et al., 2020; Caron et al., 2020; Touvron et al., 2021).\n\nContrastive methods maximize agreement between augmented views (Chen et al., 2020). Vision Transformers model long-range dependencies with self-attention (Dosovitskiy et al., 2021). Clustering-based SSL discretizes latent space to avoid collapse (Caron et al., 2020). Beyond images, multi-modal SSL leverages alignment between text and vision to learn transferable features (Radford et al., 2021).\n\nWe study how architectural inductive biases interact with SSL objectives, providing a controlled comparison of convolutional and transformer backbones under identical augmentation pipelines.\n",
        "completion": "Contrastive methods maximize agreement between augmented views (Chen et al., 2020). Vision Transformers model long-range dependencies with self-attention (Dosovitskiy et al., 2021). Clustering-based SSL discretizes latent space to avoid collapse (Caron et al., 2020)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\nIntroduction\n\nQuestion Answering over Knowledge Graphs\n\nKnowledge graph question answering (KGQA) aims to answer natural language questions by reasoning over entities and relations. Methods range from symbolic semantic parsing to neural approaches that operate over learned representations, often combining retrieval and reasoning components (Lan and Jiang, 2020; Chen et al., 2021). Benchmarks such as WebQuestionsSP and ComplexWebQuestions have spurred progress on multi-hop reasoning and compositional generalization.\n\nSemantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018).\n\nWe present a retrieval-augmented semantic parser that conditions on entity-aware encoder states and uses constrained decoding to ensure executable outputs, yielding gains on compositional and conversational KGQA tasks.\nRelated Work\n\nSelf-supervised learning (SSL) in computer vision has progressed rapidly through pretext tasks and instance discrimination, yielding strong transfer to downstream tasks (He et al., 2020; Grill et al., 2020). Architectural choices and training recipes both strongly affect representation quality, with recent efforts exploring data augmentations, projector heads, and multi-crop strategies (Chen et al., 2020; Caron et al., 2020; Touvron et al., 2021).\n\nContrastive methods maximize agreement between augmented views (Chen et al., 2020). Vision Transformers model long-range dependencies with self-attention (Dosovitskiy et al., 2021). Clustering-based SSL discretizes latent space to avoid collapse (Caron et al., 2020). Beyond images, multi-modal SSL leverages alignment between text and vision to learn transferable features (Radford et al., 2021).\n\nWe study how architectural inductive biases interact with SSL objectives, providing a controlled comparison of convolutional and transformer backbones under identical augmentation pipelines.\nRelated Work\n\nCompressing over-parameterized transformers is essential for deployment on edge devices. Recent work explores distillation, pruning, quantization, and low-rank factorization to reduce memory and latency while preserving accuracy. However, the interplay between compression objectives and downstream generalization is still underexplored.\n\nKnowledge distillation compresses large models into smaller students (Hinton et al., 2015). Attention transfer passes intermediate alignment signals between layers (Zagoruyko and Komodakis, 2017). Patient distillation slows the learning rate for deeper students (Sun et al., 2019). Pruning removes redundant parameters after training (Han et al., 2016).\n\nWe instead propose task-aware multi-teacher distillation that aligns student representations with both task-specific signals and calibration constraints. This yields improved out-of-domain robustness while maintaining compactness.\n",
        "completion": "Knowledge distillation compresses large models into smaller students (Hinton et al., 2015). Attention transfer passes intermediate alignment signals between layers (Zagoruyko and Komodakis, 2017). Patient distillation slows the learning rate for deeper students (Sun et al., 2019). Pruning removes redundant parameters after training (Han et al., 2016)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\nIntroduction\n\nQuestion Answering over Knowledge Graphs\n\nKnowledge graph question answering (KGQA) aims to answer natural language questions by reasoning over entities and relations. Methods range from symbolic semantic parsing to neural approaches that operate over learned representations, often combining retrieval and reasoning components (Lan and Jiang, 2020; Chen et al., 2021). Benchmarks such as WebQuestionsSP and ComplexWebQuestions have spurred progress on multi-hop reasoning and compositional generalization.\n\nSemantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018).\n\nWe present a retrieval-augmented semantic parser that conditions on entity-aware encoder states and uses constrained decoding to ensure executable outputs, yielding gains on compositional and conversational KGQA tasks.\nRelated Work\n\nSelf-supervised learning (SSL) in computer vision has progressed rapidly through pretext tasks and instance discrimination, yielding strong transfer to downstream tasks (He et al., 2020; Grill et al., 2020). Architectural choices and training recipes both strongly affect representation quality, with recent efforts exploring data augmentations, projector heads, and multi-crop strategies (Chen et al., 2020; Caron et al., 2020; Touvron et al., 2021).\n\nContrastive methods maximize agreement between augmented views (Chen et al., 2020). Vision Transformers model long-range dependencies with self-attention (Dosovitskiy et al., 2021). Clustering-based SSL discretizes latent space to avoid collapse (Caron et al., 2020). Beyond images, multi-modal SSL leverages alignment between text and vision to learn transferable features (Radford et al., 2021).\n\nWe study how architectural inductive biases interact with SSL objectives, providing a controlled comparison of convolutional and transformer backbones under identical augmentation pipelines.\nRelated Work\n\nCompressing over-parameterized transformers is essential for deployment on edge devices. Recent work explores distillation, pruning, quantization, and low-rank factorization to reduce memory and latency while preserving accuracy. However, the interplay between compression objectives and downstream generalization is still underexplored.\n\nKnowledge distillation compresses large models into smaller students (Hinton et al., 2015). Attention transfer passes intermediate alignment signals between layers (Zagoruyko and Komodakis, 2017). Patient distillation slows the learning rate for deeper students (Sun et al., 2019). Pruning removes redundant parameters after training (Han et al., 2016).\n\nWe instead propose task-aware multi-teacher distillation that aligns student representations with both task-specific signals and calibration constraints. This yields improved out-of-domain robustness while maintaining compactness.\nRelated Work\n\nObject detection has evolved from sliding-window classifiers to end-to-end learned architectures that balance speed and accuracy. Recent advances focus on architectural design, training recipes, and transferring general visual representations to detection tasks.\n\nOne-stage detectors predict dense boxes with shared heads (Redmon et al., 2016). Two-stage methods refine proposals for higher accuracy (Ren et al., 2015). Anchor-free designs detect centers or corners (Tian et al., 2019). Self-supervised pretraining transfers to detection benchmarks (He et al., 2020).\n\nWe pursue a complementary direction: a training-time curriculum that progressively tightens localization criteria while distilling features from self-supervised checkpoints, yielding gains at multiple compute budgets.\n",
        "completion": "One-stage detectors predict dense boxes with shared heads (Redmon et al., 2016). Two-stage methods refine proposals for higher accuracy (Ren et al., 2015). Anchor-free designs detect centers or corners (Tian et al., 2019). Self-supervised pretraining transfers to detection benchmarks (He et al., 2020)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\nIntroduction\n\nQuestion Answering over Knowledge Graphs\n\nKnowledge graph question answering (KGQA) aims to answer natural language questions by reasoning over entities and relations. Methods range from symbolic semantic parsing to neural approaches that operate over learned representations, often combining retrieval and reasoning components (Lan and Jiang, 2020; Chen et al., 2021). Benchmarks such as WebQuestionsSP and ComplexWebQuestions have spurred progress on multi-hop reasoning and compositional generalization.\n\nSemantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018).\n\nWe present a retrieval-augmented semantic parser that conditions on entity-aware encoder states and uses constrained decoding to ensure executable outputs, yielding gains on compositional and conversational KGQA tasks.\nRelated Work\n\nSelf-supervised learning (SSL) in computer vision has progressed rapidly through pretext tasks and instance discrimination, yielding strong transfer to downstream tasks (He et al., 2020; Grill et al., 2020). Architectural choices and training recipes both strongly affect representation quality, with recent efforts exploring data augmentations, projector heads, and multi-crop strategies (Chen et al., 2020; Caron et al., 2020; Touvron et al., 2021).\n\nContrastive methods maximize agreement between augmented views (Chen et al., 2020). Vision Transformers model long-range dependencies with self-attention (Dosovitskiy et al., 2021). Clustering-based SSL discretizes latent space to avoid collapse (Caron et al., 2020). Beyond images, multi-modal SSL leverages alignment between text and vision to learn transferable features (Radford et al., 2021).\n\nWe study how architectural inductive biases interact with SSL objectives, providing a controlled comparison of convolutional and transformer backbones under identical augmentation pipelines.\nRelated Work\n\nCompressing over-parameterized transformers is essential for deployment on edge devices. Recent work explores distillation, pruning, quantization, and low-rank factorization to reduce memory and latency while preserving accuracy. However, the interplay between compression objectives and downstream generalization is still underexplored.\n\nKnowledge distillation compresses large models into smaller students (Hinton et al., 2015). Attention transfer passes intermediate alignment signals between layers (Zagoruyko and Komodakis, 2017). Patient distillation slows the learning rate for deeper students (Sun et al., 2019). Pruning removes redundant parameters after training (Han et al., 2016).\n\nWe instead propose task-aware multi-teacher distillation that aligns student representations with both task-specific signals and calibration constraints. This yields improved out-of-domain robustness while maintaining compactness.\nRelated Work\n\nObject detection has evolved from sliding-window classifiers to end-to-end learned architectures that balance speed and accuracy. Recent advances focus on architectural design, training recipes, and transferring general visual representations to detection tasks.\n\nOne-stage detectors predict dense boxes with shared heads (Redmon et al., 2016). Two-stage methods refine proposals for higher accuracy (Ren et al., 2015). Anchor-free designs detect centers or corners (Tian et al., 2019). Self-supervised pretraining transfers to detection benchmarks (He et al., 2020).\n\nWe pursue a complementary direction: a training-time curriculum that progressively tightens localization criteria while distilling features from self-supervised checkpoints, yielding gains at multiple compute budgets.\nRelated Work\n\nText summarization research spans extractive selection and abstractive generation. Extractive methods pick salient sentences using graph centrality or classification (Mihalcea and Tarau, 2004; Narayan et al., 2018). Neural abstractive approaches leverage encoder\u2013decoder architectures to generate summaries (Nadesan et al., 2019). Pretrained language models such as BART and T5 have also been used for summarization (Lee and Ortega, 2020; Patel et al., 2021). ROUGE and newer semantic metrics measure summary quality (Chen and Alves, 2020; Kuo et al., 2022). Recent work also explores controllable summarization with aspect or length constraints (Fan et al., 2018; He et al., 2020). Our work studies domain adaptation for abstractive summarization in finance, with an emphasis on robust evaluation beyond surface overlap.\n",
        "completion": "Neural abstractive approaches leverage encoder\u2013decoder architectures to generate summaries (Nadesan et al., 2019). Pretrained language models such as BART and T5 have also been used for summarization (Lee and Ortega, 2020; Patel et al., 2021). ROUGE and newer semantic metrics measure summary quality (Chen and Alves, 2020; Kuo et al., 2022)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\nIntroduction\n\nQuestion Answering over Knowledge Graphs\n\nKnowledge graph question answering (KGQA) aims to answer natural language questions by reasoning over entities and relations. Methods range from symbolic semantic parsing to neural approaches that operate over learned representations, often combining retrieval and reasoning components (Lan and Jiang, 2020; Chen et al., 2021). Benchmarks such as WebQuestionsSP and ComplexWebQuestions have spurred progress on multi-hop reasoning and compositional generalization.\n\nSemantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018).\n\nWe present a retrieval-augmented semantic parser that conditions on entity-aware encoder states and uses constrained decoding to ensure executable outputs, yielding gains on compositional and conversational KGQA tasks.\nRelated Work\n\nSelf-supervised learning (SSL) in computer vision has progressed rapidly through pretext tasks and instance discrimination, yielding strong transfer to downstream tasks (He et al., 2020; Grill et al., 2020). Architectural choices and training recipes both strongly affect representation quality, with recent efforts exploring data augmentations, projector heads, and multi-crop strategies (Chen et al., 2020; Caron et al., 2020; Touvron et al., 2021).\n\nContrastive methods maximize agreement between augmented views (Chen et al., 2020). Vision Transformers model long-range dependencies with self-attention (Dosovitskiy et al., 2021). Clustering-based SSL discretizes latent space to avoid collapse (Caron et al., 2020). Beyond images, multi-modal SSL leverages alignment between text and vision to learn transferable features (Radford et al., 2021).\n\nWe study how architectural inductive biases interact with SSL objectives, providing a controlled comparison of convolutional and transformer backbones under identical augmentation pipelines.\nRelated Work\n\nCompressing over-parameterized transformers is essential for deployment on edge devices. Recent work explores distillation, pruning, quantization, and low-rank factorization to reduce memory and latency while preserving accuracy. However, the interplay between compression objectives and downstream generalization is still underexplored.\n\nKnowledge distillation compresses large models into smaller students (Hinton et al., 2015). Attention transfer passes intermediate alignment signals between layers (Zagoruyko and Komodakis, 2017). Patient distillation slows the learning rate for deeper students (Sun et al., 2019). Pruning removes redundant parameters after training (Han et al., 2016).\n\nWe instead propose task-aware multi-teacher distillation that aligns student representations with both task-specific signals and calibration constraints. This yields improved out-of-domain robustness while maintaining compactness.\nRelated Work\n\nObject detection has evolved from sliding-window classifiers to end-to-end learned architectures that balance speed and accuracy. Recent advances focus on architectural design, training recipes, and transferring general visual representations to detection tasks.\n\nOne-stage detectors predict dense boxes with shared heads (Redmon et al., 2016). Two-stage methods refine proposals for higher accuracy (Ren et al., 2015). Anchor-free designs detect centers or corners (Tian et al., 2019). Self-supervised pretraining transfers to detection benchmarks (He et al., 2020).\n\nWe pursue a complementary direction: a training-time curriculum that progressively tightens localization criteria while distilling features from self-supervised checkpoints, yielding gains at multiple compute budgets.\nRelated Work\n\nText summarization research spans extractive selection and abstractive generation. Extractive methods pick salient sentences using graph centrality or classification (Mihalcea and Tarau, 2004; Narayan et al., 2018). Neural abstractive approaches leverage encoder\u2013decoder architectures to generate summaries (Nadesan et al., 2019). Pretrained language models such as BART and T5 have also been used for summarization (Lee and Ortega, 2020; Patel et al., 2021). ROUGE and newer semantic metrics measure summary quality (Chen and Alves, 2020; Kuo et al., 2022). Recent work also explores controllable summarization with aspect or length constraints (Fan et al., 2018; He et al., 2020). Our work studies domain adaptation for abstractive summarization in finance, with an emphasis on robust evaluation beyond surface overlap.\nRelated Work\n\nHuman-computer interaction (HCI) perspectives on adaptive interfaces span empirical studies, conceptual frameworks, and practical design guidelines. A unifying challenge is how to align adaptivity with user goals while maintaining transparency and accountability.\n\nTeevan et al. (2016) study microproductivity in enterprise settings. Dourish (2001) articulates embodied interaction as a lens on system design. Amershi et al. (2019) compile guidelines for human-AI interaction. Green and Hu (2018) analyze selective labels in risk assessments.\n\nBuilding on these threads, we propose an adaptive explanation system that personalizes the level of detail while preserving auditability through interaction logs and confidence disclosures.\n",
        "completion": "Teevan et al. (2016) study microproductivity in enterprise settings. Dourish (2001) articulates embodied interaction as a lens on system design. Amershi et al. (2019) compile guidelines for human-AI interaction. Green and Hu (2018) analyze selective labels in risk assessments."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\nIntroduction\n\nQuestion Answering over Knowledge Graphs\n\nKnowledge graph question answering (KGQA) aims to answer natural language questions by reasoning over entities and relations. Methods range from symbolic semantic parsing to neural approaches that operate over learned representations, often combining retrieval and reasoning components (Lan and Jiang, 2020; Chen et al., 2021). Benchmarks such as WebQuestionsSP and ComplexWebQuestions have spurred progress on multi-hop reasoning and compositional generalization.\n\nSemantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018).\n\nWe present a retrieval-augmented semantic parser that conditions on entity-aware encoder states and uses constrained decoding to ensure executable outputs, yielding gains on compositional and conversational KGQA tasks.\nRelated Work\n\nSelf-supervised learning (SSL) in computer vision has progressed rapidly through pretext tasks and instance discrimination, yielding strong transfer to downstream tasks (He et al., 2020; Grill et al., 2020). Architectural choices and training recipes both strongly affect representation quality, with recent efforts exploring data augmentations, projector heads, and multi-crop strategies (Chen et al., 2020; Caron et al., 2020; Touvron et al., 2021).\n\nContrastive methods maximize agreement between augmented views (Chen et al., 2020). Vision Transformers model long-range dependencies with self-attention (Dosovitskiy et al., 2021). Clustering-based SSL discretizes latent space to avoid collapse (Caron et al., 2020). Beyond images, multi-modal SSL leverages alignment between text and vision to learn transferable features (Radford et al., 2021).\n\nWe study how architectural inductive biases interact with SSL objectives, providing a controlled comparison of convolutional and transformer backbones under identical augmentation pipelines.\nRelated Work\n\nCompressing over-parameterized transformers is essential for deployment on edge devices. Recent work explores distillation, pruning, quantization, and low-rank factorization to reduce memory and latency while preserving accuracy. However, the interplay between compression objectives and downstream generalization is still underexplored.\n\nKnowledge distillation compresses large models into smaller students (Hinton et al., 2015). Attention transfer passes intermediate alignment signals between layers (Zagoruyko and Komodakis, 2017). Patient distillation slows the learning rate for deeper students (Sun et al., 2019). Pruning removes redundant parameters after training (Han et al., 2016).\n\nWe instead propose task-aware multi-teacher distillation that aligns student representations with both task-specific signals and calibration constraints. This yields improved out-of-domain robustness while maintaining compactness.\nRelated Work\n\nObject detection has evolved from sliding-window classifiers to end-to-end learned architectures that balance speed and accuracy. Recent advances focus on architectural design, training recipes, and transferring general visual representations to detection tasks.\n\nOne-stage detectors predict dense boxes with shared heads (Redmon et al., 2016). Two-stage methods refine proposals for higher accuracy (Ren et al., 2015). Anchor-free designs detect centers or corners (Tian et al., 2019). Self-supervised pretraining transfers to detection benchmarks (He et al., 2020).\n\nWe pursue a complementary direction: a training-time curriculum that progressively tightens localization criteria while distilling features from self-supervised checkpoints, yielding gains at multiple compute budgets.\nRelated Work\n\nText summarization research spans extractive selection and abstractive generation. Extractive methods pick salient sentences using graph centrality or classification (Mihalcea and Tarau, 2004; Narayan et al., 2018). Neural abstractive approaches leverage encoder\u2013decoder architectures to generate summaries (Nadesan et al., 2019). Pretrained language models such as BART and T5 have also been used for summarization (Lee and Ortega, 2020; Patel et al., 2021). ROUGE and newer semantic metrics measure summary quality (Chen and Alves, 2020; Kuo et al., 2022). Recent work also explores controllable summarization with aspect or length constraints (Fan et al., 2018; He et al., 2020). Our work studies domain adaptation for abstractive summarization in finance, with an emphasis on robust evaluation beyond surface overlap.\nRelated Work\n\nHuman-computer interaction (HCI) perspectives on adaptive interfaces span empirical studies, conceptual frameworks, and practical design guidelines. A unifying challenge is how to align adaptivity with user goals while maintaining transparency and accountability.\n\nTeevan et al. (2016) study microproductivity in enterprise settings. Dourish (2001) articulates embodied interaction as a lens on system design. Amershi et al. (2019) compile guidelines for human-AI interaction. Green and Hu (2018) analyze selective labels in risk assessments.\n\nBuilding on these threads, we propose an adaptive explanation system that personalizes the level of detail while preserving auditability through interaction logs and confidence disclosures.\nRelated Work\n\nFederated learning (FL) enables multiple clients to jointly train models while keeping data local (McMahan et al., 2017). Subsequent research explored aggregation schemes, heterogeneity mitigation, and communication efficiency to cope with non-IID data and constrained networks (Kairouz et al., 2021).\n\nPrior studies on differential privacy in FL analyze noise calibration and client-level guarantees (Zhang et al., 2020; Yu and Kairouz, 2021). Gradient compression can reduce communication overhead by sparsification and quantization (Alistarh et al., 2017; Lin et al., 2018). Adversarial robustness has also been examined in federated training (Bhagoji et al., 2019; Fang et al., 2020). Our work specifically targets secure aggregation under heterogeneous client participation by coupling privacy accounting with topology-aware compression.\n\n",
        "completion": "Prior studies on differential privacy in FL analyze noise calibration and client-level guarantees (Zhang et al., 2020; Yu and Kairouz, 2021). Gradient compression can reduce communication overhead by sparsification and quantization (Alistarh et al., 2017; Lin et al., 2018). Adversarial robustness has also been examined in federated training (Bhagoji et al., 2019; Fang et al., 2020)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\nIntroduction\n\nQuestion Answering over Knowledge Graphs\n\nKnowledge graph question answering (KGQA) aims to answer natural language questions by reasoning over entities and relations. Methods range from symbolic semantic parsing to neural approaches that operate over learned representations, often combining retrieval and reasoning components (Lan and Jiang, 2020; Chen et al., 2021). Benchmarks such as WebQuestionsSP and ComplexWebQuestions have spurred progress on multi-hop reasoning and compositional generalization.\n\nSemantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018).\n\nWe present a retrieval-augmented semantic parser that conditions on entity-aware encoder states and uses constrained decoding to ensure executable outputs, yielding gains on compositional and conversational KGQA tasks.\nRelated Work\n\nSelf-supervised learning (SSL) in computer vision has progressed rapidly through pretext tasks and instance discrimination, yielding strong transfer to downstream tasks (He et al., 2020; Grill et al., 2020). Architectural choices and training recipes both strongly affect representation quality, with recent efforts exploring data augmentations, projector heads, and multi-crop strategies (Chen et al., 2020; Caron et al., 2020; Touvron et al., 2021).\n\nContrastive methods maximize agreement between augmented views (Chen et al., 2020). Vision Transformers model long-range dependencies with self-attention (Dosovitskiy et al., 2021). Clustering-based SSL discretizes latent space to avoid collapse (Caron et al., 2020). Beyond images, multi-modal SSL leverages alignment between text and vision to learn transferable features (Radford et al., 2021).\n\nWe study how architectural inductive biases interact with SSL objectives, providing a controlled comparison of convolutional and transformer backbones under identical augmentation pipelines.\nRelated Work\n\nCompressing over-parameterized transformers is essential for deployment on edge devices. Recent work explores distillation, pruning, quantization, and low-rank factorization to reduce memory and latency while preserving accuracy. However, the interplay between compression objectives and downstream generalization is still underexplored.\n\nKnowledge distillation compresses large models into smaller students (Hinton et al., 2015). Attention transfer passes intermediate alignment signals between layers (Zagoruyko and Komodakis, 2017). Patient distillation slows the learning rate for deeper students (Sun et al., 2019). Pruning removes redundant parameters after training (Han et al., 2016).\n\nWe instead propose task-aware multi-teacher distillation that aligns student representations with both task-specific signals and calibration constraints. This yields improved out-of-domain robustness while maintaining compactness.\nRelated Work\n\nObject detection has evolved from sliding-window classifiers to end-to-end learned architectures that balance speed and accuracy. Recent advances focus on architectural design, training recipes, and transferring general visual representations to detection tasks.\n\nOne-stage detectors predict dense boxes with shared heads (Redmon et al., 2016). Two-stage methods refine proposals for higher accuracy (Ren et al., 2015). Anchor-free designs detect centers or corners (Tian et al., 2019). Self-supervised pretraining transfers to detection benchmarks (He et al., 2020).\n\nWe pursue a complementary direction: a training-time curriculum that progressively tightens localization criteria while distilling features from self-supervised checkpoints, yielding gains at multiple compute budgets.\nRelated Work\n\nText summarization research spans extractive selection and abstractive generation. Extractive methods pick salient sentences using graph centrality or classification (Mihalcea and Tarau, 2004; Narayan et al., 2018). Neural abstractive approaches leverage encoder\u2013decoder architectures to generate summaries (Nadesan et al., 2019). Pretrained language models such as BART and T5 have also been used for summarization (Lee and Ortega, 2020; Patel et al., 2021). ROUGE and newer semantic metrics measure summary quality (Chen and Alves, 2020; Kuo et al., 2022). Recent work also explores controllable summarization with aspect or length constraints (Fan et al., 2018; He et al., 2020). Our work studies domain adaptation for abstractive summarization in finance, with an emphasis on robust evaluation beyond surface overlap.\nRelated Work\n\nHuman-computer interaction (HCI) perspectives on adaptive interfaces span empirical studies, conceptual frameworks, and practical design guidelines. A unifying challenge is how to align adaptivity with user goals while maintaining transparency and accountability.\n\nTeevan et al. (2016) study microproductivity in enterprise settings. Dourish (2001) articulates embodied interaction as a lens on system design. Amershi et al. (2019) compile guidelines for human-AI interaction. Green and Hu (2018) analyze selective labels in risk assessments.\n\nBuilding on these threads, we propose an adaptive explanation system that personalizes the level of detail while preserving auditability through interaction logs and confidence disclosures.\nRelated Work\n\nFederated learning (FL) enables multiple clients to jointly train models while keeping data local (McMahan et al., 2017). Subsequent research explored aggregation schemes, heterogeneity mitigation, and communication efficiency to cope with non-IID data and constrained networks (Kairouz et al., 2021).\n\nPrior studies on differential privacy in FL analyze noise calibration and client-level guarantees (Zhang et al., 2020; Yu and Kairouz, 2021). Gradient compression can reduce communication overhead by sparsification and quantization (Alistarh et al., 2017; Lin et al., 2018). Adversarial robustness has also been examined in federated training (Bhagoji et al., 2019; Fang et al., 2020). Our work specifically targets secure aggregation under heterogeneous client participation by coupling privacy accounting with topology-aware compression.\n\nRelated Work\n\nFairness in recommender systems spans user-centric, provider-centric, and societal objectives (Ekstrand et al., 2022). Optimization-based re-ranking, calibration, and debiasing have been proposed to mitigate disparate exposure and amplify diverse content while preserving utility (Singh and Joachims, 2018; Steck, 2018; Diaz et al., 2020).\n\nExposure constraints allocate recommendation slots proportionally (Bower et al., 2020). Counterfactual evaluation estimates policy value from logged data (Swaminathan and Joachims, 2015). Popularity bias skews rankings toward head items (Abdollahpouri et al., 2019). Recent works combine exposure-aware training with causal adjustments to address confounding in feedback loops (Bonner and Vasile, 2018; Schnabel et al., 2016).\n\nWe propose a provider-fair training objective paired with doubly robust evaluation to improve both exposure equity and offline estimation fidelity.\n",
        "completion": "Exposure constraints allocate recommendation slots proportionally (Bower et al., 2020). Counterfactual evaluation estimates policy value from logged data (Swaminathan and Joachims, 2015). Popularity bias skews rankings toward head items (Abdollahpouri et al., 2019)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\nIntroduction\n\nQuestion Answering over Knowledge Graphs\n\nKnowledge graph question answering (KGQA) aims to answer natural language questions by reasoning over entities and relations. Methods range from symbolic semantic parsing to neural approaches that operate over learned representations, often combining retrieval and reasoning components (Lan and Jiang, 2020; Chen et al., 2021). Benchmarks such as WebQuestionsSP and ComplexWebQuestions have spurred progress on multi-hop reasoning and compositional generalization.\n\nSemantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018).\n\nWe present a retrieval-augmented semantic parser that conditions on entity-aware encoder states and uses constrained decoding to ensure executable outputs, yielding gains on compositional and conversational KGQA tasks.\nRelated Work\n\nSelf-supervised learning (SSL) in computer vision has progressed rapidly through pretext tasks and instance discrimination, yielding strong transfer to downstream tasks (He et al., 2020; Grill et al., 2020). Architectural choices and training recipes both strongly affect representation quality, with recent efforts exploring data augmentations, projector heads, and multi-crop strategies (Chen et al., 2020; Caron et al., 2020; Touvron et al., 2021).\n\nContrastive methods maximize agreement between augmented views (Chen et al., 2020). Vision Transformers model long-range dependencies with self-attention (Dosovitskiy et al., 2021). Clustering-based SSL discretizes latent space to avoid collapse (Caron et al., 2020). Beyond images, multi-modal SSL leverages alignment between text and vision to learn transferable features (Radford et al., 2021).\n\nWe study how architectural inductive biases interact with SSL objectives, providing a controlled comparison of convolutional and transformer backbones under identical augmentation pipelines.\nRelated Work\n\nCompressing over-parameterized transformers is essential for deployment on edge devices. Recent work explores distillation, pruning, quantization, and low-rank factorization to reduce memory and latency while preserving accuracy. However, the interplay between compression objectives and downstream generalization is still underexplored.\n\nKnowledge distillation compresses large models into smaller students (Hinton et al., 2015). Attention transfer passes intermediate alignment signals between layers (Zagoruyko and Komodakis, 2017). Patient distillation slows the learning rate for deeper students (Sun et al., 2019). Pruning removes redundant parameters after training (Han et al., 2016).\n\nWe instead propose task-aware multi-teacher distillation that aligns student representations with both task-specific signals and calibration constraints. This yields improved out-of-domain robustness while maintaining compactness.\nRelated Work\n\nObject detection has evolved from sliding-window classifiers to end-to-end learned architectures that balance speed and accuracy. Recent advances focus on architectural design, training recipes, and transferring general visual representations to detection tasks.\n\nOne-stage detectors predict dense boxes with shared heads (Redmon et al., 2016). Two-stage methods refine proposals for higher accuracy (Ren et al., 2015). Anchor-free designs detect centers or corners (Tian et al., 2019). Self-supervised pretraining transfers to detection benchmarks (He et al., 2020).\n\nWe pursue a complementary direction: a training-time curriculum that progressively tightens localization criteria while distilling features from self-supervised checkpoints, yielding gains at multiple compute budgets.\nRelated Work\n\nText summarization research spans extractive selection and abstractive generation. Extractive methods pick salient sentences using graph centrality or classification (Mihalcea and Tarau, 2004; Narayan et al., 2018). Neural abstractive approaches leverage encoder\u2013decoder architectures to generate summaries (Nadesan et al., 2019). Pretrained language models such as BART and T5 have also been used for summarization (Lee and Ortega, 2020; Patel et al., 2021). ROUGE and newer semantic metrics measure summary quality (Chen and Alves, 2020; Kuo et al., 2022). Recent work also explores controllable summarization with aspect or length constraints (Fan et al., 2018; He et al., 2020). Our work studies domain adaptation for abstractive summarization in finance, with an emphasis on robust evaluation beyond surface overlap.\nRelated Work\n\nHuman-computer interaction (HCI) perspectives on adaptive interfaces span empirical studies, conceptual frameworks, and practical design guidelines. A unifying challenge is how to align adaptivity with user goals while maintaining transparency and accountability.\n\nTeevan et al. (2016) study microproductivity in enterprise settings. Dourish (2001) articulates embodied interaction as a lens on system design. Amershi et al. (2019) compile guidelines for human-AI interaction. Green and Hu (2018) analyze selective labels in risk assessments.\n\nBuilding on these threads, we propose an adaptive explanation system that personalizes the level of detail while preserving auditability through interaction logs and confidence disclosures.\nRelated Work\n\nFederated learning (FL) enables multiple clients to jointly train models while keeping data local (McMahan et al., 2017). Subsequent research explored aggregation schemes, heterogeneity mitigation, and communication efficiency to cope with non-IID data and constrained networks (Kairouz et al., 2021).\n\nPrior studies on differential privacy in FL analyze noise calibration and client-level guarantees (Zhang et al., 2020; Yu and Kairouz, 2021). Gradient compression can reduce communication overhead by sparsification and quantization (Alistarh et al., 2017; Lin et al., 2018). Adversarial robustness has also been examined in federated training (Bhagoji et al., 2019; Fang et al., 2020). Our work specifically targets secure aggregation under heterogeneous client participation by coupling privacy accounting with topology-aware compression.\n\nRelated Work\n\nFairness in recommender systems spans user-centric, provider-centric, and societal objectives (Ekstrand et al., 2022). Optimization-based re-ranking, calibration, and debiasing have been proposed to mitigate disparate exposure and amplify diverse content while preserving utility (Singh and Joachims, 2018; Steck, 2018; Diaz et al., 2020).\n\nExposure constraints allocate recommendation slots proportionally (Bower et al., 2020). Counterfactual evaluation estimates policy value from logged data (Swaminathan and Joachims, 2015). Popularity bias skews rankings toward head items (Abdollahpouri et al., 2019). Recent works combine exposure-aware training with causal adjustments to address confounding in feedback loops (Bonner and Vasile, 2018; Schnabel et al., 2016).\n\nWe propose a provider-fair training objective paired with doubly robust evaluation to improve both exposure equity and offline estimation fidelity.\nRelated Work\n\nPrompt-based learning has emerged as a central paradigm for adapting large language models to downstream tasks with limited supervision. Early work explored discrete prompts and verbalizers to align model outputs with label spaces, and subsequent research examined soft prompts and lightweight adapters to reduce training cost while preserving performance across tasks.\n\nPrompt calibration addresses label bias in few-shot settings (Zhao et al., 2021). Prefix-tuning adds trainable vectors to the input space (Li and Liang, 2021). Mixture-of-experts adapters distribute capacity across tasks (Pfeiffer et al., 2022). Data pruning before prompting improves robustness (Sorscher et al., 2022).\n\nIn contrast, our approach studies how prompt selection interacts with task uncertainty, proposing an uncertainty-aware selector that dynamically chooses prompts using confidence estimates.\n",
        "completion": "Prompt calibration addresses label bias in few-shot settings (Zhao et al., 2021). Prefix-tuning adds trainable vectors to the input space (Li and Liang, 2021). Mixture-of-experts adapters distribute capacity across tasks (Pfeiffer et al., 2022). Data pruning before prompting improves robustness (Sorscher et al., 2022)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\nIntroduction\n\nQuestion Answering over Knowledge Graphs\n\nKnowledge graph question answering (KGQA) aims to answer natural language questions by reasoning over entities and relations. Methods range from symbolic semantic parsing to neural approaches that operate over learned representations, often combining retrieval and reasoning components (Lan and Jiang, 2020; Chen et al., 2021). Benchmarks such as WebQuestionsSP and ComplexWebQuestions have spurred progress on multi-hop reasoning and compositional generalization.\n\nSemantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018).\n\nWe present a retrieval-augmented semantic parser that conditions on entity-aware encoder states and uses constrained decoding to ensure executable outputs, yielding gains on compositional and conversational KGQA tasks.\nRelated Work\n\nSelf-supervised learning (SSL) in computer vision has progressed rapidly through pretext tasks and instance discrimination, yielding strong transfer to downstream tasks (He et al., 2020; Grill et al., 2020). Architectural choices and training recipes both strongly affect representation quality, with recent efforts exploring data augmentations, projector heads, and multi-crop strategies (Chen et al., 2020; Caron et al., 2020; Touvron et al., 2021).\n\nContrastive methods maximize agreement between augmented views (Chen et al., 2020). Vision Transformers model long-range dependencies with self-attention (Dosovitskiy et al., 2021). Clustering-based SSL discretizes latent space to avoid collapse (Caron et al., 2020). Beyond images, multi-modal SSL leverages alignment between text and vision to learn transferable features (Radford et al., 2021).\n\nWe study how architectural inductive biases interact with SSL objectives, providing a controlled comparison of convolutional and transformer backbones under identical augmentation pipelines.\nRelated Work\n\nCompressing over-parameterized transformers is essential for deployment on edge devices. Recent work explores distillation, pruning, quantization, and low-rank factorization to reduce memory and latency while preserving accuracy. However, the interplay between compression objectives and downstream generalization is still underexplored.\n\nKnowledge distillation compresses large models into smaller students (Hinton et al., 2015). Attention transfer passes intermediate alignment signals between layers (Zagoruyko and Komodakis, 2017). Patient distillation slows the learning rate for deeper students (Sun et al., 2019). Pruning removes redundant parameters after training (Han et al., 2016).\n\nWe instead propose task-aware multi-teacher distillation that aligns student representations with both task-specific signals and calibration constraints. This yields improved out-of-domain robustness while maintaining compactness.\nRelated Work\n\nObject detection has evolved from sliding-window classifiers to end-to-end learned architectures that balance speed and accuracy. Recent advances focus on architectural design, training recipes, and transferring general visual representations to detection tasks.\n\nOne-stage detectors predict dense boxes with shared heads (Redmon et al., 2016). Two-stage methods refine proposals for higher accuracy (Ren et al., 2015). Anchor-free designs detect centers or corners (Tian et al., 2019). Self-supervised pretraining transfers to detection benchmarks (He et al., 2020).\n\nWe pursue a complementary direction: a training-time curriculum that progressively tightens localization criteria while distilling features from self-supervised checkpoints, yielding gains at multiple compute budgets.\nRelated Work\n\nText summarization research spans extractive selection and abstractive generation. Extractive methods pick salient sentences using graph centrality or classification (Mihalcea and Tarau, 2004; Narayan et al., 2018). Neural abstractive approaches leverage encoder\u2013decoder architectures to generate summaries (Nadesan et al., 2019). Pretrained language models such as BART and T5 have also been used for summarization (Lee and Ortega, 2020; Patel et al., 2021). ROUGE and newer semantic metrics measure summary quality (Chen and Alves, 2020; Kuo et al., 2022). Recent work also explores controllable summarization with aspect or length constraints (Fan et al., 2018; He et al., 2020). Our work studies domain adaptation for abstractive summarization in finance, with an emphasis on robust evaluation beyond surface overlap.\nRelated Work\n\nHuman-computer interaction (HCI) perspectives on adaptive interfaces span empirical studies, conceptual frameworks, and practical design guidelines. A unifying challenge is how to align adaptivity with user goals while maintaining transparency and accountability.\n\nTeevan et al. (2016) study microproductivity in enterprise settings. Dourish (2001) articulates embodied interaction as a lens on system design. Amershi et al. (2019) compile guidelines for human-AI interaction. Green and Hu (2018) analyze selective labels in risk assessments.\n\nBuilding on these threads, we propose an adaptive explanation system that personalizes the level of detail while preserving auditability through interaction logs and confidence disclosures.\nRelated Work\n\nFederated learning (FL) enables multiple clients to jointly train models while keeping data local (McMahan et al., 2017). Subsequent research explored aggregation schemes, heterogeneity mitigation, and communication efficiency to cope with non-IID data and constrained networks (Kairouz et al., 2021).\n\nPrior studies on differential privacy in FL analyze noise calibration and client-level guarantees (Zhang et al., 2020; Yu and Kairouz, 2021). Gradient compression can reduce communication overhead by sparsification and quantization (Alistarh et al., 2017; Lin et al., 2018). Adversarial robustness has also been examined in federated training (Bhagoji et al., 2019; Fang et al., 2020). Our work specifically targets secure aggregation under heterogeneous client participation by coupling privacy accounting with topology-aware compression.\n\nRelated Work\n\nFairness in recommender systems spans user-centric, provider-centric, and societal objectives (Ekstrand et al., 2022). Optimization-based re-ranking, calibration, and debiasing have been proposed to mitigate disparate exposure and amplify diverse content while preserving utility (Singh and Joachims, 2018; Steck, 2018; Diaz et al., 2020).\n\nExposure constraints allocate recommendation slots proportionally (Bower et al., 2020). Counterfactual evaluation estimates policy value from logged data (Swaminathan and Joachims, 2015). Popularity bias skews rankings toward head items (Abdollahpouri et al., 2019). Recent works combine exposure-aware training with causal adjustments to address confounding in feedback loops (Bonner and Vasile, 2018; Schnabel et al., 2016).\n\nWe propose a provider-fair training objective paired with doubly robust evaluation to improve both exposure equity and offline estimation fidelity.\nRelated Work\n\nPrompt-based learning has emerged as a central paradigm for adapting large language models to downstream tasks with limited supervision. Early work explored discrete prompts and verbalizers to align model outputs with label spaces, and subsequent research examined soft prompts and lightweight adapters to reduce training cost while preserving performance across tasks.\n\nPrompt calibration addresses label bias in few-shot settings (Zhao et al., 2021). Prefix-tuning adds trainable vectors to the input space (Li and Liang, 2021). Mixture-of-experts adapters distribute capacity across tasks (Pfeiffer et al., 2022). Data pruning before prompting improves robustness (Sorscher et al., 2022).\n\nIn contrast, our approach studies how prompt selection interacts with task uncertainty, proposing an uncertainty-aware selector that dynamically chooses prompts using confidence estimates.\nRelated Work\n\nRobust automatic speech recognition (ASR) in the wild requires adapting to new speakers, accents, and environments. Adversarial domain adaptation aligns feature distributions between source and target speech (Shinohara, 2016; Sun et al., 2017). Self-training leverages pseudo-labels to adapt acoustic models (Kahn et al., 2020). Multilingual corpora provide cross-lingual signals for transfer (Schultz and Waibel, 2001). Unsupervised adaptation with confidence calibration has also been explored (Chen et al., 2019). We propose a hybrid objective that couples invariance with uncertainty-aware selection for noisy target data.\n",
        "completion": "Adversarial domain adaptation aligns feature distributions between source and target speech (Shinohara, 2016; Sun et al., 2017). Self-training leverages pseudo-labels to adapt acoustic models (Kahn et al., 2020). Multilingual corpora provide cross-lingual signals for transfer (Schultz and Waibel, 2001)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\nIntroduction\n\nQuestion Answering over Knowledge Graphs\n\nKnowledge graph question answering (KGQA) aims to answer natural language questions by reasoning over entities and relations. Methods range from symbolic semantic parsing to neural approaches that operate over learned representations, often combining retrieval and reasoning components (Lan and Jiang, 2020; Chen et al., 2021). Benchmarks such as WebQuestionsSP and ComplexWebQuestions have spurred progress on multi-hop reasoning and compositional generalization.\n\nSemantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018).\n\nWe present a retrieval-augmented semantic parser that conditions on entity-aware encoder states and uses constrained decoding to ensure executable outputs, yielding gains on compositional and conversational KGQA tasks.\nRelated Work\n\nSelf-supervised learning (SSL) in computer vision has progressed rapidly through pretext tasks and instance discrimination, yielding strong transfer to downstream tasks (He et al., 2020; Grill et al., 2020). Architectural choices and training recipes both strongly affect representation quality, with recent efforts exploring data augmentations, projector heads, and multi-crop strategies (Chen et al., 2020; Caron et al., 2020; Touvron et al., 2021).\n\nContrastive methods maximize agreement between augmented views (Chen et al., 2020). Vision Transformers model long-range dependencies with self-attention (Dosovitskiy et al., 2021). Clustering-based SSL discretizes latent space to avoid collapse (Caron et al., 2020). Beyond images, multi-modal SSL leverages alignment between text and vision to learn transferable features (Radford et al., 2021).\n\nWe study how architectural inductive biases interact with SSL objectives, providing a controlled comparison of convolutional and transformer backbones under identical augmentation pipelines.\nRelated Work\n\nCompressing over-parameterized transformers is essential for deployment on edge devices. Recent work explores distillation, pruning, quantization, and low-rank factorization to reduce memory and latency while preserving accuracy. However, the interplay between compression objectives and downstream generalization is still underexplored.\n\nKnowledge distillation compresses large models into smaller students (Hinton et al., 2015). Attention transfer passes intermediate alignment signals between layers (Zagoruyko and Komodakis, 2017). Patient distillation slows the learning rate for deeper students (Sun et al., 2019). Pruning removes redundant parameters after training (Han et al., 2016).\n\nWe instead propose task-aware multi-teacher distillation that aligns student representations with both task-specific signals and calibration constraints. This yields improved out-of-domain robustness while maintaining compactness.\nRelated Work\n\nObject detection has evolved from sliding-window classifiers to end-to-end learned architectures that balance speed and accuracy. Recent advances focus on architectural design, training recipes, and transferring general visual representations to detection tasks.\n\nOne-stage detectors predict dense boxes with shared heads (Redmon et al., 2016). Two-stage methods refine proposals for higher accuracy (Ren et al., 2015). Anchor-free designs detect centers or corners (Tian et al., 2019). Self-supervised pretraining transfers to detection benchmarks (He et al., 2020).\n\nWe pursue a complementary direction: a training-time curriculum that progressively tightens localization criteria while distilling features from self-supervised checkpoints, yielding gains at multiple compute budgets.\nRelated Work\n\nText summarization research spans extractive selection and abstractive generation. Extractive methods pick salient sentences using graph centrality or classification (Mihalcea and Tarau, 2004; Narayan et al., 2018). Neural abstractive approaches leverage encoder\u2013decoder architectures to generate summaries (Nadesan et al., 2019). Pretrained language models such as BART and T5 have also been used for summarization (Lee and Ortega, 2020; Patel et al., 2021). ROUGE and newer semantic metrics measure summary quality (Chen and Alves, 2020; Kuo et al., 2022). Recent work also explores controllable summarization with aspect or length constraints (Fan et al., 2018; He et al., 2020). Our work studies domain adaptation for abstractive summarization in finance, with an emphasis on robust evaluation beyond surface overlap.\nRelated Work\n\nHuman-computer interaction (HCI) perspectives on adaptive interfaces span empirical studies, conceptual frameworks, and practical design guidelines. A unifying challenge is how to align adaptivity with user goals while maintaining transparency and accountability.\n\nTeevan et al. (2016) study microproductivity in enterprise settings. Dourish (2001) articulates embodied interaction as a lens on system design. Amershi et al. (2019) compile guidelines for human-AI interaction. Green and Hu (2018) analyze selective labels in risk assessments.\n\nBuilding on these threads, we propose an adaptive explanation system that personalizes the level of detail while preserving auditability through interaction logs and confidence disclosures.\nRelated Work\n\nFederated learning (FL) enables multiple clients to jointly train models while keeping data local (McMahan et al., 2017). Subsequent research explored aggregation schemes, heterogeneity mitigation, and communication efficiency to cope with non-IID data and constrained networks (Kairouz et al., 2021).\n\nPrior studies on differential privacy in FL analyze noise calibration and client-level guarantees (Zhang et al., 2020; Yu and Kairouz, 2021). Gradient compression can reduce communication overhead by sparsification and quantization (Alistarh et al., 2017; Lin et al., 2018). Adversarial robustness has also been examined in federated training (Bhagoji et al., 2019; Fang et al., 2020). Our work specifically targets secure aggregation under heterogeneous client participation by coupling privacy accounting with topology-aware compression.\n\nRelated Work\n\nFairness in recommender systems spans user-centric, provider-centric, and societal objectives (Ekstrand et al., 2022). Optimization-based re-ranking, calibration, and debiasing have been proposed to mitigate disparate exposure and amplify diverse content while preserving utility (Singh and Joachims, 2018; Steck, 2018; Diaz et al., 2020).\n\nExposure constraints allocate recommendation slots proportionally (Bower et al., 2020). Counterfactual evaluation estimates policy value from logged data (Swaminathan and Joachims, 2015). Popularity bias skews rankings toward head items (Abdollahpouri et al., 2019). Recent works combine exposure-aware training with causal adjustments to address confounding in feedback loops (Bonner and Vasile, 2018; Schnabel et al., 2016).\n\nWe propose a provider-fair training objective paired with doubly robust evaluation to improve both exposure equity and offline estimation fidelity.\nRelated Work\n\nPrompt-based learning has emerged as a central paradigm for adapting large language models to downstream tasks with limited supervision. Early work explored discrete prompts and verbalizers to align model outputs with label spaces, and subsequent research examined soft prompts and lightweight adapters to reduce training cost while preserving performance across tasks.\n\nPrompt calibration addresses label bias in few-shot settings (Zhao et al., 2021). Prefix-tuning adds trainable vectors to the input space (Li and Liang, 2021). Mixture-of-experts adapters distribute capacity across tasks (Pfeiffer et al., 2022). Data pruning before prompting improves robustness (Sorscher et al., 2022).\n\nIn contrast, our approach studies how prompt selection interacts with task uncertainty, proposing an uncertainty-aware selector that dynamically chooses prompts using confidence estimates.\nRelated Work\n\nRobust automatic speech recognition (ASR) in the wild requires adapting to new speakers, accents, and environments. Adversarial domain adaptation aligns feature distributions between source and target speech (Shinohara, 2016; Sun et al., 2017). Self-training leverages pseudo-labels to adapt acoustic models (Kahn et al., 2020). Multilingual corpora provide cross-lingual signals for transfer (Schultz and Waibel, 2001). Unsupervised adaptation with confidence calibration has also been explored (Chen et al., 2019). We propose a hybrid objective that couples invariance with uncertainty-aware selection for noisy target data.\nRelated Work: Auditing and Mitigating Bias in Machine Learning\n\nFairness in ML encompasses measurement, explanation, and mitigation under societal and regulatory constraints. Works propose definitions of fairness, procedures for auditing models, and algorithms that balance utility with fairness and privacy.\n\nDemographic parity formalizes constraints on outcome rates across protected groups (Hardt et al., 2016). SHAP explains individual predictions through additive feature attributions (Lundberg and Lee, 2017). Counterfactual fairness holds when predictions are invariant under interventions on sensitive attributes (Kusner et al., 2017). Differential privacy protects training data via noise mechanisms (Dwork et al., 2006).\n\nRecent developments unify fairness constraints with distributionally robust optimization to account for subgroup shifts (Hashimoto et al., 2018). Others focus on auditing pipelines, including dataset bias detection and intersectional subgroup analysis (Zhao et al., 2018; Kearns et al., 2018). We contribute a diagnostic that disentangles allocation from treatment disparity while remaining auditable at scale.\n",
        "completion": "Demographic parity formalizes constraints on outcome rates across protected groups (Hardt et al., 2016). SHAP explains individual predictions through additive feature attributions (Lundberg and Lee, 2017). Counterfactual fairness holds when predictions are invariant under interventions on sensitive attributes (Kusner et al., 2017). Differential privacy protects training data via noise mechanisms (Dwork et al., 2006)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\nIntroduction\n\nQuestion Answering over Knowledge Graphs\n\nKnowledge graph question answering (KGQA) aims to answer natural language questions by reasoning over entities and relations. Methods range from symbolic semantic parsing to neural approaches that operate over learned representations, often combining retrieval and reasoning components (Lan and Jiang, 2020; Chen et al., 2021). Benchmarks such as WebQuestionsSP and ComplexWebQuestions have spurred progress on multi-hop reasoning and compositional generalization.\n\nSemantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018).\n\nWe present a retrieval-augmented semantic parser that conditions on entity-aware encoder states and uses constrained decoding to ensure executable outputs, yielding gains on compositional and conversational KGQA tasks.\nRelated Work\n\nSelf-supervised learning (SSL) in computer vision has progressed rapidly through pretext tasks and instance discrimination, yielding strong transfer to downstream tasks (He et al., 2020; Grill et al., 2020). Architectural choices and training recipes both strongly affect representation quality, with recent efforts exploring data augmentations, projector heads, and multi-crop strategies (Chen et al., 2020; Caron et al., 2020; Touvron et al., 2021).\n\nContrastive methods maximize agreement between augmented views (Chen et al., 2020). Vision Transformers model long-range dependencies with self-attention (Dosovitskiy et al., 2021). Clustering-based SSL discretizes latent space to avoid collapse (Caron et al., 2020). Beyond images, multi-modal SSL leverages alignment between text and vision to learn transferable features (Radford et al., 2021).\n\nWe study how architectural inductive biases interact with SSL objectives, providing a controlled comparison of convolutional and transformer backbones under identical augmentation pipelines.\nRelated Work\n\nCompressing over-parameterized transformers is essential for deployment on edge devices. Recent work explores distillation, pruning, quantization, and low-rank factorization to reduce memory and latency while preserving accuracy. However, the interplay between compression objectives and downstream generalization is still underexplored.\n\nKnowledge distillation compresses large models into smaller students (Hinton et al., 2015). Attention transfer passes intermediate alignment signals between layers (Zagoruyko and Komodakis, 2017). Patient distillation slows the learning rate for deeper students (Sun et al., 2019). Pruning removes redundant parameters after training (Han et al., 2016).\n\nWe instead propose task-aware multi-teacher distillation that aligns student representations with both task-specific signals and calibration constraints. This yields improved out-of-domain robustness while maintaining compactness.\nRelated Work\n\nObject detection has evolved from sliding-window classifiers to end-to-end learned architectures that balance speed and accuracy. Recent advances focus on architectural design, training recipes, and transferring general visual representations to detection tasks.\n\nOne-stage detectors predict dense boxes with shared heads (Redmon et al., 2016). Two-stage methods refine proposals for higher accuracy (Ren et al., 2015). Anchor-free designs detect centers or corners (Tian et al., 2019). Self-supervised pretraining transfers to detection benchmarks (He et al., 2020).\n\nWe pursue a complementary direction: a training-time curriculum that progressively tightens localization criteria while distilling features from self-supervised checkpoints, yielding gains at multiple compute budgets.\nRelated Work\n\nText summarization research spans extractive selection and abstractive generation. Extractive methods pick salient sentences using graph centrality or classification (Mihalcea and Tarau, 2004; Narayan et al., 2018). Neural abstractive approaches leverage encoder\u2013decoder architectures to generate summaries (Nadesan et al., 2019). Pretrained language models such as BART and T5 have also been used for summarization (Lee and Ortega, 2020; Patel et al., 2021). ROUGE and newer semantic metrics measure summary quality (Chen and Alves, 2020; Kuo et al., 2022). Recent work also explores controllable summarization with aspect or length constraints (Fan et al., 2018; He et al., 2020). Our work studies domain adaptation for abstractive summarization in finance, with an emphasis on robust evaluation beyond surface overlap.\nRelated Work\n\nHuman-computer interaction (HCI) perspectives on adaptive interfaces span empirical studies, conceptual frameworks, and practical design guidelines. A unifying challenge is how to align adaptivity with user goals while maintaining transparency and accountability.\n\nTeevan et al. (2016) study microproductivity in enterprise settings. Dourish (2001) articulates embodied interaction as a lens on system design. Amershi et al. (2019) compile guidelines for human-AI interaction. Green and Hu (2018) analyze selective labels in risk assessments.\n\nBuilding on these threads, we propose an adaptive explanation system that personalizes the level of detail while preserving auditability through interaction logs and confidence disclosures.\nRelated Work\n\nFederated learning (FL) enables multiple clients to jointly train models while keeping data local (McMahan et al., 2017). Subsequent research explored aggregation schemes, heterogeneity mitigation, and communication efficiency to cope with non-IID data and constrained networks (Kairouz et al., 2021).\n\nPrior studies on differential privacy in FL analyze noise calibration and client-level guarantees (Zhang et al., 2020; Yu and Kairouz, 2021). Gradient compression can reduce communication overhead by sparsification and quantization (Alistarh et al., 2017; Lin et al., 2018). Adversarial robustness has also been examined in federated training (Bhagoji et al., 2019; Fang et al., 2020). Our work specifically targets secure aggregation under heterogeneous client participation by coupling privacy accounting with topology-aware compression.\n\nRelated Work\n\nFairness in recommender systems spans user-centric, provider-centric, and societal objectives (Ekstrand et al., 2022). Optimization-based re-ranking, calibration, and debiasing have been proposed to mitigate disparate exposure and amplify diverse content while preserving utility (Singh and Joachims, 2018; Steck, 2018; Diaz et al., 2020).\n\nExposure constraints allocate recommendation slots proportionally (Bower et al., 2020). Counterfactual evaluation estimates policy value from logged data (Swaminathan and Joachims, 2015). Popularity bias skews rankings toward head items (Abdollahpouri et al., 2019). Recent works combine exposure-aware training with causal adjustments to address confounding in feedback loops (Bonner and Vasile, 2018; Schnabel et al., 2016).\n\nWe propose a provider-fair training objective paired with doubly robust evaluation to improve both exposure equity and offline estimation fidelity.\nRelated Work\n\nPrompt-based learning has emerged as a central paradigm for adapting large language models to downstream tasks with limited supervision. Early work explored discrete prompts and verbalizers to align model outputs with label spaces, and subsequent research examined soft prompts and lightweight adapters to reduce training cost while preserving performance across tasks.\n\nPrompt calibration addresses label bias in few-shot settings (Zhao et al., 2021). Prefix-tuning adds trainable vectors to the input space (Li and Liang, 2021). Mixture-of-experts adapters distribute capacity across tasks (Pfeiffer et al., 2022). Data pruning before prompting improves robustness (Sorscher et al., 2022).\n\nIn contrast, our approach studies how prompt selection interacts with task uncertainty, proposing an uncertainty-aware selector that dynamically chooses prompts using confidence estimates.\nRelated Work\n\nRobust automatic speech recognition (ASR) in the wild requires adapting to new speakers, accents, and environments. Adversarial domain adaptation aligns feature distributions between source and target speech (Shinohara, 2016; Sun et al., 2017). Self-training leverages pseudo-labels to adapt acoustic models (Kahn et al., 2020). Multilingual corpora provide cross-lingual signals for transfer (Schultz and Waibel, 2001). Unsupervised adaptation with confidence calibration has also been explored (Chen et al., 2019). We propose a hybrid objective that couples invariance with uncertainty-aware selection for noisy target data.\nRelated Work: Auditing and Mitigating Bias in Machine Learning\n\nFairness in ML encompasses measurement, explanation, and mitigation under societal and regulatory constraints. Works propose definitions of fairness, procedures for auditing models, and algorithms that balance utility with fairness and privacy.\n\nDemographic parity formalizes constraints on outcome rates across protected groups (Hardt et al., 2016). SHAP explains individual predictions through additive feature attributions (Lundberg and Lee, 2017). Counterfactual fairness holds when predictions are invariant under interventions on sensitive attributes (Kusner et al., 2017). Differential privacy protects training data via noise mechanisms (Dwork et al., 2006).\n\nRecent developments unify fairness constraints with distributionally robust optimization to account for subgroup shifts (Hashimoto et al., 2018). Others focus on auditing pipelines, including dataset bias detection and intersectional subgroup analysis (Zhao et al., 2018; Kearns et al., 2018). We contribute a diagnostic that disentangles allocation from treatment disparity while remaining auditable at scale.\nIntroduction\n\nReinforcement learning from human feedback (RLHF) aligns generative policies with human preferences by learning a reward proxy and optimizing behavior under safety and stability constraints. Alternatives and complements include supervised fine-tuning and constrained decoding.\n\nPreference modeling from pairwise human comparisons guides policy optimization (Christiano et al., 2017). KL-regularized objectives stabilize updates toward a reference model (Jaques et al., 2019). Synthetic feedback can be generated by reward models (Stiennon et al., 2020). Direct alignment via supervised fine-tuning remains competitive (Ouyang et al., 2022).\n\nWe propose a hybrid objective that blends KL control with offline preference datasets, improving sample efficiency while maintaining alignment quality.\n",
        "completion": "Preference modeling from pairwise human comparisons guides policy optimization (Christiano et al., 2017). KL-regularized objectives stabilize updates toward a reference model (Jaques et al., 2019). Synthetic feedback can be generated by reward models (Stiennon et al., 2020). Direct alignment via supervised fine-tuning remains competitive (Ouyang et al., 2022)."
    },
    {
        "prompt": "Extract the words or sub-sentences that have an issue in citation or argumentation around prior work in the provided example, according to this definition: \n        connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n    a) Sentences are not transitioned from one to another.\n    b) The relationship between sentences describing papers is implied but not explicitly stated.\n    c) Coherence issues appear only as multiple sentences.\n    Examples of coherence issues include:\n        i) Relation between mentioned works is not explicit: \u201cSmith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).\u201d\n        ii) Lack of transitions between sentences: \u201cRecent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.\u201d \n        iii) No explanation of the cited works and relation to their own work: \u201cRecently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).\u201d \n\n        Example text: \n        Related Work\n\nAccessible human-computer interaction (HCI) research explores multimodal interfaces that support users with diverse abilities (Wobbrock and Kane, 2016). Techniques include adaptive content presentation, alternative input modalities, and environment-aware assistance to reduce effort and increase task success (Harper and Yesilada, 2012; Albusays and Chen, 2021).\n\nScreen reader optimizations reduce cognitive load in dense documents (Gleason et al., 2019). Head-mounted displays support situated assistance (Billinghurst et al., 2015). Tactile feedback delivers spatial cues through wearable bands (Kane and Morris, 2018). Integrative toolkits seek to unify these modalities into coherent workflows for navigation and information access (Zhao et al., 2019).\n\nOur work introduces a cross-modal orchestration layer that adaptively routes content across audio, visual, and haptic channels based on context and user preference.\nRelated Work\n\nReinforcement learning (RL) methods vary widely in sample efficiency, stability, and assumptions about interaction. For robotic control, constraints on safety and data collection motivate algorithms that reduce on-policy exploration.\n\nPolicy gradient methods directly optimize expected return via stochastic estimates (Sutton et al., 2000). Model-based RL plans with learned dynamics models (Deisenroth and Rasmussen, 2011). Offline RL learns from fixed datasets without exploration (Levine et al., 2020). Hierarchical RL introduces temporal abstractions through options (Bacon et al., 2017).\n\nWe target the data-efficiency of model-based approaches while leveraging offline datasets to warm-start policies, combining planning priors with conservative value learning for safer real-world deployment.\nIntroduction\n\nQuestion Answering over Knowledge Graphs\n\nKnowledge graph question answering (KGQA) aims to answer natural language questions by reasoning over entities and relations. Methods range from symbolic semantic parsing to neural approaches that operate over learned representations, often combining retrieval and reasoning components (Lan and Jiang, 2020; Chen et al., 2021). Benchmarks such as WebQuestionsSP and ComplexWebQuestions have spurred progress on multi-hop reasoning and compositional generalization.\n\nSemantic parsers map questions to logical forms executable on KGs (Berant et al., 2013). Neural symbolic models learn soft operators over graph neighborhoods (Sun et al., 2018). Subgraph retrieval narrows candidate entities before reasoning (Xiong et al., 2019). Pretraining on text-KG alignments injects entity knowledge into encoders (Yao et al., 2019). Conversational QA introduces coreference and context carryover (Saha et al., 2018).\n\nWe present a retrieval-augmented semantic parser that conditions on entity-aware encoder states and uses constrained decoding to ensure executable outputs, yielding gains on compositional and conversational KGQA tasks.\nRelated Work\n\nSelf-supervised learning (SSL) in computer vision has progressed rapidly through pretext tasks and instance discrimination, yielding strong transfer to downstream tasks (He et al., 2020; Grill et al., 2020). Architectural choices and training recipes both strongly affect representation quality, with recent efforts exploring data augmentations, projector heads, and multi-crop strategies (Chen et al., 2020; Caron et al., 2020; Touvron et al., 2021).\n\nContrastive methods maximize agreement between augmented views (Chen et al., 2020). Vision Transformers model long-range dependencies with self-attention (Dosovitskiy et al., 2021). Clustering-based SSL discretizes latent space to avoid collapse (Caron et al., 2020). Beyond images, multi-modal SSL leverages alignment between text and vision to learn transferable features (Radford et al., 2021).\n\nWe study how architectural inductive biases interact with SSL objectives, providing a controlled comparison of convolutional and transformer backbones under identical augmentation pipelines.\nRelated Work\n\nCompressing over-parameterized transformers is essential for deployment on edge devices. Recent work explores distillation, pruning, quantization, and low-rank factorization to reduce memory and latency while preserving accuracy. However, the interplay between compression objectives and downstream generalization is still underexplored.\n\nKnowledge distillation compresses large models into smaller students (Hinton et al., 2015). Attention transfer passes intermediate alignment signals between layers (Zagoruyko and Komodakis, 2017). Patient distillation slows the learning rate for deeper students (Sun et al., 2019). Pruning removes redundant parameters after training (Han et al., 2016).\n\nWe instead propose task-aware multi-teacher distillation that aligns student representations with both task-specific signals and calibration constraints. This yields improved out-of-domain robustness while maintaining compactness.\nRelated Work\n\nObject detection has evolved from sliding-window classifiers to end-to-end learned architectures that balance speed and accuracy. Recent advances focus on architectural design, training recipes, and transferring general visual representations to detection tasks.\n\nOne-stage detectors predict dense boxes with shared heads (Redmon et al., 2016). Two-stage methods refine proposals for higher accuracy (Ren et al., 2015). Anchor-free designs detect centers or corners (Tian et al., 2019). Self-supervised pretraining transfers to detection benchmarks (He et al., 2020).\n\nWe pursue a complementary direction: a training-time curriculum that progressively tightens localization criteria while distilling features from self-supervised checkpoints, yielding gains at multiple compute budgets.\nRelated Work\n\nText summarization research spans extractive selection and abstractive generation. Extractive methods pick salient sentences using graph centrality or classification (Mihalcea and Tarau, 2004; Narayan et al., 2018). Neural abstractive approaches leverage encoder\u2013decoder architectures to generate summaries (Nadesan et al., 2019). Pretrained language models such as BART and T5 have also been used for summarization (Lee and Ortega, 2020; Patel et al., 2021). ROUGE and newer semantic metrics measure summary quality (Chen and Alves, 2020; Kuo et al., 2022). Recent work also explores controllable summarization with aspect or length constraints (Fan et al., 2018; He et al., 2020). Our work studies domain adaptation for abstractive summarization in finance, with an emphasis on robust evaluation beyond surface overlap.\nRelated Work\n\nHuman-computer interaction (HCI) perspectives on adaptive interfaces span empirical studies, conceptual frameworks, and practical design guidelines. A unifying challenge is how to align adaptivity with user goals while maintaining transparency and accountability.\n\nTeevan et al. (2016) study microproductivity in enterprise settings. Dourish (2001) articulates embodied interaction as a lens on system design. Amershi et al. (2019) compile guidelines for human-AI interaction. Green and Hu (2018) analyze selective labels in risk assessments.\n\nBuilding on these threads, we propose an adaptive explanation system that personalizes the level of detail while preserving auditability through interaction logs and confidence disclosures.\nRelated Work\n\nFederated learning (FL) enables multiple clients to jointly train models while keeping data local (McMahan et al., 2017). Subsequent research explored aggregation schemes, heterogeneity mitigation, and communication efficiency to cope with non-IID data and constrained networks (Kairouz et al., 2021).\n\nPrior studies on differential privacy in FL analyze noise calibration and client-level guarantees (Zhang et al., 2020; Yu and Kairouz, 2021). Gradient compression can reduce communication overhead by sparsification and quantization (Alistarh et al., 2017; Lin et al., 2018). Adversarial robustness has also been examined in federated training (Bhagoji et al., 2019; Fang et al., 2020). Our work specifically targets secure aggregation under heterogeneous client participation by coupling privacy accounting with topology-aware compression.\n\nRelated Work\n\nFairness in recommender systems spans user-centric, provider-centric, and societal objectives (Ekstrand et al., 2022). Optimization-based re-ranking, calibration, and debiasing have been proposed to mitigate disparate exposure and amplify diverse content while preserving utility (Singh and Joachims, 2018; Steck, 2018; Diaz et al., 2020).\n\nExposure constraints allocate recommendation slots proportionally (Bower et al., 2020). Counterfactual evaluation estimates policy value from logged data (Swaminathan and Joachims, 2015). Popularity bias skews rankings toward head items (Abdollahpouri et al., 2019). Recent works combine exposure-aware training with causal adjustments to address confounding in feedback loops (Bonner and Vasile, 2018; Schnabel et al., 2016).\n\nWe propose a provider-fair training objective paired with doubly robust evaluation to improve both exposure equity and offline estimation fidelity.\nRelated Work\n\nPrompt-based learning has emerged as a central paradigm for adapting large language models to downstream tasks with limited supervision. Early work explored discrete prompts and verbalizers to align model outputs with label spaces, and subsequent research examined soft prompts and lightweight adapters to reduce training cost while preserving performance across tasks.\n\nPrompt calibration addresses label bias in few-shot settings (Zhao et al., 2021). Prefix-tuning adds trainable vectors to the input space (Li and Liang, 2021). Mixture-of-experts adapters distribute capacity across tasks (Pfeiffer et al., 2022). Data pruning before prompting improves robustness (Sorscher et al., 2022).\n\nIn contrast, our approach studies how prompt selection interacts with task uncertainty, proposing an uncertainty-aware selector that dynamically chooses prompts using confidence estimates.\nRelated Work\n\nRobust automatic speech recognition (ASR) in the wild requires adapting to new speakers, accents, and environments. Adversarial domain adaptation aligns feature distributions between source and target speech (Shinohara, 2016; Sun et al., 2017). Self-training leverages pseudo-labels to adapt acoustic models (Kahn et al., 2020). Multilingual corpora provide cross-lingual signals for transfer (Schultz and Waibel, 2001). Unsupervised adaptation with confidence calibration has also been explored (Chen et al., 2019). We propose a hybrid objective that couples invariance with uncertainty-aware selection for noisy target data.\nRelated Work: Auditing and Mitigating Bias in Machine Learning\n\nFairness in ML encompasses measurement, explanation, and mitigation under societal and regulatory constraints. Works propose definitions of fairness, procedures for auditing models, and algorithms that balance utility with fairness and privacy.\n\nDemographic parity formalizes constraints on outcome rates across protected groups (Hardt et al., 2016). SHAP explains individual predictions through additive feature attributions (Lundberg and Lee, 2017). Counterfactual fairness holds when predictions are invariant under interventions on sensitive attributes (Kusner et al., 2017). Differential privacy protects training data via noise mechanisms (Dwork et al., 2006).\n\nRecent developments unify fairness constraints with distributionally robust optimization to account for subgroup shifts (Hashimoto et al., 2018). Others focus on auditing pipelines, including dataset bias detection and intersectional subgroup analysis (Zhao et al., 2018; Kearns et al., 2018). We contribute a diagnostic that disentangles allocation from treatment disparity while remaining auditable at scale.\nIntroduction\n\nReinforcement learning from human feedback (RLHF) aligns generative policies with human preferences by learning a reward proxy and optimizing behavior under safety and stability constraints. Alternatives and complements include supervised fine-tuning and constrained decoding.\n\nPreference modeling from pairwise human comparisons guides policy optimization (Christiano et al., 2017). KL-regularized objectives stabilize updates toward a reference model (Jaques et al., 2019). Synthetic feedback can be generated by reward models (Stiennon et al., 2020). Direct alignment via supervised fine-tuning remains competitive (Ouyang et al., 2022).\n\nWe propose a hybrid objective that blends KL control with offline preference datasets, improving sample efficiency while maintaining alignment quality.\nIntroduction\n\nProgram synthesis targets automatic construction of programs from specifications, including examples, natural language, and partial sketches. Approaches span symbolic methods, neural models, and hybrids.\n\nGulwani (2011) developed FlashFill techniques for programming by example. Devlin et al. (2017) explored DeepCoder for learning to write programs from input-output pairs. Chen et al. (2021) presented Codex for general-purpose code generation.\n\nWe focus on semi-structured specifications combining examples and sketches.\n",
        "completion": "Gulwani (2011) developed FlashFill techniques for programming by example. Devlin et al. (2017) explored DeepCoder for learning to write programs from input-output pairs. Chen et al. (2021) presented Codex for general-purpose code generation."
    }
]