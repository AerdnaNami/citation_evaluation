{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e34ff07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "path = \"../synthetic_sampling/synthetic_samples.json\"\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    total_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8895ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../synthetic_sampling/for_synth_sampling.json\", 'r') as f:\n",
    "    real_smpls = json.load(f)\n",
    "\n",
    "total_real = []\n",
    "for papers in real_smpls:\n",
    "    for sample in real_smpls[papers]:\n",
    "        total_real.append({\"span\": sample['text'], \"document\": sample['full_text'], \"start\": sample['start'], \"end\": sample['end'], \"label\": sample['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5bde938",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Unsupported claim', 'Format', 'Coherence', 'Lacks synthesis']\n",
    "\n",
    "for category in categories:\n",
    "    for sample in total_real:\n",
    "        if sample['label'] == category:\n",
    "            total_data[category].append(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d6e0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the structure\n",
    "categories = ['Unsupported claim', 'Format', 'Coherence', 'Lacks synthesis']\n",
    "data = total_data\n",
    "flattened = []\n",
    "for category in categories:\n",
    "    for group in data[category]:\n",
    "        if category == 'Unsupported claim':\n",
    "            label = 'Unsupported_claim'\n",
    "        elif category == 'Lacks synthesis':\n",
    "            label = 'Lacks_synthesis'\n",
    "        else:\n",
    "            label = category\n",
    "            \n",
    "        flattened.append({**group, \"label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10520750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "Record = Dict[str, Any]\n",
    "\n",
    "def split_by_document(\n",
    "    records: List[Record],\n",
    "    train_ratio: float = 0.8,\n",
    "    dev_ratio: float = 0.1,\n",
    "    eval_ratio: float = 0.1,\n",
    "    seed: int = 42,\n",
    ") -> Tuple[List[Record], List[Record], List[Record]]:\n",
    "    assert abs((train_ratio + dev_ratio + eval_ratio) - 1.0) < 1e-9\n",
    "\n",
    "    # ---- group records by document ----\n",
    "    by_doc = defaultdict(list)\n",
    "    for r in records:\n",
    "        by_doc[r[\"document\"]].append(r)\n",
    "\n",
    "    doc_keys = list(by_doc.keys())\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(doc_keys)\n",
    "\n",
    "    n = len(doc_keys)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_dev = int(n * dev_ratio)\n",
    "\n",
    "    train_docs = doc_keys[:n_train]\n",
    "    dev_docs = doc_keys[n_train:n_train + n_dev]\n",
    "    eval_docs = doc_keys[n_train + n_dev:]\n",
    "\n",
    "    def collect(docs):\n",
    "        out = []\n",
    "        for d in docs:\n",
    "            out.extend(by_doc[d])\n",
    "        return out\n",
    "\n",
    "    train = collect(train_docs)\n",
    "    dev = collect(dev_docs)\n",
    "    eval_ = collect(eval_docs)\n",
    "\n",
    "    return train, dev, eval_\n",
    "\n",
    "\n",
    "train_data, dev_data, eval_data = split_by_document(flattened)\n",
    "\n",
    "def save_split(split: dict, path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(split, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d1b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_category(data_by_file):\n",
    "    data_by_category = {\"Unsupported_claim\": [],\n",
    "                        \"Format\": [],\n",
    "                        \"Coherence\": [],\n",
    "                        \"Lacks_synthesis\": []}\n",
    "\n",
    "    for record in data_by_file:\n",
    "        category = record['label']\n",
    "        data_by_category[category].append(record)\n",
    "    return data_by_category\n",
    "\n",
    "train_data = group_by_category(train_data)\n",
    "dev_data = group_by_category(dev_data)\n",
    "eval_data = group_by_category(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21c685fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_def = \"\"\"Issues in citation formatting such as a missing bracket and using the wrong style of citing.\n",
    "    a) Due to preprocessing errors of the source dataset, some words contain hyphens that do not require it, and some are missing hyphens where it is required. Please ignore these types of formatting issues.\n",
    "    b) Highlight the word/citation in which the formatting issue occurs in and not only the issue within the word/citation.\n",
    "    c) Formatting issues appear as either citations or parts of a citation.\n",
    "    Examples of formatting issues include:\n",
    "        i) Narrative citation missing year: “Vatswani et al.” -> should be “Vatswani et al. (2020)”\n",
    "        ii) Wrong citation style: “In (Vatswani et al., 2019)” -> should be “in Vatswani et al. (2019)”\n",
    "        iii) Wrong use of footnotes: \"Vastwani et al. 1\" -> should include the year or be reformatted as a proper footnote.\"\"\"\n",
    "\n",
    "unsupp_def = \"\"\"claim about prior work or statistics w/o citation or evidence. \n",
    "    a) The author should cite at every first mention of a study, paper, shared task, competition or dataset.\n",
    "    b) Specific information to a niche topic, despite sounding like it should be known in that topic of study, should be cited.\n",
    "    c) If a claim is made and is obvious to be a natural deduction from previous statements through common sense (i.e not requiring expert knowledge), then this claim does not fall under ‘Unsupported claim’. For example:\n",
    "        i) “However, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding.” -> it is obvious that creating a dataset is time consuming and mentally demanding.\n",
    "    d) Any mention of “recent works” should be backed up with citations to the works.\n",
    "    e) Unsupported claim issues appear as segments, phrases, sub-sentences or full sentences.\n",
    "    Examples of unsupported claims include:\n",
    "        i) Missing citations for mentions of 'recent works': “and there are many recent works that explore this topic”,\n",
    "        ii) Mention of a previous work and claim without citation: “..., while in a previous study, the authors claim …”,\n",
    "        iii) Mentioning of a specific setup of a task without citation to the work: \".. BERT was used in an AES task trained on essays ..\" \"\"\"\n",
    "\n",
    "lacksynth_def = \"\"\"occurs when either:\n",
    "    a) The author describes or cites papers without connecting them to their own work/argument \n",
    "    b) Or only follows up the summary of previous works with their own contribution without explicitly highlighting the gap their work intends to research.\n",
    "    c) It does not articulate the author's perspective or motivation.\n",
    "    d) A lack of argument/opinion in the first paragraph is permissible as it serves to be the foundation of the author's argument \n",
    "    e) Lacks synthesis issues appear either as single sentences or multiple sentences.\n",
    "    Examples of lack of synthesis include:\n",
    "        i) No elaboration of own contribution/argument:\"Following early neural approaches to question answering, many subsequent studies adopt a pipeline architecture consisting of retrieval and comprehension components. The retrieval component focuses on identifying relevant documents or passages from a large corpus, while the comprehension component extracts an answer span from the retrieved text. Initial models relied on recurrent neural networks with attention mechanisms to encode questions and contexts (Seo et al., 2017; Wang et al., 2017).\"\n",
    "        ii)  No explanation of the cited works and relation to their own work: “Recently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).” \"\"\"\n",
    "\n",
    "coherence_def = \"\"\"connection between cited works is abrupt, lacking relation to each other. It is unclear how one mentioned work is relevant to a prior mentioned work. \n",
    "    a) Sentences are not transitioned from one to another.\n",
    "    b) The relationship between sentences describing papers is implied but not explicitly stated.\n",
    "    c) Coherence issues appear only as multiple sentences.\n",
    "    Examples of coherence issues include:\n",
    "        i) Relation between mentioned works is not explicit: “Smith (2020) identified a relationship between personal belief systems and ethical decision-making frameworks. Moral foundation theory proposes several core dimensions of moral reasoning, including harm, fairness, and authority (Jones, 2015). Audience adaptation has been explored in computational argumentation. Lee et al. (2019) applied moral categories to argument generation tasks. Human annotators often disagree when labeling moral dimensions in text (Nguyen et al., 2018).”\n",
    "        ii) Lack of transitions between sentences: “Recent studies have explored various techniques for enhancing model performance. Smith et al. (2020) introduced a novel architecture that significantly improves accuracy on benchmark datasets. Additionally, Johnson and Lee (2019) proposed a data augmentation method that increases training data diversity.” \n",
    "        iii) No explanation of the cited works and relation to their own work: “Recently, several studies have explored the use of prompting techniques with pre-trained language models to influence model outputs or access latent knowledge (Brown et al., 2020; Gao et al., 2021; Liu et al., 2021; Wei et al., 2022).” \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686b0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    processed_data = {}\n",
    "\n",
    "    for category, samples in data.items():\n",
    "        if category == 'Unsupported_claim':\n",
    "            definition = unsupp_def\n",
    "        elif category == 'Lacks_synthesis':\n",
    "            definition = lacksynth_def\n",
    "        elif category == 'Coherence':\n",
    "            definition = coherence_def\n",
    "        elif category == 'Format':\n",
    "            definition = format_def\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        processed_data.setdefault(category, [])\n",
    "\n",
    "        for sample in samples:\n",
    "            span_text = sample[\"span\"].strip()\n",
    "\n",
    "            prompt = f\"\"\"You are an expert annotator.\n",
    "Task:\n",
    "Return ONLY the spans from the document that match the definition below.\n",
    "Each span MUST be wrapped exactly like this:\n",
    "\n",
    "<span>your span here</span>\n",
    "\n",
    "Do not output anything else.\n",
    "\n",
    "Definition:\n",
    "{definition}\n",
    "\n",
    "Document:\n",
    "{sample['document']}\n",
    "\"\"\"\n",
    "\n",
    "            completion = f\"<span>{span_text}</span>\"\n",
    "\n",
    "            processed_data[category].append({\n",
    "                \"prompt\": prompt,\n",
    "                \"completion\": completion\n",
    "            })\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "train_data = process_data(train_data)\n",
    "dev_data = process_data(dev_data)\n",
    "eval_data = process_data(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68d3ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "categories = {'Unsupported_claim': 'unsupported_claim', 'Lacks_synthesis': \"lacks_synthesis\", 'Coherence': \"coherence\", 'Format': \"format\"}\n",
    "\n",
    "for category in categories:\n",
    "    filename = f\"{categories[category]}/{categories[category]}_training_data.json\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_data[category], f, indent=4)\n",
    "\n",
    "for category in categories:\n",
    "    filename = f\"{categories[category]}/{categories[category]}_dev_data.json\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dev_data[category], f, indent=4)\n",
    "\n",
    "for category in categories:\n",
    "    filename = f\"{categories[category]}/{categories[category]}_eval_data.json\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(eval_data[category], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c62797b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
