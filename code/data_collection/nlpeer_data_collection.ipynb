{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d34d873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 477/477 [00:12<00:00, 39.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import os \n",
    "from tqdm import tqdm \n",
    "\n",
    "nlpeer_path = \"../nlpeer_pdf/ARR-22/data\"\n",
    "complete_reviews = []\n",
    "\n",
    "for folders in tqdm(os.listdir(nlpeer_path)):\n",
    "    if folders == 'meta.json':\n",
    "        continue\n",
    "\n",
    "    v1_folder = os.path.join(nlpeer_path, folders, 'v1', 'reviews.json')\n",
    "\n",
    "    with open(v1_folder, 'r') as f:\n",
    "        review_data = json.load(f)\n",
    "    \n",
    "    if len(review_data) != 0:\n",
    "        review_data = review_data[0]\n",
    "        complete_reviews.append({'id': folders, 'review': review_data['reviewer'], 'summary': review_data['report']['paper_summary'],\n",
    "                                'weaknesses': review_data['report']['summary_of_weaknesses'], 'suggestions': review_data['report']['comments,_suggestions_and_typos']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# keys.env with stored keys required \n",
    "key_path = 'keys.env'\n",
    "load_dotenv(dotenv_path=key_path) \n",
    "api_key = os.getenv(\"API_KEY_IRAA\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def ask_gpt(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"assistant\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96577099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/364 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364/364 [09:54<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "import re \n",
    "\n",
    "for reviews in tqdm(complete_reviews):\n",
    "    weaknesses = reviews['weaknesses']\n",
    "    suggestions = reviews['suggestions']\n",
    "\n",
    "    prompt = f\"\"\" \n",
    "    Provided to you is a review comments on the weaknesses and suggestions for the paper. Please classify it as either relevant to the introduction section, related works section, or neither. \n",
    "    Valid classifications are either: Introduction, Related Works, Neither.\n",
    "\n",
    "    Please have your output strictly at the end of your response and in this format:\n",
    "    {'classification: ...'}\n",
    "\n",
    "    Weaknesses: \n",
    "    {weaknesses}\n",
    "    Suggestions:\n",
    "    {suggestions} \"\"\"\n",
    "\n",
    "    response = ask_gpt(prompt)\n",
    "    match = re.search(r\"classification:\\s*(.*)\", response, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        reviews['gpt_classification'] = match.group(1)\n",
    "    else:\n",
    "        reviews['gpt_classification'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "811b5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/gpt_classifications_reviews_3.json\", 'w') as f:\n",
    "    json.dump(complete_reviews, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d31d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "def load_ids_from_json(path):\n",
    "    \"\"\"Load a JSON file assumed to be a list of dicts with an 'id' key.\"\"\"\n",
    "    path = Path(path)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    items = []\n",
    "    for item in data:\n",
    "        # guard in case some items don't have 'id'\n",
    "        if isinstance(item, dict) and \"id\" in item:\n",
    "            items.append({'id': item[\"id\"], 'gpt_classification': item['gpt_classification']})\n",
    "    return items\n",
    "\n",
    "\n",
    "def most_common_ids_across_files(file1, file2, file3, top_n=None):\n",
    "    all_items = []\n",
    "    for path in [file1, file2, file3]:\n",
    "        all_items.extend(load_ids_from_json(path))\n",
    "    \n",
    "    id_counts = Counter(item[\"id\"] for item in all_items)\n",
    "\n",
    "    id_to_class = {}\n",
    "    for item in all_items:\n",
    "        _id = item[\"id\"]\n",
    "        if _id not in id_to_class:\n",
    "            id_to_class[_id] = item[\"gpt_classification\"]\n",
    "\n",
    "    common = id_counts.most_common(top_n) if top_n is not None else id_counts.most_common()\n",
    "\n",
    "    result = [\n",
    "        {\n",
    "            \"id\": _id,\n",
    "            \"count\": count,\n",
    "            \"gpt_classification\": id_to_class.get(_id)\n",
    "        }\n",
    "        for _id, count in common\n",
    "    ]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "file1 = \"../data/gpt_classifications_reviews_1.json\"\n",
    "file2 = \"../data/gpt_classifications_reviews_2.json\"\n",
    "file3 = \"../data/gpt_classifications_reviews_3.json\"\n",
    "\n",
    "common_ids = most_common_ids_across_files(file1, file2, file3)\n",
    "print(len(common_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2282b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_intro = []\n",
    "\n",
    "for review in common_ids:\n",
    "    if review['gpt_classification'] != \"Neither\":\n",
    "        rw_intro.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7c6af9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:13<00:00,  8.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def copy_file_to_folder(src_file, dst_folder, filename):\n",
    "    src_file = Path(src_file)\n",
    "    dst_folder = Path(dst_folder)\n",
    "\n",
    "    dst_folder.mkdir(parents=True, exist_ok=True)  # create folder if it doesn't exist\n",
    "\n",
    "    shutil.copy2(src_file, dst_folder / f\"{filename}.tei\")  # preserves metadata\n",
    "\n",
    "for review in tqdm(rw_intro):\n",
    "    folder = review['id']\n",
    "    copy_file_to_folder(f\"../nlpeer_pdf/ARR-22/data/{folder}/v1/paper.tei\", \"../data/annotation/tei_files\", f\"pdf_{folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3919c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree \n",
    "from lxml.etree import _Element \n",
    "import re \n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "SPECIAL_EQUIVALENTS = {\"related work\", \"related works\", \"background\", \"past\", \"current\"}\n",
    "\n",
    "def normalize_target(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    # drop any leading numbering like \"2 \" or \"2. \" or \"2.1 \"\n",
    "    s = re.sub(r\"^\\d+(\\.\\d+)*\\s+\", \"\", s)\n",
    "    return s.lower()\n",
    "\n",
    "def find_top_level_heading_blocks_using_title(blocks: List[dict]) -> List[Tuple[Optional[int], str, int]]:\n",
    "    \"\"\"\n",
    "    Build a list of top-level heading candidates from <head> blocks.\n",
    "\n",
    "    Returns list of tuples (section_number_or_None, heading_text, block_index).\n",
    "    - section_number_or_None: int if @n is an integer (no dot), None otherwise.\n",
    "    - heading_text: the text content of the <head> element (this is what we match against).\n",
    "    - block_index: index into the blocks list.\n",
    "\n",
    "    IMPORTANT: Matching will be done against heading_text, NOT the @n value.\n",
    "    We only use @n to determine top-level vs dotted-subsection (e.g., \"3.1\").\n",
    "    \"\"\"\n",
    "    headings: List[Tuple[Optional[int], str, int]] = []\n",
    "\n",
    "    for idx, b in enumerate(blocks):\n",
    "        if b[\"type\"] != \"head\":\n",
    "            continue\n",
    "        el = b.get(\"element\")\n",
    "        head_text = b[\"text\"].strip()\n",
    "        n_attr = None\n",
    "        if el is not None:\n",
    "            n_attr = el.get(\"n\")\n",
    "            if n_attr:\n",
    "                n_attr = n_attr.strip() or None\n",
    "\n",
    "        # If n is an integer (no dot) treat as top-level and record that number\n",
    "        if n_attr and re.fullmatch(r\"\\d+\", n_attr):\n",
    "            num = int(n_attr)\n",
    "            headings.append((num, head_text, idx))\n",
    "            continue\n",
    "\n",
    "        # If n is dotted like \"3.1\" treat as subsection and skip from top-level list\n",
    "        if n_attr and re.fullmatch(r\"\\d+\\.\\d+(\\.\\d+)*\", n_attr):\n",
    "            # skip as top-level candidate (we don't want subsections to be slicing boundaries)\n",
    "            continue\n",
    "\n",
    "        # Otherwise (no n, or n not purely integer), include as unnumbered top-level candidate\n",
    "        # We include it because matching is done on the head text itself.\n",
    "        headings.append((None, head_text, idx))\n",
    "\n",
    "    return headings\n",
    "\n",
    "\n",
    "def fallback_find_headings_in_paragraphs_using_title(blocks: List[dict]) -> List[Tuple[Optional[int], str, int]]:\n",
    "    \"\"\"\n",
    "    Fallback heading detection scanning paragraph blocks for leading numbered headings OR\n",
    "    short title-case paragraphs that look like headings. Returns same shape as above.\n",
    "    This also uses the paragraph text (not numeric n values) for matching.\n",
    "    \"\"\"\n",
    "    headings = []\n",
    "    for idx, b in enumerate(blocks):\n",
    "        if b[\"type\"] != \"p\":\n",
    "            continue\n",
    "        text = b[\"text\"].strip()\n",
    "        m = TOP_LEVEL_HEADING_RE.match(text)\n",
    "        if m:\n",
    "            num = int(m.group(1))\n",
    "            title = m.group(2).strip().rstrip(\" .:;,-\")\n",
    "            headings.append((num, title, idx))\n",
    "        else:\n",
    "            # heuristic: short Title Case paragraph may be an unnumbered heading\n",
    "            words = text.split()\n",
    "            if 1 <= len(words) <= 6 and text == text.title():\n",
    "                headings.append((None, text.rstrip(\" .:;,-\"), idx))\n",
    "    return headings\n",
    "\n",
    "def find_by_id(items, target_id):\n",
    "    for item in items:\n",
    "        if item.get(\"id\") == target_id:\n",
    "            return item['gpt_classification']\n",
    "    return None\n",
    "\n",
    "def get_text(el):\n",
    "    if el is None:\n",
    "        return \"\"\n",
    "    return \"\".join(el.itertext()).strip()\n",
    "\n",
    "def is_special_target(norm: str) -> bool:\n",
    "    return norm in SPECIAL_EQUIVALENTS\n",
    "\n",
    "def local_name(el: _Element) -> str: \n",
    "    return etree.QName(el).localname\n",
    "\n",
    "def build_ordered_blocks_from_tei(tei_path: str) -> Tuple[List[dict], str]:\n",
    "    \"\"\"\n",
    "    Parse TEI and build a list of blocks in document order.\n",
    "    Each block: {\"type\": \"head\"|\"p\", \"text\": str, \"element\": Element}\n",
    "    Returns (blocks, full_text_preview) where full_text_preview is a collapsed preview\n",
    "    (useful for fallback scanning).\n",
    "    \"\"\"\n",
    "    tree = etree.parse(tei_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    body = root.find(\".//tei:text/tei:body\", namespaces=TEI_NS)\n",
    "    source = body if body is not None else root\n",
    "\n",
    "    blocks: List[dict] = []\n",
    "\n",
    "    # Walk in document order, collect <head>, <p>, <ab> blocks.\n",
    "    # We explicitly include <head> and paragraph-like nodes. We ignore other tags.\n",
    "    for el in source.iter():\n",
    "        name = local_name(el)\n",
    "        if name == \"head\":\n",
    "            text = get_text(el)\n",
    "            if text:\n",
    "                blocks.append({\"type\": \"head\", \"text\": text, \"element\": el})\n",
    "        elif name in (\"p\", \"ab\"):\n",
    "            text = get_text(el)\n",
    "            if text:\n",
    "                blocks.append({\"type\": \"p\", \"text\": text, \"element\": el})\n",
    "\n",
    "    # For fallback matching we also provide a single continuous preview text\n",
    "    preview_pieces = []\n",
    "    for b in blocks:\n",
    "        if b[\"type\"] == \"head\":\n",
    "            preview_pieces.append(b[\"text\"])\n",
    "        else:\n",
    "            # keep paragraph boundaries with double newlines for preview\n",
    "            preview_pieces.append(b[\"text\"])\n",
    "    full_text_preview = \" \\n\\n \".join(preview_pieces)\n",
    "    return blocks, full_text_preview\n",
    "\n",
    "def matches_candidate(candidate_title: str, target_norm: str) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if candidate_title matches target_norm according to rules.\n",
    "    Defensive: if target_norm is empty, do NOT match anything.\n",
    "    \"\"\"\n",
    "    if not target_norm:\n",
    "        return False\n",
    "    if not candidate_title:\n",
    "        return False\n",
    "    cand_low = candidate_title.lower()\n",
    "    if is_special_target(target_norm):\n",
    "        return candidate_matches_special(cand_low)\n",
    "    # otherwise substring\n",
    "    return (target_norm in cand_low) or (cand_low in target_norm)\n",
    "\n",
    "def candidate_matches_special(candidate_title: str) -> bool:\n",
    "    cand = (candidate_title or \"\").lower()\n",
    "    for tok in (\"related work\", \"related works\", \"background\", \"past\", \"current\"):\n",
    "        if tok in cand:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "FIG_TABLE_RE = re.compile(r'\\b(?:figure|table)\\s*\\d+\\s*:', flags=re.IGNORECASE)\n",
    "\n",
    "def debug_blocks(blocks: List[dict], start: int, end: int):\n",
    "    print(f\"DEBUG: slicing blocks [{start} .. {end-1}] (inclusive)\")\n",
    "    for i in range(start, end):\n",
    "        b = blocks[i]\n",
    "        t = b['type']\n",
    "        txt = b['text'].strip().replace(\"\\n\", \"\\\\n\")\n",
    "        print(f\"  idx={i:3} type={t:4} text_preview={repr(txt[:120])}\")\n",
    "\n",
    "def extract_section_in_tei(\n",
    "    tei_path: str,\n",
    "    section: str,\n",
    "    *,\n",
    "    debug: bool = False\n",
    ") -> Dict[str, Optional[object]]:\n",
    "    \"\"\"\n",
    "    Robust extractor that matches on the <head> text (not @n), slices blocks between\n",
    "    the matched top-level <head> and the next top-level <head>, excludes Figure/Table\n",
    "    paragraphs from the returned content, and RETURNS a flag `has_figure` indicating\n",
    "    whether any \"Figure <digit>:\" was present in the original slice.\n",
    "\n",
    "    Returns a dict:\n",
    "      {\n",
    "        \"matched_heading\": str | None,\n",
    "        \"content\": str | None,\n",
    "        \"start_block_idx\": int | None,\n",
    "        \"end_block_idx\": int | None,\n",
    "        \"included_blocks\": List[int],\n",
    "        \"excluded_blocks\": List[int],\n",
    "        \"has_figure\": bool\n",
    "      }\n",
    "    \"\"\"\n",
    "    # defensive checks\n",
    "    if section is None:\n",
    "        return {\n",
    "            \"matched_heading\": None, \"content\": None,\n",
    "            \"start_block_idx\": None, \"end_block_idx\": None,\n",
    "            \"included_blocks\": [], \"excluded_blocks\": [],\n",
    "            \"has_figure\": False\n",
    "        }\n",
    "    target_norm = normalize_target(section)\n",
    "    if not target_norm:\n",
    "        return {\n",
    "            \"matched_heading\": None, \"content\": None,\n",
    "            \"start_block_idx\": None, \"end_block_idx\": None,\n",
    "            \"included_blocks\": [], \"excluded_blocks\": [],\n",
    "            \"has_figure\": False\n",
    "        }\n",
    "\n",
    "    # build blocks\n",
    "    blocks, _ = build_ordered_blocks_from_tei(tei_path)\n",
    "\n",
    "    # find headings by head-text (prefer head elements; fallback to paragraphs)\n",
    "    headings = find_top_level_heading_blocks_using_title(blocks)\n",
    "    if not headings:\n",
    "        headings = fallback_find_headings_in_paragraphs_using_title(blocks)\n",
    "\n",
    "    if debug:\n",
    "        print(\"DEBUG: discovered headings (num_or_None, title, block_idx):\")\n",
    "        for h in headings:\n",
    "            print(\"  \", h)\n",
    "\n",
    "    if not headings:\n",
    "        return {\n",
    "            \"matched_heading\": None, \"content\": None,\n",
    "            \"start_block_idx\": None, \"end_block_idx\": None,\n",
    "            \"included_blocks\": [], \"excluded_blocks\": [],\n",
    "            \"has_figure\": False\n",
    "        }\n",
    "\n",
    "    # find first heading that matches the target using matches_candidate (works for special group)\n",
    "    matched_idx = None\n",
    "    matched_title = None\n",
    "    for hi, (num, title, block_idx) in enumerate(headings):\n",
    "        if matches_candidate(title, target_norm):\n",
    "            matched_idx = hi\n",
    "            matched_title = title\n",
    "            break\n",
    "\n",
    "    # fallback: try matching by numeric prefix in provided section string\n",
    "    if matched_idx is None:\n",
    "        mnum = re.match(r'^\\s*(\\d+)\\s+', section)\n",
    "        if mnum:\n",
    "            wanted = int(mnum.group(1))\n",
    "            for hi, (num, title, block_idx) in enumerate(headings):\n",
    "                if num == wanted:\n",
    "                    matched_idx = hi\n",
    "                    matched_title = title\n",
    "                    break\n",
    "\n",
    "    if matched_idx is None:\n",
    "        if debug:\n",
    "            print(\"DEBUG: no matching heading found for target:\", repr(target_norm))\n",
    "        return {\n",
    "            \"matched_heading\": None, \"content\": None,\n",
    "            \"start_block_idx\": None, \"end_block_idx\": None,\n",
    "            \"included_blocks\": [], \"excluded_blocks\": [],\n",
    "            \"has_figure\": False\n",
    "        }\n",
    "\n",
    "    # compute start/end block indices (slice from matched heading's block to just before next heading's block)\n",
    "    start_block_idx = headings[matched_idx][2]\n",
    "    if matched_idx + 1 < len(headings):\n",
    "        end_block_idx = headings[matched_idx + 1][2]\n",
    "    else:\n",
    "        end_block_idx = len(blocks)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"DEBUG: matched heading: {repr(matched_title)} at headings index {matched_idx}, start_block_idx={start_block_idx}, end_block_idx={end_block_idx}\")\n",
    "        debug_blocks(blocks, start_block_idx, end_block_idx)\n",
    "\n",
    "    # collect text pieces while excluding figure/table paragraphs\n",
    "    included_blocks: List[int] = []\n",
    "    excluded_blocks: List[int] = []\n",
    "    pieces: List[str] = []\n",
    "\n",
    "    # specific regex to detect Figure labels only\n",
    "    FIG_RE = re.compile(r'\\bfigure\\s*\\d+\\s*:', flags=re.IGNORECASE)\n",
    "\n",
    "    # flag if any figure occurs in the slice (whether or not excluded)\n",
    "    has_figure = False\n",
    "\n",
    "    for bi in range(start_block_idx, end_block_idx):\n",
    "        b = blocks[bi]\n",
    "        text = b[\"text\"] or \"\"\n",
    "        # check for any Figure label in the block\n",
    "        if FIG_RE.search(text):\n",
    "            has_figure = True\n",
    "\n",
    "        if b[\"type\"] == \"p\":\n",
    "            if FIG_TABLE_RE.search(text):\n",
    "                excluded_blocks.append(bi)\n",
    "                if debug:\n",
    "                    print(f\"DEBUG: excluding block idx={bi} (figure/table paragraph): {b['text'][:60]!r}\")\n",
    "                continue\n",
    "            included_blocks.append(bi)\n",
    "            pieces.append(text.strip())\n",
    "        elif b[\"type\"] == \"head\":\n",
    "            included_blocks.append(bi)\n",
    "            pieces.append(text.strip())\n",
    "        else:\n",
    "            # include other block types conservatively\n",
    "            included_blocks.append(bi)\n",
    "            pieces.append(text.strip())\n",
    "\n",
    "    content = \"\\n\\n\".join(pieces).strip() or None\n",
    "\n",
    "    if debug:\n",
    "        print(\"DEBUG: included_blocks:\", included_blocks)\n",
    "        print(\"DEBUG: excluded_blocks:\", excluded_blocks)\n",
    "        print(\"DEBUG: content length:\", 0 if content is None else len(content))\n",
    "        print(\"DEBUG: has_figure:\", has_figure)\n",
    "        if content:\n",
    "            print(\"DEBUG: content preview:\", repr(content[:400]))\n",
    "\n",
    "    return {\n",
    "        \"matched_heading\": matched_title,\n",
    "        \"content\": content,\n",
    "        \"start_block_idx\": start_block_idx,\n",
    "        \"end_block_idx\": end_block_idx,\n",
    "        \"included_blocks\": included_blocks,\n",
    "        \"excluded_blocks\": excluded_blocks,\n",
    "        \"has_figure\": has_figure,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54bcca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from typing import List, Dict, Optional, Union\n",
    "import re\n",
    "\n",
    "TEI_NS = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
    "\n",
    "# -------------------------\n",
    "# Extraction of biblStructs in tei files\n",
    "# \n",
    "# -------------------------\n",
    "def _get_text(el: Optional[etree._Element]) -> str:\n",
    "    return \"\" if el is None else \"\".join(el.itertext()).strip()\n",
    "\n",
    "def extract_bibl_structs(tei_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse TEI file at tei_path and return a filtered list of dicts, one per <biblStruct>.\n",
    "    Filtering heuristic:\n",
    "      - include entries that have a monogr title (journal/book-like entries), OR\n",
    "      - include analytic-title entries only when they also have some useful metadata\n",
    "        (year, pages, or a useful idno like DOI/arXiv).\n",
    "      - exclude entries that are essentially empty (e.g., only MD5 idno, empty imprint/date).\n",
    "    \"\"\"\n",
    "    tree = etree.parse(tei_path)\n",
    "    root = tree.getroot()\n",
    "    bibl_nodes = root.xpath(\".//tei:biblStruct\", namespaces=TEI_NS)\n",
    "\n",
    "    out = []\n",
    "    for b in bibl_nodes:\n",
    "        # quick introspection: detect presence of analytic/monogr titles\n",
    "        analytic_title_nodes = b.xpath(\".//tei:analytic/tei:title\", namespaces=TEI_NS)\n",
    "        monogr_title_nodes = b.xpath(\".//tei:monogr/tei:title\", namespaces=TEI_NS)\n",
    "\n",
    "        # preliminary metadata checks: date, pages, idnos\n",
    "        # Year detection\n",
    "        year = \"\"\n",
    "        date_nodes = b.xpath(\".//tei:imprint/tei:date\", namespaces=TEI_NS)\n",
    "        if date_nodes:\n",
    "            d = date_nodes[0]\n",
    "            when = d.get(\"when\")\n",
    "            if when and re.search(r\"\\d{4}\", when):\n",
    "                year = re.search(r\"(\\d{4})\", when).group(1)\n",
    "            else:\n",
    "                dt = _get_text(d)\n",
    "                m = re.search(r\"(\\d{4})\", dt)\n",
    "                if m:\n",
    "                    year = m.group(1)\n",
    "\n",
    "        # Pages detection\n",
    "        page_nodes = b.xpath(\".//tei:imprint/tei:biblScope[@unit='page']\", namespaces=TEI_NS)\n",
    "        pages = \"\"\n",
    "        if page_nodes:\n",
    "            el = page_nodes[0]\n",
    "            frm = el.get(\"from\")\n",
    "            to = el.get(\"to\")\n",
    "            if frm and to:\n",
    "                pages = f\"{frm}--{to}\"\n",
    "            else:\n",
    "                pages = _get_text(el)\n",
    "\n",
    "        # idnos: gather and check for useful identifiers\n",
    "        idno_nodes = b.xpath(\".//tei:idno\", namespaces=TEI_NS)\n",
    "        idnos = {}\n",
    "        for idn in idno_nodes:\n",
    "            typ = (idn.get(\"type\") or \"\").strip()\n",
    "            val = _get_text(idn)\n",
    "            if typ:\n",
    "                idnos[typ] = val\n",
    "            else:\n",
    "                if re.search(r\"10\\.\\d{4,9}/\", val):\n",
    "                    idnos[\"DOI\"] = val\n",
    "                elif re.search(r\"arxiv\", val, flags=re.I) or re.search(r\"\\d{4}\\.\\d{4,}\", val):\n",
    "                    idnos[\"arXiv\"] = val\n",
    "                else:\n",
    "                    idnos.setdefault(\"other\", []).append(val)\n",
    "\n",
    "        has_useful_id = bool(idnos.get(\"DOI\") or idnos.get(\"arXiv\"))\n",
    "        has_year_or_pages = bool(year or pages)\n",
    "\n",
    "        # Decide whether to include this biblStruct\n",
    "        include = False\n",
    "        if monogr_title_nodes:\n",
    "            # monogr entries are usually meaningful (journal/book)\n",
    "            include = True\n",
    "        else:\n",
    "            # if only analytic title exists, require at least one useful metadata field\n",
    "            if analytic_title_nodes and (has_year_or_pages or has_useful_id):\n",
    "                include = True\n",
    "\n",
    "        if not include:\n",
    "            # skip this biblStruct as it appears to be non-informative (e.g., only MD5 id)\n",
    "            continue\n",
    "\n",
    "        # Build the item dict (same structure as before)\n",
    "        item = {\n",
    "            \"xml_id\": b.get(\"{http://www.w3.org/XML/1998/namespace}id\") or b.get(\"xml:id\") or b.get(\"id\"),\n",
    "            \"raw_xml\": etree.tostring(b, encoding=\"unicode\", with_tail=False),\n",
    "            \"title\": \"\",\n",
    "            \"authors\": [],  # list of {\"surname\":..., \"forenames\":[...], \"raw\":...}\n",
    "            \"venue\": \"\",\n",
    "            \"publisher\": \"\",\n",
    "            \"year\": year,\n",
    "            \"pages\": pages,\n",
    "            \"idnos\": idnos\n",
    "        }\n",
    "\n",
    "        # Title: prefer analytic level='a' then analytic then monogr title then generic\n",
    "        title_nodes = b.xpath(\".//tei:analytic/tei:title[@level='a']|.//tei:analytic/tei:title|.//tei:monogr/tei:title|.//tei:title\", namespaces=TEI_NS)\n",
    "        if title_nodes:\n",
    "            item[\"title\"] = _get_text(title_nodes[0])\n",
    "\n",
    "        # Authors: try extracting persName children\n",
    "        authors = []\n",
    "        for au in b.xpath(\".//tei:author\", namespaces=TEI_NS):\n",
    "            pers = au.find(\".//{http://www.tei-c.org/ns/1.0}persName\")\n",
    "            if pers is not None:\n",
    "                surname_el = pers.find(\".//{http://www.tei-c.org/ns/1.0}surname\")\n",
    "                forename_els = pers.findall(\".//{http://www.tei-c.org/ns/1.0}forename\")\n",
    "                surname = _get_text(surname_el) if surname_el is not None else \"\"\n",
    "                forenames = [_get_text(fn) for fn in forename_els if _get_text(fn)]\n",
    "                raw = _get_text(pers)\n",
    "                authors.append({\"surname\": surname, \"forenames\": forenames, \"raw\": raw})\n",
    "            else:\n",
    "                # fallback: use author text\n",
    "                at = _get_text(au)\n",
    "                if at:\n",
    "                    parts = at.split()\n",
    "                    if len(parts) >= 2:\n",
    "                        surname = parts[-1]\n",
    "                        forenames = parts[:-1]\n",
    "                    else:\n",
    "                        surname = at\n",
    "                        forenames = []\n",
    "                    authors.append({\"surname\": surname, \"forenames\": forenames, \"raw\": at})\n",
    "        item[\"authors\"] = authors\n",
    "\n",
    "        # Venue / monogr title (prefer monogr/journal)\n",
    "        journal = b.xpath(\".//tei:monogr/tei:title[@level='j']\", namespaces=TEI_NS)\n",
    "        monogr = b.xpath(\".//tei:monogr/tei:title\", namespaces=TEI_NS)\n",
    "        if journal:\n",
    "            item[\"venue\"] = _get_text(journal[0])\n",
    "        elif monogr:\n",
    "            item[\"venue\"] = _get_text(monogr[0])\n",
    "        else:\n",
    "            gen = b.xpath(\".//tei:title\", namespaces=TEI_NS)\n",
    "            item[\"venue\"] = _get_text(gen[0]) if gen else \"\"\n",
    "\n",
    "        # Publisher (if present)\n",
    "        pub = b.xpath(\".//tei:imprint/tei:publisher\", namespaces=TEI_NS)\n",
    "        if pub:\n",
    "            item[\"publisher\"] = _get_text(pub[0])\n",
    "\n",
    "        # Keep idnos (already collected)\n",
    "        item[\"idnos\"] = idnos\n",
    "\n",
    "        out.append(item)\n",
    "\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# ACL formatting utilities\n",
    "# -------------------------\n",
    "def _format_author_acl(author: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Turn {\"surname\": \"...\", \"forenames\": [...]} into \"Surname, F.\" (use first forename initial).\n",
    "    If multiple forenames, use initials for each (e.g., \"John Paul\" -> \"J.P.\").\n",
    "    If surname missing, use raw.\n",
    "    \"\"\"\n",
    "    surname = author.get(\"surname\") or \"\"\n",
    "    forenames = author.get(\"forenames\") or []\n",
    "    raw = author.get(\"raw\") or \"\"\n",
    "    if not surname and not forenames:\n",
    "        return raw or \"\"\n",
    "    # build initials from forenames (use first forename only or all? we use all for initials)\n",
    "    initials = \"\"\n",
    "    if forenames:\n",
    "        initials = \" \".join(\n",
    "            \"\".join([part[0] + \".\" for part in fn.split()]) for fn in forenames\n",
    "        )\n",
    "        # collapse spaces to single space\n",
    "        initials = re.sub(r\"\\s+\", \" \", initials).strip()\n",
    "    if surname and initials:\n",
    "        return f\"{surname}, {initials}\"\n",
    "    if surname:\n",
    "        return surname\n",
    "    return raw\n",
    "\n",
    "def _join_authors_acl(authors: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Join formatted authors in ACL style:\n",
    "      - 1 author: A.\n",
    "      - 2 authors: A. and B.\n",
    "      - >2: A., B., and C.\n",
    "    \"\"\"\n",
    "    formatted = [_format_author_acl(a) for a in authors if _format_author_acl(a)]\n",
    "    if not formatted:\n",
    "        return \"\"\n",
    "    if len(formatted) == 1:\n",
    "        return formatted[0]\n",
    "    if len(formatted) == 2:\n",
    "        return f\"{formatted[0]} and {formatted[1]}\"\n",
    "    # 3+ authors\n",
    "    return \", \".join(formatted[:-1]) + \", and \" + formatted[-1]\n",
    "\n",
    "def format_reference_acl(ref: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Format a single reference dict (from extract_bibl_structs) into ACL-like citation string.\n",
    "    \"\"\"\n",
    "    authors = _join_authors_acl(ref.get(\"authors\", []))\n",
    "    year = ref.get(\"year\") or \"\"\n",
    "    title = ref.get(\"title\") or \"\"\n",
    "    venue = ref.get(\"venue\") or \"\"\n",
    "    pages = ref.get(\"pages\") or \"\"\n",
    "    doi = None\n",
    "    idnos = ref.get(\"idnos\", {})\n",
    "    # prefer DOI, then arXiv, then any other id\n",
    "    if idnos:\n",
    "        doi = idnos.get(\"DOI\") or idnos.get(\"doi\") or idnos.get(\"arXiv\")\n",
    "        if not doi:\n",
    "            # if 'other' is list, pick first\n",
    "            oth = idnos.get(\"other\")\n",
    "            if isinstance(oth, list) and oth:\n",
    "                doi = oth[0]\n",
    "\n",
    "    parts = []\n",
    "    if authors:\n",
    "        parts.append(authors)\n",
    "    if year:\n",
    "        parts.append(year + \".\")\n",
    "    if title:\n",
    "        parts.append(title.rstrip(\".\") + \".\")\n",
    "    if venue:\n",
    "        parts.append(\"In \" + venue.rstrip(\".\") + \".\")\n",
    "    if pages:\n",
    "        parts.append(\"pp. \" + pages)\n",
    "    if doi:\n",
    "        parts.append(doi)\n",
    "    # Join with space\n",
    "    return \" \".join(p for p in parts if p).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be265fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/234 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:04<00:00, 51.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process files: ['00aff1ff48640448a87f47a0985a0ac40c6aa18fbbd3909bdfd521eb22d67f209005b6ca9bf68bcde0d387509c0026cbed0dcd9acdd779959b4b943f677dcc4c', '0f4cb8033e92de1eecebb650474462752b609f5a11cdf226e790163ec2a020aeb4d08d7d750bc75fd8d7c0be415881528a542492eeff9c184f1fd6b090129258', '1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc', '25b37714233d1ceb645b7dbc062d3c1cae7cdd0a23dfc01d1ad6cf964be2701f56cd692d11316907ba1b92e91058a4834314321dcc6a09aa069082c8b775400f', '30e4f79e74393e179f891c8a9cec30925ff3a4aec46c79a8988d5a96e69b2aa0ecda673f3dd4d25c67a8cf306debe95f729ec54145d9a6bf25e07c12a3f86ac9', '3682a41985bc4769c5dc8fb3ca6049f7cdac04eca025ade78ad24ad1d7cb3408cc49c533fd653f236881888aedb59880277da3e30dedb69510df272588e8d612', '3bf4a49a78cbf2ee21666876aa50d4b69ebedfbd82098262e29b1d09e1c2bbad83a30d50d122733c8216a39edb9e5762f090416ee3e1d1a9fe91695ad9ee5b4b', '6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a', '6b9b7fdeb6cda7b2d2b7e202a5f69646789236f64174feed4fce7f70197891f23945c9a46efa20f56d547686f52138d93ee1887a07f2e96869d06c9c04930014', '6dd0a683862d77360df72b9125169aad1f5b6e0dab94bec6f86335b37b8c855faa4c00c6c1fa1cb60cb40153d2c6bd05bbeeeeb069a56199fc793dccee244147', 'a7412d714dac0e0ee9845a5bc3b9357020058a24bc7fec822c3be309a16d0f554be8623684385f5bb8dad11a224cde14ee73b363df60900c40a7b83f2bc32d96', 'a7b2e5cce4c1e1a8c7a4959fed755e9e0d3a73fb9dad9d5a76258f80583cb2600e57fb75a86f7b5084d87524213bc8aa51ced77e72b0a0d645a4abd320b022d9', 'c7c4295209272e28b7f122a3d20ca9a82076e7dd2e30b5249a078e4e9eb909dbbaed0414877a5a98266f51c6632b0c0206c5bc9b1c69d31f3a7bf613b02d893b', 'e825a3c26bb00f83fc361f1ba4a9855a49ca997af474f34abdfe94c969eccbe694bff9837526c04191321ca6c266d33260d33287967aff28805da237a4dbb70f', 'eb931c7319dc1b08fb3c97fb77b60b7eac9a41d699b58e0139d655f4d1cbc5c61db8d9e9dd42e3515a6cd839518933bd7d033289be73a8111b2170e8b2bf983f', 'f226ce588781360cfa7f63ac26c85ecbf5143a3325d3471a4312caac6363446f7db8b9377e5e20b74cbaff42857357dae11c5f3675371493f7bd10dc9f382623', 'f4e8081d3275bbcb7628e2d971f4bc82e26fa4034d8f7d6f472da6b569b8ea4e413cfc27067218a17ff82b77c78e43e39f6930a87cf65dd8401f0b98764a2460']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "import os \n",
    "total_text = []\n",
    "pdf_folder = '../data/annotation/pdfs'\n",
    "failed_to_process = []\n",
    "\n",
    "for file in tqdm(os.listdir(pdf_folder)):\n",
    "    if '.tei' in file:\n",
    "        # pdf_path = os.path.join(pdf_folder, file)\n",
    "        file_id = file[4:132]\n",
    "        tei_path = os.path.join(pdf_folder, f\"pdf_{file_id}.tei\")\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    section = find_by_id(rw_intro, file_id) \n",
    "    tei_section_text = extract_section_in_tei(tei_path, section, debug=False)\n",
    "\n",
    "    refs = extract_bibl_structs(tei_path)\n",
    "    references = [format_reference_acl(r) for r in refs]\n",
    "    references = \"\\n\".join(references)\n",
    "\n",
    "    if tei_section_text[\"matched_heading\"] == None or len(references) == 0:\n",
    "        failed_to_process.append(file_id)\n",
    "        continue\n",
    "\n",
    "    total_text.append({\n",
    "        'id': file,\n",
    "        'text': tei_section_text['content'],\n",
    "        \"has_figure\": tei_section_text['has_figure'],\n",
    "        'references': references\n",
    "    })\n",
    "\n",
    "print(f\"Failed to process files: {failed_to_process}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f7e1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"../data/complete_dataset.json\", 'w') as f:\n",
    "    json.dump(total_text, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "304be68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in dict_list: 100\n",
      "Saving files into folder: /mnt/c/Users/imana/Desktop/Masters/Masters_Thesis/codebase/data/to_annotate\n",
      "These files have figures in them: [3, 35, 50, 55, 56, 73, 74, 81, 86, 89]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "files_ref = []\n",
    "\n",
    "def save_to_txt(dict_list, folder_path, base_filename=\"item\"):\n",
    "    print(\"Number of items in dict_list:\", len(dict_list))\n",
    "\n",
    "    folder = Path(folder_path)\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Saving files into folder:\", folder.resolve())\n",
    "\n",
    "    for i, item in enumerate(dict_list, start=1):\n",
    "        # Only use the 'text' field\n",
    "        text = item.get(\"text\", \"\")\n",
    "        ref = item.get(\"references\", \"\")\n",
    "\n",
    "        # Use 'id' in the filename if present, else fallback to the index\n",
    "        file_id = item.get(\"id\", i)\n",
    "        file_path = folder / f\"{base_filename}_{i}.txt\"  # or use file_id if you prefer\n",
    "\n",
    "        if item['has_figure'] == True:\n",
    "            files_ref.append(i)\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(text) + \"\\n\\n References: \\n\" + str(ref))\n",
    "\n",
    "save_to_txt(total_text, \"../data/to_annotate\", \"paper\")\n",
    "print(f\"These files have figures in them: {files_ref}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f5132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
