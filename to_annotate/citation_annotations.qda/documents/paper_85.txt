Related Work

Defending and generating adversarial examples (Jia et al., 2019; have been mostly explored in NLP since the reign of pre-trained language models (LMs) (Devlin et al., 2019). Li et al. (2020); Garg and Ramakrishnan (2020); Morris et al. (2020) show that substituting words in a sentence with masked LMs (Devlin et al., 2019;Liu et al., 2019) can successfully mislead the classification and entailment model predictions to be incorrect. Template-based (McCoy et al., 2019;Glockner et al., 2018) and manually crafted (Gardner et al., 2020) perturbations on evaluation datasets have also been studied for textual entailment.

Language-based adversarial examples can be collected to study the robustness of vision-language models as well. Shekhar et al. (2017) introduces FOIL-COCO dataset to evaluate the visionlanguage model's decision when associating images with both correct and "foil" captions. Hendricks and Nematzadeh (2021) show that vision-language Transformers are worse at verb understanding than nouns. New versions of the VQA dataset (Antol et al., 2015) are proposed to study robustness of VQA models (Shah et al., 2019;Li et al., 2021). Our work is different in that we use pre-trained LMs to introduce perturbations and evaluate robustness of video-language models.

 References: 
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. 2015. Vqa: Visual question answering. In ICCV.
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In ArXiv. abs/1810.04805
Dzabraev, M. and Kalashnikov, M. 2021. Mdmmt: Multidomain multimodal transformer for video retrieval. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). pp. 3349--3358
Fang, H., Xiong, P., Xu, L., and Chen, Y. 2021. Clip2video: Mastering video-text retrieval via image clip. In Clip2video: Mastering video-text retrieval via image clip. arXiv:2106.11097
Gabeur, V., Sun, C., Alahari, K., and Schmid, C. 2020. Multi-modal transformer for video retrieval. In Computer Vision-ECCV 2020: 16th European Conference. pp. 214--229
Gardner, M., Artzi, Y., Basmova, V., Berant, J., Bogin, B., Chen, S., Dasigi, P., Dua, D., Elazar, Y., and Gottumukkala, A. 2020. Evaluating models' local decision boundaries via contrast sets. In Findings of the Association for Computational Linguistics: EMNLP 2020.
Garg, S. and Ramakrishnan, G. 1970. Bae: Bert-based adversarial examples for text classification. ArXiv, abs. In Bae: Bert-based adversarial examples for text classification. ArXiv, abs.
Glockner, M., Shwartz, V., and Goldberg, Y. 2018. Breaking nli systems with sentences that require simple lexical inferences. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 650--655
Lisa, A., Hendricks, A., and Nematzadeh 2021. Probing image-language transformers for verb understanding. In Probing image-language transformers for verb understanding. arXiv:2106.09141
Hershey, S., Chaudhuri, S., Daniel, P. W., Ellis, J. F., Gemmeke, A., Jansen, R., Moore, M., Plakal, D., Platt, R. A., Saurous, B., Seybold, M., Slaney, R. J., Weiss, K. W., and Wilson 2017. Cnn architectures for largescale audio classification. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 131--135
Honnibal, M. and Montani, I. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. In spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.
Jia, R., Raghunathan, A., Göksel, K., and Liang, P. 2019. Certified robustness to adversarial word substitutions. In EMNLP/IJCNLP (1).
Jin, D., Jin, Z., Zhou, J. T., and Szolovits, P. 2020. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In Is bert really robust? a strong baseline for natural language attack on text classification and entailment.
Diederik, P., Kingma, J., and Ba 2015. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization. abs/1412.6980
Lei, J., Li, L., Zhou, L., Gan, Z., Berg, T. L., Bansal, M., and Liu, J. 2021. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7331--7341
Li, L., Lei, J., Gan, Z., and Liu, J. 2021. Adversarial vqa: A new benchmark for evaluating the robustness of vqa models. In ArXiv. abs/2106.00245
Li, L., Ma, R., Guo, Q., Xue, X., and Qiu, X. 2020. Bert-attack: Adversarial attack against bert using bert. In The 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs. In Roberta: A robustly optimized bert pretraining approach. ArXiv, abs.
Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., and Li, T. 2021. Clip4clip: An empirical study of clip for end to end video clip retrieval. In Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv:2104.08860
Mccoy, T., Pavlick, E., and Linzen, T. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3428--3448
Miech, A., Alayrac, J., Smaira, L., Laptev, I., Sivic, J., and Zisserman, A. 2020. End-to-end learning of visual representations from uncurated instructional videos. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9876--9886
Miech, A., Laptev, I., and Sivic, J. 2018. Learning a text-video embedding from incomplete and heterogeneous data. In Learning a text-video embedding from incomplete and heterogeneous data. arXiv:1804.02516
Miech, A., Zhukov, D., Alayrac, J., Tapaswi, M., Laptev, I., and Sivic, J. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In IEEE/CVF International Conference on Computer Vision (ICCV). pp. 2630--2640
Morris, J. X., Lifland, E., Yoo, J. Y., Grigsby, J., Jin, D., and Qi, Y. 2020. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. In EMNLP.
Mrkšić, N., Diarmuid, Ó., Séaghdha, B., Thomson, M., Gašić, L., Rojas-Barahona, P., Su, D., Vandyke, T., and Wen 2016. Counter-fitting word vectors to linguistic constraints. In Proceedings of HLT-NAACL.
Park, J. S., Darrell, T., and Rohrbach, A. 2020. Identity-aware multi-sentence video description. In European Conference on Computer Vision. pp. 360--378
Portillo-Quintero, J.A., Ortiz-Bayliss, J. C., and Terashima-Marín, H. 2021. A straightforward framework for video retrieval using clip. In A straightforward framework for video retrieval using clip.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., and Agarwal, S. Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In ICML.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In Language models are unsupervised multitask learners.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In Journal of Machine Learning Research.
Reimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.
Rohrbach, A., Torabi, A., Rohrbach, M., Tandon, N., Pal, C., Larochelle, H., Courville, A., and Schiele, B. 2017. In International Journal of Computer Vision.
Schuler, K.K. 2005. VerbNet: A broadcoverage, comprehensive verb lexicon. In VerbNet: A broadcoverage, comprehensive verb lexicon.
Shah, M., Chen, X., Rohrbach, M., and Parikh, D. 2019. Cycle-consistency for robust visual question answering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 6642--6651
Shekhar, R., Pezzelle, S., Klimovich, Y., Herbelot, A., Nabi, M., Sangineto, E., and Bernardi, R. 2017. Foil it! find one mismatch between image and language caption. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 255--265
Torabi, A., Tandon, N., and Sigal, L. 2016. Learning language-visual embedding for movie understanding with natural-language. In Learning language-visual embedding for movie understanding with natural-language. arXiv:1609.08124
Vaswani, A., Shazeer, N. M., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017.
Xie, S., Sun, C., Huang, J., Tu, Z., and Murphy, K. 2018. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification.
Xu, J., Mei, T., Yao, T., and Rui, Y. 2016. Msrvtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5288--5296
Yu, Y., Kim, J., and Kim, G. 2018. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV). pp. 471--487