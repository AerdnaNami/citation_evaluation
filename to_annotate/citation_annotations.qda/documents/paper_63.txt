Introduction

BERT (Devlin et al., 2018) is a Transformer-based pretrained model, whose prosperity starts from English language and gradually spreads to many other languages. The original BERT model is trained with character-level masking (CLM). 2 A certain percentage (e.g. 15%) of tokens in the input sequence is masked and the model is learned to predict the masked tokens.

It is helpful to note that a word in the input sequence of BERT can be broken into multiple wordpiece tokens (Wu et al., 2016). 3 For example, the input sentence "She is undeniably brilliant" is converted to a wordpiece sequence "She is un ##deni ##ably brilliant", where "##" is a special prefix added to indicate that the token should be attached to the previous one. In this case the word "undeniably" is broken into three wordpieces {"un", "##deni", "##ably"}. In standard masked language modeling, CLM may mask any one of them. In this case, if the token "##ably" is masked, it is easier for the model to complete the prediction task because "un" and "##deni" are informative prompts. To address this, Whole word masking (WWM) masks all three subtokens (i.e., {"un", "##deni", "##ably"}) within a word at once. For Chinese, however, each token is an atomic character that cannot be broken into smaller pieces. Many Chinese words are compounds that consisting of multiple characters (Wood and Connelly, 2009). 4 For example, "手机" (cellphone) is a word consisting of two characters "手" (hand) and "机" (machine). Here, learning with WWM would lose the association among characters corresponding to a word.

In this work, we introduce two probing tasks to study Chinese BERT model's ability on characterlevel understanding. The first probing task is character replacement. Given a sentence and a position where the corresponding character is erroneous, the task is to replace the erroneous character with the correct one. The second probing task is character insertion. Given a sentence and the positions where a given number of characters should be inserted, the task is to insert the correct characters. We leverage the benchmark dataset on grammatical error correction (Rao et al., 2020a) and create a dataset including labels for 19,075 tokens in 10,448 sentences.

We train three baseline models based on the same text corpus of 80B characters using CLM, WWM, and both CLM and WWM, separately. We have the following major findings. (1) When one character needs to be inserted or replaced, the model trained with CLM performs the best. Moreover, the model initialized from RoBERTa (Cui et al., 2019) and trained with WWM gets worse gradually with more training steps. (2) When more than one character needs to be handled, WWM is the key to better performance. (3) When evaluating sentence-level downstream tasks, the impact of these masking strategies is minimal and the model trained with them performs comparably.

 References: 
Bryant, C., Felice, M., and Briscoe, E. 2017. Automatic annotation and evaluation of error types for grammatical error correction. Association for Computational Linguistics. In Automatic annotation and evaluation of error types for grammatical error correction. Association for Computational Linguistics.
Cui, Y., Che, W., Liu, T., Qin, B., Wang, S., and Hu, G. 2020. Revisiting pretrained models for Chinese natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. pp. 657--668
Cui, Y., Che, W., Liu, T., Qin, B., Yang, Z., Wang, S., and Hu, G. 2019. Pre-training with whole word masking for chinese bert. In Pre-training with whole word masking for chinese bert. arXiv:1906.08101
Davison, J., Feldman, J., and Rush, A. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 1173--1178
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805
Diederik, P., Kingma, J., and Ba 2014. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization. arXiv:1412.6980
Lai, Y., Liu, Y., Feng, Y., Huang, S., and Zhao, D. 2021. Lattice-bert: Leveraging multi-granularity representations in chinese pre-trained language models. In Lattice-bert: Leveraging multi-granularity representations in chinese pre-trained language models. arXiv:2104.07204
Lung-Hao Lee, G., Rao, L., Yu, E., Xun, B., Zhang, L., and Chang 2016. Overview of NLP-TEA 2016 shared task for Chinese grammatical error diagnosis. In Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA2016). pp. 40--48
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692
Petroni, F., Rocktäschel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066
Rao, G., Gong, Q., Zhang, B., and Xun, E. 2018. Overview of NLPTEA-2018 share task Chinese grammatical error diagnosis. In Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications. pp. 42--51 10.18653/v1/W18-3706
Rao, G., Yang, E., and Zhang, B. 2020. Overview of nlptea-2020 shared task for chinese grammatical error diagnosis. In Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications. pp. 25--35
Rao, G., Yang, E., and Zhang, B. 2020. Overview of NLPTEA-2020 shared task for Chinese grammatical error diagnosis. In Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications. pp. 25--35
Rao, G., Zhang, B., Xun, E., and Lee, L. 2017. IJCNLP-2017 task 1: Chinese grammatical error diagnosis. In Asian Federation of Natural Language Processing. pp. 1--8
Sennrich, R., Haddow, B., and Birch, A. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pp. 1715--1725 10.18653/v1/P16-1162
Sun, Y., Wang, S., Li, Y., Feng, S., Chen, X., Zhang, H., Tian, X., Zhu, D., Hao Tian, H., and Wu 2019. Ernie: Enhanced representation through knowledge integration. In Ernie: Enhanced representation through knowledge integration. arXiv:1904.09223
Sun, Z., Li, X., Sun, X., Meng, Y., Ao, X., He, Q., Wu, F., and Li, J. 2021. Chinesebert: Chinese pretraining enhanced by glyph and pinyin information. In Chinesebert: Chinese pretraining enhanced by glyph and pinyin information. arXiv:2106.16038
Talmor, A., Elazar, Y., Goldberg, Y., and Berant, J. 2020. olmpics-on what language model pre-training captures. In Transactions of the Association for Computational Linguistics. pp. 743--758
Wang, Z. and Hu, R. 2020. Intrinsic knowledge evaluation on chinese language models. In Intrinsic knowledge evaluation on chinese language models. arXiv:2011.14277
Wood, C. and Connelly, V. 2009. Contemporary perspectives on reading and spelling. In Contemporary perspectives on reading and spelling.
Wu, Y., Schuster, M., Chen, Z., Quoc, V., Le, M., Norouzi, W., Macherey, M., Krikun, Y., Cao, Q., Gao, K., and Macherey 2016. Google's neural machine translation system. In Bridging the gap between human and machine translation. arXiv:1609.08144
Xu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. 2020. CLUE: A Chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4762--4772 10.18653/v1/2020.coling-main.419
Xu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., and Yu, C. 2020. Clue: A chinese language understanding evaluation benchmark. In Clue: A chinese language understanding evaluation benchmark. arXiv:2004.05986
Yu, L., Lee, L., and Chang, L. 2014. Overview of grammatical error diagnosis for learning chinese as a foreign language. In Proceedings of the 1stWorkshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA'14). pp. 42--47
Zhang, H., Liu, L., Jiang, H., Li, Y., Zhao, E., Xu, K., Song, L., Zheng, S., Zhou, B., Zhu, J., Feng, X., Chen, T., Yang, T., Yu, D., Zhang, F., Kang, Z., and Shi, S. 2020. Texsmart: A text understanding system for fine-grained ner and enhanced semantic analysis. In Texsmart: A text understanding system for fine-grained ner and enhanced semantic analysis. arXiv:2012.15639
Zhang, X. and Li, H. 2020. Ambert: A pretrained language model with multi-grained tokenization. In Ambert: A pretrained language model with multi-grained tokenization. arXiv:2008.11869