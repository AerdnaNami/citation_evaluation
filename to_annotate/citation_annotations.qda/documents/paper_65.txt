Related Work

Open-Domain Passage Retrieval Open-Domain Passage Retrieval has been a hot research topic in recent years. It requires a system to extract evidence passages for a specific question from a large passage corpus like Wikipedia, and is challenging as it requires both high retrieval accuracy and specifically low latency for practical usage. Traditional approaches like TF-IDF (Ramos et al., 2003), BM25 (Robertson and Zaragoza, 2009) retrieve the evidence passages based on the lexical match between questions and passages. Although these lexical approaches meet the requirement of low latency, they fail to capture non-lexical semantic similarity, thus performing unsatisfying on retrieval accuracy.

With recent advances of pretrained language models (PrLMs) like BERT , RoBERTa (Liu et al., 2019), a series of neural approaches based on cross-encoders are proposed (Vig and Ramea, 2019;Wolf et al., 2019). Although enjoying satisfying retrieval accuracy, the retrieval latency is often hard to tolerate in practical use. More recently, the Bi-Encoder structure has captured the researchers' attention. With Bi-Encoder, the representations of the corpus at scale can be precomputed, enabling it to meet the requirement of low latency in passage retrieval.  first proposes to pretrain the Bi-Encoder with Inverse Cloze Task (ICT). Later, DPR (Karpukhin et al., 2020) introduces a contrastive learning framework to train dense passage representation, and has achieved impressive performance on both retrieval accuracy and latency. Based on DPR, many works make further improvements either by introducing better sampling strategy (Xiong et al., 2020;Lu et al., 2020;Tang et al., 2021;Qu et al., 2021) or extra pretraining (Sachan et al., 2021), or even distilling knowledge from cross-encoders (Izacard and Grave, 2021;Yang et al., 2021).

Our method follows the contrastive learning research line of ODPR. Different from previous works that focus on either improving the quality of negative sampling or using extra pretraining, we make improvements by directly optimizing the modeling granularity with an elaborately designed contrastive learning training strategy.

Contrastive Learning Contrastive learning recently is attracting researchers' attention in all area. After witnessing its superiority in Computer Vision tasks He et al., 2020), researchers in NLP are also applying this technique Karpukhin et al., 2020;Yan et al., 2021;Giorgi et al., 2021;Gao et al., 2021). For the concern of ODPR, the research lines of contrastive learning can be divided into two types: (i) Improving the sampling strategies for positive samples and hard negative samples. According to (Manmatha et al., 2017), the quality of positive samples and negative samples are of vital importance in the contrastive learning framework. Therefore, many researchers seek better sampling strategies to improve the retrieval performance (Xiong et al., 2020). (ii) Improving the contrastive learning framework. DensePhrase (Lee et al., 2021) uses memory bank like MOCO (He et al., 2020) to increase the number of in-batch negative samples without increasing the GPU memory usage, and models retrieval process on the phrase level but not passage level, achieving impressive performance.

Our proposed method follows the second research line. We investigate a special phenomenon, Contrastive Conflicts in the contrastive learning framework, and experimentally verify the effectiveness of mediating such conflicts by modeling ODPR in a smaller granularity. More similar to our work, Akkalyoncu Yilmaz et al. ( 2019) also proposes to improve dense passage retrieval based on sentence-level evidences, but their work is not in the research line of contrastive learning, and focuses more on passage re-ranking after retrieval but not retrieval itself.

 References: 
Zeynep Akkalyoncu Yilmaz, W., Yang, H., Zhang, J., and Lin 2019. Cross-domain modeling of sentence-level evidence for document retrieval. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3490--3496 10.18653/v1/D19-1352
Baudiš, P. 2015. Modeling of the question answering task in the yodaqa system. In International Conference of the cross-language evaluation Forum for European languages. pp. 222--228
Beltagy, I., Matthew, E., Peters, A., and Cohan 2004. Longformer: The long-document transformer. ArXiv preprint, abs. In Longformer: The long-document transformer. ArXiv preprint, abs.
Berant, J., Chou, A., Frostig, R., and Liang, P. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1533--1544
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E. 2020. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning. pp. 1597--1607
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423
Gao, T., Yao, X., and Chen, D. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Empirical Methods in Natural Language Processing.
Giorgi, J., Nitski, O., Wang, B., and Bader, G. 2021. DeCLUTR: Deep contrastive learning for unsupervised textual representations. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 879--895 10.18653/v1/2021.acl-long.72
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. 2020. Retrieval augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning. pp. 3929--3938
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. B. 2020. Momentum contrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9726--9735 10.1109/CVPR42600.2020.00975
Izacard, G. and Grave, E. 2021. Distilling knowledge from reader to retriever for question answering. In ICLR 2021: The Ninth International Conference on Learning Representations.
Johnson, J., Douze, M., and Jégou, H. 2019. Billion-scale similarity search with gpus. In IEEE Transactions on Big Data.
Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 1601--1611 10.18653/v1/P17-1147
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6769--6781 10.18653/v1/2020.emnlp-main.550
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. 2019. Natural questions: A benchmark for question answering research. In Transactions of the Association for Computational Linguistics. pp. 452--466 10.1162/tacl_a_00276
Lee, H., Hudson, D. A., Lee, K., and Manning, C. D. 2020. SLM: Learning a discourse language representation with sentence unshuffling. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1551--1562 10.18653/v1/2020.emnlp-main.120
Lee, J., Sung, M., Kang, J., and Chen, D. 2021. Learning dense representations of phrases at scale. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 6634--6647 10.18653/v1/2021.acl-long.518
Lee, K., Chang, M., and Toutanova, K. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 6086--6096 10.18653/v1/P19-1612
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach.
Lu, J., Hernandez Abrego, G., Ma, J., Ni, J., and Yang, Y. 2020. Neural passage retrieval with improved negative contrast. In Neural passage retrieval with improved negative contrast. abs/2010.12523
Manmatha, R., Chao-Yuan, Wu, A. J., Smola, P., and Krähenbühl 2017. Sampling matters in deep embedding learning. In IEEE International Conference on Computer Vision. pp. 2859--2867 10.1109/ICCV.2017.309
Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, W. X., Dong, D., Wu, H., and Wang, H. 2021. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 5835--5847 10.18653/v1/2021.naacl-main.466
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 2383--2392 10.18653/v1/D16-1264
Ramos, J. 2003. Using tf-idf to determine word relevance in document queries. In Proceedings of the first instructional conference on machine learning. pp. 29--48
Robertson, S. and Zaragoza, H. 2009. The probabilistic relevance framework: Bm25 and beyond. In Foundations and Trends in Information Retrieval. pp. 333--389
Sachan, D., Patwary, M., Shoeybi, M., Kant, N., Ping, W., Hamilton, W. L., and Catanzaro, B. 2021. End-to-end training of neural retrievers for open-domain question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 6648--6662 10.18653/v1/2021.acl-long.519
Tang, H., Sun, X., Jin, B., Wang, J., Zhang, F., and Wu, W. 2021. Improving document representations by generating pseudo query embeddings for dense retrieval. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 5054--5064 10.18653/v1/2021.acl-long.392
Vig, J. and Ramea, K. 2019. Comparison of transfer-learning approaches for response selection in multi-turn conversations. In Workshop on DSTC7.
Wolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents.
Wu, B., Zhang, Z., and Zhao, H. 2021. Graph-free multi-hop reading comprehension: A select-to-guide strategy. In Graph-free multi-hop reading comprehension: A select-to-guide strategy. abs/2107.11823
Wu, Z., Wang, S., Gu, J., Khabsa, M., Sun, F., and Ma, H. 2020. Clear: Contrastive learning for sentence representation. In Clear: Contrastive learning for sentence representation.
Xiong, L., Xiong, C., Li, Y., Tang, K., Liu, J., Paul, N., Bennett, J., Ahmed, A., and Overwijk 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In International Conference on Learning Representations.
Yan, Y., Li, R., Wang, S., Zhang, F., Wu, W., and Xu, W. 2021. ConSERT: A contrastive framework for self-supervised sentence representation transfer. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 5065--5075 10.18653/v1/2021.acl-long.393
Yang, P., Fang, H., and Lin, J. 2017. Anserini: Enabling the use of lucene for information retrieval research. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 1253--1256
Yang, Y., Jin, N., Lin, K., Guo, M., and Cer, D. 2021. Neural retrieval for question answering with cross-attention supervised data augmentation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 263--268 10.18653/v1/2021.acl-short.35