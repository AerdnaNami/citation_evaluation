Related Work

The KILT benchmark and public leaderboard 2 combines eleven datasets across five tasks. The main advantage of the KILT distribution of these datasets is that the provenance information from each dataset is realigned to reference the same snapshot of Wikipedia. A unified evaluation script and set of metrics is also provided. In this work, we focus on four tasks, such as Slot Filling [Levy et al., 2017, Elsahar et al., 2018, Question Answering [Kwiatkowski et al., 2019, Joshi et al., 2017, Fact Checking [Thorne et al., 2018a,c], and Dialog [Dinan et al., 2019] (see Figure 1). A set of baseline methods have been proposed for KILT. GENRE [Cao et al., 2021] is trained on BLINK  and all KILT tasks jointly using a sequence-to-sequence language model to generate the title of the Wikipedia page where the answer can be found. This method is a strong baseline to evaluate the retrieval performance, but it does not address the downstream tasks. On the other hand, generative models, such as BART [Lewis et al., 2020a] and T5 [Raffel et al., 2020], show interesting performance when finetuned on the downstream tasks relying only on the implicit knowledge stored in the weights of the 2 https://eval.ai/web/challenges/ challenge-page/689/leaderboard neural networks, without the use of any explicit retrieval component.

RAG [Lewis et al., 2020b], an end-to-end retrieval-based generative model, is the best performing baseline in KILT and it incorporates DPR [Karpukhin et al., 2020] to first retrieve relevant passages for the query, then it uses a model initialized from BART [Lewis et al., 2020a] to perform a sequence-to-sequence generation from each evidence passage concatenated with the query in order to generate the answer. Figure 2 shows the architecture of RAG.

Multi-task DPR  exploits multi-task learning by training both DPR passage and query encoder on all KILT tasks. DensePhrases [Lee et al., 2021] addresses the knowledge intensive tasks with a short answer, such as slot filling. It indexes the phrases in the corpus that can be potential answers. The extracted phrases are represented by their start and end token vectors from the final layer of a transformer initialized from SpanBERT [Joshi et al., 2020].

Knowledge Graph Induction (KGI) [Glass et al., 2021] combines DPR and RAG models, both trained with task and dataset specific training. KGI employs a two phase training procedure: first training the DPR model, i.e. both the query and context encoder, using the KILT provenance ground truth. Then, KGI trains the sequence-to-sequence generation and further trains the query encoder using only the target output as the objective. This results in large improvements in retrieval performance and, as a consequence, in the downstream tasks.

Multi-stage or cascade approaches to retrieval have received ample attention in Information Retrieval (IR) research. The multi-stage approach begins with the initial retrieval phase, where an initial set of documents or passages form the pool of candidates to be considered for ranking. Then one or more phases of increasingly computationally demanding rerankers are applied. Early approaches in learning to rank [Liu, 2009] used features and linear classifiers. Pre-trained language models, especially BERT , have shown state-ofthe-art performance when applied to the task of relevance ranking. Transformers may be applied as classifiers to each query and passage pair independently [Nogueira and Cho, 2019] or as generators to produce labels for passages in a sequence-tosequence model [Nogueira et al., 2020]. The novel tells the story of Dracula's attempt to move from Transylvania to England so that he may find new blood and spread the undead curse, and of the battle between Dracula and a small group of men and a woman led by Professor Abraham Van Helsing.

 References: 
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., and Gray, S. 2020. NeurIPS. In NeurIPS.
De Cao, N., Izacard, G., Riedel, S., and Petroni, F. 2021. Autoregressive entity retrieval. In International Conference on Learning Representations. OpenReview.net.
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 doi: 10.18653/ v1/N19-1423
Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J. 2018. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations.
Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J. 2019. Wizard of wikipedia: Knowledge-powered conversational agents. In ICLR (Poster). OpenReview.net.
Elsahar, H., Vougiouklis, P., Remaci, A., Gravier, C., Hare, J., Laforest, F., Simperl, E., and . T-Rex 2018. A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).
Glass, M., Rossiello, G., Faisal Mahbub, M., Chowdhury, A., and Gliozzo 2021. Robust retrieval augmented generation for zero-shot slot filling. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. 2020. Realm: Retrievalaugmented language model pre-training. In Realm: Retrievalaugmented language model pre-training. arXiv:2002.08909
Hinton, G., Vinyals, O., and Dean, J. 2015. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop.
Johnson, J., Douze, M., and Jégou, H. 2017. Billionscale similarity search with gpus. In Billionscale similarity search with gpus. arXiv:1702.08734
Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 1601--1611 10.18653/v1/P17-1147
Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., and Levy, O. 2020. SpanBERT: Improving pre-training by representing and predicting spans. In Transactions of the Association for Computational Linguistics. pp. 64--77 doi: 10.1162/ tacl_a_00300
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6769--6781 10.18653/v1/2020.emnlp-main.550
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. 2019. Natural questions: A benchmark for question answering research. In Transactions of the Association for Computational Linguistics. pp. 452--466 doi: 10.1162/ tacl_a_00276
Lee, J., Sung, M., Kang, J., and Chen, D. 2021. Learning dense representations of phrases at scale. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 6634--6647 10.18653/v1/2021.acl-long.518
Levy, O., Seo, M., Choi, E., and Zettlemoyer, L. 2017. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning. pp. 333--342 10.18653/v1/K17-1034
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., and Kiela, D. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. In Advances in Neural Information Processing Systems. pp. 9459--9474
Lin, C. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out. pp. 74--81
Liu, T. 2009. Learning to rank for information retrieval. In Information Retrieval. pp. 225--331
Maillard, J., Karpukhin, V., Petroni, F., Yih, W., Oguz, B., Stoyanov, V., and Ghosh, G. 2021. Multi-task retrieval for knowledge-intensive tasks. In ACL/IJCNLP (1). pp. 1098--1111
Yu, A., Malkov, and Dmitry A Yashunin 2018. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence. In Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence. pp. 824--836
Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L. 2016. Ms marco: A human generated machine reading comprehension dataset. In CoCo@ NIPS.
Nogueira, R. and Cho, K. 2019. Passage reranking with bert. In Passage reranking with bert. arXiv:1901.04085
Nogueira, R., Jiang, Z., Pradeep, R., and Lin, J. 2020. Document ranking with a pretrained sequence-to-sequence model. In Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 708--718 10.18653/v1/2020.findings-emnlp.63
Paranjape, A., Khattab, O., Potts, C., Zaharia, M., and Manning, C.D. 2021. Hindsight: Posterior-guided training of retrievers for improved open-ended generation. In Hindsight: Posterior-guided training of retrievers for improved open-ended generation. arXiv:2110.07752
Petroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M., Cao, N. D., Thorne, J., Jernite, Y., Karpukhin, V., and Maillard, J. 2021. Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2523--2544 10.18653/v1/2021.naacl-main.200
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer.
Robertson, S. and Zaragoza, H. 2009. The probabilistic relevance framework: Bm25 and beyond. Found. In Trends Inf. Retr. pp. 333--389 10.1561/1500000019
Thienes, C. and Pertschuk, J. 2019. Nboost: Neural boosting search results. In Nboost: Neural boosting search results.
Thorne, J., Vlachos, A., Christodoulopoulos, C., and Mittal, A. 2018. FEVER: a large-scale dataset for fact extraction and verification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 809--819
Thorne, J., Vlachos, A., and Cocarascu, O. 2018. Christos Christodoulopoulos, and Arpit Mittal. The fact extraction and verification (FEVER) shared task. In Christos Christodoulopoulos, and Arpit Mittal. The fact extraction and verification (FEVER) shared task. abs/1811.10971
Thorne, J., Vlachos, A., and Cocarascu, O. 2019. Christos Christodoulopoulos, and Arpit Mittal. The fever2. 0 shared task. In Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER). pp. 1--6
Wang, L., Lin, J., and Metzler, D. 2011. A cascade ranking model for efficient ranked retrieval. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. pp. 105--114
Wu, L., Petroni, F., Josifoski, M., Riedel, S., and Zettlemoyer, L. 2020. Scalable zero-shot entity linking with dense entity retrieval. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6397--6407 10.18653/v1/2020.emnlp-main.519