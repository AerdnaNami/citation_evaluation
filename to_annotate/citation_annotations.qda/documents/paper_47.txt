Related Work

Open-Domain Response Generation Recent work of open-domain response generation gener-ally follows the work of Ritter et al. (2011) where the task is treated as a machine translation task, and many of them use a Seq2Seq structure (Sutskever et al., 2014) following previous work (Vinyals and Le, 2015;Shang et al., 2015;Sordoni et al., 2015). In recent years, substantial improvements have been made (Serban et al., 2017;Li et al., 2016;Wolf et al., 2019), and embeddings are used to control response generation on extra information such as persona (Li et al., 2016), profiles (Yang et al., 2017), coherence (Xu et al., 2018, emotions (Huang et al., 2018), and dialogue attributes like response-relatedness (See et al., 2019). However, there is a lack of work that uses embeddings to control response generation over multiple corpora. Our work follows the common models of opendomain conversational systems, while we study the problem of multiple corpora of different domains.

 References: 
Akama, R., Inada, K., Inoue, N., Kobayashi, S., and Inui, K. 2017. Generating Stylistically Consistent Dialog Responses with Transfer Learning. In Proceedings of the Eighth International Joint Conference on Natural Language Processing. pp. 408--412
Bahdanau, D., Cho, K., and Bengio, Y. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In 3rd International Conference on Learning Representations.
Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F. 2007. Analysis of Representations for Domain Adaptation. In Advances in Neural Information Processing Systems 19. pp. 137--144
Bengio, Y., Louradour, J., Collobert, R., and Weston, J. 2009. Curriculum Learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09. pp. 41--48 10.1145/1553374.1553380
Chu, C., Dabre, R., and Kurohashi, S. 2017. An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 385--391 10.18653/v1/P17-2061
Chu, C. and Wang, R. 2018. A survey of domain adaptation for neural machine translation. In Proceedings of the 27th International Conference on Computational Linguistics. pp. 1304--1319
Daume, H. and Iii 2007. Frustratingly Easy Domain Adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. pp. 256--263
Dinan, E., Logacheva, V., Malykh, V., Miller, A., Shuster, K., Urbanek, J., Kiela, D., Szlam, A., Serban, I., Lowe, R., Prabhumoye, S., Black, A. W., Rudnicky, A., Williams, J., Pineau, J., Burtsev, M., and Weston, J. 2019. The Second Conversational Intelligence Challenge (Con-vAI2). In The Second Conversational Intelligence Challenge (Con-vAI2). arXiv:1902.00098
Hochreiter, S. and Schmidhuber, J. 1997. Long Short-Term Memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735
Huang, C., Zaiane, O., Trabelsi, A., and Dziri, N. 2018. Automatic Dialogue Generation with Expressed Emotions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 49--54 10.18653/v1/N18-2008
Johnson, M., Schuster, M., Le, Q. V., Krikun, M., Wu, Y., Chen, Z., Thorat, N., Viégas, F., Wattenberg, M., Corrado, G., Hughes, M., and Dean, J. 2017. Google's multilingual neural machine translation system: Enabling zero-shot translation. In Transactions of the Association for Computational Linguistics. pp. 339--351 10.1162/tacl_a_00065
Joshi, M., Dredze, M., Cohen, W. W., and Rose, C. 2012. Multi-Domain Learning: When Do Domains Matter?. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. pp. 1302--1312
Sean, H. K., Kang, H., and Pashler 2012. Learning painting styles: Spacing is advantageous when it promotes discriminative contrast. In Applied Cognitive Psychology. pp. 97--103
Kornell, N. and Bjork, R. 2008. Learning concepts and categories is spacing the "Enemy of induction. In Psychological science. pp. 585--592 10.1111/j.1467-9280.2008.02127.x
Leopold, E. and Kindermann, J. 2002. Text categorization with support vector machines. how to represent texts in input space?. In Machine Learning. pp. 423--444 10.1023/A:1012491419635
Li, J., Galley, M., Brockett, C., Spithourakis, G., Gao, J., and Dolan, B. 2016. A Persona-Based Neural Conversation Model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pp. 994--1003 10.18653/v1/P16-1094
Lin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81
Lison, P., Tiedemann, J., and Kouylekov, M. 2018. OpenSubtitles2018: Statistical Rescoring of Sentence Alignments in Large, Noisy Parallel Corpora. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018). European Language Resource Association.
Liu, C., Lowe, R., Serban, I., Noseworthy, M., Charlin, L., and Pineau, J. 2016. How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 2122--2132 10.18653/v1/D16-1230
Lowe, R., Pow, N., Serban, I., and Pineau, J. 2015. The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems. In Proceedings of the 16th. 10.18653/v1/W15-4640
Annual Meeting of the Special Interest Group on Discourse and Dialogue. In Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 285--294
Luan, Y., Brockett, C., Dolan, B., Gao, J., and Galley, M. 2017. Multi-Task Learning for Speaker-Role Adaptation in Neural Conversation Models. In Proceedings of the Eighth International Joint Conference on Natural Language Processing. pp. 605--614
Miller, A., Feng, W., Batra, D., Bordes, A., Fisch, A., Lu, J., Parikh, D., and Weston, J. 2017. ParlAI: A Dialog Research Software Platform. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. pp. 79--84 10.18653/v1/D17-2014
Niu, T. and Bansal, M. 2018. Polite Dialogue Generation Without Parallel Data. In Transactions of the Association for Computational Linguistics. pp. 373--389
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., Devito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. 2017. Automatic differentiation in PyTorch. In NIPS-W.
Press, O. and Wolf, L. 2017. Using the output embedding to improve language models. In EACL.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Better Language Models and Their Implications. In Better Language Models and Their Implications.
Ritter, A., Cherry, C., and Dolan, W. B. 2011. Data-Driven Response Generation in Social Media. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. pp. 583--593
Rohrer, D. 2012. Interleaving helps students distinguish among similar concepts. In Educational Psychology Review. pp. 355--367
Saha, A., Rai, P., Daumã©, H., Iii, and Venkatasubramanian, S. 2011. Online Learning of Multiple Tasks and Their Relationships. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. pp. 643--651
See, A., Roller, S., Kiela, D., and Weston, J. 2019. What makes a good conversation? How controllable attributes affect human judgments. In What makes a good conversation? How controllable attributes affect human judgments. arXiv:1902.08654
Vlad Serban, I., Sordoni, A., Lowe, R., Charlin, L., Pineau, J., Courville, A., and Bengio, Y. 2017. A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues. In Thirty-First AAAI Conference on Artificial Intelligence.
Shang, L., Lu, Z., and Li, H. 2015. Neural Responding Machine for Short-Text Conversation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. pp. 1577--1586 10.3115/v1/P15-1152
Shi, Y., Larson, M., and Jonker, C. M. 2015. Recurrent Neural Network Language Model Adaptation with Curriculum Learning. In Comput. Speech Lang. pp. 136--154 10.1016/j.csl.2014.11.004
Smith, E. M., Williamson, M., Shuster, K., Weston, J., and Boureau, Y. 2020. Can You Put it All Together: Evaluating Conversational Agents' Ability to Blend Skills. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2021--2030 10.18653/v1/2020.acl-main.183
Sordoni, A., Galley, M., Auli, M., Brockett, C., Ji, Y., Mitchell, M., Nie, J., Gao, J., and Dolan, B. 2015. A Neural Network Approach to Context-Sensitive Generation of Conversational Responses. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 196--205 10.3115/v1/N15-1020
Sutskever, I., Vinyals, O., and Le, Q.V. 2014. Sequence to Sequence Learning with Neural Networks. In Advances in Neural Information Processing Systems. pp. 3104--3112
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems. pp. 5998--6008
Vinyals, O. and Le, Q. 2015. A Neural Conversational Model. In A Neural Conversational Model. arXiv:1506.05869
Concatenated: I'm not sure. I'm not sure what the big banks are doing. Interleaved: What? Labeled: I'm not sure what you're talking about. Multi-Task Labeled:I'm not sure what you mean. Weighted: What's going on? OSDB Context: Rigor mortis has yet to set in. His core body temperature remains at 98 degrees. His skin color, lividity is healthy. PersonaChat (single):I'm not sure if I like him or not. I like to eat meat. In OSDB Context: You know the main reason parents kill kids? PersonaChat (single): I am not sure. I am a vegan.
Multi-Task Labeled: He is a man of great character. Weighted: He's been in the hospital for two weeks. In Labeled: He is a man of great character. Weighted: He's been in the hospital for two weeks.
PersonaChat (single): I'm not sure what that means. Concatenated: That is so sad. I hope she gets better soon. Interleaved: I'm so sorry. I'm so sorry. Labeled: I'm so sorry. In Twitter Context: Hillary ran away from rain today. Trump is back on stage minutes after assassination attempt.
Multi-Task Labeled: I'm not sure what happened to her. Weighted: She's a liar. In Labeled: I'm not sure what happened to her. Weighted: She's a liar.
Concatenated: I'm not sure what that means. I'm a big fan of the show. Interleaved: I'm not sure what happened to the epstein case. Labeled: I'm not sure what the link was. In Twitter Context: It was gruesome & extremely rare to have two witnesses. It was linked to epstein. Media didn't report it.
Multi-Task Labeled: I'm not sure what happened to the epstein case. Weighted: The media is biased and biased. In Labeled: I'm not sure what happened to the epstein case. Weighted: The media is biased and biased.
Twitter Context: This is 100% true, but can we talk about it tomorrow and make argentina the focus today? Something needs to stick. PersonaChat (single): Yes, I think so. I'm going to go to the gym tomorrow. In Twitter Context: This is 100% true, but can we talk about it tomorrow and make argentina the focus today? Something needs to stick. PersonaChat (single): Yes, I think so. I'm going to go to the gym tomorrow.
Concatenated: I am going to go to the beach. Interleaved: I'm going to install the latest version of Ubuntu. Labeled: I'm going to install a new kernel. Multi-Task Labeled: I'm going to install the latest version of Ubuntu. In Multi-Task Labeled: I'm not sure what you mean by that. I'm not sure what you mean by that.
Labeled: I'm not sure, I'm not sure if I can help you. Multi-Task Labeled: I'm in the city. I'm in a lot of places. In Labeled: I'm not sure, I'm not sure if I can help you. Multi-Task Labeled: I'm in the city. I'm in a lot of places.
Interleaved: I have a root shell, but I can't login. Labeled: I have a root shell. Multi-Task Labeled: I don't know what that is. In Interleaved: I have a root shell, but I can't login. Labeled: I have a root shell. Multi-Task Labeled: I don't know what that is.
Weighted: I love beer too. I love to drink it while I am driving. PersonaChat Context: A food bank near my house. Do you ever volunteer? PersonaChat (single): I do, I love to volunteer at the soup kitchen. Concatenated: Yes I do. I love to help people. Interleaved: I do. I have a lot of friends. Labeled: I do. I am a volunteer at the local soup kitchen. Multi-Task Labeled: I do. I love it. I volunteer at the animal shelter. In PersonaChat Context: That must be hard. I like a beer after driving my nissan pathfinder. PersonaChat (single): I like to go to the movies. I have been to a few.
Concatenated: I am a mom, I have three kids. Interleaved: I love kids, I have a son who is in high school. Labeled: I bet he is a good kid. I am a teacher. Multi-Task Labeled: I am sorry to hear that. I am a teacher. In Concatenated: I am a mom, I have three kids. Interleaved: I love kids, I have a son who is in high school. Labeled: I bet he is a good kid. I am a teacher. Multi-Task Labeled: I am sorry to hear that. I am a teacher.
Responses generated from GPT-2 fine-tuned on 4 corpora with multiple methods. In Table.