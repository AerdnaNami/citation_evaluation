Related work

Semantic textual similarity

Most recent studies tried to leverage a pretrained language model with various model architectures and training objectives for STS tasks, achieving the state-of-the-art performance. In terms of model architecture, Devlin et al. (2019) focus on exhaustive cross-correlation between sentences by taking a concatenated text of two sentences as an input, while Reimers and Gurevych (2019) improve scalability based on a Siamese network and Humeau et al. (2020) adopt a hybrid approach. Along with the progress of model architectures, many advanced objectives for STS tasks were proposed as well. Specifically, Reimers and Gurevych (2019) mainly use the classification objective for an NLI dataset, and Wu et al. (2020) adopt contrastive learning to utilize self-supervision from a large corpus. Yan et al. (2021); Gao et al. (2021) incorporate a parallel corpus such as NLI datasets into their contrastive learning framework.

Despite their effectiveness, the interpretability of the above models for STS tasks was not fully explored (Belinkov and Glass, 2019). One related task is interpretable STS, which aims to predict chunk alignment between two sentences . For this task, a variety of supervised approaches were proposed based on neural networks (Konopík et al., 2016;, linear programming (Tekumalla and Jat, 2016), and pretrained models (Maji et al., 2020). However, these methods cannot predict the similarity between sentences because they focus on finding chunk alignment only. To the best of our knowledge, no previous approaches based on a pretrained model have taken into account both sentence similarity and interpretation.

Optimal transport

Optimal transport (Monge, 1781) has been successfully adopted in natural language processing due to its ability to find a plausible correspondence between two objects (Li et al., 2020;. For example, Kusner et al. (2015) adopt optimal transport to measure the distance between two documents with pretrained word vectors. In addition, Swanson et al. (2020) discover the rationale in textmatching via optimal transport, thereby improving model interpretability.

One well-known limitation of optimal transport is that finding the optimal solution is computationally intensive, and thus approximation schemes for this problem have been extensively researched (Grauman and Darrell, 2004;Shirdhonkar and Jacobs, 2008). To get the solution efficiently, Cuturi (2013) provides a regularizer inspired by a probabilistic theory and then uses Sinkhorn's algorithm. Kusner et al. (2015) relax the problem to get the quadratic-time solution by removing one of the constraints, and Wu et al. (2018) introduce a kernel method to approximate the optimal transport.

 References: 
Agirre, E., Banea, C., Cardie, C., Cer, D., Diab, M., Gonzalez-Agirre, A., Guo, W., Lopez-Gazpio, I., Maritxalar, M., Mihalcea, R., Rigau, G., Uria, L., and Wiebe, J. 2015. and pilot on interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation. pp. 252--263 10.18653/v1/S15-2045
Agirre, E., Gonzalez-Agirre, A., Lopez-Gazpio, I., Maritxalar, M., Rigau, G., and Uria, L. 2016. SemEval-2016 task 2: Interpretable semantic textual similarity. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). pp. 512--524 10.18653/v1/S16-1082
Belinkov, Y. and Glass, J. 2019. Analysis methods in neural language processing: A survey. In Transactions of the Association for Computational Linguistics. pp. 49--72 10.1162/tacl_a_00254
Samuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/D15-1075
Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia, L. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). pp. 1--14 10.18653/v1/S17-2001
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. 2020. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning. pp. 1597--1607
Conneau, A. and Kiela, D. 2018. SentEval: An evaluation toolkit for universal sentence representations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).
Cuturi, M. 2013. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems.
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423
Gao, T., Yao, X., and Chen, D. 2021. Simcse: Simple contrastive learning of sentence embeddings. In Simcse: Simple contrastive learning of sentence embeddings. arXiv:2104.08821
Leilani, H., Gilpin, D., Bau, Ben Z Yuan, A., Bajwa, M., Specter, L., and Kagal 2018. Explaining explanations: An overview of interpretability of machine learning. In 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA). pp. 80--89
Wael, H., Gomaa, Aly, A., and Fahmy 2013. A survey of text similarity approaches. In international journal of Computer Applications. pp. 13--18
Grauman, K. and Darrell, T. 2004. Fast contour matching using approximate earth mover's distance. In Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.
Humeau, S., Shuster, K., Lachaux, M., and Weston, J. 2020. Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring. In International Conference on Learning Representations.
Konopík, M., Pražák, O., Steinberger, D., and Brychcín, T. 2016. UWB at SemEval-2016 task 2: Interpretable semantic textual similarity with distributional semantics for chunks. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). pp. 803--808 10.18653/v1/S16-1124
Kusner, M., Sun, Y., Kolkin, N., and Weinberger, K. 2015. From word embeddings to document distances. In Proceedings of the 32nd International Conference on Machine Learning. pp. 957--966
Li, J., Li, C., Wang, G., Fu, H., Lin, Y., Chen, L., Zhang, Y., Tao, C., Zhang, R., Wang, W., Shen, D., Yang, Q., and Carin, L. 2020. Improving text generation with student-forcing optimal transport. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 9144--9156 10.18653/v1/2020.emnlp-main.735
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692
Lopez-Gazpio, I., Agirre, E., and Montse 2016. iUBC at SemEval-2016 task 2: RNNs and LSTMs for interpretable STS. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). pp. 771--776 10.18653/v1/S16-1119
Maji, S., Kumar, R., Bansal, M., Roy, K., and Goyal, P. 2020. Logic constrained pointer networks for interpretable textual similarity. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20. pp. 2405--2411 10.24963/ijcai.2020/333
Majumder, G., Pakray, P., Gelbukh, A., and Pinto, D. 2016. Semantic textual similarity methods, tools, and applications: A survey. In Computación y Sistemas. pp. 647--665
Mémoire sur la théorie des déblais et des remblais. In Mémoire sur la théorie des déblais et des remblais.
Reimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410
Rogers, A., Kovaleva, O., and Rumshisky, A. 2020. A primer in BERTology: What we know about how BERT works. In Transactions of the Association for Computational Linguistics. pp. 842--866 10.1162/tacl_a_00349
Salton, G. and Buckley, C. 1988. Termweighting approaches in automatic text retrieval. Information processing & management. In Termweighting approaches in automatic text retrieval. Information processing & management. pp. 513--523
Shirdhonkar, S., David, W., and Jacobs 2008. Approximate earth mover's distance in linear time. In 2008 IEEE Conference on Computer Vision and Pattern Recognition. pp. 1--8
Md Arafat Sultan, S., Bethard, T., and Sumner 2015. DLS@CU: Sentence similarity from word alignment and semantic vector composition. In Proceedings of the 9th International Workshop on Semantic Evaluation. pp. 148--153 10.18653/v1/S15-2027
Swanson, K., Yu, L., and Lei, T. 2020. Rationalizing text matching: Learning sparse alignments via optimal transport. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5609--5626 10.18653/v1/2020.acl-main.496
Tekumalla, L. and Jat, S. 2016. IIS-CNLP at SemEval-2016 task 2: Interpretable STS with ILP based multiple chunk aligner. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). pp. 790--795 10.18653/v1/S16-1122
Villani, C. 2008. Optimal Transport: Old and New. In Optimal Transport: Old and New.
Williams, A., Nangia, N., and Bowman, S. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1112--1122 10.18653/v1/N18-1101
Wu, L., En-Hsu Yen, I., Xu, K., Xu, F., Balakrishnan, A., Chen, P., Ravikumar, P., and Witbrock, M. J. 2018. Word mover's embedding: From Word2Vec to document embedding. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 4524--4534 10.18653/v1/D18-1482
Wu, Z., Wang, S., Gu, J., Khabsa, M., Sun, F., and Ma, H. 2020. Clear: Contrastive learning for sentence representation. In Clear: Contrastive learning for sentence representation. arXiv:2012.15466
Xu, J., Zhou, H., Gan, C., Zheng, Z., and Li, L. 2021. Vocabulary learning via optimal transport for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 7361--7373 10.18653/v1/2021.acl-long.571
Yan, Y., Li, R., Wang, S., Zhang, F., Wu, W., and Xu, W. 2021. ConSERT: A contrastive framework for self-supervised sentence representation transfer. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 5065--5075 10.18653/v1/2021.acl-long.393