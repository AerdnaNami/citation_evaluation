Introduction

At present, for a large majority of natural language processing tasks, the most successful approach is fine-tuning pre-trained models with task-specific labelled data. Unfortunately, for many languages, and especially low-resource languages, such taskspecific labelled data is often not available. A potential solution is cross-lingual fine-tuning of multilingual pre-trained language models (Conneau et al., 2020;Devlin et al., 2018), using available data from some source language to model the phenomenon in a different target language for which labelled data does not exist.

Cross-lingual generalisability of large pretrained language models is often evaluated by finetuning multilingual models on English data and testing them on unseen languages (Conneau et al., 2018;Artetxe et al., 2020;Lewis et al., 2020;Hu et al., 2020). Of course, this approach is influenced by the availability of English training data for given tasks, but also then comes with the implicit assumption that English is a representative source language. This, however, may not be true in practice. Specifically, depending on the task, aspects of similarity between source and target language may be relevant for cross-lingual transfer performance (de Vries et al., 2021). If similarity between source and target language impacts performance, crosslingual transfer should not be assessed using only a single predetermined source language, especially if training sets in multiple languages are available.

Furthermore, target test languages are generally selected based on data availability for the evaluated tasks, but availability may not result in a representative subset of the world's languages. The XTreme benchmark collection (Hu et al., 2020), for example, attempts to alleviate this problem by including a varied selection of languages from different language families. This collection contains token classification, text classification, question answering and retrieval tasks in 40 languages. The language selection does, however, obfuscate the fact that for most non-Indo-European and low-resource languages no data is available for semantically rich tasks such as question answering. This imbalance regarding tasks in this type of collections may consequently inflate the perceived performance for these languages.

In this work, we aim to shed light on what factors make a language a good source and/or target language for cross-lingual transfer when fine-tuning a large multilingual model. We evaluate this via partof-speech (POS) tagging data, as this is the only task for which high-quality data is available in a large number of languages, including low-resource languages from different language families. Also, high cross-lingual POS tagging performance may be seen as a precondition for more semantically complex tasks, as a base understanding of syntactic structure in both the source and target language is necessary for any meaningful natural language processing task.

Contributions This paper is a case-study of cross-lingual transfer learning with part-of-speech tagging. We explore the limits and contributing factors to successful cross-lingual transfer and partof-speech tagging in particular. Among others, we evaluate the effects of (matching) language families, (matching) writing systems, and pre-training on cross-lingual training. Moreover, we provide insights that can help to estimate performance when one tries to transfer to a low-resource language with little or no annotated data. Source code will be released on Github, and 65 fine-tuned models will be shared via the Hugging Face Model Hub.

 References: 
Paul Adalian, R. 2010. Historical dictionary of Armenia. In Historical dictionary of Armenia.
Henning Andersen 2003. Amsterdam studies in the theory and history of linguistic science. In Series. pp. 45--76
Artetxe, M., Ruder, S., and Yogatama, D. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4623--4637 10.18653/v1/2020.acl-main.421
Barbour, S. and Carmichael, C. 2000. Language and nationalism in Europe. In Language and nationalism in Europe.
Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 8440--8451 10.18653/v1/2020.acl-main.747
Conneau, A., Rinott, R., Lample, G., Williams, A., Bowman, S., Schwenk, H., and Stoyanov, V. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of EMNLP 2018. pp. 2475--2485
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. BERT: pre-training of deep bidirectional transformers for language understanding. In BERT: pre-training of deep bidirectional transformers for language understanding. abs/1810.04805
Eberhard, D. M., Simons, G. F., and Fennig, C. D. 2021. Ethnologue: Languages of the World. In Ethnologue: Languages of the World.
Hu, J., Ruder, S., Siddhant, A., Neubig, G., Firat, O., and Johnson, M. 2003. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization. In Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization.
Kordić, S. 2010. Jezik i nacionalizam (language and nationalism). In Jezik i nacionalizam (language and nationalism).
Lewis, P., Oguz, B., Rinott, R., Riedel, S., and Schwenk, H. 2020. MLQA: Evaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7315--7330 10.18653/v1/2020.acl-main.653
Pires, T., Schlinger, E., and Garrette, D. 2019. How multilingual is multilingual BERT?. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4996--5001 10.18653/v1/P19-1493
Marco René Spruit, W., Heeringa, J., and Nerbonne 2009. Associations among linguistic levels. In The Forests behind the Trees. pp. 1624--1642 10.1016/j.lingua.2009.02.001
Wietse De Vries, M., Bartelds, M., Nissim, M., and Wieling 2021. Adapting monolingual models: Data can be scarce when language similarity is high. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 4901--4907 10.18653/v1/2021.findings-acl.433
Wichmann, S., Holman, E. W., Bakker, D., and Brown, C. H. 2010. Evaluating linguistic distance measures. In Physica A: Statistical Mechanics and its Applications. pp. 3632--3639 10.1016/j.physa.2010.05.011
Zeman, D. and Nivre, J. 2021. Universal dependencies 2.8.1. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL. In Faculty of Mathematics and Physics.