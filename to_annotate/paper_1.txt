Related Work

Large-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Schütze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.

Introducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.

There are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.

 References: 
Banerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72
Bodenreider, O. 2004. The unified medical language system (umls): integrating biomedical terminology. In Nucleic acids research. pp. D267--D270
Bosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470
Chakrabarty, T., Ghosh, D., Muresan, S., and Peng, N. 2020. Rˆ3: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7976--7986 10.18653/v1/2020.acl-main.711
Do, N. and Pavlick, E. 2021. Are rotten apples edible? challenging commonsense inference ability with exceptions. In FINDINGS.
Hendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, '., O S'eaghdha, S., Pad'o, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38
Elizabeth M Huffman, D. I., Athanasiadis, N. E., Anton, Lindsay, A., Haskett, D. L., Doster, D., Stefanidis, N. K., and Lee 2021. How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic. In The American Journal of Surgery. pp. 277--284
Hwang, J. D., Bhagavatula, C., Ronan Le Bras, J., Da, K., Sakaguchi, A., Bosselut, Y., and Choi 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In ArXiv. abs/2010.05953
Ji, H., Ke, P., Huang, S., Wei, F., Zhu, X., and Huang, M. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 725--736 10.18653/v1/2020.emnlp-main.54
Kassner, N. and Schütze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In ACL.
Katz, N. and Mcnulty, K. 1994. Reflective listening. In Reflective listening.
Kitaev, N. and Klein, D. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2676--2686 10.18653/v1/P18-1249
Lewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 1910. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs. In Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs.
Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014
Lin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81
Liu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., and Wang, P. 2020. K-bert: Enabling language representation with knowledge graph. In ArXiv. abs/1909.07606
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692
Huanru Henry Mao, B. P., Majumder, J., Mcauley, G., and Cottrell 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5988--5993 10.18653/v1/D19-1615
William, R., Miller, S., and Rollnick 2012. Motivational interviewing: Helping people change. In Motivational interviewing: Helping people change.
Papineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135
Mario R Paredes, V., Apaolaza, C., Fernandez-Robin, P., Hartmann, D., and Yañez-Martinez 2021. The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience. In Personality and Individual Differences. pp. 110455
Pérez-Rosas, V., Mihalcea, R., Resnicow, K., Singh, S., and An, L. 2016. Building a motivational interviewing dataset. In Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. pp. 42--51 10.18653/v1/W16-0305
Petroni, F., Rocktäschel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066
Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250
Radford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.
Reimers, N. and Gurevych, I. 1908. Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs. In Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs.
Reimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410
Ren, P., Chen, Z., Monz, C., Ma, J., and De Rijke, M. 2020. Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8697--8704
Sap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035
Shwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373
Song, H., Zhang, W., Cui, Y., Wang, D., and Liu, T. 2019. Exploiting persona information for diverse generation of conversational responses. In IJCAI.
Speer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.
Wolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149
Zhang, H., Khashabi, D., Song, Y., and Roth, D. 2020. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI). pp. 2020
Zhang, T., Kishore, V., Wu, F., Kilian, Q., Weinberger, Y., and Artzi 2019. Bertscore: Evaluating text generation with bert. In Bertscore: Evaluating text generation with bert. arXiv:1904.09675
Zhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390 10.18653/v1/2020.emnlp-main.272