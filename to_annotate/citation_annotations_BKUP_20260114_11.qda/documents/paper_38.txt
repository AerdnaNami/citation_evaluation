Introduction

Recently, pre-training a transformer model on a large corpus with language modeling tasks and finetuning it on different downstream tasks has become the main transfer learning paradigm in natural language processing (Devlin et al., 2019). Notably, this paradigm requires updating and storing all the model parameters for every downstream task. As the model size proliferates (e.g., 330M parameters for BERT (Devlin et al., 2019) and 175B for GPT-3 (Brown et al., 2020)), it becomes computationally expensive and challenging to fine-tune the entire pre-trained language model (LM). Thus, it is natural to ask the question of whether we can transfer the knowledge of a pre-trained LM into downstream tasks by tuning only a small portion of its parameters with most of them freezing.

Studies have attempted to address this question from different perspectives. One line of research (Li and Liang, 2021) suggests to augment the model with a few small trainable mod-ules and freeze the original transformer weight. Take Adapter (Houlsby et al., 2019;Pfeiffer et al., 2020a,b) and Compacter (Mahabadi et al., 2021) for example, both of them insert a small set of additional modules between each transformer layer. During fine-tuning, only these additional and taskspecific modules are trained, reducing the trainable parameters to ∼ 1-3% of the original transformer model per task.

Another line of works focus on prompting. The GPT-3 models (Brown et al., 2020;Schick and Schütze, 2020) find that with proper manual prompts, a pre-trained LM can successfully match the fine-tuning performance of BERT models. LM-BFF (Gao et al., 2020), EFL (Wang et al., 2021), and AutoPrompt (Shin et al., 2020) further this direction by insert prompts in the input embedding layer. However, these methods rely on grid-search for a natural language-based prompt from a large search space, resulting in difficulties to optimize.

To tackle this issue, prompt tuning (Lester et al., 2021), prefix tuning (Li and Liang, 2021), and Ptuning (Liu et al., 2021a,b) are proposed to prepend trainable prefix tokens to the input layer and train these soft prompts only during the fine-tuning stage. In doing so, the problem of searching discrete prompts are converted into an continuous optimization task, which can be solved by a variety of optimization techniques such as SGD and thus significantly reduced the number of trainable parameters to only a few thousand. However, all existing prompt-tuning methods have thus far focused on task-specific prompts, making them incompatible with the traditional LM objective. For example, it is unlikely to see many different sentences with the same prefix in the pre-training corpus. Thus, a unified prompt may disturb the prediction and lead to a performance drop. In light of these limitations, we instead ask the following question: Can we generate input-dependent prompts to smooth the domain difference?

In this paper, we present the instance-dependent prompt generation (IDPG) strategy for efficiently tuning large-scale LMs. Different from the traditional prompt-tuning methods that rely on a fixed prompt for each task, IDPG instead develops a conditional prompt generation model to generate prompts for each instance. Formally, the IDPG generator can be denoted as f (x; W), where x is the instance representation and W represents the trainable parameters. Note that by setting W to a zero matrix and only training the bias, IDPG would degenerate into the traditional prompt tuning process (Lester et al., 2021). To further reduce the number of parameters in the generator f (x; W), we propose to apply a lightweight bottleneck architecture (i.e., a two-layer perceptron) and then decompose it by a parameterized hypercomplex multiplication (PHM) layer (Zhang et al., 2021). To summarize, this works makes the following contributions:

• We introduce an input-dependent prompt generation method-IDPG-that only requires training 134K parameters per task, corresponding to ∼0.04% of a pre-trained LM such as RoBERTa-Large (Liu et al., 2019).

• Extensive evaluations on ten natural language understanding (NLU) tasks show that IDPG consistently outperforms task-specific prompt tuning methods by 1.6-3.1 points (Cf. Table 1). Additionally, it also offers comparable performance to Adapter-based methods while using much fewer parameters (134K vs. 1.55M).

• We conduct substantial intrinsic studies, revealing how and why each component of the proposed model and the generated prompts could help the downstream tasks.

 References: 
Ba, J. L., Kiros, J. R., and Hin, G. E. arXiv:1607.06450
Tom B Brown, B., Mann, N., Ryder, M., Subbiah, J., Kaplan, P., Dhariwal, A., Neelakantan, P., Shyam, G., and Sastry arXiv:2005.14165
Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia, L. 2017. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. In Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv:1708.00055
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186
Gao, T., Fisch, A., and Chen, D. 2020. Making pre-trained language models better few-shot learners. In Making pre-trained language models better few-shot learners. arXiv:2012.15723
Gao, T., Yao, X., and Chen, D. 2021. Simcse: Simple contrastive learning of sentence embeddings. In Simcse: Simple contrastive learning of sentence embeddings. arXiv:2104.08821
He, K., Zhang, X., Ren, S., and Sun, J. 2016. Deep residual learning for image recognition. In Proceedings of IEEE conference on computer vision and pattern recognition. pp. 770--778
He, R., Liu, L., Ye, H., Tan, Q., Ding, B., Cheng, L., Low, J., Bing, L., and Si, L. 2021. On the effectiveness of adapterbased tuning for pretrained language model adaptation. In On the effectiveness of adapterbased tuning for pretrained language model adaptation. arXiv:2106.03164
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. 2019. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning. pp. 2790--2799
Hu, M. and Liu, B. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 168--177
Lester, B., Al-Rfou, R., and Constant, N. 2021. The power of scale for parameter-efficient prompt tuning. In The power of scale for parameter-efficient prompt tuning. arXiv:2104.08691
Li, B., Zhou, H., He, J., Wang, M., Yang, Y., and Li, L. 2020. On the sentence embeddings from pre-trained language models. In On the sentence embeddings from pre-trained language models. arXiv:2011.05864
Xiang, L., Li, P., and Liang 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Prefix-tuning: Optimizing continuous prompts for generation. arXiv:2101.00190
Liu, X., Ji, K., Fu, Y., and Du, Z. Zhilin Yang, and Jie Tang. 2021a. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. In Zhilin Yang, and Jie Tang. 2021a. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv:2110.07602
Liu, X., Zheng, Y., Du, Z., Ding, M., and Qian, Y. Zhilin Yang, and Jie Tang. 2021b. Gpt understands, too. In Zhilin Yang, and Jie Tang. 2021b. Gpt understands, too. arXiv:2103.10385
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692
Rabeeh Karimi Mahabadi, J., Henderson, S., and Ruder 2021. Compacter: Efficient lowrank hypercomplex adapter layers. In Compacter: Efficient lowrank hypercomplex adapter layers. arXiv:2106.04647
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations.
Pang, B. and Lee, L. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. cs/0409058
Pang, B. and Lee, L. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. cs/0506075
Pennington, J., Socher, R., and Manning, C.D. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). pp. 1532--1543
Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K., and Gurevych, I. 2020. Adapterfusion: Non-destructive task composition for transfer learning. In Adapterfusion: Non-destructive task composition for transfer learning. arXiv:2005.00247
Pfeiffer, J. and Vulić, I. 2020. Mad-x: An adapter-based framework for multi-task cross-lingual transfer. In Mad-x: An adapter-based framework for multi-task cross-lingual transfer. arXiv:2005.00052
Phang, J., Févry, T., and Bowman, S.R. 2018. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. In Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. arXiv:1811.01088
Reimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv:1908.10084
Schick, T. and Schütze, H. 2020. Exploiting cloze questions for few shot text classification and natural language inference. In Exploiting cloze questions for few shot text classification and natural language inference. arXiv:2001.07676
Shin, T., Razeghi, Y., Robert L Logan, I. V., Wallace, E., and Singh, S. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv:2010.15980
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.R. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv:1804.07461
Wang, S., Fang, H., Khabsa, M., Mao, H., and Ma, H. 2021. Entailment as few-shot learner. In Entailment as few-shot learner. arXiv:2104.14690
Wiebe, J., Wilson, T., and Cardie, C. 2005. Annotating expressions of opinions and emotions in language. Language resources and evaluation. In Annotating expressions of opinions and emotions in language. Language resources and evaluation. pp. 165--210
Zhang, A., Tay, Y., Zhang, S., Chan, A., Luu, A. T., Hui, S. C., and Fu, J. 2021. Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters. In Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters. arXiv:2102.08597