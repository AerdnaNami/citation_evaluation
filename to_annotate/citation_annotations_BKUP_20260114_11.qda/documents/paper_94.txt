Related Work

Task-Oriented Dialogue. Task-oriented dialogue aims at accomplishing user's goal. Traditional systems (Williams and Young, 2007;Young et al., 2013) adopt a pipelined approach that requires dialogue state tracking for understanding user's goal, dialogue policy learning for deciding which system action to take, and natural language generation for generating dialogue responses.

Recently, to simplify the modelling effort, researchers have shifted their attention to building neural network models that address the TOD subtasks Eric et al., 2017;Lei et al., 2018;Liang et al., 2020). With the advances in pretrained language models (PLMs), Budzianowski and Vulić (2019) first applied the GPT-2 model for the NLG task. Lin et al. (2020) and  moved one step forward and utilized pretrained language models to solve all TOD sub-tasks conditioned on the history of oracle belief states. Based on the GPT-2 model, Hosseini-Asl et al. (2020) proposed a cascaded model, SimpleTOD, that addresses all TOD sub-tasks without using the oracle information. To improve the system performance, Peng et al. (2021) and Liu et al. (2021) applied dialogue pre-training over external dialogue corpora. However, both methods require the pretraining data to be fully annotated for all TOD sub-tasks (i.e., DST, POL, and NLG) which greatly limits the amount of data they can use. Additionally, Liu et al. (2021) achieved better results with noisy chanel model that requires two additional language models for outputs re-scoring. Unlike their approach, we address the task of task-oriented dialogue with a single unified model.

Language Model Pre-training. The research community has witnessed remarkable progress of pre-training methods in a wide range of NLP tasks, including language understanding (Peters et al., 2018;Devlin et al., 2019;Yang et al., 2019) and text generation (Radford et al., 2019;Lewis et al., 2020;Raffel et al., 2020).

In the dialogue domain, many models are pretrained on open-domain conversational data like Reddit. Based on GPT-2, Transfertransfo (Wolf et al., 2019b) achieves good results on ConvAI-2 competition. As another extension of GPT-2, Di-aloGPT (Zhang et al., 2020c) performs well in generating open-domain dialogue response. ConveRT ) is a language model with dual-encoder built for the task of response selection. PLATO (Bao et al., 2020) pre-trains a model with discrete latent variable structure for the response generation task.  adapts BERT with TOD pre-training and achieves strong performances on four dialogue understanding tasks.

Pre-training on Supplementary Data. Recent work (Phang et al., 2018;Aghajanyan et al., 2021) found that supplementary training on the tasks with intermediate-labelled data improves the performance of the fine-tuned models on GLUE natural language understanding benchmark (Wang et al., 2018). Our work studies a similar supplementary training setup with intermediate-labelled data for task-oriented dialogue systems. Unlike previous work, we use a single multi-task model for all relevant sub-tasks in task-oriented dialogue systems.

 References: 
Byrne, B., Krishnamoorthi, K., Sankar, C., Neelakantan, A., Goodrich, B., Duckworth, D., Yavuz, S., Dubey, A., Kim, K., and Cedilnik, A. 2019. Taskmaster-1: Toward a realistic and diverse dialog dataset. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 4515--4524 10.18653/v1/D19-1459
Casanueva, I., Temcinas, T., Gerz, D., Henderson, M., and Vulic, I. 2003. Efficient intent detection with dual sentence encoders. CoRR, abs. In Efficient intent detection with dual sentence encoders. CoRR, abs.
Chen, L., Lv, B., Wang, C., Zhu, S., Tan, B., and Yu, K. 2020. Schema-guided multi-domain dialogue state tracking with graph attention neural networks. In The Thirty-Second Innovative Applications of Artificial Intelligence Conference. pp. 7521--7528
Coucke, A., Saade, A., Ball, A., Bluche, T., Caulier, A., Leroy, D., Doumouro, C., Gisselbrecht, T., Caltagirone, F., Lavril, T., Primet, M., and Dureau, J. 2018. Snips voice platform: an embedded spoken language understanding system for privateby-design voice interfaces. In Snips voice platform: an embedded spoken language understanding system for privateby-design voice interfaces.
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423
Asri, L. E., Schulz, H., Sharma, S., Zumer, J., Harris, J., Fine, E., Mehrotra, R., and Suleman, K. 2017. Frames: a corpus for adding memory to goal-oriented dialogue systems. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue. pp. 207--219 10.18653/v1/W17-5526
Eric, M., Goel, R., Paul, S., Sethi, A., Agarwal, S., Gao, S., Kumar, A., Goyal, A. K., Ku, P., and Hakkani-Tür, D. 2020. Multiwoz 2.1: A consolidated multidomain dialogue dataset with state corrections and state tracking baselines. In Proceedings of The 12th Language Resources and Evaluation Conference. pp. 422--428
Eric, M., Krishnan, L., Charette, F., and Manning, C. D. 2017. Key-value retrieval networks for task-oriented dialogue. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue. pp. 37--49 10.18653/v1/W17-5506
Feng, Y., Wang, Y., and Li, H. 2021. A sequenceto-sequence approach to dialogue state tracking. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 1714--1725 10.18653/v1/2021.acl-long.135
Fleiss, J. L. 1971. Measuring nominal scale agreement among many raters. In Psychological Bulletin. pp. 378--382
Gao, S., Sethi, A., Agarwal, S., Chung, T., and Hakkani-Tür, D. 2019. Dialog state tracking: A neural reading comprehension approach. In Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue. pp. 264--273 10.18653/v1/W19-5932
Heck, M., Carel Van Niekerk, N., Lubis, C., Geishauser, H., Lin, M., Moresi, M., and Gasic 2020. Trippy: A triple copy strategy for value independent neural dialog state tracking. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 35--44
Henderson, M., Casanueva, I., Mrksic, N., Su, P., Wen, T., and Vulic, I. 2020. Convert: Efficient and accurate conversational representations from transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020. pp. 2161--2174 10.18653/v1/2020.findings-emnlp.196
Hosseini-Asl, E., Mccann, B., Wu, C., Yavuz, S., and Socher, R. 2020. A simple language model for task-oriented dialogue. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.
Nitish Shirish Keskar, B., Mccann, C., Xiong, R., and Socher 1904. Unifying question answering and text classification via span extraction. CoRR, abs. In Unifying question answering and text classification via span extraction. CoRR, abs.
Kim, S., Galley, M., Gunasekara, R. C., Lee, S., Atkinson, A., Peng, B., Schulz, H., Gao, J., Li, J., Adada, M., Huang, M., Lastras, L. A., Kummerfeld, J. K., Lasecki, W. S., and Hori, C. Anoop Cherian, Tim K. Marks, Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, and Raghav Gupta. 2019. The eighth dialog system technology challenge. In Anoop Cherian, Tim K. Marks, Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, and Raghav Gupta. 2019. The eighth dialog system technology challenge.
Kim, S., Yang, S., Kim, G., and Lee, S. 2020. Efficient dialogue state tracking by selectively overwriting memory. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 567--582 10.18653/v1/2020.acl-main.53
Diederik, P., Kingma, J., and Ba 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations.
Larson, S., Mahendran, A., Peper, J. J., Clarke, C., Lee, A., Hill, P., Kummerfeld, J. K., Leach, K., Laurenzano, M. A., Tang, L., and Mars, J. 2019. An evaluation dataset for intent classification and out-of-scope prediction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 1311--1316 10.18653/v1/D19-1131
Lee, H., Lee, J., and Kim, T. 2019. SUMBT: slot-utterance matching for universal and scalable belief tracking. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. pp. 5478--5483 10.18653/v1/p19-1546
Lee, S., Schulz, H., Atkinson, A., Gao, J., Suleman, K., Asri, L. E., Adada, M., Huang, M., Sharma, S., Tay, W., and Li, X. 2019. Multi-domain taskcompletion dialog challenge. In Dialog System Technology Challenges.
Lei, W., Jin, X., Kan, M., Ren, Z., He, X., and Yin, D. 2018. Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 1437--1447 10.18653/v1/P18-1133
Lewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703
Li, X., Chen, Y., Li, L., Gao, J., and Celikyilmaz, A. 2017. Investigation of language understanding impact for reinforcement learning based dialogue systems. In CoRR. abs/1703.07055
Li, X., Panda, S., Jj (jingjing, )., Liu, J., and Gao 2018. Microsoft dialogue challenge: Building Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, and Pascale Fung. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 3391--3405 10.18653/v1/2020.emnlp-main.273
Liu, B. and Lane, I. R. 2018. End-to-end learning of task-oriented dialogs. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT 2018. pp. 67--73 10.18653/v1/n18-4010
Liu, Q., Yu, L., Rimell, L., and Blunsom, P. 2021. Pretraining the noisy channel model for taskoriented dialogue. In Trans. Assoc. Comput. Linguistics. pp. 657--674
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.
Mccann, B., Shirish Keskar, N., Xiong, C., and Socher, R. 2018. The natural language decathlon: Multitask learning as question answering. In The natural language decathlon: Multitask learning as question answering. abs/1806.08730
Mehri, S., Srinivasan, T., and Eskénazi, M. 2019. Structured fusion networks for dialog. In Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue. pp. 165--177 10.18653/v1/W19-5921
Mrkšić, N., Diarmuid, Ó., Séaghdha, T., Wen, B., Thomson, S., and Young 2017. Neural belief tracker: Data-driven dialogue state tracking. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 1777--1788 10.18653/v1/P17-1163
Nouri, E. and Hosseini-Asl, E. 2018. Toward scalable neural dialogue state tracking model. In Toward scalable neural dialogue state tracking model. abs/1812.00899
Papineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135
Peng, B., Li, C., Li, J., Shayandeh, S., Liden, L., and Gao, J. 2021. Soloist: Building task bots at scale with transfer learning and machine teaching. In Transactions of the Association for Computational Linguistics.
Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/n18-1202
Phang, J., Févry, T., and Bowman, S. R. 2018. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. In Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. abs/1811.01088
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In Language models are unsupervised multitask learners.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67
Rastogi, A., Zang, X., Sunkara, S., Gupta, R., and Khaitan, P. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In The Thirty-Second Innovative Applications of Artificial Intelligence Conference. pp. 8689--8696
Ren, L., Ni, J., and Mcauley, J. J. 2019. Scalable and accurate dialogue state tracking via hierarchical sequence generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 1876--1885 10.18653/v1/D19-1196
Santra, B., Anusha, P., and Goyal, P. 2021. Hierarchical transformer for task oriented dialog systems. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 5649--5658
Shan, Y., Li, Z., Zhang, J., Meng, F., Feng, Y., Niu, C., and Zhou, J. 2020. A contextual hierarchical attention network with adaptive objective for dialogue state tracking. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 6322--6333 10.18653/v1/2020.acl-main.563
Shu, L., Molino, P., Namazifar, M., Xu, H., Liu, B., Zheng, H., and Tür, G. 2019. Flexibly-structured model for task-oriented dialogues. In Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue. pp. 178--187 10.18653/v1/W19-5922
Smith, R. W. and Hipp, D. R. 1995. Spoken Natural Language Dialog Systems: A Practical Approach. In Spoken Natural Language Dialog Systems: A Practical Approach.
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In GLUE: A multi-task benchmark and analysis platform for natural language understanding. abs/1804.07461
Tsung-Hsien Wen, D., Vandyke, N., Mrkšić, M., Gašić, L. M., Rojas-Barahona, P., Su, S., Ultes, S., and Young 2017. A networkbased end-to-end trainable task-oriented dialogue system. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics. pp. 438--449
Williams, J. D. and Young, S. J. 2007. Partially observable markov decision processes for spoken dialog systems. In Comput. Speech Lang. pp. 393--422 10.1016/j.csl.2006.06.008
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., and Louf, R. 1910. Huggingface's transformers: State-of-the-art natural language processing. In Huggingface's transformers: State-of-the-art natural language processing.
Wolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. abs/1901.08149
Wu, C., Steven, C. H., Hoi, R., Socher, C., and Xiong 2020. TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 917--929 10.18653/v1/2020.emnlp-main.66
Wu, C., Madotto, A., Hosseini-Asl, E., Xiong, C., Socher, R., and Fung, P. 2019. Transferable multi-domain state generator for task-oriented dialogue systems. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. pp. 808--819 10.18653/v1/p19-1078
Yang, Y., Cer, D., Ahmad, A., Guo, M., Law, J., Constant, N., Ábrego, G. H., Yuan, S., Tar, C., Sung, Y., Strope, B., and Kurzweil, R. 2020. Multilingual universal sentence encoder for semantic retrieval. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. pp. 87--94 10.18653/v1/2020.acl-demos.12
Yang, Y., Li, Y., and Quan, X. 2021. UBAR: towards fully end-to-end task-oriented dialog system with GPT-2. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021. pp. 14230--14238
Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 5754--5764
Young, S. J., Gasic, M., Thomson, B., and Williams, J. D. 2013. Pomdp-based statistical spoken dialog systems: A review. In Proc. IEEE. pp. 1160--1179 10.1109/JPROC.2012.2225812
Zhang, J., Hashimoto, K., Wu, C., Wan, Y., Yu, P. S., Socher, R., and Xiong, C. 2019. Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking. In Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking. abs/1910.03544
Zhang, Y., Ou, Z., Hu, M., and Feng, J. 2020. A probabilistic end-to-end task-oriented dialog model with latent belief states towards semisupervised learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 9207--9219 10.18653/v1/2020.emnlp-main.740
Zhang, Y., Ou, Z., and Yu, Z. 2020. Taskoriented dialog systems that consider multiple appropriate responses under the same context. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. pp. 9604--9611
Zhang, Y., Sun, S., Galley, M., Chen, Y., Brockett, C., Gao, X., Gao, J., Liu, J., and Dolan, B. 2020. DIALOGPT : Largescale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020, Online. pp. 270--278 10.18653/v1/2020.acl-demos.30
Zhong, V., Xiong, C., and Socher, R. 2018. Global-locally self-attentive encoder for dialogue state tracking. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 1458--1467 10.18653/v1/P18-1135
Zhou, J., Wu, H., Lin, Z., Li, G., and Zhang, Y. 2021. Dialogue state tracking with multi-level fusion of predicted dialogue states and conversations. In Dialogue state tracking with multi-level fusion of predicted dialogue states and conversations. abs/2107.05168
Zhou, L. and Small, K. 2019. Multi-domain dialogue state tracking as dynamic knowledge graph enhanced question answering. CoRR, abs/1911.06192. Model 1% of training data 5% of training data 10% of training data 20% of training data. In Multi-domain dialogue state tracking as dynamic knowledge graph enhanced question answering. CoRR, abs/1911.06192. Model 1% of training data 5% of training data 10% of training data 20% of training data.
In Inform Succ. BLEU Comb. Inform Succ. BLEU Comb. Inform Succ. BLEU Comb. Inform Succ. BLEU Comb.