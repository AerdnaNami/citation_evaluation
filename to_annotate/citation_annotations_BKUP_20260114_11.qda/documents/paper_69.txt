Related work

Uncertainty estimation in deep learning is a topic that has been studied extensively. Bayesian deep learning includes a family of methods that attempt to capture the notion of uncertainty in deep neural networks. Such methods have gained increased popularity in the deep learning literature and there exist multiple applications in subfields such as Computer Vision (Kendall and Gal, 2017;Litjens et al., 2017; and Natural Language Processing (NLP) (Siddhant and Lipton, 2020;Lyu et al., 2020;.

Despite their obvious advantage of modeling uncertainty, the main problem with Bayesian deep learning methods is the computational cost of full Bayesian inference. To tackle this problem, Gal and Ghahramani (2016) propose using standard dropout (Srivastava et al., 2014) as a practical approximation of Bayesian inference in deep neural networks and call this method Monte Carlo dropout.  use a convolutional neural network with Monte Carlo dropout in order to obtain an uncertainty estimate for active learning in the task of image classification. Houlsby et al. (2011) sample many networks with Monte Carlo simulation and propose an objective function that takes into account the disagreement and confidence of the predictions coming from these networks.

Similar methods have also been applied to NLP. In machine translation,  extend the Transformer architecture with MC dropout to get a Variational Transformer, and use it to sample multiple translations from the approximate posterior distribution. They also introduce BLEUVar, an uncertainty metric based on the BLEU score (Papineni et al., 2002) between pairs of the generated translations. Lyu et al. (2020) extend the work of  to question answering and propose an active learning approach based on a modified BLEUVar version. Similarly,  use a conditional random field to obtain uncertainty estimates for active learning and apply their method to named entity recognition.

Although summarization is a prominent NLP task, summarization uncertainty has not been widely studied.  is the only work that focuses on uncertainty for summarization, but their work does not make use of Bayesian methods. They define a generated summary's uncertainty based on the entropy of each token generated by the model during the decoding phase. Their study includes experiments on CNN/DM and XSum using the PEGASUS and BART summarization models. Their main focus is on understanding different properties of uncertainty during the decoding phase, and their work is not directly comparable to ours.

 References: 
L., Dong, N., Yang, W., Wang, F., Wei, X., Liu, Y., Wang, J., Gao, M., Zhou, H., and Hon 2019. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems. pp. 13042--13054
Filos, A., Farquhar, S., Gomez, A. N., Rudner, T. G.J., Kenton, Z., Smith, L., and Alizadeh, M. Arnoud De Kroon, and Yarin Gal. 2019. A systematic comparison of Bayesian deep learning robustness in diabetic retinopathy tasks. In Arnoud De Kroon, and Yarin Gal. 2019. A systematic comparison of Bayesian deep learning robustness in diabetic retinopathy tasks.
Gal, Y. and Ghahramani, Z. 2016. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In 33rd International Conference on Machine Learning, ICML 2016. pp. 1050--1059
Gal, Y., Islam, R., and Ghahramani, Z. 2017. Deep Bayesian active learning with image data. In 34th International Conference on Machine Learning. pp. 1183--1192 10.17863/CAM.11070
Moritz Hermann, K., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems. pp. 1693--1701
Houlsby, N. and Huszar, F. 2011. Bayesian active learning for classification and preference learning. In Bayesian active learning for classification and preference learning.
Kendall, A. and Gal, Y. 2017. What uncertainties do we need in Bayesian deep learning for computer vision?. In Advances in Neural Information Processing Systems. pp. 5580--5590
Klimt, B. and Yang, Y. 2004. The enron corpus: A new dataset for email classification research. In Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science). pp. 217--226 10.1007/978-3-540-30115-8{_}22
Kryściński, W., Mccann, B., Xiong, C., and Socher, R. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 9332--9346 10.18653/v1/2020.emnlp-main.750
Lewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703
Litjens, G., Kooi, T., Bejnordi, B. E., Setio, A.A.A., Ciompi, F., Ghafoorian, M., Jeroen, A. W.M., and Van Der Laak Bram van Ginneken, and Clara I. Sánchez. 2017. A survey on deep learning in medical image analysis. In Bram van Ginneken, and Clara I. Sánchez. 2017. A survey on deep learning in medical image analysis. 10.1016/j.media.2017.07.005
Liu, M., Wang, Z., Tu, Z., and Xu, X. 2020. LTP: a new active learning strategy for BERT-CRF based named entity recognition. In LTP: a new active learning strategy for BERT-CRF based named entity recognition.
Lyu, Z., Duolikun, D., Dai, B., Yao, Y., Minervini, P., Xiao, T. Z., and Gal, Y. 2020. You Need Only Uncertain Answers: Data Efficient Multilingual Question Answering. In Workshop on Uncertainty and Robustness in Deep Learning.
Narayan, S., Cohen, S. B., and Lapata, M. 2018. Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 1797--1807 10.18653/v1/d18-1206
Papineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics. pp. 311--318
See, A., Peter, J., Liu, C.D., and Manning 2017. Get To The Point: Summarization with Pointer-Generator Networks. In Proceedings of the 2017 Annual Meeting of the Association for Computational Linguistics. pp. 1073--1083 10.18653/v1/P17-1099
Siddhant, A. and Lipton, Z. C. 2018. Deep Bayesian active learning for natural language processing: Results of a large-scale empirical study. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 2904--2909 10.18653/v1/d18-1318
Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.Y. 2019. MASS: Masked sequence to sequence pre-training for language generation. In Proceedings of the 2019 International Conference on Machine Learning. pp. 5926--5936
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. In Journal of Machine Learning Research. pp. 1929--1958
Xiao, T. Z., Gomez, A. N., and Gal, Y. 2020. Wat zei je? Detecting out-of-distribution translations with variational transformers. In Wat zei je? Detecting out-of-distribution translations with variational transformers.
Xu, J., Desai, S., and Durrett, G. 2020. Understanding Neural Abstractive Summarization Models via Uncertainty. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 6275--6281 10.18653/v1/2020.emnlp-main.508
Zhang, J., Zhao, Y., Saleh, M., and Liu, P.J. 2020. PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. In 37th International Conference on Machine Learning. pp. 11328--11339
Zhang, R. and Tetreault, J. 2020. This email could save your life: Introducing the task of email subject line generation. In ACL 2019 -57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. pp. 446--456 10.18653/v1/p19-1043