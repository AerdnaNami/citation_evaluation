Recent advances in vision-language modeling have achieved great success. Most of them learn joint Does the model know … It is larger than: It is smaller than: Unimodal Multimodal BERT, … Oscar, … A girl is looking at the penguin. Penguins are a group of aquatic flightless birds. The word penguin first appears in the 16th century as a name for the great auk. … A plastic penguin is sitting on a chair. … VS. what is the shape of a penguin? what are the co-occurring objects of a penguin? what is the size of a penguin? ice       beak      grass     table     water   stone what is the material of a penguin? what is the color of a penguin? Figure 1: Illustration of our main idea with input “penguin” as an example. We compare unimodal and multimodal models in terms of their ability to capture commonsense knowledge. The commonsense knowledge is evaluated by five relation types: color, shape, material, size, and visual co-occurrence. To do evaluation, we compare the model outputs with the gold distribution, which is mined from Visual Genome. image and text representations from cross-modal training of transformers with self-attention, including LXMERT (Tan and Bansal, 2019), ViLBERT (Lu et al., 2019), UNITER (Chen et al., 2020), etc. Oscar (Li et al., 2020) additionally uses object tags in images as anchor points to ease the learning of image-text alignments and VinVL (Zhang et al., 2021) presents an improved object detection model. CLIP (Radford et al., 2021) learns by predicting caption-image alignment from a large internet corpus of (image, text) pairs. While our work uses textual prompt tuning techniques, there have also been work on visual prompt engineering to enhance the performance of pretrained vision-language models. Zhou et al. (2021) model context in prompts as continuous representations and learn to optimize that context. Yao et al. (2021) develop a cross-modal prompt tuning framework that reformulates visual grounding as a fill-in-the-blank problem for both image and text.
In one of the early attempts at learning visual commonsense, Vedantam et al. (2015) measure the plausibility of a commonsense assertion in the form of (obj1, relation, obj2) based on how similar it is to known plausible assertions, using both visual scenes and accompanying text. Zellers et al. (2021) learn physical commonsense through interaction, and uses this knowledge to ground language. Frank et al. (2021) probe whether vision-language models have learned to construct cross-modal representations using both modalities via cross-modal input ablation. Note that our definition of visual commonsense differs from that of Zellers et al. (2019), where the model is required to perform commonsense reasoning based on an image. Our idea of visual commonsense is more similar to the idea of stereotypic tacit assumptions (Prince, 1978) – the propositional beliefs that humans hold about generic concepts, such as “dogs have to be walked.” Weir et al. (2020) probe neural language models for such human tacit assumptions and demonstrate the models’ success. We extend this intuition to visual concepts and explore how visual information may help language models to capture such assumptions. Zhu et al. (2020) investigate the “language prior” problem in Visual Question Answering models, where models tend to answer questions based on word frequencies in the data and ignore the image contents. In this work, we explore to what extent such language prior is correct when there is no image input.
Pretrained language models such as BERT (Devlin et al., 2019) are trained on millions of tokens of text, capturing statistical regularities present in the training corpora. However, their textual training data can suffer from reporting bias, where the frequency distribution of specific events and properties in text may not reflect the real-world distribution of such properties (Gordon and Van Durme, 2013). For example, while grass is most commonly green, this may not be reported as much in web corpora, and while motorcycle crashes may be more common in the real world, plane crashes may be mentioned far more in news text. Misra et al. (2016) shows that “human-centric” image annotations contain reporting bias as well and that the noise in annotations exhibits structure and can be modeled.

 References: 
Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe Morency. 2019. Multimodal machine learning: A survey and taxonomy. IEEE Trans. Pattern Anal. Mach. Intell., 41(2):423–443. Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. Experience grounds language. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8718–8735, Online. Association for Computational Linguistics. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. UNITER: Universal image-text representation learning. In European Conference on Computer Vision (ECCV). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics. Stella Frank, Emanuele Bugliarello, and Desmond Elliott. 2021. Vision-and-language or vision-forlanguage? on cross-modal influence in multimodal transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP). Jonathan Gordon and Benjamin Van Durme. 2013. Reporting bias and knowledge acquisition. In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction. Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.
2015. Distilling the knowledge in a neural network. ArXiv, abs/1503.02531. Drew A. Hudson and Christopher D. Manning. 2019. GQA: A new dataset for compositional question answering over real-world images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (ICCV), page 6700–6709. Joo-Kyung Kim and Marie-Catherine de Marneffe.
2013. Deriving adjectival scales from continuous space word representations. In Proceedings of the
2013 Conference on Empirical Methods in Natural Language Processing, pages 1625–1630, Seattle, Washington, USA. Association for Computational Linguistics. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. 2016. Visual Genome: Connecting language and vision using crowdsourced dense image annotations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2020. ALBERT: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations (ICLR). Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. 2020. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision (ECCV). Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common objects in context. In European Conference on Computer Vision (ECCV). Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
2019. ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Annual Conference on Neural Information Processing Systems (NeurIPS). Ishan Misra, C. Lawrence Zitnick, Margaret Mitchell, and Ross Girshick. 2016. Seeing through the human reporting bias: Visual classifiers from noisy humancentric labels. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2930–2939. Cory Paik, Stéphane Aroca-Ouellette, Alessandro Roncone, and Katharina Kann. 2021. The world of an octopus: How reporting bias influences a language model’s perception of color. In Proceedings of the
2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 823–835, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Jeroen van Paridon, Qiawen Liu, and Gary Lupyan.
2021. How do blind people know that blue is cold? distributional semantics encode color-adjective associations. In Proceedings of the Annual Meeting of the Cognitive Science Society. Ellen F. Prince. 1978. On the function of existential presupposition in discourse. In Chicago Linguistic Society (Vol. 14, pp. 362–376). Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, page 5203–5212, Online. Association for Computational Linguistics. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML). Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108. Hao Tan and Mohit Bansal. 2019. LXMERT: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP). Hao Tan and Mohit Bansal. 2020. V okenization: Improving language understanding with contextualized, visual-grounded supervision. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2066–2080, Online. Association for Computational Linguistics. Ramakrishna Vedantam, Xiaoyu Lin, Tanmay Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. Learning common sense through visual abstraction. In
2015 IEEE International Conference on Computer Vision (ICCV), pages 2542–2550. Nathaniel Weir, Adam Poliak, and Benjamin Van Durme. 2020. On the existence of tacit assumptions in contextualized language models. Proceedings of the Annual Meeting of the Cognitive Science Society. Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. 2021. CPT: colorful prompt tuning for pre-trained vision-language models. ArXiv, abs/2109.11797. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. In Transactions of the Association for Computational Linguistics, volume 2, pages 67–78. Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Visual commonsense reasoning. In 2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Rowan Zellers, Ari Holtzman, Matthew Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi, and Yejin Choi. 2021. PIGLeT: Language grounding through neuro-symbolic interaction in a 3D world. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2040–2050. Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. 2021. VinVL: Making visual representations matter in vision-language models. In 2021 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2021. Learning to prompt for visionlanguage models. arXiv preprint arXiv:2109.01134. Xi Zhu, Zhendong Mao, Chunxiao Liu, Peng Zhang, Bin Wang, and Yongdong Zhang. 2020. Overcoming language priors with self-supervised learning for visual question answering. In International Joint Conference on Artificial Intelligence (IJCAI), page 1083–1089.
A Appendix A.1 List of Objects Table 7 shows the list of all possible attributes for relations color, shape, and material. Table 8 shows the list of objects in the five categories of relation size. Visual co-ocurrence has a large number of objects that are not listed here for space reasons. Relation Classes Color black, blue (aqua, azure, cyan, indigo, navy), brown (khaki, tan), gray (grey), green (turquoise), orange (amber), pink (magenta), purple (lavender, violet), red (burgundy, crimson, maroon, scarlet), silver, white (beige), yellow (blond, gold, golden) Shape cross, heart, octagon, oval, polygon (heptagon, hexagon, pentagon), rectangle, rhombus (diamond), round (circle), semicircle, square, star, triangle Material bronze (copper), ceramic, cloth, concrete, cotton, denim, glass, gold, iron, jade, leather, metal, paper, plastic, rubber, stone (cobblestone, slate), tin (pewter), wood (wooden) Table 7: List of all objects for relation color, shape, and material. Inside the parentheses are the attributes that are grouped into the object class. Size Objects Tiny ant, leaf, earring, candle, lip, ear, eye, nose, pebble, shrimp, pendant, spoon, dirt, pill, bee Small bird, tomato, pizza, purse, bowl, cup, mug, tape, plate, potato, bottle, faucet, pot, knob, dish, book, laptop, menu, flower, pillow, clock, teapot, lobster, duck, balloon, helmet, hand, face, lemon, microphone, foot, towel, shoe Medium human, door, dog, cat, window, lamp, chair, tire, tv, table, desk, sink, guitar, bicycle, umbrella, printer, scooter, pumpkin, monitor, bag, coat, vase, deer, horse, kite Large elephant, car, tree, suv, pillar, stairway, bed, minivan, fireplace, bus, boat, cheetah, wall, balcony, bear, lion Huge building, airplane, plane, clocktower, tower, earth, pool, mountain, sky, road, house, hotel, tank, town, city, dinasour, whale, school Table 8: List of objects in five size categories. A.2 Additional Probing Best template mode Table 9 contains zero-shot results under the “best template” mode, for BERT (base), Oscar (base), BERT distilled from Oscar, RoBERTa (base), ALBERT (base), and V okenization. These results demonstrate similar trends as the ones in the “average template” mode. purpleorange blue red graygreenyellowsilverbrownblackwhitepink 0.0 0.5Spearmanr color star triangle oval heart rhombussquare round rectangle 0.5 0.0 0.5 Spearmanr shape bronzeclothdenimpaperleatherceramicconcrete gold cottonstoneplasticwoodmetalglassrubber 0.0 0.5Spearmanr material bert clip Figure 4: Spearman correlation per object class for BERT and CLIP with the logistic regression head, for color, shape, and material. The error margins are the standard deviations. Per-object analysis Fig. 4 illustrates the finegrained Spearman correlation ± standard deviation per object group for BERT and CLIP. 0.100 0.075 0.050 0.025 0.000 0.025 0.050 0.075 bert cos_sim 0.100 0.075 0.050 0.025 0.000 0.025 0.050 0.075 oscar cos_sim tiny small medium large huge Figure 5: The size projection scores from BERT and Oscar, where each point is one object. Cosine similarities are correlated between Oscar and BERT. Size per-object Fig. 5 shows how the per-object projection scores on the size spectrum from BERT and Oscar are correlated. Per-Subject Comparison Fig. 6 and Fig. 7 show how the Spearman correlations of 10 individual subjects improve after soft prompt tuning and after multimodal pretraining. Consistent improvement Color Shape Material Cooccur Model Spearman ρ Acc@1 Spearman ρ Acc@1 Spearman ρ Acc@1 Spearman ρ BERTb 47.5 ± 21.6 41.8 48.2 ± 12.0 64.3 41.9 ± 15.4 55.3 6.1 ± 4.0 Oscarb 50.0 ± 19.8 59.8 52.7 ± 10.0 89.3 46.5 ± 13.7 74.6 10.1 ± 7.2 Distilled 53.7 ± 21.3 57.7 51.4 ± 11.1 74.3 46.0 ± 13.6 74.6 10.4 ± 7.8 RoBERTab 44.8 ± 19.8 41.6 45.4 ± 12.4 69.3 33.0 ± 15.5 39.1 1.1 ± 1.4 ALBERTb 20.2 ± 24.8 13.4 29.8 ± 15.7 13.6 25.0 ± 17.9 27.8 6.6 ± 5.1 V okenization 47.6 ± 20.9 51.6 49.8 ± 13.1 72.9 39.4 ± 16.0 52.5 6.0 ± 3.7 Table 9: Spearman correlation and top-1 accuracy (both × 100) of zero shot probing. This is the “best template” case discussed in Section 4.1. can be seen in color, material, and cooccurrence. Although we report average Spearman correlations in Table 3 and there are large standard deviations, here we show that when improvement is observed collectively, it is also consistent across subjects. With shape, the improvement is less obvious (45.9 to 50.4 for prompt tuning and 49.2 to 50.4 for multimodal pretraining). A.3 Error Analysis Data The three subjects with the highest and lowest Spearman correlation are shown in Fig. 8 and Fig. 9. Wikipedia Table 10 shows the number of (noun, attribute) pairs of the three relation types in Wikipedia. Shape has fewer occurrences than material and color. Color Shape Material Total 331480 195921 307879 Avg 12 27623.3 16326.8 24634.7 Table 10: First row is the total number of occurrences of (noun, attribute) pairs for relations shape, material, and color in Wikipedia. Second row is the average number of occurrences across the top 12 attributes for each relation. Shape has the fewest number of occurrences. Model Table 11 shows the errors made by BERT and Oscar in the “average template” mode before prompt tuning. Overall, subjects with low correlation are those that are less often reported in Visual Genome as well as in textual data. A.4 Resources BERT, RoBERTa, ALBERT We use the Huggingface implementations of BERT, RoBERTa, and ALBERT. Oscar See the GitHub repository for the code and pretrained Oscar: https://github.com/ microsoft/Oscar. CLIP We use the CLIP model released by OpenAI: https://github.com/openai/ CLIP. Vokenization See the GitHub repository for the pretrained model: https://github.com/ airsplay/vokenization. False True 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0 Spearman correlation color False True 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 shape False True 0.4 0.5 0.6 0.7 0.8 0.9 material False True 0.10 0.15 0.20 0.25 0.30 0.35 0.40 cooccur Figure 6: Spearman correlation of 10 subjects for each relation type before and after soft prompt tuning, with Oscar (base). Almost all individual subject has increased correlation after prompt tuning, except in relation shape. bert-base oscar-base 0.6 0.7 0.8 0.9Spearman correlation color bert-base oscar-base 0.4 0.5 0.6 0.7 shape bert-base oscar-base 0.4 0.5 0.6 0.7 0.8 0.9 material bert-base oscar-base 0.10 0.15 0.20 0.25 0.30 0.35 0.40 cooccur Figure 7: Spearman correlation of 10 subjects for each relation type with BERT (base) and Oscar (base), after soft prompt tuning. Almost all individual subject has higher correlation with Oscar than with BERT, except in relation shape. High Corr Subjs Low Corr Subjs Relation BERTb Oscarb BERTb Oscarb Color lace, jacket, design balloon, jacket, apple flush, water faucet, muffler hinge, leg, slack Shape mirror, vase, container chair, pizza, vase connector, log, knot banana, toast, phone Material wall, tray, board fence, wall, shelf sheep, fabric, patch elephant, rug, patch Table 11: Three subjects each with high and low correlations for relations color, shape, and material. 0.00 0.25 0.50 0.75 1.00probability high correlation CoDa 'pretzel' VG 'pretzel' CoDa 'tangerine' VG 'tangerine' CoDa 'cherry' VG 'cherry'
1 2 3 4 5 6 7 8 9 10 11 colors 0.00 0.25 0.50 0.75 1.00probability low correlation CoDa 'gondola' VG 'gondola' CoDa 'canoe' VG 'canoe' CoDa 'butterfly' VG 'butterfly' Figure 8: VG vs. CoDa distribution of 3 subjects with the lowest and highest correlation, ordered by probability of colors in CoDa. 0.0 0.2 0.4probability high correlation CoDa 'water' VG 'water' CoDa 'rice' VG 'rice' CoDa 'rose' VG 'rose'
1 2 3 4 5 6 7 8 9 10 11 colors 0.00 0.25 0.50 0.75 1.00probability low correlation CoDa 'drawer' VG 'drawer' CoDa 'penguin' VG 'penguin' CoDa 'toilet' VG 'toilet' Figure 9: Wikipedia vs. CoDa distribution of 3 subjects with the lowest and highest correlation, ordered by probability of colors in CoDa.