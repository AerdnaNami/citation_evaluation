Background and Related Work

We work with the eight corpora covering six tasks summarized below and exemplified in Table 2. We select two corpora for question answering: CommonsenseQA (Talmor et al., 2019) and COPA (Roemmele et al., 2011). CommonsenseQA consists of multi-choice questions (5 candidate answers) that require some degree of commonsense. COPA presents a premise (e.g., The man broke his toe) and a question (e.g., What was the cause of this?) and the system must choose between two plausible alternatives (e.g. He got a hole in his sock or He dropped a hammer on his foot).

For textual similarity and paraphrasing, we select QQP 2 and STS-B (Cer et al., 2017). QQP consists of pairs of questions and the task is to determine whether they are paraphrases. STS-B consists of pairs of texts and the task is to determine how semantically similar they are with a score from 0 to 5.

We select one corpus for the remaining tasks. For inference, we work with QNLI (Rajpurkar et al., 2016), which consists in determining whether a text is a valid answer to a question. We use WiC (Pilehvar and Camacho-Collados, 2019) for word sense disambiguation. WiC consists in determining whether two instances of the same word (in two sentences; italicized in Table 2) are used with the same meaning. For coreference resolution, we choose WSC (Levesque et al., 2012), which consists in determining whether a pronoun and a noun phrase are co-referential (italicized in Table 2). Finally, we work with SST-2 (Socher et al., 2013) for sentiment analysis. The task consists in determining whether a sentence from a collection of movie reviews has positive or negative sentiment.

For convenience, we work with the formatted versions of these corpora in the GLUE (Wang et al., 2018) and SuperGLUE  benchmarks. The only exception is CommonsenseQA, which is not part of these benchmarks. Related Work Previous work has shown that SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) have annotation artifacts (e.g., negation is a strong indicator of contradictions) (Gururangan et al., 2018). The literature has also shown that simple adversarial attacks including negation cues are very effective (Naik et al., 2018;Wallace et al., 2019). Kovatchev et al. (2019) analyze 11 paraphrasing systems and show that they obtain substantially worse results when negation is present.

More recently, Ribeiro et al. (2020) show that negation is one of the linguistic phenomena commercial sentiment analysis struggle with. Several previous works have investigated the (lack of) ability of transformers to make inferences when negation is present. For example, Ettinger (2020) conclude that BERT is unable to complete sentences when negation is present. BERT also faces challenges solving the task of natural language inference (i.e., identifying entailments and contradictions) with monotonicity and negation (Geiger et al., 2020;Yanaka et al., 2019). Warstadt et al. (2019) show the limitations of BERT making acceptability judgments with sentences that contain negative polarity items. Most related to out work, Hossain et al. ( 2020) analyze the role of negation in three natural language inference corpora: RTE Bar-Haim et al., 2006;Giampiccolo et al., 2007;Bentivogli et al., 2009), SNLI and MNLI. In this paper, we present a similar analysis, but we move beyond natural language inference and work with eight corpora spanning six natural language understanding tasks.

 References: 
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magnini, B., and Szpektor, I. 2006. The second pascal recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment. pp. 6--10
Bentivogli, L., Clark, P., Dagan, I., and Giampiccolo, D. 2009. The fifth pascal recognizing textual entailment challenge. In The fifth pascal recognizing textual entailment challenge.
Samuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/D15-1075
Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia, L. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). pp. 1--14 10.18653/v1/S17-2001
Ido Dagan, O., Glickman, B., and Magnini 2006. The pascal recognising textual entailment challenge. In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW'05. pp. 177--190 10.1007/11736790_9
Ettinger, A. 2020. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. In Transactions of the Association for Computational Linguistics. pp. 34--48 10.1162/tacl_a_00298
Geiger, A., Richardson, K., and Potts, C. 2020. Neural natural language inference models partially embed theories of lexical entailment and negation. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. pp. 163--173 10.18653/v1/2020.blackboxnlp-1.16
Giampiccolo, D., Magnini, B., Dagan, I., and Dolan, B. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. pp. 1--9
Suchin Gururangan, S., Swayamdipta, O., Levy, R., Schwartz, S., Bowman, N. A., and Smith 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 107--112 10.18653/v1/N18-2017
Md Mosharaf Hossain, V., Kovatchev, P., Dutta, T., Kao, E., Wei, E., and Blanco 2020. An analysis of natural language inference benchmarks through the lens of negation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 9106--9118 10.18653/v1/2020.emnlp-main.732
Khandelwal, A. and Sawant, S. 2020. Neg-BERT: A transfer learning approach for negation detection and scope resolution. In Proceedings of the 12th Language Resources and Evaluation Conference. pp. 5739--5748
Venelin Kovatchev, M. A., Marti, M., Salamo, J., and Beltran 2019. A qualitative evaluation framework for paraphrase identification. In Proceedings of the International Conference on Recent Advances in Natural Language Processing. pp. 568--577 10.26615/978-954-452-056-4_067
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. 2019. Natural questions: A benchmark for question answering research. In Transactions of the Association for Computational Linguistics. pp. 452--466 10.1162/tacl_a_00276
Levesque, H. J., Davis, E., and Morgenstern, L. 2012. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR'12. pp. 552--561
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692
Morante, R. and Daelemans, W. 2012. ConanDoyle-neg: Annotation of negation cues and their scope in conan doyle stories. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12). pp. 1563--1568
Naik, A., Ravichander, A., Sadeh, N., Rose, C., and Neubig, G. 2018. Stress test evaluation for natural language inference. In Proceedings of the 27th International Conference on Computational Linguistics. pp. 2340--2353
Phang, J., Yeres, P., Swanson, J., Liu, H., Tenney, I. F., Phu Mon Htut, C., Vania, A., Wang, S. R., and Bowman 2020. jiant 2.0: A software toolkit for research on general-purpose text understanding models. In 2020. jiant 2.0: A software toolkit for research on general-purpose text understanding models.
Taher Pilehvar, M. and Camacho-Collados, J. 2019. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1267--1273 10.18653/v1/N19-1128
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 2383--2392 10.18653/v1/D16-1264
Marco Tulio Ribeiro, T., Wu, C., Guestrin, S., and Singh 2020. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4902--4912 10.18653/v1/2020.acl-main.442
Roemmele, M., Cosmin Adrian Bejan, A.S., and Gordon 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning. pp. 90--95
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642
Stanovsky, G., Michael, J., Zettlemoyer, L., and Dagan, I. 2018. Supervised open information extraction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 885--895 10.18653/v1/N18-1081
Talmor, A., Herzig, J., Lourie, N., and Berant, J. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4149--4158 10.18653/v1/N19-1421
Wallace, E., Feng, S., Kandpal, N., Gardner, M., and Singh, S. 2019. Universal adversarial triggers for attacking and analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2153--2162 10.18653/v1/D19-1221
Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems. pp. 3261--3275
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP. pp. 353--355 10.18653/v1/W18-5446
Warstadt, A., Cao, Y., Grosu, I., Peng, W., Blix, H., Nie, Y., Alsop, A., Bordia, S., Liu, H., Parrish, A., Wang, S., Phang, J., Mohananey, A., Phu Mon Htut, P., Jeretic, S. R., and Bowman 2019. Investigating BERT's knowledge of language: Five analysis methods with NPIs. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2877--2887 10.18653/v1/D19-1286
Williams, A., Nangia, N., and Bowman, S. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1112--1122
Wu, W., Wang, F., Yuan, A., Wu, F., and Li, J. 2020. CorefQA: Coreference resolution as query-based span prediction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 6953--6963 10.18653/v1/2020.acl-main.622
Yanaka, H., Mineshima, K., Bekki, D., Inui, K., Sekine, S., Abzianidze, L., and Bos, J. 2019. HELP: A dataset for identifying shortcomings of neural models in monotonicity reasoning. In Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019). pp. 250--255 10.18653/v1/S19-1027