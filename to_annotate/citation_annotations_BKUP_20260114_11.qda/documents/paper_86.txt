Introduction

Information extraction (IE) is a task that aims to extract information of interest from text data and represent the extracted information in a structured form. Traditional IE tasks include named entity recognition which recognizes entities and their types (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Devlin et al., 2019), relation extraction which identifies the relationships between entities (Zheng et al., 2017; Zeng et al., 2018; Zhong and Chen, 2020), etc. Since the results of IE are structured, they can be easily used by computer systems in different applications such as text mining.

In this work, we study IE in a new setting, referred to as text-to-table. First, the system receives a training dataset containing text-table pairs. Each text-table pair contains a text and a table (or tables) representing information extracted from the text. The system learns a model for information extraction. Next, the system employs the learned model to conduct information extraction from a new text and outputs the result in a table (or tables). Figure 1 gives an example of text-to-table, where the input (above) is a report of a basketball game, and the output (below) is two tables summarizing the scores of the teams and players from the input.

Text-to-table is unique compared to the traditional IE approaches. First, it is mainly designed to extract structured data in a complex form from a long text. As in the example in Figure 1, extraction of information is performed from the entire document. The extracted information contains multiple types of scores of teams and players in a basketball game structured in table format. Second, the schemas for extraction are implicitly included in the training data, and there is no need to explicitly define the schemas. This reduces the need for manual efforts for schema design and annotations.

Our work is inspired by research on the so-called table-to-text (or data-to-text) problem, which is the task of generating a description for a given table . Table -to-text is useful in applications where the content of a table needs to be described in natural language. Thus, text-to-table can be regarded as an inverse problem of table-to-text. However, there are also differences. Most notably, their applications are different. Text-to-table can be applied to document summarization, text mining, etc.

In this work, we formalize text-to-table as a sequence-to-sequence (seq2seq) task. More specifically, we translate the text into a sequence representation of a table (or tables), where the schema of the table is implicitly contained in the representation. We also build the seq2seq model on top of a pre-trained language model, which is the stateof-the-art approach for seq2seq tasks (Lewis et al., 2019;Raffel et al., 2020). Although the approach is a natural application of existing technologies, as far as we know, there has been no previous study to investigate to what extent the approach works. We also develop a new method for text-to-table within the seq2seq approach with two additional techniques, table constraint and table relation embeddings. Table constraint controls the creation of rows in a table and table relation embeddings affect the alignments between cells and their row headers and column headers. Both are to make the generated table well-formulated.

The approach to IE based on seq2seq has already been proposed. Methods for conducting individual tasks of relation extraction (Zeng et al., 2018;Nayak and Ng, 2020), named entity recognition (Chen and Moschitti, 2018;Yan et al., 2021), and event extraction (Lu et al., 2021) have been developed. Methods for jointly performing multiple tasks of named entity recognition, relation extraction, and event extraction have also been devised (Paolini et al., 2021). Most of the methods exploit suitable pre-trained models such as BERT. However, all the existing methods rely on pre-defined schemas for extraction. Moreover, their models are designed to extract information from short texts, rather than long texts, and extract information with simple structures (such as an entity and its type), rather than information with complicated structures (such as a table ).

We conduct extensive experiments on four datasets. Results show that the vanilla seq2seq model fine-tuned from BART (Lewis et al., 2019) can outperform the state-of-the-art IE models finetuned from BERT (Devlin et al., 2019;Zhong and Chen, 2020). Furthermore, results show that our proposed approach to text-to-table with the two techniques can further improve the extraction accuracies. We also summarize the challenging issues with the seq2seq approach to text-to-table for future research.

Our contributions are summarized as follows:

1. We propose the new task of text-to-table for IE. We derive four new datasets for the task from existing datasets. 2. We formalize the task as a seq2seq problem and propose a new method within the seq2seq approach using the techniques of Traditionally, researchers formalize the task as a language understanding problem. The state-ofthe-art methods for NER perform the task on the basis of the pre-trained language model BERT (Devlin et al., 2019). The pipeline approach to RE divides the problem into NER and relation classification, and conducts the two sub-tasks in a sequential manner (Zhong and Chen, 2020), while the end-to-end approach jointly carries out the two sub-tasks (Zheng et al., 2017;Zeng et al., 2018;. The state-of-the-art methods for EE also employ BERT and usually jointly train the models with other tasks such as NER and RE Zhang et al., 2019;Lin et al., 2020). All the methods assume the use of pre-defined schemas (e.g., entity types for NER, entity and relation types for RE, and event templates for EE). Besides, most methods are designed for extraction from short texts. Therefore, existing methods for IE cannot be directly applied to text-to-table.

Another series of related work is open information extraction (OpenIE), which aims to extract information from texts without relying on explicitly defined schemas (Banko et al., 2007;Wu and Weld, 2010;Mausam et al., 2012;Stanovsky et al., 2018;Zhan and Zhao, 2020). However, OpenIE aims to extract information with simple structures (i.e., relation tuples) from short texts, and the methods in OpenIE cannot be directly applied to text-to-table . IE is also conducted at document level, referred to as doc-level IE. For example, some NER methods directly perform NER on a long document (Strubell et al., 2017;Luo et al., 2018), and others encode each sentence in a document, use attention to fuse document-level information, and perform NER on each sentence (Hu et al., 2020;Xu et al., 2018). There are also RE methods that predict the relationships between entities in a document (Yao et al., 2019;Nan et al., 2020a). However, existing doc-level IE approaches usually do not consider extraction of complex relations between many items.

Sequence-to-sequence (seq2seq) is the general problem of transforming one text into another text (Sutskever et al., 2014;Bahdanau et al., 2014), which includes machine translation, text summarization, etc. The use of the pre-trained language models of BART (Lewis et al., 2019) and T5 (Raffel et al., 2020) can significantly boost the performances of seq2seq, such as machine translation (Lewis et al., 2019;Raffel et al., 2020;) and text summarization (Lewis et al., 2019;Raffel et al., 2020;.

Recently, some researchers also formalize the IE problems as seq2seq, that is, transforming the input text into an internal representation. One advantage is that one can employ a single model to extract multiple types of information. Results show that this approach works better than or equally well as the traditional approach of language understanding, in RE (Zeng et al., 2018;Nayak and Ng, 2020), NER (Chen and Moschitti, 2018;Yan et al., 2021) and EE (Lu et al., 2021). Methods for jointly performing multiple tasks including NER, RE and EE have also been devised (Paolini et al., 2021).

Data-to-text aims to generate natural language descriptions from the input structured data such as sport commentaries (Wiseman et al., 2017). The structured data is usually represented as tables (Wiseman et al., 2017;Thomson et al., 2020;, sets of table cells (Parikh et al., 2020;Bao et al., 2018), semantic representations (Novikova et al., 2017), or sets of relation triples (Gardent et al., 2017;Nan et al., 2020b). The task requires the model to select the salient information from the data, organize it in a logical order, and generate an accurate and fluent natural language description (Wiseman et al., 2017). Data-to-text models usually adopt the encoder-decoder architecture. The encoders are specifically designed to model the input data, such as multi-layer perceptron (Puduppully et al., 2019a,b), recurrent neural network (Juraska et al., 2018;Shen et al., 2020), graph neural network (Marcheggiani and Perez-Beltrachini, 2018;Koncel-Kedziorski et al., 2019), or Transformer (Gong et al., 2019).

 References: 
Bahdanau, D., Cho, K., and Bengio, Y. 2014. Neural machine translation by jointly learning to align and translate. In Neural machine translation by jointly learning to align and translate. arXiv:1409.0473
Banko, M., Cafarella, M. J., Soderland, S., Broadhead, M., and Etzioni, O. 2007. Open information extraction from the web. In IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence. pp. 2670--2676
Bao, J., Tang, D., Duan, N., Yan, Z., Lv, Y., Zhou, M., and Zhao, T. 2018. Tableto-text: Describing table region with natural language. In Proceedings of the AAAI Conference on Artificial Intelligence.
Chen, L. and Moschitti, A. 2018. Learning to progressively recognize new named entities with sequence to sequence models. In Proceedings of the 27th International Conference on Computational Linguistics. pp. 2181--2191
Chen, W., Chen, J., Su, Y., Chen, Z., and Wang, W. Y. 2020. Logical natural language generation from open-domain tables. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7929--7942
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186
Gardent, C., Shimorina, A., Narayan, S., and Perez-Beltrachini, L. 2017. Creating training corpora for nlg micro-planning. In 55th annual meeting of the Association for Computational Linguistics (ACL).
Gong, L., Josep, M., Crego, J., and Senellart 2019. Enhanced transformer model for data-to-text generation. In Proceedings of the 3rd Workshop on Neural Generation and Translation. pp. 148--156
Hu, A., Dou, Z., Nie, J., and Wen, J. 2020. Leveraging multi-token entities in document-level named entity recognition. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 7961--7968
Huang, D., Cui, L., Yang, S., Bao, G., Wang, K., Xie, J., and Zhang, Y. 2020. What have we achieved on text summarization?. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 446--469
Huang, Z., Xu, W., and Yu, K. 2015. Bidirectional lstm-crf models for sequence tagging. In Bidirectional lstm-crf models for sequence tagging. arXiv:1508.01991
Juraska, J., Karagiannis, P., Bowden, K., and Walker, M. 2018. A deep ensemble model with slot alignment for sequence-to-sequence natural language generation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 152--162
Koncel-Kedziorski, R., Bekal, D., Luan, Y., Lapata, M., and Hajishirzi, H. 2019. Text generation from knowledge graphs with graph transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2284--2293
Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., and Dyer, C. 2016. Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 260--270
Lebret, R., Grangier, D., and Auli, M. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 1203--1213
Lewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461
Lin, Y., Ji, H., Huang, F., and Wu, L. 2020. A joint neural model for information extraction with global features. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7999--8009
Liu, T., Wang, K., Sha, L., Chang, B., and Sui, Z. 2018. Table-to-text generation by structure-aware seq2seq learning. In Thirty-Second AAAI Conference on Artificial Intelligence.
Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., and Zettlemoyer, L. 2020. Multilingual denoising pre-training for neural machine translation. In Transactions of the Association for Computational Linguistics. pp. 726--742
Lu, Y., Lin, H., Xu, J., Han, X., Tang, J., Li, A., Sun, L., Liao, M., and Chen, S. 2021. Text2event: Controllable sequence-tostructure generation for end-to-end event extraction. In Text2event: Controllable sequence-tostructure generation for end-to-end event extraction. arXiv:2106.09232
Luan, Y., Wadden, D., He, L., Shah, A., Ostendorf, M., and Hajishirzi, H. 2019. A general framework for information extraction using dynamic span graphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3036--3046
Luo, L., Yang, Z., Yang, P., Zhang, Y., Wang, L., Lin, H., and Wang, J. 2018. An attention-based bilstm-crf approach to documentlevel chemical named entity recognition. In Bioinformatics. pp. 1381--1388
Ma, X. and Hovy, E. 2016. End-to-end sequence labeling via bi-directional lstm-cnns-crf. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pp. 1064--1074
Marcheggiani, D. and Perez-Beltrachini, L. 2018. Deep graph convolutional encoders for structured data to text generation. In Proceedings of the 11th International Conference on Natural Language Generation. pp. 1--9
Mausam, M., Schmitz, S., Soderland, R., Bart, O., and Etzioni 2012. Open language learning for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. pp. 523--534
Nan, G., Guo, Z., Sekulic, I., and Lu, W. 2020. Reasoning with latent structure refinement for document-level relation extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 1546--1557
Nan, L., Radev, D., Zhang, R., Rau, A., Sivaprasad, A., Hsieh, C., Tang, X., Vyas, A., Verma, N., and Krishna, P. 2020. Dart: Open-domain structured data record to text generation. In Dart: Open-domain structured data record to text generation. arXiv:2007.02871
Nayak, T. and Ng, H.T. 2020. Effective modeling of encoder-decoder architecture for joint entity and relation extraction. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8528--8535
Novikova, J., Dušek, O., and Rieser, V. 2017. The e2e dataset: New challenges for end-toend generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue. pp. 201--206
Paolini, G., Athiwaratkun, B., Krone, J., Ma, J., Achille, A., and Anubhai, R. Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages. In Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages. arXiv:2101.05779
Parikh, A., Wang, X., Gehrmann, S., Faruqui, M., Dhingra, B., Yang, D., and Das, D. 2020. Totto: A controlled table-totext generation dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1173--1186
Puduppully, R., Dong, L., and Lapata, M. 2019. Data-to-text generation with content selection and planning. In Proceedings of the AAAI conference on artificial intelligence. pp. 6908--6915
Puduppully, R., Dong, L., and Lapata, M. 2019. Data-to-text generation with entity modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 2023--2035
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In Journal of Machine Learning Research. pp. 1--67
Shen, X., Chang, E., Su, H., Niu, C., and Klakow, D. 2020. Neural data-to-text generation via jointly learning the segmentation and correspondence. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7155--7165
Stanovsky, G., Michael, J., Zettlemoyer, L., and Dagan, I. 2018. Supervised open information extraction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018. pp. 885--895 10.18653/v1/n18-1081
Strubell, E., Verga, P., Belanger, D., and Mccallum, A. 2017. Fast and accurate entity recognition with iterated dilated convolutions. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pp. 2670--2680
Sutskever, I., Vinyals, O., and Le, Q.V. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems. pp. 3104--3112
Thomson, C., Reiter, E., and Sripada, S. 2020. Sportsett: Basketball-a robust and maintainable data-set for natural language generation. In Proceedings of the Workshop on Intelligent Information Processing and Natural Language Generation. pp. 32--40
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. pp. 5998--6008
Wadden, D., Wennberg, U., Luan, Y., and Hajishirzi, H. 2019. Entity, relation, and event extraction with contextualized span representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 5784--5789
Wiseman, S., Stuart, M., Shieber, A.M., and Rush 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pp. 2253--2263
Wu, F. and Weld, D. S. 2010. Open information extraction using wikipedia. In ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. pp. 118--127
Xu, G., Wang, C., and He, X. 2018. Improving clinical named entity recognition with global neural attention. In Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM) Joint International Conference on Web and Big Data. pp. 264--279
Yan, H., Gui, T., Dai, J., and Guo, Q. Zheng Zhang, and Xipeng Qiu. 2021. A unified generative framework for various ner subtasks. In Zheng Zhang, and Xipeng Qiu. 2021. A unified generative framework for various ner subtasks. arXiv:2106.01223
Yao, Y., Ye, D., Li, P., Han, X., Lin, Y., Liu, Z., Liu, Z., Huang, L., Zhou, J., and Sun, M. 2019. Docred: A large-scale document-level relation extraction dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 764--777
Zeng, X., Zeng, D., He, S., Liu, K., and Zhao, J. 2018. Extracting relational facts by an end-to-end neural model with copy mechanism. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 506--514
Zhan, J. and Zhao, H. 2020. The Thirty-Second Innovative Applications of Artificial Intelligence Conference. In The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence. pp. 9523--9530
Zhang, T., Heng, J., and Sil, A. 2019. Joint entity and event extraction with generative adversarial imitation learning. In Data Intelligence. pp. 99--120
Zheng, S., Wang, F., Bao, H., Hao, Y., Zhou, P., and Xu, B. 2017. Joint extraction of entities and relations based on a novel tagging scheme. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 1227--1236
Zhong, Z. and Chen, D. 2020. A frustratingly easy approach for joint entity and relation extraction. In A frustratingly easy approach for joint entity and relation extraction. arXiv:2010.12812