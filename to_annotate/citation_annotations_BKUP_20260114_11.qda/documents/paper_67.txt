Related Work

The standard practice of using BERT is fine-tuning, i.e. the entirety of the model parameters is adjusted on the training corpus of the downstream task, so that the model is adapted to that specific task (Devlin et al., 2019). There is also an alternative feature-based approach, used by ELMo (Peters et al., 2018). In the latter approach, the pre-trained model is regarded as a feature extractor with frozen parameters. During the learning of a downstream task, one feeds a fixed or learnable combination of the model's intermediate representations as input to the task-specific module, and only the parameters of the latter will be updated. It has been shown that the fine-tuning approach is generally superior to the feature-based approach for BERT in terms of task performance (Devlin et al., 2019;Peters et al., 2019).

A natural middle ground between these two approaches is partial fine-tuning, i.e. only fine-tuning some topmost layers of BERT while keeping the remaining bottom layers frozen. This approach has been studied in (Houlsby et al., 2019;Merchant et al., 2020), where the authors observed that finetuning only the top layers can almost achieve the performance of full fine-tuning on several GLUE tasks. The approach of partial fine-tuning essentially regards the bottom layers of BERT as a feature extractor. Freezing weights from bottom layers is a sensible idea as previous studies show that the mid layer representations produced by BERT are most transferrable, whereas the top layers representations are more task-oriented (Wang et al., 2019;Tenney et al., 2019b,a;Merchant et al., 2020).

 References: 
Aguilar, G., Ling, Y., Zhang, Y., Yao, B., Fan, X., and Guo, C. 2020. Knowledge distillation from internal representations. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. pp. 7350--7357
Alonso, H. and Plank, B. 2017. When is multitask learning effective? semantic sequence prediction under varying data conditions. In Proceedings of the 15th Conference of the European Chapter. pp. 44--53
Bingel, J. and Sogaard, A. 2017. Identifying beneficial task relations for multi-task learning in deep neural networks. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics. pp. 164--169
Caruana, R. 1997. Multitask Learning. Machine Learning. In Multitask Learning. Machine Learning. 10.1023/A:1007379606734
Clark, K., Luong, M., Khandelwal, U., Manning, C. D., Quoc, V., and Le 2019. BAM! born-again multi-task networks for natural language understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 5931--5937 10.18653/v1/P19-1595
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423
Hinton, G., Vinyals, O., and Dean, J. 2015. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop.
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning. pp. 2790--2799
Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. 2020. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 4163--4174 10.18653/v1/2020.findings-emnlp.372
Liu, N. F., Gardner, M., Belinkov, Y., Peters, M. E., and Smith, N. A. 2019. Linguistic knowledge and transferability of contextual representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1073--1094 10.18653/v1/N19-1112
Liu, X., He, P., Chen, W., and Gao, J. 2019. Multi-task deep neural networks for natural language understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4487--4496 10.18653/v1/P19-1441
Liu, X., Wang, Y., Ji, J., Cheng, H., Zhu, X., Awa, E., He, P., Chen, W., Poon, H., Cao, G., and Gao, J. 2020. The Microsoft toolkit of multitask deep neural networks for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. pp. 118--126 10.18653/v1/2020.acl-demos.16
Merchant, A., Rahimtoroghi, E., Pavlick, E., and Tenney, I. 2020. What happens to BERT embeddings during fine-tuning?. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. pp. 33--44 10.18653/v1/2020.blackboxnlp-1.4
Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/N18-1202
Peters, M. E., Ruder, S., and Smith, N. A. 2019. To tune or not to tune? adapting pretrained representations to diverse tasks. In Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019). pp. 7--14 10.18653/v1/W19-4302
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. In Journal of Machine Learning Research. pp. 1--67
Ruder, S. 2017. An overview of multitask learning in deep neural networks. In An overview of multitask learning in deep neural networks. abs/1706.05098
Sanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. abs/1910.01108
Sun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 4323--4332 10.18653/v1/D19-1441
Sun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 4323--4332 10.18653/v1/D19-1441
Tenney, I., Das, D., and Pavlick, E. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4593--4601 10.18653/v1/P19-1452
Tenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., Mccoy, T., Kim, N., Van Durme, B., Bowman, S., Das, D., and Pavlick, E. 2019. What do you learn from context? probing for sentence structure in contextualized word representations. In International Conference on Learning Representations.
Turc, I., Chang, M., Lee, K., and Toutanova, K. 2019. Well-read students learn better: On the importance of pre-training compact models. In Well-read students learn better: On the importance of pre-training compact models.
Wang, A., Hula, J., Xia, P., Pappagari, R., Mccoy, R. T., Patel, R., Kim, N., Tenney, I., Huang, Y., Yu, K., Jin, S., Chen, B., Van Durme, B., Grave, E., Pavlick, E., and Bowman, S. R. 2019. Can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4465--4476 10.18653/v1/P19-1439
Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., and Zhou, M. 2020. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.
Wu, S., Hongyang, R., Zhang, C., and RÃ© 2020. Understanding and improving information transfer in multi-task learning. In Understanding and improving information transfer in multi-task learning. arXiv:2005.00944
Xu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. BERT-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 7859--7869 10.18653/v1/2020.emnlp-main.633
Xu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. BERT-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 10 10.18653/v1/2020.emnlp-main.633