Related Work

D2T Generation with PLMs Large neural language models pretrained on self-supervised tasks (Lewis et al., 2020;Liu et al., 2019;Devlin et al., 2019) have recently gained a lot of traction in D2T generation research (Ferreira et al., 2020). Following Chen et al. (2020c), other works adopt PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a). Kale and Rastogi (2020b) and Ribeiro et al. (2020) showed that PLMs using linearized representations of data can outperform graph neural networks on graph-to-text datasets, recently surpassed again by graph-based models (Ke et al., 2021;Chen et al., 2020a). Although the models make use of general-domain pretraining tasks, all of them are eventually finetuned on domain-specific data.

Templates in Data-Driven D2T Generation Using simple handcrafted templates for individual keys or predicates is an efficient way of introducing domain knowledge while preventing text-to-text models from overfitting to a specific data format (Heidari et al., 2021;Kale and Rastogi, 2020a;. Transforming individual triples to text is also used in Laha et al. (2020) whose work is the most similar to ours. They also build a three-step pipeline for zero-shot D2T generation, but they use handcrafted rules for producing the output text and do not address content planning.

Content Planning in D2T Generation Content planning, i.e. ordering input facts and aggregating them into individual sentences, is a traditional part of the D2T generation pipeline (Ferreira et al., 2019;Gatt and Krahmer, 2018;Reiter and Dale, 1997). As previously demonstrated, using a content plan in neural D2T generation has important impact on the overall text quality (Moryossef et al., 2019a,b;Puduppully et al., 2019;Trisedya et al., 2020). Recently,  have shown that using a content plan leads to improved quality of PLM outputs. All the aforementioned models plan directly using predicates or keys in the D2T datasets representing the corresponding data item. Unlike these works, our planner is trained on ordering sentences in natural language.

Sentence Ordering Sentence ordering is the task of organizing a set of natural language sentences to increase the coherence of a text (Barzilay et al., 2001;Lapata, 2003). Several neural methods for this task were proposed, using either interactions between pairs of sentences Li and Jurafsky, 2017), global interactions (Gong et al., 2016;Wang and Wan, 2019), or combination of both (Cui et al., 2020). We base our ordering module ( §5.1) on the recent work of Calizzano et al. (2021), who use a pointer network (Wang and Wan, 2019;Vinyals et al., 2015) on top of a PLM.

Fact Aggregation The compact nature of the target text description results in aggregating multiple facts in a single sentence. Previous works (Wiseman et al., 2018;Shao et al., 2019;Shen et al., 2020;Xu et al., 2021) capture the segments which correspond to individual parts of the input as latent variables. Unlike these works, we adopt a simpler scenario using an already ordered sequence of facts, in which we selectively insert delimiters marking sentence boundaries.  Paragraph Compression We introduce paragraph compression as a new task in our D2T generation pipeline. As the last step in the pipeline, it is closely related to linguistic realisation, howeversince we already work with natural language in this step-the focus of our task is on sentence fusion, rephrasing, and coreference resolution. Unlike text summarization or simplification Jiang et al., 2020), we aim to convey the complete semantics of the text without omitting any facts. In contrast to sentence fusion (Geva et al., 2019;Barzilay and McKeown, 2005) or sentence compression (Filippova and Altun, 2013), we operate in the context of multiple sentences in a paragraph. The task is the central focus of our WIKIFLUENT corpus ( §4), which we synthesize using a model for the reverse task, split-andrephrase, i.e. splitting a complex sentence into simpler ones while preserving semantics (Botha et al., 2018;.

 References: 
Agarwal, O., Ge, H., Shakeri, S., and Al-Rfou, R. 2021. Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3554--3565
Attardi, G. 2015.
Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., and Ives, Z. 2007. Dbpedia: A nucleus for a web of open data. In The semantic web. pp. 722--735
Banerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72
Barzilay, R., Elhadad, N., and Mckeown, K. 2001. Sentence ordering in multidocument summarization. In Proceedings of the first international conference on Human language technology research.
Barzilay, R. and Kathleen R Mckeown 2005. Sentence fusion for multidocument news summarization. In Computational Linguistics. pp. 297--328
Ben-David, E., Keller, O., and Malmi, E. Idan Szpektor, and Roi Reichart. 2020. Semantically driven sentence fusion: Modeling and evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. pp. 1491--1505
Bird, S. 2006. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions. pp. 69--72
Botha, J. A., Faruqui, M., Alex, J., Baldridge, J., and Das, D. 2018. Learning to split and rephrase from Wikipedia edit history. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 732--737 10.18653/v1/D18-1080
Calizzano, R., Ostendorff, M., and Rehm, G. 2021. Ordering sentences and paragraphs with pretrained encoder-decoder transformers and pointer ensembles. In Proceedings of the 21st ACM Symposium on Document Engineering. pp. 1--9
Chang, E., Shen, X., Yeh, H., and Demberg, V. 2021. On training instance selection for few-shot neural text generation. In On training instance selection for few-shot neural text generation. arXiv:2107.03176
Chang, E., Shen, X., Zhu, D., Demberg, V., and Su, H. 2021. Neural data-to-text generation with lm-based text augmentation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 758--768
Chen, W., Su, Y., Yan, X., and Wang, W. Y. 2020. KGPT: Knowledge-grounded pretraining for data-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 8635--8648 10.18653/v1/2020.emnlp-main.697
Chen, X., Qiu, X., and Huang, X. 2016. Neural sentence ordering. In Neural sentence ordering. arXiv:1607.06952
Chen, Z., Chen, W., Zha, H., Zhou, X., Zhang, Y., Sundaresan, S., and Wang, W. Y. 2020. Logic2text: High-fidelity natural language generation from logical forms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. pp. 2096--2111
Chen, Z., Eavani, H., Chen, W., Liu, Y., and Wang, W. Y. 2020. Few-shot NLG with pre-trained language model. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 183--190
Cui, B., Li, Y., and Zhang, Z. 2020. Bert-enhanced relational sentence ordering network. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6310--6320
Dale, R. 2020. Natural language generation: The commercial state of the art in 2020. In Natural Language Engineering. pp. 481--487
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186
Dušek, O., David, M., Howcroft, V., and Rieser 2019. Semantic noise matters for neural natural language generation. In Proceedings of the 12th International Conference on Natural Language Generation. pp. 421--426
Dušek, O. and Jurčíček, F. 2015. Training a natural language generator from unaligned data. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. pp. 451--461
Dušek, O. and Kasner, Z. 2020. Evaluating semantic accuracy of data-to-text generation with natural language inference. In Proceedings of the 13th International Conference on Natural Language Generation. pp. 131--137
Dušek, O., Novikova, J., and Rieser, V. 2020. Evaluating the state-of-the-art of end-to-end natural language generation: The e2e nlg challenge. In Computer Speech & Language. pp. 123--156
2019. Pytorch lightning. In Pytorch lightning.
Ferreira, T., Gardent, C., Ilinykh, N., Van Der Lee, C., Mille, S., Moussallem, D., and Shimorina, A. 2020. The 2020 bilingual, bidirectional webnlg+ shared task overview and evaluation results (webnlg+ 2020). In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+).
Thiago Castro Ferreira, D., Moussallem, E., Krahmer, S., and Wubben 2018. Enriching the webnlg corpus. In Proceedings of the 11th International Conference on Natural Language Generation. pp. 171--176
Thiago Castro Ferreira, C., Van Der Lee, Emiel Van Miltenburg, E., and Krahmer 2019. Neural datato-text generation: A comparison between pipeline and end-to-end architectures. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 552--562
Filippova, K. and Altun, Y. 2013. Overcoming the lack of parallel data in sentence compression. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1481--1491
Gardent, C., Shimorina, A., Narayan, S., and Perez-Beltrachini, L. 2017. The WebNLG challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation. pp. 124--133
Gardner, M., Grus, J., Neumann, M., Tafjord, O., Dasigi, P., Nelson, F., Liu, M., Peters, M., Schmitz, L., and Zettlemoyer 2018. Allennlp: A deep semantic natural language processing platform. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS). pp. 1--6
Gatt, A. and Krahmer, E. 2018. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. In Journal of Artificial Intelligence Research. pp. 65--170
Geva, M., Malmi, E., Szpektor, I., and Berant, J. 2019. Discofuse: A large-scale dataset for discourse-based sentence fusion. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3443--3455
Gong, J., Chen, X., Qiu, X., and Huang, X. 2016. End-to-end neural sentence ordering using pointer network. In End-to-end neural sentence ordering using pointer network. arXiv:1611.04953
Harkous, H., Groves, I., and Saffari, A. 2020. Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 2410--2424
Heidari, P., Einolghozati, A., Jain, S., Batra, S., Callender, L., Arun, A., Mei, S., Gupta, S., Donmez, P., and Bhardwaj, V. 2021. Getting to production with few-shot natural language generation models. In Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 66--76
Jiang, C., Maddela, M., Lan, W., Zhong, Y., and Xu, W. 2020. Neural crf model for sentence alignment in text simplification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7943--7960
Kale, M. and Rastogi, A. 2020. Template guided text generation for task-oriented dialogue. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6505--6520 10.18653/v1/2020.emnlp-main.527
Kale, M. and Rastogi, A. 2020. Text-to-text pre-training for data-to-text tasks. In Proceedings of the 13th International Conference on Natural Language Generation. pp. 97--102
Kasner, Z. and Dušek, O. 2020. Data-to-text generation with iterative text editing. In Proceedings of the 13th International Conference on Natural Language Generation. pp. 60--67
Ke, P., Ji, H., Ran, Y., Cui, X., Wang, L., Song, L., Zhu, X., and Huang, M. 2021. Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. In Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. arXiv:2106.10502
Nitish Shirish Keskar, B., Mccann, Lav, R., Varshney, C., Xiong, R., and Socher 2019. Ctrl: A conditional transformer language model for controllable generation. In Ctrl: A conditional transformer language model for controllable generation. arXiv:1909.05858
Diederik, P., Kingma, J., and Ba 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations.
Laha, A., Jain, P., Mishra, A., and Sankaranarayanan, K. 2020. Scalable micro-planned generation of discourse from structured data. In Computational Linguistics. pp. 737--763
Lapata, M. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics. pp. 545--552
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880
Li, J. and Jurafsky, D. 2017. Neural net models of open-domain discourse coherence. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pp. 198--209
Bill Yuchen Lin, M., Shen, Y., Xing, P., Zhou, X., and Ren 2019. Commongen: A constrained text generation dataset towards generative commonsense reasoning. In Commongen: A constrained text generation dataset towards generative commonsense reasoning.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692
Moryossef, A., Goldberg, Y., and Dagan, I. 2019. Improving quality and efficiency in plan-based neural data-to-text generation. In Proceedings of the 12th International Conference on Natural Language Generation. pp. 377--382
Moryossef, A., Goldberg, Y., and Dagan, I. 2019. Step-by-step: Separating planning from realization in neural data-to-text generation. In Step-by-step: Separating planning from realization in neural data-to-text generation. arXiv:1904.03396
Nan, L., Radev, D., Zhang, R., Rau, A., Sivaprasad, A., Hsieh, C., Tang, X., Vyas, A., Verma, N., and Krishna, P. 2021. Dart: Open-domain structured data record to text generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 432--447
Narayan, S., Gardent, C., Cohen, S. B., and Shimorina, A. 2017. Split and rephrase. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pp. 606--616 10.18653/v1/D17-1064
Novikova, J., Dušek, O., and Rieser, V. 2017. The E2E Dataset: New Challenges for End-to-End Generation. In Proceedings of the 18th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 201--206
Papineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135
Parikh, A., Wang, X., Gehrmann, S., Faruqui, M., Dhingra, B., Yang, D., and Das, D. 2020. Totto: A controlled table-to-text generation dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1173--1186
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., and Antiga, L. 2019. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems. pp. 8026--8037
Puduppully, R., Dong, L., and Lapata, M. 2019. Data-to-text generation with content selection and planning. In Proceedings of the AAAI conference on artificial intelligence. pp. 6908--6915
Rebuffel, C., Roberti, M., Soulier, L., and Scoutheeten, G. Rossella Cancelliere, and Patrick Gallinari. 2021. Controlling hallucinations at word level in data-to-text generation. In Rossella Cancelliere, and Patrick Gallinari. 2021. Controlling hallucinations at word level in data-to-text generation. arXiv:2102.02810
Reiter, E. and Dale, R. 1997. Building applied natural language generation systems. In Natural Language Engineering. pp. 57--87
Leonardo, F. R., Ribeiro, M., and Schmitt Hinrich Schütze, and Iryna Gurevych. 2020. Investigating pretrained language models for graph-to-text generation. In Hinrich Schütze, and Iryna Gurevych. 2020. Investigating pretrained language models for graph-to-text generation. arXiv:2007.08426
Shao, Z., Huang, M., Wen, J., Xu, W., and Zhu, X. 2019. Long and diverse text generation with planning-based hierarchical variational model. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3257--3268 10.18653/v1/D19-1321
Su, Y., Meng, Z., Baker, S., and Collier, N. 2021. Few-shot table-to-text generation with prototype memory. In Findings of the Association for Computational Linguistics: EMNLP 2021. pp. 910--917
Su, Y., Vandyke, D., Wang, S., Fang, Y., and Collier, N. 2021. Plan-then-generate: Controlled data-to-text generation via planning. In Plan-then-generate: Controlled data-to-text generation via planning. arXiv:2108.13740
Trisedya, B., Qi, J., and Zhang, R. 2020. Sentence generation for entity description with content-plan attention. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 9057--9064
Vinyals, O., Fortunato, M., and Jaitly, N. 2015. Pointer networks. In Advances in Neural Information Processing Systems. pp. 2692--2700
Wang, T. and Wan, X. 2019. Hierarchical attention networks for sentence ordering. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 7184--7191
Williams, A., Nangia, N., and Bowman, S. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1112--1122
Wiseman, S., Stuart, M., Shieber, A.M., and Rush 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pp. 2253--2263
Wiseman, S., Stuart, M., Shieber, A.M., and Rush 2018. Learning neural templates for text generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 3174--3187
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., and Funtowicz, M. 2019. Huggingface's transformers: State-ofthe-art natural language processing. In Huggingface's transformers: State-ofthe-art natural language processing. arXiv:1910.03771
Xu, X. and Dušek, O. Verena Rieser, and Ioannis Konstas. 2021. Agggen: Ordering and aggregating while generating. In Verena Rieser, and Ioannis Konstas. 2021. Agggen: Ordering and aggregating while generating. arXiv:2106.05580
Zhang, J., Zhao, Y., Saleh, M., and Liu, P. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning. pp. 11328--11339
Zhao, C., Walker, M., and Chaturvedi, S. 2020. Bridging the structural gap between encoding and decoding for data-to-text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2481--2491