Introduction

Text-to-SQL parsing is the task of mapping natural language questions to executable SQL queries on relational databases (Zhong et al., 2017). It provides an easy way for common users unfamiliar with query languages to access large databases and has attracted great attention. Recently, lexicological alignments, which align question phrases to their corresponding SQL query fragments, have been proved to be very helpful in improving parsing performance (Shi et al., 2020). As shown in Figure 1, the token "competitor" should be aligned to "c1" in the SQL query. To capture such alignments, several attention-based models were proposed (Shi et al., 2020;Lei et al., 2020;Liu et al., 2021), which employ the attention weights among tokens to indicate the alignments. Specifically, they use an attention module to perform schema linking at the encoding stage (Lei et al., 2020;Liu et al., 2021), and may use another attention to align each output token to its corresponding input tokens at the decoding stage (Shi et al., 2020).  However, we argue that the attention mechanism is not an appropriate way to capture and leverage lexico-logical alignments. It mainly has the following two problems. First, the standard attention can only model alignments at the token level rather than the phrase level, while there are many multi-granular, non-continuous alignments in the text-to-SQL task. For the example in Figure 1, "order by . . . limit 1" is a SQL keyword pattern representing a superlative operation. However, the standard attention module can only align "order", "by", "limit", and "1" to "the longest" token by token, rather than regarding them as a whole. It may confuse the decoder and lead to the failure to generate this pattern correctly (Herzig and Berant, 2020). Second, traditional attentionbased approaches are prone to overfitting the training data, which is harmful to the model's generalization capability. It is not only the domain generalization (Dong et al., 2019) but also the compositional generalization (Herzig and Berant, 2020).

To solve the aforementioned problems, we propose a neural parsing framework to leverage explicit lexico-logical alignments. Dong et al. (2019) have pointed out that if we align question tokens to columns or values in databases before parsing, it will help to improve the model's generalization among different domains (databases). Motivated by this, our framework consists of two steps. Specifi-cally, we first implement a simple model to obtain possible lexico-logical alignments before parsing. While in the second step, we inject such alignments into a standard seq2seq parser by treating them as additional contexts, similar to "prompt information" or "evidence" in machine reading comprehension (Mihaylov and Frank, 2018;Tu et al., 2020;Niu et al., 2020). Moreover, to alleviate the negative effects on the parser caused by noise alignments, we propose a data augmentation method that adds noisy alignments during the training procedure. Experimental results on an open-released dataset, SQUALL (Shi et al., 2020), show that our framework achieves state-of-the-art performance and obtains an absolute improvement of 3.4% compared with existing attention-based models.

 References: 
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186
Dong, Z., Sun, S., Liu, H., Lou, J., and Zhang, D. 2019. Data-anonymous encoding for text-to-sql generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5405--5414
Finegan-Dollak, C., Kummerfeld, J. K., Zhang, L., Ramanathan, K., Sadasivam, S., Zhang, R., and Radev, D. 2018. Improving textto-sql evaluation methodology. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 351--360
Herzig, J. and Berant, J. 2020. Spanbased semantic parsing for compositional generalization. In Spanbased semantic parsing for compositional generalization. arXiv:2009.06040
Diederik, P., Kingma, J., and Ba 2015. Adam: A method for stochastic optimization. In ICLR (Poster).
Lei, W., Wang, W., Ma, Z., Gan, T., Lu, W., Kan, M., and Chua, T. 2020. Reexamining the role of schema linking in text-to-sql. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6943--6954
Xi Victoria Lin, R., Socher, C., and Xiong 2020. Bridging textual and tabular data for crossdomain text-to-sql semantic parsing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. pp. 4870--4888
Liu, Q., Yang, D., Zhang, J., Guo, J., Zhou, B., and Lou, J. 2021. Awakening latent grounding from pretrained language models for semantic parsing. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 1174--1189
Mihaylov, T. and Frank, A. 2018. Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 821--832
Niu, Y., Jiao, F., Zhou, M., and Yao, T. Jingfang Xu, and Minlie Huang. 2020. A self-training method for machine reading comprehension with soft evidence extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 3916--3927
Pasupat, P. and Liang, P. 2015. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. pp. 1470--1480
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., and Antiga, L. 2019. Pytorch: An imperative style, high-performance deep learning library. In Advances in neural information processing systems. pp. 8026--8037
See, A., Peter, J., Liu, C.D., and Manning 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 1073--1083
Shi, T., Zhao, C., Boyd-Graber, J., Daum√©, H., Iii, and Lee, L. 2020. On the potential of lexico-logical alignments for semantic parsing to sql queries. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. pp. 1849--1864
Tu, M., Huang, K., Wang, G., Huang, J., He, X., and Zhou, B. 2020. Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 9073--9080
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Patrick Von Platen, C., Ma, Y., Jernite, J., Plu, C., Xu, T. L., Scao, S., Gugger, M., Drame, Q., Lhoest, A. M., and Rush 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. pp. 38--45
Zhong, V., Xiong, C., and Socher, R. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. In Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv:1709.00103