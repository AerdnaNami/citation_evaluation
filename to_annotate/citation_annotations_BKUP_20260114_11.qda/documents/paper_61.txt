Related Work

In this section, we recap related work in dense retrieval and adversarial domain adaptation. Dense Retrieval Different from sparse first stage retrieval models, dense retrieval with Transformerbased models (Vaswani et al., 2017) such as BERT  conducts retrieval in the dense embedding space (Lee et al., 2019b;Guu et al., 2020;Karpukhin et al., 2020;Luan et al., 2021). Compared with its sparse counterparts, DR improves retrieval efficiency and also provides comparable or even superior effectiveness for in-domain datasets.

Recently, challenges of ZeroDR have attracted much attention (Thakur et al., 2021;Zhang et al., 2021;Li and Lin, 2021). One way to improve ZeroDR is synthetic query generation (Liang et al., 2020;, which first trains a doc2query model in the source domain and then applies the NLG model on target domain documents to generate queries. The target domain documents and generated queries form weak supervision labels for DR model training. Our method differs from them and focuses on directly improving the generalization ability of the learned representation space. Adversarial Domain Adaptation Unsupervised domain adaptation (UDA) has been studied extensively for computer vision applications. For example, maximum mean discrepancy (Long et al., 2013;Tzeng et al., 2014;Sun and Saenko, 2016) measures domain difference with a pre-defined metric and explicitly minimizes the difference; adversarial domain adaptation tries to adversarially trained the main model to confuse the domain classifier (Ganin and Lempitsky, 2015;Bousmalis et al., 2016;Tzeng et al., 2017;Luo et al., 2017). MoDIR builds upon the success of these UDA methods and introduces a new momentum learning technique that is necessary to address the unique challenges in ZeroDR.

 References: 
Bajaj, P., Campos, D., Craswell, N., Deng, L., Gao, J., Liu, X., Majumder, R., Mcnamara, A., Mitra, B., and Nguyen, T. 2016. MS MARCO: A human generated machine reading comprehension dataset. In MS MARCO: A human generated machine reading comprehension dataset. arXiv:1611.09268
Bondarenko, A., Fröbe, M., Beloucif, M., Gienapp, L., Ajjour, Y., Panchenko, A., Biemann, C., Stein, B., Wachsmuth, H., Potthast, M., and Hagen, M. 2020. Overview of Touché 2020: Argument Retrieval. In Working Notes Papers of the CLEF 2020 Evaluation Labs.
Boteva, V., Gholipour, D., Sokolov, A., and Riezler, S. 2016. A full-text learning to rank dataset for medical information retrieval. In European Conference on Information Retrieval. pp. 716--722
Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., and Erhan, D. 2016. Domain separation networks. In Advances in Neural Information Processing Systems.
Chang, W., Yu, F. X., Chang, Y., Yang, Y., and Kumar, S. 2020. Pre-training tasks for embedding-based large-scale retrieval. In International Conference on Learning Representations.
Chen, Q., Wang, H., Li, M., Ren, G., Li, S., Zhu, J., Li, J., Liu, C., Zhang, L., and Wang, J. 2018. SPTAG: A library for fast approximate nearest neighbor search. In SPTAG: A library for fast approximate nearest neighbor search.
Cohan, A., Feldman, S., Beltagy, I., Downey, D., and Weld, D. 2020. SPECTER: Document-level representation learning using citation-informed transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2270--2282
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186
Diggelmann, T., Boyd-Graber, J., Bulian, J., Ciaramita, M., and Leippold, M. 2020. CLIMATE-FEVER: A dataset for verification of real-world climate claims. In CLIMATE-FEVER: A dataset for verification of real-world climate claims. arXiv:2012.00614
Ganin, Y. and Lempitsky, V. 2015. Unsupervised domain adaptation by backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. pp. 1180--1189
Guo, R., Sun, P., Lindgren, E., Geng, Q., Simcha, D., Chern, F., and Kumar, S. 2020. Accelerating large-scale inference with anisotropic vector quantization. In Proceedings of the 37th International Conference on Machine Learning. pp. 3887--3896
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. 2020. Realm: Retrievalaugmented language model pre-training. In Realm: Retrievalaugmented language model pre-training. arXiv:2002.08909
Hasibi, F., Nikolaev, F., Xiong, C., Balog, K., Bratsberg, S. E., Kotov, A., and Callan, J. 2017. Dbpedia-entity v2: A test collection for entity search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '17. pp. 1265--1268
Hofstätter, S., Lin, S., Yang, J., Lin, J., and Hanbury, A. 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21. pp. 113--122
Hoogeveen, D., Verspoor, K. M., and Baldwin, T. 2015. Cqadupstack: A benchmark data set for community question-answering research. In Proceedings of the 20th Australasian Document Computing Symposium, ADCS '15.
Izacard, G. and Grave, E. 2020. Leveraging passage retrieval with generative models for open domain question answering. In Leveraging passage retrieval with generative models for open domain question answering. arXiv:2007.01282
Johnson, J., Douze, M., and Jégou, H. 2021. Billion-scale similarity search with gpus. In IEEE Transactions on Big Data. pp. 535--547
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6769--6781
Khattab, O. and Zaharia, M. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd.
International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20. In International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20. pp. 39--48
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. 2019. Natural questions: A benchmark for question answering research. In Transactions of the Association for Computational Linguistics. pp. 452--466
Lee, K., Chang, M., and Toutanova, K. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. pp. 6086--6096
Lee, K., Chang, M., and Toutanova, K. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 6086--6096
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., and Yih, W. Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv:2005.11401
Li, M. and Lin, J. 2021. Encoder adaptation of dense passage retrieval for open-domain question answering. In Encoder adaptation of dense passage retrieval for open-domain question answering. arXiv:2110.01599
Liang, D., Xu, P., Shakeri, S., Nogueira, C., Santos, R., Nallapati, Z., Huang, B., and Xiang 2020. Embedding-based zero-shot retrieval through query generation. In Embedding-based zero-shot retrieval through query generation. arXiv:2009.10270
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. RoBERTa: A robustly optimized bert pretraining approach. In RoBERTa: A robustly optimized bert pretraining approach. arXiv:1907.11692
Long, M., Wang, J., Ding, G., Sun, J., and Yu, P. S. 2013. Transfer feature learning with joint distribution adaptation. In Proceedings of the IEEE International Conference on Computer Vision (ICCV).
Luan, Y., Eisenstein, J., Toutanova, K., and Collins, M. 2021. Sparse, dense, and attentional representations for text retrieval. In Transactions of the Association for Computational Linguistics. pp. 329--345
Luo, Z., Zou, Y., Hoffman, J., and Li F Fei-Fei 2017. Label efficient learning of transferable representations acrosss domains and tasks. In Advances in Neural Information Processing Systems.
Ma, J., Korotkov, I., Yang, Y., Hall, K., and Mcdonald, R. 2020. Zero-shot neural retrieval via domain-targeted synthetic query generation. In Zero-shot neural retrieval via domain-targeted synthetic query generation. arXiv:2004.14503
Ma, J., Korotkov, I., Yang, Y., Hall, K., and Mcdonald, R. 2021. Zero-shot neural passage retrieval via domain-targeted synthetic question generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 1075--1088
Maia, M., Handschuh, S., Freitas, A., Davis, B., Mcdermott, R., Zarrouk, M., and Balahur, A. 2018. Www'18 open challenge: Financial opinion mining and question answering. In Companion Proceedings of the The Web Conference. pp. 1941--1942
Nogueira, R. and Cho, K. 2019. Passage re-ranking with BERT. In Passage re-ranking with BERT. arXiv:1901.04085
Nogueira, R., Jiang, Z., Pradeep, R., and Lin, J. 2020. Document ranking with a pretrained sequence-to-sequence model. In Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 708--718
Stephen, E., Robertson, K. S., and Jones 1976. Relevance weighting of search terms. In JASIS. pp. 129--146
Sun, B. and Saenko, K. 2016. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision. pp. 443--450
Thakur, N., Reimers, N., and Rücklé, A. 2021. BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. In BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv:2104.08663
Thorne, J., Vlachos, A., Christodoulopoulos, C., and Mittal, A. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 809--819
Tsatsaronis, G., Balikas, G., Malakasiotis, P., Partalas, I., Zschunke, M., Michael R Alvers, D., Weissenborn, A., Krithara, S., Petridis, D., and Polychronopoulos 2015. An overview of the BIOASQ largescale biomedical semantic indexing and question answering competition. In BMC bioinformatics. pp. 1--28
Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. 2017. Adversarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., and Darrell, T. 2014. Deep domain confusion: Maximizing for domain invariance. In Deep domain confusion: Maximizing for domain invariance. arXiv:1412.3474
Van Der Maaten, L. and Hinton, G. 2008. Visualizing data using t-sne. In Journal of Machine Learning Research. pp. 2579--2605
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems.
Voorhees, E., Alam, T., Bedrick, S., Demner-Fushman, D., Hersh, W. R., Lo, K., Roberts, K., Soboroff, I., and Wang, L. L. 2021. TREC-COVID: Constructing a pandemic information retrieval test collection. In SIGIR Forum. pp. 54
Wachsmuth, H., Syed, S., and Stein, B. 2018. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 241--251
Wadden, D., Lin, S., Lo, K., Wang, L. L., Van Zuylen, M., Cohan, A., and Hajishirzi, H. 2020. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 7534--7550
Xiong, L., Xiong, C., Li, Y., Tang, K., Liu, J., Bennett, P. N., Ahmed, J., and Overwĳk, A. 2021. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In International Conference on Learning Representations.
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 2369--2380
Zhang, X., Ma, X., Shi, P., and Lin, J. 2021. Mr. TyDi: A multi-lingual benchmark for dense retrieval. In Mr. TyDi: A multi-lingual benchmark for dense retrieval. arXiv:2108.08787