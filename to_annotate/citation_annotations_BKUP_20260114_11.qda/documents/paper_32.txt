Related Work

Effective utilization of annotation budgets has been the area of focus for numerous active learning works, showing improvements for different tasks like POS tagging (Ringger et al., 2007), sentiment analysis (Karlos et al., 2012;Li et al., 2013;Brew et al., 2010;Ju and Li, 2012), syntactic parsing (Duong et al., 2018), and named entity recognition (Settles and Craven, 2008;Shen et al., 2018). The focus of most of these works, however, has been on learning for a single language (often English). Prior work on AL that uses a multilingual setup or cross-lingual information sharing and that goes beyond training a separate model for each language has thus been limited. The closest work where multiple languages influence each other's acquisition is that of Qian et al. (2014); however, they still train a separate model for each language.

For transfer to multiple languages, recent advances in building MPLMs (Devlin et al., 2019;Conneau et al., 2020;Liu et al., 2020;Xue et al., 2020) have been extremely effective, especially in zero-shot transfer (Pires et al., 2019;Liu et al., 2020). Ein-Dor et al. (2020) studied the dataeffectiveness of these models when used in conjunction with AL, but, as with other AL work, with a single language focus. Finally, Lauscher et al. (2020) studied the effectiveness of the zero-shot setup, showing that adding a few examples to a model trained on English improves performance over zero-shot transfer. However, this assumes the availability of a full English task-specific corpus.

 References: 
David, A., Smith, N.A., and Smith 2007. Probabilistic models of nonprojective dependency trees. In EMNLP.
Tjong, E. F., Sang, K., and De Meulder, F. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In NAACL.
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Patrick Von Platen, C., Ma, Y., Jernite, J., Plu, C., and Xu 2020. Transformers: State-of-the-art natural language processing. In EMNLP: System Demo. 10.18653/v1/2020.emnlp-demos.6
Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., and Siddhant, A. Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer. In Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv:2010.11934