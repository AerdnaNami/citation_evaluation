Related Work

Template-based data generation has been previously used for data augmentation, for example to inject numerical skills (Geva et al., 2020), and to improve consistency (Asai and Hajishirzi, 2020), and zero-shot accuracy (Zhao et al., 2019). In addition, templates were used for dataset construction (Talmor and Berant, 2018;Thorne et al., 2021), and to analyse model generalization (Rozen et al., 2019). In this work, we automatically generate examples by instantiating templates using structured data. Since our method relies solely on tables as input, it is highly scalable, has rich lexical diversity, and can be easily extended to new skills and domains.

Recently, Thorne et al. ( 2021) introduced the WIKINLDB dataset, which includes queries that require reasoning over a set of textual facts. Queries are instantiated with values from a knowledge graph (KG), and facts are generated by a LM. Unlike this work, WIKINLDB is focused on evaluating reasoning skills. We, on the other hand, show that generated examples can be used to endow a pretrained LM with new reasoning skills. Moreover, tables are much easier to collect at scale compared to KGs, which tend to have limited coverage. Data augmenatation techniques have been extensively explored in RC, QA, and dialogue (Feng refers to all questions that do not require reasoning over the image modality.     Khashabi et al., 2020;Alberti et al., 2019;Puri et al., 2020;Bartolo et al., 2021). Here, we focus on tables as a valuable source for data generation.

Pre-training over tables has focused in the past on reasoning over tables and knowledge-bases Yin et al., 2020;Herzig et al., 2020;M端ller et al., 2021;Yu et al., 2021;Neeraja et al., 2021b). Here, we use pre-training over tables to improve reasoning over text. We leave evaluation on tasks beyond RC to future work.

Error-driven sampling has been considered in the past in the context of active learning (Sharma et al., 2018), reinforcement learning (Graves et al., 2017;Glover and Hokamp, 2019;Xu et al., 2019), transfer learning Pilault et al., 2021), and distributionally robust optimization (Oren et al., 2019;Sagawa et al., 2020), where the goal is to perform well over a family of distributions. Similar to Gottumukkala et al. (2020), we compute heterogeneous batches based on error rates, and show that this improves efficiency and performance.

 References: 
Chen, K., Xu, W., Cheng, X., Xiaochuan, Z., Zhang, Y., Song, L., Wang, T., Qi, Y., and Chu, W. 2020. Question directed graph attention network for numerical reasoning over text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6759--6768 10.18653/v1/2020.emnlp-main.549
Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li, S., Zhou, X., and Wang, W. Y. 2020. Tabfact: A large-scale dataset for table-based fact verification. In 8th International Conference on Learning Representations.
Clark, P., Tafjord, O., and Richardson, K. 2020. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. pp. 3882--3890 10.24963/ijcai.2020/537
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423
Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2368--2378 10.18653/v1/N19-1246
Efron, B. and Tibshirani, R. J. 1993. An Introduction to the Bootstrap. In Number 57 in Monographs on Statistics and Applied Probability.
Eisenschlos, J., Krichene, S., and M端ller, T. 2020. Understanding tables with intermediate pre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 281--296 10.18653/v1/2020.findings-emnlp.27
Steven, Y., Feng, V., Gangal, J., Wei, S., Chandar, S., and Vosoughi Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for NLP. In Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for NLP.
Ferguson, J., Gardner, M., Hajishirzi, H., Khot, T., and Dasigi, P. 2020. IIRC: A dataset of incomplete information reading comprehension questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1137--1147 10.18653/v1/2020.emnlp-main.86
Fetahu, B., Anand, A., and Koutraki, M. 2019. Tablenet: An approach for determining finegrained relations for wikipedia tables. In The World Wide Web Conference. pp. 2736--2742 10.1145/3308558.3313629
Geva, M., Gupta, A., and Berant, J. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 946--958 10.18653/v1/2020.acl-main.89
Glover, J. and Hokamp, C. 2019. Task selection policies for multitask learning. In Task selection policies for multitask learning.
Gottumukkala, A., Dua, D., Singh, S., and Gardner, M. 2020. Dynamic sampling strategies for multi-task reading comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 920--924 10.18653/v1/2020.acl-main.86
Graves, A., Bellemare, M. G., Menick, J., Munos, R., and Kavukcuoglu, K. 2017. Automated curriculum learning for neural networks. In Proceedings of the 34th International Conference on Machine Learning. pp. 1311--1320
Gupta, N., Lin, K., Roth, D., Singh, S., and Gardner, M. 2020. Neural module networks for reasoning over text. In 8th International Conference on Learning Representations.
Gupta, V. and Mehta, M. Pegah Nokhiz, and Vivek Srikumar. 2020b. INFOTABS: Inference on tables as semi-structured data. In Proceedings of the 58th. 10.18653/v1/2020.acl-main.210
Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 2309--2324
Herzig, J., Nowak, K., M端ller, T., Piccinno, F., and Eisenschlos, J. 2020. TaPas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4320--4333 10.18653/v1/2020.acl-main.398
Hidey, C., Chakrabarty, T., Alhindi, T., Varia, S., Krstovski, K., Diab, M., and Muresan, S. 2020. DeSePtion: Dual sequence prediction and adversarial examples for improved fact-checking. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 8593--8606 10.18653/v1/2020.acl-main.761
Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., and Hajishirzi, H. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 1896--1907 10.18653/v1/2020.findings-emnlp.171
Khot, T., Khashabi, D., Richardson, K., Clark, P., and Sabharwal, A. 2021. Text modular networks: Learning to decompose tasks in the language of existing models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1264--1279
Kirkpatrick, J., Pascanu, R., Rabinowitz, N. C., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., and Ramalho, T. 2016. Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. In Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. abs/1612.00796
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019.
M端ller, T., Martin Eisenschlos, J., and Krichene, S. 2021. Tapas at semeval-2021 task. In Tapas at semeval-2021 task.
Nan, L., Hsieh, C., Mao, Z., Victoria Lin, X., Verma, N., Zhang, R., Kryscinski, W., Schoelkopf, N., Kong, R., Tang, X., and Mutuma, M. Caiming Xiong, and Dragomir R. Radev. 2021. Fetaqa: Free-form table question answering. CoRR. In Caiming Xiong, and Dragomir R. Radev. 2021. Fetaqa: Free-form table question answering. CoRR. abs/2104.00369
Neeraja, J., Gupta, V., and Srikumar, V. 2021. Incorporating external knowledge to enhance tabular reasoning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2799--2809
Neeraja, J., Gupta, V., and Srikumar, V. 2021. Incorporating external knowledge to enhance tabular reasoning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2799--2809
Ni, A., Gardner, M., and Dasigi, P. 2021. Mitigating false-negative contexts in multi-document questionanswering with retrieval marginalization. In Mitigating false-negative contexts in multi-document questionanswering with retrieval marginalization. abs/2103.12235
Oren, Y., Sagawa, S., Hashimoto, T., and Liang, P. 2019. Distributionally robust language modeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 4227--4237 10.18653/v1/D19-1432
Pilault, J., El, A., and Pal, C. 2021. Conditionally adaptive multi-task learning: Improving transfer learning in NLP using fewer parameters & less data. In International Conference on Learning Representations.
Puri, R., Spring, R., Shoeybi, M., Patwary, M., and Catanzaro, B. 2020. Training question answering models from synthetic data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 5811--5826 10.18653/v1/2020.emnlp-main.468
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In Journal of Machine Learning Research. pp. 1--67
Ram, O., Kirstain, Y., and Berant, J. Amir Globerson, and Omer Levy. 2021. Few-shot question answering by pretraining span selection. In Association for Computational Linguistics (ACL).
Qiu Ran, Y., Lin, P., Li, J., Zhou, Z., and Liu 2019. NumNet: Machine reading comprehension with numerical reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2474--2484 10.18653/v1/D19-1251
Rozen, O., Shwartz, V., Aharoni, R., and Dagan, I. 2019. Diversify your datasets: Analyzing generalization via controlled variance in adversarial datasets. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). pp. 196--205 10.18653/v1/K19-1019
Sagawa, S., Wei Koh, P., Hashimoto, T. B., and Liang, P. 2020. Distributionally robust neural networks. In 8th International Conference on Learning Representations.
Sharma, S., Kumar Jha, A., Hegde, P., and Ravindran, B. 2018. Learning to multitask by active sampling. In 6th International Conference on Learning Representations.
Talmor, A. and Berant, J. 2018. The web as a knowledge-base for answering complex questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 641--651 10.18653/v1/N18-1059
Talmor, A. and Berant, J. 2019. MultiQA: An empirical investigation of generalization and transfer in reading comprehension. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4911--4921 10.18653/v1/P19-1485
Talmor, A., Elazar, Y., Goldberg, Y., and Berant, J. 2020. oLMpics-on what language model pre-training captures. In Transactions of the Association for Computational Linguistics. pp. 743--758 10.1162/tacl_a_00342
Talmor, A., Herzig, J., Lourie, N., and Berant, J. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4149--4158 10.18653/v1/N19-1421
Talmor, A., Yoran, O., Catav, A., Lahav, D., Wang, Y., Asai, A., Ilharco, G., Hajishirzi, H., and Berant, J. 2021. Mul-timodalQA: complex question answering over text, tables and images. In International Conference on Learning Representations.
Thawani, A., Pujara, J., Ilievski, F., and Szekely, P. 2021. Representing numbers in NLP: a survey and a vision. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 644--656
Thorne, J., Yazdani, M., Saeidi, M., Silvestri, F., Riedel, S., and Halevy, A. Y.
Wallace, E., Wang, Y., Li, S., Singh, S., and Gardner, M. 2019. Do NLP models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5307--5315 10.18653/v1/D19-1534
Wang, X., Tsvetkov, Y., and Neubig, G. 2020. Balancing training for multilingual neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 8526--8537 10.18653/v1/2020.acl-main.754
Warstadt, A., Cao, Y., Grosu, I., Peng, W., Blix, H., Nie, Y., Alsop, A., Bordia, S., Liu, H., Parrish, A., Wang, S., Phang, J., Mohananey, A., Phu Mon Htut, P., Jeretic, S. R., and Bowman 2019. Investigating BERT's knowledge of language: Five analysis methods with NPIs. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2877--2887 10.18653/v1/D19-1286
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Patrick Von Platen, C., Ma, Y., Jernite, J., Plu, C., Xu, T. L., Scao, S., Gugger, M., Drame, Q., and Lhoest, A. M.
Xu, Y., Liu, X., Shen, Y., Liu, J., and Gao, J. 2019. Multi-task learning with sample re-weighting for machine reading comprehension. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2644--2655 10.18653/v1/N19-1271
Yin, P., Neubig, G., Wen-Tau, Y., and Riedel, S. 2020. TaBERT: Pretraining for joint understanding of textual and tabular data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 8413--8426 10.18653/v1/2020.acl-main.745
Yogatama, D., De Masson D'autume, C., Connor, J., Kocisky, T., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W., Yu, L., Dyer, C., and Blunsom, P. 2019. Learning and evaluating general linguistic intelligence. In Learning and evaluating general linguistic intelligence.
Yu, T., Wu, C., Victoria Lin, X., Chern Tan, Y., Yang, X., Radev, D., Richard, S., and Xiong, C. 2021. GraPPa: Grammar-augmented pre-training for table semantic parsing. In International Conference on Learning Representations.
Zhang, S. and Zhang, X. 2020. Worst-case-aware curriculum learning for zero and few shot transfer. In Worst-case-aware curriculum learning for zero and few shot transfer.
Zhao, Z., Zhu, S., and Yu, K. 2019. Data augmentation with atomic templates for spoken language understanding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3637--3643 10.18653/v1/D19-1375