Introduction

Pre-trained language models (PLMs) have been widely explored both in natural language understanding (NLU) and generation (NLG) in recent years, this pre-training and fine-tuning paradigm sheds light on various downstream tasks in natural language processing (NLP). Compared with general pre-trained models, task-oriented pre-trained models (such as Summarization, Dialog and etc.), which is designed in line with task characteristics, may achieve better performance and be more robust. In this paper, we proposes a novel pre-trained dialog response generation model based on previous research.

Dialogue Response Generation (DSG) in open domain is a challenging task with a wide range of application scenarios. Recent advances in DSG utilize pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT2 (Radford et al., 2019) in two major categories. The first one focuses on how to fine-tune PLMs in downstream tasks and address the various application-specific needs and challenges (Lin et al., 2020). The second one augments dialog specific tasks into the PLM training Bao et al., 2020) and then fine-tunes the new pre-trained model in downstream tasks. We study the latter in this paper.

There is a proverbial one-to-many problem in DSG, i.e., a single dialog context could be followed by multiple reasonable responses. Existing works introduce latent variables to model this problem. For example, VHRED (Serban et al., 2017) incorporates latent continuous variable into the sequenceto-sequence (Seq2Seq) RNN model to improve the diversity of generated responses. VAE-Seq2Seq (Bahuleyan et al., 2017) proposes variational attention to replace the vanilla encoder-decoder attention (Luong et al., 2015), to avoid attention to bypass the latent space and invalidate the latent variable. For controllability and interpretability, some discrete VAEs have also been proposed, such as (Oord et al., 2017;Vahdat et al., 2018).

Recently, PLATO (Bao et al., 2020) firstly introduces latent variables into their pre-training dialog model, where the authors introduce a K-way (K = 20) categorical latent variable, and the pretrained model shows significant gains in multiple downstream response generation tasks. Continuous latent variables besides discrete latent variables is popularly used for modeling one-to-many mapping in dialog system, but the potential of incorporating continuous latent variables with large-scale language pretraining is less explored.

In this paper, we propose a pre-trained latent Variable Encoder-Decoder model for Dialog generation, which is called DialogVED. In this model, we introduce a continuous latent variable into the enhanced encoder-decoder pre-training framework and we adopt the optimization techniques based on the VAEs literature to learn the model with continuous latent variables. More specifically, we conduct the pre-training by optimizing the following 4 pre-training objectives simultaneously: 1) masked language spans loss to enhance the encoder's understanding of context, 2) response generation with n-gram loss to improve the decoder's planning ability, 3) Kullback-Leibler divergence loss to minimize the difference between the posterior and prior distribution of the latent variables, and 4) bag-ofwords loss to reduce posterior distribution collapse. In addition, we also explore the effect of absolute and relative position embeddings specific for conversational data on the model performance.

We conduct experiments on three different kinds of conversation tasks: chit-chat, knowledge grounded conversation, and conversational question answering. Experimental results verify the effectiveness and superiority of our model compared with the previous state-of-the-art method. We further carry out ablation study to better understand the impact of different components in the DialogVED on model performance including latent space sizes, different decoding strategies, and position embeddings for turns and roles.

Our pre-trained models and source code will be released, hoping to facilitate further research progress in dialogue generation. The main contributions of this paper can be summarized as follows: 1) We propose a pretrained dialog model, which incorporates continuous latent variables into the enhanced encoder-decoder pre-training framework; We explore the impact of latent variable sizes, different decoding strategies, and position embeddings for turns and roles in our model; Extensive experiments show that the proposed model achieves the new state-of-the-art (SOTA) in multiple downstream tasks, and our model has better performance both on relevance and diversity than previous SOTA in response generation.

 References: 
Alamri, H., Cartillier, V., Das, A., Wang, J., Cherian, A., Essa, I., Batra, D., Marks, T. K., Hori, C., and Anderson, P. 2019. Audio visual scene-aware dialog. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7558--7567
Alamri, H., Hori, C., Marks, T. K., Batra, D., and Parikh, D. 2019. Audio visual scene-aware dialog (AVSD) track for natural language generation in DSTC7. In AAAI workshop on the 7th of Dialog System Technology Challenge (DSTC7).
Bahuleyan, H., Mou, L., Vechtomova, O., and Poupart, P. 2017. Variational attention for sequence-to-sequence models. In Variational attention for sequence-to-sequence models. arXiv:1712.08207
Bao, S., He, H., Wang, F., Wu, H., and Wang, H. 2020. PLATO: Pre-trained dialogue generation model with discrete latent variable. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 85--96 10.18653/v1/2020.acl-main.9
Bowman, S., Vilnis, L., Vinyals, O., Dai, A., Jozefowicz, R., and Bengio, S. 2016. Generating sentences from a continuous space. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. pp. 10--21
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186
Fang, L., Li, C., Gao, J., Dong, W., and Chen, C. 2019. Implicit deep latent variable models for text generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3937--3947
Galley, M., Brockett, C., Gao, X., Gao, J., and Dolan, B. 2019. Grounded response generation task at dstc7. In AAAI Dialog System Technology Challenges Workshop.
Gao, J., Bi, W., Liu, X., Li, J., and Shi, S. 2019. Generating multiple diverse responses for short-text conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 6383--6390
Golovanov, S., Kurbanov, R., Nikolenko, S., Truskovskyi, K., Tselousov, A., and Wolf, T. 2019. Large-scale transfer learning for natural language generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 6053--6058
Joshi, M., Chen, D., Liu, Y., Daniel, S., Weld, L., Zettlemoyer, O., and Levy 2020. Spanbert: Improving pre-training by representing and predicting spans. In Transactions of the Association for Computational Linguistics. pp. 64--77
Diederik, P., Kingma, J., and Ba 2014. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization. arXiv:1412.6980
Diederik, P., Kingma, T., Salimans, R., Jozefowicz, X., Chen, I., Sutskever, M., and Welling 2016. Improving variational inference with inverse autoregressive flow. In Improving variational inference with inverse autoregressive flow. arXiv:1606.04934
Helena, C. and Kraemer 2014. Kappa coefficient. Wiley StatsRef: Statistics Reference Online. In Kappa coefficient. Wiley StatsRef: Statistics Reference Online. pp. 1--4
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880
Li, C., Gao, X., Li, Y., Li, X., Peng, B., Zhang, Y., and Gao, J. 2020. Optimus: Organizing sentences via pre-trained modeling of a latent space. In Optimus: Organizing sentences via pre-trained modeling of a latent space. arXiv:2004.04092
Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, W.B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119
Li, J., Monroe, W., and Jurafsky, D. 2016. A simple, fast diverse decoding algorithm for neural generation. In A simple, fast diverse decoding algorithm for neural generation. arXiv:1611.08562
Li, Y., Su, H., Shen, X., Li, W., Cao, Z., and Niu, S. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing. pp. 986--995
Lin, Z., Madotto, A., and Fung, P. 2020. Exploring versatile generative language model via parameter-efficient transfer learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. pp. 441--459
Liu, S., Chen, H., Ren, Z., Feng, Y., Liu, Q., and Yin, D. 2018. Knowledge diffusion for neural dialogue generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 1489--1498
Luong, M., Pham, H., and Manning, C.D. 2015. Effective approaches to attentionbased neural machine translation. In Effective approaches to attentionbased neural machine translation. arXiv:1508.04025
Van Den Oord, A., Vinyals, O., and Kavukcuoglu, K. 2017. Neural discrete representation learning. In Neural discrete representation learning. arXiv:1711.00937
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In fairseq: A fast, extensible toolkit for sequence modeling. arXiv:1904.01038
Papineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311--318
Qi, W., Yan, Y., Gong, Y., Liu, D., Duan, N., Chen, J., Zhang, R., and Zhou, M. 2020. Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. pp. 2401--2410
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In Language models are unsupervised multitask learners.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In Journal of Machine Learning Research. pp. 1--67
Sanabria, R., Palaskar, S., and Metze, F. 2019. Cmu sinbad's submission for the dstc7 avsd challenge. In Cmu sinbad's submission for the dstc7 avsd challenge.
Serban, I., Sordoni, A., Lowe, R., Charlin, L., Pineau, J., Courville, A., and Bengio, Y. 2017. A hierarchical latent variable encoder-decoder model for generating dialogues. In Proceedings of the AAAI Conference on Artificial Intelligence.
Shaw, P., Uszkoreit, J., and Vaswani, A. 2018. Self-attention with relative position representations. In Self-attention with relative position representations. arXiv:1803.02155
Vahdat, A., Macready, W., Bian, Z., Khoshaman, A., and Andriyash, E. 2018. Dvae++: Discrete variational autoencoders with overlapping transformations. In International Conference on Machine Learning. pp. 5035--5044
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Attention is all you need. arXiv:1706.03762
Vinyals, O. and Le, Q. 2015. A neural conversational model. In A neural conversational model. arXiv:1506.05869
Wu, S., Li, Y., Zhang, D., Zhou, Y., and Wu, Z. 2020. Diverse and informative dialogue generation with context-specific commonsense knowledge awareness. In Proceedings of the 58th annual meeting of the association for computational linguistics. pp. 5811--5820
Xiao, D., Zhang, H., Li, Y., Sun, Y., Hao Tian, H., Wu, H., and Wang 2020. Ernie-gen: An enhanced multi-flow pre-training and fine-tuning framework for natural language generation. In Ernie-gen: An enhanced multi-flow pre-training and fine-tuning framework for natural language generation. arXiv:2001.11314
Xu, C., Wu, W., Tao, C., Hu, H., Schuerman, M., and Wang, Y. 2019. Neural response generation with meta-words. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 5416--5426 10.18653/v1/P19-1538
Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., and Weston, J. 2018. Personalizing dialogue agents: I have a dog. In Personalizing dialogue agents: I have a dog.
Zhang, Y., Sun, S., Galley, M., Chen, Y., Brockett, C., Gao, X., Gao, J., Liu, J., and Dolan, B. 2020. Dialogpt: Large-scale generative pre-training for conversational response generation. In ACL, system demonstration.
Zhao, T., Lee, K., and Eskenazi, M. 2018. Unsupervised discrete sentence representation learning for interpretable neural dialog generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 1098--1107 10.18653/v1/P18-1101