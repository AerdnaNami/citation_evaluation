Related Work

Long Document Summarization Long document summarization has been studied in multiple domains, such as news (Nallapati et al., 2016), patterns (Trappey et al., 2009, books , scientific publications (Qazvinian and Radev, 2008), and med-ical records (Cohan et al., 2018). Gidiotis and Tsoumakas (2020) proposed a divide-and-conquer method by splitting the input into multiple segments, summarizing them separately, and combining the summary pieces. Grail et al. (2021) proposed a hierarchical neural model to process segmented input blocks. Compared with SUMM N , these models only split the input once, implying the lack of flexibility when handling longer input.

The GovReport dataset was recently introduced containing documents with more than 9000 words, thus greatly challenging the capabilities of current models such as PEGASUS , TLM (Subramanian et al., 2019), and BIG-BIRD (Zaheer et al., 2020). To handle this dataset, Huang et al. (2021) proposed head-wise positional strides to reduce the cost of the encoderdecoder attention. Similarly, models such as Longformer (Beltagy et al., 2020) and Reformer (Kitaev et al., 2020) adjust attention mechanisms in Transformers to consume longer inputs. However, these models sparsify the attention structure of the pretrained model to fit the longer source text. By contrast, SUMM N is able to maintain the full structure of various pretrained models.

Long Dialogue Summarization Various models have also been proposed to handle long dialogue summarization. HMNet (Zhu et al., 2020) and HAT-BART (Rohde et al., 2021) leverage a twolevel transformer-based model to obtain word level and sentence level representations. DialLM (Zhong et al., 2021a), Longformer-BART-arg (Fabbri et al., 2021) use finetuning or data augmentation to incorporate the external knowledge to maintain the accuracy of lengthy input. Different from these models, SUMM N is a framework without modifying the structure of the backbone attention model.

 References: 
Beltagy, I., Peters, M. E., and Cohan, A. 2020. Longformer: The long-document transformer. In Longformer: The long-document transformer. arXiv:2004.05150
Chen, J. and Yang, D. 2021. Structure-aware abstractive conversation summarization via discourse and action graphs. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1380--1391 10.18653/v1/2021.naacl-main.109
Chen, M., Chu, Z., Wiseman, S., and Gimpel, K. 2021. Summscreen: A dataset for abstractive screenplay summarization. In Summscreen: A dataset for abstractive screenplay summarization. arXiv:2104.07091
Chen, Z., Chen, L., Xu, Z., Zhao, Y., Zhu, S., and Yu, K. 2020. Credit: Coarse-to-fine sequence generation for dialogue state tracking. In Credit: Coarse-to-fine sequence generation for dialogue state tracking. arXiv:2009.10435
Cohan, A., Dernoncourt, F., Doo, S., Kim, T., Bui, S., Kim, W., Chang, N., and Goharian 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 615--621 10.18653/v1/N18-2097
Fabbri, A., Rahman, F., Rizvi, I., Wang, B., Li, H., Mehdad, Y., and Radev, D. 2021. ConvoSumm: Conversation summarization benchmark and improved abstractive summarization with argument mining. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 6866--6880 10.18653/v1/2021.acl-long.535
Fan, A., Lewis, M., and Dauphin, Y. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 889--898 10.18653/v1/P18-1082
Feng, X., Feng, X., Qin, B., and Geng, X. 2021. Dialogue discourse-aware graph model and data augmentation for meeting summarization. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence. pp. 3808--3814 10.24963/ijcai.2021/524
Gehrmann, S., Deng, Y., and Rush, A. 2018. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 4098--4109 10.18653/v1/D18-1443
Gidiotis, A. and Tsoumakas, G. 2020. A divide-and-conquer approach to the summarization of long documents. In Speech, and Language Processing. pp. 3029--3040 10.1109/TASLP.2020.3037401
Gliwa, B., Mochol, I., Biesek, M., and Wawer, A. 2019. SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization. pp. 70--79 10.18653/v1/D19-5409
Grail, Q., Perez, J., and Gaussier, E. 2021. Globalizing BERT-based transformer architectures for long document summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 1792--1810 10.18653/v1/2021.eacl-main.154
Huang, L., Cao, S., Nikolaus, N., Parulian, H., Ji, L., and Wang 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 1419--1436 10.18653/v1/2021.naacl-main.112
Janin, A., Baron, D., Edwards, J., Ellis, D., Gelbart, D., Morgan, N., Peskin, B., Pfau, T., Shriberg, E., and Stolcke, A. 2003. The icsi meeting corpus. In 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing.
Kitaev, N., Kaiser, L., and Levskaya, A. 2020. Reformer: The efficient transformer. In International Conference on Learning Representations.
Kryściński, W. and Rajani, N. 2021. Booksum: A collection of datasets for longform narrative summarization. In Booksum: A collection of datasets for longform narrative summarization. arXiv:2105.08209
Lewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703
Li, M., Zhang, L., Heng, J., and Radke, R. J. 2019. Keep meeting summaries on topic: Abstractive multi-modal meeting summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 2190--2196 10.18653/v1/P19-1210
Lin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81
Mccowan, I., Carletta, J., Kraaij, W., Ashby, S., Bourban, Flynn, M., Guillemot, T., Hain, Kadlec, V., and Karaiskos 2005. The ami meeting corpus. In Proceedings of the 5th International Conference on Methods and Techniques in Behavioral Research.
Mihalcea, R. and Tarau, P. 2004. TextRank: Bringing order into text. In Proceedings of the 2004 Conference on Empirical Methods in Natural guage Processing. pp. 404--411
Nallapati, R., Zhou, B., Cicero Dos Santos, C., Gulcehre, B., and Xiang 2016. Abstractive text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. pp. 280--290 10.18653/v1/K16-1028
Qazvinian, V. and Radev, D. R. 2008. Scientific paper summarization using citation summary networks. In Proceedings of the 22nd International Conference on Computational Linguistics. pp. 689--696
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. In Journal of Machine Learning Research. pp. 1--67
Rohde, T., Wu, X., and Liu, Y. 2021. Hierarchical learning for generation with long source sequences. In Hierarchical learning for generation with long source sequences. arXiv:2104.07545
See, A., Peter, J., Liu, C. D., and Manning 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 1073--1083 10.18653/v1/P17-1099
Subramanian, S., Li, R., Pilault, J., and Pal, C. 2019. On extractive and abstractive neural document summarization with transformer language models. In On extractive and abstractive neural document summarization with transformer language models. arXiv:1909.03186
Tay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D. 2020. Sparse sinkhorn attention. In International Conference on Machine Learning. pp. 9438--9447
Amy, J. C., Trappey, C. V., Trappey, C., and Wu 2009. Automatic patent document summarization for collaborative knowledge systems and services. In Journal of Systems Science and Systems Engineering. pp. 71--94
Vig, J., Kryściński, W., Goel, K., and Rajani, N.F. 2021. Summvis: Interactive visual analysis of models, data, and evaluation for text summarization. In Summvis: Interactive visual analysis of models, data, and evaluation for text summarization.
Wu, J., Ouyang, L., Daniel, M., Ziegler, N., Stiennon, R., Lowe, J., Leike, P., and Christiano 2021. Recursively summarizing books with human feedback. In Recursively summarizing books with human feedback. arXiv:2109.10862
Xu, Y. and Lapata, M. 2020. Coarse-to-fine query focused multi-document summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3632--3645 10.18653/v1/2020.emnlp-main.296
Zaheer, M., Guruganesh, G., Kumar Avinava Dubey, J., Ainslie, C., Alberti, S., Ontanon, P., Pham, A., Ravula, Q., Wang, L., and Yang 2020. Big bird: Transformers for longer sequences. In NeurIPS.
Zhang, J., Zhao, Y., Saleh, M., and Liu, P. J. 2019. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In Pegasus: Pre-training with extracted gap-sentences for abstractive summarization.
Zhang, Y., Ni, A., Yu, T., Zhang, R., Zhu, C., Deb, B., and Celikyilmaz, A. Ahmed Hassan Awadallah, and Dragomir Radev. 2021. An exploratory study on long dialogue summarization. In What works and what's next. arXiv:2109.04609
Zhao, Y., Saleh, M., and Liu, P.J. 2020. Seal: Segment-wise extractive-abstractive long-form text summarization. In Seal: Segment-wise extractive-abstractive long-form text summarization. arXiv:2006.10213
Zhong, M., Liu, Y., Xu, Y., Zhu, C., and Zeng, M. 2021. Dialoglm: Pre-trained model for long dialogue understanding and summarization. In Dialoglm: Pre-trained model for long dialogue understanding and summarization. arXiv:2109.02492
Zhong, M., Da Yin, T., Yu, A., Zaidi, M., Mutuma, R., Jha, A., Hassan Awadallah, A., Celikyilmaz, Y., Liu, X., Qiu, D., and Radev 2021. QMSum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 5905--5921 10.18653/v1/2021.naacl-main.472