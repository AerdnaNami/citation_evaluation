Related Work

Previous work in the image domain has analysed the output of specific layers in an attempt to identify adversarial examples or adversarial subspaces. First, (Feinman et al., 2017) proposed that adversarial subspaces have a lower probability density, motivating the use of the Kernel Density (KD) metric to detect the adversarial examples. Nevertheless, (Ma et al., 2018) found Local Intrinsic Dimensionality (LID) was a better metric in defining the subspace for more complex data. In contrast to the local subspace focused approaches of KD and LID, (Carrara et al., 2019b) showed that trajectories of hidden layer features can be used to train a LSTM network to accurately discriminate between authentic and adversarial examples. Out performing all previous methods, (  introduced an effective detection framework using Mahalanobis Distance Analysis (MDA), where the distance is calculated between a test sample and the closest class-conditional Gaussian distribution in the space defined by the output of the final layer of the classifier (logit space). (Li and Li, 2016) also explored using the output of convolutional layers for image classification systems to identify statistics that distinguish adversarial samples from original samples. They find that by performing a PCA decomposition the statistical variation in the least principal directions is the most significant and can be used to separate original and adversarial samples. However, they argue this is ineffective as an adversary can easily suppress the tail distribution. Hence, (Li and Li, 2016) extract statistics from the convolutional layer output to train a cascade classifier to separate the original and adversarial samples. Most recently, (Mao et al., 2019) avoid the use of artificially designed metrics and combine the adversarial subspace identification stage and the detecting adversaries stage into a single framework, where a parametric model adaptively learns the deep features for detecting adversaries.

In contrast to the embedding space detection approaches, (Cohen et al., 2019) shows that influence functions combined with Nearest Neighbour distances perform comparably or better than the above standard detection approaches. Other detection approaches have explored the use of uncertainty: (Smith and Gal, 2018) argues that adversarial examples are out of distribution and do not lie on the manifold of real data. Hence, a discriminative Bayesian model's epistemic (model) uncertainty should be high. Therefore, calculations of the model uncertainty are thought to be useful in detecting adversarial examples, independent of the domain. However, Bayesian approaches aren't always practical in implementation and thus many different approaches to approximate this uncertainty have been suggested in literature (Leibig et al., 2017;Gal, 2016;Gal and Ghahramani, 2016).

There are a number of existing NLP specific detection approaches. For character level attacks, detection approaches have exploited the grammatical (Sakaguchi et al., 2017) and spelling (Mays et al., 1991;Islam and Inkpen, 2009) inconsistencies to identify and detect the adversarial samples. However, these character level attacks are unlikely to be employed in practice due to the simplicity with which they can be detected. Therefore, detection approaches for the more difficult semantically similar attack samples are of greater interest, where the meaning of the textual input is maintained without compromising the spelling or gram-matical integrity. To tackle such word-level, semantically similar examples,  designed a discriminator to classify each token representation as part of an adversarial perturbation or not, which is then used to 'correct' the perturbation. Other detection approaches (Raina et al., 2020;Han et al., 2020;Minervini and Riedel, 2018) have shown some success in using perplexity to identify adversarial textual examples. Most recently, (Mozes et al., 2020) achieved state of the art performance with the Frequency Guided Word Substitution (FGWS) detector, where a change in model prediction after substituting out low frequency words is revealing of adversarial samples.

 References: 
Adversarial texts with gradient methods. In Adversarial texts with gradient methods.
Bhambri, S., Muku, S., Tulasi, A., and Buduru, A.B. 1667. A study of black box adversarial attacks in computer vision. In A study of black box adversarial attacks in computer vision.
Biggio, B. and Roli, F. 2017. Wild patterns: Ten years after the rise of adversarial machine learning. In Wild patterns: Ten years after the rise of adversarial machine learning. abs/1712.03141
Blohm, M., Jagfeld, G., Sood, E., Yu, X., and Vu, N. T. 2018. Comparing attentionbased convolutional and recurrent neural networks: Success and limitations in machine reading comprehension. In Comparing attentionbased convolutional and recurrent neural networks: Success and limitations in machine reading comprehension. abs/1808.08744
Carrara, F., Becarelli, R., Caldelli, R., Falchi, F., and Amato, G. 2019. Adversarial examples detection in features distance spaces. In Computer Vision -ECCV 2018 Workshops. pp. 313--327
Carrara, F., Becarelli, R., Caldelli, R., Falchi, F., and Amato, G. 2019. Adversarial Examples Detection in Features Distance Spaces: Subvolume B. In Adversarial Examples Detection in Features Distance Spaces: Subvolume B. pp. 313--327 10.1007/978-3-030-11012-3_26
Cer, D., Yang, Y., Kong, S., Hua, N., Limtiaco, N., St, R., John, N., Constant, M., Guajardo-Cespedes, S., and Yuan 2018. Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. In Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. abs/1803.11175
Chambers, L. and Ingham, K. 2011. The BULATS online speaking test. In Research Notes. pp. 21--25
Chen, X., Liu, X., Qian, Y., Gales, M. J.F., and Woodland, P. C. 2016. Cued-rnnlm -an open-source toolkit for efficient training and evaluation of recurrent neural network language models. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 6000--6004 10.1109/ICASSP.2016.7472829
Cheng, M., Yi, J., Zhang, H., Chen, P., and Hsieh, C. 2018. Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples. In Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples. abs/1803.01128
Clark, K., Luong, M., Le, Q. V., and Manning, C. D. 2003. ELECTRA: pretraining text encoders as discriminators rather than generators. CoRR, abs. In ELECTRA: pretraining text encoders as discriminators rather than generators. CoRR, abs.
Cohen, G., Sapiro, G., and Giryes, R. 2019. Detecting adversarial samples using influence functions and nearest neighbors. In Detecting adversarial samples using influence functions and nearest neighbors. abs/1909.06872
2001. Common European Framework of Reference for Languages: Learning, Teaching. In Assessment.
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. BERT: pre-training of deep bidirectional transformers for language understanding. In BERT: pre-training of deep bidirectional transformers for language understanding. abs/1810.04805
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding.
Feinman, R., Curtin, R. R., Shintre, S., and Gardner, A. B. 2017. Detecting adversarial samples from artifacts. In Detecting adversarial samples from artifacts.
Y 2016. Uncertainty in deep learning. In Uncertainty in deep learning.
Gal, Y. and Ghahramani, Z. 2016. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Dropout as a bayesian approximation: Representing model uncertainty in deep learning.
Garg, S. and Ramakrishnan, G. 1970. BAE: bert-based adversarial examples for text classification. CoRR, abs. In BAE: bert-based adversarial examples for text classification. CoRR, abs.
Goodfellow, I., Shlens, J., and Szegedy, C. 2015. Explaining and harnessing adversarial examples. In International Conference on Learning Representations.
Grosse, K., Papernot, N., Manoharan, P., Backes, M., and Mcdaniel, P. D. 2016. Adversarial perturbations against deep neural networks for malware classification. In Adversarial perturbations against deep neural networks for malware classification.
Han, W., Zhang, L., Jiang, Y., and Tu, K. 2020. Adversarial attack and defense of structured prediction models. In Adversarial attack and defense of structured prediction models.
He, K., Zhang, X., Ren, S., and Sun, J. 2015. Deep residual learning for image recognition. In Deep residual learning for image recognition. abs/1512.03385
Huang, A., Al-Dujaili, A., Hemberg, E., and Reilly, U.O. 2018. Adversarial deep learning for robust detection of binary encoded malware. In Adversarial deep learning for robust detection of binary encoded malware. abs/1801.02950
Islam, A. and Inkpen, D. 2009. Real-word spelling correction using google web it 3-grams. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. pp. 1241--1249
Iyyer, M., Wieting, J., Gimpel, K., and Zettlemoyer, L. 2018. Adversarial example generation with syntactically controlled paraphrase networks. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1875--1885 10.18653/v1/N18-1170
Jia, R. and Liang, P. 2017. Adversarial examples for evaluating reading comprehension systems. In Adversarial examples for evaluating reading comprehension systems. abs/1707.07328
Jin, D., Jin, Z., Zhou, J. T., and Szolovits, P. 1907. Is BERT really robust? natural language attack on text classification and entailment. CoRR, abs. In Is BERT really robust? natural language attack on text classification and entailment. CoRR, abs.
Krizhevsky, A., Nair, V., and Hinton, G. Cifar-100 (canadian institute for advanced research). In Cifar-100 (canadian institute for advanced research).
Kurakin, A., Goodfellow, I. J., and Bengio, S. 2016. Adversarial machine learning at scale. CoRR. In Adversarial machine learning at scale. CoRR. abs/1611.01236
Lee, K., Lee, K., Lee, H., and Shin, J. 2018. A simple unified framework for detecting outof-distribution samples and adversarial attacks. In A simple unified framework for detecting outof-distribution samples and adversarial attacks.
Leibig, C., Vaneeda Allken, M., Ayhan, P., Berens, S., and Wahl 2017. Leveraging uncertainty information from deep neural networks for disease detection. In Scientific Reports. pp. 7
Li, J., Ji, S., Du, T., Li, B., and Wang, T. 2018. Textbugger: Generating adversarial text against real-world applications. In Textbugger: Generating adversarial text against real-world applications. abs/1812.05271
Li, X. and Li, F. 2016. Adversarial examples detection in deep networks with convolutional filter statistics. In Adversarial examples detection in deep networks with convolutional filter statistics. abs/1612.07767
Lin, T., Maire, M., Belongie, S. J., Bourdev, L. D., Girshick, R. B., Hays, J., Perona, P., Ramanan, D., Doll√°r, P., and Zitnick, C. L. 2014.
Ma, X., Li, B., Wang, Y., Erfani, S. M., Sudanthi, N. R., Wijewickrema, M. E., Houle, G., Schoenebeck, D., Song, J., and Bailey 2018. Characterizing adversarial subspaces using local intrinsic dimensionality. In Characterizing adversarial subspaces using local intrinsic dimensionality. abs/1801.02613
Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. pp. 142--150
Mao, X., Chen, Y., Li, Y., He, Y., and Xue, H. 2019. Learning to characterize adversarial subspaces. In Learning to characterize adversarial subspaces.
Mays, E., Damerau, F. J., and Mercer, R. L. 1991. Context based spelling correction. In Information Processing Management. pp. 517--522 10.1016/0306-4573(91)90066-U
Minervini, P. and Riedel, S. 2018. Adversarially regularising neural NLI models to integrate logical background knowledge. In Adversarially regularising neural NLI models to integrate logical background knowledge. abs/1808.08609
Seyed-Mohsen Moosavi-Dezfooli, A., Fawzi, O., Fawzi, P., and Frossard 2016. abs/1610.08401
Morris, J., Lifland, E., Yoo, J. Y., Grigsby, J., Jin, D., and Qi, Y. 2020. TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. pp. 119--126 10.18653/v1/2020.emnlp-demos.16
Mozes, M., Stenetorp, P., Kleinberg, B., and Griffin, L. D. 2004. Frequency-guided word substitutions for detecting textual adversarial examples. CoRR, abs. In Frequency-guided word substitutions for detecting textual adversarial examples. CoRR, abs.
Neekhara, P., Hussain, S., Dubnov, S., and Koushanfar, F. 2018. Adversarial reprogramming of sequence classification neural networks. In Adversarial reprogramming of sequence classification neural networks. abs/1809.01829
Niu, T. and Bansal, M. 2018. Adversarial oversensitivity and over-stability strategies for dialogue models. In Adversarial oversensitivity and over-stability strategies for dialogue models. abs/1809.02079
Papernot, N., Mcdaniel, P. D., Swami, A., and Harang, R. E. 2016. Crafting adversarial input sequences for recurrent neural networks. In Crafting adversarial input sequences for recurrent neural networks. abs/1604.08275
Vyas Raina, Mark, J. F., Gales, K. M., and Knill 2020. Universal Adversarial Attacks on Spoken Language Assessment Systems. In Proc. Interspeech 2020. pp. 3855--3859 10.21437/Interspeech.2020-1890
Shuhuai Ren, Y., Deng, K., He, W., and Che 2019. Generating natural language adversarial examples through probability weighted word saliency. In ACL (1). pp. 1085--1097 10.18653/v1/p19-1103
Marco Tulio Ribeiro, S., Singh, C., and Guestrin 2018. Semantically equivalent adversarial rules for debugging NLP models. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 856--865 10.18653/v1/P18-1079
Rosenberg, I., Shabtai, A., Rokach, L., and Elovici, Y. 2017. Generic black-box end-to-end attack against rnns and other API calls based malware classifiers. In Generic black-box end-to-end attack against rnns and other API calls based malware classifiers. abs/1707.05970
Sakaguchi, K., Post, M., and Van Durme, B. 2017. Grammatical error correction with neural reinforcement learning. In Grammatical error correction with neural reinforcement learning. abs/1707.00299
Samanta, S. and Mehta, S. 2017. Towards crafting text adversarial samples. In Towards crafting text adversarial samples. abs/1707.02812
Saravia, E., Liu, H.T., Huang, Y., Wu, J., and Chen, Y. 2018. CARER: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 3687--3697 10.18653/v1/D18-1404
Serban, A., Poll, E., and Visser, J. 2008. Adversarial examples on object recognition: A comprehensive survey. CoRR, abs. In Adversarial examples on object recognition: A comprehensive survey. CoRR, abs.
Smith, L. and Gal, Y. 2018. Understanding measures of uncertainty for adversarial example detection. In Understanding measures of uncertainty for adversarial example detection.
Sun, M., Tang, F., Yi, J., Wang, F., and Zhou, J. 2018. Identify susceptible locations in medical records via adversarial attacks on deep predictive models. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery amp; Data Mining, KDD '18. pp. 793--801 10.1145/3219819.3219909
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. 2014. Intriguing properties of neural networks. In International Conference on Learning Representations.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. abs/1706.03762
Wang, Y. and Bansal, M. 2018. Robust machine comprehension models via adversarial training. In Robust machine comprehension models via adversarial training. abs/1804.06473
Jin, Yoo, Y., and Qi, Y. 2021. Towards improving adversarial training of nlp models. In Towards improving adversarial training of nlp models.
Zhang, X., Zhao, J., and Lecun, Y. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems.
Zhao, Z., Dua, D., and Singh, S. 2017. Generating natural adversarial examples. In Generating natural adversarial examples. abs/1710.11342
Zhou, Y., Jiang, J., Chang, K., and Wang, W. 2019. Learning to discriminate perturbations for blocking adversarial attacks in text classification. In Learning to discriminate perturbations for blocking adversarial attacks in text classification. abs/1909.03084