Problems in Past Evaluations

A common issue occurs that can potentially impact the validity of results is filtering the set of systems to be evaluated via automatic metric scores. Since metric scores are known to be a poor substitute for human assessment, this only results in the pos-sibility that the best system according to human judges is inadvertently filtered out at this stage. For example, ConvAI2 (Dinan et al., 2019) ranked models firstly using automatic metrics before top models according to metric scores were assessed by crowd-sourced workers on Mechanical Turk, while similarly in the sixth Dialog System Technology Challenge (DSTC6) systems were filtered according to metric scores prior to human evaluation.

In terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.

Despite challenges, competitions that operate in the public domain, making data and evaluation techniques available to researchers (such as ourselves) should be applauded for such efforts.

On the other hand, competitions that (for one reason or another) do not release data and evaluation techniques into the public domain have reported relative success in terms of human evaluation. However until such methods can be accessed and independently verified through replication studies, they will unfortunately have little impact . The first Amazon Alexa Socialbot Grand Challenge required human assessors to score how coherent and engaging conversations were on a 1-5 rating scale by two distinct groups: volunteer Amazon employees (experts), and general Alexa users (crowds) (Ram et al., 2018), are reported to achieve a correlation of overall scores for the two types of human assessors at 0.93. The absolute average rating across all chatbots was reported to be 20% lower for experts compared to general users. In an additional effort to evaluate models, conversational user experience, coherence, engagement, domain coverage, topical diversity, and conversational depth were assessed (1-5 scale), with combined scores reported to correlate with those of general users at r = 0.66. In addition to methods and data not being publicly available, correlations are difficult to interpret since no detail is provided about the number of judgments on which the correlation is calculated for example.

In addition to competitions that generally aim to include human evaluation of systems, automatic metrics are often proposed for dialogue evaluation, themselves requiring a human evaluation data set on which to evaluate the proposed metric. How-ever, inappropriate statistics are often applied. For example, Pang et al. (2020) propose a holistic metric to automatically evaluate four distinct aspects of dialogue, and a human evaluation experiment is deployed on Mechanical Turk using a 1-5 rating scale. The mean correlation between human assessors is reported as r = 0.61. However, mean correlations are unfortunately difficult to interpret, since correlation coefficients are not additive , averages calculated in the usual way cannot be assumed to reflect central tendency, and unfortunately, the distribution of correlations is not reported (Alexander, 1990). Mehri and Eskenazi (2020b) propose USR (Un-Supervised and Reference-free), an unsupervised model that predicts the quality of dialog for a range of criteria using various rating scales: understandable (0-1 rating scale), natural (1-3), maintains context (1-3), interesting (1-3), uses knowledge (0-1); overall quality (1-5). Despite human evaluation being carried out by experts inter-annotator agreement levels varied depending on criteria being measured, ranging from as low as 0.298. Additionally, although correlations between human assessments are reported as significant at p < 0.01, despite such statistics often being reported for correlations, they are unfortunately not very meaningful in terms of their impact on correlation interpretation and can be somewhat misleading. Contrary to common expectations, even small effect sizes (low r) can produce very low p-values (strong significance) in such tests. Aiming to achieve a significant correlation is an extremely low bar to reach in terms of consistency, since a low p-value in this case simply rejects the null hypothesis that the correlation is zero.

In addition to the above issues, human evaluation of dialogue systems rarely take into account the fact that differences in performance can occur simply by chance. The method of human evaluation we propose provides a means of applying standard tests for statistical significance to avoid concluding differences that are highly likely to have occurred simply by chance.

 References: 
Ralph, A. 1990. A note on averaging correlations. In Bulletin of the Psychonomic Society. pp. 335--336
Alexandrov, A. 2010. Characteristics of singleitem measures in likert scale format. In The Electronic Journal of Business Research Methods. pp. 1--12
Barrault, L., Biesialska, M., Bojar, O., Costa-Jussà, M. R., Federmann, C., Graham, Y., Grundkiewicz, R., Haddow, B., Huck, M., Joanis, E., Kocmi, T., Koehn, P., Lo, C., Ljubešić, N., Monz, C., Morishita, M., Nagata, M., and Nakazawa, T. Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. In Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. pp. 1--54
Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., Soricut, R., Specia, L., and Tamchyna, A. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation. pp. 12--58
Bojar, O., Buck, C., Callison-Burch, C., Federmann, C., Haddow, B., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2013. Proceedings of the Eighth Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation. pp. 1--44
Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation. pp. 10--51
Callison-Burch, C., Koehn, P., Monz, C., and Zaidan, O. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 22--64
Denkowski, M. and Lavie, A. 2011. Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 85--91
Dinan, E., Logacheva, V., Malykh, V., Miller, A. H., Shuster, K., Urbanek, J., Kiela, D., Szlam, A., Serban, I., and Lowe, R. 2019.
Corr abs/1902.00098
Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J. 2018. Wizard of wikipedia: Knowledge-powered conversational agents. In Wizard of wikipedia: Knowledge-powered conversational agents. abs/1811.01241
Finch, S. E. and Choi, J. D. 2020. Towards unified dialogue system evaluation: A comprehensive analysis of current evaluation protocols. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 236--245
Graham, Y., Baldwin, T., Moffat, A., and Zobel, J. 2013. Crowd-sourcing of human judgments of machine translation fluency. In Proceedings of the Australasian Language Technology Association Workshop 2013 (ALTA 2013). pp. 16--24
Hochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735
Humeau, S., Shuster, K., Lachaux, M., and Weston, J. 1905. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. In Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring.
Lasecki, W. S., Teevan, J., and Kamar, E. 2014. Information extraction and manipulation threats in crowd-powered systems. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing, CSCW '14. pp. 248--256 10.1145/2531602.2531733
Lin, C. and Hovy, E. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology. pp. 71--78
Liu, Q., Ihler, A. T., and Steyvers, M. 2013. Scoring workers in crowdsourcing: How many control questions are enough?. In Advances in Neural Information Processing Systems.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.
Miller, A. H., Fisch, A., Dodge, J., Amir-Hossein, Karimi, A., Bordes, J., and Weston 2016. Key-value memory networks for directly reading documents. In Key-value memory networks for directly reading documents.
Pang, B., Nijkamp, E., Han, W., Zhou, L., Liu, Y., and Tu, K. 2020. Towards holistic and automatic evaluation of open-domain dialogue generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 3619--3629 10.18653/v1/2020.acl-main.333
Papineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. pp. 311--318 10.3115/1073083.1073135
Ram, A., Prasad, R., Khatri, C., Venkatesh, A., Gabriel, R., Liu, Q., Nunn, J., Hedayatnia, B., Cheng, M., Nagar, A., and King, E. 2018. Gene Hwang, and Art Pettigrue. In Gene Hwang, and Art Pettigrue.
Sorodoc, I., Lau, J. H., Aletras, N., and Baldwin, T. 2017. Multimodal topic labelling. In Proceedings of the 15th Conference of the European Chapter. pp. 701--706
Valencia, S. Association for Computational Linguistics. In Association for Computational Linguistics.
Sutskever, I., Vinyals, O., Quoc, V., and Le 2014. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems. pp. 3104--3112
Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., and Wang, W. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. In Oriol Vinyals. abs/1609.08144
Zhang, Y., Sun, S., Galley, M., Chen, Y., Brockett, C., Gao, X., Gao, J., Liu, J., and Dolan, B. 2020. DIALOGPT : Largescale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. pp. 270--278 10.18653/v1/2020.acl-demos.30