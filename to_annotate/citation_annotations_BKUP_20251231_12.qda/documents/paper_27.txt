Introduction

Given some text (typically, a sentence) t mentioning an entity pair (e 1 , e 2 ), the goal of relation extraction (RE) is to predict the relationships between e 1 and e 2 that can be inferred from t. Let B(e 1 , e 2 ) denote the set of all sentences (bag) in the corpus mentioning e 1 and e 2 and let R(e 1 , e 2 ) denote all relations from e 1 to e 2 in a KB. Distant supervision (DS) trains RE models given B(e 1 , e 2 ) and R(e 1 , e 2 ), without sentence level annotation (Mintz et al., 2009). Most DS-RE models use the "at-least one" assumption: ∀r ∈ R(e 1 , e 2 ), ∃t r ∈ B(e 1 , e 2 ) such that t r expresses (e 1 , r, e 2 ).

Recent neural approaches to DS-RE encode each sentence t ∈ B(e 1 , e 2 ) and then aggregate sentence embeddings using an aggregation operator -the common operator being intra-bag attention (Lin et al., 2016). Various models differ in their approach to encoding (e.g., PCNNs, GCNs, BERT) and their loss functions (e.g., contrastive learning, MLM), but agree on the design choice of encoding each sentence independently of the others (Vashishth et al., 2018;Alt et al., 2019;Christou and Tsoumakas, 2021;Chen et al., 2021). We posit that this choice leads to a suboptimal usage of the available data -information from other sentences might help in better encoding a given sentence.

We explore this hypothesis by developing a simple baseline solution. We first construct a passage P (e 1 , e 2 ) by concatenating all sentences in B(e 1 , e 2 ). We then encode the whole passage through BERT (Devlin et al., 2019) (or mBERT for multilingual setting). This produces a contextualized embedding of every token in the bag. To make these embeddings aware of the candidate relation, we take a (trained) relation query vector, r, to generate a relation-aware summary of the whole passage using attention. This is then used to predict whether (e 1 , r, e 2 ) is a valid prediction.

Despite its simplicity, our baseline has some conceptual advantages. First, each token is able to exchange information with other tokens from other sentences in the bag -so the embeddings are likely more informed. Second, in principle, the model may be able to relax a part of the at-least-one assumption. For example, if no sentence individually expresses a relation, but if multiple facts in different sentences collectively predict the relation, our model may be able to learn to extract that.

We name our baseline model Passage-Attended Relation Extraction, PARE (mPARE for multilingual DS-RE). We experiment on four DS-RE datasets -three in English, NYT-10d (Riedel et al., 2010), NYT-10m, and Wiki-20m (Gao et al., 2021), and one multilingual, DiS-ReX (Bhartiya et al., 2021). We find that in all four datasets, our proposed baseline significantly outperforms existing state of the art, yielding up to 5 point AUC gain. Further attention analysis and ablations provide additional insight into model performance. We release our code for reproducibility. We believe that our work represents a simple but strong baseline that can form the basis for further DS-RE research.

 References: 
Alt, C., Hübner, M., and Hennig, L. 2019. Fine-tuning pre-trained transformer language models to distantly supervised relation extraction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 1388--1398 10.18653/v1/P19-1134
Bhartiya, A., Badola, K., and Mausam 2021. Dis-rex: A multilingual dataset for distantly supervised relation extraction. In Dis-rex: A multilingual dataset for distantly supervised relation extraction.
Australia 2018. Association for Computational Linguistics. In Association for Computational Linguistics. pp. 2137--2147
Radford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.
Marco Tulio Ribeiro, T., Wu, C., Guestrin, S., and Singh 2020. Beyond accuracy: Behavioral testing of nlp models with checklist. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4902--4912
Riedel, S., Yao, L., and Mccallum, A. 2010. Modeling relations and their mentions without labeled text. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. pp. 148--163
Ritter, A., Zettlemoyer, L., Mausam, and Etzioni, O. 2013. Modeling missing data in distant supervision for information extraction. In Trans. Assoc. Comput. Linguistics. pp. 367--378
Surdeanu, M., Tibshirani, J., Nallapati, R., and Manning, C. D. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. pp. 455--465
Vashishth, S., Joshi, R., Sai Suman Prayaga, C., Bhattacharyya, P., and Talukdar 2018. RESIDE: Improving distantly-supervised neural relation extraction using side information. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 1257--1266 10.18653/v1/D18-1157
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., and Funtowicz, M. 2019. Huggingface's transformers: State-of-the-art natural language processing. In Huggingface's transformers: State-of-the-art natural language processing. arXiv:1910.03771
Zeng, D., Liu, K., and Chen, Y. 2015. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of the 2015 conference on empirical methods in natural language processing. pp. 1753--1762
Wiki-20m
Dis-Rex
Auc M-F1 Auc P@m Auc M-F1 Auc M-F1