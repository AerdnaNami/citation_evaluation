Related Work

Named Entity Recognition The mainstream NER systems are designed to recognize flat entities and based on a sequence tagging framework. Collobert et al. (2011) introduced the linear-chain conditional random field (CRF) into neural networkbased sequence tagging models, which can explicitly encode the transition likelihoods between adjacent tags. Many researchers followed this work, and employed LSTM as the encoder. In addition, character-level representations are typically used for English tasks (Huang et al., 2015;Lample et al., 2016;Ma and Hovy, 2016;Chiu and Nichols, 2016), whereas lexicon information is helpful for Chinese NER (Zhang and Yang, 2018;Ma et al., 2020;Li et al., 2020a).

Nested NER allows a token to belong to multiple entities, which conflicts with the plain sequence tagging framework. Ju et al. (2018) proposed to use stacked LSTM-CRFs to predict from inner to outer entities. Straková et al. (2019) concatenated the BILOU tags for each token inside the nested entities, which allows the LSTM-CRF to work as for flat entities. Li et al. (2020b) reformulated nested NER as a machine reading comprehension task. Shen et al. (2021) proposed to recognize nested entities by the two-stage object detection method widely used in computer vision.

Recent years, a body of literature emerged on span-based models, which were compatible with both flat and nested entities, and achieved SOTA performance (Eberts and Ulges, 2020;Yu et al., 2020;Li et al., 2021). These models typically enumerate all possible candidate text spans and then classify each span into entity types. In this work, the biaffine model (Yu et al., 2020) is chosen and re-implemented with slight modifications as our baseline, because of its high performance and compatibility with boundary smoothing.

In addition, pretrained language models, also known as contextualized embeddings, were also widely introduced to NER models, and significantly boosted the model performance (Peters et al., 2018;Devlin et al., 2019). They are used in our baseline by default.

Label Smoothing Szegedy et al. (2016) proposed the label smoothing as a regularization technique to improve the accuracy of the Inception networks on ImageNet. By explicitly assigning a small probability to non-ground-truth labels, label smoothing can prevent the models from becoming too confident about the predictions, and thus improve generalization. It turned out to be a useful alternative to the standard cross entropy loss, and has been widely adopted to fight against the over-confidence (Zoph et al., 2018;Chorowski and Jaitly, 2017;Vaswani et al., 2017), improve the model calibration (Müller et al., 2019), and denoise incorrect labels (Lukasik et al., 2020).

Our proposed boundary smoothing applies the smoothing technique to entity boundaries, rather than labels. This is driven by the observation that entity boundaries are more ambiguous and inconsistent to annotate in NER engineering. To the best of our knowledge, this study is the first that focuses on the effect of smoothing regularization on NER models.

 References: 
Akbik, A., Blythe, D., and Vollgraf, R. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics. pp. 1638--1649
Che, W., Wang, M., Manning, C. D., and Liu, T. 2013. Named entity recognition with bilingual constraints. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 52--62
Chen, C. and Kong, F. 2021. Enhancing entity boundary detection for better Chinese named entity recognition. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 20--25 10.18653/v1/2021.acl-short.4
Jason, P. C., Chiu, E., and Nichols 2016. Named entity recognition with bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics. In Named entity recognition with bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics. pp. 357--370 10.1162/tacl_a_00104
Chorowski, J. and Jaitly, N. 2017. Towards better decoding and language model integration in sequence to sequence models. In INTERSPEECH 2017. pp. 523--527
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. 2011. Natural language processing (almost) from scratch. In Journal of Machine Learning Research. pp. 2493--2537
Cui, Y., Che, W., Liu, T., Qin, B., Wang, S., and Hu, G. 2020. Revisiting pretrained models for Chinese natural language processing. In Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 657--668 10.18653/v1/2020.findings-emnlp.58
Cui, Y., Che, W., Liu, T., Qin, B., Yang, Z., Wang, S., and Hu, G. 2019. Pre-training with whole word masking for Chinese BERT. In Pre-training with whole word masking for Chinese BERT. arXiv:1906.08101
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423
Eberts, M. and Ulges, A. 2020. Span-based joint entity and relation extraction with Transformer pre-training. In Proceedings of the 24th European Conference on Artificial Intelligence.
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.Q. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning. pp. 1321--1330
Hochreiter, S. and Schmidhuber, J. 1997. Flat minima. In Neural Computation. pp. 1--42
Hornik, K., Stinchcombe, M., and White, H. 1989. Multilayer feedforward networks are universal approximators. In Neural networks. pp. 359--366
Huang, Z., Xu, W., and Yu, K. 2015. Bidirectional lstm-crf models for sequence tagging. In Bidirectional lstm-crf models for sequence tagging. arXiv:1508.01991
Jiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., and Bengio, S. 2019. Fantastic generalization measures and where to find them. In Fantastic generalization measures and where to find them. arXiv:1912.02178
Ju, M., Miwa, M., and Ananiadou, S. 2018. A neural layered model for nested named entity recognition. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1446--1459 10.18653/v1/N18-1131
Katiyar, A. and Cardie, C. 2018. Nested named entity recognition revisited. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 861--871 10.18653/v1/N18-1079
Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., and Dyer, C. 2016. Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 260--270 10.18653/v1/N16-1030
Levow, G. 2006. The third international Chinese language processing bakeoff: Word segmentation and named entity recognition. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing. pp. 108--117
Li, F., Lin, Z., Zhang, M., and Ji, D. 2021. A span-based model for joint overlapped and discontinuous named entity recognition. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 4814--4828 10.18653/v1/2021.acl-long.372
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. 2018. Visualizing the loss landscape of neural nets. In Proceedings of the 32nd International Conference on Neural Information Processing Systems. pp. 6391--6401
Li, X., Yan, H., Qiu, X., and Huang, X. 2020. FLAT: Chinese NER using flatlattice transformer. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 6836--6842 10.18653/v1/2020.acl-main.611
Li, X., Feng, J., Meng, Y., Han, Q., Wu, F., and Li, J. 2020. A unified MRC framework for named entity recognition. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5849--5859 10.18653/v1/2020.acl-main.519
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. RoBERTa: A robustly optimized BERT pretraining approach. In RoBERTa: A robustly optimized BERT pretraining approach. arXiv:1907.11692
Loshchilov, I. and Hutter, F. 2018. Decoupled weight decay regularization. In International Conference on Learning Representations.
Lu, W. and Roth, D. 2015. Joint mention extraction and classification with mention hypergraphs. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 857--867 10.18653/v1/D15-1102
Lukasik, M., Bhojanapalli, S., Menon, A., and Kumar, S. 2020. Does label smoothing mitigate label noise?. In Proceedings of the 37th International Conference on Machine Learning. pp. 6448--6458
Ma, R., Peng, M., Zhang, Q., Wei, Z., and Huang, X. 2020. Simplify the usage of lexicon in Chinese NER. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5951--5960 10.18653/v1/2020.acl-main.528
Ma, X. and Hovy, E. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pp. 1064--1074 10.18653/v1/P16-1101
Müller, R., Kornblith, S., and Hinton, G. E. 2019. When does label smoothing help?. In Advances in Neural Information Processing Systems.
Pascanu, R., Mikolov, T., and Bengio, Y. 2013. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning. pp. 1310--1318
Peng, N. and Dredze, M. 2015. Named entity recognition for Chinese social media with jointly trained embeddings. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 548--554 10.18653/v1/D15-1064
Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/N18-1202
Sameer Pradhan, A., Moschitti, N., Xue, H.T., Ng, A., Björkelund, O., Uryupina, Y., Zhang, Z., and Zhong 2013. Towards robust linguistic analysis using OntoNotes. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning. pp. 143--152
Shen, Y., Ma, X., Tan, Z., Zhang, S., Wang, W., and Lu, W. 2021. Locate and label: A two-stage identifier for nested named entity recognition. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 2782--2794 10.18653/v1/2021.acl-long.216
Straková, J. and Straka, M. 2019. Neural architectures for nested NER through linearization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 5326--5331 10.18653/v1/P19-1527
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. 2016. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2818--2826
Tjong, E. F., Sang, K., and Veenstra, J. 1999. Representing text chunks. In Ninth Conference of the European Chapter of the Association for Computational Linguistics. pp. 173--179
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. pp. 5998--6008
Wang, Z., Shang, J., Liu, L., Lu, L., Liu, J., and Han, J. 2019. CrossWeigh: Training named entity tagger from imperfect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5154--5163 10.18653/v1/D19-1519
Wu, S., Song, X., and Feng, Z. 2021. MECT: Multi-metadata embedding based crosstransformer for Chinese named entity recognition. In MECT: Multi-metadata embedding based crosstransformer for Chinese named entity recognition. 10.18653/v1/2021.acl-long.121