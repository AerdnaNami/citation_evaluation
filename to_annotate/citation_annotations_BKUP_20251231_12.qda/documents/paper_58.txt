Related Work

Vision-and-language navigation. Vision-and-Language Navigation (VLN) requires an agent to find the routes to the desired target based on instructions Thomason et al., 2020;Nguyen and Daumé III, 2019;Chen et al., 2019;Krantz et al., 2020). Specifically, there are two key challenges in VLN: grounding the natural language instruction to visual environments and generalizing to unseen environments. To address the first challenge, one line of research in VLN utilizes carefully designed cross-modal attention modules (Wang et al., , 2019aTan et al., 2019;Landi et al., 2019;Xia et al., 2020;Wang et al., 2020b,a;Zhu et al., 2020;Zhu et al., 2021;, progress monitor modules (Ma et al., 2019b,a;Ke et al., 2019), and object-action aware modules (Qi et al., 2020a). Another line of research improves vision and language co-grounding by improving vision and language representations with pre-training techniques (Li et al., 2019;Huang et al., 2019b;Hong et al., 2021). Li et al. (2019) directly adopts pre-trained BERT for encoding instructions,  and Hong et al. (2021) learn from a large amount of image-textaction triplets,  learns from large amount of text-image pairs from the web, and Huang et al. (2019b) transfers language and visual representation to in-domain representation with auxiliary tasks. Different from them, we utilize the visually-aligned multilingual instructions to learn a cross-lingual language representation that inherently captures visual semantics underlying the instruction.

Multiple methods have been proposed to encourage generalization to unseen environments during training (Zhang et al., 2020;Tan et al., 2019;Wang et al., 2020c;Fu et al., 2020). Zhang et al. (2020) demonstrates that it is the low-level appearance information that causes the large performance gap between seen and unseen environments. Tan et al. (2019) proposes to use environment dropout on visual features to create new environments and Fu et al. (2020) utilizes adversarial path sampling to encourage generalization. However, both of these methods rely on a speaker module to generate synthetic training data and can be considered as data augmentation methods, which are complementary to our proposed environment-agnostic visual representation. The closest work to ours is Wang et al. (2020c), where they proposes to pair an environment classifier with gradient reversal layer to learn an environment-agnostic representation. However, they only consider one single environment when learning the visual representation for a given path (i.e., given one path and predict its environment). In our environment-agnostic representation learning, we explore the connections between multiple environments (i.e., maximize the similarity between paths from different environments). Vision-and-language with multilinguality. There has been growing interest in combining vision and language for tasks such as visual-guided machine translation (Sigurdsson et al., 2020;Surís et al., 2020;Huang et al., 2020), multi-lingual visual question answering (Gao et al., 2015;Gupta et al., 2020;Shimizu et al., 2018), multi-lingual image captioning (Gu et al., 2018;Lan et al., 2017), multilingual video captioning (Wang et al., 2019b), and multi-lingual image-sentence retrieval Burns et al., 2020). In this paper, we work on multi-lingual vision-and-language navigation. We use vision (i.e., navigation path) as a bridge between multi-lingual instructions and learn a crosslingual representation that captures visual concepts. Furthermore, our approach also use language as a bridge between different visual environments to learn an environment-agnostic visual representation.

 References: 
Aharoni, R., Johnson, M., and Firat, O. 2019. Massively multilingual neural machine translation. In Massively multilingual neural machine translation. arXiv:1903.00089
An, D., Qi, Y., Huang, Y., Wu, Q., Wang, L., and Tan, T. 2021. Neighbor-view enhanced model for vision and language navigation. In Neighbor-view enhanced model for vision and language navigation. arXiv:2107.07201
Anderson, P., Chang, A., Singh Chaplot, D., Dosovitskiy, A., Gupta, S., Koltun, V., Kosecka, J., and Malik, J. Manolis Savva, et al. 2018a. On evaluation of embodied navigation agents. In Manolis Savva, et al. 2018a. On evaluation of embodied navigation agents. arXiv:1807.06757
Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sünderhauf, N., Reid, I., Gould, S., Van Den, A., and Hengel 2018. Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3674--3683
Burns, A., Kim, D., Wijaya, D., Saenko, K., and Plummer, B.A. 2020. Learning to scale multilingual representations for visionlanguage tasks. In European Conference on Computer Vision. pp. 197--213
Chen, H., Suhr, A., Misra, D., Snavely, N., and Artzi, Y. 2019. Touchdown: Natural language navigation and spatial reasoning visual street environments. In CVPR. pp. 12538--12547
Chen, S., Guhur, P., Schmid, C., and Laptev, I. 2021. History aware multimodal transformer for vision-and-language navigation. In Advances in Neural Information Processing Systems. pp. 34
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL. pp. 4171--4186 10.18653/v1/N19-1423
Dosovitskiy, A., Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, M., Dehghani, M., Minderer, G., Heigold, and Gelly 2020. An image is worth 16× 16 words: Transformers for image recognition at scale. arxiv 2020. In An image is worth 16× 16 words: Transformers for image recognition at scale. arxiv 2020. arXiv:2010.11929
Fried, D., Hu, R., Cirik, V., Rohrbach, A., Andreas, J., Morency, L., Berg-Kirkpatrick, T., Saenko, K., Klein, D., and Darrell, T. 2018. Speaker-follower models for vision-and-language navigation. In Advances in Neural Information Processing Systems. pp. 3314--3325
Fu, T., Wang, X. E., Matthew, F., Peterson, Scott, T., Grafton, M. P., Eckstein, W. Y., and Wang 2020. Counterfactual vision-and-language navigation via adversarial path sampler. In European Conference on Computer Vision. pp. 71--86
Gao, H., Mao, J., Zhou, J., Huang, Z., Wang, L., and Xu, W. 2015. Are you talking to a machine? dataset and methods for multilingual image question answering. In Are you talking to a machine? dataset and methods for multilingual image question answering. arXiv:1505.05612
Gao, T., Yao, X., and Chen, D. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In EMNLP.
Gu, J., Joty, S., Cai, J., and Wang, G. 2018. Unpaired image captioning by language pivoting. In Proceedings of the European Conference on Computer Vision (ECCV). pp. 503--519
Gupta, A., Dollar, P., and Girshick, R. 2019. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5356--5364
Gupta, D. and Lenka, P. Asif Ekbal, and Pushpak Bhattacharyya. 2020. A unified framework for multilingual and code-mixed visual question answering. In ACL. pp. 900--913
Hao, W., Li, C., Li, X., Carin, L., and Gao, J. 2020. Towards learning a generic agent for vision-and-language navigation via pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13137--13146
He, K., Gkioxari, G., Dollár, P., and Girshick, R. 2017. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision. pp. 2961--2969
He, K., Zhang, X., Ren, S., and Sun, J. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770--778
Hinton, G., Srivastava, N., and Swersky, K. 2012. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. In Cited on. pp. 14
Hong, Y., Wu, Q., Qi, Y., Rodriguez-Opazo, C., and Gould, S. 2021. A recurrent visionand-language bert for navigation. In CVPR.
Huang, H., Jain, V., Mehta, H., Baldridge, J., and Ie, E. 1905. Multi-modal discriminative model for vision-and-language navigation. In Multi-modal discriminative model for vision-and-language navigation.
Huang, H., Jain, V., Mehta, H., Ku, A., Magalhaes, G., Baldridge, J., and Ie, E. 2019. Transferable representation learning in vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7404--7413
Huang, P., Hu, J., Chang, X., and Hauptmann, A. 2020. Unsupervised multimodal neural machine translation with pseudo visual pivoting. In Unsupervised multimodal neural machine translation with pseudo visual pivoting. arXiv:2005.03119
Jain, V., Magalhaes, G., Ku, A., Vaswani, A., Ie, E., and Baldridge, J. 2019. Stay on the path: Instruction fidelity in vision-andlanguage navigation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 1862--1872 10.18653/v1/P19-1181
Ke, L., Li, X., Bisk, Y., Holtzman, A., Gan, Z., Liu, J., Gao, J., Choi, Y., and Srinivasa, S. 2019. Tactical rewind: Selfcorrection via backtracking in vision-and-language navigation. In CVPR. pp. 6741--6749
Kim, D., Saito, K., Saenko, K., Sclaroff, S., and Plummer, B. 2020. Mule: Multimodal universal language embedding. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 11254--11261
Krantz, J., Wijmans, E., Majumdar, A., Batra, D., and Lee, S. 2020. Beyond the nav-graph: Vision-and-language navigation in continuous environments. In ECCV. pp. 104--120
Ku, A., Anderson, P., Patel, R., Ie, E., and Baldridge, J. 2020. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. In EMNLP. pp. 4392--4412 10.18653/v1/2020.emnlp-main.356
Lan, W., Li, X., and Dong, J. 2017. Fluency-guided cross-lingual image captioning. In Proceedings of the 25th ACM international conference on Multimedia. pp. 1549--1557
Landi, F., Baraldi, L., Cornia, M., Corsini, M., and Cucchiara, R. 2019. Perceive, transform, and act: Multi-modal attention networks for vision-and-language navigation. In Perceive, transform, and act: Multi-modal attention networks for vision-and-language navigation. arXiv:1911.12377
Li, J., Tan, H., and Bansal, M. 2021. Improving cross-modal alignment in vision language navigation via syntactic information. In ACL. pp. 1041--1050 10.18653/v1/2021.naacl-main.82
Li, X., Li, C., Xia, Q., Bisk, Y., Çelikyilmaz, A., Gao, J., Smith, N. A., and Choi, Y. 2019. Robust navigation with language pretraining and stochastic sampling. In EMNLP/IJCNLP.
Loshchilov, I. and Hutter, F. 2017. arXiv:1711.05101
Ma, C., Lu, J., Wu, Z., Alregib, G., Kira, Z., Socher, R., and Xiong, C. 2019. Self-monitoring navigation agent via auxiliary progress estimation. In Proceedings of the International Conference on Learning Representations.
Ma, C., Wu, Z., Alregib, G., Xiong, C., and Kira, Z. 2019. The regretful agent: Heuristic-aided navigation through progress estimation. In CVPR. pp. 6732--6740
Magalhaes, G., Jain, V., Ku, A., Ie, E., and Baldridge, J. 2019. Effective and general evaluation for instruction conditioned navigation using dynamic time warping. In Effective and general evaluation for instruction conditioned navigation using dynamic time warping. arXiv:1907.05446
Majumdar, A., Shrivastava, A., Lee, S., Anderson, P., Parikh, D., and Batra, D. 2020. Improving vision-and-language navigation with imagetext pairs from the web. In ECCV.
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. 2016. Asynchronous methods for deep reinforcement learning. In International conference on machine learning. pp. 1928--1937
Nguyen, K., Daumé, H., and Iii 2019. Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. In EMNLP-IJCNLP. pp. 684--695 10.18653/v1/D19-1063
Pratap, V., Sriram, A., Tomasello, P., Hannun, A., Liptchinsky, V., Synnaeve, G., and Collobert, R. 2020. Massively multilingual asr: 50 languages, 1 model, 1 billion parameters. In Massively multilingual asr: 50 languages, 1 model, 1 billion parameters. arXiv:2007.03001
Qi, Y., Zizheng Pan, S., Zhang, A. V.D., Hengel, Q., and Wu 2020. Object-and-action aware model for visual language navigation. In ECCV.
Qi, Y., Wu, Q., Anderson, P., Wang, X., Wang, W. Y., Shen, C., Van Den, A., and Hengel 2020. Reverie: Remote embodied visual referring expression in real indoor environments. In CVPR. pp. 9982--9991
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., and Agarwal, S. Learning transferable visual models from natural language supervision. In Image. pp. T2
Russakovsky, O., Deng, J., Su, H., Krause, S., Satheesh, S., Ma, Z., Huang, A., Karpathy, A., Khosla, M., and Bernstein 2015. Imagenet large scale visual recognition challenge. In International journal of computer vision. pp. 211--252
Shen, S., Li, L. H., Tan, H., Bansal, M., Rohrbach, A., Chang, K., Yao, Z., and Keutzer, K. 2021. How much can clip benefit vision-and-language tasks? arXiv preprint. In How much can clip benefit vision-and-language tasks? arXiv preprint. arXiv:2107.06383
Shimizu, N., Rong, N., and Miyazaki, T. 2018. Visual question answering dataset for bilingual image understanding: A study of cross-lingual transfer using attention maps. In Proceedings of the 27th International Conference on Computational Linguistics. pp. 1918--1928
Gunnar, A., Sigurdsson, J., Alayrac, A., Nematzadeh, L., Smaira, M., Malinowski, J., Carreira, P., Blunsom, A., and Zisserman 2020. Visual grounding in video for unsupervised word translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10850--10859
Surís, D., Epstein, D., and Vondrick, C. 2020. Globetrotter: Unsupervised multilingual translation from visual alignment. In Globetrotter: Unsupervised multilingual translation from visual alignment. arXiv:2012.04631
Hao Tan, L., Yu, M., and Bansal 2019. Learning to navigate unseen environments: Back translation with environmental dropout. In NAACL. pp. 2610--2621
Thomason, J., Murray, M., Cakmak, M., and Zettlemoyer, L. 2019. Vision-and-dialog navigation. In Vision-and-dialog navigation.
Thomason, J., Murray, M., Cakmak, M., and Zettlemoyer, L. 2020. Vision-and-dialog navigation. In Conference on Robot Learning. pp. 394--406
Wang, H., Wang, W., Shu, T., Liang, W., and Shen, J. 2020. Active visual information gathering for vision-language navigation. In ECCV.
Wang, H., Wu, Q., and Shen, C. 2020. Soft expert reward learning for vision-and-language navigation. In Soft expert reward learning for vision-and-language navigation. arXiv:2007.10835
Wang, X., Huang, Q., Celikyilmaz, A., Gao, J., Shen, D., Wang, Y., Wang, W. Y., and Zhang, L. 2019. Reinforced crossmodal matching and self-supervised imitation learning for vision-language navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6629--6638
Wang, X., Jain, V., Ie, E., Wang, W. Y., Kozareva, Z., and Ravi, S. 2020. Environment-agnostic multitask learning for natural language grounded navigation. In Environment-agnostic multitask learning for natural language grounded navigation. arXiv:2003.00443
Wang, X., Wu, J., Chen, J., Li, L., Wang, Y., and Wang, W. Y. 2019. Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4581--4591
Wang, X., Xiong, W., Wang, H., and Wang, W. 2018. Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation. In ECCV.
Wu, Y., Kirillov, A., Massa, F., Lo, W., and Girshick, R. 2019.
Xia, Q., Li, C., Li, Y., Bisk, Z., Sui, Y., Choi, N. A., and Smith 2003. Multi-view learning for vision-and-language navigation. In ArXiv.
Zhang, Y., Tan, H., and Bansal, M. 2020. Diagnosing the environment bias in vision-and-language navigation. In IJCAI 2020. pp. 890--897 10.24963/ijcai.2020/124
Zhu, F., Liang, X., Zhu, Y., Yu, Q., Chang, X., and Liang, X. 2021. Soon: Scenario oriented object navigation with graph-based exploration. In CVPR. pp. 12689--12699
Zhu, W., Hu, H., Chen, J., Deng, Z., Jain, V., Ie, E., and Sha, F. 2020. Babywalk: Going farther in vision-and-language navigation by taking baby steps. In ACL. pp. 2539--2556