Related Work

From the period of statistical machine translation (SMT), IMT has been widely exploited to reduce human’s effort by using the human’s feedback to help models to do translation (Foster et al., 1997; Langlais et al., 2000; Simard et al., 2007; Barrachina et al., 2009; González-Rubio et al., 2013; Cheng et al., 2016). Recently, with the development of NMT (Bahdanau et al., 2015; Vaswani et al., 2017), researchers turned to employing IMT on it. A classical type of IMT uses a left-to-right sentence completing framework proposed in Langlais et al. (2000), in which human translators can only do revisions on the translation generated by models from left to right. Generally, the text portion from the beginning to the current modified part is called prefix, and the system will generate a new translation based on the given prefix (SanchisTrilles et al., 2014; Peris et al., 2017a; Knowles and Koehn, 2016).
Cheng et al. (2016) propose a pick-revise framework that enables translators do revisions on arbitrary positions to improve efficiency. Huang et al. (2021) allow users to make any interaction on random position by using LCD (Hokamp and Liu, 2017; Post and Vilar, 2018), algorithms in the decoding stage which can integrate lexical constraints into translation. However, LCD can not achieve a win-win of decoding speed and translation quality. Weng et al. (2019) propose a bidirectional IMT framework also based on LCD, which could fix minor mistakes left to the revisions by doing two constrained decoding processes with opposite directions in tandem. However, it needs to train two 
decoders, and in each constrained decoding process, the model can only use part of the constraints supplied by translators, making it inefficient both in using human knowledge and decoding speed. But BiTIIMT constructs all constraints into a template as part of the input, which makes it possible for models to use all human knowledge at the same time to fix minor mistakes automatically in the whole sentence. Another series of works (Alkhouli et al., 2019; Song et al., 2020; Chen et al., 2021) apply alignment information to improve the decoding efficiency of LCD. Alkhouli et al. (2019) use alignment extracted by vanilla transformer, which is reported by Garg et al. is poor. Song et al. (2020) need an external aligner to train the alignment module. These works can only do constrained decoding based on a dictionary-style constraint pair, which means a burden for human translators.


 References: 
Alkhouli, T., Bretschner, G., and Ney, H. 2019. On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation. In Proceedings of the Third Conference on Machine Translation: Research Papers. pp. 177--185 10.18653/v1/w18-6318
Bahdanau, D., Cho, K. H., and Bengio, Y. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings. pp. 1--15
Barrachina, S., Bender, O., Casacuberta, F., Civera, J., Cubel, E., Khadivi, S., Lagarda, A., Ney, H., Tomás, J., Vidal, E., and Vilar, J. 2009. In Statistical Approaches to Computer-Assisted Translation. Computational Linguistics. pp. 3--28 10.1162/coli.2008.07-055-R2-06-29
Berglund, M. and Leo Bidirectional recurrent neural networks as generative models. In Advances in Neural Information Processing Systems. pp. 1--10
Chen, G., Chen, Y., and Li, V.O.K. 2021. Lexically Constrained Neural Machine Translation with Explicit Alignment Guidance. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 12630--12638
Cheng, S., Huang, S., Chen, H., Dai, X., and Chen, J. 2016. PRIMT: A pick-revise framework for interactive machine translation. In 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 -Proceedings of the Conference. pp. 1240--1249 10.18653/v1/n16-1148
Dinu, G., Federico, M., and Al-Onaizan, Y. 2019. Training Neural Machine Translation To Apply Terminology Constraints. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3063--3068
Fedus, W., Goodfellow, I., and Dai, A.M. 2018. MaskGAN: Better Text Generation via Filling in the . International Conference on Learning Representations. In MaskGAN: Better Text Generation via Filling in the . International Conference on Learning Representations.
Foster, G., Isabelle, P., and Plamondon, P. 1997. Target-Text Mediated Interactive Machine Translation. In Machine Translation. pp. 175--194 10.1023/A:1007999327580
Garg, S. and Peitz, S. Jointly Learning to Align and Translate with Transformer Models. In Jointly Learning to Align and Translate with Transformer Models.
González-Rubio, J.
José, M., Benedí, F., and Casacuberta 2013. Interactive machine translation using hierarchical translation models. In EMNLP 2013 -2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference. pp. 244--254
Hokamp, C. and Liu, Q. 2017. Lexically constrained decoding for sequence generation using grid beam search. In ACL 2017 -55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers). pp. 1535--1546 10.18653/v1/P17-1141
Huang, G., Liu, L., Wang, X., Wang, L., Li, H., Tu, Z., Huang, C., and Shi, S. 2021. TranSmart: A Practical Interactive Machine Translation System. In TranSmart: A Practical Interactive Machine Translation System. pp. 1--21 arXiv:2105.13072
Diederik, P., Kingma, J. L., and Ba 2015. Adam-A method for stochastic optimization. In Adam-A method for stochastic optimization. pp. 1--15 arXiv:1412.6980
Knowles, R. and Koehn, P. 2016. Neural Interactive Translation Prediction. In Proceedings -AMTA 2016: 12th Conference of the Association for Machine Translation in the Americas. pp. 107--120
Langlais, P., Foster, G., and Lapalme, G. 2000. TransType: a Computer-Aided Translation Typing System. In Workshop: Embedded Machine Translation Systems.
Li, H., Huang, G., Cai, D., and Liu, L. 2020. Neural Machine Translation With Noisy Lexical Constraints. In Speech, and Language Processing. pp. 1 10.1109/TASLP.2020.2999724
Ng, N., Grangier, D., Auli, M., and Gross, S. 2019. A Fast. In Extensible Toolkit for Sequence Modeling. arXiv:1904.01038
Papineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. BLEU : a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311--318
Peris, Á., Cebrián, L., and Casacuberta, F. 2017. Online Learning for Neural Ma. In Online Learning for Neural Ma. pp. 1--12 arXiv:1706.03196
Peris, Á., Domingo, M., and Casacuberta, F. 2017. Interactive neural machine translation. In Computer Speech & Language. pp. 201--220 10.1016/j.csl.2016.12.003
Post, M. and Vilar, D. 2018. Fast lexically constrained decoding with dynamic beam allocation for neural machine translation. In NAACL HLT 2018 -2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. pp. 1314--1324 10.18653/v1/n18-1119
Sanchis-Trilles, G., Alabau, V., Buck, C., Carl, M., and Casacuberta, F.
Luis A Leiva, B., Mesa-Lao, D., Ortiz-Martínez, H., Saint-Amand, C., Tsoukala, E., and Vidal 2014. Interactive translation prediction versus conventional post-editing in practice: a study with the CasMaCat workbench. In Machine Translation. pp. 217--235 10.1007/s10590-014-9157-9
Sennrich, R., Haddow, B., and Birch, A. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.
Simard, M., Ueffing, N., Isabelle, P., and Kuhn, R. 2007. Rule-based translation with statistical phrase-based post-editing. In Proceedings of the Second Workshop on Statistical Machine Translation. pp. 203--206 10.3115/1626355.1626383
Song, K., Wang, K., Yu, H., Zhang, Y., Huang, Z., Luo, W., Duan, X., and Zhang, M. 2020. Alignment-enhanced transformer for constraining NMT with pre-specified translations. In AAAI 2020 -34th AAAI Conference on Artificial Intelligence. pp. 8886--8893 10.1609/aaai.v34i05.6418
Song, K., Zhang, Y., Yu, H., Luo, W., Wang, K., and Zhang, M. 2019. Code-switching for enhancing NMT with pre-specified translation. In NAACL HLT 2019 -2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. pp. 449--459
Sutskever, I., Vinyals, O., and Le, Q.V. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems. pp. 3104--3112
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. pp. 5999--6009
Weng, R., Zhou, H., Huang, S., Li, L., Xia, Y., and Chen, J. 2019. Correct-andmemorize: Learning to translate from interactive revisions. In IJCAI International Joint Conference on Artificial Intelligence. pp. 5255--5263 10.24963/ijcai.2019/730
Zhao, T., Liu, L., Huang, G., Tu, Z., Li, H., Liu, Y., Liu, G., and Shi, S. 2020. Balancing quality and human involvement: An effective approach to interactive neural machine translation. In AAAI 2020 -34th AAAI Conference on Artificial Intelligence. pp. 9660--9667 10.1609/aaai.v34i05.6514
Zhu, W., Hu, Z., and Xing, E. P. arXiv:1901.00158