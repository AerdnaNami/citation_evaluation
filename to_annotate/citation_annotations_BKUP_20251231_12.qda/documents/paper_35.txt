Introduction

Research on long-range language models (LRLMs) aims to process extremely long input sequences by making the base Transformer architecture more efficient (e.g., through sparse attention, recurrence, or cached memory). These modifications are commonly validated by training LRLMs on PG-19 (Rae et al., 2020), a long-document language modeling dataset, and demonstrating small perplexity decreases over shorter context models (Roy et al., 2021). However, recent analysis experiments (Sun et al., 2021;Press et al., 2021) show that modern LRLMs rely mostly on local context (i.e., the immediately preceding 1-2K tokens) and are insensitive to tokens earlier in the input sequence.

In this paper, we move beyond token-level perplexity by evaluating LRLMs on a task that requires a rich understanding of long-range dependencies. Our task is an instance of suffix identification, in which a language model is given a long input sequence (or prefix) and asked to disambiguate the next n-token segment from a set of hard negatives that are randomly sampled from the same narrative. To succeed at this task, an LRLM should assign high probability to the ground-truth next segment and low probability to the negatives. To specifically test long-range dependencies, we restrict our prefixes to end at chapter breaks of a longer cohesive narrative (e.g., a novel).

We construct a challenge dataset, CHAPTER-BREAK, by automatically detecting chapter boundaries within both held-out PG-19 documents (indomain for pretrained LRLMs) and works of fan fiction published on the Archive of Our Own (out of domain). We perform a detailed analysis of the types of chapter transitions in our dataset and discover a high frequency of narrative shifts in point-of-view, location, and time, all of which require global narrative understanding. For example, Figure 1 contains a complex prefix in which the time-traveling Billy Pilgrim moves between World War II, 1960s suburban life, and an alien planet. Understanding the cliffhanger ending, in which the narrative abruptly switches from a wartime scene to a 1967 alien abduction, requires an LRLM to make connective inferences using details buried far back in the context (e.g., Billy's age in 1967).

We evaluate three LRLMs on CHAPTERBREAK, including BigBird (Zaheer et al., 2020), the Routing Transformer (Roy et al., 2021), and its local attention variant, all pretrained or fine-tuned on PG-19. Our experiments show that these LRLMs perform poorly at selecting the ground-truth suffix, regardless of the length of the input sequence. As an upper bound, we train a small RoBERTa-based suffix-level language model on PG-19 and discover that it substantially outperforms all LRLMs on the task. Finally, we perform an analysis on the instances in which all models struggle to choose the correct suffix, which reveals that chapters containing shifts in location and events in focus are particularly challenging to disambiguate. Taken together, these results suggest that CHAPTERBREAK is a useful benchmark for future research into LRLMs.

 References: 
Ainslie, J., Ontanon, S., Alberti, C., Cvicek, V., Fisher, Z., Pham, P., Ravula, A., Sanghai, S., Wang, Q., and Yang, L. 2020. Etc: Encoding long and structured inputs in transformers. In Etc: Encoding long and structured inputs in transformers.
Albrecht, J. E. and Myers, J. L. 1995. Role of context in accessing distant information during reading. In Journal of experimental psychology. Learning, memory, and cognition. pp. 1459--1468
Beltagy, I., Peters, M. E., and Cohan, A. 2020. Longformer: The long-document transformer. In Longformer: The long-document transformer.
Tom B Brown, B., Mann, N., Ryder, M., Subbiah, J., Kaplan, P., Dhariwal, A., Neelakantan, P., Shyam, G., and Sastry arXiv:2005.14165
Child, R., Gray, S., Radford, A., and Sutskever, I. 2019. Generating long sequences with sparse transformers. In Generating long sequences with sparse transformers.
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., and Salakhutdinov, R. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th. 10.18653/v1/P19-1285
Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 2978--2988
Fan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J., and Auli, M. 2019. Eli5: Long form question answering. In Eli5: Long form question answering.
Guo, M., Ainslie, J., Uthus, D., Ontanon, S., Ni, J., Sung, Y., and Yang, Y. 2021. Longt5: Efficient text-to-text transformer for long sequences. In Longt5: Efficient text-to-text transformer for long sequences.
Huang, L., Cao, S., Parulian, N., Heng, J., and Wang, L. 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1419--1436 10.18653/v1/2021.naacl-main.112
Ippolito, D., Grangier, D., Eck, D., and Callison-Burch, C. 2020. Toward better storylines with sentence-level language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7472--7478 10.18653/v1/2020.acl-main.666
Kr Ireland 1986. Towards a grammar of narrative sequence: The model of the french lieutenant's woman. In Poetics today. pp. 397--420
Katharopoulos, A. and Vyas, A. Nikolaos Pappas, and Francois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning.
Kryściński, W. and Rajani, N. 2021. Booksum: A collection of datasets for long-form narrative summarization. In Booksum: A collection of datasets for long-form narrative summarization.
Liu, S. and Zhang, X. 2020. Corpora for document-level neural machine translation. In Proceedings of the 12th Language Resources and Evaluation Conference. pp. 3775--3781
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019.
Myers, J., Edward, O., Brien, J., Albrecht, R., and Mason 1994. Maintaining global coherence during reading. In Journal of Experimental Psychology: Learning, Memory, and Cognition. pp. 876--886 10.1037/0278-7393.20.4.876
Richard Yuanzhe Pang, A., Parrish, N., Joshi, N., Nangia, J., Phang, A., Chen, V., Padmakumar, J., Ma, J., Thompson, H., He, S. R., and Bowman 2021. Quality: Question answering with long input texts, yes! Ofir Press. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 5493--5505 10.18653/v1/2021.acl-long.427
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In OpenAI blog. pp. 9
Rae, J. W., Potapenko, A., Siddhant, M., Jayakumar, C., Hillier, T. P., and Lillicrap 2020. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J.
Roy, A., Saffar, M., Vaswani, A., and Grangier, D. 2021. Efficient content-based sparse attention with routing transformers. In Transactions of the Association for Computational Linguistics. pp. 53--68 10.1162/tacl_a_00353
Shaham, U., Segal, E., Ivgi, M., Efrat, A., Yoran, O., Haviv, A., Gupta, A., and Xiong, W.
Shaw, P., Uszkoreit, J., and Vaswani, A. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 464--468 10.18653/v1/N18-2074
Shen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. 2020. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8815--8821 10.1609/aaai.v34i05.6409
Stevick, P. 1970. The Chapter in Fiction: Theories of Narrative Division. In The Chapter in Fiction: Theories of Narrative Division.
Stock, P., Fan, A., Graham, B., Grave, E., Gribonval, R., Jegou, H., and Joulin, A. 2021. Training with quantization noise for extreme model compression. In International Conference on Learning Representations.
Sun, S., Krishna, K., Mattarella-Micke, A., and Iyyer, M. 2021. Do long-range language models actually use long-range context?. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 807--822 10.18653/v1/2021.emnlp-main.62
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. 2021. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations.
Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. 2020. Efficient transformers: A survey. In Efficient transformers: A survey.
Van Den Oord, A., Li, Y., and Vinyals, O. 2018. Representation learning with contrastive predictive coding. In ArXiv. abs/1807.03748
Weston, J., Chopra, S., and Bordes, A. 2015.
Zaheer, M., Guruganesh, G., Kumar Avinava Dubey, J., Ainslie, C., Alberti, S., Ontanon, P., Pham, A., Ravula, Q., Wang, L., and Yang 2020. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems. pp. 33
Zellers, R., Bisk, Y., Schwartz, R., and Choi, Y. 2018. SWAG: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 93--104 10.18653/v1/D18-1009