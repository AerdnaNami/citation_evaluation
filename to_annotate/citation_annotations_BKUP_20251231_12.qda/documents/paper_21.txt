Introduction

Natural language processing (NLP) datasets are plagued with artifacts and biases, which allow models to perform tasks without learning the desired underlying language capabilities. For instance, in natural language inference (NLI) datasets, models can predict an entailment relationship y from the hypothesis text H alone, without considering the premise P at all (Gururangan et al., 2018;Poliak et al., 2018). Another identified source of bias is lexical overlap between P and H, which is associ-ated with an entailment prediction (McCoy et al., 2019). We refer to such biases as structural biases, cases where an undesired subset of the input alone incorrectly identifies the label. Relying on such biases results in poor out-of-distribution (o.o.d) generalization when models are applied to data without bias. Furthermore, models that contain such biases may make surprising predictions when the bias is present, causing problems in critical systems.

A line of work has attempted to improve the performance on o.o.d datasets by proposing different objective functions (e.g., Utama et al., 2020a;Karimi Mahabadi et al., 2020). However, these methods typically still result in a significant gap between the performance in and out of distribution, which indicates that the models are still biased. Table 1 shows this gap, which we term the o.o.d generalization gap (∆).

In this work, we reformulate classification as a generative task, where the model's task is to generate the remainder features R conditioned on the biased features B and the label y. Using Bayes' Rule, we decompose the posterior p(y | B, R) into the likelihood p(R | y, B) and the prior p(y | B). This reformulation lets us control the amount of bias present in the final model. By setting a uniform prior we can obtain a provably unbiased model. We denote this generative model as GEN..

To assess the extent to which a given model is biased w.r.t a specific structural bias, we consider two metrics: the o.o.d generalization gap and the correlation between a model and a biased model p(y | B), such as a hypothesis-only or overlap-only model. We first experiment with injecting synthetic bias into a fraction of the training set and evaluating on test sets with and without that bias. We find that the discriminative model's performance decreases as the amount of bias increases, while GEN maintains similar performance at all bias levels. Moreover, the biased-ness of the discriminative model increases, while GEN  Next, we experiment with two kinds of natural bias: hypothesis-only and overlap. We demonstrate that GEN is unbiased compared to the discriminative baseline as measured by its low ∆ and low absolute correlation with a biased model (ρ).

However, while our approach leads to unbiased models, it performs worse than the discriminative baseline even on o.o.d data. We then identify and quantify several causes for the poor performance of GEN. We show that generative modeling is a more challenging task than discriminative modeling, and that it requires learning a large amount of spurious signal compared to the discriminative model.

Finally, to mitigate the difficulty of the generative modeling task, we fine-tune GEN with a discriminative objective (Lewis and Fan, 2019). While this leaks some bias into the model, the final model (denoted as GEN-FT) matches or surpasses the discriminative baseline while maintaining a relatively small o.o.d generalization gap.

To conclude, our contributions are as follows:

• We develop a generative modeling approach, which provably eliminates structural biases in natural language understanding tasks.

• We demonstrate experimentally on two bias types and different NLI datasets that this approach leads to unbiased models.

• We analyze the strengths and weaknesses of the generative model.

• We show how discriminative fine-tuning improves the generative model, while allowing some bias to leak into the model.

 References: 
Agrawal, A., Batra, D., Parikh, D., and Kembhavi, A. 2018. Don't just assume; look and answer: Overcoming priors for visual question answering. In 2018 IEEE Conference on Computer Vision and Pattern Recognition. pp. 4971--4980 10.1109/CVPR.2018.00522
Belinkov, Y., Poliak, A., Shieber, S., Van Durme, B., and Rush, A. 2019. Don't take the premise for granted: Mitigating artifacts in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 877--891 10.18653/v1/P19-1084
Belinkov, Y., Poliak, A., Shieber, S., Van Durme, B., and Rush, A. 2019. On adversarial removal of hypothesis-only bias in natural language inference. In Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019). pp. 256--262 10.18653/v1/S19-1028
Samuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/D15-1075
Camburu, O., Rocktäschel, T., Lukasiewicz, T., and Blunsom, P. 2018. e-snli: Natural language inference with natural language explanations. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. pp. 9560--9572
Das, A., Anjum, S., and Gurari, D. 2019. Dataset bias: A case study for visual question answering. In Proceedings of the Association for Information Science and Technology. pp. 58--67
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423
Suchin Gururangan, S., Swayamdipta, O., Levy, R., Schwartz, S., Bowman, N. A., and Smith 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 107--112 10.18653/v1/N18-2017
He, H., Zha, S., and Wang, H. 2019. Unlearn dataset bias in natural language inference by fitting the residual. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP. pp. 132--142 10.18653/v1/D19-6115
Rabeeh Karimi Mahabadi, Y., Belinkov, J., and Henderson 2020. End-to-end bias mitigation by modelling biases in corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 8706--8716 10.18653/v1/2020.acl-main.769
Kaushik, D. and Lipton, Z. C. 2018. How much reading does reading comprehension require? a critical investigation of popular benchmarks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 5010--5015 10.18653/v1/D18-1546
Kokhlikyan, N., Miglani, V., Martin, M., Wang, E., Alsallakh, B., Reynolds, J., Melnikov, A., Kliushkina, N., Araya, C., and Yan, S. 2020. Captum: A unified and generic model interpretability library for pytorch. In Captum: A unified and generic model interpretability library for pytorch. arXiv:2009.07896
Lester, B., Al-Rfou, R., and Constant, N. 2021. The power of scale for parameter-efficient prompt tuning. In The power of scale for parameter-efficient prompt tuning. arXiv:2104.08691
Lewis, M. and Fan, A. 2019. Generative question answering: Learning to answer the whole question. In 7th International Conference on Learning Representations. ICLR 2019
Lewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703
Mccoy, T., Pavlick, E., and Linzen, T. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3428--3448 10.18653/v1/P19-1334
Papineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135
Poliak, A., Naradowsky, J., Haldar, A., Rudinger, R., and Van Durme, B. 2018. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics. pp. 180--191 10.18653/v1/S18-2023
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In Journal of Machine Learning Research. pp. 1--67
Sanh, V., Wolf, T., Belinkov, Y., and Rush, A.M. 2021. Learning from others' mistakes: Avoiding dataset biases without modeling them. In International Conference on Learning Representations.
Schuster, T., Shah, D., Yeo, Y.J.S., Roberto Filizzola, D., Ortiz, E., Santus, R., and Barzilay 2019. Towards debiasing fact verification models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3419--3425 10.18653/v1/D19-1341
Stacey, J., Minervini, P., Dubossarsky, H., Riedel, S., and Rocktäschel, T. 2020. Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 8281--8291 10.18653/v1/2020.emnlp-main.665
Sundararajan, M., Taly, A., and Yan, Q. 2017. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning. pp. 3319--3328
Tsuchiya, M. 2018. Performance impact caused by hidden bias of training data for recognizing textual entailment. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).
Prasetya Ajie Utama, N.S., Moosavi, I., and Gurevych 2020. Mind the trade-off: Debiasing NLU models without degrading the in-distribution performance. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 8717--8729 10.18653/v1/2020.acl-main.770
Prasetya Ajie Utama, N.S., Moosavi, I., and Gurevych 2020. Towards debiasing NLU models from unknown biases. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 7597--7610 10.18653/v1/2020.emnlp-main.613
Williams, A., Nangia, N., and Bowman, S. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1112--1122 10.18653/v1/N18-1101
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., and Funtowicz, M. 2019. Huggingface's transformers: Stateof-the-art natural language processing. In ArXiv. pp. 1910
Zhu, Y., Lu, S., Zheng, L., Guo, J., Zhang, W., Wang, J., and Yu, Y. 2018. Texygen: A benchmarking platform for text generation models. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018. pp. 1097--1100 10.1145/3209978.3210080