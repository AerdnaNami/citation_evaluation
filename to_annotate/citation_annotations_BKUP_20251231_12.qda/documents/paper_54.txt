Introduction

Text summarization is an important natural language processing (NLP) task, aiming at generating concise summaries for given texts while preserving the key information. It has extensive real-world applications such as headline generation (Nenkova et al., 2011).

State-of-the-art text summarization models are typically trained in a supervised way with large training corpora, comprising pairs of long texts and their summaries (Zhang et al., 2020;Aghajanyan et al., 2020Aghajanyan et al., , 2021. However, such parallel data are expensive to obtain, preventing the applications to less popular domains and less spoken languages.

Unsupervised text generation has been attracting increasing interest, because it does not require parallel data for training. One widely used approach is to compress a long text into a short one, and to reconstruct it to the long text by a cycle consistency loss (Miao and Blunsom, 2016;Wang and Lee, 2018;Baziotis et al., 2019). Due to the indifferentiability of the compressed sentence space, such an approach requires reinforcement learning (or its variants), which makes the training difficult (Kreutzer et al., 2021).

Recently, Schumann et al. (2020) propose an edit-based approach for unsupervised summarization. Their model maximizes a scoring function that evaluates the quality (fluency and semantics) of the generated summary, achieving higher performance than cycle-consistency methods. However, the search approach is slow in inference because hundreds of search steps are needed for each data sample. Moreover, their approach can only select words from the input sentence with the word order preserved. Thus, it is restricted and may generate noisy summaries due to the local optimality of search algorithms.

To address the above drawbacks, we propose a Non-Autoregressive approach to Unsupervised Summarization (NAUS). The idea is to perform search as in Schumann et al. (2020) and, inspired by Li et al. (2020), to train a machine learning model to smooth out such noise and to speed up the inference process. Different from Li et al. (2020), we propose to utilize non-autoregressive text generators, which generate all tokens in the output in parallel, based on our following observations:

• Non-autoregressive models are several times faster than autoregressive generation, which is important when the system is deployed.

• The input and output of the summarization task have a strong correspondence. Non-autoregressive generation supports encoder-only architectures, which can better utilize such input-output correspondence and even outperform autoregressive models for summarization.

• For non-autoregressive models, we can design a length-control algorithm based on dynamic programming. This can satisfy the output length constraint, which is typical in summarization but can-not be easily achieved with autoregressive models.

We conducted experiments on Gigaword headline generation (Graff et al., 2003) and DUC2004 (Over and Yen, 2004) datasets. Experiments show that our NAUS achieves state-of-the-art performance on unsupervised summarization; especially, it outperforms its teacher (i.e., the search approach), confirming that NAUS can indeed smooth out the search noise. Regarding inference efficiency, our NAUS with truncating is 1000 times more efficient than the search approach; even with dynamic programming for length control, NAUS is still 100 times more efficient than search and several times more efficient than autoregressive models. Our NAUS is also able to perform length-transfer summary generation, i.e., generating summaries of different lengths from training.

 References: 
Aghajanyan, A., Gupta, A., Shrivastava, A., Chen, X., Zettlemoyer, L., and Gupta, S. 2021. Muppet: Massive multi-task representations with pre-finetuning. In Muppet: Massive multi-task representations with pre-finetuning. arXiv:2101.11038
Aghajanyan, A., Shrivastava, A., Gupta, A., Goyal, N., Zettlemoyer, L., and Gupta, S. 2020. Better fine-tuning by reducing representational collapse. In International Conference on Learning Representations.
Baziotis, C. 2019. Seq3: Differentiable sequence-to-sequence-to-sequence autoencoder for unsupervised abstractive sentence compression. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 673--681
Chan, W., Saharia, C., Hinton, G., Norouzi, M., and Jaitly, N. 2020. Imputer: Sequence modelling via imputation and dynamic programming. In Proceedings of the International Conference on Machine Learning. pp. 1403--1413
Dorr, B., Zajic, D., and Schwartz, R. 2003. Hedge trimmer: A parse-and-trim approach to headline generation. In Proceedings of the HLT-NAACL 03 Text Summarization Workshop. pp. 1--8
Févry, T. and Phang, J. 2018. Unsupervised sentence compression using denoising autoencoders. In Proceedings of the Conference on Computational Natural Language Learning. pp. 413--422
Graff, D., Kong, J., Chen, K., and Maeda, K. 2003.
Graves, A., Fernández, S., Gomez, F., and Schmidhuber, J. 2006. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the International Conference on Machine Learning. pp. 369--376 10.1145/1143844.1143891
Gu, J., Bradbury, J., Xiong, C., Victor, O. K., Li, R., and Socher 2018. Non-autoregressive neural machine translation. In International Conference on Learning Representations.
Gu, J. and Kong, X. 2021. Fully nonautoregressive neural machine translation: tricks of the trade. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 120--133
He, Z., Chen, C., Bu, J., Wang, C., Zhang, L., Cai, D., and He, X. 2012. Document summarization based on data reconstruction. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 620--626
Jia, R., Cao, Y., Shi, H., Fang, F., Yin, P., and Wang, S. 2021. Flexible nonautoregressive extractive summarization with threshold: How to extract a non-fixed number of summary sentences. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 13134--13142
Kreutzer, J., Riezler, S., and Lawrence, C. 2021. Offline reinforcement learning from human feedback in real-world sequence-to-sequence tasks. In Proceedings of the Workshop on Structured Prediction for NLP. pp. 37--43
Lee, J., Mansimov, E., and Cho, K. 2018. Deterministic non-autoregressive neural sequence modeling by iterative refinement. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 1173--1182
Li, J., Li, Z., Mou, L., Jiang, X., Lyu, M., and King, I. 2020. Unsupervised text generation by learning from search. In Advances in Neural Information Processing Systems. pp. 10820--10831
Lin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81
Liu, X., Mou, L., Meng, F., Zhou, H., Zhou, J., and Song, S. 2020. Unsupervised paraphrasing by simulated annealing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics. pp. 302--312
Liu, Y., Dou, Z., and Liu, P. 2021. RefSum: Refactoring neural summarization. In Proceedings of the Annual Meeting of the Association for Computational Linguistics. pp. 1437--1448
Meister, C., Cotterell, R., and Vieira, T. 2020. If beam search is the answer, what was the question?. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 2173--2185
Miao, Y. and Blunsom, P. 2016. Language as a latent variable: Discrete generative models for sentence compression. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 319--328
Nenkova, A., Maskey, S., and Liu, Y. 2011. Automatic summarization. In Proceedings of the Annual Meeting of the Association for Computational Linguistics. pp. 1--86 10.5555/2002465.2002468
Over, P. and Yen, J. 2004. An introduction to DUC-2004: Intrinsic evaluation of generic news text summarization systems. In Proceedings of the Document Understanding Conference.
Qi, W., Gong, Y., Jiao, J., Yan, Y., Chen, W., Liu, D., Tang, K., Li, H., Chen, J., Zhang, R., Zhou, M., and Duan, N. 2021. Bang: Bridging autoregressive and non-autoregressive generation with large scale pretraining. In Proceedings of the International Conference on Machine Learning. pp. 8630--8639
Qian, L., Zhou, H., Bao, Y., Wang, M., Qiu, L., Zhang, W., Yu, Y., and Li, L. 2021. Glancing transformer for non-autoregressive neural machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing. pp. 1993--2003
Rush, A. M., Chopra, S., and Weston, J. 2015. A neural attention model for abstractive sentence summarization. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 379--389 10.18653/v1/D15-1044
Saharia, C., Chan, W., Saxena, S., and Norouzi, M. 2020. Non-autoregressive machine translation with latent alignments. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 1098--1108
Schumann, R., Mou, L., and Lu, Y. Olga Vechtomova, and Katja Markert. 2020. Discrete optimization for unsupervised sentence summarization with word-level extraction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics. pp. 5032--5042
Su, Y., Cai, D., Wang, Y., Vandyke, D., Baker, S., Li, P., and Collier, N. 2021. Nonautoregressive text generation with pre-trained language models. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics. pp. 234--243
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. pp. 5998--6008
Wang, Y. and Lee, H. 2018. Learning to encode text as human-readable summaries using generative adversarial networks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 4187--4195
West, P. and Holtzman, A. 2019. BottleSum: Unsupervised and selfsupervised sentence summarization using the information bottleneck principle. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing. pp. 3752--3761
Yang, K., Lei, W., Liu, D., Qi, W., and Lv, J. 2021. POS-constrained parallel decoding for non-autoregressive generation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing. pp. 5990--6000
Yang, Z., Zhu, C., Gmyr, R., Zeng, M., Huang, X., and Darve, E. 2020. TED: A pretrained unsupervised summarization model with theme modeling and denoising. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 1865--1874
Zajic, D., Dorr, B., and Schwartz, R. 2004. BBN/UMD at DUC-2004: Topiary. In Proceedings of the HLT-NAACL Document Understanding Workshop. pp. 112--119
Zhang, J., Zhao, Y., Saleh, M., and Liu, P. 2020. PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the International Conference on Machine Learning. pp. 11328--11339
Zhou, J. and Rush, A. 2019. Simple unsupervised summarization by contextual matching. In Proceedings of the Annual Meeting of the Association for Computational Linguistics. pp. 5101--5106