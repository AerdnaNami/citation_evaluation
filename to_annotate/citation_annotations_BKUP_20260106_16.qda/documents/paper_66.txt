Introduction

General-purpose pretrained vision and language (V&L) models have gained notable performance on many V&L tasks (Lu et al., 2019;Tan and Bansal, 2019;Li et al., 2019;Li et al., 2020a;Su et al., 2020). As a result, V&L research has changed its focus from task-specific architectures to fine-tuning large V&L models.

Current benchmarks give a good perspective on model performance on a wide range of V&L tasks Lourie et al., 2021;Li et al., 2021), but the field is only starting to assess why models perform so well and whether models learn specific capabilities that span multiple V&L tasks. Specifically, we lack detailed understanding of the extent to which such models are able to ground linguistic phenomena-from morphosyntax to semantics-in the visual modality (Bernardi and Pezzelle, 2021). For example, recent evidence suggests that models are insensitive to linguistic distinctions of verb-argument structure (Hendricks and Nematzadeh, 2021) and word order (Cirik et al., 2018;Akula et al., 2020).

Our work addresses this gap with VALSE (Vision And Language Structured Evaluation), a benchmark for V&L model evaluation comprising six tasks, or 'pieces', where each piece has the same structure: given a visual input, a model is asked to distinguish real captions from foils, where a foil is constructed from a caption by altering a word or phrase that realizes a specific linguistic phenomenon, e.g., semantic number of nouns, verb argument structure, or coreference. VALSE uses a resource-lean diagnostic setup that dispenses with large-scale annotation (e.g., of bounding boxes), and builds on existing high-quality image captioning and VQA data. VALSE is designed to leverage the existing prediction heads in pretrained (or finetuned) V&L models; for that reason, our benchmark does not include any re-training and can be interpreted as a zero-shot evaluation. We build test data for each piece so as to safeguard against the possibility of models exploiting artefacts or statistical biases in the data, a well-known issue with highly parameterised neural models pretrained on large amounts of data (Goyal et al., 2017;Madhyastha et al., 2018;Kafle et al., 2019). With this in view, we propose novel methods to guard against the emergence of artefacts during foiling.

Our main contributions are: i) We introduce VALSE, a novel benchmark aimed at gauging the sensitivity of pre-trained V&L models to foiled instances. ii) We cover a wide spectrum of basic linguistic phenomena affecting the linguistic and visual modalities: existence, plurality, counting, spatial relations, actions, and entity coreference. iii) We investigate novel strategies to build valid foils that include automatic and human validation. We balance word frequency distributions between captions and foils, and test against pretrained models solving the benchmark unimodally. We employ masked language modeling (MLM) in foil creation and semantic inference for validating foils, and finally collect human annotations for the entire benchmark. iv) We establish initial experimental results for pretrained V&L models of diverse architectures on VALSE. These models' overall weak performance indicates that the time is ripe for a novel, reliable foiling dataset targeting the visual grounding capabilities of V&L models through the lens of linguistic constructs. 1

2 Background and Related work

Pretrained V&L models learn to combine vision and language through self-supervised multitask learning. Tasks include multimodal masked modeling-where words in the text and object labels or regions in the image are masked out, then predictedand image-sentence alignment, whereby a model learns to predict whether an image and a text correspond. Major architectures are single-and dualstream multimodal transformers: single-stream models concatenate word and image features, and encode the resulting sequence with a single transformer stack; dual-stream models use distinct transformer stacks to handle visual and textual inputs, and additional layers (e.g. co-attention) to fuse these into multimodal features.

Benchmarking V&L models V&L models (Li et al., 2019;Lu et al., 2019;Tan and Bansal, 2019;Lu et al., 2020;Li et al., 2020b;Kim et al., 2021) are commonly evaluated on V&L tasks such as VQA (Goyal et al., 2017), visual reasoning (Suhr et al., 2019), or image retrieval (Lin et al., 2014;Plummer et al., 2015). Given how well transformer-based models perform across unimodal and multimodal tasks, research efforts have recently started to address what makes them so effective, and to what extent they learn generalisable representations. Techniques to address these questions in unimodal and multimodal V&L contexts include: adversarial examples (Jia and Liang, 2017;Jia et al., 2019); investigation of the impact of bias, be it linguistic (Gururangan et al., 2018), visual semantic (Agarwal et al., 2020), or socio-economic (Garg et al., 2019); and the use of linguistically-informed counterfactual and minimally-edited examples (Levesque et al., 1 We release our dataset containing all annotators' votes (Prabhakaran et al., 2021) and code upon acceptance. 2012; Gardner et al., 2020). A trend within the latter research line that is specific to V&L models is vision-and-language foiling (Shekhar et al., 2017b;Gokhale et al., 2020;Bitton et al., 2021;Parcalabescu et al., 2021;Rosenberg et al., 2021), where the idea is to create counterfactual (i.e., foiled) and/or minimally edited examples by performing data augmentation on captions (Shekhar et al., 2017b,a) or images (Rosenberg et al., 2021).

Since most V&L models are pretrained on some version of the image-text alignment task, it is possible to test their ability to distinguish correct from foiled captions (in relation to an image) in a zeroshot setting. The construction of foils can serve many investigation purposes. With VALSE, we target the linguistic grounding capabilities of V&L models, focusing on pervasive linguistic phenomena that span multiple tokens, described in §3.1- §3.6. At the same time, we ensure that our data is robust to perturbations and artefacts by i) controlling for word frequency biases between captions and foils, and ii) testing against unimodal collapse, a known issue of V&L models (Goyal et al., 2017;Madhyastha et al., 2018), thereby preventing models from solving the task using a single input modality. The issue of neural models exploiting data artefacts is well-known (Gururangan et al., 2018;Jia et al., 2019;Wang et al., 2020b;He et al., 2021) and methods have been proposed to uncover such effects, including gradient-based, adversarial perturbations or input reduction techniques (cf. Wallace et al., 2020). Yet, these methods are still not fully understood (He et al., 2021) and can be unreliable (Wang et al., 2020b).

Our work is related to Gardner et al. (2020), who construct task-specific contrast sets for NLU. However, our focus is on modelling linguistic phenomena instead of tasks, and we construct carefully curated, balanced, single foils from valid instances that we select from multiple multimodal datasets.

 References: 
Agarwal, V., Shetty, R., and Fritz, M. 2020. Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9690--9698
Akula, A., Gella, S., Al-Onaizan, Y., Zhu, S., and Reddy, S. 2020. Words aren't enough, their order matters: On the robustness of grounding visual referring expressions. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 6555--6565 10.18653/v1/2020.acl-main.586
Bernardi, R. and Pezzelle, S. 2021. Linguistic issues behind visual question answering. Language and Linguistics Compass. In Linguistic issues behind visual question answering. Language and Linguistics Compass. pp. 1--25 10.1111/lnc3.12417
Bitton, Y., Stanovsky, G., Schwartz, R., and Elhadad, M. 2021. Automatic generation of contrast sets from scene graphs: Probing the compositional consistency of GQA. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 94--105
Samuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/D15-1075
Cao, J., Gan, Z., Cheng, Y., Yu, L., Chen, Y., and Liu, J. 2020. Behind the scene: Revealing the secrets of pre-trained vision-andlanguage models. In Behind the scene: Revealing the secrets of pre-trained vision-andlanguage models. arXiv:2005.07310
Chen, X., Fang, H., Lin, T., Vedantam, R., Zitnick, L., Gupta, S., and Doll, P. 2015. Microsoft COCO Captions : Data Collection and Evaluation Server. arXiv, 1504. In Microsoft COCO Captions : Data Collection and Evaluation Server. arXiv, 1504. pp. 1--7
Chen, Y., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z., Cheng, Y., and Liu, J. 2020. Uniter: Universal image-text representation learning. In ECCV.
Cirik, V., Morency, L., and Berg-Kirkpatrick, T. 2018. Visual referring expression recognition: What do systems actually learn?. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 781--787 10.18653/v1/N18-2123
Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., José, M. F., and Moura
Parikh, D. and Batra 2017. Visual Dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423
Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. In WordNet: An Electronic Lexical Database.
Gardner, M., Artzi, Y., Basmov, V., Berant, J., Bogin, B., Chen, S., Dasigi, P., Dua, D., Elazar, Y., Gottumukkala, A., Gupta, N., Hajishirzi, H., Ilharco, G., Khashabi, D., Lin, K., Liu, J., Liu, N. F., Mulcaire, P., Ning, Q., Singh, S., Smith, N. A., Subramanian, S., Tsarfaty, R., Wallace, E., Zhang, A., and Zhou, B. 2020. Evaluating models' local decision boundaries via contrast sets. In Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 1307--1323 10.18653/v1/2020.findings-emnlp.117
Garg, S., Perot, V., Limtiaco, N., Taly, A., Chi, E. H., and Beutel, A. 2019. Counterfactual fairness in text classification through robustness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES '19. pp. 219--226 10.1145/3306618.3317950
Gatt, A. and Reiter, E. 2009. SimpleNLG: A realisation engine for practical applications. In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG 2009). pp. 90--93
Goh, G., Cammarata, N., †, Voss, C., †, Carter, S., Petrov, M., Schubert, L., Radford, A., and Olah, C. 2021. Multimodal neurons in artificial neural networks. In Multimodal neurons in artificial neural networks. 10.23915/distill.00030
Gokhale, T., Banerjee, P., Baral, C., and Yang, Y. 2020. MUTANT: A training paradigm for out-of-distribution generalization in visual question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 878--892 10.18653/v1/2020.emnlp-main.63
Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6904--6913
Bryan, A., Plummer, L., Wang, C. M., Cervantes, J. C., Caicedo, J., Hockenmaier, S., and Lazebnik 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models. In Proceedings of the IEEE international conference on computer vision. pp. 2641--2649
Vinodkumar Prabhakaran, A. M., Davani, M., and Díaz 2021. On releasing annotator-level labels and information in datasets. In On releasing annotator-level labels and information in datasets.
Pratt, S., Yatskar, M., Weihs, L., Farhadi, A., and Kembhavi, A. 2020. Grounded situation recognition. In Computer Vision -ECCV 2020 -16th European Conference. pp. 314--332 10.1007/978-3-030-58548-8_19
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., and Clark, J. 2021. Learning transferable visual models from natural language supervision. In Learning transferable visual models from natural language supervision. arXiv:2103.00020
Radford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In OpenAI blog. pp. 9
Marco Tulio Ribeiro, T., Wu, C., Guestrin, S., and Singh 2020. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4902--4912 10.18653/v1/2020.acl-main.442
Rosenberg, D., Gat, I., Feder, A., and Reichart, R. 2021. Are VQA systems RAD? Measuring robustness to augmented data with focused interventions. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 61--70 10.18653/v1/2021.acl-short.10
Sharma, P., Ding, N., Goodman, S., and Soricut, R. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2556--2565 10.18653/v1/P18-1238
Shekhar, R., Pezzelle, S., Herbelot, A., Nabi, M., Sangineto, E., and Bernardi, R. 2017. Vision and language integration: Moving beyond objects. In IWCS 2017 -12th International Conference on Computational Semantics -Short papers.
Shekhar, R., Pezzelle, S., Klimovich, Y., Herbelot, A., Nabi, M., Sangineto, E., and Bernardi, R. 2017. FOIL it! find one mismatch between image and language caption. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 255--265 10.18653/v1/P17-1024
Shekhar, R., Takmaz, E., Fernández, R., and Bernardi, R. 2019. Evaluating the representational hub of language and vision models. In Proceedings of the 13th International Conference on Computational Semantics -Long Papers. pp. 211--222 10.18653/v1/W19-0418
Shekhar, R., Venkatesh, A., Baumgärtner, T., Bruni, E., Plank, B., Bernardi, R., and Fernández, R. 2019. Beyond task success: A closer look at jointly learning to see, ask, and Guess-What. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2578--2587 10.18653/v1/N19-1265
Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., and Dai, J. 2020. Vl-bert: Pretraining of generic visual-linguistic representations. In Vl-bert: Pretraining of generic visual-linguistic representations.