Related Work

Multimodal machine translation is a cross-domain task in the filed of machine translation. Early attempts mainly focused on enhancing the MMT model by better incorporation of the vision features (Calixto and Liu, 2017;Elliott and Kádár, 2017;Delbrouck and Dupont, 2017). However, directly encoding the whole image feature brings additional noise to the text (Yao and Wan, 2020;Liu et al., 2021a). To address the above issue, Yao and Wan (2020) proposed a multimodal self-attention to consider the relative difference of information between two modalities. Similarly, Liu et al. (2021a) used a Gumbel Softmax to achieve the same goal.

Researchers also realize that the vision modality maybe redundant. Irrelevant images have little impact on the translation quality, and no significant BLEU drop is observed even the image is absent (Elliott, 2018). Encouraging results appeared in  2021) proposed a cross-lingual visual pretraining approach. In this work, we make a systematic study on whether stronger vision features are helpful. We also extend the research to enhanced features, such as object-detection and image captioning, which is complementary to previous work.

 References: 
Caglayan, O., Barrault, L., and Bougares, F. 2016. Multimodal attention for neural machine translation. In Multimodal attention for neural machine translation. abs/1609.03976
Caglayan, O., Kuyu, M., Mustafa Sercan Amac, P., Madhyastha, E., Erdem, A., Erdem, L., and Specia 2021. Cross-lingual visual pretraining for multimodal machine translation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 1317--1324
Caglayan, O., Madhyastha, P., Specia, L., and Barrault, L. 2019. Probing the need for visual context in multimodal machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4159--4170 10.18653/v1/N19-1422
Calixto, I. and Liu, Q. 2017. Incorporating global visual features into attention-based neural machine translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pp. 992--1003 10.18653/v1/D17-1105
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. 2020. End-to-end object detection with transformers. In Computer Vision -ECCV 2020 -16th European Conference. pp. 213--229
Delbrouck, J. and Dupont, S. 2017. An empirical study on the effectiveness of images in multimodal neural machine translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pp. 910--919 10.18653/v1/D17-1095
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event.
Elliott, D. 2018. Adversarial evaluation of multimodal machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 2974--2978 10.18653/v1/D18-1329
Elliott, D., Frank, S., Barrault, L., Bougares, F., and Specia, L. 2017. Findings of the second shared task on multimodal machine translation and multilingual image description. In Proceedings of the Second Conference on Machine Translation. pp. 215--233 10.18653/v1/W17-4718
Elliott, D., Frank, S., Sima'an, K., and Specia, L. 2016. Multi30k: Multilingual englishgerman image descriptions. In Proceedings of the 5th Workshop on Vision and Language, hosted by the 54th Annual Meeting of the Association for Computational Linguistics, VL@ACL 2016. 10.18653/v1/w16-3210
Elliott, D. and Kádár, Á. 2017. Imagination improves multimodal translation. In Proceedings of the Eighth International Joint Conference on Natural Language Processing. pp. 130--141
Fang, Y., Yang, S., Wang, X., Li, Y., Fang, C., and Shan, Y. In Bin Feng.
Grönroos, S., Huet, B., Kurimo, M., Laaksonen, J., Merialdo, B., Pham, P., Sjöberg, M., Sulubacak, U., Tiedemann, J., Troncy, R., and Vázquez, R. 2018. The MeMAD submission to the WMT18 multimodal translation task. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers. pp. 603--611 10.18653/v1/W18-6439
Diederik, P., Kingma, J., and Ba 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations.
Lala, C., Swaroop Madhyastha, P., Scarton, C., and Specia, L. 2018. Sheffield submissions for WMT18 multimodal translation shared task. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers. pp. 624--631 10.18653/v1/W18-6442
Li, B., Liu, H., Wang, Z., Jiang, Y., Xiao, T., Zhu, J., Liu, T., and Li, C. 2020. Does multi-encoder help? a case study on contextaware neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 3512--3518 10.18653/v1/2020.acl-main.322
Libovický, J. and Helcl, J. 2017. Attention strategies for multi-source sequence-to-sequence learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 196--202 10.18653/v1/P17-2031
Lin, H., Meng, F., Su, J., Yin, Y., Yang, Z., Ge, Y., Zhou, J., and Luo, J. 2020. Dynamic context-guided capsule network for multimodal machine translation. In MM '20: The 28th ACM International Conference on Multimedia, Virtual Event. pp. 1320--1329 10.1145/3394171.3413715
Liu, P., Cao, H., and Zhao, T. 2021. Gumbel-attention for multi-modal machine translation. In Gumbel-attention for multi-modal machine translation. abs/2103.08862
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. In Swin transformer: Hierarchical vision transformer using shifted windows. abs/2103.14030
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). pp. 48--53
Bryan, A., Plummer, L., Wang, C. M., Cervantes, J. C., Caicedo, J., Hockenmaier, S., and Lazebnik 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models. In 2015 IEEE International Conference on Computer Vision, ICCV 2015. pp. 2641--2649 10.1109/ICCV.2015.303
Specia, L., Frank, S., Sima'an, K., and Elliott, D. 2016. A shared task on multimodal machine translation and crosslingual image description. In Proceedings of the First Conference on Machine Translation. pp. 543--553 10.18653/v1/W16-2346
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008
Wang, D. and Xiong, D. 2021. Efficient objectlevel visual context modeling for multimodal machine translation: Masking irrelevant objects helps grounding. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. pp. 2720--2728
Wu, Z., Kong, L., Bi, W., Li, X., and Kao, B. 2021. Good for misconceived reasons: An empirical revisiting on the need for visual context in multimodal machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 6153--6166 10.18653/v1/2021.acl-long.480
Yao, S. and Wan, X. 2020. Multimodal transformer for multimodal machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4346--4350 10.18653/v1/2020.acl-main.400
Yin, Y., Meng, F., Su, J., Zhou, C., Yang, Z., Zhou, J., and Luo, J. 2020. A novel graph-based multi-modal fusion encoder for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 3025--3035 10.18653/v1/2020.acl-main.273
Zhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li, Z., and Zhao, H. 2020. Neural machine translation with universal visual representation. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa.
Zhao, Y., Komachi, M., Kajiwara, T., and Chu, C. 2020. Double attention-based multimodal neural machine translation with semantic image regions. In Proceedings of the 22nd Annual Conference of the European Association for Machine Translation. pp. 105--114