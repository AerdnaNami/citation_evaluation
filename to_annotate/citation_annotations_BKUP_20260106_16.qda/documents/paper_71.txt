Introduction

Contextualized embedding algorithms, such as BERT (Devlin et al., 2019), have achieved impressive performance on a wide variety of tasks (Huang et al., 2019;Chan and Fan, 2019;Yoosuf and Yang, 2019). One application of BERT is as a measure of sentence similarity (Zhang et al., 2019;Sellam et al., 2020), based on the assumption that BERT will produce similar representations for the words in two sentences with similar semantics.

We propose to use paraphrases with alignments between words as a tool for studying how BERT represents words and phrases. Figure 1 shows an example. Critically, when considering an aligned word pair, we can assume the context has a similar impact on both words because we know the phrases are semantically similar. Previously, paraphrases have been used to probe whether compositionality is accurately captured by BERT (Yu and Ettinger, 2020), but we believe they can be used to explore many other questions.

Using the Paraphrase Database (Pavlick et al., 2015), we explore how consistent contextual representations are when controlling for the semantics of the context. We find that BERT does consistently represent phrases that are paraphrases. Looking at words, BERT effectively handles variation in spelling, but does less well with spelling errors. BERT effectively handles words of varying levels of polysemy, but the representations for synonyms are surprisingly diverse, with a much broader distribution of similarity scores. These findings confirm results from prior work using other methods, while revealing new details.

We also consider a range of other models' word representations, finding that they have similar patterns to BERT, but with words that are the same and aligned receiving even more consistent representations than from BERT. BERT gives less contextualized representations to paraphrased words than non-paraphrased words, with the exception of punctuation. Finally, we re-evaluate work looking at patterns across BERT's layers and find that when controlling for semantics the later layers actually produce more similar representations (in contrast to previous work).

These results show that paraphrases are a useful tool for studying representations. By controlling for meaning while presenting interesting surface variations, they provide a unique probe of behavior.

 References: 
Bannard, C. and Callison-Burch, C. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics. pp. 597--604 10.3115/1219840.1219914
Bird, S., Klein, E., and Loper, E. 2009. Natural language processing with Python: Analyzing text with the natural language toolkit. In Natural language processing with Python: Analyzing text with the natural language toolkit.
Chan, Y. and Fan, Y. 2019. BERT for question generation. In Proceedings of the 12th International Conference on Natural Language Generation. pp. 173--177 10.18653/v1/W19-8624
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186
Ethayarajh, K. 2019. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 55--65 10.18653/v1/D19-1006
Fellbaum, C. 1998. 10.1002/9781405198431.wbeal1285
Ganitkevitch, J., Van Durme, B., and Callison-Burch, C. 2013. PPDB: The paraphrase database. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 758--764
Aina, G., Soler, M., and Apidianaki 2020. BERT knows punta cana is not just beautiful, it's gorgeous: Ranking scalar adjectives with contextualised representations. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 7371--7385 10.18653/v1/2020.emnlp-main.598
Graff, D., Kong, K., Chen, K., and Maeda 2003. English gigaword. Linguistic Data Consortium. In English gigaword. Linguistic Data Consortium. pp. 34
Huang, L., Sun, C., Qiu, X., and Huang, X. 2019. GlossBERT: BERT for word sense disambiguation with gloss knowledge. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3509--3514 10.18653/v1/D19-1355
Kovaleva, O., Romanov, A., Rogers, A., and Rumshisky, A. 2019. Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 4365--4374 10.18653/v1/D19-1445
Mahmoud, S. and Torki, M. 2020. AlexU-AUX-BERT at SemEval-2020 task 3: Improving BERT contextual similarity using multiple auxiliary contexts. In Proceedings of the Fourteenth Workshop on Semantic Evaluation. pp. 270--274
Mickus, T., Paperno, D., Constant, M., and Van Deemeter, K. 2020. What do you mean, BERT? Assessing BERT as a distributional semantics model. In Proceedings of the Society for Computation in Linguistics. pp. 3
Mikolov, T., Quoc, V., Le, I., and Sutskever 2013. Exploiting similarities among languages for machine translation. In Exploiting similarities among languages for machine translation. arXiv:1309.4168
Pavlick, E., Rastogi, P., Ganitkevitch, J., Van Durme, B., and Callison-Burch, C. 2015. PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. pp. 425--430 10.3115/v1/P15-2070
Rastogi, P., Van Durme, B., and Arora, R. 2015. Multiview LSA: Representation learning via generalized CCA. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 556--566 10.3115/v1/N15-1058
Reimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410
Rogers, A., Kovaleva, O., and Rumshisky, A. 2020. A primer in BERTology: What we know about how BERT works. In A primer in BERTology: What we know about how BERT works. arXiv:2002.12327
Rush, A. M., Chopra, S., and Weston, J. 2015. A neural attention model for abstractive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 10.18653/v1/d15-1044
Sellam, T., Das, D., and Parikh, A. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7881--7892 10.18653/v1/2020.acl-main.704
Soler, J., Company, -., and Wanner, L. 2017. On the relevance of syntactic and discourse features for author profiling and identification. In Proceedings of the 15th Conference of the European Chapter. pp. 681--687
Spearman, C. 1904. Correlation calculated from faulty data. In British Journal of Psychology. pp. 271--295
Tatman, R. and Paullada, A. 2017. Social identity and punctuation variation in the #bluelivesmatter and #blacklivesmatter twitter communities. In The 33rd Northwest Linguistics Conference.
Tiedemann, J. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12). pp. 2214--2218
Tsvetkov, Y., Faruqui, M., Ling, W., Macwhinney, B., and Dyer, C. 2016. Learning the curriculum with Bayesian optimization for taskspecific word representation learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pp. 130--139 10.18653/v1/P16-1013
Wiedemann, G., Remus, S., Chawla, A., and Biemann, C. 2019. Does BERT make any sense? Interpretable word sense disambiguation with contextualized embeddings. In Konferenz zur Verarbeitung nat√ºrlicher Sprache / Conference on Natural Language Processing.
Yi, J. and Tao, J. 2019. Self-attention based model for punctuation prediction using word and speech embeddings. In IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7270--7274
Yoosuf, S. and Yang, Y. 2019. Fine-grained propaganda detection with fine-tuned BERT. In Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda. pp. 87--91 10.18653/v1/D19-5011