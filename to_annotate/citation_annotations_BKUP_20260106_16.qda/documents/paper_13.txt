Introduction

Few-shot learning is the problem of learning classifiers with only a few training examples. Zero-shot learning (Larochelle et al., 2008), also known as dataless classification (Chang et al., 2008), is the extreme case, in which no labeled data is used. For text data, this is usually accomplished by representing the labels of the task in a textual form, which can either be the name of the label or a concise textual description.

In recent years, there has been a surge in zeroshot and few-shot approaches to text classification. One approach (Yin et al., 2019, 2020; Halder et al., 2020;Wang et al., 2021) makes use of entailment models. Textual entailment (Dagan et al., 2006), also known as natural language inference (NLI) (Bowman et al., 2015), is the problem of predicting whether a textual premise implies a textual hypothesis in a logical sense. For example, Emma loves apples implies that Emma likes apples.

The entailment approach for text classification sets the input text as the premise and the text repre-senting the label as the hypothesis. A NLI model is applied to each input pair and the entailment probability is used to identify the best matching label.

In this paper, we investigate an alternative based on Siamese Networks (SN) (Bromley et al., 1993), also known as dual encoders. These models embed both input and label texts into a common vector space. The similarity of the two items can then be computed using a similarity function such as the dot product. The advantage is that input and label text are encoded independently, which means that the label embeddings can be pre-computed. Therefore, at inference time, only a single call to the model per input is needed. In contrast, the models typically applied in the entailment approach are Cross Attention (CA) models which need to be executed for every combination of text and label. On the other hand, they allow for interaction between the tokens of label and input, so that in theory they should be superior in classification accuracy. However, in this work we show that in practice, the difference in quality is small.

Both CA and SNs also support the few-shot learning setup by fine-tuning the models on a small number of labeled examples. This is usually done by updating all parameters of the model, which in turn makes it impossible to share the models between different tasks. In this work, we show that when using a SN, one can decide to only fine-tune the label embeddings. We call this Label Tuning (LT). With LT the encoder can be shared between different tasks, which greatly eases the deployment of this approach in a production setup. LT comes with a certain drop in quality, but this drop can be compensated by using a variant of knowledge distillation (Hinton et al., 2014).

Our contributions are as follows: We perform a large study on a diverse set of tasks showing that CA models and SN yield similar performance for both zero-shot and few-shot text classification.  (LT). At training time, input and label texts (hypotheses) are processed by the encoder. LT then tunes the labels using a cross entropy (CE) loss. At inference time, the input text is passed through the same encoder. The tuned label embeddings and a similarity function are then used to score each label. The encoder remains unchanged and can be shared between multiple tasks.

In contrast to most prior work, we also show that these results can also be achieved for languages other than English. We compare the hypothesis patterns commonly used in the literature and using the plain label name (null hypothesis) and find that on average there is no significant difference in performance. Finally, we present LT as an alternative to full fine-tuning that allows using the same model for many tasks and thus greatly increases the scalability of the method. We will release the code and trained models used in our experiments.

 References: 
Ovesdotter Alm, C., Roth, D., and Sproat, R. 2005. Emotions from text: machine learning for text-based emotion prediction. In Proceedings of the conference on human language technology and empirical methods in natural language processing.
Gulli, A. 2005. AG's corpus of news articles. In AG's corpus of news articles. pp. 2021--2027
Halder, K., Akbik, A., Krapac, J., and Vollgraf, R. 2020. Task-aware representation of sentences for generic text classification. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 3202--3213 10.18653/v1/2020.coling-main.285
Hambardzumyan, K., Khachatrian, H., and Jonathan 2021. WARP: Word-level Adversarial ReProgramming. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 4921--4933 10.18653/v1/2021.acl-long.381
Hearst, M. A., Susan, T., Dumais, E., Osuna, J., Platt, B., and Scholkopf 1998. Support vector machines. In IEEE Intelligent Systems and their applications. pp. 18--28
Henderson, M. L., Al-Rfou, R., Strope, B., Sung, Y., Lukács, L., Guo, R., Kumar, S., Miklos, B., and Kurzweil, R. 2017. Efficient natural language response suggestion for smart reply. In Efficient natural language response suggestion for smart reply. abs/1705.00652
Hinton, G. E., Vinyals, O., and Dean, J. 2014. Distilling the knowledge in a neural network. In The NIPS 2014 Learning Semantics Workshop.
Keung, P., Lu, Y., Szarvas, G., and Smith, N. A. 2010. The multilingual amazon reviews corpus. CoRR, abs. In The multilingual amazon reviews corpus. CoRR, abs.
Koehn, P. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. pp. 388--395
Larochelle, H., Erhan, D., and Bengio, Y. 2008. Zero-data learning of new tasks. In Proceedings of the 23rd National Conference on Artificial Intelligence. pp. 646--651
Teven, L., Scao, A., and Rush 2021. How many data points is a prompt worth. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2627--2636 10.18653/v1/2021.naacl-main.208
Li, X. and Roth, D. 2002. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics.
Li, Y., Su, H., Shen, X., Li, W., Cao, Z., and Niu, S. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset. In Dailydialog: A manually labelled multi-turn dialogue dataset. arXiv:1710.03957
Liu, V., Banea, C., and Mihalcea, R. 2007. Grounded emotions. In International Conference on Affective Computing and Intelligent Interaction.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.
Robert, L., Logan, I. V., Balazevic, I., Wallace, E., Petroni, F., Singh, S., and Riedel, S. 2021. Cutting down on prompts and parameters: Simple few-shot learning with language models. In Cutting down on prompts and parameters: Simple few-shot learning with language models.
Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. pp. 142--150
Mikolov, T., Chen, K., Corrado, G., and Dean, J. 2013. Efficient estimation of word representations in vector space. In 1st International Conference on Learning Representations, ICLR 2013.
Mohammad, S. 2012. #Emotional Tweets. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics. pp. 246--255
Saif, M., Mohammad, P., Sobhani, S., and Kiritchenko 2017. Stance and sentiment in tweets. In ACM Transactions on Internet Technology (TOIT). pp. 1--23
Saif, M., Mohammad, X., Zhu, S., Kiritchenko, J., and Martin 2015. Sentiment, emotion, purpose, and style in electoral tweets. In Information Processing & Management. pp. 480--499
Nakov, P., Ritter, A., Rosenthal, S., Stoyanov, V., and Sebastiani, F. 2016. SemEval-2016 task 4: Sentiment analysis in Twitter. In Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval '16.
Navas-Loro, M., Rodríguez-Doncel, V., Santana-Perez, I., and Sánchez, A. 2017. Spanish corpus for sentiment analysis towards brands. In Speech and Computer. pp. 680--689
Nie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J., and Kiela, D. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.
Pang, B. and Lee, L. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04). pp. 271--278 10.3115/1218955.1218990
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. 2011. Scikit-learn: Machine learning in Python. In Journal of Machine Learning Research. pp. 2825--2830
Perez, E., Kiela, D., and Cho, K. 2021. True few-shot learning with language models. In True few-shot learning with language models. abs/2105.11447
Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250
Reimers, N. and Gurevych, I. 2019. Sentencebert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.
Scherer, K. and Wallbott, H. G. 1994. Evidence for universality and cultural variation of differential emotion response patterning. In Journal of personality and social psychology. pp. 310--328
Schick, T. and Schütze, H. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 255--269
Song, K., Tan, X., Qin, T., Lu, J., and Liu, T. 2020. Mpnet: Masked and permuted pretraining for language understanding. In Advances in Neural Information Processing Systems. pp. 16857--16867
Troiano, E., Padó, S., and Klinger, R. 2019. Crowdsourcing and validating event-focused emotion corpora for German and English. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4005--4011 10.18653/v1/P19-1391
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, I., and Polosukhin 2017. Attention is all you need. In Advances in Neural Information Processing Systems.
Vilares, D. and Gómez-Rodríguez, C. 2019. HEAD-QA: A healthcare dataset for complex reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 960--966 10.18653/v1/P19-1092
Wang, S., Fang, H., Khabsa, M., Mao, H., and Ma, H. 2021. Entailment as few-shot learner. In ArXiv. abs/2104.14690
Warstadt, A., Singh, A., and SamuelR 2019. Neural network acceptability judgments. In Transactions of the Association for Computational Linguistics. pp. 625--641 10.1162/tacl_a_00290
Williams, A., Nangia, N., and Bowman, S. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1112--1122
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Patrick Von Platen, C., Ma, Y., Jernite, J., Plu, C., Xu, T. L., Scao, S., Gugger, M., and Drame 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. pp. 38--45 10.18653/v1/2020.emnlp-demos.6
Yin, W., Hay, J., and Roth, D. 2019. Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3914--3923 10.18653/v1/D19-1404
Yin, W., Nazneen Fatema Rajani, D., Radev, R., Socher, C., and Xiong 2020. Universal natural language processing with limited annotations: Try few-shot textual entailment as a start. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 8229--8239 10.18653/v1/2020.emnlp-main.660
Zhang, X., Zhao, J., and Lecun, Y. 2015. language German English Spanish name n GNAD Amazon deISEAR sb10k Amazon SemEval Unified Amazon HeadQA SAB s Mean. In Advances in Neural Information Processing Systems.