Related Work

Deep learning, to a large extent, has freed data scientists from doing feature engineering, which has been one of the essential obstacles to annotation with AL. This advantage has sparked a series of works on deep active learning (DAL) in natural language processing (NLP). Shen et al. (2017) conduct one of the first investigations on DAL in sequence tagging tasks. They propose an efficient way of quantifying the uncertainty of sentences, namely maximal normalized log probability (MNLP), by averaging log probabilities of their tokens. They also address the problem of excessive duration of a neural network training step during an AL iteration by interleaving online learning with training from scratch. In our work, we take MNLP as a query strategy for experiments on sequence tagging tasks since it has demonstrated a good trade-off between quality and computational performance. We consider that online learning can potentially be used as a complement to our algorithms. Since the most time-consuming part of an AL iteration can be model inference instead of training, in this work, we also pay attention to the acceleration of the inference step.

Several recent publications investigate deep pretrained models based on the Transformer architecture (Vaswani et al., 2017), ELMo (Peters et al., 2018), and ULMFiT (Howard and Ruder, 2018) in AL on NLP tasks (Prabhu et al., 2019;Ein-Dor et al., 2020;Yuan et al., 2020;Shelmanov et al., 2021). We continue this line of works by relying on pre-trained Transformers since this architecture has been shown promising for AL in NLP due to its good qualitative and computational performance.

A few works have experimented with Bayesian query strategies for AL. Shen et al. (2017), Sid-dhant and Lipton (2018), Ein-Dor et al. (2020), and Shelmanov et al. (2021 leverage Monte Carlo dropout (Gal and Ghahramani, 2016) for quantifying uncertainty of models. Siddhant and Lipton (2018) also apply the Bayes by backprop algorithm (Blundell et al., 2015) for performing variational inference of a Bayesian neural network. This approach demonstrates the best improvements upon the baseline but introduces large computational overhead both for training and uncertainty estimation of a model, as well as the memory overhead for storing parameters of a Bayesian neural network. The query strategies based on Monte Carlo dropout do not affect the model training procedure and do not change the memory footprint. However, they also suffer from slow uncertainty estimation due to the necessity of making multiple stochastic predictions, while their empirical evaluations with Transformers in recent works (Ein-Dor et al., 2020;Shelmanov et al., 2021) do not demonstrate big advantages. Therefore, we do not use Bayesian query strategies in our experiments and adhere to the classical uncertainty-based query strategies.

Recently proposed alternatives to uncertaintybased query strategies leverage reinforcement learning and imitation learning (Fang et al., 2017;Liu et al., 2018;Vu et al., 2019;Brantley et al., 2020). This series of works aims at constructing trainable policy-based query strategies. However, this requires an excessive amount of computation while the transferability of learned policies across domains and tasks is underresearched.

Finally, Lowell et al. ( 2019) question the usefulness of AL techniques in general. They demonstrate that due to the ASM problem, AL can be even detrimental to the performance of the successor. This finding is also revealed for classical machine learning models by Baldridge and Osborne (2004), Tomanek andMorik (2011), Hu et al. (2016) and supported by experiments with Transformers in (Shelmanov et al., 2021). Our work directly addresses the question raised by Lowell et al. (2019) and suggests a simple solution to the ASM problem. Moreover, we combine it with the method proposed by Shelmanov et al. (2021), who suggest using distilled models for instance acquisition and their teacher models as successors.

 References: 
Baldridge, J. and Osborne, M. 2004. Active learning and the total cost of annotation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. pp. 9--16
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. 2015. Weight uncertainty in neural network. In Proceedings of the 32nd International Conference on Machine Learning. pp. 1613--1622
Brantley, K., Daumé, H., Iii, and Sharaf, A. 2020. Active imitation learning with noisy guidance. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2093--2105
Clark, K., Luong, M., Le, Q. V., and Manning, C. D. 2020. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations.
David A Cohn, Z. and Ghahramani, M.I.J. 1996. Active learning with statistical models. In Journal of artificial intelligence research. pp. 129--145
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423
Liat Ein-Dor, A., Halfon, A., Gera, E., Shnarch, L., Dankin, L., Choshen, M., Danilevsky, R., Aharonov, Y., Katz, N., and Slonim 2020. Active Learning for BERT: An Empirical Study. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 7949--7962 10.18653/v1/2020.emnlp-main.638
Erdmann, A., Wrisley, D. J., Allen, B., Brown, C., Cohen-Bodénès, S., Elsner, M., Feng, Y., Joseph, B., Joyeux-Prunel, B., and De Marneffe, M. 2019. Practical, efficient, and customizable active learning for named entity recognition in the digital humanities. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2223--2234 10.18653/v1/N19-1231
Fang, M., Li, Y., and Cohn, T. 2017. Learning how to active learn: A deep reinforcement learning approach. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pp. 595--605 10.18653/v1/D17-1063
Gal, Y. and Ghahramani, Z. 2016. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016. pp. 1050--1059
Hinton, G. E., Vinyals, O., and Dean, J. 2015. Distilling the knowledge in a neural network. In Distilling the knowledge in a neural network. abs/1503.02531
Howard, J. and Ruder, S. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 328--339 10.18653/v1/P18-1031
Hu, R., Namee, B. M., and Delany, S. J. 2016. Active learning for text classification with reusability. Expert Systems with Applications. In An International Journal. pp. 438--449
Ibragimov, B. and Gusev, G. 2019. Minimal variance sampling in stochastic gradient boosting. In Advances in Neural Information Processing Systems.
Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., and Liu, T. 2017. Lightgbm: A highly efficient gradient boosting decision tree. In Advances in Neural Information Processing Systems.
Lee, K., Lee, K., Lee, H., and Shin, J. 2018. A simple unified framework for detecting outof-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems.
David, D., Lewis, W. A., and Gale 1994. A sequential algorithm for training text classifiers. In Proceedings of the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval. pp. 3--12 10.1007/978-1-4471-2099-5_1
Liu, M., Buntine, W., and Haffari, G. 2018. Learning how to actively learn: A deep imitation learning approach. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 1874--1883 10.18653/v1/P18-1174
Pennington, J., Socher, R., and Manning, C. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1532--1543 10.3115/v1/D14-1162
Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/N18-1202
Podolskiy, A., Lipin, D., Bout, A., Artemova, E., and Piontkovskaya, I. 2021. Revisiting mahalanobis distance for transformer-based out-of-domain detection. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021. pp. 13675--13682
Prabhu, A., Dognin, C., and Singh, M. 2019. Sampling bias in deep active classification: An empirical study. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 4049--4059 10.18653/v1/D19-1417
Sameer Pradhan, A., Moschitti, N., Xue, H.T., Ng, A., Björkelund, O., Uryupina, Y., Zhang, Z., and Zhong 2013. Towards robust linguistic analysis using OntoNotes. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning. pp. 143--152
Pruthi, G., Liu, F., Kale, S., and Sundararajan, M. 2020. Estimating training data influence by tracing gradient descent. In Advances in Neural Information Processing Systems.
Tjong, E. F., Sang, K., and De Meulder, F. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning. pp. 142--147
Sanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. In DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv:1910.01108
Settles, B. 2009. Active learning literature survey. In Active learning literature survey. 1648
Settles, B. and Craven, M. 2008. An analysis of active learning strategies for sequence labeling tasks. In Conference on Empirical Methods in Natural Language Processing. pp. 1070--1079
Shelmanov, A., Puzyrev, D., Kupriyanova, L., Belyakov, D., Larionov, D., Khromov, N., Kozlova, O., Artemova, E., Dmitry, V., Dylov, A., and Panchenko 2021. Active learning for sequence tagging with deep pre-trained models and Bayesian uncertainty estimates. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 1698--1712
Shen, Y., Yun, H., Lipton, Z., Kronrod, Y., and Anandkumar, A. 2017. Deep active learning for named entity recognition. In Proceedings of the 2nd Workshop on Representation Learning for NLP. pp. 252--256 10.18653/v1/W17-2630
Siddhant, A. and Lipton, Z. C. 2018. Deep Bayesian active learning for natural language processing: Results of a large-scale empirical study. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 2904--2909 10.18653/v1/D18-1318
Tomanek, K. and Morik, K. 2011. Inspecting sample reusability for active learning. In Active Learning and Experimental Design workshop In conjunction with AISTATS 2010. pp. 169--181
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008
Vu, T., Liu, M., Phung, D., and Haffari, G. 2019. Learning how to active learn by dreaming. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4091--4101 10.18653/v1/P19-1401
Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Russ, R., Salakhutdinov, Q.V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems.
Yuan, M., Lin, H., and Boyd, J. Cold-start active learning through self-supervised language modeling. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 7935--7948 10.18653/v1/2020.emnlp-main.637
Zhang, X., Zhao, J., and Lecun, Y. 2015. Character-level convolutional networks for text classification. In Proceedings of the 28th International Conference on Neural Information Processing Systems. pp. 649--657
Zhou, W., Liu, F., and Chen, M. 2021. Contrastive out-of-distribution detection for pretrained transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 1100--1111 10.18653/v1/2021.emnlp-main.84