we instructed participants to mark examples where they weren’t confident, such as those that contained words or cultural references they didn’t understand.

 References: 
Beata Beigman Klebanov, Chee Wee (Ben) Leong, and Michael Flor. 2018. A corpus of non-native written English annotated for metaphor. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 86–91, New Orleans, Louisiana. Association for Computational Linguistics. Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185–5198, Online. Association for Computational Linguistics. Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. Experience grounds language. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8718–8735, Online. Association for Computational Linguistics. Yuri Bizzoni and Mehdi Ghanimifard. 2018. Bigrams and BiLSTMs two neural networks for sequential metaphor detection. In Proceedings of the Workshop on Figurative Language Processing, pages 91–101, New Orleans, Louisiana. Association for Computational Linguistics. Yuri Bizzoni and Shalom Lappin. 2018. Predicting human metaphor paraphrase judgments with deep neural networks. In Proceedings of the Workshop on Figurative Language Processing, pages 45–55, New Orleans, Louisiana. Association for Computational Linguistics. Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh- Tensorflow. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. Robyn Carston and Catherine Wearing. 2011. Metaphor, hyperbole and simile: A pragmatic approach. Language and Cognition, 3(2):283–312. Tuhin Chakrabarty, Yejin Choi, and Vered Shwartz.
2021. It’s not rocket science : Interpreting figurative language in narratives. ArXiv, abs/2109.00087. Thomas C. Cooper. 1999. Processing of idioms by l2 learners of english. TESOL Quarterly, 33:233–262. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. Gilles Fauconnier and Mark Turner. 2003. Conceptual blending, form and meaning. Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books. Susan Fussell and Mallie Moss. 2008. Figurative language in emotional communication. Ge Gao, Eunsol Choi, Yejin Choi, and Luke Zettlemoyer. 2018. Neural metaphor detection in context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 607–613, Brussels, Belgium. Association for Computational Linguistics. Kahlil Gibran. 1926. Sand and Foam; a book of aphorisms. A.A Knopf. Sam Glucksberg. 2003. The psycholinguistics of metaphor. Trends in cognitive sciences, 7:92–96. Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith.
2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107–112, New Orleans, Louisiana. Association for Computational Linguistics. Harsh Jhamtani, Varun Gangal, Eduard Hovy, and Taylor Berg-Kirkpatrick. 2021. Investigating robustness of dialog models to popular figurative language constructs. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7476–7485, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. G. Lakoff and M. Johnson. 1981. Metaphors we Live By. University of Chicago Press. Chee Wee (Ben) Leong, Beata Beigman Klebanov, Chris Hamill, Egon Stemle, Rutuja Ubale, and Xianyang Chen. 2020. A report on the 2020 VUA and TOEFL metaphor detection shared task. In Proceedings of the Second Workshop on Figurative Language Processing, pages 18–29, Online. Association for Computational Linguistics. Chee Wee (Ben) Leong, Beata Beigman Klebanov, and Ekaterina Shutova. 2018. A report on the 2018 VUA metaphor detection shared task. In Proceedings of the Workshop on Figurative Language Processing, pages 56–66, New Orleans, Louisiana. Association for Computational Linguistics. Hector J. Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR’12, page 552–561. AAAI Press. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. Edward Loper and Steven Bird. 2002. Nltk: The natural language toolkit. CoRR, cs.CL/0205028. Rui Mao, Chenghua Lin, and Frank Guerin. 2018. Word embedding and WordNet based metaphor identification and interpretation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1222– 1231, Melbourne, Australia. Association for Computational Linguistics. J. S Mio and A. N Katz. 1996. Metaphor: Implications and Applications. Psychology Press. Saif Mohammad, Ekaterina Shutova, and Peter Turney. 2016. Metaphor as a medium for emotion: An empirical study. In Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics, pages 23–33, Berlin, Germany. Association for Computational Linguistics. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. Malay Pramanick, Ashim Gupta, and Pabitra Mitra.
2018. An LSTM-CRF based approach to token-level metaphor detection. In Proceedings of the Workshop on Figurative Language Processing , pages 67–75, New Orleans, Louisiana. Association for Computational Linguistics. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Winogrande: An adversarial winograd schema challenge at scale. In AAAI. Ekaterina Shutova. 2010. Automatic metaphor interpretation as a paraphrasing task. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 1029–1037, Los Angeles, California. Association for Computational Linguistics. Ekaterina Shutova. 2011. Computational approaches to figurative language. Gerard J Steen, Alleta G Dorst, Berenike Herrmann, Anna A Kaal, Tina Krennmayr, and Trynetje Pasma.
2010. A Method for Linguistic Metaphor Identification: From MIP to MIPVU. John Benjamins. Kevin Stowe and Martha Palmer. 2018. Leveraging syntactic constructions for metaphor identification. In Proceedings of the Workshop on Figurative Language Processing, pages 17–26, New Orleans, Louisiana. Association for Computational Linguistics. Chang Su, Shuman Huang, and Yijiang Chen. 2017. Automatic detection and interpretation of nominal metaphor based on the theory of meaning. Neurocomputing, 219:300–311. John Sweller. 2006. Discussion of ’emerging topics in cognitive load research: Using learner and information characteristics in the design of powerful learning environments’. Applied Cognitive Psychology - APPL COGNITIVE PSYCHOL, 20:353–357. Xiaoyu Tong, Ekaterina Shutova, and Martha Lewis.
2021. Recent advances in neural metaphor processing: A linguistic, cognitive and social perspective. In NAACL 2021. Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, Eric Nyberg, and Chris Dyer. 2014. Metaphor detection with cross-lingual model transfer. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 248–258, Baltimore, Maryland. Association for Computational Linguistics. P. Wolff and Dedre Gentner. 2000. Evidence for roleneutral initial processing of metaphors. Journal of experimental psychology. Learning, memory, and cognition, 26 2:529–41.
A Crowdsourcing Details We crowdsource metaphorical expressions and their interpretations through Amazon Mechanical Turk. Workers were recruited from the United States and were limited to those who had a > 98% approval rating on the platform, and who had also completed more than 1000 Human Intelligence Tasks (HITs). Data collection was split into two stages: in the first stage, 1458 train examples, and all the dev and test examples were collected. In the second stage, the remaining 6558 training examples were collected. We identified some workers who created especially good examples in the first stage, and recruited them back for more examples in the second stage. Workers were paid $0.33 for each pair of sentences and were asked to generate 3 pairs at a time. An author of this paper wrote an initial pilot set of sentences, and timed themselves while writing some sentences. They found that each pair took around 1 minute to write, though this varied (less creative examples took less time, while more creative examples took more time). This extrapolates to an hourly rate of 19.80 USD, which is above the minimum wage in all US states, where workers were located. Our HIT task was structured as follows: At the top of the page, the workers are shown the following instructions: "Your task is to generate three pairs of sentences with opposite or very different meanings, both of which contain rare/creative metaphors, which means metaphors that would not appear often in text on the internet, books, social media or news sites, but that can still be easily understood by people. For each metaphor, you should also provide a literal (non-metaphorical) sentence with the same meaning." Then, we display one example of a valid sentence pair. There is a button that opens a modal with more detailed instructions and some more valid/invalid examples for reference. Below that, we display three random words, which workers are encouraged to use in their sentences if they get stuck. Finally, we display three sets of
5 text fields for workers to fill in: one for the start phrase, two for each metaphorical phrase, and two for each literal interpretation. As the user types in each start phrase, we prepend a copy of their phrase before the corresponding metaphor fields in the UI using some embedded JavaScript, which we found helped reduce confusion and resulted in less improperly formatted responses. We launched many batches of these HITs until we had collected the desired quantity of data. Then, we converted the form responses into sentence pairs and validated each pair by hand before adding it to our dataset. B Invalid Examples Figurative language examples collected from crowdworkers were excluded if they (a) did not make sense given the meaning and the metaphorical expression, (b) had grammar or spelling errors that rendered them unintelligible, or (c) did not follow the format specified by the task template. Examples are given below:
1. Do not make sense given the meaning and the metaphorical expression Paired sentences Possible answers He was resourceful like toilet paper He was very resourceful.He was resourceful like a mess He wasn’t resourceful at all The night was as long as a spool of thread The night is longThe night was as long as a winding road The night dragged on the concert of the lession is a main and a major we concert everyonethe concert of the lession features we concert our loved one Table 7: Examples that were rejected due to being nonsensical.
2. Significant grammar or spelling errors Paired sentences Possible answersfallten data are very much trusted fallten are nicefallten data are very valuable flatten are safe CAR IS BIRD FEATHEAR CAR SITE IS ROUGHCAR IS COTTON CAR SITE IS HARD Inflation is as natural as Minnesota rainfall in June Inflation is perfectly naturalInflation is as natural as Minnesota snowfall in June Patient is in a natural result of other things Table 8: Examples that were rejected due to having significant spelling or grammar errors.
3. Do not follow format Paired sentences Possible answers This attack is as weak as a feather The attack is uselessThis attack is as weak as a breeze The attack doesn’t work My car motor is dusty like old cave Car motor is very rustyMy car motor is dusty like abandon building car motor is very dusty the writer is stuck between a rock And another hard place He is just stuck doesnt have a choicethe writer is stuck between a rock And a pebble The writer can get over the pebble Table 9: Examples that were rejected due to not following the specified format. Efforts were made to ensure that the final dataset contains no offensive content or personally identifiable information. WorkerID and other potentailly personally identifying information were not included. C Backward accuracies Model Zero-shot Fine-tuned (L) GPT-2 52.18 52.00 GPT-neo 1.3B 54.36 63.44 GPT-3 Curie 58.46 74.83 Table 10: Zero-shot and finetuned backward autoregressive model accuracies on the test set D Accuracy breakdown by Part-of-Speech D.1 Subject Part of speech Accuracy Frequency NN 0.8569 538 PRP 0.8526 156 PRP$ NN 0.9 110 NN NN 0.8889 63 DT NN 0.8182 44 NN NN NN 0.9375 32 JJ NN 0.9167 12 Table 11: Accuracy breakdown and frequency of parts of speech in metaphor subjects. Only part-of-speech patterns with greater than 10 occurrences are shown. D.2 Relation Part of speech Accuracy Frequency VBZ NN IN 0.8421 152 VBD RB JJ IN 0.8904 146 VBZ RB JJ IN 0.8889 99 VBZ 0.8352 91 VBD NN IN 0.8806 67 VBD 0.9180 61 VBN IN 0.9545 22 NN IN 0.8636 22 VBD JJ IN 0.9048 21 NNS IN 0.8889 18 VBD IN 0.8462 13 VBZ IN 1.0 13 VBD RB VBN IN 0.8182 11 Table 12: Accuracy breakdown and frequency of parts of speech in metaphor relations. Only part-of-speech patterns with greater than 10 occurrences are shown. D.3 Object Part of speech Accuracy Frequency NN 0.8788 429 NN NN 0.8992 129 JJ NN 0.8352 91 NN IN NN 0.8372 43 JJ NN NN 0.8710 31 NN NN NN 0.9130 23 VBG NN 0.9545 22 NN IN JJ NN 0.6154 13 PRP$ NN 1.0 11 JJ 0.6364 11 NN IN NN NN 0.8182 11 Table 13: Accuracy breakdown and frequency of parts of speech in metaphor objects. Only part-of-speech patterns with greater than 10 occurrences are shown. E Accuracy breakdown by hypernyms E.1 Subject Synset Accuracy Frequency adult.n.01 0.8736 182 male.n.02 0.8684 152 woman.n.01 0.7391 46 female.n.02 0.9130 46 show.n.03 0.875 24 product.n.02 0.8636 22 motor_vehicle.n.01 0.9048 21 activity.n.01 0.8421 19 emotion.n.01 0.6667 18 publication.n.01 0.8333 18 feline.n.01 0.9375 16 being.n.01 0.7143 14 performer.n.01 0.8333 12 canine.n.02 12 body_covering.n.01 0.8333 12 vessel.n.03 0.8333 12 sound.n.01 1.0 12 domestic_animal.n.01 0.9167 12 person.n.01 0.8 10 scheme.n.01 0.9 10 contestant.n.01 1.0 10 Table 14: Accuracy breakdown and frequency of Word- Net hypernyms in metaphor subjects. Only hypernyms with 10 or greater occurrences are shown. E.2 Object Synset Accuracy Frequency time_period.n.01 0.85 20 natural_object.n.01 0.8947 19 person.n.01 0.8824 17 large_integer.n.01 0.9286 14 adult.n.01 1.0 14 solid.n.01 0.9167 13 male.n.02 1.0 13 child.n.02 0.8333 12 body_of_water.n.01 0.75 12 body_covering.n.01 0.8333 12 digit.n.01 0.9167 12 region.n.01 0.8182 11 beverage.n.01 0.8182 11 juvenile.n.01 0.8182 11 container.n.01 0.9 10 rodent.n.01 0.9 10 feline.n.01 0.7 10 building.n.01 0.8 10 time_unit.n.01 0.9 10 travel.v.01 0.7 10 Table 15: Accuracy breakdown and frequency of Word- Net hypernyms in metaphor objects. Only hypernyms with 10 or greater occurrences are shown. F Generation examples Generation examples can be found in Table 16. Startphrase Completion Label His temper was a marshmallow. That is to say, he was very easily angered, and once he was, he was very hard to cool down Incorrect He’s got the swimming ability of a pack of dolphins. That is to say, he’s got none Incorrect The villain is as beautiful as the Joker. That is to say, he’s not Correct The child has the energy of a sloth. That is to say, he is lazy Correct The girl moved as fast as a turtle. That is to say, she moved at a turtle’s pace Literal The sadness of her death was a flea.That is to say, It was a flea that was a sadness Literal Table 16: Examples of completions generated by GPT-3 Davinci.