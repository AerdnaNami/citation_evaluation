Related Work

Math Problem Understanding Math problem understanding task focuses on understanding the text, formulas and symbols in math domain. A surge of works aim to understand the math formulas for problem solving or mathematical information retrieval. In this way, the formula is usually transformed as a tree or graph (e.g., Operator Tree (Zanibbi and Blostein, 2012)), then network embedding methods Mansouri et al. (2019) and graph neural networkSong and Chen (2021) are utilized to encode it. Besides, a number of works focus on understanding math problem based on the textual information. Among them, Math Word Problem (MWP) Solving is a popular task that generates answers of math word problems. Numerous deep learning based methods have been proposed to tackle MWP, ranging from Seq2Seq (Chiang and Chen, 2019;Li et al., 2019), Seq2Tree Qin et al., 2020), to Pre-trained Language Models Liang et al., 2021). More recently, several works attempt to modeling more complex math problems (Huang et al., 2020;Hendrycks et al., 2021) that require to understand both textual and formula information.

 References: 
Beltagy, I., Lo, K., and Cohan, A. 2019. Scibert: A pretrained language model for scientific text. In Scibert: A pretrained language model for scientific text. arXiv:1903.10676
Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., and Yakhnenko, O. 2013. Translating embeddings for modeling multirelational data. In Advances in neural information processing systems. pp. 26
Ting, R., Chiang, Y., and Chen 2019. Semantically-aligned equation generation for solving and reasoning math word problems. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2656--2668
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805
Ghosal, D., Hazarika, D., Roy, A., Majumder, N., Mihalcea, R., and Poria, S. 2020. Kingdom: Knowledge-guided domain adaptation for sentiment analysis. In Kingdom: Knowledge-guided domain adaptation for sentiment analysis. arXiv:2005.00791
Suchin Gururangan, A., Marasović, S., Swayamdipta, K., Lo, I., Beltagy, D., Downey, N. A., and Smith 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 8342--8360 10.18653/v1/2020.acl-main.740
Hendrycks, D., Burns, C., Kadavath, S., Arora, A., and Basart, S. Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. In Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv:2103.03874
Huang, Z., Liu, Q., Gao, W., Wu, J., Yin, Y., Wang, H., and Chen, E. 2020. Neural mathematical solver with enhanced formula structure. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 1729--1732
Kim, B., Ki, K. S., Lee, D., and Gweon, G. 2020. Point to the expression: Solving algebraic word problems using the expression-pointer transformer model. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3768--3779
Kim, Y. 2014. Convolutional neural networks for sentence classification. In Convolutional neural networks for sentence classification. arXiv:1408.5882
Lai, S., Xu, L., Liu, K., and Zhao, J. 2015. Recurrent convolutional neural networks for text classification. In Twenty-ninth AAAI conference on artificial intelligence.
Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., Ho So, C., and Kang, J. 2020. Biobert: a pre-trained biomedical language representation model for biomedical text mining. In Bioinformatics. pp. 1234--1240
Li, J., Wang, L., Zhang, J., Wang, Y., Dai, B. T., and Zhang, D. 2019. Modeling intrarelation in math word problems with different functional multi-head attentions. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 6162--6167
Li, J., Zhang, Z., Zhao, H., Zhou, X., and Zhou, X. 2020. Task-specific objectives of pre-trained language models for dialogue adaptation. In Task-specific objectives of pre-trained language models for dialogue adaptation. arXiv:2009.04984
Liang, Z., Zhang, J., Shao, J., and Zhang, X. 2021. Mwp-bert: A strong baseline for math word problems. In Mwp-bert: A strong baseline for math word problems. arXiv:2107.13435
Liu, Q., Huang, Z., Huang, Z., Liu, C., Chen, E., Su, Y., and Hu, G. 2018. Finding similar exercises in online education systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. pp. 1821--1830
Loshchilov, I. and Hutter, F. 2017. arXiv:1711.05101
Mansouri, B., Rohatgi, S., Douglas, W., Oard, J., Wu, L., Giles, R., and Zanibbi 2019. Tangent-cft: An embedding model for mathematical formulas. In Proceedings of the 2019 ACM SIGIR international conference on theory of information retrieval. pp. 11--18
Peng, S., Yuan, K., Gao, L., and Tang, Z. 2021. Mathbert: A pre-trained model for mathematical formula understanding. In Mathbert: A pre-trained model for mathematical formula understanding. arXiv:2105.00377
Qin, J., Lin, L., Liang, X., Zhang, R., and Lin, L. 2020. Semantically-aligned universal tree-structured solver for math word problems. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3780--3789
Reusch, A. and Thiele, M.
Michael Sejr Schlichtkrull, Thomas, N., Kipf, P., Bloem, R., Van Den, Berg, I., Titov, M., and Welling 2018. Modeling relational data with graph convolutional networks. In ESWC.
Tracy Shen, J., Yamashita, M., Prihar, E., Heffernan, N., Wu, X., Graff, B., and Lee, D. 2021. Mathbert: A pre-trained language model for general nlp tasks in mathematics education. In Mathbert: A pre-trained language model for general nlp tasks in mathematics education. arXiv:2106.07340
Song, Y. and Chen, X. 2021. Searching for mathematical formulas based on graph representation learning. In International Conference on Intelligent Computer Mathematics. pp. 137--152
Veličković, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., and Bengio, Y. 2018. Graph attention networks. In International Conference on Learning Representations.
Wang, L., Zhang, D., Zhang, J., Xu, X., Gao, L., Dai, B. T., and Shen, H.T. 2019. Template-based math word problem solvers with recursive neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 7144--7151
Wang, W., Tang, Q., and Livescu, K. 2020. Unsupervised pre-training of bidirectional speech encoders via masked reconstruction. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 6889--6893
Xiang, S., Fu, Y., You, G., and Liu, T. 2020. Unsupervised domain adaptation through synthesis for person re-identification. In 2020 IEEE International Conference on Multimedia and Expo (ICME). pp. 1--6
Zanibbi, R. and Blostein, D. 2012. Recognition and retrieval of mathematical expressions. In International Journal on Document Analysis and Recognition (IJDAR). pp. 331--357
Zhou, W., Lee, D., Selvam, R. K., and Lee, S. Bill Yuchen Lin, and Xiang Ren. 2020. Pre-training text-to-text transformers for concept-centric common sense. In Bill Yuchen Lin, and Xiang Ren. 2020. Pre-training text-to-text transformers for concept-centric common sense. arXiv:2011.07956