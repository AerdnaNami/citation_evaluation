Introduction

As neural networks are achieving extraordinary prediction performance in dominating NLP tasks, it becomes increasingly important to explain why a model makes a specific prediction. Recent work starts to extract snippets of input text as the faithful rationale of prediction (Jain et al., 2020;Paranjape et al., 2020), with rationale defined as "shortest yet sufficient subset of input to predict the same label" (Lei et al., 2016;Bastings et al., 2019). The underneath assumption is two fold: (1) by retaining the label, we are extracting texts used by predictors (Jain et al., 2020); and (2) short rationales are more readable and intuitive for end users,  (Socher et al., 2013). and therefore are preferred for human understanding (Vafa et al., 2021). Importantly, prior work has knowingly traded off some amount of model performance in order to achieve shortest rationales. For example, when using less than 50% of text as rationales-for-predictions, Paranjape et al. (2020) achieved an accuracy of 84.0% (compared to 91.0% if using the full text). But existing work propose shortest rationales have better human interpretability by intuition rather than from empirical human studies (Vafa et al., 2021). Moreover, when the rationale is too short, the model has a much higher chance of missing the main point in the full text. In Figure 1(A), though the model is able to make the correct positive prediction when using only 20% of the text, it relies on a particular adjective, "lifeaffirming", which is seemingly positive but does not reflect the author's sentiment. They may simply be confused when presented to end users.

In this work, we ask: is shortest rationales really supportive of human understanding? and examine the effects of rationale length on human understanding and performance. Our work includes two steps: First, we design and train a self-explaining model that allows for sparsity control. That is, the model can flexible extract rationales of a targeted length, such that we can compare user perceptions on a set of rationales with varying lengths. As shown in Figure 1, our model design consider three aspects: (A) controllability on rationale length, (B) being context-aware such to priorize certain amount of semantic information in the text, and, (C) extracting continuous text for readability. Through automated valuation on ERASER (DeYoung et al., 2019) datasets, we show that our model outperforms existing self-explaining baselines on both end-task prediction and rationale alignment with human ground annotations.

Using the rationales with different lengths generated from the model, we conduct human studies to evaluate human accuracy and confidence on predicting the document categories given only rationales. Our results show the best explanations for human understanding are largely not the shortest rationales. Given rationales with short length at 10%, human accuracy on predicting model class is worse than accuracy on the random baseline. Furthermore, while most prior work extracts 10%-30% of text to be rationale (Jain et al., 2020;Paranjape et al., 2020), human accuracy tend to stablize after seeing 40% of the full text. Our result sounds a cautionary note, and we encourage future work to more rigorously define or evaluate the typical assumption of "shorter rationales are easier to interpret" before trading off model accuracy for it.

 References: 
Bastings, J., Aziz, W., and Titov, I. 2019. Interpretable neural predictions with differentiable binary variables. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 2963--2977 10.18653/v1/P19-1284
Chang, S., Zhang, Y., Yu, M., and Jaakkola, T. 2020. Invariant rationalization. In International Conference on Machine Learning. pp. 1448--1458
Chen, J., Song, L., Martin, J., and Wainwright, M.I.J. 2018. Learning to explain: An information-theoretic perspective on model interpretation. In Proceedings of IEEE Conference on Machine Learning (ICML).
Deyoung, J., Jain, S., Fatema Rajani, N., Lehman, E., Xiong, C., Socher, R., and Wallace, B.C. 2019. Eraser: A benchmark to evaluate rationalized nlp models. In Eraser: A benchmark to evaluate rationalized nlp models. arXiv:1911.03429
Fong, R., Patrick, M., and Vedaldi, A. 2019. Understanding deep networks via extremal perturbations and smooth masks. In Proceedings of IEEE International Conference on Computer Vision (ICCV).
Jain, S., Wiegreffe, S., Pinter, Y., and Wallace, B. C. 2020. Learning to faithfully rationalize by construction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4459--4473 10.18653/v1/2020.acl-main.409
Jang, E., Gu, S., and Poole, B. 2017. Categorical reparameterization with gumbel-softmax. In Proceedings of International Conference on Learning Representations (ICLR).
Lei, T., Barzilay, R., and Jaakkola, T. 2016. Rationalizing neural predictions. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 107--117
Lertvittayakumjorn, P. and Toni, F. 2019. Human-grounded evaluations of explanation methods for text classification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5195--5205 10.18653/v1/D19-1523
Bhargavi Paranjape, M., Joshi, J., Thickstun, H., Hajishirzi, L., and Zettlemoyer 2020. An information bottleneck approach for controlling conciseness in rationale extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1938--1952
Marco Tulio Ribeiro, S., Singh, C., and Guestrin 2016. why should i trust you?"" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. pp. 1135--1144
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.