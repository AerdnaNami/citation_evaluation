Related Work

Bandit learning has been applied to a variety of NLP problems including neural machine translation (NMT; Sokolov et al., 2017;Kreutzer et al., 2018a,b;Mendoncca et al., 2021), structured prediction (Sokolov et al., 2016), semantic parsing (Lawrence and Riezler, 2018), intent recognition (Falke and Lehnen, 2021), and summarization (Gunasekara et al., 2021). Human feedback has been studied as a direct learning signal for NMT (Kreutzer et al., 2018b;Mendoncca et al., 2021), semantic parsing (Lawrence and Riezler, 2018), summarization (Stiennon et al., 2020), and dialogue (Jaques et al., 2020). Nguyen et al. (2017) simulates bandit feedback to improve an MT system fully trained on a large annotated dataset, including analyzing robustness to feedback perturbations. Our work shows that simulated bandit feedback is an effective learning signal for extractive question answering tasks. Our work differs in focus on reducing annotation costs by relying on few annotated examples only to train the initial model, or by eliminating the need for in-domain annotation completely by relying on data in other domains to train initial models. Alternative forms of supervision for QA have been explored in prior work, such as explicitly providing fine-grained information (Dua et al., 2020;Khashabi et al., 2020a). Kratzwald et al. (2020) resembles our setting in that it seeks binary feed-back to replace span annotation, but their goal is to create supervised data more economically. Domain adaptation for QA has been studied in prior work (Fisch et al., 2019;Khashabi et al., 2020b), including using data augmentation (Yue et al., 2021), adversarial training , contrastive method (Yue et al., 2021), back-training (Kulshreshtha et al., 2021, and exploiting small lottery subnetworks (Zhu et al., 2021).

 References: 
Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. 2002. The nonstochastic multiarmed bandit problem. In SIAM Journal on Computing. pp. 48--77
Bietti, A., Agarwal, A., and Langford, J. 2018. A contextual bandit bake-off. In A contextual bandit bake-off.
Choi, E., He, H., Iyyer, M., Yatskar, M., Wen Tau Yih, Y., Choi, P., Liang, L., and Zettlemoyer 2018. Quac: Question answering in context. EMNLP. In Quac: Question answering in context. EMNLP.
Dua, D., Singh, S., and Gardner, M. 2020. Benefits of intermediate annotations in reading comprehension. In Benefits of intermediate annotations in reading comprehension.
Dunn, M., Sagun, L., Higgins, M., Güney, V. U., Cirik, V., and Cho, K. 2017. Searchqa: A new q&a dataset augmented with context from a search engine. In Searchqa: A new q&a dataset augmented with context from a search engine.
Falke, T. and Lehnen, P. 2021. Feedback attribution for counterfactual bandit learning in multidomain spoken language understanding. In EMNLP.
Fisch, A., Talmor, A., Jia, R., Seo, M., Choi, E., and Chen, D. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In MRQA@EMNLP.
Gunasekara, C., Feigenblat, G., Sznajder, B., Aharonov, R., and Joshi, S. 2021. Using question answering rewards to improve abstractive summarization. In Findings@EMNLP.
Daniel, G., Horvitz, D. J., and Thompson 1952. A generalization of sampling without replacement from a finite universe. In Journal of the American Statistical Association.
Jaques, N., Shen, J. H., Ghandeharioun, A., Ferguson, C., Lapedriza, A., Jones, N., Gu, S., and Picard, R. 2020. Human-centric dialog training via offline reinforcement learning. In EMNLP.
Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., and Levy, O. 2020. Spanbert: Improving pre-training by representing and predicting spans. In Spanbert: Improving pre-training by representing and predicting spans.
Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
Khashabi, D., Khot, T., and Sabharwal, A. 2020. More bang for your buck: Natural perturbation for robust question answering. In More bang for your buck: Natural perturbation for robust question answering.
Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P. E., and Hajishirzi, H. 2020. Unifiedqa: Crossing format boundaries with a single qa system. Findings@EMNLP. In Unifiedqa: Crossing format boundaries with a single qa system. Findings@EMNLP.
Kratzwald, B., Feuerriegel, S., and Sun, H. 2020. Learning a cost-effective annotation policy for question answering. In EMNLP.
Kreutzer, J., Khadivi, S., Matusov, E., and Riezler, S. 2018. Can neural machine translation be improved with user feedback? NAACL. In Can neural machine translation be improved with user feedback? NAACL.
Kreutzer, J., Uyheng, J., and Riezler, S. 2018. Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning. In Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning.
Kulshreshtha, D., Belfer, R., Serban, I., and Reddy, S. 2021. Back-training excels selftraining at unsupervised domain adaptation of question generation and passage retrieval. In EMNLP.
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A. P., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M., Dai, A. M., Uszkoreit, J., Le, Q. V., and Petrov, S. 2019. Natural questions: A benchmark for question answering research. TACL. In Natural questions: A benchmark for question answering research. TACL.
Lamm, M., Palomaki, J., Alberti, C., Andor, D., Choi, E., Soares, L.B., and Collins, M. 2021. Qed: A framework and dataset for explanations in question answering. In Qed: A framework and dataset for explanations in question answering.
Langford, J. and Zhang, T. 2007. The epochgreedy algorithm for contextual multi-armed bandits. In NeurIPS.
Lawrence, C. and Riezler, S. 2018. Improving a neural semantic parser by counterfactual learning from human bandit feedback. In ACL.
Lazaridou, A., Kuncoro, A., Gribovskaya, E., Agrawal, D., Liska, A., Terzi, T., Gimenez, M., De Masson D'autume, C., Kociský, T., Ruder, S., Yogatama, D., and Cao, K.
Lee, S., Kim, D., and Park, J. 2019. Domain-agnostic question-answering with adversarial training. In Domain-agnostic question-answering with adversarial training.
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Tau Yih, W., Rocktäschel, T., Riedel, S., and Kiela, D. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Retrieval-augmented generation for knowledge-intensive nlp tasks.
Mendoncca, V., Rei, R., Coheur, L., Sardinha, A., Ana, L., Santos, I., Lisboa, I., Superior T'ecnico, A. I., and Unbabel 2021.
Online learning meets machine translation evaluation: Finding the best systems with the least human effort. In Online learning meets machine translation evaluation: Finding the best systems with the least human effort.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Marc, G., Bellemare, A., Graves, M., Riedmiller, A. K., Fidjeland, G., and Ostrovski 2015. Human-level control through deep reinforcement learning. In Nature. pp. 518
Nguyen, K., Daumé, H., and Boyd-Graber, J. L. 2017. Reinforcement learning for bandit neural machine translation with simulated human feedback. In Reinforcement learning for bandit neural machine translation with simulated human feedback.
Papineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Bleu: a method for automatic evaluation of machine translation.
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. 2016. Squad: 100,000+ questions for machine comprehension of text. In EMNLP.
Ram, O., Kirstain, Y., Berant, J., Globerson, A., and Levy, O. 2021. Few-shot question answering by pretraining span selection. In Few-shot question answering by pretraining span selection.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. 2017. Proximal policy optimization algorithms. arXiv. In Proximal policy optimization algorithms. arXiv.
Seo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H. 2017. Bidirectional attention flow for machine comprehension. In Bidirectional attention flow for machine comprehension.
Sokolov, A., Kreutzer, J., Lo, C., and Riezler, S. 2016. Learning structured predictors from bandit feedback for interactive nlp. In Learning structured predictors from bandit feedback for interactive nlp.
Sokolov, A., Kreutzer, J., Sunderland, K., Danchenko, P., Szymaniak, W., Fürstenau, H., and Riezler, S. 2017. A shared task on bandit learning for machine translation. In A shared task on bandit learning for machine translation.
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R. J., Voss, C., Radford, A., Amodei, D., and Christiano, P. 2020. Learning to summarize from human feedback. In Learning to summarize from human feedback.
Su, D., Xu, Y., Indra Winata, G., Xu, P., Kim, H., Liu, Z., and Fung, P. 2019. Generalizing question answering system with pre-trained language model fine-tuning. In Generalizing question answering system with pre-trained language model fine-tuning.
Sutton, R. S. and Barto, A. G. 1998. Reinforcement learning: An introduction. In Reinforcement learning: An introduction.
Trischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K. 2017. Newsqa: A machine comprehension dataset. In Newsqa: A machine comprehension dataset.
Williams, R. J. 2004. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. In Machine Learning.
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., and Louf, R. Morgan Funtowicz, and Jamie Brew. 2020. Huggingface's transformers: State-of-the-art natural language processing. EMNLP. In Morgan Funtowicz, and Jamie Brew. 2020. Huggingface's transformers: State-of-the-art natural language processing. EMNLP.
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Hotpotqa: A dataset for diverse, explainable multi-hop question answering.
Yao, Z., Tang, Y., Wen-Tau Yih, H., Sun, Y., and Su 2020. An imitation game for learning semantic parsers from user interaction. In An imitation game for learning semantic parsers from user interaction.
Yu, A. W., Dohan, D., Luong, M., Zhao, R., Chen, K., Norouzi, M., and Quoc, V.
Le 2018. Qanet: Combining local convolution with global self-attention for reading comprehension. In Qanet: Combining local convolution with global self-attention for reading comprehension.
Yue, Z., Kratzwald, B., and Feuerriegel, S. 2021. Contrastive domain adaptation for question answering using limited text corpora. In EMNLP.
Michael, J. Q., Zhang, E., and Choi 2021. Situat-edQA: Incorporating extra-linguistic contexts into QA. In EMNLP.
Zhang, T., Wu, F., Katiyar, A., Kilian, Q., Weinberger, Y., and Artzi 2021. Revisiting fewsample bert fine-tuning. In Revisiting fewsample bert fine-tuning.
Zhu, H., Wang, Z., Zhang, H., Liu, M., Zhao, S., and Qin, B. 2021. Less is more: Domain adaptation with lottery ticket for reading comprehension. In Findings@EMNLP.