Related Work

Pre-processing steps can substantially alter the results of the LDA models even in languages with good tokenization heuristics such as English (Schofield and Mimno, 2016;May et al., 2016). We believe that languages that do not have clear tokenization standards deserve investigation into what kind of processing is appropriate. Many works recognize that LDA results can be improved when input are including phrases (Lindsey et al., 2012;Lau et al., 2013;Yu et al., 2013;El-Kishky et al., 2014;Wang et al., 2016;Bin et al., 2018;Li et al., 2018). We consider it valuable to specifically assess approaches to determining these phrases. Despite their popularity in analyzing large amounts of text data, LDA models are notoriously complex to evaluate. One must evaluate both the statistical fit of a model and the human-registered thematic coherence of the words found to arise in the high-probability words, or keys, of a topic, which may not correlate (Chang et al., 2009). Analyses often combine evaluations of fit (Wallach et al., 2009) and automated approximations of human judgments of coherence (Bouma, 2009;Mimno et al., 2011) based on mutual information, even with the expectation these may only somewhat correlate with true human judgments (Lau et al., 2014). A limitation of these existing approaches, however, is that they expect the vocabulary and tokenization to remain constant between two models. For our evaluation, we use a normalized log likelihood approach to capture fit while accounting for changes in vocabulary (Schofield and Mimno, 2016).

 References: 
Aroonmanakun, W. 2007. Creating the thai national corpus. In MANUSYA: Journal of Humanities. pp. 4--17
Ge Bin, C., He, Sheng-Ze, H. U., and Cheng, G. 2018. Chinese news hot subtopic discovery and recommendation method based on key phrase and the lda model. In DEStech Transactions on Engineering and Technology Research.
Bird, S. 2006. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions. pp. 69--72
David, M., Blei, Andrew, Y., and Ng, M.I.J. 2003. Latent dirichlet allocation. In Journal of machine Learning research. pp. 993--1022
Bouma, G. 2009. Normalized (pointwise) mutual information in collocation extraction. In Proceedings of GSCL. pp. 31--40
Chang, J., Gerrish, S., Wang, C., Boyd-Graber, J., and Blei, D. 2009. Reading tea leaves: How humans interpret topic models. In Advances in Neural Information Processing Systems.
Chormai, P., Prasertsom, P., Cheevaprawatdomrong, J., and Rutherford, A. 2020. Syllable-based neural thai word segmentation. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4619--4637
Chouigui, A., Ben Khiroun, O., and Elayeb, B. 2017. Ant corpus: an arabic news text collection for textual classification. In 2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA). pp. 135--142
El-Kishky, A., Song, Y., Wang, C., Voss, C., and Han, J. 2014. Scalable topical phrase mining from text corpora. In Scalable topical phrase mining from text corpora. arXiv:1406.6312
Jey Han Lau, T., Baldwin, D., and Newman 2013. On collocations and topic models. In ACM Transactions on Speech and Language Processing (TSLP). pp. 1--14
Jey Han Lau, D., Newman, T., and Baldwin 2014. Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality. In Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality.
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. pp. 530--539
Li, B., Yang, X., Zhou, R., Wang, B., Liu, C., and Zhang, Y. 2018. An efficient method for high quality and cohesive topical phrase mining. In IEEE Transactions on Knowledge and Data Engineering. pp. 120--137
Lindsey, R., Headden, W., and Stipicevic, M. 2012. A phrase-discovering topic model using hierarchical pitman-yor processes. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. pp. 214--222
Manning, C. and Schutze, H. 1999. Foundations of statistical natural language processing. In Foundations of statistical natural language processing.
May, C., Cotterell, R., and Van Durme, B. 2016. An analysis of lemmatization on topic models of morphologically rich language. In An analysis of lemmatization on topic models of morphologically rich language. arXiv:1608.03995
Mccallum, A.K. 2002. Mallet: A machine learning for language toolkit. In Mallet: A machine learning for language toolkit.
Mccann, P. 2020. fugashi, a tool for tokenizing Japanese in python. In Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS). pp. 44--51
Merity, S., Xiong, C., Bradbury, J., and Socher, R. 2016. Pointer sentinel mixture models. In Pointer sentinel mixture models.
Mikolov, T., Chen, K., Corrado, G., and Dean, J. 2013. Efficient estimation of word representations in vector space. In Efficient estimation of word representations in vector space. arXiv:1301.3781
Mimno, D., Wallach, H., Talley, E., Leenders, M., and Mccallum, A. 2011. Optimizing semantic coherence in topic models. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. pp. 262--272
Obeid, O., Zalmout, N., Khalifa, S., Taji, D., Oudah, M., Alhafni, B., Inoue, G., Eryani, F., Erdmann, A., and Habash, N. 2020. CAMeL tools: An open source python toolkit for Arabic natural language processing. In Proceedings of the 12th Language Resources and Evaluation Conference. pp. 7022--7032
Eunjeong, L., Park, S., and Cho 2014. Konlpy: Korean natural language processing in python. In Proceedings of the 26th Annual Conference on Human & Cognitive Language Technology.
Proisl, T. and Uhrig, P. 2016. SoMaJo: State. In SoMaJo: State.