Related Work

Sparse seq2seq models. Our proposed α-ReLU transformation is based on the α-entmax transformation of Peters et al. (2019), which in turn is a generalization of the sparsemax transformation (Martins and Astudillo, 2016). In our work, we study sparseness at the output of a neural network. Nevertheless, there are a number of works aimed at sparsification within a neural network. For example, Malaviya et al. (2018); Peters et al. (2019);Correia et al. (2019) show that sparsemax and αentmax can replace softmax in the attention mechanism with some success. A recent work of Zhang et al. (2021) attempted to replace softmax with a component-wise ReLU in the attention mechanism. Unfortunately, in its pure form, this replacement leads to the inability of the model to learn at all, since its loss function does not decrease during optimization. The authors solve this problem by adding a normalizing layer on top of the attention layer.

These and other works (Zhang et al., 2019) state that sparsity in the weights of attention produces more interpretable patterns. However, Meister et al. (2021) questioned this claim and were unable to find clear evidence to support it. Therefore, in this work, we focused on the application of α-ReLU to the output of the transformer model, and not to the mechanism of attention, but at the same time we do not deny the possibility of studying the latter.

Self-normalization. Self-normalizing training aims to bypass the need of normalization during inference time. This is done by tweaking the learning mechanism so that the sum of all predictions sums (approximately) to a constant value. Theoretical work on why this works is poorly understood (Andreas et al., 2015) but early work in neural machine translation has shown its empirical value. Vaswani et al. (2013) achieves that by using noisecontrastive estimation (the neural model is used to re-rank the output of a hierarchical phrase-based machine translation system). Noise-contrastive estimation is also the standard training mechanism for word2vec (more popular than the alternative hierarchical softmax), which also eschews any expensive normalization. Differently, Devlin et al. (2014) changes the training loss to include a factor that encourages the normalizing factor to be 1. At inference time, this is just assumed and decoding time is reported to achieve a 15x speed-up.

 References: 
Andreas, J., Rabinovich, M., Michael I Jordan, D., and Klein 2015. On the accuracy of selfnormalized log-linear models. In Proceedings of the 28th International Conference on Neural Information Processing Systems. pp. 1783--1791
Barry, C., Arnold, N., Balakrishnan, H. N., and Nagaraja 2008. A First Course in Order Statistics. In Classics in Applied Mathematics). Society for Industrial and Applied Mathematics.
Ba, J. L., Kiros, J. R., and Hin, G. E. arXiv:1607.06450
Berard, A., Calapodescu, I., Dymetman, M., Roux, C., Meunier, J., and Nikoulina, V. 2019. Machine translation of restaurant reviews: New corpus for domain adaptation and robustness. In Proceedings of the 3rd Workshop on Neural Generation and Translation. pp. 168--176 10.18653/v1/D19-5617
Bojar, O., Buck, C., Callison-Burch, C., Federmann, C., Haddow, B., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2013. Proceedings of the Eighth Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation. pp. 1--44
Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., Soricut, R., Specia, L., and Tamchyna, A. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014. pp. 12--58 10.3115/v1/w14-3302
Cettolo, M., Niehues, J., Stüker, S., Bentivogli, L., and Federico, M. 2014. Report on the 11th iwslt evaluation campaign. In Report on the 11th iwslt evaluation campaign.
Gonçalo, M., Correia, V., Niculae, A. F.T., and Martins 2019. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 2174--2184 10.18653/v1/D19-1223
Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., and Makhoul, J. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. pp. 1370--1380 10.3115/v1/P14-1129
Fan, A., Lewis, M., and Dauphin, Y. N. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018. pp. 889--898
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. 2020. The curious case of neural text degeneration. In 8th International Conference on Learning Representations.
Jacot, A., Hongler, C., and Gabriel, F. 2018. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. pp. 8580--8589
Klein, G., Kim, Y., Deng, Y., Senellart, J., and Rush, A. 2017. OpenNMT: Opensource toolkit for neural machine translation. In Proceedings of ACL 2017, System Demonstrations. pp. 67--72
Malaviya, C., Ferreira, P., and Martins, A. F.T. 2018. Sparse and constrained attention for neural machine translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia. pp. 370--376 10.18653/v1/P18-2059
André, F. T., Martins, R., and Fernandez Astudillo 2016. From softmax to sparsemax: A sparse model of attention and multi-label classification. In Proceedings of the 33nd International Conference on Machine Learning. pp. 1614--1623
Meister, C., Lazov, S., Augenstein, I., and Cotterell, R. 2021. Is sparse attention more interpretable?. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 122--129 10.18653/v1/2021.acl-short.17
Papineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318
Peters, B., André, F. T., and Martins 2021. Smoothing and shrinking the sparse seq2seq search space. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 2642--2654 10.18653/v1/2021.naacl-main.210
Peters, B., Niculae, V., and Martins, A. F.T. 2019. Sparse sequence-to-sequence models. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. pp. 1504--1519
Post, M. 2018. A call for clarity in reporting bleu scores. In A call for clarity in reporting bleu scores. arXiv:1804.08771
Sennrich, R., Haddow, B., and Birch, A. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016.
Stahlberg, F. and Byrne, B. 2019. On NMT search errors and model errors: Cat got your tongue?. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 3354--3360
Sutskever, I., Vinyals, O., Quoc, V., and Le 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. pp. 3104--3112
Tsallis, C. 1988. Possible generalization of boltzmann-gibbs statistics. In Journal of statistical physics. pp. 479--487
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008
Vaswani, A., Zhao, Y., Fossum, V., and Chiang, D. 2013. Decoding with large-scale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1387--1392
Zhang, B., Titov, I., and Sennrich, R. 2021. Sparse attention with linear units. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 7--11
Zhang, J., Zhao, Y., Li, H., and Zong, C. 2019. Attention with sparsity regularization for neural machine translation and summarization. In IEEE ACM Trans. Audio Speech Lang. Process. pp. 507--518 10.1109/TASLP.2018.2883740