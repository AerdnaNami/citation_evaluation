Introduction

Neural machine translation (NMT) (Bahdanau et al., 2014;Gehring et al., 2017;Vaswani et al., 2017) has made remarkable achievements in recent years. Generally, NMT models are trained to maximize the likelihood of the next target token given ground-truth tokens as inputs (Johansen and Juselius, 1990;Goodfellow et al., 2016). Due to the token imbalance phenomenon in natural language (Zipf, 1949), for an NMT model, the learning difficulties of different target tokens may be various. However, the vanilla NMT model equally weights the training losses of different target tokens, irrespective of their difficulties.

Recently, various adaptive training approaches (Gu et al., 2020;Xu et al., 2021) have been proposed to alleviate the above problem for NMT. Generally, these approaches re-weight the losses of different target tokens based on specific statistical metrics. For example, Gu et al. (2020) take the token frequency as an indicator and encourage the NMT model to focus more on low-frequency tokens. Xu et al. (2021) further propose the bilingual mutual information (BMI) to measure the word mapping diversity between bilinguals, and down-weight the tokens with relatively lower BMI values.

Despite their achievements, there are still limitations in these adaptive training approaches. Given that the standard translation model autoregressively makes predictions on the condition of previous tar-get contexts, we argue that the statistical metrics used in the above approaches ignore target context information and may assign inaccurate weights for target tokens. Specifically, although existing statistical metrics can reflect complex characteristics of target tokens (e.g., mapping diversity), they fail to model how these properties vary across different target contexts. Secondly, for the identical target tokens in different positions of a target sentence (e.g., two 'traffic' tokens in the Figure 1), they may be mapped from different source-side tokens, but such target-context-free metrics cannot distinguish the above different mappings. In summary, it is necessary to incorporate target context information into the above statistical metrics. One possible solution is to directly take target context information into account and conduct target-context-aware statistical calculations. But in this way, the calculation cost and storage overhead will become huge and unrealistic . Therefore, it is non-trivial to design a suitable target-context-aware statistical metric for adaptive training in the field of NMT.

In this paper, we aim to address the above issues in adaptive training methods. Firstly, we propose a novel target-context-aware metric, named Conditional Bilingual Mutual Information (CBMI), to measure the importance of different target tokens by their dependence on the source sentence. Specifically, we calculate CBMI by the mutual information between a target token and its source sentence on the condition of its target contexts. With the aid of target-context-aware calculations, CBMI can easily model the various characteristics of target tokens under different target contexts, and of course can distinguish identical target tokens with different source mappings. Regarding the computational efficiency, through decomposing the conditional joint distribution in the aforementioned mutual information, our CBMI can be formalized as the log quotient of the translation model probability and language model probability 3 . Therefore, CBMI can be efficiently calculated during model training without any pre-specific statistical calculations and huge storage overhead, which makes it feasible to supplement target context in-formation for statistical metrics. Subsequently, we design an adaptive training approach based on both the token-and sentence-level CBMI, which dynamically re-weights the training losses of the corresponding target tokens.

We evaluate our approach on the WMT14 English-German and WMT19 Chinese-English translation tasks. Experimental results on both datasets demonstrate that our approach can significantly outperform the Transformer baseline and other adaptive training methods. Further analyses reveal that CBMI can also reflect the adequacy of translation, and our CBMI-based adaptive training can improve translation adequacy meanwhile maintain fluency. The main contributions of this paper can be summarized as follows:

• We propose a novel target-context-aware metric, named CBMI, which can reflect the importance of target tokens for NMT models. Theoretical analysis and experimental results show that CBMI is computationally efficient, which makes it feasible to complement target context information in statistical metrics.

• We further propose an adaptive training approach based on both the token-and sentencelevel CMBI, which dynamically re-weights the training losses of target tokens.

• Further analyses show that CBMI can also reflect the adequacy of translation, and CBMIbased adaptive training can improve translation adequacy meanwhile maintain fluency.

 References: 
Bahdanau, D., Cho, K., and Bengio, Y. 2014. Neural machine translation by jointly learning to align and translate. In Neural machine translation by jointly learning to align and translate. arXiv:1409.0473
Baziotis, C., Haddow, B., and Birch, A. 2020. Language model prior for low-resource neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 7622--7634 10.18653/v1/2020.emnlp-main.615
Cohen, J. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement. In A coefficient of agreement for nominal scales. Educational and psychological measurement. pp. 37--46
Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y. N. 2017. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning. pp. 1243--1252
Goodfellow, I., Bengio, Y., and Courville, A. 2016. Deep Learning. In Deep Learning.
Gu, S., Zhang, J., Meng, F., Feng, Y., Xie, W., Zhou, J., and Yu, D. 2020. Token-level adaptive training for neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1035--1046 10.18653/v1/2020.emnlp-main.76
Johansen, S. and Juselius, K. 1990. Maximum likelihood estimation and inference on cointegration-with appucations to the demand for money. In Oxford Bulletin of Economics and statistics. pp. 169--210
Kingma, D. and Ba, J. 2014. Adam: A method for stochastic optimization. In International Conference on Learning Representations.
Koehn, P. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. pp. 388--395
Landis, R. and Koch, G.G. 1977. The measurement of observer agreement for categorical data. In biometrics. pp. 159--174
Li, Z., Wang, R., Chen, K., Utiyama, M., Sumita, E., Zhang, Z., and Zhao, H. 2020. Data-dependent gaussian prior objective for language generation. In International Conference on Learning Representations.
Lin, T., Goyal, P., and Girshick, R. 2017. Focal loss for dense object detection. In 2017 IEEE International Conference on Computer Vision (ICCV). pp. 2999--3007 10.1109/ICCV.2017.324
Miao, M., Meng, F., Liu, Y., Zhou, X., and Zhou, J. 2021. Prevent the language model from being overconfident in neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 3456--3468 10.18653/v1/2021.acl-long.268
Raunak, V., Dalmia, S., Gupta, V., and Metze, F. 2020. On long-tailed phenomena in neural machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 3088--3095 10.18653/v1/2020.findings-emnlp.276
Reddy, R. 1977. Speech understanding systems: summary of results of the five-year research effort at carnegie-mellon university. In Speech understanding systems: summary of results of the five-year research effort at carnegie-mellon university.
Sennrich, R., Haddow, B., and Birch, A. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pp. 1715--1725 10.18653/v1/P16-1162
Shannon, C. E. 1948. A mathematical theory of communication. In The Bell System Technical Journal. pp. 379--423 10.1002/j.1538-7305.1948.tb01338.x
Tan, Z., Zhang, J., Huang, X., Chen, G., Wang, S., Sun, M., Luan, H., and Liu, Y. 2020. THUMT: An opensource toolkit for neural machine translation. In Proceedings of the 14th Conference of the Association for Machine Translation in the Americas. pp. 116--122
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17. pp. 6000--6010
Wan, Y., Yang, B., Wong, D. F., Zhou, Y., Chao, L. S., Zhang, H., and Chen, B. 2020. Self-paced learning for neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1074--1080 10.18653/v1/2020.emnlp-main.80
Weng, R., Yu, H., Wei, X., and Luo, W. 2020. Towards enhancing faithfulness for neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 2675--2684 10.18653/v1/2020.emnlp-main.212
Xu, Y., Liu, Y., Meng, F., Zhang, J., Xu, J., and Zhou, J. 2021. Bilingual mutual information based adaptive training for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 511--516 10.18653/v1/2021.acl-short.65
Zhang, J., Ding, Y., Shen, S., Cheng, Y., Sun, M., Luan, H., and Liu, Y. 2017. Thumt: An open source toolkit for neural machine translation. In Thumt: An open source toolkit for neural machine translation.
Kingsley, G. and Zipf 1949. Human behavior and the principle of least effort. In Human behavior and the principle of least effort.