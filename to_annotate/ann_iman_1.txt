Search parameters
==========
Coding by: All coders
Codes: 
Coherence. Format. Lacks synthesis. Unsupported claim. Codes: 4 / 4
Files:
paper_1.txt, paper_10.txt, paper_100.txt, paper_11.txt, paper_13.txt, paper_14.txt, paper_15.txt, paper_16.txt, paper_17.txt, paper_18.txt, paper_19.txt, paper_2.txt, paper_20.txt, paper_21.txt, paper_22.txt, paper_23.txt, paper_24.txt, paper_25.txt, paper_26.txt, paper_27.txt, paper_28.txt, paper_3.txt, paper_30.txt, paper_31.txt, paper_32.txt, paper_33.txt, paper_34.txt, paper_35.txt, paper_36.txt, paper_37.txt, paper_38.txt, paper_39.txt, paper_4.txt, paper_40.txt, paper_41.txt, paper_42.txt, paper_43.txt, paper_44.txt, paper_45.txt, paper_46.txt, paper_47.txt, paper_49.txt, paper_5.txt, paper_50.txt, paper_51.txt, paper_52.txt, paper_53.txt, paper_54.txt, paper_55.txt, paper_56.txt, paper_57.txt, paper_58.txt, paper_59.txt, paper_6.txt, paper_61.txt, paper_62.txt, paper_63.txt, paper_64.txt, paper_65.txt, paper_66.txt, paper_67.txt, paper_68.txt, paper_69.txt, paper_7.txt, paper_70.txt, paper_71.txt, paper_72.txt, paper_73.txt, paper_74.txt, paper_75.txt, paper_76.txt, paper_77.txt, paper_78.txt, paper_79.txt, paper_8.txt, paper_80.txt, paper_81.txt, paper_82.txt, paper_83.txt, paper_84.txt, paper_85.txt, paper_86.txt, paper_87.txt, paper_88.txt, paper_89.txt, paper_9.txt, paper_90.txt, paper_91.txt, paper_92.txt, paper_93.txt, paper_94.txt, paper_95.txt, paper_96.txt, paper_97.txt, paper_98.txt, paper_99.txt,  Files:  96 / 96

==========

Code count totals: 82
============
Coherence : 8
Format : 22
Lacks synthesis : 11
Unsupported claim : 41
============
Text code statistics:
Coherence | paper_100.txt | Count: 1 | Percent of file: 6.58%
Coherence | paper_43.txt | Count: 1 | Percent of file: 3.37%
Coherence | paper_36.txt | Count: 1 | Percent of file: 6.78%
Coherence | paper_34.txt | Count: 1 | Percent of file: 5.57%
Coherence | paper_19.txt | Count: 1 | Percent of file: 3.41%
Coherence | paper_7.txt | Count: 1 | Percent of file: 3.57%
Coherence | paper_17.txt | Count: 1 | Percent of file: 6.46%
Coherence | paper_23.txt | Count: 1 | Percent of file: 1.67%
Format | paper_43.txt | Count: 3 | Percent of file: 1.46%
Format | paper_40.txt | Count: 11 | Percent of file: 2.19%
Format | paper_34.txt | Count: 1 | Percent of file: 0.01%
Format | paper_33.txt | Count: 1 | Percent of file: 0.58%
Format | paper_19.txt | Count: 1 | Percent of file: 0.12%
Format | paper_18.txt | Count: 2 | Percent of file: 1.18%
Format | paper_15.txt | Count: 1 | Percent of file: 0.06%
Format | paper_1.txt | Count: 1 | Percent of file: 0.32%
Format | paper_17.txt | Count: 1 | Percent of file: 0.9%
Lacks synthesis | paper_100.txt | Count: 2 | Percent of file: 15.82%
Lacks synthesis | paper_90.txt | Count: 1 | Percent of file: 7.43%
Lacks synthesis | paper_36.txt | Count: 1 | Percent of file: 5.82%
Lacks synthesis | paper_33.txt | Count: 1 | Percent of file: 14.29%
Lacks synthesis | paper_30.txt | Count: 1 | Percent of file: 5.2%
Lacks synthesis | paper_22.txt | Count: 2 | Percent of file: 17.71%
Lacks synthesis | paper_20.txt | Count: 1 | Percent of file: 3.64%
Lacks synthesis | paper_7.txt | Count: 1 | Percent of file: 8.92%
Lacks synthesis | paper_2.txt | Count: 1 | Percent of file: 13.84%
Unsupported claim | paper_44.txt | Count: 1 | Percent of file: 3.28%
Unsupported claim | paper_100.txt | Count: 1 | Percent of file: 0.64%
Unsupported claim | paper_46.txt | Count: 1 | Percent of file: 0.56%
Unsupported claim | paper_43.txt | Count: 6 | Percent of file: 4.26%
Unsupported claim | paper_42.txt | Count: 1 | Percent of file: 1.69%
Unsupported claim | paper_40.txt | Count: 3 | Percent of file: 3.2%
Unsupported claim | paper_34.txt | Count: 1 | Percent of file: 1.14%
Unsupported claim | paper_33.txt | Count: 2 | Percent of file: 2.74%
Unsupported claim | paper_30.txt | Count: 2 | Percent of file: 2.99%
Unsupported claim | paper_28.txt | Count: 1 | Percent of file: 1.29%
Unsupported claim | paper_27.txt | Count: 1 | Percent of file: 2.42%
Unsupported claim | paper_26.txt | Count: 1 | Percent of file: 0.91%
Unsupported claim | paper_22.txt | Count: 1 | Percent of file: 1.64%
Unsupported claim | paper_21.txt | Count: 1 | Percent of file: 1.56%
Unsupported claim | paper_19.txt | Count: 1 | Percent of file: 1.23%
Unsupported claim | paper_16.txt | Count: 1 | Percent of file: 0.47%
Unsupported claim | paper_15.txt | Count: 2 | Percent of file: 2.29%
Unsupported claim | paper_9.txt | Count: 1 | Percent of file: 1.9%
Unsupported claim | paper_7.txt | Count: 1 | Percent of file: 1.37%
Unsupported claim | paper_4.txt | Count: 5 | Percent of file: 8.61%
Unsupported claim | paper_1.txt | Count: 2 | Percent of file: 2.16%
Unsupported claim | paper_17.txt | Count: 1 | Percent of file: 0.81%
Unsupported claim | paper_8.txt | Count: 1 | Percent of file: 0.58%
Unsupported claim | paper_3.txt | Count: 1 | Percent of file: 2.46%
Unsupported claim | paper_13.txt | Count: 1 | Percent of file: 0.68%
Unsupported claim | paper_23.txt | Count: 1 | Percent of file: 1.14%
========

[419-967] Coherence, File: paper_100.txt,  Coder: default
 Recent works are usually built with neural network. For example, Moon IME (Huang et al., 2018) integrates attention-based neural network and an information retrieval module. Zhang et al. (2019) improves an LSTM-
based encoder-decoder model with online vocabulary adaptation. For abbreviated pinyin, CoCAT (Huang et al., 2015) uses machine translation technology to reduce the number of the typing letters. Huang and Zhao (2018) propose an LSTM-based encoder-decoder approach with the concatenation of context words and abbreviated pinyin as input.


[1650-2259] Coherence, File: paper_17.txt,  Coder: default
It has been a rising interest in event extraction under less data scenario. Liu et al. (2020) uses a machine reading comprehension formulation to conduct event extraction in a low-resource regime. Text2Event (Lu et al., 2021), a sequence-to-structure generation paradigm, first presents events in a linearized format, and then trains a generative model to generate the linearized event sequence. Text2Event's unnatural output format hinders the model from fully leveraging pre-trained knowledge. Hence, their model falls short on the cases with only extremely low data being available (as shown in Section 3).


[723-1169] Coherence, File: paper_19.txt,  Coder: default
In our work, we take MNLP as a query strategy for experiments on sequence tagging tasks since it has demonstrated a good trade-off between quality and computational performance. We consider that online learning can potentially be used as a complement to our algorithms. Since the most time-consuming part of an AL iteration can be model inference instead of training, in this work, we also pay attention to the acceleration of the inference step.


[13-203] Coherence, File: paper_23.txt,  Coder: default

The goal of constrained textual generation is to find the sequence of tokens x1:T which maximises p(x1:T | c), given a constraint c. Few methods address the constrained textual generation. 


[1701-2244] Coherence, File: paper_34.txt,  Coder: default
Niu et al. (2020) rely on neural models to generate high quality paraphrases, using a decoding method that enforces diversity by preventing repetitive copying of the input tokens. Liu et al. (2020) optimize a quality oriented objective by casting paraphrase generation as an optimization problem, and searching the sentence space to find the optimal point. Garg et al. (2021) and Siddique et al. (2020) use reinforcement learning with quality-oriented reward combining textual entailment, semantic similarity, expression diversity and fluency.


[14-784] Coherence, File: paper_36.txt,  Coder: default
Bandit learning has been applied to a variety of NLP problems including neural machine translation (NMT; Sokolov et al., 2017;Kreutzer et al., 2018a,b;Mendoncca et al., 2021), structured prediction (Sokolov et al., 2016), semantic parsing (Lawrence and Riezler, 2018), intent recognition (Falke and Lehnen, 2021), and summarization (Gunasekara et al., 2021). Human feedback has been studied as a direct learning signal for NMT (Kreutzer et al., 2018b;Mendoncca et al., 2021), semantic parsing (Lawrence and Riezler, 2018), summarization (Stiennon et al., 2020), and dialogue (Jaques et al., 2020). Nguyen et al. (2017) simulates bandit feedback to improve an MT system fully trained on a large annotated dataset, including analyzing robustness to feedback perturbations.


[14-452] Coherence, File: paper_43.txt,  Coder: default
Grammatical Error Correction (GEC) task has a purpose to correct grammatical errors in natural texts. It includes correcting errors in spelling, punctuation, grammar, morphology, word choice, and others. Intelligent GEC system receives text containing mistakes and produces its corrected version. GEC task is complicated and challenging: the accuracy of edits, inference speed, and memory limitations are the topics of intensive research.


[393-735] Coherence, File: paper_7.txt,  Coder: default
The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs.


[755-788] Format, File: paper_1.txt,  Coder: default
Mao et al. (2019) generates story


[75-80] Format, File: paper_15.txt,  Coder: default
filed


[2261-2346] Format, File: paper_17.txt,  Coder: default
Another thread of works are using meta-learning to deal with the less label challenge


[1443-1496] Format, File: paper_18.txt,  Coder: default
 and the quality of the lexicon may not be satisfied.


[1529-1610] Format, File: paper_18.txt,  Coder: default
we observe that the regularity exists in the common NER types (e.g., ORG and LOC)


[2907-2923] Format, File: paper_19.txt,  Coder: default
uncertaintybased


[430-474] Format, File: paper_33.txt,  Coder: default
and graph neural networkSong and Chen (2021)


[667-668] Format, File: paper_34.txt,  Coder: default
.


[399-440] Format, File: paper_40.txt,  Coder: default
[Levy et al., 2017, Elsahar et al., 2018,


[460-506] Format, File: paper_40.txt,  Coder: default
[Kwiatkowski et al., 2019, Joshi et al., 2017,


[521-545] Format, File: paper_40.txt,  Coder: default
[Thorne et al., 2018a,c]


[558-578] Format, File: paper_40.txt,  Coder: default
[Dinan et al., 2019]


[656-674] Format, File: paper_40.txt,  Coder: default
[Cao et al., 2021]


[1011-1032] Format, File: paper_40.txt,  Coder: default
[Lewis et al., 2020a]


[1040-1061] Format, File: paper_40.txt,  Coder: default
[Raffel et al., 2020]


[1339-1360] Format, File: paper_40.txt,  Coder: default
[Lewis et al., 2020b]


[1593-1614] Format, File: paper_40.txt,  Coder: default
[Lewis et al., 2020a]


[1913-1931] Format, File: paper_40.txt,  Coder: default
[Lee et al., 2021]


[2277-2297] Format, File: paper_40.txt,  Coder: default
[Glass et al., 2021]


[14-98] Format, File: paper_43.txt,  Coder: default
Grammatical Error Correction (GEC) task has a purpose to correct grammatical errors 


[1090-1140] Format, File: paper_43.txt,  Coder: default
(Grundkiewicz et al., 2019) leveraged Transformer 


[1350-1406] Format, File: paper_43.txt,  Coder: default
Recently, (Rothe et al., 2021) built their system on top


[1110-1583] Lacks synthesis, File: paper_100.txt,  Coder: default
In addition, there are some works handling
pinyin with typing errors. Chen and Lee (2000) investigate a typing model which handles spelling correction in sentence-based pinyin input method. CHIME (Zheng et al., 2011) is a error-tolerant Chinese pinyin input method. It finds similar pinyin which will be further ranked with Chinese specific features. Jia and Zhao (2014) propose a joint graph model to globally optimize the tasks of pinyin input method and typo correction.


[1684-2529] Lacks synthesis, File: paper_100.txt,  Coder: default
Our methodology also relates to pretrained models that use pinyin information. Sun et al. (2021) propose a general-purpose Chinese BERT with
new embedding layers to inject pinyin and glyph information of characters. There are also task-specific BERT models, especially for the task of grammatical error correction since an important type of error is caused by characters pronounced with the same pinyin. Zhang et al. (2021a) add a pinyin embedding layer and learns to predict characters from similarly pronounced candidates. PLOME (Liu et al., 2021) add two embedding layers implemented with two GRU networks to inject both pinyin and shape of characters, respectively. Xu et al. (2021) add a hierarchical encoder to inject the pinyin letters at character and sentence levels, and add a ResNet encoder to use graphic features of character image.


[867-2648] Lacks synthesis, File: paper_2.txt,  Coder: default
Several of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.

Other proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two "higher order" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a "meta-inventory" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.


[14-760] Lacks synthesis, File: paper_20.txt,  Coder: default
D2T Generation with PLMs Large neural language models pretrained on self-supervised tasks (Lewis et al., 2020;Liu et al., 2019;Devlin et al., 2019) have recently gained a lot of traction in D2T generation research (Ferreira et al., 2020). Following Chen et al. (2020c), other works adopt PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a). Kale and Rastogi (2020b) and Ribeiro et al. (2020) showed that PLMs using linearized representations of data can outperform graph neural networks on graph-to-text datasets, recently surpassed again by graph-based models (Ke et al., 2021;Chen et al., 2020a). Although the models make use of general-domain pretraining tasks, all of them are eventually finetuned on domain-specific data.


[42-964] Lacks synthesis, File: paper_22.txt,  Coder: default

Most recent studies tried to leverage a pretrained language model with various model architectures and training objectives for STS tasks, achieving the state-of-the-art performance. In terms of model architecture, Devlin et al. (2019) focus on exhaustive cross-correlation between sentences by taking a concatenated text of two sentences as an input, while Reimers and Gurevych (2019) improve scalability based on a Siamese network and Humeau et al. (2020) adopt a hybrid approach. Along with the progress of model architectures, many advanced objectives for STS tasks were proposed as well. Specifically, Reimers and Gurevych (2019) mainly use the classification objective for an NLI dataset, and Wu et al. (2020) adopt contrastive learning to utilize self-supervision from a large corpus. Yan et al. (2021); Gao et al. (2021) incorporate a parallel corpus such as NLI datasets into their contrastive learning framework.


[1697-2745] Lacks synthesis, File: paper_22.txt,  Coder: default
Optimal transport (Monge, 1781) has been successfully adopted in natural language processing due to its ability to find a plausible correspondence between two objects (Li et al., 2020;. For example, Kusner et al. (2015) adopt optimal transport to measure the distance between two documents with pretrained word vectors. In addition, Swanson et al. (2020) discover the rationale in textmatching via optimal transport, thereby improving model interpretability.

One well-known limitation of optimal transport is that finding the optimal solution is computationally intensive, and thus approximation schemes for this problem have been extensively researched (Grauman and Darrell, 2004;Shirdhonkar and Jacobs, 2008). To get the solution efficiently, Cuturi (2013) provides a regularizer inspired by a probabilistic theory and then uses Sinkhorn's algorithm. Kusner et al. (2015) relax the problem to get the quadratic-time solution by removing one of the constraints, and Wu et al. (2018) introduce a kernel method to approximate the optimal transport.


[1225-1956] Lacks synthesis, File: paper_30.txt,  Coder: default
There is a proverbial one-to-many problem in DSG, i.e., a single dialog context could be followed by multiple reasonable responses. Existing works introduce latent variables to model this problem. For example, VHRED (Serban et al., 2017) incorporates latent continuous variable into the sequenceto-sequence (Seq2Seq) RNN model to improve the diversity of generated responses. VAE-Seq2Seq (Bahuleyan et al., 2017) proposes variational attention to replace the vanilla encoder-decoder attention (Luong et al., 2015), to avoid attention to bypass the latent space and invalidate the latent variable. For controllability and interpretability, some discrete VAEs have also been proposed, such as (Oord et al., 2017;Vahdat et al., 2018).


[14-1104] Lacks synthesis, File: paper_33.txt,  Coder: default
Math Problem Understanding Math problem understanding task focuses on understanding the text, formulas and symbols in math domain. A surge of works aim to understand the math formulas for problem solving or mathematical information retrieval. In this way, the formula is usually transformed as a tree or graph (e.g., Operator Tree (Zanibbi and Blostein, 2012)), then network embedding methods Mansouri et al. (2019) and graph neural networkSong and Chen (2021) are utilized to encode it. Besides, a number of works focus on understanding math problem based on the textual information. Among them, Math Word Problem (MWP) Solving is a popular task that generates answers of math word problems. Numerous deep learning based methods have been proposed to tackle MWP, ranging from Seq2Seq (Chiang and Chen, 2019;Li et al., 2019), Seq2Tree Qin et al., 2020), to Pre-trained Language Models Liang et al., 2021). More recently, several works attempt to modeling more complex math problems (Huang et al., 2020;Hendrycks et al., 2021) that require to understand both textual and formula information.


[1153-1814] Lacks synthesis, File: paper_36.txt,  Coder: default
Alternative forms of supervision for QA have been explored in prior work, such as explicitly providing fine-grained information (Dua et al., 2020;Khashabi et al., 2020a). Kratzwald et al. (2020) resembles our setting in that it seeks binary feed-back to replace span annotation, but their goal is to create supervised data more economically. Domain adaptation for QA has been studied in prior work (Fisch et al., 2019;Khashabi et al., 2020b), including using data augmentation (Yue et al., 2021), adversarial training , contrastive method (Yue et al., 2021), back-training (Kulshreshtha et al., 2021, and exploiting small lottery subnetworks (Zhu et al., 2021).


[1003-1857] Lacks synthesis, File: paper_7.txt,  Coder: default
Compared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.


[14-602] Lacks synthesis, File: paper_90.txt,  Coder: default
Other works have observed limitations of the softmax layer when modelling infrequent classes for image classification (Kang et al., 2020) and rare words for MT (Nguyen and Chiang, 2018;Raunak et al., 2020). They show that normalising the magnitude of the softmax weight vectors improves predictions for infrequent classes. However, the motivation for weight normalisation is guided empirically. From the perspective of this work, weight normalisation provably prevents Stolen Probability from arising when a softmax layer has no bias term. For more details, see Section D in the Appendix.


[467-557] Unsupported claim, File: paper_1.txt,  Coder: default
Thus, recent works have also explored enhancing pretrained models with external knowledge.


[1372-1505] Unsupported claim, File: paper_1.txt,  Coder: default
For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets


[218-271] Unsupported claim, File: paper_100.txt,  Coder: default
A majority of existing works focus on perfect pinyin.


[14-105] Unsupported claim, File: paper_13.txt,  Coder: default
Few-shot learning is the problem of learning classifiers with only a few training examples.


[649-716] Unsupported claim, File: paper_15.txt,  Coder: default
Researchers also realize that the vision modality maybe redundant. 


[1046-1184] Unsupported claim, File: paper_15.txt,  Coder: default
 We also extend the research to enhanced features, such as object-detection and image captioning, which is complementary to previous work.


[856-906] Unsupported claim, File: paper_16.txt,  Coder: default
AM-PERE  for proposition classification in reviews


[1649-1725] Unsupported claim, File: paper_17.txt,  Coder: default
 It has been a rising interest in event extraction under less data scenario.


[14-174] Unsupported claim, File: paper_19.txt,  Coder: default
Deep learning, to a large extent, has freed data scientists from doing feature engineering, which has been one of the essential obstacles to annotation with AL.


[14-194] Unsupported claim, File: paper_21.txt,  Coder: default
Natural language processing (NLP) datasets are plagued with artifacts and biases, which allow models to perform tasks without learning the desired underlying language capabilities.


[43-225] Unsupported claim, File: paper_22.txt,  Coder: default
Most recent studies tried to leverage a pretrained language model with various model architectures and training objectives for STS tasks, achieving the state-of-the-art performance. 


[4260-4389] Unsupported claim, File: paper_23.txt,  Coder: default
When used in a myopic decoding strategy, classification errors will cause the generation process to deviate further and further. 


[14-101] Unsupported claim, File: paper_26.txt,  Coder: default
Figure 1 shows the increase in travel to the ACL annual meeting over the past 40 years.


[857-994] Unsupported claim, File: paper_27.txt,  Coder: default
Various models differ in their approach to encoding (e.g., PCNNs, GCNs, BERT) and their loss functions (e.g., contrastive learning, MLM),


[262-394] Unsupported claim, File: paper_28.txt,  Coder: default
Classes in real-world data often have highly skewed distribution, leading to substantial gaps between majority and minority classes.


[715-1194] Unsupported claim, File: paper_3.txt,  Coder: default
However, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks.


[14-278] Unsupported claim, File: paper_30.txt,  Coder: default
Pre-trained language models (PLMs) have been widely explored both in natural language understanding (NLU) and generation (NLG) in recent years, this pre-training and fine-tuning paradigm sheds light on various downstream tasks in natural language processing (NLP).


[725-882] Unsupported claim, File: paper_30.txt,  Coder: default
Recent advances in DSG utilize pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT2 (Radford et al., 2019) in two major categories.


[144-256] Unsupported claim, File: paper_33.txt,  Coder: default
 A surge of works aim to understand the math formulas for problem solving or mathematical information retrieval.


[502-599] Unsupported claim, File: paper_33.txt,  Coder: default
Besides, a number of works focus on understanding math problem based on the textual information. 


[14-125] Unsupported claim, File: paper_34.txt,  Coder: default
Many recent works on paraphrase generation have been focused on attempting to achieve high-quality paraphrases.


[579-726] Unsupported claim, File: paper_4.txt,  Coder: default
while similarly in the sixth Dialog System Technology Challenge (DSTC6) systems were filtered according to metric scores prior to human evaluation.


[728-1010] Unsupported claim, File: paper_4.txt,  Coder: default
In terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.


[1202-1396] Unsupported claim, File: paper_4.txt,  Coder: default
On the other hand, competitions that (for one reason or another) do not release data and evaluation techniques into the public domain have reported relative success in terms of human evaluation.


[1541-1589] Unsupported claim, File: paper_4.txt,  Coder: default
The first Amazon Alexa Socialbot Grand Challenge


[1599-1907] Unsupported claim, File: paper_4.txt,  Coder: default
human assessors to score how coherent and engaging conversations were on a 1-5 rating scale by two distinct groups: volunteer Amazon employees (experts), and general Alexa users (crowds) (Ram et al., 2018), are reported to achieve a correlation of overall scores for the two types of human assessors at 0.93.


[14-101] Unsupported claim, File: paper_40.txt,  Coder: default
The KILT benchmark and public leaderboard 2 combines eleven datasets across five tasks.


[649-841] Unsupported claim, File: paper_40.txt,  Coder: default
 GENRE [Cao et al., 2021] is trained on BLINK  and all KILT tasks jointly using a sequence-to-sequence language model to generate the title of the Wikipedia page where the answer can be found.


[2782-2899] Unsupported claim, File: paper_40.txt,  Coder: default
Multi-stage or cascade approaches to retrieval have received ample attention in Information Retrieval (IR) research. 


[566-715] Unsupported claim, File: paper_42.txt,  Coder: default
However, the bulk of the work has addressed biases linked to the social and cultural experience of English speaking individuals in the United States.


[454-525] Unsupported claim, File: paper_43.txt,  Coder: default
Currently, Machine Translation (MT) is the mainstream approach for GEC.


[915-968] Unsupported claim, File: paper_43.txt,  Coder: default
and recent powerful Transformer-based Seq2Seq models.


[1547-1695] Unsupported claim, File: paper_43.txt,  Coder: default
The sequence tagging approach that generates a sequence of text edit operations encoded by tags for errorful input text is becoming more common now.


[1959-2039] Unsupported claim, File: paper_43.txt,  Coder: default
 LaserTagger combines a BERT encoder with an autoregressive Transformer decoder,


[2072-2131] Unsupported claim, File: paper_43.txt,  Coder: default
Parallel Iterative Edit (PIE) model  does parallel decoding


[2359-2502] Unsupported claim, File: paper_43.txt,  Coder: default
GECToR system uses various Transformers as an encoder, linear layers with softmax for tag prediction and error detection instead of a decoder. 


[1742-2144] Unsupported claim, File: paper_44.txt,  Coder: default
When people communicate with each other, their perception of dialogue context will evoke their past memory about relevant life experience, taste and values, which we refer to as personal memory. The aroused fragment of personal memory further guides their interest and preference for different knowledge. In other words, there exists a mapping from one's personal memory to its selection of knowledge.



[360-430] Unsupported claim, File: paper_46.txt,  Coder: default
Many researchers followed this work, and employed LSTM as the encoder.


[261-392] Unsupported claim, File: paper_7.txt,  Coder: default
 Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem.


[1585-1688] Unsupported claim, File: paper_8.txt,  Coder: default
Despite their success, it is still unknown whether or how well the instance difficulty can be learned. 


[14-260] Unsupported claim, File: paper_9.txt,  Coder: default
Several approaches to zero-shot cross-lingual transfer exist and can broadly be divided into: a) Databased Transfer, which focuses on training data transformation and b) Model-based Transfer that centres around modifying models' training routine.
