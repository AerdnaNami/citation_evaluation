Introduction

Multilingual models are critical for the democratization of AI. Cross-lingual information retrieval (CLIR) (Braschler et al., 1999;Shakery and Zhai, 2013;Jiang et al., 2020;Asai et al., 2021a), for example, can find relevant text in a high-resource language such as English even when the query is posed in a different, possibly low-resource, language. In this work, we develop useful CLIR models for this constrained, yet important, setting where a retrieval corpus is available only in a single high-resource language (English in our experiments).

A straightforward solution to this problem can be based on machine translation (MT) of the query into English, followed by English IR (Asai et al., 2021a). While this two-stage process is capable of providing accurate predictions, an alternative end-to-end approach that can tackle the problem purely cross-lingually, i.e., without involving MT, would clearly be more efficient and cost-effective. Pre-trained multilingual masked language models (PLMs) such as multilingual BERT  or XLM-RoBERTa (XLM-R) (Conneau et al., 2020) can provide the foundation for such an approach, as one can simply fine-tune a PLM with labeled CLIR data (Asai et al., 2021b).

Here we first run an empirical evaluation of these two approaches on a public CLIR benchmark (Asai et al., 2021a), which includes both in-domain and zero-shot out-of-domain tests. We use ColBERT (Khattab and Zaharia, 2020;Khattab et al., 2021) as our IR architecture 1 and XLM-R as the underlying PLM for both methods ( §2). Results indicate that the MT-based solution can be vastly more effective than CLIR fine-tuning, with observed differences in Recall@5kt of 22.2-28.6 points ( §3). Crucially, the modular design of the former allows it to leverage additional English-only training data for its IR component, providing significant boosts to its results.

The above findings lead naturally to the central research question of this paper: Can a highperformance CLIR model be trained that can operate without having to rely on MT? To answer the question, instead of viewing the MT-based approach as a competing one, we propose to leverage its strength via knowledge distillation (KD) into an end-to-end CLIR model. KD (Hinton et al., 2014) is a powerful supervision technique typically used to distill the knowledge of a large teacher model about some task into a smaller student model (Mukherjee and Awadallah, 2020;Turc et al., 2020). Here we propose to use it in a slightly different context, where the teacher and the student retriever are identical in size, but the former has superior performance simply due to utilizing MT output and consequently operating in a high-resource and lowdifficulty monolingual environment.

We run two independent KD operations ( §2.2). One directly optimizes an IR objective by utiliz- ing labeled CLIR data: parallel questions (English and non-English) and corresponding relevant and non-relevant English passages. The teacher and the student are shown the English and non-English versions of the questions, respectively; the training objective is for the student to match the soft query-passage relevance predictions of the teacher.

The second KD task is representation learning from parallel text, where the student learns to encode a non-English text in a way that matches the teacher's encoding of the aligned English text, at the token level. The cross-lingual token alignment needed to create the training data for this task is generated using a greedy alignment process that exploits the PLM's multilingual representations.

In our experiments on the XOR-TyDi dataset (Asai et al., 2021a), the KD student outperforms the fine-tuned ColBERT baseline by 25.4 (in-domain) and 14.9 (zero-shot) Recall@5kt, recovering much of the performance loss from the MT-based solution. It is also the best single-model system on the XOR-TyDi leaderboard 2 at the time of this writing. Ablation studies show that each of our two KD processes contribute significantly towards the final performance of the student model.

Our contributions can be summarized as follows:

(1) We present an empirical study of the effectiveness of a SOTA IR method (ColBERT) on crosslingual IR with and without MT.
(2) We propose a novel end-to-end cross-lingual solution that uses knowledge distillation to learn both improved text representation and retrieval. 
(3) We demonstrate with a new cross-lingual alignment algorithm that distillation using parallel text can strongly augment cross-lingual IR training. 
(4) We achieve new single-model SOTA results on XOR-TyDi.

 References: 
Asai, A., Kasai, J., Clark, J., Lee, K., Choi, E., and Hajishirzi, H. 2021. XOR QA: Cross-lingual Open-Retrieval Question Answering. In NAACL.
Asai, A., Yu, X., Kasai, J., and Hajishirzi, H. 2021. One question answering model for many languages with cross-lingual dense passage retrieval. In NeurIPS.
Braschler, M., Krause, J., Peters, C., and Schäuble, P. Cross-language information retrieval (clir) track overview. In TREC.
Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In ACL.
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL.
Hinton, G., Vinyals, O., and Dean, J. 2014. Distilling the knowledge in a neural network. In NeurIPS Deep Learning Workshop.
Jiang, Z., El-Jaroudi, A., and Hartmann, W. Damianos Karakos, and Lingjun Zhao. 2020. Crosslingual information retrieval with bert. In Damianos Karakos, and Lingjun Zhao. 2020. Crosslingual information retrieval with bert. arXiv:2004.13005
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.
Khattab, O., Potts, C., and Zaharia, M. 2021. Relevance-guided supervision for openqa with colbert. In Transactions of the ACL.
Khattab, O. and Zaharia, M. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In SIGIR.
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kelcey, M., Devlin, J., Lee, K., Toutanova, K. N., Jones, L., Chang, M., Dai, A., Uszkoreit, J., Le, Q., and Petrov, S. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics. In Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics.
Longpre, S., Lu, Y., and Daiber, J. 2020. MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering. In MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering. arXiv:2007.15207
Mukherjee, S. and Awadallah, A. 2020. XtremeDistil: Multi-stage Distillation for Massive Multilingual Models. In ACL.
Shakery, A. and Zhai, C. 2013. Leveraging comparable corpora for cross-lingual information retrieval in resource-lean language pairs. Information retrieval. In Leveraging comparable corpora for cross-lingual information retrieval in resource-lean language pairs. Information retrieval. pp. 1--29
Turc, I., Chang, M., Lee, K., and Toutanova, K. 2020. Well-Read Students Learn Better: On the Importance of Pre-training Compact Models. In ICLR.
Query, K. and English, :.우.마.배.누.처.발. 1967. East Asia and Chile. A major recent focus of microwave radio astronomy has been mapping the cosmic microwave background radiation (CMBR) discovered in 1964 by radio astronomers Arno Penzias and Robert Wilson. This faint background radiation, which fills the universe and is almost the same in all directions, is "relic radiation" from the Big Bang, and is one of the few sources of information about conditions in the early universe. Due to the expansion and thus cooling of the Russian Query: В каком сражении участвовал крейсер "Аврора. In English Translation: What battle did the cruiser Aurora take part in? Answer: Battle of Tsushima Baseline Top Passage: Battle of the Arar The Battle of the Arar was fought between the migrating tribes of the Helvetii, and four Roman legions (Legions VII, VIII, IX "Hispana" and X "Equestris"), under the command of Gaius Julius Caesar.
German, B.;. and German, B. On the etiology of hysteria"). One of the studies published in his essay involved a young woman by the name of Anna O. Among her many ailments, she suffered from stiff paralysis on the right side of her Arabic Query: English Translation: What is the largest region of Germany? Answer: Bavaria Baseline Top Passage: the original name of Montana was adopted. Montana is one of the nine Mountain States, located in the north of the region known as the Western United States. It borders North Dakota and South Dakota to the east. Wyoming is to the south, Idaho is to the west and southwest, and three Canadian provinces, British Columbia, Alberta, and Saskatchewan, are to the north. With an area of , Montana is slightly larger than Japan. It is the fourth largest state in the United States after Alaska, Texas, and California; it is the largest landlocked U.S. state. The state's topography is KD Student Top Passage. In the Dogger Bank incident. On 27 and 28 May 1905 "Aurora" took part in the Battle of Tsushima, along with the rest of the Russian squadron.