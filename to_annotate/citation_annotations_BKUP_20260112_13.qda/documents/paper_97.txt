Introduction

Although masked language models build contextualized word representations, they are pre-trained with losses that minimize distance to uncontextualized word embeddings (Peters et al., 2018;Devlin et al., 2019;Liu et al., 2019). In this paper, we introduce Question Answering Infused Pre-training (QUIP), a new pre-training loss based on question answering (QA) that depends much more directly on context, and learns improved token-level representations for a range of zero-and few-shot tasks.

Our intuition for QUIP is that the contextualized representation for a phrase in a passage should contain enough information to identify all the questions that the phrase could answer in context. For example, in Figure 1, the representation for Johannes Brahms should be similar to the representation of all questions it can answer, such as "Who wrote the violin concerto?" We anticipate that optimizing passage representations for QA should benefit many downstream tasks, as question-answer pairs have been used as broad-coverage meaning representations (He et al., 2015; Michael et al., 2018), and a wide range of NLP tasks can be cast as QA problems (Levy et al., 2017; McCann et al., 2018; Gardner et al., 2019), For instance, our learned representations should encode whether a phrase answers a question like “Why was the movie considered good?”, which corresponds to identifying rationales for sentiment analysis.

We train QUIP with a bi-encoder extractive QA objective. The bi-encoder model independently encodes passages and questions such that the representation of each phrase in a passage is similar to the representation of reading comprehension questions answered by that phrase. To train this model, we use a question generation model to synthesize 80 million QA examples, then train the bi-encoder to match the predictions of a cross-encoder QA
model, which processes the passage and question together, on these examples. Bi-encoder QA has been used before for efficient open-domain QA via phrase retrieval (Seo et al., 2018, 2019; Lee et al., 2020, 2021), but its lower accuracy compared to cross-encoder QA has previously been viewed as a drawback. We instead view the relative weakness of bi-encoder QA as an opportunity to improve contextual representations via knowledge distillation, as self-training can be effective when the student model must solve a harder problem than the teacher (Xie et al., 2020). In particular, since the bi-encoder does not know the question when encoding the passage, it must produce a single passage representation that simultaneously encodes the answers to all possible questions. In contrast, while
cross-encoder QA models are more accurate, they depend on a specific question when encoding a passage; thus, they are less suited to downstream use cases that require contextualized representations of passages in isolation.

We show that QUIP token-level representations are useful in a variety of zero-shot and few-shot learning settings, both because the representations directly encode useful contextual information, and because we can often reduce downstream tasks to QA. For few-shot paraphrase detection, QUIP with BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across four datasets. For few-shot named entity recognition (NER), QUIP combined with an initialization scheme that uses question embeddings improves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analysis, QUIP with question prompts improves over RoBERTa-large with MLM-style prompts by 5 accuracy points across three datasets, and extracts interpretable rationales as a side effect. Through ablations, we show that using real questions, a strong teacher model, and the bi-encoder architecture are all crucial to the success of QUIP. We will release code to reproduce all results upon publication.

 References: 
Gardner, M., Berant, J., and Hajishirzi, H. 2019. Question answering is a format. In Question answering is a format. arXiv:1909.11291
He, H., Ning, Q., and Roth, D. 2020. QuASE: Question-answer driven sentence encoding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 8743--8758 10.18653/v1/2020.acl-main.772
He, L., Lewis, M., and Zettlemoyer, L. 2015. Question-answer driven semantic role labeling: Using natural language to annotate natural language. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 643--653 10.18653/v1/D15-1076
Hinton, G., Vinyals, O., and Dean, J. 2015. Distilling the knowledge in a neural network. In NeurIPS Deep Learning and Representation Learning Workshop.
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.
Hu, M. and Liu, B. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 168--177
Huang, J., Li, C., Subudhi, K., Jose, D., Balakrishnan, S., Chen, W., Peng, B., Gao, J., and Han, J. 2020. Few-shot named entity recognition: A comprehensive study. In Few-shot named entity recognition: A comprehensive study. arXiv:2012.14978
Iyer, S., Dandekar, N., and Csernai, K. 2017. First quora dataset release: Question pairs. In First quora dataset release: Question pairs.
Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 1601--1611 10.18653/v1/P17-1147
Kembhavi, A., Seo, M., Schwenk, D., Choi, J., Farhadi, A., and Hajishirzi, H. 2017. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 5376--5384 10.1109/CVPR.2017.571
Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., and Hajishirzi, H. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 1896--1907 10.18653/v1/2020.findings-emnlp.171
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics. In Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics. pp. 452--466 10.1162/tacl_a_00276
Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. 2017. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pp. 785--794 10.18653/v1/D17-1082
Lee, D. 2013. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In ICML 2013 Workshop on Challenges in Representation Learning (WREPL).
Lee, J., Seo, M., Hajishirzi, H., and Kang, J. 2020. Contextualized sparse representations for real-time open-domain question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 912--919 10.18653/v1/2020.acl-main.85
Lee, J., Sung, M., Kang, J., and Chen, D. 2021. Learning dense representations of phrases at scale. In Association for Computational Linguistics (ACL).
Levy, O., Seo, M., Choi, E., and Zettlemoyer, L. 2017. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning. pp. 333--342 10.18653/v1/K17-1034
Lewis, M. and Fan, A. 2019. Generative question answering: Learning to answer the whole question. In International Conference on Learning Representations.
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703
Lewis, P., Denoyer, L., and Riedel, S. 2019. Unsupervised question answering by cloze translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4896--4910 10.18653/v1/P19-1484
Lewis, P., Wu, Y., Liu, L., Minervini, P., Küttler, H., Piktus, A., Stenetorp, P., and Riedel, S. 2021. Paq: 65 million probably-asked questions and what you can do with them. In Paq: 65 million probably-asked questions and what you can do with them. arXiv:2102.07033
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. RoBERTa: A robustly optimized bert pretraining approach. In RoBERTa: A robustly optimized bert pretraining approach. arXiv:1907.11692
Mccann, B., Shirish Keskar, N., Xiong, C., and Socher, R. 2018. The natural language decathlon: Multitask learning as question answering. In The natural language decathlon: Multitask learning as question answering. arXiv:1806.08730
Michael, J., Stanovsky, G., He, L., Dagan, I., and Zettlemoyer, L. 2018. Crowdsourcing question-answer meaning representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 560--568 10.18653/v1/N18-2089
Mussmann, S., Jia, R., and Liang, P. 2020. On the Importance of Adaptive Data Collection for Extremely Imbalanced Pairwise Tasks. In Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 3400--3413 10.18653/v1/2020.findings-emnlp.305
Pang, B. and Lee, L. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05). pp. 115--124 10.3115/1219840.1219855
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. 2011. Scikit-learn: Machine learning in Python. In Journal of Machine Learning Research. pp. 2825--2830
Perez, E., Kiela, D., and Cho, K. 2021. True few-shot learning with language models. In True few-shot learning with language models. arXiv:2105.11447
Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/N18-1202
Phang, J., Févry, T., and Bowman, S.R. 2018. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. In Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. arXiv:1811.01088
Pruksachatkun, Y., Phang, J., Liu, H., Phu Mon Htut, X., Zhang, R. Y., Pang, C., Vania, K., and Kann, S. R. Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work?. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5231--5247 10.18653/v1/2020.acl-main.467
Puri, R., Spring, R., Shoeybi, M., Patwary, M., and Catanzaro, B. 2020. Training question answering models from synthetic data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 5811--5826 10.18653/v1/2020.emnlp-main.468
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 2383--2392 10.18653/v1/D16-1264
Ram, O., Kirstain, Y., and Berant, J. Amir Globerson, and Omer Levy. 2021. Few-shot question answering by pretraining span selection. In Association for Computational Linguistics (ACL).
Reimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410
Saha, A., Aralikatte, R., Mitesh, M., Khapra, K., and Sankaranarayanan 2018. DuoRC: Towards complex language understanding with paraphrased reading comprehension. In Proceedings of the 56th. 10.18653/v1/P18-1156
Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 1683--1693
Schick, T. and Schütze, H. 2021. It's not just size that matters: Small language models are also fewshot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2339--2352
Seo, M., Kwiatkowski, T., Parikh, A., Farhadi, A., and Hajishirzi, H. 2018. Phraseindexed question answering: A new challenge for scalable document comprehension. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 559--564 10.18653/v1/D18-1052
Seo, M., Lee, J., Kwiatkowski, T., Parikh, A., Farhadi, A., and Hajishirzi, H. 2019. Real-time open-domain question answering with dense-sparse phrase index. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4430--4441 10.18653/v1/P19-1436
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642
Thakur, N., Reimers, N., Daxenberger, J., and Gurevych, I. 2021. Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 296--310
Tjong, E. F., Sang, K., and De Meulder, F. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003. pp. 142--147
Trischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K. 2017. NewsQA: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP. pp. 191--200 10.18653/v1/W17-2623
Tsatsaronis, G., Balikas, G., Malakasiotis, P., Partalas, I., Zschunke, M., Michael R Alvers, D., Weissenborn, A., and Krithara Ion Androutsopoulos, and Georgios Paliouras. 2015. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. In Sergios Petridis, Dimitris Polychronopoulos, Yannis Almirantis, John Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Artieres, Axel Ngonga. pp. 138 10.1186/s12859-015-0564-6
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Patrick Von Platen, C., Ma, Y., Jernite, J., Plu, C., Xu, T. L., Scao, S., Gugger, M., Drame, Q., Lhoest, A., and Rush 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. pp. 38--45 10.18653/v1/2020.emnlp-demos.6
Xie, Q., Luong, M., Hovy, E., Quoc, V., and Le 2020. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
Yang, Y. and Katiyar, A. 2020. Simple and effective few-shot named entity recognition with structured nearest neighbor learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6365--6375 10.18653/v1/2020.emnlp-main.516
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 2369--2380 10.18653/v1/D18-1259
Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. 2020. BERTScore: Evaluating text generation with bert. In International Conference on Learning Representations.
Zhang, Y., Baldridge, J., and He, L. 2019. PAWS: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1298--1308 10.18653/v1/N19-1131
Zhao, T. Z., Wallace, E., Feng, S., Klein, D., and Singh, S. 2021. Calibrate before use: Improving few-shot performance of language models. In Calibrate before use: Improving few-shot performance of language models. arXiv:2102.09690
Zhao, Y., Ni, X., Ding, Y., and Ke, Q. 2018. Paragraph-level neural question generation with maxout pointer and gated self-attention networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 3901--3910 10.18653/v1/D18-1424