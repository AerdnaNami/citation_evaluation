Current

 References: 
Bahdanau, D., Cho, K., and Bengio, Y. 2014. Neural machine translation by jointly learning to align and translate. In Neural machine translation by jointly learning to align and translate. arXiv:1409.0473
Barrault, L., Bojar, O., Costa-Jussà, M. R., Federmann, C., Fishel, M., Graham, Y., Haddow, B., Huck, M., Koehn, P., Malmasi, S., Monz, C., and Müller, M. 2019. Findings of the 2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation. pp. 1--61 10.18653/v1/W19-5301
Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., Soricut, R., Specia, L., and Tamchyna, A. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation. pp. 12--58 10.3115/v1/W14-3302
Cao, S. and Wang, L. 2021. Inference time style control for summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 5942--5953 10.18653/v1/2021.naacl-main.476
Fan, A., Lewis, M., and Dauphin, Y. 2019. Strategies for structuring story generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 2650--2660 10.18653/v1/P19-1254
Ficler, J. and Goldberg, Y. 2017. Controlling linguistic style aspects in neural language generation. In Proceedings of the Workshop on Stylistic Variation. pp. 94--104 10.18653/v1/W17-4912
Samuel Gehman, S., Gururangan, M., Sap, Y., Choi, N. A., and Smith 2020. RealToxi-cityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 3356--3369 10.18653/v1/2020.findings-emnlp.301
Gehrmann, S., Deng, Y., and Rush, A. 2018. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 4098--4109 10.18653/v1/D18-1443
Goyal, T. and Durrett, G. 2020. Neural syntactic preordering for controlled paraphrase generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 238--252 10.18653/v1/2020.acl-main.22
Goyal, T. and Durrett, G. 2021. Annotating and modeling fine-grained factuality in summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1449--1462 10.18653/v1/2021.naacl-main.114
Peter, E., Hart, N. J., Nilsson, B., and Raphael 1968. A formal basis for the heuristic determination of minimum cost paths. In IEEE transactions on Systems Science and Cybernetics. pp. 100--107
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.
Holtzman, A., Buys, J., Forbes, M., Bosselut, A., Golub, D., and Choi, Y. 2018. Learning to write with cooperative discriminators. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 1638--1649 10.18653/v1/P18-1152
Huang, L., Zhao, K., and Ma, M. 2017. When to finish? optimal beam search for neural text generation (modulo beam size). In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pp. 2134--2139 10.18653/v1/D17-1227
Iyyer, M., Wieting, J., Gimpel, K., and Zettlemoyer, L. 2018. Adversarial example generation with syntactically controlled paraphrase networks. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1875--1885 10.18653/v1/N18-1170
Koehn, P., Och, F. J., and Marcu, D. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics. pp. 127--133
Kryscinski, W., Mccann, B., Xiong, C., and Socher, R. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 9332--9346 10.18653/v1/2020.emnlp-main.750
Lewis, M., Liu, Y., and Goyal, N.
Levy, V., Stoyanov, L., and Zettlemoyer 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703
Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014
Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., and Zettlemoyer, L. 2020. Multilingual denoising pre-training for neural machine translation. In Transactions of the Association for Computational Linguistics. pp. 726--742 10.1162/tacl_a_00343
Maynez, J., Narayan, S., Bohnet, B., and Mcdonald, R. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 1906--1919 10.18653/v1/2020.acl-main.173
Meister, C., Amini, A., Vieira, T., and Cotterell, R. 2021. Conditional Poisson stochastic beams. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 664--681
Meister, C., Cotterell, R., and Vieira, T. 2020. If beam search is the answer, what was the question?. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 2173--2185 10.18653/v1/2020.emnlp-main.170
Meister, C., Forster, M., and Cotterell, R. 2021. Determinantal beam search. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 6551--6562 10.18653/v1/2021.acl-long.512
Meister, C., Vieira, T., and Cotterell, R. 2020. Best-first beam search. In Transactions of the Association for Computational Linguistics. pp. 795--809 10.1162/tacl_a_00346
Narayan, S., Cohen, S. B., and Lapata, M. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 1797--1807 10.18653/v1/D18-1206
Och, F.J., Ueffing, N., and Ney, H. 2001. An efficient A* search algorithm for statistical machine translation. In Proceedings of the ACL 2001 Workshop on Data-Driven Methods in Machine Translation.
Omelianchuk, K., Atrasevych, V., Chernodub, A., and Skurzhanskyi, O. 2020. GECToR -grammatical error correction: Tag, not rewrite. In Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications. pp. 163--170 10.18653/v1/2020.bea-1.16
Papineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135
Pearl, J. 1984. Heuristics: intelligent search strategies for computer problem solving. In Heuristics: intelligent search strategies for computer problem solving.
Post, M. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers. pp. 186--191 10.18653/v1/W18-6319
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In Language models are unsupervised multitask learners.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. In Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv eprint 1910.10683
Shu, R. and Nakayama, H. 2018. Improving beam search by removing monotonic constraint for neural machine translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 339--344 10.18653/v1/P18-2054
Song, K., Wang, B., Feng, Z., and Liu, F. 2021. A new approach to overgenerating and scoring abstractive summaries. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1392--1404 10.18653/v1/2021.naacl-main.110
Stahlberg, F. and Byrne, B. 2019. On NMT search errors and model errors: Cat got your tongue?. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3356--3362 10.18653/v1/D19-1331
Sutskever, I., Vinyals, O., and Le, Q.V. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems. pp. 3104--3112
Ashwin, K., Vijayakumar, M., Cogswell, Ramprasaath, R., Selvaraju, Q., Sun, S., Lee, D., Crandall, D., and Batra 2016. Diverse beam search: Decoding diverse solutions from neural sequence models. In Diverse beam search: Decoding diverse solutions from neural sequence models.
Welleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., and Weston, J. 2020. Neural text generation with unlikelihood training. In International Conference on Learning Representations.
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Patrick Von Platen, C., Ma, Y., Jernite, J., Plu, C., Xu, T. L., Scao, S., Gugger, M., and Drame 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. pp. 38--45 10.18653/v1/2020.emnlp-demos.6
Wu, Y., Schuster, M., Chen, Z., Quoc, V., Le, M., Norouzi, W., Macherey, M., Krikun, Y., Cao, Q., Gao, K., and Macherey 2016. Google's neural machine translation system. In Bridging the gap between human and machine translation. arXiv:1609.08144
Xu, J., Desai, S., and Durrett, G. 2020. Understanding neural abstractive summarization models via uncertainty. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6275--6281 10.18653/v1/2020.emnlp-main.508
Xu, J. and Durrett, G. 2021. Dissecting generation modes for abstractive summarization models via ablation and attribution. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 6925--6940 10.18653/v1/2021.acl-long.539
Yang, K. and Klein, D. 2021. FUDGE: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3511--3535 10.18653/v1/2021.naacl-main.276
Yu, L., Zhang, W., Wang, J., and Yu, Y. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In Proceedings of the AAAI conference on artificial intelligence.
Zhang, Z., Wang, R., Utiyama, M., Sumita, E., and Zhao, H. 2018. Exploring recombination for efficient decoding of neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 4785--4790 10.18653/v1/D18-1511
Zhong, R., Lee, K., Zhang, Z., and Klein, D. 2021. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. In Findings of the Association for Computational Linguistics: EMNLP 2021. pp. 2856--2878
Zhu, Y., Lu, S., Zheng, L., Guo, J., Zhang, W., Wang, J., and Yu, Y. 2018. Texygen: A benchmarking platform for text generation models. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. pp. 1097--1100