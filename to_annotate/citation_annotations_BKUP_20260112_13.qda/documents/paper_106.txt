and pinyin-enhanced pretrained models here. 7 Model Games Culture Sports P@1 P@5 P@10 P@1 P@5 P@10 P@1 P@5 P@10 GPT (ours) 24.04 32.78 34.23 21.86 29.33 30.94 28.54 37.13 38.69 PinyinGPT-Concat 25.78 38.26 41.89 22.10 33.33 36.72 29.81 43.56 46.95 Real Estate Medical Finance P@1 P@5 P@10 P@1 P@5 P@10 P@1 P@5 P@10 GPT (ours) 26.53 35.27 36.74 33.59 43.54 44.93 29.00 37.24 38.47 PinyinGPT-Concat 27.28 40.16 43.86 34.76 49.28 52.56 29.17 42.17 45.52 Table 6: Results of 6 sample domains over WD using abbreviated pinyin. Each score is averaged over all the context-target length conﬁgurations. The table of all 15 domains is attached in the Appendix. Model Time (ms) P@5 GPT (ours, 6L) 94 27.45 GPT (ours, 12L) 142 34.48 PinyinGPT-Concat (6L) 94 32.70 PinyinGPT-Concat (12L) 145 41.51 Table 7: Average inference time for one instance and the overall P@5 for the conﬁguration of (4-9, 4-9). Pinyin Input Method We describe existing works based on whether the input pinyin is perfect or abbreviated. A majority of existing works focus on perfect pinyin. Traditional models are typically based on statistical language models (Chen and Lee, 2000) and statistical machine translation (Yang et al., 2012). Recent works are usually built with neural network. For example, Moon IME (Huang et al., 2018) integrates attention-based neural network and an information retrieval module. Zhang et al. (2019) improves an LSTM- based encoder-decoder model with online vocabulary adaptation. For abbreviated pinyin, Co- CAT (Huang et al., 2015) uses machine translation technology to reduce the number of the typing letters. Huang and Zhao (2018) propose an LSTM-based encoder-decoder approach with the concatenation of context words and abbreviated pinyin as input. Our work differs from existing works in that we are the ﬁrst one to exploit GPT and verify the pros and cons of GPT in different situations. In addition, there are some works handling pinyin with typing errors. Chen and Lee (2000) investigate a typing model which handles spelling correction in sentence-based pinyin input method. CHIME (Zheng et al., 2011) is a error-tolerant Chinese pinyin input method. It ﬁnds similar pinyin which will be further ranked with Chinese speciﬁc features. Jia and Zhao (2014) propose a joint graph model to globally optimize the tasks of pinyin input method and typo correction. We leave errortolerant pinyin input method as a future work. Pinyin-enhanced Pretrained Models Our methodology also relates to pretrained models that use pinyin information. Sun et al. (2021) propose a general-purpose Chinese BERT with new embedding layers to inject pinyin and glyph information of characters. There are also task-speciﬁc BERT models, especially for the task of grammatical error correction since an important type of error is caused by characters pronounced with the same pinyin. Zhang et al. (2021a) add a pinyin embedding layer and learns to predict characters from similarly pronounced candidates. PLOME (Liu et al., 2021) add two embedding layers implemented with two GRU networks to inject both pinyin and shape of characters, respectively. Xu et al. (2021) add a hierarchical encoder to inject the pinyin letters at character and sentence levels, and add a ResNet encoder to use graphic features of character image.

 References: 
Zheng Chen and Kai-Fu Lee. 2000. A new statistical
approach to Chinese Pinyin input. In Proceedings of
the 38th Annual Meeting of the Association for Com-
putational Linguistics, pages 241–247, Hong Kong.
Association for Computational Linguistics.
Zeyao Du. 2019. Gpt2-chinese: Tools for training gpt2
model in chinese language. https://github.
com/Morizeyao/GPT2-Chinese.
Guoping Huang, Jiajun Zhang, Yu Zhou, and
Chengqing Zong. 2015. A new input method for
human translators: Integrating machine translation
effectively and imperceptibly. In Proceedings of the
24th International Conference on Artiﬁcial Intelli-
gence, IJCAI’15, page 1163–1169. AAAI Press.
Yafang Huang, Zuchao Li, Zhuosheng Zhang, and Hai
Zhao. 2018. Moon IME: Neural-based Chinese
Pinyin aided input method with customizable associ-
ation. In Proceedings of ACL 2018, System Demon-
strations, pages 140–145, Melbourne, Australia. As-
sociation for Computational Linguistics.
Yafang Huang and Hai Zhao. 2018. Chinese Pinyin
aided IME, input what you have not keystroked
yet. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing ,
pages 2923–2929, Brussels, Belgium. Association
for Computational Linguistics.
Zhongye Jia and Hai Zhao. 2013. KySS 1.0: a frame-
work for automatic evaluation of Chinese input
method engines. In Proceedings of the Sixth Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 1195–1201, Nagoya, Japan. Asian
Federation of Natural Language Processing.
Zhongye Jia and Hai Zhao. 2014. A joint graph model
for Pinyin-to-Chinese conversion with typo correc-
tion. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1512–1523, Baltimore,
Maryland. Association for Computational Linguis-
tics.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.
Shulin Liu, Tao Yang, Tianchi Yue, Feng Zhang, and
Di Wang. 2021. PLOME: Pre-training with mis-
spelled knowledge for Chinese spelling correction.
In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
2991–3000, Online. Association for Computational
Linguistics.
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.
Alec Radford, Jeff Wu, Rewon Child, David Luan, 
Dario Amodei, and Ilya Sutskever. 2019. Language 
models are unsupervised multitask learners. 
Zijun Sun, Xiaoya Li, Xiaofei Sun, Yuxian Meng, Xi- 
ang Ao, Qing He, Fei Wu, and Jiwei Li. 2021. Chi- 
neseBERT: Chinese pretraining enhanced by glyph 
and Pinyin information. In Proceedings of the 59th 
Annual Meeting of the Association for Computa- 
tional Linguistics and the 11th International Joint 
Conference on Natural Language Processing (Vol- 
ume 1: Long Papers), pages 2065–2075, Online. As- 
sociation for Computational Linguistics. 
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob 
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz 
Kaiser, and Illia Polosukhin. 2017. Attention is all 
you need. In Advances in neural information pro- 
cessing systems, pages 5998–6008. 
Heng-Da Xu, Zhongli Li, Qingyu Zhou, Chao Li, 
Zizhen Wang, Yunbo Cao, Heyan Huang, and Xian- 
Ling Mao. 2021. Read, listen, and see: Leveraging 
multimodal information helps Chinese spell check- 
ing. In Findings of the Association for Computa- 
tional Linguistics: ACL-IJCNLP 2021 , pages 716– 
728, Online. Association for Computational Linguis- 
tics. 
Liang Xu, Xuanwei Zhang, and Qianqian Dong. 
2020. Cluecorpus2020: A large-scale chinese 
corpus for pre-training language model. ArXiv, 
abs/2003.01355. 
Shaohua Yang, Hai Zhao, and Bao-liang Lu. 2012. To- 
wards a semantic annotation of English television 
news - building and evaluating a constraint grammar 
FrameNet. In Proceedings of the 26th Paciﬁc Asia 
Conference on Language, Information, and Compu- 
tation, pages 333–342, Bali, Indonesia. Faculty of 
Computer Science, Universitas Indonesia. 
Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, 
Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie 
Tang. 2021. Wudaocorpora: A super large-scale chi- 
nese corpora for pre-training language models. AI 
Open, 2:65–68. 
Haisong Zhang, Lemao Liu, Haiyun Jiang, Yangming 
Li, Enbo Zhao, Kun Xu, Linfeng Song, Suncong 
Zheng, Botong Zhou, Jianchen Zhu, Xiao Feng, 
Tao Chen, Tao Yang, Dong Yu, Feng Zhang, Zhan- 
hui Kang, and Shuming Shi. 2020. Texsmart: 
A text understanding system for ﬁne-grained ner 
and enhanced semantic analysis. arXiv preprint 
arXiv:2012.15639. 
Ruiqing Zhang, Chao Pang, Chuanqiang Zhang, Shuo- 
huan Wang, Zhongjun He, Yu Sun, Hua Wu, and 
Haifeng Wang. 2021a. Correcting Chinese spelling 
errors with phonetic pre-training. In Findings of 
the Association for Computational Linguistics: ACL- 
IJCNLP 2021 , pages 2250–2261, Online. Associa- 
tion for Computational Linguistics. 
9
Xihu Zhang, Chu Wei, and Hai Zhao. 2017. Tracing
a loose wordhood for chinese input method engine.
CoRR, abs/1712.04158.
Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian
Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji,
Jian Guan, et al. 2021b. Cpm: A large-scale gener-
ative chinese pre-trained language model. AI Open,
2:93–99.
Zhuosheng Zhang, Yafang Huang, and Hai Zhao. 2019.
Open vocabulary learning for neural Chinese Pinyin
IME. In Proceedings of the 57th Annual Meeting
of the Association for Computational Linguistics ,
pages 1584–1594, Florence, Italy. Association for
Computational Linguistics.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved Chinese word segmentation system with
conditional random ﬁeld. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, pages 162–165, Sydney, Australia. Association
for Computational Linguistics.
Yabin Zheng, Chen Li, and Maosong Sun. 2011.
Chime: An efﬁcient error-tolerant chinese pinyin
input method. In IJCAI, pages 2551–2556. IJ-
CAI/AAAI.
Xiaohua Zhou, Xiaohua Hu, Xiaodan Zhang, and Xia-
jiong Shen. 2007. A segment-based hidden markov
model for real-setting pinyin-to-chinese conversion.
In Proceedings of the Sixteenth ACM Conference on
Conference on Information and Knowledge Manage-
ment, CIKM ’07, page 1027–1030, New York, NY ,
USA. Association for Computing Machinery.
10
Model Top1 Top5 Top10 Top1 Top5 Top10 Top1 Top5 Top10
Entertainment Automobile Technology
GPT (ours) 26.84 35.97 37.73 27.84 36.56 38.03 26.01 34.48 35.86
PinyinGPT-Concat 28.74 41.68 45.48 28.74 41.55 45.28 26.82 40.17 43.65
Education Agriculture Economy
GPT (ours) 27.31 36.71 38.28 26.57 35.08 36.59 27.93 36.04 37.20
PinyinGPT-Concat 27.65 41.17 44.87 27.27 39.73 43.17 28.47 40.99 44.53
Games Culture Sports
GPT (ours) 24.04 32.78 34.23 21.86 29.33 30.94 28.54 37.13 38.69
PinyinGPT-Concat 25.78 38.26 41.89 22.10 33.33 36.72 29.81 43.56 46.95
International Society Military
GPT (ours) 26.42 34.82 36.24 26.57 36.15 37.78 24.46 32.26 33.75
PinyinGPT-Concat 27.49 40.16 43.66 27.34 40.94 44.89 24.82 36.73 40.03
Real Estate Medical Finance
GPT (ours) 26.53 35.27 36.74 33.59 43.54 44.93 29.00 37.24 38.47
PinyinGPT-Concat 27.28 40.16 43.86 34.76 49.28 52.56 29.17 42.17 45.52
Table 8: Results of different domains over WD using abbreviated pinyin. Each score is averaged over all the
context-target length conﬁgurations.
Models
1-3 4-9 10+
T P@1 P@5 P@10 T P@1 P@5 P@10 T P@1 P@5 P@10
0-3
GPT (ours, 6L) 38 26.74 38.45 41.50 98 10.46 14.41 15.19 201 2.72 3.70 3.85
GPT (ours, 12L) 58 30.11 42.27 45.25 148 13.33 18.24 18.99 303 4.16 5.86 6.00
PinyinGPT-Concat (6L) 40 29.17 45.17 50.73 98 11.92 19.55 21.84 197 3.20 5.67 6.22
PinyinGPT-Concat (12L) 61 31.72 48.09 53.94 148 15.21 24.39 26.94 305 5.58 9.22 10.09
4-9
GPT (ours, 6L) 38 44.02 59.02 62.32 94 20.02 27.45 28.76 198 5.72 8.05 8.31
GPT (ours, 12L) 57 49.83 65.03 67.96 142 25.53 34.48 35.89 301 9.38 12.70 13.03
PinyinGPT-Concat (6L) 38 45.66 65.08 70.56 94 20.25 32.70 36.14 192 5.98 10.23 11.29
PinyinGPT-Concat (12L) 58 50.78 70.11 75.58 145 26.44 41.51 45.52 298 10.20 17.02 18.80
10+
GPT (ours, 6L) 42 54.38 69.94 72.92 99 28.81 38.98 40.41 198 10.32 14.18 14.64
GPT (ours, 12L) 64 59.39 75.00 77.60 149 35.42 46.32 47.94 301 14.96 20.11 20.63
PinyinGPT-Concat (6L) 43 53.91 73.21 78.14 98 27.21 42.36 46.45 198 9.15 15.49 17.05
PinyinGPT-Concat (12L) 66 59.89 78.81 83.33 154 34.99 51.99 56.62 306 14.93 24.78 27.03
Table 9: Experiment results for different conﬁgurations over WD using abbreviated pinyin, each score is averaged
over all the domains. The ﬁrst column is the context length while the ﬁrst row is the target length. The ﬁeld T is
the average inference time in millisecond.
11