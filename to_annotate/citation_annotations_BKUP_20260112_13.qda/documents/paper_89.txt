Introduction

Around 200 languages in the world are signed rather than spoken, featuring their own vocabulary and grammatical structures. For example the American Sign Language (ASL) is not a mere translation of English into signs and is unrelated to the British Sign Language (BSL). This introduces many novel challenges to their automated processing. Research on Sign Language Processing (SLP) encompasses tasks such as sign language detection, i.e. recognising if and which signed language is performed (Moryossef et al., 2020) and sign language recognition (SLR) (Koller, 2020), i.e. the identification of signs either in isolation or in continuous speech. Other tasks concern the translation from signed to spoken (or written) (Camgoz et al.,2018) language or the production of signs from text (Rastgoo et al., 2021). With the recent success of deep learning-based approaches in computer vision (CV), as well as advancements in —from the CV perspective—related tasks of action and gesture recognition (Asadi-Aghbolaghi et al., 2017), SLP is gaining more attention in the CV community (Zheng et al., 2017).

Some recent approaches to various SLP tasks rely on phonological features, perhaps due to the complexity of the tasks (Tornay, 2021; Metaxas et al., 2018; Gebre et al., 2013; avella et al., 2021). Surprisingly, however, little work has been carried out on explicitly modelling the phonology of signed languages. This presents a timely opportunity to investigate signed languages from a linguist’s perspective (Yin et al., 2021). In the context of signed languages, phonology typically distinguishes between manual features, such as usage, position and movement of hands and fingers, and non-manual features, such as facial expression. Sign language phonology is a matured field with well-developed
theoretical frameworks (Liddell and Johnson, 1989; Fenlon et al., 2017; Sandler, 2012). These phonological features, or phonemes, are drawn from a fixed inventory of possible configurations which is typically much smaller than the vocabulary of signed languages (Borg and Camilleri, 2020). For example, there is only a limited number of fingers
that can be used to perform a sign due to anatomical constraints. Hence, different signs share phonological properties and well performing classifiers can be used to predict those properties for signs unseen during training. This potentially holds even across different languages, because, while different languages may dictate different combinations
of phonemes, there are also significant overlaps (Tornay et al., 2020).

Finally, these phonological properties have a strong discriminatory power when determining signs. For example, in ASL-Lex (Caselli et al., 2017), a lexicon which also captures phonology information, the authors report that more than 50% of its 994 described signs have a unique combination of only six phonological properties and more than 80% of the signs share their combination with at most two other signs. By relying on additional (i.e., phonological) information from resources such as ASL-Lex, many signs can be determined from (predicted) phonological properties alone, without encountering them in training data. This is a capability that current data-driven approaches to SLR lack by design (Koller, 2020). Thus, in combination, mature approaches to phonology recognition can facilitate the development of sign language resources. This is an important task for both documenting low-resource sign languages as well as rapid developing of large-scale datasets, to fully harness data-driven CV approaches.

To spur research in this direction, we extend the preliminary work by Tavella et al. (2021) and introduce the task of Phonological Property Recognition (PPR). More specifically, this paper contributes (i) WLASLLex2001, a large-scale, automatically constructed PPR dataset, (ii) an analysis of the dataset quality, and (iii) an empirical study of the performance of different deep-learning based baselines thereon.

 References: 
Asadi-Aghbolaghi, M., Clapes, A., Bellantonio, M., Escalante, H. J., Ponce-Lopez, V., Baro, X., Guyon, I., Kasaei, S., and Escalera, S. 2017. A Survey on Deep Learning Based Approaches for Action and Gesture Recognition in Image Sequences. In Proceedings -12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 -1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild. pp. 476--483 10.1109/FG.2017.150
Battison, R. 1978. Lexical borrowing in american sign language. In Lexical borrowing in american sign language.
Borg, M. and Camilleri, K. P. 2020. Phonologically-Meaningful Subunits for Deep Learning-Based Sign Language Recognition. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and. In Lecture Notes in Bioinformatics. pp. 12536--12199 10.1007/978-3-030-66096-3{_}15
Necati Cihan Camgoz, S., Hadfield, O., Koller, H., Ney, R., and Bowden 2018. Neural Sign Language Translation. In Neural Sign Language Translation.
Carreira, J. and Zisserman, A. 2017. Quo vadis, action recognition? a new model and the kinetics dataset. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4724--4733 10.1109/CVPR.2017.502
Caselli, N. K., Sehyr, Z. S., Cohen-Goldberg, A. M., and Emmorey, K. 2017. Asllex: A lexical database of american sign language. In Behavior Research Methods. pp. 784--801
Chen, D., Bolton, J., and Manning, C. D. 2016. A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pp. 2358--2367 10.18653/v1/P16-1223
Fenlon, J., Kearsy, A., Cormier, D., and Brentari 2017. Sign language phonology. In Routledge Handbook of Phonological Theory.
Binyam Gebrekidan Gebre, P., Wittenburg, T., and Heskes 2013. Automatic sign language identification. In IEEE International Conference on Image Processing. pp. 2626--2630 10.1109/ICIP.2013.6738541
Jiang, S., Sun, B., Wang, L., Bai, Y., Li, K., and Fu, Y. In 2021a. Sign Language Recognition via Skeleton-Aware Multi-Model Ensemble.
Jiang, S., Sun, B., Wang, L., Bai, Y., Li, K., and Fu, Y. 2021. Skeleton Aware Multimodal Sign Language Recognition. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. pp. 3408--3418 10.1109/CVPRW53098.2021.00380
Koller, O. 2020. Quantitative Survey of the State of the Art in Sign Language Recognition. In Quantitative Survey of the State of the Art in Sign Language Recognition.
Li, D., Rodriguez, C., Yu, X., and Li, H. 2020. Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison. In The IEEE Winter Conference on Applications of Computer Vision. pp. 1459--1469
Scott, K., Liddell, R. E., and Johnson 1989. American Sign Language: The Phonological Base. In Sign Language Studies. pp. 195--277 10.1353/sls.1989.0027
Matthews, B. W. 1975. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. In Biochimica et Biophysica Acta (BBA) -Protein Structure. pp. 442--451 10.1016/0005-2795(75)90109-9
Metaxas, D., Dilsizian, M., and Neidle, C. 2018. Scalable ASL sign recognition using modelbased machine learning and linguistically annotated corpora. In Scalable ASL sign recognition using modelbased machine learning and linguistically annotated corpora.
Moryossef, A., Tsochantaridis, I., Aharoni, R., Ebling, S., and Narayanan, S. 2020. Real-Time Sign Language Detection Using Human Pose Estimation. In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics. pp. 12536--12237 10.1007/978-3-030-66096-3{_}17
Rastgoo, R., Kiani, K., and Escalera, S. 2021. Sign Language Recognition: A Deep Survey. In Expert Systems with Applications. pp. 113794 10.1016/J.ESWA.2020.113794
Rong, Y., Shiratori, T., and Joo, H. 2021. Frankmocap: A monocular 3d whole-body pose estimation system via regression and integration. In IEEE International Conference on Computer Vision Workshops.
Sandler, W. 2012. The Phonological Organization of Sign Languages. Language and Linguistics Compass. In The Phonological Organization of Sign Languages. Language and Linguistics Compass. pp. 162--182 10.1002/LNC3.326
Tavella, F., Galata, A., and Cangelosi, A. 2021. Phonology recognition in american sign language. In Phonology recognition in american sign language.
Tornay, S. 2021. Explainable Phonology-based Approach for Sign Language Recognition and Assessment. In Explainable Phonology-based Approach for Sign Language Recognition and Assessment. 10.5075/epfl-thesis-8177
Tornay, S., Razavi, M., and Mathew Magimai.-Doss 2020. Towards Multilingual Sign Language Recognition. In Towards Multilingual Sign Language Recognition. ICASSP 2020 -2020
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 6304--6308
Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Mu, Y., Tan, M., Wang, X., Liu, W., and Xiao, B. 2019. Deep high-resolution representation learning for visual recognition. In Deep high-resolution representation learning for visual recognition.