Related Work

Sparse attention mechanism The full attention mechanism has a quadratic memory cost. Prior research works have proposed different sparse attention mechanisms to reduce the memory cost. Longformer (Beltagy et al., 2020) uses a dilated sliding window of blocks and global attention patterns. BigBird (Zaheer et al., 2020) employs sliding window and random blocks. Reformer (Kitaev et al., 2020) uses the locality-sensitive hashing. In addition to optimizing the encoder self-attention, Huang et al. (2021) proposes head-wise positional strides to reduce the cost of the encoder-decoder attention. However, sparse attention diminishes the benefits of pretraining and sacrifices parts of the receptive field.

Extract-then-generate method The model first extracts salient text snippets from the input, followed by generating a concise overall summary. Most two-stage summarization approaches (Zhang et al., 2019;Lebanoff et al., 2019;Xu and Durrett, 2019;Bajaj et al., 2021) are trained separately, which suffer from information loss due to the cascaded errors. Some approaches attempt to reduce that loss by bridging the two stages. Chen and Bansal (2018) adopts reinforcement learning with a sentence-level policy gradient method. Bae et al. (2019) proposes summary-level policy gradient. In addition to the drawbacks explained in Section 2.3, our model is different as we jointly train an extractthen-generate model for summarization using latent variables.

Divide-and-conquer approach A common approach in long input summarization is divide-andconquer (Gidiotis and Tsoumakas, 2020;Grail et al., 2021). This approach breaks a long input into multiple parts, which are summarized separately and combined to produce a final complete summary. However, these models do not capture the contextual dependencies across parts and assumes a certain structure of the input (such as paper sections).

Hierarchical models Various hierarchical models have been proposed to handle the longer inputs. Cohan et al. (2018) models the document discourse structure with a hierarchical encoder and a discourse-aware decoder to generate the summary. HAT-Bart (Rohde et al., 2021) proposes a new Hierarchical Attention Transformer-based architecture that attempts to capture sentence and paragraphlevel information. HMNet (Zhu et al., 2020) builds a hierarchical structure that includes discourselevel information and speaker roles. However, these models focus mainly on model performance and not on reducing the memory and computational cost.

 References: 
Bae, S., Kim, T., Kim, J., and Lee, S. 2019. Summary level training of sentence rewriting for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization. pp. 10--20
Bahdanau, D., Cho, K., and Bengio, Y. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations.
Bajaj, A., Dangati, P., Krishna, K., Pradhiksha, A., Kumar, R., Uppaal, B., Windsor, E., Brenner, D., Dotterrer, R., Das, A., and Mccallum 2021. Long document summarization in a low resource setting using pretrained language models. In Proceedings of the ACL-IJCNLP 2021 Student Research Workshop, ACL 2021. pp. 71--80
Beltagy, I., Peters, M. E., and Cohan, A. 2004. Longformer: The long-document transformer. CoRR, abs. In Longformer: The long-document transformer. CoRR, abs.
Bražinskas, A., Lapata, M., and Titov, I. 2021. Learning opinion summarizers by selecting informative reviews. In Learning opinion summarizers by selecting informative reviews. pp. 2109
Carletta, J., Ashby, S., Bourban, S., Flynn, M., Guillemot, M., Hain, T., Kadlec, J., Karaiskos, V., Kraaij, W., and Kronenthal, M. 2005. The ami meeting corpus: A pre-announcement. In International workshop on machine learning for multimodal interaction. pp. 28--39
Chen, T., Xu, B., Zhang, C., and Guestrin, C. 2016. Training deep nets with sublinear memory cost. In Training deep nets with sublinear memory cost. abs/1604.06174
Chen, Y. and Bansal, M. 2018. Fast abstractive summarization with reinforce-selected sentence rewriting. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 675--686
Child, R., Gray, S., Radford, A., and Sutskever, I. 1904. Generating long sequences with sparse transformers. CoRR, abs. In Generating long sequences with sparse transformers. CoRR, abs.
Cohan, A., Dernoncourt, F., Doo, S., Kim, T., Bui, S., Kim, W., Chang, N., and Goharian 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 615--621
Cui, P. and Hu, L. 2021. Sliding selector network with dynamic memory for extractive summarization of long documents. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 5881--5891
Gidiotis, A. and Tsoumakas, G. 2020. A divide-and-conquer approach to the summarization of long documents. In Speech, and Language Processing. pp. 3029--3040
Grail, Q., Perez, J., and Gaussier, E. 2021. Globalizing BERT-based transformer architectures for long document summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 1792--1810
Huang, L., Cao, S., Nikolaus, N., Parulian, H., Ji, L., and Wang 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 1419--1436
Janin, A., Baron, D., Edwards, J., Ellis, D., Gelbart, D., Morgan, N., Peskin, B., Pfau, T., Shriberg, E., and Stolcke, A. 2003. The icsi meeting corpus. In 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing.
Kitaev, N., Kaiser, L., and Levskaya, A. 2020. Reformer: The efficient transformer. In International Conference on Learning Representations.
Lebanoff, L., Song, K., Dernoncourt, F., Doo, S., Kim, S., Kim, W., Chang, F., and Liu 2019. Scoring sentence singletons and pairs for abstractive summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 2175--2189
Lewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of ACL 2020. pp. 7871--7880
Patrick, S. H., Lewis, E., Perez, A., Piktus, F., Petroni, V., Karpukhin, N., Goyal, H., Küttler, M., Lewis, W., Yih, T., Rocktäschel, S., Riedel, D., and Kiela 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.
Lin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.
Narayan, S., Shay, B., Cohen, M., and Lapata 2018. Ranking sentences for extractive summarization with reinforcement learning. In NAACL-HLT.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67
Rohde, T., Wu, X., and Liu, Y. 2021. Hierarchical learning for generation with long source sequences. In Hierarchical learning for generation with long source sequences. arXiv:2104.07545
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. 2020. Long range arena: A benchmark for efficient transformers. In Long range arena: A benchmark for efficient transformers. arXiv:2011.04006
Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. 2009. Efficient transformers: A survey. In Efficient transformers: A survey.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Attention is all you need. pp. 5998--6008
Williams, R. J. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. In Mach. Learn. pp. 229--256
Wu, J., Ouyang, L., Daniel, M., Ziegler, N., Stiennon, R., Lowe, J., Leike, P., and Christiano 2021. Recursively summarizing books with human feedback. In Recursively summarizing books with human feedback. arXiv:2109.10862
Xiao, W. and Carenini, G. 2019. Extractive summarization of long documents by combining global and local context. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3011--3021
Xu, J. and Durrett, G. 2019. Neural extractive text summarization with syntactic compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3292--3303
Zaheer, M., Guruganesh, G., Kumar Avinava Dubey, J., Ainslie, C., Alberti, S., Ontanon, P., Pham, A., Ravula, Q., Wang, L., and Yang 2020. Big bird: Transformers for longer sequences. In NeurIPS.
Zhang, H., Cai, J., Xu, J., and Wang, J. 2019. Pretraining-based natural language generation for text summarization. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). pp. 789--797
Zhang, J., Zhao, Y., Saleh, M., and Liu, P. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning. pp. 11328--11339
Zhong, M., Liu, Y., Xu, Y., Zhu, C., and Zeng, M. 2021. Dialoglm: Pre-trained model for long dialogue understanding and summarization. In Dialoglm: Pre-trained model for long dialogue understanding and summarization. arXiv:2109.02492
Zhong, M., Da Yin, T., Yu, A., Zaidi, M., Mutuma, R., Jha, A., Hassan Awadallah, A., Celikyilmaz, Y., Liu, X., Qiu, D. R., and Radev 2021. Qmsum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 5905--5921
Zhu, C., Xu, R., Zeng, M., and Huang, X. 2020. A hierarchical network for abstractive meeting summarization with cross-domain pretraining. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. pp. 194--203
A Additional Dynamic Weight Visualization. In A Additional Dynamic Weight Visualization.