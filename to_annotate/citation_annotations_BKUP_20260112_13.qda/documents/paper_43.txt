Introduction

Grammatical Error Correction (GEC) task has a purpose to correct grammatical errors in natural texts. It includes correcting errors in spelling, punctuation, grammar, morphology, word choice, and others. Intelligent GEC system receives text containing mistakes and produces its corrected version. GEC task is complicated and challenging: the accuracy of edits, inference speed, and memory limitations are the topics of intensive research.

Currently, Machine Translation (MT) is the mainstream approach for GEC. In this setting, errorful sentences correspond to the source language, and error-free sentences correspond to the target language. Early GEC-MT methods leveraged phrase-based statistical machine translation (PBSMT) (Yuan and Felice, 2013). Then they rapidly evolved to sequence-to-sequence Neural Machine Translation (NMT) based on gated recurrent neural networks (Yuan and Briscoe, 2016) and recent powerful Transformer-based Seq2Seq models. They autoregressively capture full dependency among output tokens; however, it might be slow due to sequential decoding. (Grundkiewicz et al., 2019) leveraged Transformer model (Vaswani et al., 2017) which was pre-trained on synthetic GEC data and right-to-left re-ranking for ensemble. (Kaneko et al., 2020) adopted several strategies of BERT (Devlin et al., 2018) usage for GEC. Recently, (Rothe et al., 2021) built their system on top of T5 (Xue et al., 2021), a xxl version of T5 Transformer encoder-decoder model and reached new state-of-the-art results (11B parameters).

The sequence tagging approach that generates a sequence of text edit operations encoded by tags for errorful input text is becoming more common now. LaserTagger (Malmi et al., 2019) is a sequence tagging model that casts text generation as a text editing task. Corrected texts are reconstructed from the inputs using three main edit operations: keeping a token, deleting it, and adding a phrase before the token. LaserTagger combines a BERT encoder with an autoregressive Transformer decoder, which predicts edit operations. Parallel Iterative Edit (PIE) model  does parallel decoding, achieving quality that is competitive with the Seq2Seq models 3 . It predicts edits instead of tokens and iteratively refines predictions to capture dependencies. A similar approach is presented in (Omelianchuk et al., 2020). GECToR system uses various Transformers as an encoder, linear layers with softmax for tag prediction and error detection instead of a decoder. It also managed to achieve competitive results being potentially several times faster than Seq2Seq because of replacing autoregressive decoder with linear output layers. Also, nowadays generation of synthetic data is becoming significant for most GEC models. Natural languages are rich, and their Grammars contain many rules and exceptions; therefore, professional linguists usually need to annotate highquality corpora for further training of ML-based systems mostly in a supervised manner (Dahlmeier et al., 2013), . At the same time, human annotation is expensive, so researchers are working on methods for augmentation of training data, synthetic data generation, and strategies for its efficient usage (Lichtarge et al., 2019), , (Stahlberg and Kumar, 2021). Most of the latest works use synthetic data to pre-train Transformer-based components of their models.

In this work, we are focusing on exploring sequence tagging models and their ensembles. Although most of our developments might eventually be applied to other languages, we work with English only in this study. Being a rich-resource language, English provides a highly competitive area for GEC task 3 . We leave dealing with other languages for future work.

 References: 
Awasthi, A., Sarawagi, S., Goyal, R., Ghosh, S., and Piratla, V. 2019. Parallel iterative edit models for local sequence transduction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 4259--4269
Awasthi, A., Sarawagi, S., Goyal, R., Ghosh, S., and Piratla, V. 2019. Parallel iterative edit models for local sequence transduction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 4260--4270 10.18653/v1/D19-1435
Bryant, C., Felice, M., Andersen, Ø. E., and Briscoe, T. 2019. The BEA-2019 shared task on grammatical error correction. In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications. pp. 52--75 10.18653/v1/W19-4406
Bryant, C., Felice, M., and Briscoe, T. 2017. Automatic annotation and evaluation of error types for grammatical error correction. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 793--805
Bryant, C., Felice, M., Andersen, Ø. E., and Briscoe, T. 2019. The bea-2019 shared task on grammatical error correction. In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications. pp. 52--75
Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., and Koehn, P. 2013. One billion word benchmark for measuring progress in statistical language modeling. In One billion word benchmark for measuring progress in statistical language modeling. abs/1312.3005
Dahlmeier, D., Hwee Tou Ng, S.M., and Wu 2013. Building a large annotated corpus of learner english: The nus corpus of learner english. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications. pp. 22--31
Devlin, J., Chang, M., Lee, K., and Toutanova, K. N. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186
Gardner, M., Grus, J., Neumann, M., Tafjord, O., Dasigi, P., Liu, N. F., Peters, M., Schmitz, M., and Zettlemoyer, L. S. 2017. Allennlp: A deep semantic natural language processing platform. In Allennlp: A deep semantic natural language processing platform.
Grundkiewicz, R., Junczys-Dowmunt, M., and Heafield, K. 2019. Neural grammatical error correction systems with unsupervised pre-training on synthetic data. In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications. pp. 252--263 10.18653/v1/W19-4427
He, P., Liu, X., Gao, J., and Chen, W. 2020. Deberta: Decoding-enhanced bert with disentangled attention. In Deberta: Decoding-enhanced bert with disentangled attention. arXiv:2006.03654
Hinton, G., Vinyals, O., and Dean, J. 2015. Distilling the knowledge in a neural network. In Distilling the knowledge in a neural network. arXiv:1503.02531
Kaneko, M., Mita, M., Kiyono, S., Suzuki, J., and Inui, K. 2020. Encoder-decoder models can benefit from pre-trained masked language models in grammatical error correction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4248--4254
Kim, Y. and Rush, A. M. 2016. Sequencelevel knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 1317--1327 10.18653/v1/D16-1139
Diederik, P., Kingma, J., and Ba 2015. Adam (2014), a method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), arXiv preprint arXiv.
Kiyono, S., Suzuki, J., Mita, M., Mizumoto, T., and Inui, K. 2019. An empirical study of incorporating pseudo data into grammatical error correction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 1236--1242
Kiyono, S., Suzuki, J., Mita, M., Mizumoto, T., and Inui, K. 2019. An empirical study of incorporating pseudo data into grammatical error correction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 1236--1242 10.18653/v1/D19-1119
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. Albert: A lite bert for self-supervised learning of language representations. In ICLR 2020 : Eighth International Conference on Learning Representations.
Liang, D., Zheng, C., Guo, L., Cui, X., Xiong, X., Rong, H., and Dong, J. 2020. Bert enhanced neural machine translation and sequence tagging model for chinese grammatical error diagnosis. In Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications. pp. 57--66
Lichtarge, J., Alberti, C., Kumar, S., Shazeer, N., Parmar, N., and Tong, S. 2019. Corpora generation for grammatical error correction. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3291--3301
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692
Malmi, E., Krause, S., Rothe, S., Mirylenka, D., and Severyn, A. 2019. Encode, tag, realize: High-precision text editing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5054--5065 10.18653/v1/D19-1510
Omelianchuk, K., Atrasevych, V., Chernodub, A. N., and Skurzhanskyi, O. 2020. Gector -grammatical error correction: Tag, not rewrite. In Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications. pp. 163--170
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2018. Language models are unsupervised multitask learners. In Language models are unsupervised multitask learners.
Rothe, S., Mallinson, J., and Malmi, E. Sebastian Krause, and Aliaksei Severyn. 2021. A Simple Recipe for Multilingual Grammatical Error Correction. In Proc. of ACL-IJCNLP.
Schler, J., Koppel, M., Argamon, S., and Pennebaker, J. W. 2005. Effects of age and gender on blogging. In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs. pp. 199--205
Sennrich, R., Haddow, B., and Birch, A. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pp. 1715--1725 10.18653/v1/P16-1162
Stahlberg, F. and Kumar, S. 2021. Synthetic data generation for grammatical error correction with tagged corruption models. In Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications. pp. 37--47
Tajiri, T., Komachi, M., and Matsumoto, Y. 2012. Tense and aspect error correction for esl learners using global context. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. pp. 198--202
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. 2017. Attention is all you need. In Advances in neural information processing systems. pp. 5998--6008
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., and Funtowicz, M. 2019. Huggingface's transformers: State-ofthe-art natural language processing. In Huggingface's transformers: State-ofthe-art natural language processing. arXiv:1910.03771
Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., and Siddhant, A. Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 483--498 10.18653/v1/2021.naacl-main.41
Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems. pp. 5753--5763
Yannakoudakis, H., Briscoe, T., and Medlock, B. 2011. A new dataset and method for automatically grading esol texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. pp. 180--189
Yuan, Z. and Briscoe, T. 2016. Grammatical error correction using neural machine translation. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 380--386
Yuan, Z. and Felice, M. 2013. Constrained grammatical error correction using statistical machine translation. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task. pp. 52--61