Related Work

Knowledge can be elicited from pretrained language models. Numerous works have shown that pretrained language models implicitly contain large a amount of knowledge that can be queried via conditional generation (Davison et al., 2019;Petroni et al., 2019;. Consequently, these models can directly perform inference on tasks like commonsense reasoning (Trinh and Le, 2018;), text classification (Shin et al., 2020;Puri and Catanzaro, 2019), and natural language inference (Shin et al., 2020;Schick and Schütze, 2021). Inspired by these observations, we elicit question-related knowledge in an explicit form from language models and use them to guide the inference.

Leveraging external knowledge for commonsense reasoning. Some work uses external commonsense knowledge bases to make improvements on various NLP tasks, including commonsense reasoning. One approach is to inject commonsense knowledge into language models, either by pretraining on knowledge bases (Ma et al., 2021;Chang et al., 2020;Mitra et al., 2019;Zhong et al., 2019) or finetuning the model so that it can reason with additional retrieved knowledge (Chang et al., 2020;Mitra et al., 2019;Bian et al., 2021). Another direction is to ground the question into a knowledge graph and do inference with graph-based reasoning (Lin et al., 2019;Lv et al., 2020;Yasunaga et al., 2021).

A common prerequisite of these methods is a high-quality, high-coverage, in-domain commonsense knowledge base (Ma et al., 2019). Some commonsense reasoning datasets are derived from existing knowledge bases; for example, CommonsenseQA (Talmor et al., 2019) is derived from ConceptNet (Speer et al., 2017), and Social IQA (Sap et al., 2019b) is derived from ATOMIC (Sap et al., 2019a). For such datasets, it is natural to elicit related knowledge from the underlying knowledge base that derived them, and typically this would demonstrate considerable gains (Mitra et al., 2019; Chang et al., 2020). However, if there is a domain mismatch between the dataset and the knowledge base, such gains tend to diminish (Mitra et al., 2019; Ma et al., 2019). This becomes a bottleneck when encountering datasets that have no suitable knowledge base (e.g. NumerSense (Lin et al., 2020) and CommonsenseQA 2.0 (Talmor
et al., 2021)), or when the system needs to handle commonsense queries that do not fit in any of the commonsense domains represented by an existing knowledge base. Our work overcomes this difficulty by leveraging pretrained language models as the source of commonsense knowledge.

Adding generated text during inference. Recently, several works show that model performance on commonsense reasoning can be boosted by augmenting the question with model-generated text, such as clarifications, explanations, and implications. Self-talk (Shwartz et al., 2020) elicits clarifications to concepts in the question and appends them to the inference model input. Contrastive explanations (Paranjape et al., 2021) prompts inference models with generated explanations that contrast between two answer choices. The aforementioned methods depend on task-specific templates to inquire the generator, which means they are only capable of eliciting a limited variety of knowledge and require careful hand-crafting to transfer to new tasks. Other explanation-based methods (Latcinnik and Berant, 2020;Rajani et al., 2019) finetune the generator model so that it produces explanations that are used for question augmentation. DynaGen  uses pretrained commonsense models to generate implications of a question and expands the inference input with these generations. However, its usage of COMeT (Bosselut et al., 2019) as the generator confines its applicability to the social commonsense domain. Our work contributes to this general line of research, yet different from these previous methods that elicit knowledge with task-specific templates or from finetuned knowledge generators, our method requires only a few human-written demonstrations in the style of the task, making it much more flexible, easy-to-transfer, and engineering-efficient.

 References: 
Bian, N., Han, X., Chen, B., and Sun, L. 2021. Benchmarking knowledge-enhanced commonsense question answering via knowledge-to-text transformation. In Benchmarking knowledge-enhanced commonsense question answering via knowledge-to-text transformation. arXiv:2101.00760
Bosselut, A., Ronan Le Bras, Y., and Choi 2021. Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering. In Proceedings of the 35th AAAI Conference on Artificial Intelligence (AAAI).
Bosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470
Tom B Brown, B., Mann, N., Ryder, M., Subbiah, J., Kaplan, P., Dhariwal, A., Neelakantan, P., Shyam, G., and Sastry arXiv:2005.14165
Chang, T., Liu, Y., Gopalakrishnan, K., Hedayatnia, B., Zhou, P., and Hakkani-Tur, D. 2020. Incorporating commonsense knowledge graph in pretrained models for social commonsense tasks. In Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. pp. 74--79 10.18653/v1/2020.deelio-1.9
Davison, J., Feldman, J., and Rush, A. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 1173--1178 10.18653/v1/D19-1109
Geoffrey, E. and Hinton 2002. Training products of experts by minimizing contrastive divergence. In Neural computation. pp. 1771--1800
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. 2019. The curious case of neural text degeneration. In The curious case of neural text degeneration. arXiv:1904.09751
Jiang, Z. and Xu, F. F. 2020. How can we know what language models know?. In Transactions of the Association for Computational Linguistics. pp. 423--438 10.1162/tacl_a_00324
Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., and Hajishirzi, H. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 1896--1907 10.18653/v1/2020.findings-emnlp.171
Khot, T., Clark, P., Guerquin, M., Jansen, P., and Sabharwal, A. 2020. Qasc: A dataset for question answering via sentence composition. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8082--8090
Landis, R. and Koch, G.G. 1977. The measurement of observer agreement for categorical data. In biometrics. pp. 159--174
Latcinnik, V. and Berant, J. 2020. Explaining question answering models through text generation. In Explaining question answering models through text generation. arXiv:2004.05569
Bill Yuchen Lin, X., Chen, J., Chen, X., and Ren 2019. KagNet: Knowledge-aware graph networks for commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2829--2839 10.18653/v1/D19-1282
Bill Yuchen Lin, S., Lee, R., Khanna, X., and Ren 2020. Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6862--6868 10.18653/v1/2020.emnlp-main.557
Lourie, N., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2021. Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark. In Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark. arXiv:2103.13009
Lv, S., Guo, D., Xu, J., Tang, D., Duan, N., Gong, M., Shou, L., Jiang, D., Cao, G., and Hu, S. 2020. Graphbased reasoning over heterogeneous external knowledge for commonsense question answering. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8449--8456
Ma, K., Francis, J., Lu, Q., Nyberg, E., and Oltramari, A. 2019. Towards generalizable neuro-symbolic systems for commonsense question answering. In Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing. pp. 22--32 10.18653/v1/D19-6003
Ma, K., Ilievski, F., Francis, J., Bisk, Y., Nyberg, E., and Oltramari, A. 2021. Knowledge-driven data construction for zero-shot evaluation in commonsense question answering. In 35th AAAI Conference on Artificial Intelligence.
Mitra, A. and Banerjee, P. Kuntal Kumar Pal. In Swaroop Mishra, and Chitta Baral. 2019. How additional knowledge can improve natural language commonsense question answering? arXiv preprint. arXiv:1909.08855
Paranjape, B., Michael, J., Ghazvininejad, M., Hajishirzi, H., and Zettlemoyer, L. 2021. Prompting contrastive explanations for commonsense reasoning tasks. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 4179--4192 10.18653/v1/2021.findings-acl.366
Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Language models as knowledge bases?. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250
Puri, R. and Catanzaro, B. 2019. Zero-shot text classification with generative language models. In Zero-shot text classification with generative language models. arXiv:1912.10165
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683
Nazneen Fatema Rajani, B., Mccann, C., Xiong, R., and Socher 2019. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4932--4942 10.18653/v1/P19-1487
Sap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for if-then reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035
Sap, M., Rashkin, H., Chen, D., Ronan Le Bras, Y., and Choi 2019. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 4463--4473 10.18653/v1/D19-1454
Schick, T. and Schütze, H. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 255--269
Shin, T., Razeghi, Y., Logan, R. L., IV, Wallace, E., and Singh, S. 2020. Auto-Prompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4222--4235 10.18653/v1/2020.emnlp-main.346
Shwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373
Speer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Thirty-first AAAI conference on artificial intelligence.
Talmor, A., Herzig, J., Lourie, N., and Berant, J. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4149--4158 10.18653/v1/N19-1421
Talmor, A., Yoran, O., Ronan, L., Bras, C., Bhagavatula, Y., Goldberg, Y., Choi, J., and Berant 2021. Commonsenseqa 2.0: Exposing the limits of ai through gamification. In Commonsenseqa 2.0: Exposing the limits of ai through gamification.
Trieu, H., Trinh, Q.V., and Le 2018. A simple method for commonsense reasoning. In A simple method for commonsense reasoning. arXiv:1806.02847
2021. Usc ink submission on numersense. In Usc ink submission on numersense.
Yang, J., Lin, S., Nogueira, R., Tsai, M., Wang, C., and Lin, J. 2020. Designing templates for eliciting commonsense knowledge from pretrained sequence-to-sequence models. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 3449--3453 10.18653/v1/2020.coling-main.307
Yasunaga, M., Ren, H., Bosselut, A., Liang, P., and Leskovec, J. 2021. QA-GNN: Reasoning with language models and knowledge graphs for question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 535--546 10.18653/v1/2021.naacl-main.45
Zhang, Y. 2021. Stanford submission on numersense. In Stanford submission on numersense.
Zhong, W., Tang, D., Duan, N., Zhou, M., Wang, J., and Yin, J. 2019. Improving question answering by commonsense-based pre-training. In CCF International Conference on Natural Language Processing and Chinese Computing. pp. 16--28