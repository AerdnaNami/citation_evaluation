Introduction

Vision and language are two of most important information sources, and the fact that humans can reason jointly with both sources at the same time has motivated artificial intelligence research to consider visually-grounded language understanding. Most work in this area has focused on reasoning with local evidence (Suhr et al., 2018;Hudson and Manning, 2019;Lu et al., 2020;Liu et al., 2021), e.g. asking about factoid questions such as the colors or shapes of objects and numbers of people, yet few of works encourage open-ended reasoning where a model needs to look beyond task inputs. However, humans can relate visual cues to corresponding contextual information that could be multi-modal, and draw on background knowledge when interpreting and grounding images. For example, as Figure 1 shows, people that are familiar with the news can infer that the location is Times Square through the iconic screen panels, and further estimate the period of time by looking a the crowds and the signs. And, this can be done without explicitly including related news pieces as input. In fact, even though some people would not have the prior knowledge to identify the relevant events, it is likely that they would have good estimate of the location and time by interpreting textual evidence in the image, the language, entity names, building styles, and other details in the input image.

In this work, we identify and formulate this problem, spatio-temporal grounding of images, a task aiming at identifying the time and location the given image was taken. Specifically, we develop a novel dataset TARA, ( Time and pl Ace for Reasoning beyond the im Age), a challenging and important task that tasks models with grounding images to real-world spatial and temporal information. In our collection, we make sure that if models can accurately find imagesâ€™ creation time and location, they would need to successfully link the visual clues with contexts, which are often only found in texts such as news, stories and encyclopedias. As a result, this task motivates models to consider the association between visual information and language more closely and in a more open-ended setting. Figure 2 shows an example from TARA, and Figure 3 shows a possible way for a model to ground the image to its spatio-temporal information. The system starts with grounding multiple segments from the image, and uses the information to conduct a constrained search in a large news-base, until it locates specific textual information related to the image. This demonstrates the complexity and significance of this task.

TARA is collected via a rigorous process that involves rule-based distant supervision extraction from news-images data which results in 16k image examples. While the training data has high label correctness (around 95%), we further run a crowdsourced validation on 3k examples to form the evaluation dataset. During the validation, annotators are asked to validate that there exists a potential path for humans to derive the correct answer, which encourages proper reasoning in future works. To better support the study of domain transfer and supervision sizes, we collect an additional 61k examples from the Wikipedia domain. We apply the state-of-the-art joint model CLIP (Radford et al., 2021) and show that it only achieves accuracy of 11.11% and 0.46% for time and location, respectively, on our dataset. Additionally, we present a new CLIP-based baseline model that reasons on object and facial segments and achieves 16.46% and 1.07% accuracy for time and location, respectively. We show that there exists a large gap (around 70% in accuracy) between state-of-the-art models and human performance, suggesting that the TARA data will provide a benchmark to motivate reasoning based approaches and support significant future work.

 References: 
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, L., and Parikh, D. 2015. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision. pp. 2425--2433
Chen, W., Lucchi, A., and Hofmann, T. 2016. A semi-supervised framework for image captioning. In A semi-supervised framework for image captioning. arXiv:1611.05321
Gritta, M., Taher Pilehvar, M., Limsopatham, N., and Collier, N. 2018. What's missing in geographical parsing?. In Language Resources and Evaluation. pp. 603--623
Guo, J. and Deng, J. Alexandros Lattas, and Stefanos Zafeiriou. 2021. Sample and computation redistribution for efficient face detection. In Alexandros Lattas, and Stefanos Zafeiriou. 2021. Sample and computation redistribution for efficient face detection. arXiv:2105.04714
Drew, A., Hudson, C.D., and Manning 2019. Gqa: a new dataset for compositional question answering over real-world images. In Gqa: a new dataset for compositional question answering over real-world images. pp. 3 arXiv:1902.09506
Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Zitnick, L., and Girshick, R. 2017. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2901--2910
Kulkarni, S., Jain, S., Hosseini, M. J., Baldridge, J., Ie, E., and Zhang, L. 2008. Spatial language representation with multilevel geocoding. ArXiv, abs. In Spatial language representation with multilevel geocoding. ArXiv, abs.
Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., and Dyer, C. 2016. Neural architectures for named entity recognition. In Neural architectures for named entity recognition. arXiv:1603.01360
Liu, F., Bugliarello, E., Edoardo, M., Ponti, S., Reddy, N., Collier, D., and Elliott 2021. Visually grounded reasoning across languages and cultures. In Visually grounded reasoning across languages and cultures. arXiv:2109.13238
Lu, J., Goswami, V., Rohrbach, M., Parikh, D., and Lee, S. 2020. 12-in-1: Multi-task vision and language representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10437--10446
Ning, Q., Zhou, B., Feng, Z., Peng, H., and Roth, D. 2018. Cogcomptime: A tool for understanding time in natural language. In EMNLP.
Matthew, E., Peters, W., Ammar, C., Bhagavatula, R., and Power 2017. Semi-supervised sequence tagging with bidirectional language models. In Semi-supervised sequence tagging with bidirectional language models. arXiv:1705.00108
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., and Clark, J. 2021. Learning transferable visual models from natural language supervision. In Learning transferable visual models from natural language supervision. arXiv:2103.00020
Shen, J., Qiu, W., Meng, Y., Shang, J., Ren, X., and Han, J. 2021. Taxoclass: Hierarchical multi-label text classification using only class names. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4239--4249
Srinivasan, K., Raman, K., Chen, J., Bendersky, M., and Najork, M. 2021. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. arXiv:2103.01913
Suhr, A., Lewis, M., Yeh, J., and Artzi, Y. 2017. A corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 217--223
Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and Artzi, Y. 2018. A corpus for reasoning about natural language grounded in photographs. In A corpus for reasoning about natural language grounded in photographs. arXiv:1811.00491
Uzzaman, N., Llorens, H., Derczynski, L., Allen, J. F., Verhagen, M., and Pustejovsky, J. 2013. Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. In Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations.
Zhou, B., Ning, Q., Khashabi, D., and Roth, D. 2020. Temporal Common Sense Acquisition with Minimal Supervision. In Temporal Common Sense Acquisition with Minimal Supervision.
Zhou, B., Richardson, K., Ning, Q., Khot, T., Sabharwal, A., and Roth, D. 2021. Temporal reasoning on implicit events from distant supervision. In Temporal reasoning on implicit events from distant supervision.