Introduction

Open-domain dialogue system often suffers from safe response (Li et al., 2015; problem as they could only refer to the context when generating a response. To alleviate this, Knowledge-grounded conversation (KGC) is proposed to introduce external fact and real-world commonsense as prior knowledge (Zhou et al., 2018a;Dinan et al., 2019;Zhao et al., 2020a), such that a dialogue system is able to ground the conversation with the provided knowledge and therefore generate informative and engaging responses. As external knowledge supplements the background to the inputs and decides what to say, knowledge selection is a key ingredient in KGC.

Numerous methods have been developed to tackle the knowledge selection problem by sequential latent variables (Kim et al., 2020;Meng et al., 2020), reinforcement learning (Zhao et al., 2020b), or expectation maximization algorithm (Li et al., 2020). In spite of the progress in this task, knowledge selection remains an unsolved problem as the precision is still far from satisfactory in Wizard of Wikipedia (Dinan et al., 2019) and other benchmarks in KGC (Gopalakrishnan et al., 2019), which also hinders the optimization of subsequent response generation models. A crucial point is, they often make assumption that the golden knowledge is distinguishable as long as the dialogue context is known, yet this is not always held true because there exists a one-to-many relationship in conversation and the past utterance history in a dialogue session is insufficient to decide the knowledge selection or the future trend of a dialogue.

As is shown in Figure 1, personalization is a key to success in the task because knowledge selection is a personal or subjective process in nature. When people communicate with each other, their perception of dialogue context will evoke their past memory about relevant life experience, taste and values, which we refer to as personal memory. The aroused fragment of personal memory further guides their interest and preference for different knowledge. In other words, there exists a mapping from one's personal memory to its selection of knowledge.

Importing persona memory into knowledge selection is a non-trivial task. One of the challenge is concretization of personal memory. Personal memory is an abstract concept related to user-specific experience, which is difficult to depict or model. Though it has been discussed in open-domain dialogue (Li et al., 2016;Zhang et al., 2018), no previous research sheds light on the personalization issue in KGC and there exists no dialogue dataset featured with external facts and personal memory at the same time. Besides, there is no annotated label to indicate which knowledge candidate a person will choose based on his or her personal memory. Namely, the mapping between personal memory and knowledge selection is highly unconstrained without golden label.

To address the above issue, we construct a KGC dataset featured with personalized memory repository, collecting user-specific utterance history under multiple types of context, which is a reflection of one's personal memory. And to discover the underlying relationship between the dialogue context, personal memory and knowledge, we propose a variational method and introduce two latent variables Z p and Z k to indicate the fragment of personal memory to evoke and the knowledge candidate to select respectively. And to model the mapping from Z p to Z k , we introduce an inverse mapping as a dual task and employ dual learning to allow the two mappings to teach each other. The motivation behind this is intuitive: The reconstruction of personal memory from selected knowledge candidate is natural and easy if the mapping from personal memory to knowledge is accurate. Extensive experiment shows that our methods outperform competitive baselines in both automatic evaluation and human evaluation, justifying the importance of introducing personal memory and the effect of the dual learning mechanism empirically.

The contributions of this work are three-fold:

(1) We explore the personalization issue of the knowledge selection task in KGC and construct a dataset featured with user-specific personal mem-ory to benefit relevant research in the future. We are the first to explore the possibility of introducing personal memory into KGC.

(2) We propose a novel variational method and introduce two latent variables to model the interdependency between the persona and knowledge. Besides, we employ dual learning to optimize the relationship between the dialogue context, personal memory and knowledge in a unified framework.

(3) We conduct extensive experiments and verify the proposed methods empirically. Both the automatic and human evaluation evidence the efficacy of our proposed method.

 References: 
Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Jos√©, M. F., Moura, D., Parikh, D., and Batra 2017. Visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 326--335
Denkowski, M. and Lavie, A. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805
Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J. 2019. Wizard of wikipedia: Knowledge-powered conversational agents. In ICLR.
Joseph, L. and Fleiss 1971. Measuring nominal scale agreement among many raters. In Psychological bulletin. pp. 378
Ghazvininejad, M., Brockett, C., Chang, M., Dolan, B., and Gao, J. 2018. A knowledge-grounded neural conversation model. In Wen-tau Yih, and Michel Galley.
Gopalakrishnan, K., Hedayatnia, B., Chen, Q., Gottardi, A., Kwatra, S., Venkatesh, A., and Gabriel, R. 2019. Topical-chat: Towards knowledge-grounded open-domain conversations. In Proc. Interspeech. pp. 1891--1895
He, D., Lu, H., Xia, Y., Qin, T., Wang, L., and Liu, T. 2017. Decoding with value networks for neural machine translation. In Proceedings of the 31st International Conference on Neural Information Processing Systems. pp. 177--186
He, D., Xia, Y., Qin, T., Wang, L., Yu, N., Liu, T., and Ma, W. 2016. Dual learning for machine translation. In Advances in neural information processing systems. pp. 820--828
Kim, B., Ahn, J., and Kim, G. 2020. Sequential latent knowledge selection for knowledge-grounded dialogue. In Sequential latent knowledge selection for knowledge-grounded dialogue. arXiv:2002.07510
Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2015. A diversity-promoting objective function for neural conversation models. In NAACL. pp. 110--119
Li, J., Monroe, W., Ritter, A., Jurafsky, D., Galley, M., and Gao, J. 2016. Deep reinforcement learning for dialogue generation. In EMNLP. pp. 1192--1202
Li, L., Xu, C., Wu, W., Zhao, Y., Zhao, X., and Tao, C. 2020. Zero-resource knowledge-grounded dialogue generation. In Zero-resource knowledge-grounded dialogue generation. arXiv:2008.12918
Lin, C. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out. pp. 74--81
Lin, J., Xia, Y., Qin, T., Chen, Z., and Liu, T. 2018. Conditional image-to-image translation. In Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5524--5532
Liu, Q., Chen, Y., Chen, B., Lou, J., Chen, Z., Zhou, B., and Zhang, D. 2020. You impress me: Dialogue generation via mutual persona perception. In You impress me: Dialogue generation via mutual persona perception. arXiv:2004.05388
Meng, C., Ren, P., Chen, Z., Sun, W., and Ren, Z. 2020. Dukenet: A dual knowledge interaction network for knowledge-grounded conversation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 1151--1160
Moghe, N., Arora, S., Banerjee, S., and Khapra, M.M. 2018. Towards exploiting background knowledge for building conversation systems. In Towards exploiting background knowledge for building conversation systems. arXiv:1809.08205
Papineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics. pp. 311--318
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In Language models are unsupervised multitask learners.
Ren, Y., Tan, X., Qin, T., Zhao, S., Zhao, Z., and Liu, T. 2019. Almost unsupervised text to speech and automatic speech recognition. In International Conference on Machine Learning. pp. 5410--5419
Song, H., Wang, Y., Zhang, K., Zhang, W., and Liu, T. 2021. BoB: BERT over BERT for training persona-based dialogue models from limited personalized data. In Proceedings of the 59th. 10.18653/v1/2021.acl-long.14
Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. In Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 167--177
Richard, S., Sutton, D. A., Mcallester, Satinder, P., Singh, Y., and Mansour 2000. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems. pp. 1057--1063
Tang, D., Duan, N., Qin, T., Yan, Z., and Zhou, M. 2017. Question answering and question generation as dual tasks. In Question answering and question generation as dual tasks. arXiv:1706.02027
Wang, Y., Xia, Y., He, T., Tian, F., Qin, T., Zhai, C., and Liu, T. 2019. Multiagent dual learning. In Proceedings of the International Conference on Learning Representations (ICLR). pp. 2019
Wolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149
Wu, W., Guo, Z., Zhou, X., Wu, H., Zhang, X., Lian, R., and Wang, H. 2019. Proactive human-machine conversation with explicit conversation goals. In Proactive human-machine conversation with explicit conversation goals. arXiv:1906.05572
Xia, Y., Qin, T., Chen, W., Bian, J., Yu, N., and Liu, T. 2017. Dual supervised learning. In International Conference on Machine Learning. pp. 3789--3798
Xia, Y., Tan, X., Tian, F., Qin, T., Yu, N., and Liu, T. 2018. Model-level dual learning. In International Conference on Machine Learning. pp. 5383--5392
Yi, Z., Hao, (., Richard, )., Zhang, P., Tan, M., and Gong 2017. Dualgan: Unsupervised dual learning for image-to-image translation. In IEEE International Conference on Computer Vision. pp. 2868--2876 10.1109/ICCV.2017.310
Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., and Weston, J. 2018. Personalizing dialogue agents: I have a dog. In Personalizing dialogue agents: I have a dog. arXiv:1801.07243
Zhang, Y., Sun, S., Galley, M., Chen, Y., Brockett, C., Gao, X., Gao, J., Liu, J., and Dolan, B. 2019. Dialogpt: Large-scale generative pre-training for conversational response generation. In Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv:1911.00536
Zhao, X., Wu, W., Tao, C., Xu, C., Zhao, D., and Yan, R. 2020. Low-resource knowledge-grounded dialogue generation. In Low-resource knowledge-grounded dialogue generation. arXiv:2002.10348
Zhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390
Zhou, H., Young, T., Huang, M., Zhao, H., Xu, J., and Zhu, X. 2018. Commonsense knowledge aware conversation generation with graph attention. In IJCAI. pp. 4623--4629
Zhou, K., Prabhumoye, S., and Black, A. W. 2018. A dataset for document grounded conversations. In A dataset for document grounded conversations. arXiv:1809.07358