Introduction

Pretrained language models have become crucial for achieving state-of-the-art performance in modern natural language processing. In particular, multilingual language models (Conneau and Lample, 2019;Conneau et al., 2020a;Doddapaneni et al., 2021) have attracted considerable attention particularly due to their utility in cross-lingual transfer.

In zero-shot cross-lingual transfer, a pretrained encoder is fine-tuned in a single resource-rich language (typically English), and then evaluated on other languages never seen during fine-tuning. A key to solving cross-lingual transfer tasks is to obtain representations that generalize well across languages. Several studies aim to improve multilingual models with cross-lingual supervision such as bilingual word dictionaries (Conneau et al., 2020b) or parallel sentences (Conneau and Lample, 2019).

Another source of such information is the crosslingual mappings of Wikipedia entities (articles). Wikipedia entities are aligned across languages via inter-language links and the text contains numerous entity annotations (hyperlinks). With these data, models can learn cross-lingual correspondence such as the words Tokyo and 東京 refers to the same entity. Wikipedia entity annotations have been shown to provide rich cross-lingual alignment information to improve multilingual language models (Iacer Calixto and Pasini, 2021;Xiaoze Jian and Duan, 2021). However, previous studies only incorporate entity information through an auxiliary loss function during pretraining, and the models do not explicitly have entity representations used for downstream tasks.

In this study, we investigate the effectiveness of entity representations in multilingual language models. Entity representations are known to enhance language models in mono-lingual settings (Zhang et al., 2019;Peters et al., 2019;Xiong et al., 2020;Yamada et al., 2020) presumably by introducing real-world knowledge. We argue that using entity representations facilitates cross-lingual transfer by providing languageindependent features. To this end, we present a multilingual extension of LUKE (Yamada et al., 2020). The model is trained with the multilingual masked language modeling (MLM) task as well as the masked entity prediction (MEP) task with Wikipedia entity embeddings.

We investigate two ways of using the entity representations in cross-lingual transfer tasks: (1) perform entity linking for the input text, and append the detected entity tokens to the input sequence. The entity tokens are expected to provide languageindependent features to the model. We evaluate this approach with cross-lingual question answering (QA) datasets: XQuAD (Artetxe et al., 2020) and MLQA ; (2) use the entity [MASK] token from the MEP task as a language-independent feature extractor. In the MEP task, word tokens in a mention span are associated with an entity [MASK] token, the contextualized representation of which is used to train the model to predict its original identity. Here, we apply similar input formulations to tasks involving mention-span classification, relation extraction (RE) and named entity recognition (NER): the attribute of a mention or a pair of mentions is predicted using their contextualized entity [MASK] feature. We evaluate this approach with the RELX (Köksal and Özgür, 2020) and CoNLL NER (Tjong Kim Sang, 2002;Tjong Kim Sang and De Meulder, 2003) datasets.

The experimental results show that these entitybased approaches consistently outperform wordbased baselines. Our analysis reveals that entity representations provide more language-agnostic features to solve the downstream tasks.

We also explore solving a multilingual zero-shot cloze prompt task  with the entity [MASK] token. Recent studies have shown that we can address various downstream tasks by querying a language model for blanks in prompts (Petroni et al., 2019;Cui et al., 2021). Typically, the answer tokens are predicted from the model's word-piece vocabulary but here we incorporate the prediction from the entity vocabulary queried by the entity [MASK] token. We evaluate our approach with the mLAMA dataset (Kassner et al., 2021) in various languages and show that using the entity [MASK] token reduces language bias and elicits correct factual knowledge more likely than using only the word [MASK] token.

 References: 
Artetxe, M., Ruder, S., and Yogatama, D. 2020. On the Cross-lingual Transferability of Monolingual Representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Cao, S., Kitaev, N., and Klein, D. 2020. Multilingual Alignment of Contextual Word Representations. In International Conference on Learning Representations.
Chi, Z., Dong, L., Wei, F., Yang, N., Singhal, S., Wang, W., Song, X., Mao, X., Huang, H., and Zhou, M. 2021. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online.
Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Conneau, A. and Lample, G. 2019. Crosslingual Language Model Pretraining. In Advances in Neural Information Processing Systems.
Conneau, A., Wu, S., Li, H., Zettlemoyer, L., and Stoyanov, V. 2020. Emerging Cross-lingual Structure in Pretrained Language Models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Cui, L., Wu, Y., Liu, J., Yang, S., and Zhang, Y. 2021. Template-Based Named Entity Recognition Using BART. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021.
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
Doddapaneni, S., Ramesh, G., Kunchukuttan, A., Kumar, P., and Khapra, M. M. 2021. A Primer on Pretrained Multilingual Language Models. In 2021. A Primer on Pretrained Multilingual Language Models. abs/2107.00676
Févry, T., Livio, B., Soares, N., Fitzgerald, E., Choi, T., and Kwiatkowski 2020. Entities as Experts: Sparse Memory Access with Entity Supervision. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.
Fujinuma, Y., Boyd-Graber, J., and Paul, M. J. 2019. A Resource-Free Evaluation Metric for Cross-Lingual Word Embeddings Based on Graph Modularity. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.
Raganato, A., Calixto, I., and Pasini, T. 2021. Wikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting Wikipedia Hyperlinks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics.
Mayhew Karthikeyan, S., K, Wang, Z., and Roth, D. 2020. Cross-Lingual Ability of Multilingual BERT: An Empirical Study. In International Conference on Learning Representations.
Kassner, N., Dufter, P., and Schütze, H. 2021. Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume.
Köksal, A. and Özgür, A. 2020. The RELX Dataset and Matching the Multilingual Blanks for Cross-Lingual Relation Classification. In Findings of the Association for Computational Linguistics: EMNLP 2020.
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In International Conference on Learning Representations.
Lewis, P., Oguz, B., Rinott, R., Riedel, S., and Schwenk, H. 2020. MLQA: Evaluating Cross-lingual Extractive Question Answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. 2021. Pretrain, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. In ArXiv. abs/2107.13586
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. In RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv. abs/1907.11692
Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., and Zettlemoyer, L. 2020. Multilingual Denoising Pre-training for Neural Machine Translation. In Transactions of the Association for Computational Linguistics. pp. 726--742
Loshchilov, I. and Hutter, F. 2019. Decoupled Weight Decay Regularization. In International Conference on Learning Representations.
Ouyang, X., Wang, S., Pang, C., Sun, Y., Hao Tian, H., Wu, H., and Wang 2020. Erniem: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora. In ArXiv. abs/2012.15674
Peters, M. E., Neumann, M., Logan, R., Schwartz, R., Joshi, V., Singh, S., and Smith, N. A. 2019. Knowledge Enhanced Contextual Word Representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing.
Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Language Models as Knowledge Bases?. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing.
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.
Mohammad, G., Sohrab, M., and Miwa 2018. Deep Exhaustive Model for Nested Named Entity Recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.
ErikF and Sang, T.K. 2002. Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition. In COLING-02: The 6th Conference on Natural Language Learning.
Tjong, E. F., Sang, K., and De Meulder, F. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. 2017. Attention Is All You Need. In Advances in Neural Information Processing Systems.
Wang, X., Gao, T., Zhu, Z., Liu, Z., Li, J., and Tang, J. 2021. KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation. In Transactions of the Association for Computational Linguistics. pp. 176--194
Weizhu Chen Xiaoze Jian, Y., Liang, N., and Duan 2021. XLM-K: Improving Cross-Lingual Language Model Pre-Training with Multilingual Knowledge. In ArXiv. abs/2109.12573
Xiong, W., Du, J., Wang, W. Y., and Stoyanov, V. 2020. Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model. In International Conference on Learning Representations.
Yamada, I., Asai, A., Shindo, H., Takeda, H., and Matsumoto, Y. 2020. LUKE: Deep Contextualized Entity Representations with Entityaware Self-attention. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.
Zhang, D. and Wang, D. 2015. Relation Classification via Recurrent Neural Network. In ArXiv. abs/1508.01006
Zhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., and Liu, Q. 2019. ERNIE: Enhanced Language Representation with Informative Entities. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.