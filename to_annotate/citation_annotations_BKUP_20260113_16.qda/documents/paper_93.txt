Introduction

As a fundamental task in the natural language processing (NLP) field, unsupervised sentence representation learning (Kiros et al., 2015;Hill et al., 2016) aims to derive high-quality sentence representations that can benefit various downstream tasks, especially for low-resourced domains or computationally expensive tasks, e.g., zero-shot text semantic match (Qiao et al., 2016), large-scale semantic similarity comparison (Agirre et al., 2015), and document retrieval (Le and Mikolov, 2014).

As a widely used semantic representation approach, pre-trained language models (PLMs) (Devlin et al., 2019) have achieved remarkable performance on various NLP tasks. However, several studies have found that the original sentence representations derived by PLMs are not uniformly distributed with respect to directions, but instead occupy a narrow cone in the vector space (Ethayarajh, 2019), which largely limits their expressiveness. To address this issue, contrastive learning (Chen et al., 2020) has been adopted to refine PLM-derived sentence representations. It pulls semantically close neighbors together to improve the alignment, while pushing apart non-neighbors for the uniformity of the whole representation space. In the learning process, both positive and negative examples are involved in contrast with the original sentence. For positive examples, previous works apply data augmentation strategies (Yan et al., 2021) on the original sentence to generate highly similar variations. While, negative examples are commonly randomly sampled from the batch or training data (e.g., inbatch negatives (Gao et al., 2021)), due to the lack of ground-truth negatives.

Although such a negative sampling way is simple and convenient, it may cause sampling bias and affects the sentence representation learning. First, the sampled negatives are likely to be false negatives that are indeed semantically close to the original sentence. As shown in Figure 1, given a random input sentence, about half of in-batch negatives have a cosine similarity above 0.7 with the original sentence based on the SimCSE model (Gao et al., 2021). It may hurt the semantics of the sentence representations by simply pushing apart sampled negatives. Second, due to the anisotropy problem (Ethayarajh, 2019), the sampled negatives are from the narrow representation cone spanned by PLMs, which cannot fully reflect the overall semantics of the representation space. Hence, it is sub-optimal for learning the uniformity objective of sentence representations.

To address the above issues, we propose a debiased contrastive learning framework for unsupervised sentence representation learning. The core idea is to improve the random negative sampling strategy for alleviating the sampling bias problem. First, in our framework, we design an instance weighting method to punish the sampled false negatives during training. We incorporate a complementary model to evaluate the similarity score between each negative and the original sentence, and assign lower weight for negatives with a higher similarity score. In this way, we can detect semantically-close false negatives and further reduce their influence. Second, we randomly initialize new negatives based on random Gaussian noise to simulate sampling within the whole semantic space, and devise a gradient-based algorithm to optimize the noise-based negatives towards the most nonuniform points. By learning to contrast with the nonuniform noise-based negatives, we can extend the occupied space of sentence representations and improve the uniformity of the representation space.

To this end, we propose DCLR, a general framework towards Debiased Contrastive Learning of unsupervised sentence Representations. In our approach, we first initialize the noise-based negatives from a Gaussian distribution, and leverage a gradient-based algorithm to update the new negatives by considering the uniformity of the representation space. Then, we adopt the complementary model to produce the weights for the new negatives and randomly sampled negatives, where the false negatives will be punished. Finally, we augment the positive examples via dropout (Gao et al., 2021) and combine it with the negatives for contrastive learning. We demonstrate that our DCLR outperforms competitive baselines on semantic textual similarity (STS) tasks using BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019).

Our contributions are summarized as follows:
(1) To our knowledge, our approach is the first attempt to reduce the sampling bias in contrastive learning of unsupervised sentence representations.
(2) We propose DCLR, a debiased contrastive learning framework that utilizes an instance weighting method to punish false negatives and generates noise-based negatives to guarantee the uniformity of the whole representation space.
(3) Experimental results on seven semantic textual similarity tasks show the effectiveness of our framework.

 References: 
Agirre, E., Banea, C., Cardie, C., Cer, D. M., Diab, M. T., Gonzalez-Agirre, A., Guo, W., Lopez-Gazpio, I., Maritxalar, M., Mihalcea, R., Rigau, G., Uria, L., and Wiebe, J. 2015. Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability. In NAACL-HLT. pp. 252--263 10.18653/v1/s15-2045
Agirre, E., Banea, C., Cardie, C., Cer, D. M., Diab, M. T., Gonzalez-Agirre, A., Guo, W., Mihalcea, R., Rigau, G., and Wiebe, J. 2014. Semeval-2014 task 10: Multilingual semantic textual similarity. In COLING. pp. 81--91 10.3115/v1/s14-2010
Agirre, E., Banea, C., Cer, D. M., Diab, M. T., Gonzalez-Agirre, A., Mihalcea, R., Rigau, G., and Wiebe, J. 2016. Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In COLING. pp. 497--511 10.18653/v1/s16-1081
Agirre, E., Cer, D. M., Diab, M. T., and Gonzalez-Agirre, A. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In NAACL-HLT. pp. 385--393
Agirre, E., Cer, D. M., Diab, M. T., Gonzalez-Agirre, A., and Guo, W. 2013. *sem 2013 shared task: Semantic textual similarity. In *SEM. pp. 32--43
Samuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In EMNLP. pp. 632--642 10.18653/v1/d15-1075
Cer, D., Yang, Y., Kong, S., Hua, N., Limtiaco, N., St, R., John, N., Constant, M., Guajardo-Cespedes, S., Yuan, C., Tar, B., Strope, R., and Kurzweil 2018. Universal sentence encoder for english. In EMNLP. pp. 169--174 10.18653/v1/d18-2029
Daniel, M., Cer, M. T., Diab, E., Agirre, I., Lopez-Gazpio, L., and Specia 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In ACL. pp. 1--14 10.18653/v1/S17-2001
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E. 2020. A simple framework for contrastive learning of visual representations. In of Proceedings of Machine Learning Research. pp. 1597--1607
Conneau, A. and Kiela, D. 2018. Senteval: An evaluation toolkit for universal sentence representations. In LREC.
Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bordes, A. 2017. Supervised learning of universal sentence representations from natural language inference data. In EMNLP. pp. 670--680 10.18653/v1/d17-1070
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT. pp. 4171--4186 10.18653/v1/n19-1423
Ethayarajh, K. 2019. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and GPT-2 embeddings. In EMNLP-IJCNLP. pp. 55--65 10.18653/v1/D19-1006
Fang, H. and Xie, P. 2005. CERT: contrastive self-supervised learning for language understanding. CoRR, abs. In CERT: contrastive self-supervised learning for language understanding. CoRR, abs.
Gao, T., Yao, X., and Chen, D. 2021. Simcse: Simple contrastive learning of sentence embeddings. In EMNLP. pp. 6894--6910
Hadsell, R., Chopra, S., and Lecun, Y. 2006. Dimensionality reduction by learning an invariant mapping. In CVPR. pp. 1735--1742 10.1109/CVPR.2006.100
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. B. 2020. Momentum contrast for unsupervised visual representation learning. In CVPR. pp. 9726--9735 10.1109/CVPR42600.2020.00975
Hill, F., Cho, K., and Korhonen, A. 2016. Learning distributed representations of sentences from unlabelled data. In NAACL-HLT. pp. 1367--1377 10.18653/v1/n16-1162
Hinton, G. E., Vinyals, O., and Dean, J. 2015. Distilling the knowledge in a neural network. In Distilling the knowledge in a neural network. abs/1503.02531
Huang, J., Tang, D., Zhong, W., Lu, S., Shou, L., Gong, M., Jiang, D., and Duan, N. 2021. Whiteningbert: An easy unsupervised sentence embedding approach. In Whiteningbert: An easy unsupervised sentence embedding approach. abs/2104.01767
Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Zhao, T. 2020. SMART: robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization. In ACL. pp. 2177--2190 10.18653/v1/2020.acl-main.197
Kim, T., Kang Min Yoo, S., and Lee 2021. Self-guided contrastive learning for BERT sentence representations. In ACL. pp. 2528--2540 10.18653/v1/2021.acl-long.197
Diederik, P., Kingma, J., and Ba 2015. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization.
Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Urtasun, R., Torralba, A., and Fidler, S. 2015. Skip-thought vectors. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. pp. 3294--3302
Kurakin, A., Goodfellow, I. J., and Bengio, S. 2017. Adversarial examples in the physical world. In Adversarial examples in the physical world.
Quoc, V., Le, T., and Mikolov 2014. Distributed representations of sentences and documents. In ICML. pp. 1188--1196
Li, B., Zhou, H., He, J., Wang, M., Yang, Y., and Li, L. 2020. On the sentence embeddings from pre-trained language models. In EMNLP. pp. 9119--9130 10.18653/v1/2020.emnlp-main.733
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. 2018. Towards deep learning models resistant to adversarial attacks. In ICLR.
Marelli, M., Menini, S., Baroni, M., Bentivogli, L., Bernardi, R., and Zamparelli, R. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In LREC. pp. 216--223
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems. pp. 3111--3119
Miyato, T., Dai, A. M., and Goodfellow, I. J. 2017. Adversarial training methods for semisupervised text classification. In ICLR.
Miyato, T., Shin-Ichi Maeda, M., Koyama, S., and Ishii 2019. Virtual adversarial training: A regularization method for supervised and semisupervised learning. In IEEE Trans. Pattern Anal. Mach. Intell. pp. 1979--1993 10.1109/TPAMI.2018.2858821
Pennington, J., Socher, R., and Manning, C. D. 2014. Glove: Global vectors for word representation. In EMNLP. pp. 1532--1543 10.3115/v1/d14-1162
Qiao, R., Liu, L., Shen, C., Van Den, A., and Hengel 2016. Less is more: zero-shot learning from online textual documents with noise suppression. In CVPR. pp. 2249--2257
Qin, C., Martens, J., Gowal, S., Krishnan, D., Dvijotham, K., Fawzi, A., De, S., Stanforth, R., and Kohli, P. 2019. Adversarial robustness through local linearization. In NeurIPS. pp. 13824--13833
Reimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In EMNLP-IJCNLP. pp. 3980--3990 10.18653/v1/D19-1410
Su, J., Cao, J., Liu, W., and Ou, Y. 2021. Whitening sentence representations for better semantics and faster retrieval. In Whitening sentence representations for better semantics and faster retrieval. abs/2103.15316
Sun, H., Wang, R., Chen, K., Lu, X., Utiyama, M., Sumita, E., and Zhao, T. 2020. Robust unsupervised neural machine translation with adversarial denoising training. In COLING. pp. 4239--4250 10.18653/v1/2020.coling-main.374
Williams, A., Nangia, N., and SamuelR 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL-HLT. pp. 1112--1122 10.18653/v1/n18-1101
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Patrick Von Platen, C., Ma, Y., Jernite, J., Plu, C., Xu, T. L., Scao, S., Gugger, M., Drame, Q., Lhoest, A. M., and Rush 2020. Transformers: State-of-the-art natural language processing. In EMNLP -Demos. pp. 38--45 10.18653/v1/2020.emnlp-demos.6
Wu, Z., Wang, S., Gu, J., Khabsa, M., Sun, F., and Ma, H. 2012. CLEAR: contrastive learning for sentence representation. In CLEAR: contrastive learning for sentence representation.
Yan, Y., Li, R., Wang, S., Zhang, F., Wu, W., and Xu, W. 2021. Consert: A contrastive framework for self-supervised sentence representation transfer. In ACL/IJCNLP. pp. 5065--5075 10.18653/v1/2021.acl-long.393
Zhu, C., Cheng, Y., Gan, Z., Sun, S., Goldstein, T., and Liu, J. 2020. Freelb: Enhanced adversarial training for natural language understanding. In ICLR.