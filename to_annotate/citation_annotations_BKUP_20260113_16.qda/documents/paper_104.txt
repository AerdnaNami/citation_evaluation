For NLP tasks, adapters are introduced for the transformer architecture. A set of adapter parameters was added at each transformer layer, which is mostly bottleneck architectures Houlsby et al. (2019). By keeping the output dimension identical, they cause no change to the structure or parameters of the original model. Adapters quickly gained popularity in NLP with various applications. For multi-task learning (Caruana, 1997; Zhang and Yang, 2017; Liu et al., 2019b), a projected self-attention layer is proposed by Stickland and Murray (2019), while Bapna et al. (2019) proposed an additional layer norm suitable for machine translation. Besides the applications of adapters, researchers are also dedicated to improving their performance. Based on the architecture introduced by Houlsby et al. (2019), AdapterFusion (Pfeiffer et al., 2020a) leveraged knowledge from multiple tasks with a new two-stage learning algorithm. Despite the recent popularity of these methods, they still train a relatively large number of training parameters. Recently, studies start to focus on improving the parameter-efﬁciency of adapters. Diff-pruning (Guo et al., 2020) achieves parameter efﬁciency by adding a sparse, task-speciﬁc difference-vector to the ﬁxed original parameters. The vector is adaptively pruned during training with a differentiable approximation to the L0-norm penalty to encourage sparsity. Rücklé et al. (2020) introduced AdapterDrop, which has been recently integrated into AdapterHub (Pfeiffer et al., 2020b) by removing adapters from lower transformer layers during training and inference, which can dynamically reduce the computational cost. Mahabadi et al. (2021) proposed Compacter, which improved the trade-off between performance and trainable parameters per task with low-rank optimization. On the other hand, without modifying the architecture of the PLM, BitFit (Ben Zaken et al., 2021) shows that ﬁne-tuning only the bias terms of a large PLM is also competitive with ﬁne-tuning the entire model. Fine-tuning only the bias terms can be considered as adding a task-speciﬁc shift to the token representation. BitFit is most similar to our work. While in BitFit, the shifts added to all the representations are exactly the same for all input tokens, in our work, the shifts are token-dependent.

 References: 
Ankur Bapna, Naveen Arivazhagan, and Orhan Firat.
2019. Simple, scalable adaptation for neural machine translation. arXiv preprint arXiv:1909.08478. Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning for transformer-based masked languagemodels. arXiv e-prints, pages arXiv–2106. Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The ﬁfth pascal recognizing textual entailment challenge. In TAC. Rich Caruana. 1997. Multitask learning. Machine learning, 28(1):41–75. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055. Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. 2018. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. arXiv preprint arXiv:1805.01070. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and William B Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1–9. Demi Guo, Alexander M Rush, and Yoon Kim. 2020. Parameter-efﬁcient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efﬁcient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR. Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144. Ganesh Jawahar, Benoît Sagot, and Djamé Seddah.
2019. What does bert learn about the structure of language? In ACL 2019-57th Annual Meeting of the Association for Computational Linguistics. Ian T Jolliffe. 2002. Springer series in statistics. Principal component analysis, 29. Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of bert. arXiv preprint arXiv:1908.08593. Nelson F Liu, Matt Gardner, Yonatan Belinkov, Matthew E Peters, and Noah A Smith. 2019a. Linguistic knowledge and transferability of contextual representations. arXiv preprint arXiv:1903.08855. Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019b. Multi-task deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019c. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Christos Louizos, Max Welling, and Diederik P Kingma. 2017. Learning sparse neural networks through l_0 regularization. arXiv preprint arXiv:1712.01312. Chris J Maddison, Andriy Mnih, and Yee Whye Teh.
2016. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712. Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. 2021. Compacter: Efﬁcient lowrank hypercomplex adapter layers. arXiv preprint arXiv:2106.04647. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2020a. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247. Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vuli ´c, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. 2020b. Adapterhub: A framework for adapting transformers. arXiv preprint arXiv:2007.07779. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250. Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. 2020. Adapterdrop: On the efﬁ- ciency of adapters in transformers. arXiv preprint arXiv:2010.11918. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642. Asa Cooper Stickland and Iain Murray. 2019. Bert and pals: Projected attention layers for efﬁcient adaptation in multi-task learning. In International Conference on Machine Learning, pages 5986–5995. PMLR. Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. Bert rediscovers the classical nlp pipeline. arXiv preprint arXiv:1905.05950. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461. Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625–641. Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771. Yu Zhang and Qiang Yang. 2017. A survey on multitask learning. arXiv preprint arXiv:1707.08114.
A Appendix A.1 Training Details We train our model on Pytorch. The training details are shown in Table A. In addition, the bottleneck of Adapters (Houlsby et al., 2019) and is 32. A.2 L0-norm regularization in AdapterBias In Table B, we report the remain parameter of utilizing L0-norm regularization compared with the original AdapterBias. BERT-base (BB) and BERT- large (BL) are used as PLMs. A.3 The direction of representation shifts in different tasks Different from BitFit (Ben Zaken et al., 2021), where all the representation shifts are identical within one task, AdapterBias produces different weights for the shift based on each token. In this section, we compare the transformed tokens in AdapterBias and BitFit. We utilize PCA (Jolliffe, 2002) to reduce the dimension of the vectors. In Figure A, we input ﬁve sentences from the evaluation set of SST-2. We experiment on the last transformer layer since it has the most obvious shifts compared to the previous layers. ’0’ with lighter color indicates the representation before shifting, which is the output of the ﬁrst layer normalization. ’1’ with darker color is the shifted representation, which is the output of the second layer normalization. The color red represents positive sentences, and blue are the negative ones. The result shows that BitFit shifts all tokens towards the same direction regardless of the groundtruth label. On the other hand, AdapterBias discerns the label of the sentences and thus shifts the tokens of different sentences toward different directions. Figure A: We utilize PCA (Jolliffe, 2002) to visualize the shifting difference between Bitﬁt (Ben Zaken et al., 2021) and AdapterBias on SST-2 validation set. ’0’ with light color means the embedding before shifting. ’1’ with dark color means the embedding after shifting. The color red represents positive sentences, and blue represents negative sentences. CoLA SST-2 MRPC QNLI RTE STS-B MNLI-m MNLI-mm QQP Max_len 128 128 128 512 350 512 128 128 350 Batchsize 32 32 32 16 32 16 32 32 32 Learning rate 10−3 10−3 10−3 10−4 4 ×10−4 10−3 4 ×10−4 4 ×10−4 4 ×10−4 Epoch 20 10 10 10 20 20 10 10 10 Table A: Our training details of GLUE benchmark(Wang et al., 2018). Method CoLA SST-2 MRPC QNLI RTE STS-B MNLI-m MNLI-mm QQP BB AdapterBias (L0) 26.2% 82.0% 83.1% 82.3% 81.0% 83.0% 83.2% 83.3% 83.4% BL AdapterBias (L0) 83.2% 83.0% 83.3% 83.7% 83.2% 83.2% 83.4% 83.7% 83.6% Table B: Percentage of remaining parameters compared with the original parameters of the linear layer (Lα). Here we experiment with two models: BERT-base (BB) and BERT-large (BL). The setting follows by Table 1. Figure B: Word cloud of SST-2 in layer 0 to layer 6. Figure C: Word cloud of SST-2 in layer 7 to layer 12. Figure D: Word cloud of CoLA in layer 0 to layer 6. Figure E: Word cloud of CoLA in layer 7 to layer 12.