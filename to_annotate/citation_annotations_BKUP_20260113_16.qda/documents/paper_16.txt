Related Work

To facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al., 2018). Datasets for long docu-ments include Sharma et al. (2019), Cohan et al. (2018), andFisas et al. (2016). In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely structurecontrollable text generation.

Researchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AM-PERE  for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs. Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation. There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.

A wide range of control perspectives has been explored in controllable generation, including style control (e.g., sentiments (Duan et al., 2020), politeness (Madaan et al., 2020), formality , domains (Takeno et al., 2017) and persona ) and content control (e.g., length (Duan et al., 2020), entities (Fan et al., 2018a), and keywords (Tang et al., 2019)). Our structure-controlled generation differs from these works as we control the high-level output structure, rather than the specific styles or the surface details of which keywords to include in the generated output. Our task also differs from content planning (Reiter and Dale, 1997;Shao et al., 2019;, which involves explicitly selecting and arranging the input content. Instead, we provide the model with the high-level control labels, and let the model decide on its own the relevant styles and contents.

 References: 
Bhatia, C. and Pradhan, T. 2020. Metagen: An academic meta-review generation system. In Proceedings of ACM-SIGIR.
Carbonell, J. and Goldstein, J. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of ACM-SIGIR.
Cheng, L., Bing, L., Yu, Q., Lu, W., and Si, L. 2020. Argument pair extraction from peer review and rebuttal via multi-task learning. In Proceedings of EMNLP.
Cohan, A., Dernoncourt, F., Doo, S., Kim, T., Bui, S., Kim, W., Chang, N., and Goharian 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of NAACL.
Duan, Y., Xu, C., Pei, J., Han, J., and Li, C. 2020. Pre-train and plug-in: Flexible conditional text generation with variational autoencoders. In Proceedings of the ACL.
Erkan, G. and Dragomir R Radev 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. In Artificial Intelligence Research.
Richard Fabbri, A., Li, I., She, T., Li, S., and Radev, D. 2019. Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of ACL.
Fan, A., Grangier, D., and Auli, M. 2018. Controllable abstractive summarization. In Proceedings of WNGT.
Fan, A., Lewis, M., and Dauphin, Y. 2018. Hierarchical neural story generation. In Proceedings of ACL.
Fisas, B. and Ronzano, F. 2016. A multi-layered annotated corpus of scientific papers. In Proceedings of LREC.
Gardent, C., Shimorina, A., Narayan, S., and Perez-Beltrachini, L. 2017. The webnlg challenge: Generating text from rdf data. In Proceedings of INLG.
Grusky, M., Naaman, M., and Artzi, Y. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of NAACL.
Moritz Hermann, K., Kociskỳ, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Proceedings of NIPS.
Hochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Computation.
Hua, X., Nikolov, M., Badugu, N., and Wang, L. 2019. Argument mining for understanding peer reviews. In Proceedings of NAACL.
Hua, X. and Wang, L. 2019. Sentence-level content planning and style specification for neural text generation. In Proceedings of EMNLP-IJCNLP.
Kang, D., Ammar, W., Dalvi, B., Van Zuylen, M., Kohlmeier, S., Hovy, E., and Schwartz, R. 2018. A dataset of peer reviews (peerread): Collection, insights and nlp applications. In Proceedings of NAACL.
2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT.
Diederik, P., Kingma, J., and Ba 2014. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization.
Kryściński, W., Paulus, R., Xiong, C., and Socher, R. 2018. Improving abstraction in text summarization. In Proceedings of EMNLP.
Lafferty, J., Mccallum, A., and Pereira, F.C. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML.
Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., and Dyer, C. 2016. Neural architectures for named entity recognition. In Proceedings of NAACL.
Lauscher, A., Glavaš, G., and Eckert, K. 2018. Arguminsci: A tool for analyzing argumentation and rhetorical aspects in scientific writing. In Proceedings of ACL.
Lewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of ACL.
Liakata, M., Teufel, S., Siddharthan, A., and Batchelor, C. 2010. Corpora for the conceptualisation and zoning of scientific papers. In Proceedings of LREC.
Liao, Y., Bing, L., Li, P., Shi, S., Lam, W., and Zhang, T. 2018. QuaSE: Sequence editing under quantifiable guidance. In Proceedings of EMNLP.
Lin, C. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.
Liu, Y. and Lapata, M. 2019. Hierarchical transformers for multi-document summarization. In Proceedings of ACL.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach.
Madaan, A., Setlur, A., Parekh, T., Póczos, B., Neubig, G., Yang, Y., Salakhutdinov, R., Black, A. W., and Prabhumoye, S. 2020. Politeness transfer: A tag and generate approach. In Proceedings of ACL.
Mihalcea, R. and Tarau, P. 2004. Textrank: Bringing order into text. In Proceedings of EMNLP.
Nallapati, R., Zhou, B., Cicero Dos Santos, Ç., Gulçehre, B., and Xiang 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proceedings of SIGNLL.
Napoles, C., Matthew R Gormley, B., and Van Durme 2012. Annotated gigaword. In Proceedings of AKBC-WEKEX.
Narayan, S., Shay, B., Cohen, M., and Lapata 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of EMNLP.
Over, P. and Yen, J. 2004. An introduction to duc-2004. In Proceedings of DUC.
Owczarzak, K. and Dang, H. T. 2011. Overview of the tac 2011 summarization track: Guided task and aesop task. In Proceedings of TAC.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In Journal of Machine Learning Research.
La Ramshaw 1995. Text chunking using transformation-based learning. In Proceedings of Third Workshop on Very Large Corpora.
Ratinov, L. and Roth, D. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of CoNLL.
Reimers, N. and Gurevych, I. 2019. Sentencebert: Sentence embeddings using siamese bertnetworks. In Proceedings of EMNLP.
Reiter, E. and Dale, R. 1997. Building applied natural language generation systems. In Natural Language Engineering.
Sandhaus, E. 2008. The new york times annotated corpus. Linguistic Data Consortium. In The new york times annotated corpus. Linguistic Data Consortium.
Shang, M., Li, P., Fu, Z., Bing, L., Zhao, D., Shi, S., and Yan, R. 2019. Semi-supervised text style transfer: Cross projection in latent space. In Proceedings of EMNLP-IJCNLP.
Shao, Z., Huang, M., Wen, J., Xu, W., and Zhu, X. 2019. Long and diverse text generation with planning-based hierarchical variational model. In Proceedings of EMNLP-IJCNLP.
Sharma, E., Li, C., and Wang, L. 2019. Bigpatent: A large-scale dataset for abstractive and coherent summarization. In Proceedings of ACL.
Shen, T., Lei, T., Barzilay, R., and Jaakkola, T. S. 2017. Style transfer from non-parallel text by cross-alignment. In Proceedings of NIPS.
Takeno, S., Nagata, M., and Yamamoto, K. 2017. Controlling target features in neural machine translation via prefix constraints. In Proceedings of WAT.
Tan, J., Wan, X., and Xiao, J. 2017. Abstractive document summarization with a graphbased attentional neural model. In Proceedings of ACL.
Tang, J., Zhao, T., Xiong, C., Liang, X., Xing, E., and Hu, Z. 2019. Targetguided open-domain conversation. In Proceedings of ACL.
Teufel, S., Carletta, J., and Moens, M. 1999. An annotation scheme for discourse-level argumentation in research articles. In Proceedings of EACL.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. 2017. Attention is all you need. In Proceedings of NIPS.
Wang, Y., Wu, Y., Mou, L., Li, Z., and Chao, W. 2019. Harnessing pre-trained neural networks with rules for formality style transfer. In Proceedings of EMNLP-IJCNLP.
Wolf, T., Chaumond, J., Debut, L., Sanh, V., Delangue, C., Moi, A., Cistac, P., Funtowicz, M., Davison, J., and Shleifer, S. 2020. Transformers: State-of-theart natural language processing. In Proceedings of EMNLP.
Xing, X., Fan, X., and Wan, X. 2020. Automatic generation of citation texts in scholarly papers: A pilot study. In Proceedings of ACL.
Zhang, J., Zhao, Y., Saleh, M., and Liu, P. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of ICML.
Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., and Weston, J. 2018. Personalizing dialogue agents: I have a dog, do you have pets too?. In Proceedings of ACL.