Introduction

Relation extraction is a fundamental problem in natural language processing, which aims to identify the semantic relation between a pair of entities mentioned in the text. Recent progress in supervised relation extraction has achieved great successes (Zeng et al., 2014;Zhou et al., 2016;Soares et al., 2019), but these approaches usually require large-scale labeled data. While in practice, human annotation is time-consuming and labor-intensive. 1 We will release our code after blind review. : The Doctor tries to restore the universe with the help of River and the alternative universe versions of his companions Amy Pond(Karen Gillan) and Rory Williams(Arthur Darvill).  To alleviate the human annotation efforts in relation extraction, some recent studies use distant supervision to generate labeled data for training (Mintz et al., 2009;Lin et al., 2016). However, in the real-world setting, the relations of instances are not always included in the training data, and existing supervised methods cannot well recognize unobserved relations due to weak generalization ability.

To address the aforementioned limitations, zeroshot relation extraction has been proposed to extract relational facts where the target relations cannot be observed at the training stage. The challenge of zero-shot relation extraction models is how to learn effective representations based on seen relations at the training stage and well generalize to unseen relations at the test stage. Two studies (Levy et al., 2017;Obamuyide and Vlachos, 2018) treat zero-shot relation extraction as a different task (i.e., question answering and textual entailment), but they both need human annotation auxiliary in-formation for input, i.e., pre-defining question templates and relation descriptions. ZS-BERT (Chen and Li, 2021) predicts unseen relations with attribute representation learning. Despite promising improvements on directly predicting unseen relations, ZS-BERT still makes wrong predictions due to similar relations or similar entities. The same problem arises in supervised methods under the zero-shot settings.

As shown in Figure 1, there are two types of similar errors: Similar Relations and Similar Entities. For similar relations (see Z 1 and Z 2 ), existing methods predict wrongly results because the unseen relations possess similar semantics and data points belong to two relations in the representation space are overlapped. For similar entities (i.e., 2014 contest and 2002 Contest), since entities are the context of relation and relation representations are derived from entities, the relation representations containing similar entities are close (see f (Z 3 ) and f (Z 4 )) and baselines wrongly consider f (Z 4 ) belongs to follows in the representation space, even if two unseen relations are not related. Recently, Instancewise Contrastive Learning (Instance-CL) (He et al., 2020;Yan et al., 2021;Gao et al., 2021; has achieved remarkable success in representation learning. Instance-CL is used to learn an effective representation by pulling together the instances from the same class, while pushing apart instances from different classes. Inspired by Instance-CL, we attempt to use Instance-CL on seen relations to learn the difference between similar relations and the divergence of relation representations derived from similar entities.

In this paper, we propose a novel Relation Contrastive Learning framework (RCL) to solve the above-mentioned problems. Figure 1 depicts the overview of the proposed model, which consists of four steps: (i) The input for RCL is a batch of sentences containing the pair of target entities and each sentence is sent into input sentence encoder to generate the contextual sentence embeddings 2 . (ii) Taking the sentence embeddings as input, relation augmentation layer is designed to obtain the relation representations f (X i ) and their corresponding augmented views f ( Xi ). (iii) By jointly optimizing a contrastive loss and a relation classification loss on seen relations, RCL can learn subtle difference between instances and achieve better separation between relations in the representation space simultaneously to obtain an effective projection function f . (iv) With the learned f , the whole test set Z can be projected for unseen relation representations in the representation space and zero-shot prediction is performed on unseen relation representations by K-Means.

To summarize, the major contributions of our work are as follows: (i) We propose a novel framework based on contrastive learning for zero-shot relation extraction. It effectively mitigates two types of similar problems: similar relations and similar entities by learning representations jointly optimized with contrastive loss and classification loss. (ii) We explore various data augmentation strategies in relation augmentation to minimize semantic impact for contrastive instance learning and experimental results show dropout noise as minimal data augmentation can help RCL learn the difference between similar instances better. (iii) We conduct experiments on two well-known datasets. Experimental results show that RCL can advance stateof-the-art performance by a large margin. Besides, even if the number of seen relations is insufficient, RCL can also achieve comparable results with the model trained on the full training set.

 References: 
Bagga, A. and Baldwin, B. 1998. Entitybased cross-document coreferencing using the vector space model. In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics. pp. 79--85
Chen, C. and Li, C. 2021. ZS-BERT: Towards zero-shot relation extraction with attribute representation learning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3470--3479 10.18653/v1/2021.naacl-main.272
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning. pp. 1597--1607
Chen, T., Sun, Y., Shi, Y., and Hong, L. 2017. On sampling strategies for neural networkbased collaborative filtering. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 767--776
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186
Gao, T., Yao, X., and Chen, D. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 6894--6910
Han, X., Zhu, H., Yu, P., Wang, Z., Yao, Y., Liu, Z., and Sun, M. 2018. Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 4803--4809
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. 2020. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9729--9738
Hendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, Ó., Séaghdha, S., Padó, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38
Geoffrey E Hinton, N., Srivastava, A., Krizhevsky, I., Sutskever, R.R., and Salakhutdinov 2012. Improving neural networks by preventing coadaptation of feature detectors. In Improving neural networks by preventing coadaptation of feature detectors. arXiv:1207.0580
Hu, X., Wen, L., Xu, Y., Zhang, C., and Philip, S.Y. 2020. Selfore: Self-supervised relational feature learning for open relation extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3673--3682
Diederik, P., Kingma, J., and Ba 2014. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization. arXiv:1412.6980
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations.
Levy, O., Seo, M., Choi, E., and Zettlemoyer, L. 2017. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning. pp. 333--342 10.18653/v1/K17-1034
Lin, Y., Shen, S., Liu, Z., Luan, H., and Sun, M. 2016. Neural relation extraction with selective attention over instances. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pp. 2124--2133 10.18653/v1/P16-1200
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692
Mintz, M., Bills, S., Snow, R., and Jurafsky, D. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. pp. 1003--1011
Obamuyide, A. and Vlachos, A. 2018. Zeroshot relation classification as textual entailment. In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER). pp. 72--78 10.18653/v1/W18-5511
Qian, L., Zhou, G., Kong, F., Zhu, Q., and Qian, P. 2008. Exploiting constituent dependencies for tree kernel-based semantic relation extraction. In Proceedings of the 22nd International Conference on Computational Linguistics. pp. 697--704
Saha, S. 2018. Open information extraction from conjunctive sentences. In Proceedings of the 27th International Conference on Computational Linguistics. pp. 2288--2299
Sculley, D. 2010. Web-scale k-means clustering. In Proceedings of the 19th international conference on World wide web. pp. 1177--1178
Shen, D., Zheng, M., Shen, Y., Qu, Y., and Chen, W. 2020. A simple but toughto-beat data augmentation approach for natural language understanding and generation. In A simple but toughto-beat data augmentation approach for natural language understanding and generation. arXiv:2009.13818
Simon, É., Guigue, V., and Piwowarski, B. 2019. Unsupervised information extraction: Regularizing discriminative approaches with relation distribution losses. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 1378--1387 10.18653/v1/P19-1133
Livio Baldini, Soares, N., Fitzgerald, J., Ling, T., and Kwiatkowski 2019. Matching the blanks: Distributional similarity for relation learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 2895--2905
Stanovsky, G., Michael, J., Zettlemoyer, L., and Dagan, I. 2018. Supervised open information extraction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 885--895 10.18653/v1/N18-1081
Van Der Maaten, L. and Hinton, G. 2008. Visualizing data using t-sne. In Journal of machine learning research. pp. 9
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Patrick Von Platen, C., Ma, Y., Jernite, J., Plu, C., Xu, T. L., Scao, S., Gugger, M., and Drame 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. pp. 38--45 10.18653/v1/2020.emnlp-demos.6
Wu, R., Yao, Y., Han, X., Xie, R., Liu, Z., Lin, F., Lin, L., and Sun, M. 2019. Open relation extraction: Relational knowledge transfer from supervised data to unsupervised data. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 219--228 10.18653/v1/D19-1021
Wu, S. and He, Y. 2019. Enriching pretrained language model with entity information for relation classification. In Proceedings of the 28th ACM international conference on information and knowledge management. pp. 2361--2364
Yan, Y., Li, R., Wang, S., Zhang, F., Wu, W., and Xu, W. 2021. ConSERT: A contrastive framework for self-supervised sentence representation transfer. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 5065--5075 10.18653/v1/2021.acl-long.393
Yu, D., Huang, L., and Ji, H. 2017. Open relation extraction and grounding. In Proceedings of the Eighth International Joint Conference on Natural Language Processing. pp. 854--864
Zeng, D., Liu, K., Lai, S., Zhou, G., and Zhao, J. 2014. Relation classification via convolutional deep neural network. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. pp. 2335--2344
Zhang, D., Nan, F., Wei, X., Li, S., Zhu, H., Mckeown, K., Nallapati, R., Arnold, A. O., and Xiang, B. 2021. Supporting clustering with contrastive learning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 5419--5430 10.18653/v1/2021.naacl-main.427
Zhang, T., Ramakrishnan, R., and Livny, M. 1996. Birch: an efficient data clustering method for very large databases. In ACM sigmod record. pp. 103--114
Zhou, P., Shi, W., Tian, J., Qi, Z., Li, B., Hao, H., and Xu, B. 2016. Attention-based bidirectional long short-term memory networks for relation classification. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pp. 207--212 10.18653/v1/P16-2034
Later in the game, she joins Snake in rescuing Dr Marv. In Later in the game, she joins Snake in rescuing Dr Marv.
2014. Vienna hosted the Eurovision Song Contest following Austria's victory in the. In Vienna hosted the Eurovision Song Contest following Austria's victory in the.
Group1: the song was succeeded as Romanian representative at the 2002 Contest by Monica Anghel & Marcel Pavel with "Tell Me Why. In Group1: the song was succeeded as Romanian representative at the 2002 Contest by Monica Anghel & Marcel Pavel with "Tell Me Why.
1939. the Second World War began with the German Invasion of Poland, and two days later the United Kingdom declared. In Group2: On.
Group2: During the War of 1812, Rolette, like many other French-Canadian Fur Traders in the Old Northwest. In Group2: During the War of 1812, Rolette, like many other French-Canadian Fur Traders in the Old Northwest.