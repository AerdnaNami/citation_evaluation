Related Work

Vision-language few-shot learning. Recently, several few-shot learners on vision-language tasks were proposed including GPT (Radford et al., 2019;Brown et al., 2020), Frozen (Tsimpoukelli et al., 2021), PICa , and SimVLM . Frozen (Tsimpoukelli et al., 2021) is a large language model based on GPT-2 (Radford et al., 2019), and is transformed into a multimodal few-shot learner by extending the soft prompting to incorporate a set of images and text. Their approach shows the fewshot capability on visual question answering and image classification tasks. Similarly, PICa  uses GPT-3 (Brown et al., 2020) to solve VQA tasks in a few-shot manner by providing a few in-context VQA examples. It converts images into textual descriptions so that GPT-3 can understand the images. SimVLM  is trained with prefix language modeling on weakly-supervised datasets. It demonstrates its effectiveness on a zero-shot captioning task. While these models achieve improvement on few-shot tasks, they are impractical to use in real-world applications due to their model sizes.

Language model prompting. Providing prompts or task descriptions play an vital role in improving pre-trained language models in many tasks Radford et al., 2021;Schick and Schütze, 2020a,b;Brown et al., 2020). Among them, GPT models (Radford et al., 2019;Brown et al., 2020) achieved great success in prompting or task demonstrations in NLP tasks. In light of this direction, prompt-based approaches improve small pre-trained models in few-shot text classification tasks Schick and Schütze, 2020a,b). CLIP (Radford et al., 2021) also explores prompt templates for image classification which affect zero-shot performance. We follow these core ideas so we aim to improve zero-shot and few-shot performance using prompts in visionlanguage tasks. We pretrain FEWVLM with masked language modeling (MaskedLM) and prefix language modeling (Pre-fixLM).

 References: 
Agrawal, H., Desai, K., Wang, Y., Chen, X., Jain, R., Johnson, M., Batra, D., Parikh, D., Lee, S., and Anderson, P. 2019. nocaps: novel object captioning at scale. In Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 8948--8957
Anderson, P., Fernando, B., Johnson, M., and Gould, S. 2016. Spice: Semantic propositional image caption evaluation. In European conference on computer vision. pp. 382--398
Tom B Brown, B., Mann, N., Ryder, M., Subbiah, J., Kaplan, P., Dhariwal, A., Neelakantan, P., Shyam, G., and Sastry arXiv:2005.14165
Chen, X., Fang, H., Lin, T., Vedantam, R., Gupta, S., Dollár, P., and Zitnick, C.L. 2015. Microsoft coco captions: Data collection and evaluation server. In Microsoft coco captions: Data collection and evaluation server. arXiv:1504.00325
Chen, Y., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z., Cheng, Y., and Liu, J. 2019. Uniter: Learning universal image-text representations. In Uniter: Learning universal image-text representations.
Cho, J., Lei, J., Tan, H., and Bansal, M. 2021. Unifying vision-and-language tasks via text generation. In Unifying vision-and-language tasks via text generation. arXiv:2102.02779
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805
Gao, T., Fisch, A., and Chen, D. 2020. Making pre-trained language models better few-shot learners. In Making pre-trained language models better few-shot learners. arXiv:2012.15723
Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6904--6913
Drew, A., Hudson, C.D., and Manning 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 6700--6709
Karpathy, A. and Fei-Fei, L. 2015. Deep visualsemantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3128--3137
Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L., and Shamma, D. A. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. In International journal of computer vision. pp. 32--73
Li, K., Zhang, Y., Li, K., and Fu, Y. 2020. Adversarial feature hallucination networks for fewshot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13470--13479
Li, L.H., Yatskar, M., Yin, D., Hsieh, C., and Chang, K. 2019. Visualbert: A simple and performant baseline for vision and language. In Visualbert: A simple and performant baseline for vision and language. arXiv:1908.03557
Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong, L., and Wei, F. 2020. Oscar: Objectsemantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision. pp. 121--137
Lin, T., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C.L. 2014. Microsoft coco: Common objects in context. In European conference on computer vision. pp. 740--755
Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. 2019. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3195--3204
Perez, E., Kiela, D., and Cho, K. 2021. True few-shot learning with language models. In True few-shot learning with language models. arXiv:2105.11447
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., and Clark, J. 2021. Learning transferable visual models from natural language supervision. In Learning transferable visual models from natural language supervision. arXiv:2103.00020
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In OpenAI blog. pp. 9
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683
Shaoqing Ren, K., He, R., Girshick, J., and Sun 2015. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems. pp. 91--99
Schick, T. and Schütze, H. 2020. Exploiting cloze questions for few shot text classification and natural language inference. In Exploiting cloze questions for few shot text classification and natural language inference. arXiv:2001.07676
Schick, T. and Schütze, H. 2020. It's not just size that matters: Small language models are also few-shot learners. In It's not just size that matters: Small language models are also few-shot learners. arXiv:2009.07118
Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., and Dai, J. 2019. Vl-bert: Pretraining of generic visual-linguistic representations. In Vl-bert: Pretraining of generic visual-linguistic representations. arXiv:1908.08530
Tan, H. and Bansal, M. 2019. Lxmert: Learning cross-modality encoder representations from transformers. In Lxmert: Learning cross-modality encoder representations from transformers. arXiv:1908.07490
Tsimpoukelli, M., Menick, J., Cabi, S., Sm Eslami, O., Vinyals, F., and Hill 2021. Multimodal few-shot learning with frozen language models. In Multimodal few-shot learning with frozen language models. arXiv:2106.13884
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. 2017. Attention is all you need. In Advances in neural information processing systems. pp. 5998--6008
Vedantam, R., Zitnick, L., and Parikh, D. 2015. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4566--4575
Vinyals, O., Blundell, C., Lillicrap, T., and Wierstra, D. 2016. Matching networks for one shot learning. In Advances in neural information processing systems. pp. 3630--3638
Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., and Cao, Y. 2021. Simvlm: Simple visual language model pretraining with weak supervision. In Simvlm: Simple visual language model pretraining with weak supervision. arXiv:2108.10904
Yang, Z., Gan, Z., Wang, J., Hu, X., Lu, Y., Liu, Z., and Wang, L. 2021. An empirical study of gpt-3 for few-shot knowledge-based vqa. In An empirical study of gpt-3 for few-shot knowledge-based vqa. arXiv:2109.05014
Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. In Transactions of the Association for Computational Linguistics. pp. 67--78
Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., and Gao, J. 2021. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5579--5588
Zhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J., and Gao, J. 2020. Unified vision-language pre-training for image captioning and vqa. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 13041--13049