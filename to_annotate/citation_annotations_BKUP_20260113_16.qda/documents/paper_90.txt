Related Work

Other works have observed limitations of the softmax layer when modelling infrequent classes for image classification (Kang et al., 2020) and rare words for MT (Nguyen and Chiang, 2018;Raunak et al., 2020). They show that normalising the magnitude of the softmax weight vectors improves predictions for infrequent classes. However, the motivation for weight normalisation is guided empirically. From the perspective of this work, weight normalisation provably prevents Stolen Probability from arising when a softmax layer has no bias term. For more details, see Section D in the Appendix.

 References: 
Barber, C., Dobkin, D., and Huhdanpaa, H. 1996. The quickhull algorithm for convex hulls. In ACM Trans. Math. Softw. pp. 469--483
Boyd, S., Stephen, P., Boyd, L., and Vandenberghe 2004. Convex optimization. Cambridge university press. In Convex optimization. Cambridge university press.
Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 8440--8451 10.18653/v1/2020.acl-main.747
Cover, T. M. 1967. The number of linearly inducible orderings of points in d-space*. In Siam Journal on Applied Mathematics. pp. 434--439
Demeter, D., Kimmel, G., and Downey, D. 2020. Stolen probability: A structural weakness of neural language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2191--2197 10.18653/v1/2020.acl-main.198
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423
Ganea, O., Gelly, S., Bécigneul, G., and Severyn, A. 2019. Breaking the softmax bottleneck via learnable monotonic pointwise nonlinearities. In ICML. pp. 2073--2082
Good, I. and Tideman, T. 1977. Stirling numbers and a geometric ,structure from voting theory. In Journal of Combinatorial Theory, Series A. pp. 34--45 10.1016/0097-3165(77)90077-2
Gurobi Optimization. 2021. Gurobi Optimizer Reference Manual. In Gurobi Optimization. 2021. Gurobi Optimizer Reference Manual.
Hess, S., Duivesteijn, W., and Mocanu, D.C. 1987. Softmax-based classification is k-means clustering: Formal proof, consequences for adversarial attacks, and improvement through centroid based tailoring. In ArXiv.
Geoffrey, E., Hinton, R., and Zemel 1994. Autoencoders, minimum description length and helmholtz free energy. In Advances in Neural Information Processing Systems.
Junczys-Dowmunt, M., Grundkiewicz, R., Dwojak, T., Hoang, H., Heafield, K., Neckermann, T., Seide, F., Germann, U., Fikri Aji, A., Bogoychev, N., André, F. T., Martins, A., and Birch 2018. Marian: Fast neural machine translation in C++. In Proceedings of ACL 2018, System Demonstrations. pp. 116--121 10.18653/v1/P18-4020
Kamiya, H. and Takemura, A. 2005. Characterization of rankings generated by linear discriminant analysis. In Journal of multivariate analysis. pp. 343--358
Kamiya, H., Takemura, A., and Terao, H. 2011. Ranking patterns of unfolding models of codimension one. In Advances in Applied Mathematics. pp. 379--400 10.1016/j.aam.2010.11.002
Kanai, S., Fujiwara, Y., Yamanaka, Y., and Adachi, S. 2018. Sigsoftmax: Reanalysis of the softmax bottleneck. In Advances in Neural Information Processing Systems.
Kang, B., Xie, S., Rohrbach, M., Yan, Z., and Gordo, A. Jiashi Feng, and Yannis Kalantidis. 2020. Decoupling representation and classifier for long-tailed recognition. In International Conference on Learning Representations.
Henry, A., Kautz, A., Sabharwal, B., and Selman 2009. Incomplete algorithms. In Handbook of Satisfiability.
Kim, Y. and Rush, A. M. 2016. Sequencelevel knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 1317--1327 10.18653/v1/D16-1139
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. In ArXiv. abs/2107.13586
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. abs/1907.11692
Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., and Zettlemoyer, L. 2020. Multilingual denoising pre-training for neural machine translation. In Transactions of the Association for Computational Linguistics. pp. 726--742 10.1162/tacl_a_00343
Merity, S., Shirish Keskar, N., and Socher, R. 2018. Regularizing and optimizing LSTM language models. In International Conference on Learning Representations.
Ng, N., Yee, K., Baevski, A., Ott, M., Auli, M., and Edunov, S. 2019. Facebook FAIR's WMT19 news translation task submission. In Proceedings of the Fourth Conference on Machine Translation. pp. 314--319 10.18653/v1/W19-5333
Nguyen, T. and Chiang, D. 2018. Improving lexical choice in neural machine translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 334--343 10.18653/v1/N18-1031
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models unsupervised multitask learners. In Language models unsupervised multitask learners.
Raunak, V., Dalmia, S., Gupta, V., and Metze, F. 2020. On long-tailed phenomena in neural machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 3088--3095 10.18653/v1/2020.findings-emnlp.276
Sennrich, R., Birch, A., Currey, A., Germann, U., Haddow, B., Heafield, K., Valerio Miceli, A., Barone, P., and Williams 2017. The University of Edinburgh's neural MT systems for WMT17. In Proceedings of the Second Conference on Machine Translation. pp. 389--399 10.18653/v1/W17-4739
Sennrich, R., Haddow, B., and Birch, A. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pp. 1715--1725 10.18653/v1/P16-1162
Smith, W. D. 2014. D-dimensional orderings and stirling numbers. In Online; accessed 05.
Stanley, R. P. 2004. An introduction to hyperplane arrangements. In Lecture notes, IAS/Park City Mathematics Institute.
Tiedemann, J. 2020. The Tatoeba Translation Challenge -Realistic data sets for low resource and multilingual MT. In Proceedings of the Fifth Conference on Machine Translation. pp. 1174--1182
Tiedemann, J. and Thottingal, S. 2020. OPUS-MT -building open translation services for the world. In Proceedings of the 22nd Annual Conference of the European Association for Machine Translation. pp. 479--480
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, I., and Polosukhin 2017. Attention is all you need. In Advances in Neural Information Processing Systems.
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Patrick Von Platen, C., Ma, Y., Jernite, J., Plu, C., Xu, T. L., Scao, S., Gugger, M., Drame, Q., Lhoest, A. M., and Rush 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. pp. 38--45
Yang, Z., Dai, Z., Salakhutdinov, R., and Cohen, W. W. 2018. Breaking the softmax bottleneck: A high-rank RNN language model. In International Conference on Learning Representations. pp. 3628800