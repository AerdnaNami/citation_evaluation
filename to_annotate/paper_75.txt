Introduction

Natural languages are highly contextual (Fodor, 2001): for a listener, recovering the speaker's intended meaning requires integrating information from different streams, such as grounding in perception (Pecher and Zwaan, 2005), shared world knowledge, and temporal reasoning (Wilson and Sperber, 1998). These processes, more generally, fall under the umbrella term of pragmatics (Grice, 1957). Despite recent progress in multimodal systems, it remains unclear to which extent they can handle settings where context plays a major role, such as in real-world communication.

To this end, we present a new challenge that requires multimodal models to leverage context to retrieve images from text. In particular, given a contextual description and a set of minimally contrastive candidate images, i.e. differing only in some details, the model has to retrieve the target image. In order to discriminate between similar images, human annotators naturally produce highly nuanced and grammatically complex descriptions. An example of our new challenging dataset, Image Retrieval from Contextual Descriptions (IMAGECODE), is shown in Figure 1.

During the data collection process, sets of similar images are selected among static pictures from Open Images (Kuznetsova et al., 2020) and (a larger portion) among video frames from diverse domains. Including both types of images allows for diversifying the dataset while representing different degrees of visual similarity within each set. Next, we crowdsource a contextual description of a target image (presented together with the rest of the set) that contains only differences relevant for retrieval. After a filtering phase involving human retrievers, we obtain a large-scale dataset with 94,020 images and 21,202 descriptions associated with image sets of size 10.

As a result of this annotation protocol, successfully completing the task requires models to integrate several kinds of context: i) the image set, as the descriptions only make sense in the context of several other images and are not suitable as stand-alone captions. In fact, aspects of the image that are very salient and that therefore would normally be emphasized are not useful in our proposed task. Instead, the focus of our descriptions are finegrained details that help discriminate between images (see Figure 1); ii) the speaker's intention. Due to their high degree of image similarity, contextual descriptions may be literally true for multiple images; however, once the speaker's intention is taken into account, the correct image can be determined by virtue of pragmatics (see Figure 2); iii) temporal sequences: for video frames temporal reasoning is also required to compare different moments of an unfolding event.

On our new dataset IMAGECODE, we benchmark a series of vision-and-language models that achieve state-of-the-art performance on other multimodal tasks, including both cross-encoders such as ViLBERT (Lu et al., 2019) and bi-encoders such as CLIP (Radford et al., 2021). We report several findings. First, accuracy on static images is vastly superior than on video frames. Therefore, the degree of similarity among the candidate images has an overwhelming impact on retrieval performance. Second, all state-of-the-art models generally struggle with image retrieval from contextual descriptions, whereas humans consistently achieve high accuracy.

Hence, we propose model variants capable of better taking context into account: i) once an image description pair is encoded, we refine this representation by attending to the other images in the set; ii) we augment image encodings with special temporal embeddings. Based on our results, models take advantage of this additional information fruitfully but only to a limited degree.

Because of its challenging nature, due to the minimally contrastive images and complex descriptions, we believe that IMAGECODE will help make visio-linguistic models more context-aware and sensitive to fine-grained details. The dataset and models would be publicly released with the camera-ready version.

 References: 
Andreas, J. and Klein, D. 2016. Reasoning about Pragmatics with Neural Listeners and Speakers. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 1173--1182 10.18653/v1/D16-1125
Bansal, A., Zhang, Y., and Chellappa, R. 2020. Visual question answering on image sets. In ECCV.
Bogin, B., Gupta, S., Gardner, M., and Berant, J. 2021. COVR: A test-bed for visually grounded compositional generalization with real images. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 9824--9846
Bugliarello, E., Cotterell, R., Okazaki, N., and Elliott, D. 2021. Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs. In Transactions of the Association for Computational Linguistics. pp. 978--994 10.1162/tacl_a_00408
Cohn-Gordon, R., Goodman, N., and Potts, C. 2018. Pragmatically informative image captioning with character-level inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 439--443 10.18653/v1/N18-2070
Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., José, M. F., Moura, D., Parikh, D., and Batra 2017. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 326--335
Das, P., Xu, C., Richard, F., Doell, J. J., and Corso 2013. A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching. In Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2634--2641
Harm De Vries, F., Strub, S., Chandar, O., Pietquin, H., Larochelle, A. C., and Courville 2017. Guesswhat?! visual object discovery through multi-modal dialogue. In Conference on Computer Vision and Pattern Recognition (CVPR).
Fodor, J. 2001. Language, thought and compositionality. Royal Institute of Philosophy Supplements. In Language, thought and compositionality. Royal Institute of Philosophy Supplements. pp. 227--242 10.1111/1468-0017.00153
Forbes, M., Kaeser-Chen, C., Sharma, P., and Belongie, S. 2019. Neural Naturalist: Generating Fine-Grained Image Comparisons. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 708--717 10.18653/v1/D19-1065
Geva, M., Goldberg, Y., and Berant, J. 2019. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 1161--1166 10.18653/v1/D19-1107
Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6904--6913
H Paul Grice 1957. Utterer's Meaning and Intentions. In The Philosophical Review. pp. 147--177
Lisa, A., Hendricks, A., and Nematzadeh 2021. Probing Image-Language Transformers for Verb Understanding. In Probing Image-Language Transformers for Verb Understanding. ArXiv: 2106.09141
Honnibal, M. and Montani, I. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. In 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.
Hosseini, A., Reddy, S., Bahdanau, D., Hjelm, D., Sordoni, A., and Courville, A. 2021. Understanding by understanding not: Modeling negation in language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1301--1312 10.18653/v1/2021.naacl-main.102
Hosseinzadeh, M. and Wang, Y. 2021. Image change captioning by learning from an auxiliary task. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2725--2734
Hu, H., Misra, I., and Van Der Maaten, L. 2019. Binary Image Selection (BISON): Interpretable Evaluation of Visual Grounding. In Binary Image Selection (BISON): Interpretable Evaluation of Visual Grounding. arXiv:1901.06595
Drew, A., Hudson, C.D., and Manning 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 6700--6709
Jhamtani, H. and Berg-Kirkpatrick, T. 2018. Learning to Describe Differences Between Pairs of Similar Images. In Learning to Describe Differences Between Pairs of Similar Images. ArXiv: 1808.10584
Jhamtani, H. and Berg-Kirkpatrick, T. 2018. Learning to describe differences between pairs of similar images. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 4024--4034 10.18653/v1/D18-1436
Kassner, N. and Schütze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7811--7818 10.18653/v1/2020.acl-main.698
Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., Duerig, T., and Ferrari, V. 2020. The open images dataset v4: Unified image classification, object detection. In The open images dataset v4: Unified image classification, object detection. 10.1007/s11263-020-01316-z
Li, J., Wong, Y., Zhao, Q., and Kankanhalli, M. S. 2019. Video storytelling: Textual summaries for events. In IEEE Transactions on Multimedia. pp. 554--565
Liu, F., Bugliarello, E., Edoardo, M., Ponti, S., Reddy, N., Collier, D., and Elliott 2021. Visually grounded reasoning across languages and cultures. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 10467--10485
Lu, J., Batra, D., Parikh, D., and Lee, S. 2019. ViLBERT: Pretraining Task. In ViLBERT: Pretraining Task.
Agnostic Visiolinguistic Representations for Visionand-Language Tasks. In Advances in Neural Information Processing Systems. pp. 32
Miech, A., Alayrac, J., Laptev, I., Sivic, J., and Zisserman, A. 2021. Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers. In Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers. arXiv:2103.16553[cs].ArXiv:2103.16553
Mittal, A., Anush, K., Moorthy, A. C., and Bovik 2012. No-reference image quality assessment in the spatial domain. In IEEE Transactions on image processing. pp. 4695--4708
Pecher, D. and Zwaan, R. A. 2005. Grounding cognition: The role of perception and action in memory, language, and thinking. In Grounding cognition: The role of perception and action in memory, language, and thinking.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. 2021. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning. pp. 8748--8763
Smith, R. 2007. An overview of the Tesseract OCR engine. In Ninth international conference on document analysis and recognition (ICDAR 2007). pp. 629--633
Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and Artzi, Y. 2019. A Corpus for Reasoning about Natural Language Grounded in Photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 6418--6428 10.18653/v1/P19-1644
Tjuka, A. 2021. A list of color, emotion, and human body part concepts. In 2021. A list of color, emotion, and human body part concepts. 10.5281/zenodo.5572303
Tomar, S. 2006. Converting video formats with ffmpeg. In Linux Journal. pp. 10
Vedantam, R., Bengio, S., Murphy, K., Parikh, D., and Chechik, G. 2017. Context-Aware Captions from Context-Agnostic Supervision. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1070--1079 10.1109/CVPR.2017.120
Wilson, D. and Sperber, D. 1998. Pragmatics and time. Pragmatics and Beyond New Series. In Pragmatics and time. Pragmatics and Beyond New Series. pp. 1--22
Xie, N., Lai, F., Doran, D., and Kadav, A. 2019. Visual entailment: A novel task for fine-grained image understanding. In Visual entailment: A novel task for fine-grained image understanding. arXiv:1901.06706
Xu, J., Mei, T., Yao, T., and Rui, Y. 2016. Msrvtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5288--5296
Yan, A., Wang, X., Fu, T., and Wang, W. Y. 2021. L2C: Describing visual differences needs semantic understanding of individuals. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 2315--2320 10.18653/v1/2021.eacl-main.196