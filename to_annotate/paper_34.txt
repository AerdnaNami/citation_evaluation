Related Work

Many recent works on paraphrase generation have been focused on attempting to achieve high-quality paraphrases. These works can be divided into supervised and unsupervised approaches.

Supervised Approaches To achieve diversity, some works focused on diverse decoding using heuristics such as Hamming distance or distinct n-grams to preserve diverse options during beam search (Vijayakumar et al., 2018). Other works MSCOCO WikiAns ParaBank2 q sem ↑ q syn ↑ q lex ↑ i-BLEU↓ q sem ↑ q syn ↑ q lex ↑ i-BLEU↓ q sem ↑ q syn ↑ q lex ↑ i-BLEU↓ Gold 29.9 34. generate multiple outputs by perturbing latent representations (Gupta et al., 2018;Park et al., 2019). or by using distinct generators (Qian et al., 2019). These methods achieve some diversity, but do not control generation in an interpretable manner.

The works that are most similar to ours strive to gain diversity using controlled-paraphrase generation, by exposing control mechanisms that are manipulated to produce either lexically (Zeng et al., 2019;Thompson and Post, 2020) or syntactically (Chen et al., 2019; Goyal and Durrett, 2020) diverse paraphrases. One approach is to use an exemplar sentence for guiding the syntax of the generated paraphrase Bao et al., 2019). An alternative is to directly employ constituency tree as the syntax guidance (Iyyer et al., 2018;Li and Choi, 2020). Goyal and Durrett (2020) promote syntactic diversity by conditioning over possible syntactic rearrangements of the input. Zeng et al. (2019) use keywords as lexical guidance for the generation process. Here we introduce a simple model for jointly controlling the lexical, syntactic and semantic aspects of the generated paraphrases.

Unsupervised Approaches Niu et al. (2020) rely on neural models to generate high quality paraphrases, using a decoding method that enforces diversity by preventing repetitive copying of the input tokens. Liu et al. (2020) optimize a quality oriented objective by casting paraphrase generation as an optimization problem, and searching the sentence space to find the optimal point. Garg et al. (2021) and Siddique et al. (2020) use reinforcement learning with quality-oriented reward combining textual entailment, semantic similarity, expression diversity and fluency. In this work, we employ similar metrics for guiding the generation of paraphrases within the supervised framework.

 References: 
Bao, Y., Zhou, H., Huang, S., Li, L., Mou, L., Vechtomova, O., Dai, X., and Chen, J. 2019. Generating sentences from disentangled syntactic and semantic spaces. In Generating sentences from disentangled syntactic and semantic spaces. arXiv:1907.05789
Bhagat, R. and Hovy, E. 2013. What is a paraphrase?. In Computational Linguistics. pp. 463--472
Chen, M., Tang, Q., Wiseman, S., and Gimpel, K. 2019. Controllable paraphrase generation with a syntactic exemplar. In Controllable paraphrase generation with a syntactic exemplar. arXiv:1906.00565
Clark, K., Luong, M., Quoc, V., Le, C.D., and Manning 2020. Electra: Pre-training text encoders as discriminators rather than generators. In Electra: Pre-training text encoders as discriminators rather than generators. arXiv:2003.10555
Devlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423
Fader, A., Zettlemoyer, L., and Etzioni, O. 2014. Open question answering over curated and extracted knowledge bases. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 1156--1165
Fader, A., Zettlemoyer, L., and Etzioni, O. 2014. Open Question Answering Over Curated and Extracted Knowledge Bases. In KDD.
Garg, S., Prabhu, S., Misra, H., and Srinivasaraghavan, G. 2021. Unsupervised contextual paraphrase generation using lexical control and reinforcement learning. In Unsupervised contextual paraphrase generation using lexical control and reinforcement learning.
Goyal, T. and Durrett, G. 2020. Neural syntactic preordering for controlled paraphrase generation. In Neural syntactic preordering for controlled paraphrase generation. arXiv:2005.02013
Guo, M., Shen, Q., Yang, Y., Ge, H., Cer, D., Abrego, G. H., Stevens, K., Constant, N., Sung, Y., Strope, B., and Kurzweil, R. 2018. Effective parallel corpus mining using bilingual sentence embeddings. In Proceedings of the Third Conference on Machine Translation: Research Papers. pp. 165--176 10.18653/v1/W18-6317
Gupta, A., Agarwal, A., Singh, P., and Rai, P. 2018. A deep generative framework for paraphrase generation. In Proceedings of the AAAI Conference on Artificial Intelligence.
Hu, J. E., Singh, A., Holzenberger, N., Post, M., and Van Durme, B. 2019. Largescale, diverse, paraphrastic bitexts via sampling and clustering. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). pp. 44--54 10.18653/v1/K19-1005
Iyyer, M., Wieting, J., Gimpel, K., and Zettlemoyer, L. 2018. Adversarial example generation with syntactically controlled paraphrase networks. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
Li, C. and Choi, J. D. 2020. Transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5709--5714 10.18653/v1/2020.acl-main.505
Li, Z., Jiang, X., Shang, L., and Liu, Q. 2019. Decomposable neural paraphrase generation. In Decomposable neural paraphrase generation. arXiv:1906.09741
Lin, T., Maire, M., Belongie, S. J., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In ECCV.
Liu, X., Mou, L., Meng, F., Zhou, H., Zhou, J., and Song, S. 2019. Unsupervised paraphrasing by simulated annealing. In Unsupervised paraphrasing by simulated annealing. arXiv:1909.03588
Liu, X., Mou, L., Meng, F., Zhou, H., Zhou, J., and Song, S. 2020. Unsupervised paraphrasing by simulated annealing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 302--312 10.18653/v1/2020.acl-main.28
Mccann, B., Shirish Keskar, N., Xiong, C., and Socher, R. 2018. The natural language decathlon: Multitask learning as question answering. In The natural language decathlon: Multitask learning as question answering. abs/1806.08730
Mccarthy, P. M., Guess, R. H., and Mcnamara, D. 2009. The components of paraphrase evaluations. In Behavior Research Methods. pp. 682--690
Niu, T., Yavuz, S., Zhou, Y., and Wang, H. Nitish Shirish Keskar, and Caiming Xiong. 2020. Unsupervised paraphrase generation via dynamic blocking. In Nitish Shirish Keskar, and Caiming Xiong. 2020. Unsupervised paraphrase generation via dynamic blocking.
Park, S., Seung-Won, Hwang, F., Chen, J., Choo, J., Ha, S., Kim, J., and Yim 2019. Paraphrase diversification using counterfactual debiasing. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 6883--6891
Qian, L., Qiu, L., Zhang, W., Jiang, X., and Yu, Y. 2019. Exploring diverse expressions for paraphrase generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3173--3182
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2019. Exploring the limits of transfer learning with a unified text-to. In Exploring the limits of transfer learning with a unified text-to.
Reimers, N. and Gurevych, I. 2020. Making monolingual sentence embeddings multilingual using knowledge distillation. In EMNLP.
Rush, A. M., Chopra, S., and Weston, J. 2015. A neural attention model for abstractive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.
Sellam, T., Das, D., and Parikh, A.P. 2020. Bleurt: Learning robust metrics for text generation. In Bleurt: Learning robust metrics for text generation. arXiv:2004.04696
Ab Siddique, S., Oymak, V., and Hristidis 2020. Unsupervised paraphrasing via deep reinforcement learning. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. pp. 1800--1809
Sun, H. and Zhou, M. 2012. Joint learning of a dual smt system for paraphrase generation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. pp. 38--42
Thompson, B. and Post, M. 2020. Paraphrase generation as zero-shot multilingual translation: Disentangling semantic similarity from lexical and syntactic diversity. In Proceedings of the Fifth Conference on Machine Translation. pp. 561--570
Ashwin, K., Vijayakumar, M., Cogswell, Ramprasaath, R., Selvaraju, Q., Sun, S., Lee, D., Crandall, D., and Batra 2018. Diverse beam search for improved description of complex scenes. In Thirty-Second AAAI Conference on Artificial Intelligence.
Yu, A. W., Dohan, D., Luong, M., Zhao, R., Chen, K., Norouzi, M., and Quoc, V.
Le 2018. Qanet: Combining local convolution with global self-attention for reading comprehension. In Qanet: Combining local convolution with global self-attention for reading comprehension.
Zeng, D., Zhang, H., Xiang, L., Wang, J., and Ji, G. 2019. User-oriented paraphrase generation with keywords controlled network. In IEEE Access. pp. 80542--80551
Zhang, K. and Shasha, D. 1989. Simple fast algorithms for the editing distance between trees and related problems. In SIAM journal on computing. pp. 1245--1262