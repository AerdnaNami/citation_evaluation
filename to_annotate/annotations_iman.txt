[836-1053] Coherence, File: paper_16.txt,  Coder: default
ant. Irrelevant images have little impact on the translation quality, and no signiﬁcantBLEU drop is observed even the image is absent(Elliott, 2018). Encouraging results appeared inCaglayan et al. (2019)’s work. They pointed out that the visual modality is still useful when the linguistic context is scarce, but is less sensitive when exposed to complete sentences. More recently, Wu et al. (2021) attributed the BLEU gain on MMT tasks to the regularization training. Caglayan et al. (2021) proposed a cross-lingual 


[698-1217] Coherence, File: paper_28.txt,  Coder: default
ion model based on previous research.Dialogue Response Generation (DSG) in opendomain is a challenging task with a wide range ofapplication scenarios. Recent advances in DSGutilize pre-trained language models (PLMs) suchas BERT (Devlin et al., 2019) and GPT2 (Radford et al., 2019) in two major categories. The first one focuses on how to fine-tune PLMs in downstream tasks and address the various application-specific needs and challenges (Lin et al., 2020). The second one augments dialog specific tasks into the PLM training (Zhang et al., 2020; Bao et al., 2020) and then fine-tunes the new pre-trained model in downstream tasks. We study the latter in this paper. There is a proverbial one-to-many problem in DSG, i.e., a single dialog context could be followed by multiple reasonable responses. Existing works int


[0-4284] Lacks synthesis, File: paper_1.txt,  Coder: default
and NLPDistributional semantics has been an integral partof computational linguistics since the beginningsof the field, but in a discontinuous manner that encompasses at least two distinct eras. Firstly, duringthe 1950s and 60s, Harris was integral to the mathematization of linguistics after the Second WorldWar (Rubenstein and Goodenough, 1965; Léon,2021). Firth was skeptical of efforts to mechanizelinguistics, but he nonetheless consulted for someof the early work on machine translation at Cambridge (Léon, 2007, 410). Secondly, when computational linguistics returned to its “empiricist” rootsin probabilistic methods and information theory inthe mid-80s and early 90s (Norvig 2012, 30; Léon2021, 141), Firth and Harris were alongside Shannon among the authors who were evoked, in an ACL “Special Issue on Computational Linguistics Using Large Corpora,” as foundational figures of a tradition that had been overshadowed for decades by the “rationalism” of characters such as Noam Chomsky and Marvin Minsky (Church and Mercer, 1993, 15). During this “corpus turn,” the rapid automation of linguistics was driven by a resumed connection with postwar computational linguistics and information theory (Léon, 2021, 3). However, important papers on non-neural lexical semantics from the era (Landauer and Dumais, 1997; Deerwester et al., 1989, 1990) and other vector space models or their applications (Schutze, 1992; Schütze, 1993; Schütze and Pedersen, 1993; Burgess, 1998; Blei et al., 2002) generally did not cite Firth or Harris. In short, while Firth and Harris were not regularly used as stand-ins for linguistic theory during this time of “count based” models (Baroni et al., 2014), a general revival of empiricism and distributional approaches to meaning did signal a potential resurgence of interest in their thinking. At the turn of the millennium, computational approaches to lexical semantics shifted toward predictive neural language modeling (Henderson, 2020). During the 2000s, the application of neural networks to language modeling tasks (e.g. Bengio et al., 2003) and the development of selfsupervision techniques (e.g. Raina et al., 2007) set the stage for the word embedding breakthroughs of the early 2010s (e.g. Mikolov et al., 2013). By the end of the decade, the introduction of Long Short Term Memory networks (LSTMs) (e.g. Graves et al., 2013) and then of the Transformer model (Vaswani et al., 2017) had made way for the next breakthrough, the large-language modeling revolution (e.g. Devlin et al., 2019; Peters et al., 2018). Following the introduction of the word2vec model and its powerful but “static” embeddings, Harris in particular was frequently cited (Bojanowski et al., 2017; Le and Mikolov, 2014; Levy and Goldberg, 2014; Levy et al., 2015), often (but not always) along with Firth (Goldberg, 2017; Jurafsky and Martin, 2009, 2021; Hamilton et al., 2016; Eisenstein, 2019; Lin and Dyer, 2010; Bruni et al., 2014). However, despite an explosion of citations (Bisk et al., 2020, 8719), this interest has not been very engaged. In fact, the canonization of Firth and Harris during this time is paradoxical. 2On the one hand it seems that they are invoked tolend theoretical authority to a field that strugglesto lift its gaze from the latest state-of-the-art numbers (Manning, 2015; Bender and Koller, 2020).Yet, on the other hand, the unspoken conclusionfrom the ascent of connectionism and the languagemodeling revolution in particular was that “learning from data made linguistic theories irrelevant”(Henderson, 2020, 6295). In other words, just asNLP seemed to lose interest with linguistic theory,it elevated two pioneering theoreticians to canonical status, but without deeply engaging with theirwork. Firth and Harris become figures who justifya relatively narrow conception of meaning, one thatis predominantly intra-linguistic, without much tosay about its uses in the world.Though Léon discusses the contrast betweenHarris and Firth in the context of corpus linguistics(2008) and their influence on the history of computational linguistics (2021), her work does notaddress the differences in their distributional theories and conceptions of “context,” nor the renewedand paradoxical significance of the two authorsfor language modeling. In our contribution, weemphasize the gap between the ideas of Firth andHarris as well as the potential insights a re-readingof their work offer for e


[4519-6269] Lacks synthesis, File: paper_1.txt,  Coder: default
l as the potential insights a re-readingof their work offer for expanding the scope of computational semantics.3 Harris’s distributional structuralismFew linguists contributed more to linguistic theorythan Zellig Harris (1909–1992), and not just forserving as Chomsky’s doctoral advisor. In fact, thetwo came to share little in common (Goldsmith,2005; Nevin, 2010); whereas Chomsky’s generative grammar repositioned linguistics as a cognitivescience seeking to understand, in so few words, theidealized mental representations and structures enabling language acquisition and production (e.g.Chomsky, 1972), Harris’ radically distributionalapproach to language saw the natural language corpus as the sole starting point from which a generallinguistic theory could arise.This theory consisted of a linguistic structurebest understood as segmentable into a finite setof formal objects characterized by structured, constrained patterns of correspondence with one another (Harris, 1951, 1954, 1991). Such patterns ofcorrespondence can be observed only in languagein-use, that is, in natural language corpora. In hisfoundational paper “Distributional Structure,” forexample, he provides a purely distributional account of how one might induce the semantic meanings of oculist, eye-doctor, and lawyer from the partial (in the case of oculist and lawyer) or nearly complete (in the case of oculist and eye-doctor) overlap in their observed “environments” of use (Harris, 1954, 156–157). This general approach applies to other levels of linguistic analysis, such as morphophonemics (e.g., Harris, 1954, 155). Rather than producing a series of descriptive rules for the distribution of each phoneme, morpheme, or word, the goal was to achieve greater parsimony by grouping these elements into structurally equivalent classes sharing the same distributional rules, compounding elements in a hierarchical manner. In this section we draw attention to three aspects of Harris’s distributional linguistics: the relationship it posits between meaning and form; assum


[0-2112] Lacks synthesis, File: paper_12.txt,  Coder: default
2021a). Adaptive kNN-MT is derived from kNNMT (Khandelwal et al., 2021) by inserting a lightweight Meta-kNetwork that fuses kNN retrievals with various k to alleviate the possible noise induced by a single k. Formally, it is formulated as two steps: target-side datastore creation and MetakNetwork predictions. Target-side Datastore Creation. The datastore constists of a set of key-value pairs. Given a bilingual sentence pair (s,t) in a corpus (S,T), a pretrained general domain NMT model autoregressively extracts the context representation hi of the i-th target token conditioned on both source and target context (s,t<i), denoted as hi = f(s,t<i). The datastore is ﬁnally constructed by taking hi as keys and ti as values: (K,V) =⋃(s,t)∈(S,T){(hi,ti),∀ti ∈t}. Meta-k Network Prediction. Meta-k Network (fβ) is a two-layer feed-forward network followed by a non-linear activation function. Based on the constructed datastore, it considers a set of various ks that are smaller than an upper bound K, the standard setting is k ∈ Qwhere Q = {0}∪{ 2j|j ∈ N,2j ≤ K}. K nearest neighbors of the current context query ˆhi from the datastore are ﬁrst retrieved at the i-th decoding step. Then the l2 distance from ˆhi to each neighbor (hj,vj) is denoted as di = ∥hj,ˆhi∥2. And the count of distinct values in top j neighbors are denoted as cj. The normalized weights of each available kare obtained: pβ(k) = softmax(fβ([d1,...,d k; c1,...,c k])), where fβ denotes the Meta-kNetwork. The ultimate prediction probability is ensembled: p(ti|s,ˆt<i) =∑kj∈Spβ(kj) ·pkjNN(ti|s,ˆt<i), Note that a validation set is usually required to study the Meta-kNetwork before predicting on test 2Figure 2: The framework of our approach. First, the cluster-based Compact Network is used to reduce the key’sdimensionality of the original datastore and a new datastore is reconstructed. Then the cluster- based pruning isapplied to reduce the datastore size.Figure 3: The Compact Network illustration. fα is fordimension reduction and fθ is for NCE training.sets. During training, only the parameters of theMeta-kNetwork need to update.

 References: 
Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei
Xu, et al. 1996. A density-based algorithm for
discovering clusters in large sp


[0-1231] Lacks synthesis, File: paper_16.txt,  Coder: default
Multimodal machine translation is a cross-domaintask in the ﬁled of machine translation. Early attempts mainly focused on enhancing the MMTmodel by better incorporation of the vision features(Calixto and Liu, 2017; Elliott and Kádár, 2017;Delbrouck and Dupont, 2017). However, directlyencoding the whole image feature brings additionalnoise to the text (Yao and Wan, 2020; Liu et al.,2021a). To address the above issue, Yao and Wan(2020) proposed a multimodal self-attention to consider the relative difference of information betweentwo modalities. Similarly, Liu et al. (2021a) used aGumbel Softmax to achieve the same goal.Researchers also realize that the vision modalitymaybe redundant. Irrelevant images have little impact on the translation quality, and no signiﬁcantBLEU drop is observed even the image is absent(Elliott, 2018). Encouraging results appeared inCaglayan et al. (2019)’s work. They pointed out that the visual modality is still useful when the linguistic context is scarce, but is less sensitive when exposed to complete sentences. More recently, Wu et al. (2021) attributed the BLEU gain on MMT tasks to the regularization training. Caglayan et al. (2021) proposed a cross-lingual visual pretraining approach. In this work, we make a systematic study on whether stronger vision features are helpful. We also extend the research to enhanced features, such as o


[446-1709] Lacks synthesis, File: paper_2.txt,  Coder: default
ith some downstream applications, especially when the model needs to make inferencebased on context (Do and Pavlick, 2021; Kassnerand Schütze, 2020). Thus, recent works have alsoexplored enhancing pretrained models with external knowledge.Introducing knowledge into language modelshas been shown to be successful on various downstream tasks and model architecture (Ren et al.,2020; Zhao et al., 2020; Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations. There are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include ConceptNet (Speer et al., 2017), ATOMIC (Sap et al., 2019), and TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS(Bodenreider, 2004) and OHAMA 1. We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge


[350-2106] Lacks synthesis, File: paper_21.txt,  Coder: default
 Some recent works extend pivots with autoencoders and contextualized embeddings (Ziser and Reichart, 2017; Miller, 2019; BenDavid et al., 2020). (2) Pseudo-labeling leverages a trained classifier to predict labels on unlabeled examples, which are subsequently considered as gold labels (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006). Recent works also combine this technique with pre-trained language models (Lim et al., 2020; Ye et al., 2020). (3) Domain Adversarial Neural Networks (Ganin et al., 2016). Some approaches leverage Wasserstein distance to stabilize adversarial training (Shen et al., 2018; Shah et al., 2018), and combine it with posttraining which can produce better adversarial results (Du et al., 2020). (4) Adaptive pre-training is a more straightforward but effective method (Gururangan et al., 2020; Han and Eisenstein, 2019; Karouzos et al., 2021) by leveraging the objective of masked language model (MLM). Contrastive learningCL has recently gained popularity as a reliable approach for unsupervised representation learning. For Computer Vision, there are approaches obtaining augmented images using transformations including cropping, rotation, etc. (Chen et al., 2020a; Khosla et al., 2020; He et al., 2020; Chen et al., 2020b). As for Natural Language Processing, many works study different label-preserving augmentations, such as backtranslations, synonyms, adversaries, dropout, and their combinations (Qu et al., 2021; Gao et al., 2021). In addition, many pre-trained language models trained with contrastive loss are also released. DeCLUTR (Giorgi et al., 2021) and CLEAR (Wu et al., 2020) jointly train the model with a contrastive objective and a masked language model setting. ConSERT (Yan et al., 2021) overcomes the collapse problem of BERT-derived sentence representations and makes them more suitable for downstream applications by using unlabeled texts.

 References: 
Sanjeev Arora, H. Khandeparkar, M. Khodak, Orestis
Plevrakis, and Nikunj Saunshi. 2019. A theoretical
analysis of contrastive unsuperv


[183-1780] Lacks synthesis, File: paper_24.txt,  Coder: default
eneration is toﬁnd the sequence of tokens x1:T which maximisesp(x1:T |c), given a constraint c. Few methodsaddress the constrained textual generation.Class-conditional language models. Classconditional language models (CC-LMs), as theConditional Transformer Language (CTRL) model(Keskar et al., 2019), train or ﬁne-tune the weightsθof a single neural model directly for controllablegeneration, by appending a control code in thebeginning of a training sequence. The control codeindicates the constraint to verify and is related toa class containing texts that satisfy the constraint.For the sake of simplicity, we will denote withoutdistinction the class, the constraint veriﬁed byits texts and the associated control code by c.Trained with different control codes, the modellearns pθ(x1:T |c) =∏Tt=1 pθ(xt |x1:t−1,c). Theconstraint can then be applied during generationby appending the corresponding control code tothe prompt. While this method gives some kindof control over the generation, the control codesneed to be deﬁned upfront and the LM still needsto be trained speciﬁcally for each set of controlcodes. This is an important limitation since thecurrent trend in text generation is the use of largepre-trained model which can hardly be ﬁne-tuned(for instance, the last version of GPT, GPT-3,cannot be ﬁne-tuned without access to very largehardware resources).Discriminator-based methods The general ideaof discriminator-guided generation is to combinea disciminator D with a generative LM. The discriminator explicitly models the constraint by calculating the probability pD(c | x1:T) of the sequence x1:T to satisfy the constraint c. This probability is directly related to p(x1:T |c) through Bayes’ rule : p(x1:T |c) ∝pD(c|x1:T)pθ(x1:T). Discriminator-based methods alleviate the training cost problem, as discriminators are easier to train than a LM. Moreover, any additional constrain


[2022-3381] Lacks synthesis, File: paper_24.txt,  Coder: default
ier to train than a LM. Moreover, any additional constraint can be deﬁned a posteriori without tuning the LM, only by training another discriminator. The discriminators have been used in different ways to explore the search space. In the work of (Holtzman et al., 2018; Scialom et al., 2020), the space is ﬁrst searched using beam search to generate a pool of proposals with a high likelihood pθ(x1:T), and then the discriminator is used to re-rank them. However, in addition that beam search can miss sequences with high likelihood, it is biased towards the likelihood, while the best sequence might only have an average likelihood, but satisﬁes the constraint perfectly. Hence, it might be more suitable to take the discriminator probability into account during decoding rather than after generating a whole sequence. In this case, the discriminator is used at each generation step to get the probability pD(c|x1:t) for each token of the vocabulary V, and merge it to the likelihood pθ(x1:t) to choose which token to emit. In order to reduce the cost of using a discriminator on every possible continuation, GeDi (Krause et al., 2020) proposes to use CC-LMs as generative discriminators. The method relies on the fact that the CC-LM computes pθ(xt |x1:t−1,c) for all tokens of the vocabulary which can be used to get pθ(c |x1:t) for all tokens using Bayes’ equation. This approach is thus at the intersection of tuning the LM and using a discriminator: it tunes a small LM (the CC-LM) to guide a bigger one. In Plug And Play Language Model (PPLM) (Dathathri et al., 2020), the discriminator is used to shift the hidden states of the pre-trained transformer-


[3381-4556] Lacks synthesis, File: paper_24.txt,  Coder: default
equation. This approach is thus at the intersection of tuning the LM and using a discriminator: it tunes a small LM (the CC-LM) to guide a bigger one. In Plug And Play Language Model (PPLM) (Dathathri et al., 2020), the discriminator is used to shift the hidden states of the pre-trained transformer-based LM towards the desired class at every generation step. PPLM can be used on any LM and with any discriminator. However, PPLM needs to access the LM to modify its hidden states, while our approach only requires the output logits. As some LM can only be used through access to logits (e.g. GPT-3 API), this makes our approach more plug and play than PPLM. A common drawback of all these approaches is their lack of a long-term vision of the generation. Indeed, the discriminator probabilities become necessarily more meaningful as the sequence grows 2and might only be trustable to guide the searchwhen the sequence is (nearly) ﬁnished. When usedin a myopic decoding strategy, classiﬁcation errorswill cause the generation process to deviate furtherand further. Trying to optimize a score deﬁned inthe long horizon by making short term decisions isvery similar to common game setups such as chess,where the Monte Carlo Tree Search (MCTS) hasproven to be really effective (Silver et al., 2018),which motivated our approach.

 References: 
Emily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can lang


[1414-2483] Lacks synthesis, File: paper_28.txt,  Coder: default
G, i.e., a single dialog context could be followed by multiple reasonable responses. Existing works introduce latent variables to model this problem. For example, VHRED (Serban et al., 2017) incorporates latent continuous variable into the sequenceto-sequence (Seq2Seq) RNN model to improve the diversity of generated responses. V AE-Seq2Seq (Bahuleyan et al., 2017) proposes variational attention to replace the vanilla encoder-decoder attention (Luong et al., 2015), to avoid attention to bypass the latent space and invalidate the latent variable. For controllability and interpretability, some discrete V AEs have also been proposed, such as (Oord et al., 2017; Vahdat et al., 2018). Recently, PLATO (Bao et al., 2020) firstly introduces latent variables into their pre-training dialog model, where the authors introduce a K-way (K = 20) categorical latent variable, and the pretrained model shows significant gains in multiple downstream response generation tasks. Continuous latent variables besides discrete latent variables is popularly used for modeling one-to-many mapping in dialog system, but the potential of incorporating continuous latent variables with large-scale language pretraining is less explored. In this paper, we propose a pre-trained latent Variable Encoder-Decoder model for Dialog generation, which is called DialogVED. In this model, we int


[0-1505] Lacks synthesis, File: paper_30.txt,  Coder: default
Effective utilization of annotation budgets has beenthe area of focus for numerous active learningworks, showing improvements for different taskslike POS tagging (Ringger et al., 2007), sentimentanalysis (Karlos et al., 2012; Li et al., 2013; Brewet al., 2010; Ju and Li, 2012), syntactic parsing(Duong et al., 2018), and named entity recognition(Settles and Craven, 2008; Shen et al., 2018). Thefocus of most of these works, however, has beenon learning for a single language (often English).Prior work on AL that uses a multilingual setupor cross-lingual information sharing and that goesbeyond training a separate model for each languagehas thus been limited. The closest work where multiple languages influence each other’s acquisition isthat of Qian et al. (2014); however, they still traina separate model for each language.For transfer to multiple languages, recent advances in building MPLMs (Devlin et al., 2019;Conneau et al., 2020; Liu et al., 2020; Xue et al.,2020) have been extremely effective, especially inzero-shot transfer (Pires et al., 2019; Liu et al.,2020). Ein-Dor et al. (2020) studied the dataeffectiveness of these models when used in conjunction with AL, but, as with other AL work, witha single language focus. Finally, Lauscher et al.(2020) studied the effectiveness of the zero-shotsetup, showing that adding a few examples to amodel trained on English improves performanceover zero-shot transfer. However, this assumes theavailability of a full English task-specific corpus.

 References: 
Anthony Brew, Derek Greene, and Pádraig Cunningham.
2010. Using crowdsourcing and active learning to
track sentiment in online media. 


[3164-3291] Mischaracterized citation, File: paper_10.txt,  Coder: default
ing on comprehensionof narratives, targeting students from kindergartento eighth grade (Table 1). We focus on narrativecomprehension for two reasons. First, narrativecomprehension is a high-level comprehension skillstrongly predictive of reading achievement (Lynchet al., 2008) and plays a central role in daily life aspeople frequently encounter narratives in diﬀerentforms (Goldie, 2003). Second, narrative storieshave a clea


[4199-4305] Overclaiming, File: paper_19.txt,  Coder: default
over, we experiment on a practical medical dataset (CBLUE) tofurther demonstrate the ability of RICON.Our contributions can be summarized as follows:• This is the first work that explicitly exploresthe internal regularity of entity mentions forChinese NER.• We propose a simple but effective method forChinese NER, which effectively utilizes regularity information while avoiding excessivefocus on intra-sp


[4002-4082] Overclaiming, File: paper_25.txt,  Coder: default
geof a previous domain lost after training in a newdomain? These challenges are crucial for valueclassification because of its domain-specific nature.We perform the first comprehensive crossdomain evaluation of a value classifier. We employ the Moral Foundation Twitter Corpus (Hooveret al., 2020), consisting of seven datasets spanningdifferent socio-political areas, annotated w


[84-187] Unsupported claim, File: paper_10.txt,  Coder: default
Reading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Questionanswering (QA) are fundamental for supportinghumans’ development of reading comprehensionskills, as questions serve as both instruments forevaluation and tools to facilitate learning. Toachieve this goal, comprehension questions shouldStory Titl


[1806-1903] Unsupported claim, File: paper_10.txt,  Coder: default
tions to be able to identify students’ performance in speciﬁc sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension. However, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some


[2361-2564] Unsupported claim, File: paper_10.txt,  Coder: default
20). However, existing datasets arenot particularly suitable for training question generation (QG) models for educational purposes (Daset al., 2021). This is primarily because the datasetsare not typically structured around the speciﬁc dimensions of reading comprehension sub-skills, nordo they provide suﬃcient information on what subskills are tested. As a consequence, QG modelsbuilt on these datasets only yield one single "comprehension" score without a more detailed breakdown of performance on co


[3508-3599] Unsupported claim, File: paper_10.txt,  Coder: default
er narratives in diﬀerentforms (Goldie, 2003). Second, narrative storieshave a clear structure of speciﬁc elements and relations among these elements, and there are existing validated narrative comprehension frameworksaround this structure, which provides a basis fordeveloping the annotation schema for our dataset.We employed education experts who generated10,580 question-answer pairs bas


[422-519] Unsupported claim, File: paper_13.txt,  Coder: default
ually accomplished by representing the labels of the task in a textual form, whichcan either be the name of the label or a concisetextual description.In recent years, there has been a surge in zeroshot and few-shot approaches to text classiﬁcation.One approach (Yin et al., 2019, 2020; Halder et al.,2020; Wang et al., 2021) makes use of entailmentmodels. Textual entailment (Dagan et al., 2006),a


[625-690] Unsupported claim, File: paper_16.txt,  Coder: default
to consider the relative difference of information betweentwo modalities. Similarly, Liu et al. (2021a) used aGumbel Softmax to achieve the same goal.Researchers also realize that the vision modalitymaybe redundant. Irrelevant images have little impact on the translation quality, and no signiﬁcantBLEU drop is observed even the image is absent(Elliott, 2018). Enco


[0-177] Unsupported claim, File: paper_23.txt,  Coder: default
Natural language processing (NLP) datasets areplagued with artifacts and biases, which allow models to perform tasks without learning the desiredunderlying language capabilities. For instance, innatural language inference (NLI) datasets, modelscan predict an entailment relationship yfrom thehypothesis text H alone, without co


[674-801] Unsupported claim, File: paper_23.txt,  Coder: default
cCoy et al., 2019). We refer to such biases as structural biases, cases where an undesired subset of the input alone incorrectly identiﬁes the label. Relying on such biases results in poor out-of-distribution (o.o.d) generalization when models are applied to data without bias. Furthermore, models that contain such biases may make surprising predictions when the bias is present, causing problems in critical systems. A line o


[1119-1286] Unsupported claim, File: paper_23.txt,  Coder: default
ed to improve the performance on o.o.d datasets by proposing different objective functions (e.g., Utama et al., 2020a; Karimi Mahabadi et al., 2020). However, these methods typically still result in a signiﬁcant gap between the performance in and out of distribution, which indicates that the models are still biased. Table 1 shows this gap, which we term the o.o.d generalization gap (∆). In this work, we reformulate classiﬁcation as a generative task, where the mo


[1727-1794] Unsupported claim, File: paper_23.txt,  Coder: default
osterior p(y|B,R) into the likelihood p(R|y,B) and the prior p(y |B). This reformulation lets us control the amount of bias present in the ﬁnal model. By setting a uniform prior we can obtain a provably unbiased model. We denote this generative model as GEN.. To assess the extent to which a given model is biased w.r.t a speciﬁc structural bias, we consider two metr


[2951-3067] Unsupported claim, File: paper_23.txt,  Coder: default
.18 −0.65 58.55 57 .33 +1.22GEN (BART) 70.58 72 .19 −1.61 64 .09 65 .74 −1.65Table 1: Results on regular and hard (o.o.d) test sets of SNLI and MNLI. Prior work exhibits large o.o.d generalization gaps (∆), while our generative approach reduces the gap signiﬁcantly.Next, we experiment with two kinds of naturalbias: hypothesis-only and overlap. We demonstratethat GEN is unbiased compared to the discriminative base


[129-182] Unsupported claim, File: paper_24.txt,  Coder: default
The goal of constrained textual generation is toﬁnd the sequence of tokens x1:T which maximisesp(x1:T |c), given a constraint c. Few methodsaddress the constrained textual generation.Class-conditional language models. Classconditional language models (CC-LMs), as theConditional Transformer Language (CTRL) model(Keskar et al., 2019


[1896-2021] Unsupported claim, File: paper_24.txt,  Coder: default
e : p(x1:T |c) ∝pD(c|x1:T)pθ(x1:T). Discriminator-based methods alleviate the training cost problem, as discriminators are easier to train than a LM. Moreover, any additional constraint can be deﬁned a posteriori without tuning the LM, only by training another discriminator. The discriminators have been used in different ways to explore the search space. In the work of (Holtzman et al., 2018; Scialom et al., 2020), the sp


[246-586] Unsupported claim, File: paper_25.txt,  Coder: default
lity can be represented, understood, and explained by a finite number of irreducible basicelements, referred to as moral values (Grahamet al., 2013). The difference in our preferencesover moral values explains how and why we thinkdifferently. For instance, both conservatives andliberals may agree that individual welfare is important. However, a conservative, who cherishes thevalues of freedom and independence, may believethat taxes should be decreased to attain more individual welfare. In contrast, a liberal, who cherishesthe values of community and care, may believethat taxes should be increased to obtain welfare(Graham et al., 200


[2217-2491] Unsupported claim, File: paper_25.txt,  Coder: default
nderlying a piece of text on the fly. For instance, Mooijman et al. (2018) show that detecting moral values from tweets can predict violent protests. Existing value classifiers are evaluated on a specific dataset, without re-training or testing the classifier on a different dataset. This shows the ability of the classifier to predict values from text, but not the ability to transfer the learned knowledge across datasets. A critical aspect of moral values is that they are intrinsically linked to the domain under discussion (Pommeranz et al., 2012; Liscio et al., 2021).


[573-708] Unsupported claim, File: paper_26.txt,  Coder: default
evel annotation(Mintz et al., 2009). Most DS-RE models usethe “at-least one” assumption: ∀r ∈ R(e1,e2),∃tr ∈B(e1,e2) such that tr expresses (e1,r,e2).Recent neural approaches to DS-RE encode eachsentence t ∈ B(e1,e2) and then aggregate sentence embeddings using an aggregation operator– the common operator being intra-bag attention(Lin et al., 2016). Various models differ in theirapproach to encoding (e.g., PCNNs, GCNs, BERT)and the


[0-113] Unsupported claim, File: paper_27.txt,  Coder: default
An explanation or rationale 2, typically consistsof a subset of the input that contributes more tothe prediction. Extracting faithful explanations isimportant for studying model behavior (Adebayoet al., 2020) and assisting in tasks requiring human decision making


[0-140] Unsupported claim, File: paper_28.txt,  Coder: default
Pre-trained language models (PLMs) have beenwidely explored both in natural language understanding (NLU) and generation (NLG) in recentyears, this pre-training and fine-tuning paradigmsheds light on various downstream tasks in naturallanguage processing (NLP). Compared with general pre-tra


[297-549] Unsupported claim, File: paper_29.txt,  Coder: default
019), question answering (Chandrasekaran et al.,2020; Cheng and Erk, 2020) and knowledge baseconstruction (Goel et al., 2021; Al-Moslmi et al.,2020). To this end, entity set expansion (ESE) is acrucial task that uses a textual corpus to enhance aset of seed entities (e.g., ‘mini bar’, ‘tv unit’) withnew entities (e.g., ‘coffee’, ‘clock’) that belong tothe same semantic concept (e.g., room features).Since training data in new domains is scarce,many existing ESE methods expand a small seedroom features mini bartv unitrefrigerator cable tvcoﬀee loca


[1634-2078] Unsupported claim, File: paper_29.txt,  Coder: default
tterns, and (b) language model-based methods (Zhang et al., 2020a) that probe a pre-trained language model with prompts to rank the entity candidates. Despite the recent progress, reported success of ESE methods is largely limited to benchmarks focusing on named entities (e.g., countries, diseases) and well-written text such as Wikipedia. Furthermore, the evaluation is limited to top 10-50 predictions regardless of the actual size of the entity set. As a result, it is unclear whether the reported effectiveness of ESE methods is conditional to datasets, domains, and/or evaluation methods. In this paper, we conduct a comprehensive study to investigate the generalizability of ESE methods in low-resource settings. Speciﬁcally, we focus on


[0-211] Unsupported claim, File: paper_33.txt,  Coder: default
Research on long-range language models(LRLMs)aims to process extremely long input sequences bymaking the base Transformer architecture more efficient (e.g., through sparse attention, recurrence,or cached memory). These modifications are commonly validated by training LRLMs on PG-19 (Raeet al., 2020), a long-document language modeling dataset, and demonstratin


[992-1130] Unsupported claim, File: paper_35.txt,  Coder: default
t al., 2021) is trained with preﬁx language modeling on weakly-supervised datasets. It demonstrates its effectiveness on a zero-shot captioning task. While these models achieve improvement on few-shot tasks, they are impractical to use in real-world applications due to their model sizes. Language model prompting. Providing prompts or task descriptions play an vital role in improving pre-trained language models in many tasks (Gao et al
