Search parameters
==========
Coding by: All coders
Codes: 
Coherence. Format. Lacks synthesis. Unsupported claim. Codes: 4 / 4
Files:
paper_1.txt, paper_10.txt, paper_2.txt, paper_3.txt, paper_4.txt, paper_5.txt, paper_6.txt, paper_7.txt, paper_8.txt, paper_9.txt,  Files:  10 / 10

==========

Code count totals: 30
============
Format : 4
Lacks synthesis : 11
Unsupported claim : 15
============
Text code statistics:
Format | paper_7.txt | Count: 2 | Percent of file: 2.12%
Format | paper_5.txt | Count: 2 | Percent of file: 1.33%
Lacks synthesis | paper_8.txt | Count: 1 | Percent of file: 1.11%
Lacks synthesis | paper_6.txt | Count: 1 | Percent of file: 1.81%
Lacks synthesis | paper_1.txt | Count: 2 | Percent of file: 4.27%
Lacks synthesis | paper_2.txt | Count: 3 | Percent of file: 17.76%
Lacks synthesis | paper_4.txt | Count: 3 | Percent of file: 8.78%
Lacks synthesis | paper_3.txt | Count: 1 | Percent of file: 3.59%
Unsupported claim | paper_8.txt | Count: 4 | Percent of file: 3.68%
Unsupported claim | paper_6.txt | Count: 1 | Percent of file: 0.98%
Unsupported claim | paper_1.txt | Count: 1 | Percent of file: 1.95%
Unsupported claim | paper_2.txt | Count: 1 | Percent of file: 0.97%
Unsupported claim | paper_4.txt | Count: 3 | Percent of file: 7.56%
Unsupported claim | paper_9.txt | Count: 1 | Percent of file: 0.93%
Unsupported claim | paper_3.txt | Count: 2 | Percent of file: 1.68%
Unsupported claim | paper_10.txt | Count: 2 | Percent of file: 3.23%
========

[752-833] Format, File: paper_5.txt,  Coder: default
namely sentiment analysis (SST movie reviews) and relation extraction (Wikipedia)


[1104-1315] Format, File: paper_5.txt,  Coder: default
In our analysis, we compare human attention to pre-trained Transformers (BERT, RoBERTa and T5), from-scratch training of two shallow sequence labeling architectures (Lecun and Bengio, 1995; Vaswani et al., 2017)


[1222-1373] Format, File: paper_7.txt,  Coder: default
  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019)


[2340-2392] Format, File: paper_7.txt,  Coder: default
(Gu et al., 2016;See et al.,  Gehrmann et al., 2018)


[559-739] Lacks synthesis, File: paper_1.txt,  Coder: default
Introducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019)


[1623-1884] Lacks synthesis, File: paper_1.txt,  Coder: default
 There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.


[359-865] Lacks synthesis, File: paper_2.txt,  Coder: default
Rokeach (1973) already described the two concepts of (1) a value as a belief pertaining to desirable end states or modes of conduct and (2) a value system as prioritization of values based on cultural, social, and personal factors. These definitions attribute values to persons rather than to objects, facilitating a systematic analysis (Rokeach, 1973). The paper at hand targets the personal values behind arguments, meaning the values in the former sense that the arguments, mostly implicitly, resort to.


[867-1364] Lacks synthesis, File: paper_2.txt,  Coder: default
Several of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.



[1365-2648] Lacks synthesis, File: paper_2.txt,  Coder: default
Other proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two "higher order" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a "meta-inventory" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.


[14-714] Lacks synthesis, File: paper_3.txt,  Coder: default
Starting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).



[579-726] Lacks synthesis, File: paper_4.txt,  Coder: default
while similarly in the sixth Dialog System Technology Challenge (DSTC6) systems were filtered according to metric scores prior to human evaluation.


[1012-1540] Lacks synthesis, File: paper_4.txt,  Coder: default
Despite challenges, competitions that operate in the public domain, making data and evaluation techniques available to researchers (such as ourselves) should be applauded for such efforts.

On the other hand, competitions that (for one reason or another) do not release data and evaluation techniques into the public domain have reported relative success in terms of human evaluation. However until such methods can be accessed and independently verified through replication studies, they will unfortunately have little impact .


[3372-3695] Lacks synthesis, File: paper_4.txt,  Coder: default
Mehri and Eskenazi (2020b) propose USR (Un-Supervised and Reference-free), an unsupervised model that predicts the quality of dialog for a range of criteria using various rating scales: understandable (0-1 rating scale), natural (1-3), maintains context (1-3), interesting (1-3), uses knowledge (0-1); overall quality (1-5)


[1790-2025] Lacks synthesis, File: paper_6.txt,  Coder: default
There are growing interests recently to incorporate prompts on pre-trained language models in order to guide the models' behavior or elicit knowledge (Shin et al., 2020;Schick and Schütze, 2021;Qin and Eisner, 2021;Scao and Rush, 2021)


[104-303] Lacks synthesis, File: paper_8.txt,  Coder: default
With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community.


[264-465] Unsupported claim, File: paper_1.txt,  Coder: default
However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Schütze, 2020)


[98-293] Unsupported claim, File: paper_10.txt,  Coder: default
Question answering (QA) are fundamental for supporting humans’ development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning.


[1785-1992] Unsupported claim, File: paper_10.txt,  Coder: default
This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way.


[5458-5583] Unsupported claim, File: paper_2.txt,  Coder: default
We argue that analyzing the values found in a collection of arguments provides a new perspective to aspects in argumentation,


[1017-1194] Unsupported claim, File: paper_3.txt,  Coder: default
most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks.


[3714-3864] Unsupported claim, File: paper_3.txt,  Coder: default
To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.



[3697-3858] Unsupported claim, File: paper_4.txt,  Coder: default
Despite human evaluation being carried out by experts inter-annotator agreement levels varied depending on criteria being measured, ranging from as low as 0.298.


[4155-4486] Unsupported claim, File: paper_4.txt,  Coder: default
Contrary to common expectations, even small effect sizes (low r) can produce very low p-values (strong significance) in such tests. Aiming to achieve a significant correlation is an extremely low bar to reach in terms of consistency, since a low p-value in this case simply rejects the null hypothesis that the correlation is zero.


[4488-4856] Unsupported claim, File: paper_4.txt,  Coder: default
In addition to the above issues, human evaluation of dialogue systems rarely take into account the fact that differences in performance can occur simply by chance. The method of human evaluation we propose provides a means of applying standard tests for statistical significance to avoid concluding differences that are highly likely to have occurred simply by chance.


[1642-1769] Unsupported claim, File: paper_6.txt,  Coder: default
Accordingly, directly applying their methods to the zero-shot cross-lingual setting would result in less-preferred performance.


[104-303] Unsupported claim, File: paper_8.txt,  Coder: default
With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community.


[413-475] Unsupported claim, File: paper_8.txt,  Coder: default
Thus, how to measure instance difficulty is a crucial problem.


[751-1063] Unsupported claim, File: paper_8.txt,  Coder: default
However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.


[1604-1690] Unsupported claim, File: paper_8.txt,  Coder: default
Despite their success, it is still unknown whether or how well the instance difficulty


[2779-2900] Unsupported claim, File: paper_9.txt,  Coder: default
Our proposed methods only use the English task data (which is relatively limited) and its translations for each language.
