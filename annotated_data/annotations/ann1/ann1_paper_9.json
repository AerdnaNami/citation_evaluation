[
    {
        "filename": "paper_9.txt",
        "start": 2779,
        "end": 2900,
        "label": "Unsupported claim",
        "text": "Our proposed methods only use the English task data (which is relatively limited) and its translations for each language.",
        "full_text": "Related Work\n\nSeveral approaches to zero-shot cross-lingual transfer exist and can broadly be divided into: a) Databased Transfer, which focuses on training data transformation and b) Model-based Transfer that centres around modifying models' training routine.\n\nData-based Transfer Translating utterances for the intent classification task is relatively straightforward so previous works focused on projecting and/or aligning the entity labels between translated utterances (Dou and Neubig, 2021). This technique is followed by standard supervised training with those pseudo-labels and is commonly known as a translate-train method. One of the earliest works still being used for this purpose is fastalign (Dyer et al., 2013). It's an unsupervised word aligner trained on a parallel corpus to map each word (thus its entity label) in the source utterance to the word(s) in the target user utterance. Projecting the entity labels can also be done with word-by-word translation and source label copying (Yi and Cheng, 2021). A teacher model then weakly labels the target data, which is used to train the final student model. Sometimes, this type of label projection is complemented with an additional entity alignment step (Li et al., 2021a). Better performance can be achieved by using machine translation with entity matching and distributional statistics (Jain et al., 2019) though this can be a costly process for each language. A category of 'word substitution' methods such as code-switching (Qin et al., 2020;Kuwanto et al., 2021) or dictionary-enhanced pretraining  have also been shown to improve cross-lingual transfer.\n\nModel-based Transfer Prior to the adoption of multilingual transformers (Lample and Conneau, 2019), task-oriented XNLU methods employed a BiLSTM encoder combined with different multilingual embeddings (Schuster et al., 2019). Newer approaches usually involve a pretrained XLM and the addition of some new training component with the inference routine remaining mostly unchanged. Xu et al. (2020) learn to jointly align and predict entity labels by fusing the source and target language embeddings with attention and using the resulting cross-lingual representation for entity prediction. Qi and Du (2020) include an adversarial language detector in training whose loss encourages the model to generate language-agnostic sentence representations for improved zero-shot trans-fer. Pan et al. (2020) and Chi et al. (2020) added a contrastive loss to pretraining that treats translated sentences as positive examples and unrelated sentences as negative samples. This training step helps the XLM produce similar embeddings in different languages. However, these methods require large annotated datasets and expensive model pretraining (Chi et al., 2020). Our proposed methods only use the English task data (which is relatively limited) and its translations for each language.\n\nThe most related prior works are Arivazhagan et al. (2019) for machine translation and  for task-oriented XNLU. Both of these are cross-lingual alignment methods that use translated training data to zero-shot transfer the source language model to the target language. We focus on the latter work, called XeroAlign, which reported the most recent SOTA scores on our evaluation datasets. XeroAlign works by generating a sentence embedding of the user utterance for each language, e.g. English (source) and Thai (target) using the CLS token of the XLM. A Mean Squared Error loss function minimises the difference between the multilingual sentence embeddings and is backpropagated along with the main task loss. XeroAlign aims to bring sentence embeddings in different languages closer together with a bias towards intent classification due to the CLS embedding, which is the standard input to the intent classifier. We reproduce this method for analysis and comparisons but add a small post-processing step that distinctly improves the reported scores.\n\n References: \nArivazhagan, N., Bapna, A., Firat, O., Aharoni, R., Johnson, M., and Macherey, W. 2019. The missing ingredient in zero-shot neural machine translation. In The missing ingredient in zero-shot neural machine translation. arXiv:1903.07091\nArtetxe, M., Ruder, S., and Yogatama, D. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th.\nAnnual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 4623--4637\nArtetxe, M. and Schwenk, H. 2019. Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond. In Transactions of the Association for Computational Linguistics. pp. 597--610\nCasanueva, I., Budzianowski, P., Su, P., Mrk\u0161i\u0107, N., Wen, T., Ultes, S., Rojas-Barahona, L., Young, S., and Ga\u0161i\u0107, M. 2017. A benchmarking environment for reinforcement learning based task oriented dialogue management. In A benchmarking environment for reinforcement learning based task oriented dialogue management. arXiv:1711.11023\nChaudhary, A., Raman, K., Srinivasan, K., and Chen, J. 2020. Dict-mlm: Improved multilingual pre-training using bilingual dictionaries. In Dict-mlm: Improved multilingual pre-training using bilingual dictionaries. arXiv:2010.12566\nChen, Q., Zhuo, Z., and Wang, W. 2019. Bert for joint intent classification and slot filling. In Bert for joint intent classification and slot filling. arXiv:1902.10909\nChi, Z., Dong, L., Wei, F., Yang, N., Singhal, S., Wang, W., Song, X., Mao, X., Huang, H., and Zhou, M. 2020. foxlm: An information-theoretic framework for cross-lingual language model pre-training. In foxlm: An information-theoretic framework for cross-lingual language model pre-training. arXiv:2007.07834\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm\u00e1n, F., Grave, \u00c9., Ott, M., Zettlemoyer, L., and Stoyanov, V. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 8440--8451\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186\nZi, Y., Dou, G., and Neubig 2021. Word alignment by fine-tuning embeddings on parallel corpora. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 2112--2128\nDyer, C., Chahuneau, V., and Smith, N.A. 2013. A simple, fast, and effective reparameterization of ibm model 2. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 644--648\nGritta, M. and Iacobacci, I. 2021. Xeroalign: Zero-shot cross-lingual transformer alignment. In Xeroalign: Zero-shot cross-lingual transformer alignment. arXiv:2105.02472\nGritta, M., Lampouras, G., and Iacobacci, I. 2021. Conversation graph: Data augmentation, training, and evaluation for non-deterministic dialogue management. In Transactions of the Association for Computational Linguistics. pp. 36--52\nGroenendijk, R., Karaoglu, S., Gevers, T., and Mensink, T. 2021. Multi-loss weighting with coefficient of variations. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 1469--1478\nHu, J., Ruder, S., Siddhant, A., Neubig, G., Firat, O., and Johnson, M. 2020. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In International Conference on Machine Learning. pp. 4411--4421\nJain, A., Paranjape, B., and ZacharyC 2019. Entity projection via machine translation for cross-lingual NER. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 1083--1092 10.18653/v1/D19-1100\nKuwanto, G., Feyza Aky\u00fcrek, A., Tourni, I. C., Li, S., and Wijaya, D. 2021. Low-resource machine translation for low-resource languages: Leveraging comparable data, codeswitching and compute resources. In Low-resource machine translation for low-resource languages: Leveraging comparable data, codeswitching and compute resources. arXiv:2103.13272\nLample, G. and Conneau, A. 2019. Crosslingual language model pretraining. In Crosslingual language model pretraining. arXiv:1901.07291\nLi, B., He, Y., and Xu, W. 2021. Cross-lingual named entity recognition using parallel corpus: A new approach using xlm-roberta alignment. In Cross-lingual named entity recognition using parallel corpus: A new approach using xlm-roberta alignment. arXiv:2101.11112\nLi, H., Arora, A., Chen, S., Gupta, A., Gupta, S., and Mehdad, Y. 2021. MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 2950--2962\nLiang, Y., Duan, N., Gong, Y., Wu, N., Guo, F., Qi, W., Gong, M., Shou, L., Jiang, D., and Cao, G. 2020. Xglue: A new benchmark datasetfor cross-lingual pre-training, understanding and generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6008--6018\nVan Den Oord, A., Li, Y., and Vinyals, O. 2018. Representation learning with contrastive predictive coding. In Representation learning with contrastive predictive coding. arXiv:1807.03748\nPan, L., Hang, C., Qi, H., Shah, A., Yu, M., and Potdar, S. 2020. Multilingual bert post-pretraining alignment. In Multilingual bert post-pretraining alignment. arXiv:2010.12547\nQi, K. and Du, J. 2020. Translation-based matching adversarial network for cross-lingual natural language inference. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8632--8639\nQin, L., Ni, M., Zhang, Y., and Che, W. 2020. Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp. In Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp. arXiv:2006.06402\nRazumovskaia, E., Glava\u0161, G., Majewska, O., Korhonen, A., and Vuli\u0107, I. 2021. Crossing the conversational chasm: A primer on multilingual task-oriented dialogue systems. In Crossing the conversational chasm: A primer on multilingual task-oriented dialogue systems. arXiv:2104.08570\nErikTjong, Sang, K., and De Meulder, F. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003. pp. 142--147\nSchuster, S., Gupta, S., Shah, R., and Lewis, M. 2019. Cross-lingual transfer learning for multilingual task oriented dialog. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3795--3805\nSiddhant, A., Johnson, M., Tsai, H., Ari, N., Riesa, J., Bapna, A., Firat, O., and Raman, K. 2020. Evaluating the cross-lingual effectiveness of massively multilingual neural machine translation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8854--8861\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.R. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems. pp. 32\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.R. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv:1804.07461\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., and Funtowicz, M. 2019. Huggingface's transformers: State-ofthe-art natural language processing. In ArXiv. pp. 1910\nWu, C., Wu, F., and Huang, Y. 2021. Rethinking infonce: How many negative samples do you need? arXiv preprint. In Rethinking infonce: How many negative samples do you need? arXiv preprint. arXiv:2105.13003\nXia, M., Zheng, G., Mukherjee, S., and Shokouhi, M. Graham Neubig, and Ahmed Hassan Awadallah. 2021. Metaxl: Meta representation transformation for low-resource cross-lingual learning. In Graham Neubig, and Ahmed Hassan Awadallah. 2021. Metaxl: Meta representation transformation for low-resource cross-lingual learning. arXiv:2104.07908\nXu, W., Haider, B., and Mansour, S. 2020. End-to-end slot alignment and recognition for crosslingual nlu. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 5052--5063\nYi, H. and Cheng, J. 2021. Zero-shot entity recognition via multi-source projection and unlabeled data. In IOP Conference Series: Earth and Environmental Science. pp. 12084"
    }
]