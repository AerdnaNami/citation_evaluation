[
    {
        "filename": "paper_6.txt",
        "start": 1790,
        "end": 2025,
        "label": "Lacks synthesis",
        "text": "There are growing interests recently to incorporate prompts on pre-trained language models in order to guide the models' behavior or elicit knowledge (Shin et al., 2020;Schick and Sch\u00fctze, 2021;Qin and Eisner, 2021;Scao and Rush, 2021)",
        "full_text": "Related Work\n\nZero-shot cross-lingual structured prediction. Zero-shot cross-lingual learning becomes an emerging research topic as it eliminates the requirement of labeled data for training models in low-resource languages. Various structured prediction tasks have be studied, including named entity recognition (Pan et al., 2017;Hu et al., 2020), dependency parsing (Ahmad et al., 2019, relation extraction (Zou et al., 2018;Ni and Florian, 2019), and event argument extraction (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). Most of them are classification-based models that build classifiers on top of a multilingual pre-trained masked language models. To further deal with the discrepancy between languages, some of them require additional information, such as bilingual dictionaries (Liu et al., 2019; Ni and Florian, 2019), translation pairs (Zou et al., 2018), and dependency parse trees (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). However, as pointed out by previous literature (Li et al., 2021;Hsu et al., 2021), classification-based models are less powerful to model dependencies between entities compared to generation-based models.\n\nGeneration-based structured prediction. Several works have demonstrated the great success of generation-based models on monolingual structured prediction tasks, including named entity recognition (Yan et al., 2021), relation extraction (Huang et al., 2021;Paolini et al., 2021), and event extraction (Du et al., 2021;Hsu et al., 2021;Lu et al., 2021). Yet, as mentioned in Section 1, their designed generating targets are language-dependent. Accordingly, directly applying their methods to the zero-shot cross-lingual setting would result in less-preferred performance.\n\nPrompting methods. There are growing interests recently to incorporate prompts on pre-trained language models in order to guide the models' behavior or elicit knowledge (Shin et al., 2020;Schick and Sch\u00fctze, 2021;Qin and Eisner, 2021;Scao and Rush, 2021). Following the taxonomy in (Liu et al., 2021), these methods can be classified depending on whether the language models' parameters are tuned and on whether trainable prompts are introduced. Our method belongs to the category that fixes the prompts and tune the language models' parameters. Despite the flourish of the research in prompting methods, there is only limited attention being put on multilingual tasks (Winata et al., 2021).\n\n References: \nWasi Uddin Ahmad, N., Peng, K., and Chang 2021. GATE: graph attention transformer encoder for cross-lingual relation and event extraction. In Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI).\nWasi Uddin Ahmad, Z., Zhang, X., Ma, E. H., Hovy, K., Chang, N., and Peng 2019. On difficulties of cross-lingual transfer with order differences: A case study on dependency parsing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\nDe Cao, N., Wu, L., Popat, K., Artetxe, M., Goyal, N., Plekhanov, M., Zettlemoyer, L., Cancedda, N., Riedel, S., and Petroni, F. 2021. Multilingual autoregressive entity linking. In Multilingual autoregressive entity linking. arXiv:2103.12528\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm\u00e1n, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).\nDoddington, G. R., Mitchell, A., Przybocki, M. A., Ramshaw, L. A., Strassel, S. M., and Weischedel, R. M. 2004. The automatic content extraction (ACE) program -tasks, data, and evaluation. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC).\nDu, X., Rush, A. M., and Cardie, C. 2021. GRIT: generative role-filler transformers for document-level event entity extraction. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL).\nHsu, I., Huang, K., Boschee, E., Miller, S., Natarajan, P., Chang, K., and Peng, N. 2021. Degree: A data-efficient generative event extraction model. In Degree: A data-efficient generative event extraction model. arXiv:2108.12724\nHu, J., Ruder, S., Siddhant, A., Neubig, G., Firat, O., and Johnson, M. 2020. XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation. In Proceedings of the 37th International Conference on Machine Learning (ICML).\nHuang, K., Tang, S., and Peng, N. 2021. Document-level entity-based extraction as template generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.\nThomas, N., Kipf, M., and Welling 2017. Semisupervised classification with graph convolutional networks. In 5th International Conference on Learning Representations (ICLR).\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\nLi, S., Heng, J., and Han, J. 2021. Documentlevel event argument extraction by conditional generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\nLin, Y., Ji, H., Huang, F., and Wu, L. 2020. A joint neural model for information extraction with global features. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).\nLiu, J., Chen, Y., and Liu, K. 2019. Neural cross-lingual event detection with minimal parallel resources. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. In Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv:2107.13586\nLiu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., and Zettlemoyer, L. 2020. Multilingual denoising pre-training for neural machine translation. In Trans. Assoc. Comput. Linguistics. pp. 726--742\nLu, Y., Lin, H., Xu, J., Han, X., Tang, J., Li, A., Sun, L., Liao, M., and Chen, S. 2021. Text2event: Controllable sequence-tostructure generation for end-to-end event extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP).\nVan Nguyen, M. and Huu Nguyen, T. 2021. Improving cross-lingual transfer for event argument extraction with language-universal sentence structures. In Proceedings of the Sixth Arabic Natural Language Processing Workshop.\nNi, J. and Florian, R. 2019. Neural cross-lingual relation extraction based on bilingual word embedding mapping. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\nPan, X., Zhang, B., May, J., Nothman, J., Knight, K., and Ji, H. 2017. Crosslingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL).\nPaolini, G., Athiwaratkun, B., Krone, J., Ma, J., Achille, A., Anubhai, R., Nogueira, C., Santos, B., Xiang, S., and Soatto 2021. Structured prediction as translation between augmented natural languages. In 9th International Conference on Learning Representations.\nPeng, H., Parikh, A. P., Faruqui, M., Dhingra, B., and Das, D. 2019. Text generation with exemplar-based adaptive decoding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\nQi, P., Zhang, Y., Zhang, Y., Bolton, J., and Manning, C. D. 2020. Stanza: A python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL).\nQin, G. and Eisner, J. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res.\nTeven, L., Scao, A. M., and Rush 2021. How many data points is a prompt worth. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\nSchick, T. and Sch\u00fctze, H. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL).\nSee, A., Peter, J., Liu, C. D., and Manning 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.\nShin, T., Razeghi, Y., Logan, R. L., IV, Wallace, E., and Singh, S. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.\nSong, Z., Bies, A., Strassel, S. M., Riese, T., Mott, J., Ellis, J., Wright, J., Kulick, S., Ryant, N., and Ma, X. 2015. From light to rich ERE: annotation of entities, relations, and events. In Proceedings of the The 3rd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, (EVENTS@HLP-NAACL).\nSubburathinam, A., Lu, D., Ji, H., May, J., Chang, S., Sil, A., and ClareR\nVoss 2019. Cross-lingual structure transfer for relation and event extraction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\nTang, Y., Tran, C., Li, X., Chen, P., Goyal, N., Chaudhary, V., Gu, J., and Fan, A. 2020. Multilingual translation with extensible multilingual pretraining and finetuning. In Multilingual translation with extensible multilingual pretraining and finetuning. arXiv:2008.00401\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017 (NeurIPS).\nWadden, D., Wennberg, U., Luan, Y., and Hajishirzi, H. 2019. Entity, relation, and event extraction with contextualized span representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing.\nWang, X., Wang, Z., Han, X., Liu, Z., Li, J., Li, P., Sun, M., Zhou, J., and Ren, X. 2019. HMEAE: hierarchical modular event argument extraction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing.\nGenta Indra Winata, A., Madotto, Z., Lin, R., Liu, J., Yosinski, P., and Fung 2021. Language models are few-shot multilingual learners. In Language models are few-shot multilingual learners. arXiv:2109.07684\nXu, H., Ebner, S., Yarmohammadi, M., White, A. S., Van Durme, B., and Murray, K. W. 2021. Gradual fine-tuning for low-resource domain adaptation. In Gradual fine-tuning for low-resource domain adaptation. arXiv:2103.02205\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., and Siddhant, A. Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\nYan, H., Gui, T., Dai, J., Guo, Q., Zhang, Z., and Qiu, X. 2021. A unified generative framework for various NER subtasks. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP).\nZou, B., Xu, Z., Hong, Y., and Zhou, G. 2018. Adversarial feature adaptation for cross-lingual relation classification. In Proceedings of the 27th International Conference on Computational Linguistics (COLING)."
    },
    {
        "filename": "paper_6.txt",
        "start": 1642,
        "end": 1769,
        "label": "Unsupported claim",
        "text": "Accordingly, directly applying their methods to the zero-shot cross-lingual setting would result in less-preferred performance.",
        "full_text": "Related Work\n\nZero-shot cross-lingual structured prediction. Zero-shot cross-lingual learning becomes an emerging research topic as it eliminates the requirement of labeled data for training models in low-resource languages. Various structured prediction tasks have be studied, including named entity recognition (Pan et al., 2017;Hu et al., 2020), dependency parsing (Ahmad et al., 2019, relation extraction (Zou et al., 2018;Ni and Florian, 2019), and event argument extraction (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). Most of them are classification-based models that build classifiers on top of a multilingual pre-trained masked language models. To further deal with the discrepancy between languages, some of them require additional information, such as bilingual dictionaries (Liu et al., 2019; Ni and Florian, 2019), translation pairs (Zou et al., 2018), and dependency parse trees (Subburathinam et al., 2019;Ahmad et al., 2021;Nguyen and Nguyen, 2021). However, as pointed out by previous literature (Li et al., 2021;Hsu et al., 2021), classification-based models are less powerful to model dependencies between entities compared to generation-based models.\n\nGeneration-based structured prediction. Several works have demonstrated the great success of generation-based models on monolingual structured prediction tasks, including named entity recognition (Yan et al., 2021), relation extraction (Huang et al., 2021;Paolini et al., 2021), and event extraction (Du et al., 2021;Hsu et al., 2021;Lu et al., 2021). Yet, as mentioned in Section 1, their designed generating targets are language-dependent. Accordingly, directly applying their methods to the zero-shot cross-lingual setting would result in less-preferred performance.\n\nPrompting methods. There are growing interests recently to incorporate prompts on pre-trained language models in order to guide the models' behavior or elicit knowledge (Shin et al., 2020;Schick and Sch\u00fctze, 2021;Qin and Eisner, 2021;Scao and Rush, 2021). Following the taxonomy in (Liu et al., 2021), these methods can be classified depending on whether the language models' parameters are tuned and on whether trainable prompts are introduced. Our method belongs to the category that fixes the prompts and tune the language models' parameters. Despite the flourish of the research in prompting methods, there is only limited attention being put on multilingual tasks (Winata et al., 2021).\n\n References: \nWasi Uddin Ahmad, N., Peng, K., and Chang 2021. GATE: graph attention transformer encoder for cross-lingual relation and event extraction. In Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI).\nWasi Uddin Ahmad, Z., Zhang, X., Ma, E. H., Hovy, K., Chang, N., and Peng 2019. On difficulties of cross-lingual transfer with order differences: A case study on dependency parsing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\nDe Cao, N., Wu, L., Popat, K., Artetxe, M., Goyal, N., Plekhanov, M., Zettlemoyer, L., Cancedda, N., Riedel, S., and Petroni, F. 2021. Multilingual autoregressive entity linking. In Multilingual autoregressive entity linking. arXiv:2103.12528\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm\u00e1n, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).\nDoddington, G. R., Mitchell, A., Przybocki, M. A., Ramshaw, L. A., Strassel, S. M., and Weischedel, R. M. 2004. The automatic content extraction (ACE) program -tasks, data, and evaluation. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC).\nDu, X., Rush, A. M., and Cardie, C. 2021. GRIT: generative role-filler transformers for document-level event entity extraction. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL).\nHsu, I., Huang, K., Boschee, E., Miller, S., Natarajan, P., Chang, K., and Peng, N. 2021. Degree: A data-efficient generative event extraction model. In Degree: A data-efficient generative event extraction model. arXiv:2108.12724\nHu, J., Ruder, S., Siddhant, A., Neubig, G., Firat, O., and Johnson, M. 2020. XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation. In Proceedings of the 37th International Conference on Machine Learning (ICML).\nHuang, K., Tang, S., and Peng, N. 2021. Document-level entity-based extraction as template generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.\nThomas, N., Kipf, M., and Welling 2017. Semisupervised classification with graph convolutional networks. In 5th International Conference on Learning Representations (ICLR).\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\nLi, S., Heng, J., and Han, J. 2021. Documentlevel event argument extraction by conditional generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\nLin, Y., Ji, H., Huang, F., and Wu, L. 2020. A joint neural model for information extraction with global features. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).\nLiu, J., Chen, Y., and Liu, K. 2019. Neural cross-lingual event detection with minimal parallel resources. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. In Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv:2107.13586\nLiu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., and Zettlemoyer, L. 2020. Multilingual denoising pre-training for neural machine translation. In Trans. Assoc. Comput. Linguistics. pp. 726--742\nLu, Y., Lin, H., Xu, J., Han, X., Tang, J., Li, A., Sun, L., Liao, M., and Chen, S. 2021. Text2event: Controllable sequence-tostructure generation for end-to-end event extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP).\nVan Nguyen, M. and Huu Nguyen, T. 2021. Improving cross-lingual transfer for event argument extraction with language-universal sentence structures. In Proceedings of the Sixth Arabic Natural Language Processing Workshop.\nNi, J. and Florian, R. 2019. Neural cross-lingual relation extraction based on bilingual word embedding mapping. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\nPan, X., Zhang, B., May, J., Nothman, J., Knight, K., and Ji, H. 2017. Crosslingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL).\nPaolini, G., Athiwaratkun, B., Krone, J., Ma, J., Achille, A., Anubhai, R., Nogueira, C., Santos, B., Xiang, S., and Soatto 2021. Structured prediction as translation between augmented natural languages. In 9th International Conference on Learning Representations.\nPeng, H., Parikh, A. P., Faruqui, M., Dhingra, B., and Das, D. 2019. Text generation with exemplar-based adaptive decoding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\nQi, P., Zhang, Y., Zhang, Y., Bolton, J., and Manning, C. D. 2020. Stanza: A python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL).\nQin, G. and Eisner, J. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res.\nTeven, L., Scao, A. M., and Rush 2021. How many data points is a prompt worth. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\nSchick, T. and Sch\u00fctze, H. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL).\nSee, A., Peter, J., Liu, C. D., and Manning 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.\nShin, T., Razeghi, Y., Logan, R. L., IV, Wallace, E., and Singh, S. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.\nSong, Z., Bies, A., Strassel, S. M., Riese, T., Mott, J., Ellis, J., Wright, J., Kulick, S., Ryant, N., and Ma, X. 2015. From light to rich ERE: annotation of entities, relations, and events. In Proceedings of the The 3rd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, (EVENTS@HLP-NAACL).\nSubburathinam, A., Lu, D., Ji, H., May, J., Chang, S., Sil, A., and ClareR\nVoss 2019. Cross-lingual structure transfer for relation and event extraction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\nTang, Y., Tran, C., Li, X., Chen, P., Goyal, N., Chaudhary, V., Gu, J., and Fan, A. 2020. Multilingual translation with extensible multilingual pretraining and finetuning. In Multilingual translation with extensible multilingual pretraining and finetuning. arXiv:2008.00401\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017 (NeurIPS).\nWadden, D., Wennberg, U., Luan, Y., and Hajishirzi, H. 2019. Entity, relation, and event extraction with contextualized span representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing.\nWang, X., Wang, Z., Han, X., Liu, Z., Li, J., Li, P., Sun, M., Zhou, J., and Ren, X. 2019. HMEAE: hierarchical modular event argument extraction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing.\nGenta Indra Winata, A., Madotto, Z., Lin, R., Liu, J., Yosinski, P., and Fung 2021. Language models are few-shot multilingual learners. In Language models are few-shot multilingual learners. arXiv:2109.07684\nXu, H., Ebner, S., Yarmohammadi, M., White, A. S., Van Durme, B., and Murray, K. W. 2021. Gradual fine-tuning for low-resource domain adaptation. In Gradual fine-tuning for low-resource domain adaptation. arXiv:2103.02205\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., and Siddhant, A. Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\nYan, H., Gui, T., Dai, J., Guo, Q., Zhang, Z., and Qiu, X. 2021. A unified generative framework for various NER subtasks. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP).\nZou, B., Xu, Z., Hong, Y., and Zhou, G. 2018. Adversarial feature adaptation for cross-lingual relation classification. In Proceedings of the 27th International Conference on Computational Linguistics (COLING)."
    }
]