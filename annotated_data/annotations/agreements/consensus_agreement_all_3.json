[
    {
        "filename": "paper_2.txt",
        "label": "Lacks synthesis",
        "text": "Other proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two \"higher order\" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a \"meta-inventory\" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.",
        "start": 1365,
        "end": 2648,
        "spans_a": [
            {
                "filename": "paper_2.txt",
                "start": 1365,
                "end": 2648,
                "label": "Lacks synthesis",
                "text": "Other proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two \"higher order\" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a \"meta-inventory\" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.",
                "full_text": "Background\n\nHuman values are of concern to most if not to all social sciences (Rokeach, 1973) and have also been integrated into computational frameworks of argumentation (Bench-Capon, 2003). In NLP, values have been analyzed for personality profiling (Maheshwari et al., 2017), but not yet for argument mining, as considered here.\n\nValues in Social Science\n\nRokeach (1973) already described the two concepts of (1) a value as a belief pertaining to desirable end states or modes of conduct and (2) a value system as prioritization of values based on cultural, social, and personal factors. These definitions attribute values to persons rather than to objects, facilitating a systematic analysis (Rokeach, 1973). The paper at hand targets the personal values behind arguments, meaning the values in the former sense that the arguments, mostly implicitly, resort to.\n\nSeveral of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.\n\nOther proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two \"higher order\" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a \"meta-inventory\" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.\n\nValues in Argumentation Research\n\nFormal argumentation employs value systems to model audience-specific preferences, that is, an argument's strength depends on the degree to which the audience reveres the values the argument resorts to. Examples include value-based argumentation schemes (van der Weide et al., 2009), defeasible logic programming (Teze et al., 2019), and the value-based argumentation framework of Bench-Capon (2003). The latter is an extension of the abstract argumentation framework of Dung (1995) that has already been applied manually to analyze interactions with reasoning and persuasion subject to a specific value system (Atkinson and Bench-Capon, 2021). This paper present a first step towards the large-scale automatic application of these works as it takes values to argument mining. Feldman (2021) recently showed the strong connection between values and the moral foundation theory (Haidt, 2012). Like personal values, this theory analyzes ethical reasoning behind human choices, but considers five rather abstract \"foundations:\" care, fairness, loyalty, authority, and purity. Alshomary and Wachsmuth (2021) hypothesized that the foundations could be used for audiencespecific argument generation. Kobbe et al. (2020) tried to classify arguments by foundations, but noted a low human agreement due to the vagueness of the foundations. We assume values can here contribute to the classification by foundations.\n\nPartly, values overlap with ideas of framing in communication, that is, the selection and emphasis of specific aspects of (perceived) reality to promote a particular problem definition, causal interpretation, ethical evaluation, and/or recommendation (Entman, 1993). In frames, values can define the costs and benefits of options (Entman, 1993) whereas common value systems are used for evaluation. Frames have been studied computationally for news (Naderi and Hirst, 2015), political speech (De Vreese, 2005), and argumentation (Ajjour et al., 2019). In argumentation, some values are so prevalent that they constitute frames of their own, indicating a potential use of values in frame identification. For example, 14 of the 54 values we use in this work are also frames in the dataset of Ajjour et al. 1 Values may be considered as aspects under which to group arguments. Some researchers have mined aspects from text (Trautmann, 2020) or used them to control argument generation (Schiller et al., 2021). Others have studied the task of opinion summarization in arguments (Egan et al., 2016;Misra et al., 2016;Chen et al., 2019), aiming at the most important aspects discussed in a debate. Related, the task of key point analysis (Bar-Haim et al., 2020;Friedman et al., 2021) is to generate a small set of concise statements that each represent a different aspect. We argue that analyzing the values found in a collection of arguments provides a new perspective to aspects in argumentation, focusing on the \"why\" behind an argument's reasoning.\n\n References: \nAjjour, Y., Alshomary, M., Wachsmuth, H., and Stein, B. 2019. Modeling Frames in Argumentation. In 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP 2019). pp. 2922--2932\nAlshomary, M. and Wachsmuth, H. 2021. Toward audience-aware argument generation. In Patterns. pp. 100253 10.1016/j.patter.2021.100253\nKatie Atkinson and Trevor Bench-Capon. 2021. Valuebased argumentation. In Journal of Applied Logics. pp. 1543--1588\nBabbar, R., Partalas, I., Gaussier, E., and Amini, M. 2013. On flat versus hierarchical classification in large-scale taxonomies. In 27th Annual Conference on Neural Information Processing Systems (NIPS 2013). pp. 1824--1832\nBar-Haim, R., Eden, L., Friedman, R., Kantor, Y., Lahav, D., and Slonim, N. 2020. From arguments to key points: Towards automatic argument summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4029--4039 10.18653/v1/2020.acl-main.371\nTrevor, J. M. 2003. Persuasion in practical argument using value-based argumentation frameworks. In J. Log. Comput. pp. 429--448 10.1093/logcom/13.3.429\nTrevor, J. M. 2021. Bench-Capon. 2021. Audiences and argument strength. In 3rd Workshop on Argument Strength.\nBrown, D. and Kelly Crace, R. 2002. Life values inventory facilitator's guide. In Life values inventory facilitator's guide.\nChen, S., Khashabi, D., Yin, W., Callison-Burch, C., and Roth, D. 2019. Seeing things from a different angle:discovering diverse perspectives about claims. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019). pp. 542--557 10.18653/v1/N19-1053\nCheng, A. and Fleischmann, K. R. 2010. Developing a meta-inventory of human values. In 73rd ASIS&T Annual Meeting (ASIST 2010). pp. 1--10 10.1002/meet.14504701232\nVreese, C.H.D. 2005. News framing: Theory and typology. Information design journal & document design. In News framing: Theory and typology. Information design journal & document design. pp. 13\nPhan Minh, D. 1995. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. In Artificial Intelligence. pp. 321--357 10.1016/0004-3702(94)00041-X\nEgan, C., Siddharthan, A., and Wyner, A. Z. 2016. Summarising the points made in online political debates. In Proceedings of the Third Workshop on Argument Mining, hosted by the 54th Annual Meeting of the Association for Computational Linguistics, ArgMining@ACL 2016. 10.18653/v1/w16-2816\nEngland, G. W. 1967. Personal value systems of american managers. In Academy of Management journal. pp. 53--68\nRobert M Entman 1993. Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. In Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. pp. 390--397\nFeldman, G. 2021. Personal values and moral foundations: Examining relations and joint prediction of moral variables. Social Psychological and Personality. In Science. pp. 676--686\nFriedman, R., Dankin, L., Katz, Y., Hou, Y., and Slonim, N. 2021. Overview of KPA-2021 shared task: Key point based quantitative summarization. In Proceedings of the 8th Workshop on Argumentation Mining.\nGretz, S., Friedman, R., Cohen-Karlik, E., Toledo, A., Lahav, D., Aharonov, R., and Slonim, N. 2020. A large-scale dataset for argument quality ranking: Construction and analysis. In 34th AAAI Conference on Artificial Intelligence (AAAI 2020). pp. 7805--7813 10.1609/aaai.v34i05.6285\nHaerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova, K., Diez-Medrano, J., Lagos, M., Norris, P., Ponarin, E., and Puranen, B. 2020. World values survey. In World values survey. 10.14281/18241.13\nHaidt, J. 2012. The righteous mind: Why good people are divided by politics and religion. In The righteous mind: Why good people are divided by politics and religion.\nHovy, D., Berg-Kirkpatrick, T., Vaswani, A., and Hovy, E. 2013. Learning whom to trust with mace. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013). pp. 1120--1130\nLynn R Kahle, B., Poulos, A., and Sukhdial 1988. Changes in social values in the united states during the past decade. In Journal of Advertising Research. pp. 35--41\nKobbe, J., Rehbein, I., Hulpus, I., and Stuckenschmidt, H. 2020. Exploring morality in argumentation. In Proceedings of the 7th Workshop on Argument Mining. pp. 30--40\nKrippendorff, K. 2004. Measuring the reliability of qualitative text analysis data. In Quality & quantity. pp. 787--800 10.1007/s11135-004-8107-7\nLoza Menc\u00eda, E. and Jannsen, F. 2016. Learning rules for multi-label classification: a stacking and a separate-and-conquer approach. In Machine Learning. pp. 77--126 10.1007/s10994-016-5552-1\nMaheshwari, T., Reganti, A. N., Gupta, S., Jamatia, A., Kumar, U., Gamb\u00e4ck, B., and Das, A. 2017. A societal sentiment analysis: Predicting the values and ethics of individuals by analysing social media content. In 15th Conference of the European Chapter of the Association for Computational Linguistics. pp. 731--741 10.18653/v1/e17-1069\nMisra, A., Ecker, B., and Walker, M. 2016. Measuring the similarity of sentential arguments in dialogue. In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 276--287 10.18653/v1/W16-3636\nNaderi, N. and Hirst, G. 2015. Argumentation mining in parliamentary discourse. In Principles and practice of multi-agent systems. pp. 16--25\nRokeach, M. 1973. The nature of human values. In The nature of human values.\nSchiller, B., Daxenberger, J., and Gurevych, I. 2021. Aspect-controlled neural argument generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 380--396 10.18653/v1/2021.naacl-main.34\nShalom, H. and Schwartz 1994. Are there universal aspects in the structure and contents of human values?. In Journal of Social Issues. pp. 19--45 10.1111/j.1540-4560.1994.tb01196.x\nShalom, H., Schwartz, J., Cieciuch, M., Vecchione, E., Davidov, R., Fischer, C., Beierlein, A., Ramos, M., Verkasalo, J., L\u00f6nnqvist, K., and Demirutku 2012. Refining the theory of basic individual values. In Journal of personality and social psychology. pp. 103 10.1037/a0029393\nJohn R Searle 2003. Rationality in action. In Rationality in action.\nCarlos Teze, J., Perello-Moragues, A., Godo, L., and Noriega, P. 2019. Practical reasoning using values: an argumentative approach based on a hierarchy of values. In Annals of Mathematics and Artificial Intelligence. pp. 293--319 10.1007/s10472-019-09660-8\nTrautmann, D. 2020. Aspect-based argument mining. In Proceedings of the 7th Workshop on Argument Mining. pp. 41--52\nThomas, L., Van Der Weide, F., Dignum, J., Ch, Meyer, H., Prakken, G., and Vreeswijk 2009. Practical reasoning using values. In Argumentation in Multi-Agent Systems. pp. 79--93 10.1007/978-3-642-12805-9_5\nWilcox, R. R. 1996. Statistics for the Social Sciences. In Statistics for the Social Sciences."
            }
        ],
        "spans_b": [
            {
                "filename": "paper_2.txt",
                "start": 867,
                "end": 2648,
                "label": "Lacks synthesis",
                "text": "Several of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.\n\nOther proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two \"higher order\" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a \"meta-inventory\" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.",
                "full_text": "Background\n\nHuman values are of concern to most if not to all social sciences (Rokeach, 1973) and have also been integrated into computational frameworks of argumentation (Bench-Capon, 2003). In NLP, values have been analyzed for personality profiling (Maheshwari et al., 2017), but not yet for argument mining, as considered here.\n\nValues in Social Science\n\nRokeach (1973) already described the two concepts of (1) a value as a belief pertaining to desirable end states or modes of conduct and (2) a value system as prioritization of values based on cultural, social, and personal factors. These definitions attribute values to persons rather than to objects, facilitating a systematic analysis (Rokeach, 1973). The paper at hand targets the personal values behind arguments, meaning the values in the former sense that the arguments, mostly implicitly, resort to.\n\nSeveral of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.\n\nOther proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two \"higher order\" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a \"meta-inventory\" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.\n\nValues in Argumentation Research\n\nFormal argumentation employs value systems to model audience-specific preferences, that is, an argument's strength depends on the degree to which the audience reveres the values the argument resorts to. Examples include value-based argumentation schemes (van der Weide et al., 2009), defeasible logic programming (Teze et al., 2019), and the value-based argumentation framework of Bench-Capon (2003). The latter is an extension of the abstract argumentation framework of Dung (1995) that has already been applied manually to analyze interactions with reasoning and persuasion subject to a specific value system (Atkinson and Bench-Capon, 2021). This paper present a first step towards the large-scale automatic application of these works as it takes values to argument mining. Feldman (2021) recently showed the strong connection between values and the moral foundation theory (Haidt, 2012). Like personal values, this theory analyzes ethical reasoning behind human choices, but considers five rather abstract \"foundations:\" care, fairness, loyalty, authority, and purity. Alshomary and Wachsmuth (2021) hypothesized that the foundations could be used for audiencespecific argument generation. Kobbe et al. (2020) tried to classify arguments by foundations, but noted a low human agreement due to the vagueness of the foundations. We assume values can here contribute to the classification by foundations.\n\nPartly, values overlap with ideas of framing in communication, that is, the selection and emphasis of specific aspects of (perceived) reality to promote a particular problem definition, causal interpretation, ethical evaluation, and/or recommendation (Entman, 1993). In frames, values can define the costs and benefits of options (Entman, 1993) whereas common value systems are used for evaluation. Frames have been studied computationally for news (Naderi and Hirst, 2015), political speech (De Vreese, 2005), and argumentation (Ajjour et al., 2019). In argumentation, some values are so prevalent that they constitute frames of their own, indicating a potential use of values in frame identification. For example, 14 of the 54 values we use in this work are also frames in the dataset of Ajjour et al. 1 Values may be considered as aspects under which to group arguments. Some researchers have mined aspects from text (Trautmann, 2020) or used them to control argument generation (Schiller et al., 2021). Others have studied the task of opinion summarization in arguments (Egan et al., 2016;Misra et al., 2016;Chen et al., 2019), aiming at the most important aspects discussed in a debate. Related, the task of key point analysis (Bar-Haim et al., 2020;Friedman et al., 2021) is to generate a small set of concise statements that each represent a different aspect. We argue that analyzing the values found in a collection of arguments provides a new perspective to aspects in argumentation, focusing on the \"why\" behind an argument's reasoning.\n\n References: \nAjjour, Y., Alshomary, M., Wachsmuth, H., and Stein, B. 2019. Modeling Frames in Argumentation. In 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP 2019). pp. 2922--2932\nAlshomary, M. and Wachsmuth, H. 2021. Toward audience-aware argument generation. In Patterns. pp. 100253 10.1016/j.patter.2021.100253\nKatie Atkinson and Trevor Bench-Capon. 2021. Valuebased argumentation. In Journal of Applied Logics. pp. 1543--1588\nBabbar, R., Partalas, I., Gaussier, E., and Amini, M. 2013. On flat versus hierarchical classification in large-scale taxonomies. In 27th Annual Conference on Neural Information Processing Systems (NIPS 2013). pp. 1824--1832\nBar-Haim, R., Eden, L., Friedman, R., Kantor, Y., Lahav, D., and Slonim, N. 2020. From arguments to key points: Towards automatic argument summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4029--4039 10.18653/v1/2020.acl-main.371\nTrevor, J. M. 2003. Persuasion in practical argument using value-based argumentation frameworks. In J. Log. Comput. pp. 429--448 10.1093/logcom/13.3.429\nTrevor, J. M. 2021. Bench-Capon. 2021. Audiences and argument strength. In 3rd Workshop on Argument Strength.\nBrown, D. and Kelly Crace, R. 2002. Life values inventory facilitator's guide. In Life values inventory facilitator's guide.\nChen, S., Khashabi, D., Yin, W., Callison-Burch, C., and Roth, D. 2019. Seeing things from a different angle:discovering diverse perspectives about claims. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019). pp. 542--557 10.18653/v1/N19-1053\nCheng, A. and Fleischmann, K. R. 2010. Developing a meta-inventory of human values. In 73rd ASIS&T Annual Meeting (ASIST 2010). pp. 1--10 10.1002/meet.14504701232\nVreese, C.H.D. 2005. News framing: Theory and typology. Information design journal & document design. In News framing: Theory and typology. Information design journal & document design. pp. 13\nPhan Minh, D. 1995. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. In Artificial Intelligence. pp. 321--357 10.1016/0004-3702(94)00041-X\nEgan, C., Siddharthan, A., and Wyner, A. Z. 2016. Summarising the points made in online political debates. In Proceedings of the Third Workshop on Argument Mining, hosted by the 54th Annual Meeting of the Association for Computational Linguistics, ArgMining@ACL 2016. 10.18653/v1/w16-2816\nEngland, G. W. 1967. Personal value systems of american managers. In Academy of Management journal. pp. 53--68\nRobert M Entman 1993. Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. In Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. pp. 390--397\nFeldman, G. 2021. Personal values and moral foundations: Examining relations and joint prediction of moral variables. Social Psychological and Personality. In Science. pp. 676--686\nFriedman, R., Dankin, L., Katz, Y., Hou, Y., and Slonim, N. 2021. Overview of KPA-2021 shared task: Key point based quantitative summarization. In Proceedings of the 8th Workshop on Argumentation Mining.\nGretz, S., Friedman, R., Cohen-Karlik, E., Toledo, A., Lahav, D., Aharonov, R., and Slonim, N. 2020. A large-scale dataset for argument quality ranking: Construction and analysis. In 34th AAAI Conference on Artificial Intelligence (AAAI 2020). pp. 7805--7813 10.1609/aaai.v34i05.6285\nHaerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova, K., Diez-Medrano, J., Lagos, M., Norris, P., Ponarin, E., and Puranen, B. 2020. World values survey. In World values survey. 10.14281/18241.13\nHaidt, J. 2012. The righteous mind: Why good people are divided by politics and religion. In The righteous mind: Why good people are divided by politics and religion.\nHovy, D., Berg-Kirkpatrick, T., Vaswani, A., and Hovy, E. 2013. Learning whom to trust with mace. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013). pp. 1120--1130\nLynn R Kahle, B., Poulos, A., and Sukhdial 1988. Changes in social values in the united states during the past decade. In Journal of Advertising Research. pp. 35--41\nKobbe, J., Rehbein, I., Hulpus, I., and Stuckenschmidt, H. 2020. Exploring morality in argumentation. In Proceedings of the 7th Workshop on Argument Mining. pp. 30--40\nKrippendorff, K. 2004. Measuring the reliability of qualitative text analysis data. In Quality & quantity. pp. 787--800 10.1007/s11135-004-8107-7\nLoza Menc\u00eda, E. and Jannsen, F. 2016. Learning rules for multi-label classification: a stacking and a separate-and-conquer approach. In Machine Learning. pp. 77--126 10.1007/s10994-016-5552-1\nMaheshwari, T., Reganti, A. N., Gupta, S., Jamatia, A., Kumar, U., Gamb\u00e4ck, B., and Das, A. 2017. A societal sentiment analysis: Predicting the values and ethics of individuals by analysing social media content. In 15th Conference of the European Chapter of the Association for Computational Linguistics. pp. 731--741 10.18653/v1/e17-1069\nMisra, A., Ecker, B., and Walker, M. 2016. Measuring the similarity of sentential arguments in dialogue. In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 276--287 10.18653/v1/W16-3636\nNaderi, N. and Hirst, G. 2015. Argumentation mining in parliamentary discourse. In Principles and practice of multi-agent systems. pp. 16--25\nRokeach, M. 1973. The nature of human values. In The nature of human values.\nSchiller, B., Daxenberger, J., and Gurevych, I. 2021. Aspect-controlled neural argument generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 380--396 10.18653/v1/2021.naacl-main.34\nShalom, H. and Schwartz 1994. Are there universal aspects in the structure and contents of human values?. In Journal of Social Issues. pp. 19--45 10.1111/j.1540-4560.1994.tb01196.x\nShalom, H., Schwartz, J., Cieciuch, M., Vecchione, E., Davidov, R., Fischer, C., Beierlein, A., Ramos, M., Verkasalo, J., L\u00f6nnqvist, K., and Demirutku 2012. Refining the theory of basic individual values. In Journal of personality and social psychology. pp. 103 10.1037/a0029393\nJohn R Searle 2003. Rationality in action. In Rationality in action.\nCarlos Teze, J., Perello-Moragues, A., Godo, L., and Noriega, P. 2019. Practical reasoning using values: an argumentative approach based on a hierarchy of values. In Annals of Mathematics and Artificial Intelligence. pp. 293--319 10.1007/s10472-019-09660-8\nTrautmann, D. 2020. Aspect-based argument mining. In Proceedings of the 7th Workshop on Argument Mining. pp. 41--52\nThomas, L., Van Der Weide, F., Dignum, J., Ch, Meyer, H., Prakken, G., and Vreeswijk 2009. Practical reasoning using values. In Argumentation in Multi-Agent Systems. pp. 79--93 10.1007/978-3-642-12805-9_5\nWilcox, R. R. 1996. Statistics for the Social Sciences. In Statistics for the Social Sciences."
            }
        ]
    },
    {
        "filename": "paper_2.txt",
        "label": "Lacks synthesis",
        "text": "Several of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.",
        "start": 867,
        "end": 1364,
        "spans_a": [
            {
                "filename": "paper_2.txt",
                "start": 867,
                "end": 1364,
                "label": "Lacks synthesis",
                "text": "Several of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.",
                "full_text": "Background\n\nHuman values are of concern to most if not to all social sciences (Rokeach, 1973) and have also been integrated into computational frameworks of argumentation (Bench-Capon, 2003). In NLP, values have been analyzed for personality profiling (Maheshwari et al., 2017), but not yet for argument mining, as considered here.\n\nValues in Social Science\n\nRokeach (1973) already described the two concepts of (1) a value as a belief pertaining to desirable end states or modes of conduct and (2) a value system as prioritization of values based on cultural, social, and personal factors. These definitions attribute values to persons rather than to objects, facilitating a systematic analysis (Rokeach, 1973). The paper at hand targets the personal values behind arguments, meaning the values in the former sense that the arguments, mostly implicitly, resort to.\n\nSeveral of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.\n\nOther proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two \"higher order\" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a \"meta-inventory\" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.\n\nValues in Argumentation Research\n\nFormal argumentation employs value systems to model audience-specific preferences, that is, an argument's strength depends on the degree to which the audience reveres the values the argument resorts to. Examples include value-based argumentation schemes (van der Weide et al., 2009), defeasible logic programming (Teze et al., 2019), and the value-based argumentation framework of Bench-Capon (2003). The latter is an extension of the abstract argumentation framework of Dung (1995) that has already been applied manually to analyze interactions with reasoning and persuasion subject to a specific value system (Atkinson and Bench-Capon, 2021). This paper present a first step towards the large-scale automatic application of these works as it takes values to argument mining. Feldman (2021) recently showed the strong connection between values and the moral foundation theory (Haidt, 2012). Like personal values, this theory analyzes ethical reasoning behind human choices, but considers five rather abstract \"foundations:\" care, fairness, loyalty, authority, and purity. Alshomary and Wachsmuth (2021) hypothesized that the foundations could be used for audiencespecific argument generation. Kobbe et al. (2020) tried to classify arguments by foundations, but noted a low human agreement due to the vagueness of the foundations. We assume values can here contribute to the classification by foundations.\n\nPartly, values overlap with ideas of framing in communication, that is, the selection and emphasis of specific aspects of (perceived) reality to promote a particular problem definition, causal interpretation, ethical evaluation, and/or recommendation (Entman, 1993). In frames, values can define the costs and benefits of options (Entman, 1993) whereas common value systems are used for evaluation. Frames have been studied computationally for news (Naderi and Hirst, 2015), political speech (De Vreese, 2005), and argumentation (Ajjour et al., 2019). In argumentation, some values are so prevalent that they constitute frames of their own, indicating a potential use of values in frame identification. For example, 14 of the 54 values we use in this work are also frames in the dataset of Ajjour et al. 1 Values may be considered as aspects under which to group arguments. Some researchers have mined aspects from text (Trautmann, 2020) or used them to control argument generation (Schiller et al., 2021). Others have studied the task of opinion summarization in arguments (Egan et al., 2016;Misra et al., 2016;Chen et al., 2019), aiming at the most important aspects discussed in a debate. Related, the task of key point analysis (Bar-Haim et al., 2020;Friedman et al., 2021) is to generate a small set of concise statements that each represent a different aspect. We argue that analyzing the values found in a collection of arguments provides a new perspective to aspects in argumentation, focusing on the \"why\" behind an argument's reasoning.\n\n References: \nAjjour, Y., Alshomary, M., Wachsmuth, H., and Stein, B. 2019. Modeling Frames in Argumentation. In 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP 2019). pp. 2922--2932\nAlshomary, M. and Wachsmuth, H. 2021. Toward audience-aware argument generation. In Patterns. pp. 100253 10.1016/j.patter.2021.100253\nKatie Atkinson and Trevor Bench-Capon. 2021. Valuebased argumentation. In Journal of Applied Logics. pp. 1543--1588\nBabbar, R., Partalas, I., Gaussier, E., and Amini, M. 2013. On flat versus hierarchical classification in large-scale taxonomies. In 27th Annual Conference on Neural Information Processing Systems (NIPS 2013). pp. 1824--1832\nBar-Haim, R., Eden, L., Friedman, R., Kantor, Y., Lahav, D., and Slonim, N. 2020. From arguments to key points: Towards automatic argument summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4029--4039 10.18653/v1/2020.acl-main.371\nTrevor, J. M. 2003. Persuasion in practical argument using value-based argumentation frameworks. In J. Log. Comput. pp. 429--448 10.1093/logcom/13.3.429\nTrevor, J. M. 2021. Bench-Capon. 2021. Audiences and argument strength. In 3rd Workshop on Argument Strength.\nBrown, D. and Kelly Crace, R. 2002. Life values inventory facilitator's guide. In Life values inventory facilitator's guide.\nChen, S., Khashabi, D., Yin, W., Callison-Burch, C., and Roth, D. 2019. Seeing things from a different angle:discovering diverse perspectives about claims. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019). pp. 542--557 10.18653/v1/N19-1053\nCheng, A. and Fleischmann, K. R. 2010. Developing a meta-inventory of human values. In 73rd ASIS&T Annual Meeting (ASIST 2010). pp. 1--10 10.1002/meet.14504701232\nVreese, C.H.D. 2005. News framing: Theory and typology. Information design journal & document design. In News framing: Theory and typology. Information design journal & document design. pp. 13\nPhan Minh, D. 1995. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. In Artificial Intelligence. pp. 321--357 10.1016/0004-3702(94)00041-X\nEgan, C., Siddharthan, A., and Wyner, A. Z. 2016. Summarising the points made in online political debates. In Proceedings of the Third Workshop on Argument Mining, hosted by the 54th Annual Meeting of the Association for Computational Linguistics, ArgMining@ACL 2016. 10.18653/v1/w16-2816\nEngland, G. W. 1967. Personal value systems of american managers. In Academy of Management journal. pp. 53--68\nRobert M Entman 1993. Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. In Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. pp. 390--397\nFeldman, G. 2021. Personal values and moral foundations: Examining relations and joint prediction of moral variables. Social Psychological and Personality. In Science. pp. 676--686\nFriedman, R., Dankin, L., Katz, Y., Hou, Y., and Slonim, N. 2021. Overview of KPA-2021 shared task: Key point based quantitative summarization. In Proceedings of the 8th Workshop on Argumentation Mining.\nGretz, S., Friedman, R., Cohen-Karlik, E., Toledo, A., Lahav, D., Aharonov, R., and Slonim, N. 2020. A large-scale dataset for argument quality ranking: Construction and analysis. In 34th AAAI Conference on Artificial Intelligence (AAAI 2020). pp. 7805--7813 10.1609/aaai.v34i05.6285\nHaerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova, K., Diez-Medrano, J., Lagos, M., Norris, P., Ponarin, E., and Puranen, B. 2020. World values survey. In World values survey. 10.14281/18241.13\nHaidt, J. 2012. The righteous mind: Why good people are divided by politics and religion. In The righteous mind: Why good people are divided by politics and religion.\nHovy, D., Berg-Kirkpatrick, T., Vaswani, A., and Hovy, E. 2013. Learning whom to trust with mace. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013). pp. 1120--1130\nLynn R Kahle, B., Poulos, A., and Sukhdial 1988. Changes in social values in the united states during the past decade. In Journal of Advertising Research. pp. 35--41\nKobbe, J., Rehbein, I., Hulpus, I., and Stuckenschmidt, H. 2020. Exploring morality in argumentation. In Proceedings of the 7th Workshop on Argument Mining. pp. 30--40\nKrippendorff, K. 2004. Measuring the reliability of qualitative text analysis data. In Quality & quantity. pp. 787--800 10.1007/s11135-004-8107-7\nLoza Menc\u00eda, E. and Jannsen, F. 2016. Learning rules for multi-label classification: a stacking and a separate-and-conquer approach. In Machine Learning. pp. 77--126 10.1007/s10994-016-5552-1\nMaheshwari, T., Reganti, A. N., Gupta, S., Jamatia, A., Kumar, U., Gamb\u00e4ck, B., and Das, A. 2017. A societal sentiment analysis: Predicting the values and ethics of individuals by analysing social media content. In 15th Conference of the European Chapter of the Association for Computational Linguistics. pp. 731--741 10.18653/v1/e17-1069\nMisra, A., Ecker, B., and Walker, M. 2016. Measuring the similarity of sentential arguments in dialogue. In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 276--287 10.18653/v1/W16-3636\nNaderi, N. and Hirst, G. 2015. Argumentation mining in parliamentary discourse. In Principles and practice of multi-agent systems. pp. 16--25\nRokeach, M. 1973. The nature of human values. In The nature of human values.\nSchiller, B., Daxenberger, J., and Gurevych, I. 2021. Aspect-controlled neural argument generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 380--396 10.18653/v1/2021.naacl-main.34\nShalom, H. and Schwartz 1994. Are there universal aspects in the structure and contents of human values?. In Journal of Social Issues. pp. 19--45 10.1111/j.1540-4560.1994.tb01196.x\nShalom, H., Schwartz, J., Cieciuch, M., Vecchione, E., Davidov, R., Fischer, C., Beierlein, A., Ramos, M., Verkasalo, J., L\u00f6nnqvist, K., and Demirutku 2012. Refining the theory of basic individual values. In Journal of personality and social psychology. pp. 103 10.1037/a0029393\nJohn R Searle 2003. Rationality in action. In Rationality in action.\nCarlos Teze, J., Perello-Moragues, A., Godo, L., and Noriega, P. 2019. Practical reasoning using values: an argumentative approach based on a hierarchy of values. In Annals of Mathematics and Artificial Intelligence. pp. 293--319 10.1007/s10472-019-09660-8\nTrautmann, D. 2020. Aspect-based argument mining. In Proceedings of the 7th Workshop on Argument Mining. pp. 41--52\nThomas, L., Van Der Weide, F., Dignum, J., Ch, Meyer, H., Prakken, G., and Vreeswijk 2009. Practical reasoning using values. In Argumentation in Multi-Agent Systems. pp. 79--93 10.1007/978-3-642-12805-9_5\nWilcox, R. R. 1996. Statistics for the Social Sciences. In Statistics for the Social Sciences."
            }
        ],
        "spans_b": [
            {
                "filename": "paper_2.txt",
                "start": 867,
                "end": 2648,
                "label": "Lacks synthesis",
                "text": "Several of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.\n\nOther proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two \"higher order\" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a \"meta-inventory\" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.",
                "full_text": "Background\n\nHuman values are of concern to most if not to all social sciences (Rokeach, 1973) and have also been integrated into computational frameworks of argumentation (Bench-Capon, 2003). In NLP, values have been analyzed for personality profiling (Maheshwari et al., 2017), but not yet for argument mining, as considered here.\n\nValues in Social Science\n\nRokeach (1973) already described the two concepts of (1) a value as a belief pertaining to desirable end states or modes of conduct and (2) a value system as prioritization of values based on cultural, social, and personal factors. These definitions attribute values to persons rather than to objects, facilitating a systematic analysis (Rokeach, 1973). The paper at hand targets the personal values behind arguments, meaning the values in the former sense that the arguments, mostly implicitly, resort to.\n\nSeveral of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.\n\nOther proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two \"higher order\" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a \"meta-inventory\" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.\n\nValues in Argumentation Research\n\nFormal argumentation employs value systems to model audience-specific preferences, that is, an argument's strength depends on the degree to which the audience reveres the values the argument resorts to. Examples include value-based argumentation schemes (van der Weide et al., 2009), defeasible logic programming (Teze et al., 2019), and the value-based argumentation framework of Bench-Capon (2003). The latter is an extension of the abstract argumentation framework of Dung (1995) that has already been applied manually to analyze interactions with reasoning and persuasion subject to a specific value system (Atkinson and Bench-Capon, 2021). This paper present a first step towards the large-scale automatic application of these works as it takes values to argument mining. Feldman (2021) recently showed the strong connection between values and the moral foundation theory (Haidt, 2012). Like personal values, this theory analyzes ethical reasoning behind human choices, but considers five rather abstract \"foundations:\" care, fairness, loyalty, authority, and purity. Alshomary and Wachsmuth (2021) hypothesized that the foundations could be used for audiencespecific argument generation. Kobbe et al. (2020) tried to classify arguments by foundations, but noted a low human agreement due to the vagueness of the foundations. We assume values can here contribute to the classification by foundations.\n\nPartly, values overlap with ideas of framing in communication, that is, the selection and emphasis of specific aspects of (perceived) reality to promote a particular problem definition, causal interpretation, ethical evaluation, and/or recommendation (Entman, 1993). In frames, values can define the costs and benefits of options (Entman, 1993) whereas common value systems are used for evaluation. Frames have been studied computationally for news (Naderi and Hirst, 2015), political speech (De Vreese, 2005), and argumentation (Ajjour et al., 2019). In argumentation, some values are so prevalent that they constitute frames of their own, indicating a potential use of values in frame identification. For example, 14 of the 54 values we use in this work are also frames in the dataset of Ajjour et al. 1 Values may be considered as aspects under which to group arguments. Some researchers have mined aspects from text (Trautmann, 2020) or used them to control argument generation (Schiller et al., 2021). Others have studied the task of opinion summarization in arguments (Egan et al., 2016;Misra et al., 2016;Chen et al., 2019), aiming at the most important aspects discussed in a debate. Related, the task of key point analysis (Bar-Haim et al., 2020;Friedman et al., 2021) is to generate a small set of concise statements that each represent a different aspect. We argue that analyzing the values found in a collection of arguments provides a new perspective to aspects in argumentation, focusing on the \"why\" behind an argument's reasoning.\n\n References: \nAjjour, Y., Alshomary, M., Wachsmuth, H., and Stein, B. 2019. Modeling Frames in Argumentation. In 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP 2019). pp. 2922--2932\nAlshomary, M. and Wachsmuth, H. 2021. Toward audience-aware argument generation. In Patterns. pp. 100253 10.1016/j.patter.2021.100253\nKatie Atkinson and Trevor Bench-Capon. 2021. Valuebased argumentation. In Journal of Applied Logics. pp. 1543--1588\nBabbar, R., Partalas, I., Gaussier, E., and Amini, M. 2013. On flat versus hierarchical classification in large-scale taxonomies. In 27th Annual Conference on Neural Information Processing Systems (NIPS 2013). pp. 1824--1832\nBar-Haim, R., Eden, L., Friedman, R., Kantor, Y., Lahav, D., and Slonim, N. 2020. From arguments to key points: Towards automatic argument summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4029--4039 10.18653/v1/2020.acl-main.371\nTrevor, J. M. 2003. Persuasion in practical argument using value-based argumentation frameworks. In J. Log. Comput. pp. 429--448 10.1093/logcom/13.3.429\nTrevor, J. M. 2021. Bench-Capon. 2021. Audiences and argument strength. In 3rd Workshop on Argument Strength.\nBrown, D. and Kelly Crace, R. 2002. Life values inventory facilitator's guide. In Life values inventory facilitator's guide.\nChen, S., Khashabi, D., Yin, W., Callison-Burch, C., and Roth, D. 2019. Seeing things from a different angle:discovering diverse perspectives about claims. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019). pp. 542--557 10.18653/v1/N19-1053\nCheng, A. and Fleischmann, K. R. 2010. Developing a meta-inventory of human values. In 73rd ASIS&T Annual Meeting (ASIST 2010). pp. 1--10 10.1002/meet.14504701232\nVreese, C.H.D. 2005. News framing: Theory and typology. Information design journal & document design. In News framing: Theory and typology. Information design journal & document design. pp. 13\nPhan Minh, D. 1995. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. In Artificial Intelligence. pp. 321--357 10.1016/0004-3702(94)00041-X\nEgan, C., Siddharthan, A., and Wyner, A. Z. 2016. Summarising the points made in online political debates. In Proceedings of the Third Workshop on Argument Mining, hosted by the 54th Annual Meeting of the Association for Computational Linguistics, ArgMining@ACL 2016. 10.18653/v1/w16-2816\nEngland, G. W. 1967. Personal value systems of american managers. In Academy of Management journal. pp. 53--68\nRobert M Entman 1993. Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. In Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. pp. 390--397\nFeldman, G. 2021. Personal values and moral foundations: Examining relations and joint prediction of moral variables. Social Psychological and Personality. In Science. pp. 676--686\nFriedman, R., Dankin, L., Katz, Y., Hou, Y., and Slonim, N. 2021. Overview of KPA-2021 shared task: Key point based quantitative summarization. In Proceedings of the 8th Workshop on Argumentation Mining.\nGretz, S., Friedman, R., Cohen-Karlik, E., Toledo, A., Lahav, D., Aharonov, R., and Slonim, N. 2020. A large-scale dataset for argument quality ranking: Construction and analysis. In 34th AAAI Conference on Artificial Intelligence (AAAI 2020). pp. 7805--7813 10.1609/aaai.v34i05.6285\nHaerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova, K., Diez-Medrano, J., Lagos, M., Norris, P., Ponarin, E., and Puranen, B. 2020. World values survey. In World values survey. 10.14281/18241.13\nHaidt, J. 2012. The righteous mind: Why good people are divided by politics and religion. In The righteous mind: Why good people are divided by politics and religion.\nHovy, D., Berg-Kirkpatrick, T., Vaswani, A., and Hovy, E. 2013. Learning whom to trust with mace. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013). pp. 1120--1130\nLynn R Kahle, B., Poulos, A., and Sukhdial 1988. Changes in social values in the united states during the past decade. In Journal of Advertising Research. pp. 35--41\nKobbe, J., Rehbein, I., Hulpus, I., and Stuckenschmidt, H. 2020. Exploring morality in argumentation. In Proceedings of the 7th Workshop on Argument Mining. pp. 30--40\nKrippendorff, K. 2004. Measuring the reliability of qualitative text analysis data. In Quality & quantity. pp. 787--800 10.1007/s11135-004-8107-7\nLoza Menc\u00eda, E. and Jannsen, F. 2016. Learning rules for multi-label classification: a stacking and a separate-and-conquer approach. In Machine Learning. pp. 77--126 10.1007/s10994-016-5552-1\nMaheshwari, T., Reganti, A. N., Gupta, S., Jamatia, A., Kumar, U., Gamb\u00e4ck, B., and Das, A. 2017. A societal sentiment analysis: Predicting the values and ethics of individuals by analysing social media content. In 15th Conference of the European Chapter of the Association for Computational Linguistics. pp. 731--741 10.18653/v1/e17-1069\nMisra, A., Ecker, B., and Walker, M. 2016. Measuring the similarity of sentential arguments in dialogue. In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 276--287 10.18653/v1/W16-3636\nNaderi, N. and Hirst, G. 2015. Argumentation mining in parliamentary discourse. In Principles and practice of multi-agent systems. pp. 16--25\nRokeach, M. 1973. The nature of human values. In The nature of human values.\nSchiller, B., Daxenberger, J., and Gurevych, I. 2021. Aspect-controlled neural argument generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 380--396 10.18653/v1/2021.naacl-main.34\nShalom, H. and Schwartz 1994. Are there universal aspects in the structure and contents of human values?. In Journal of Social Issues. pp. 19--45 10.1111/j.1540-4560.1994.tb01196.x\nShalom, H., Schwartz, J., Cieciuch, M., Vecchione, E., Davidov, R., Fischer, C., Beierlein, A., Ramos, M., Verkasalo, J., L\u00f6nnqvist, K., and Demirutku 2012. Refining the theory of basic individual values. In Journal of personality and social psychology. pp. 103 10.1037/a0029393\nJohn R Searle 2003. Rationality in action. In Rationality in action.\nCarlos Teze, J., Perello-Moragues, A., Godo, L., and Noriega, P. 2019. Practical reasoning using values: an argumentative approach based on a hierarchy of values. In Annals of Mathematics and Artificial Intelligence. pp. 293--319 10.1007/s10472-019-09660-8\nTrautmann, D. 2020. Aspect-based argument mining. In Proceedings of the 7th Workshop on Argument Mining. pp. 41--52\nThomas, L., Van Der Weide, F., Dignum, J., Ch, Meyer, H., Prakken, G., and Vreeswijk 2009. Practical reasoning using values. In Argumentation in Multi-Agent Systems. pp. 79--93 10.1007/978-3-642-12805-9_5\nWilcox, R. R. 1996. Statistics for the Social Sciences. In Statistics for the Social Sciences."
            }
        ]
    },
    {
        "filename": "paper_3.txt",
        "label": "Unsupported claim",
        "text": "most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks.",
        "start": 1017,
        "end": 1194,
        "spans_a": [
            {
                "filename": "paper_3.txt",
                "start": 1017,
                "end": 1194,
                "label": "Unsupported claim",
                "text": "most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks.",
                "full_text": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n\u2022 To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n\u2022 We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n\u2022 Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n References: \nBaevski, A., Auli, M., and Mohamed, A. 2019. Effectiveness of self-supervised pretraining for speech recognition. In Effectiveness of self-supervised pretraining for speech recognition. arXiv:1911.03912\nBaevski, A., Zhou, Y., Mohamed, A., and Auli, M. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 12449--12460\nChen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., and Wei, F. 2021. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. In Wavlm: Large-scale self-supervised pre-training for full stack speech processing. arXiv:2110.13900\nChen, Y., Chi, P., Yang, S., Chang, K., Lin, J., Huang, S., Liu, D., and Liu, C. Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. In Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. arXiv:2105.03070\nChen, Z., Zhang, Y., Rosenberg, A., Ramabhadran, B., Wang, G., and Moreno, P. 2021. Injecting text in self-supervised speech pretraining. In Injecting text in self-supervised speech pretraining. arXiv:2108.12226\nChuang, Y., Liu, C., Lee, H., and Lee, L. 2019. Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. In Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. arXiv:1910.11559\nChung, J.S., Huh, J., and Mun, S. 2020. Delving into VoxCeleb: Environment invariant speaker recognition. In Proceedings of Odyssey. pp. 349--356\nChung, Y. and Glass, J. 2018. Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. In Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. arXiv:1803.08976\nChung, Y., Zhang, Y., Han, W., Chiu, C., Qin, J., Pang, R., and Wu, Y. 2021. W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. In W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. arXiv:2108.06209\nChung, Y., Zhu, C., and Zeng, M. 2021. SPLAT: Speech-language joint pre-training for spoken language understanding. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1897--1907 10.18653/v1/2021.naacl-main.152\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423\nMattia, A. D., Gangi, R., Cattoni, L., Bentivogli, M., Negri, M., and Turchi 2019. MuST-C: a Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2012--2017 10.18653/v1/N19-1202\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H. 2019. Unified language model pre-training for natural language understanding and generation. In Proceedings of the 33rd Conference on Neural Information Processing Systems. pp. 13063--13075\nGulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H., Bougares, F., Schwenk, H., and Bengio, Y. 2015. On using monolingual corpora in neural machine translation. In On using monolingual corpora in neural machine translation. arXiv:1503.03535\nHori, T., Watanabe, S., and Hershey, J. 2017. Joint CTC/attention decoding for end-to-end speech recognition. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 518--529 10.18653/v1/P17-1048\nHsu, W., Bolte, B., Tsai, Y.H., and Lakhotia, K. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. In Hubert: Self-supervised speech representation learning by masked prediction of hidden units. arXiv:2106.07447\nHuang, W., Hayashi, T., Wu, Y., Kameoka, H., and Toda, T. 2021. Pretraining techniques for sequence-to-sequence voice conversion. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 745--755 10.1109/TASLP.2021.3049336\nInaguma, H., Kiyono, S., Duh, K., Karita, S., NelsonEnrique, Soplin, Y., Hayashi, T., and Watanabe, S. 2020. Espnet-st: Allin-one speech translation toolkit. In Espnet-st: Allin-one speech translation toolkit. arXiv:2004.10234\nJensen, J. and Taal, C. H. 2016. An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers. In IEEE/ACM Transactions on Audio Speech and Language Processing. pp. 2009--2022 10.1109/TASLP.2016.2585878\nKameoka, H., Huang, W., Tanaka, K., Kaneko, T., Hojo, N., and Toda, T. 2021. Many-to-many voice transformer network. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 656--670 10.1109/TASLP.2020.3047262\nKharitonov, E., Lee, A., Polyak, A., Adi, Y., Copet, J., Lakhotia, K., Nguyen, T., Rivi\u00e8re, M., Mohamed, A., and Dupoux, E. 2021. Text-free prosody-aware generative spoken language modeling. In Text-free prosody-aware generative spoken language modeling. arXiv:2109.03264\nKim, M., Kim, G., Lee, S., and Ha, J. 2021. St-bert: Cross-modal language model pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7478--7482 10.1109/ICASSP39728.2021.9414558\nDiederik, P., Kingma, J., and Ba 2014. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization. arXiv:1412.6980\nKominek, J. and Black, A. W. 2004. The cmu arctic speech databases. In Proceedings of the Fifth ISCA workshop on speech synthesis.\nKong, J., Kim, J., and Bae, J. 2020. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 17022--17033\nLakhotia, K., Kharitonov, E., Hsu, W., Adi, Y., Polyak, A., Bolte, B., Nguyen, T., Copet, J., and Baevski, A. Generative spoken language modeling from raw audio. In Generative spoken language modeling from raw audio. arXiv:2102.01192\nLample, G. and Conneau, A. 2019. Crosslingual language model pretraining. In Crosslingual language model pretraining. arXiv:1901.07291\nLe, H., Pino, J., Wang, C., Gu, J., Schwab, D., and Besacier, L. 2021. Lightweight adapter tuning for multilingual speech translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 817--824 10.18653/v1/2021.acl-short.103\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, N., Liu, S., Liu, Y., Zhao, S., and Liu, M. 2019. Neural speech synthesis with transformer network. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 6706--6713 10.1609/aaai.v33i01.33016706\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nMittag, G. and M\u00f6ller, S. 2020. Deep learning based assessment of synthetic speech naturalness. In Proceedings of the 2020 Interspeech. pp. 1748--1752\nNagrani, A. 2017. Joon Son Chung, and Andrew Zisserman. In Voxceleb: A large-scale speaker identification dataset. arXiv:1706.08612\nVan Den Oord, A., Vinyals, O., and Kavukcuoglu, K. 2017. Neural discrete representation learning. In Neural discrete representation learning. arXiv:1711.00937\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In fairseq: A fast, extensible toolkit for sequence modeling. arXiv:1904.01038\nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. 2015. Librispeech: an asr corpus based on public domain audio books. In Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5206--5210 10.1109/ICASSP.2015.7178964\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/N18-1202\nQian, Y., Bianv, X., Shi, Y., Kanda, N., Shen, L., Xiao, Z., and Zeng, M. 2021. Speech-language pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7458--7462 10.1109/ICASSP39728.2021.9414900\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In OpenAI blog. pp. 9\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683\nChandan, K. A., Reddy, H., Dubey, V., Gopal, R., Cutler, S., and Braun Hannes Gamper, Robert Aichner, and Sriram Srinivasan. 2021. ICASSP 2021 deep noise suppression challenge. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6623--6627 10.1109/ICASSP39728.2021.9415105\nShuo Ren, L., Zhou, S., Liu, F., Wei, M., Zhou, S., and Ma 2021. Semface: Pre-training encoder and decoder with a semantic interface for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 4518--4527 10.18653/v1/2021.acl-long.348\nRix, A. W., Beerends, J. G., Hollier, M. P., and AP 2001. Perceptual evaluation of speech quality (PESQ) -A new method for speech quality assessment of telephone networks and codecs. In Proceedings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. pp. 749--752 10.1109/ICASSP.2001.941023\nSebastian, B. and Ivan, T. 2020. Data augmentation and loss normalization for deep noise suppression. In Proceedings of Speech and Computer. pp. 79--86\nShaw, P., Uszkoreit, J., and Vaswani, A. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 464--468 10.18653/v1/N18-2074\nShen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomvrgiannakis, Y., and Wu, Y. 2018. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4779--4783 10.1109/ICASSP.2018.8461368\nSnyder, D., Garcia-Romero, D., Sell, G., Povey, D., and Khudanpur, S. 2018. Xvectors: Robust DNN embeddings for speaker recognition. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5329--5333 10.1109/ICASSP.2018.8461375\nSong, X., Wang, G., Wu, Z., Huang, Y., Su, D., Yu, D., and Meng, H. 2019. Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. In Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. arXiv:1910.10387\nSynnaeve, G., Xu, Q., Kahn, J., Likhomanenko, T., Grave, E., Pratap, V., Sriram, A., Liptchinsky, V., and Collobert, R. 2020. End-to-end asr: from supervised to semi-supervised learning with modern architectures. In End-to-end asr: from supervised to semi-supervised learning with modern architectures. arXiv:1911.08460\nTachibana, H., Uenoyama, K., and Aihara, S. 2018. Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4784--4788 10.1109/ICASSP.2018.8461829\nTjandra, A., Sakti, S., and Nakamura, S. 2020. Machine speech chain. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 976--989 10.1109/TASLP.2020.2977776\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. 2017. Attention is all you need. In Proceedings of the 31st Conference on Neural Information Processing Systems. pp. 6000--6010 10.5555/3295222.3295349\nWang, C., Tang, Y., Ma, X., Wu, A., Okhonko, D., and Pino, J. 2020. Fairseq s2t: Fast speech-to-text modeling with fairseq. In Fairseq s2t: Fast speech-to-text modeling with fairseq. arXiv:2010.05171\nWang, C., Wu, Y., Qian, Y., Kumatani, K., Liu, S., Wei, F., Zeng, M., and Huang, X. 2021. Unispeech: Unified speech representation learning with labeled and unlabeled data. In Unispeech: Unified speech representation learning with labeled and unlabeled data. arXiv:2101.07597\nWatanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., NelsonEnrique, Soplin, Y., Heymann, J., Wiesner, M., and Chen, N. 2018. Espnet: Endto-end speech processing toolkit. In Espnet: Endto-end speech processing toolkit. arXiv:1804.00015\nWatanabe, S., Hori, T., Kim, S., Hershey, J. R., and Hayashi, T. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. In IEEE Journal of Selected Topics in Signal Processing. pp. 1240--1253 10.1109/JSTSP.2017.2763455\nWichern, G., Antognini, J., Flynn, M., Zhu, L. R., Mcquinn, E., Crow, D., Manilow, E., and Roux, J. L. 2019. WHAM!: Extending speech separation to noisy environments. In WHAM!: Extending speech separation to noisy environments. arXiv:1907.01160\nYamamoto, R., Song, E., and Kim, J. 2020. Parallel Wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6199--6203 10.1109/ICASSP40776.2020.9053795\nShu-Wen Yang, P., Chi, Y., Chuang, Cheng, -.J., Lai, K., Lakhotia, Yist, Y., Lin, A. T., Liu, J., Shi, X., Chang, G., and Lin 2021. Superb: Speech processing universal performance benchmark. In Superb: Speech processing universal performance benchmark. arXiv:2105.01051"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_3.txt",
                "start": 715,
                "end": 1194,
                "label": "Unsupported claim",
                "text": "However, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks.",
                "full_text": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n\u2022 To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n\u2022 We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n\u2022 Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n References: \nBaevski, A., Auli, M., and Mohamed, A. 2019. Effectiveness of self-supervised pretraining for speech recognition. In Effectiveness of self-supervised pretraining for speech recognition. arXiv:1911.03912\nBaevski, A., Zhou, Y., Mohamed, A., and Auli, M. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 12449--12460\nChen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., and Wei, F. 2021. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. In Wavlm: Large-scale self-supervised pre-training for full stack speech processing. arXiv:2110.13900\nChen, Y., Chi, P., Yang, S., Chang, K., Lin, J., Huang, S., Liu, D., and Liu, C. Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. In Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. arXiv:2105.03070\nChen, Z., Zhang, Y., Rosenberg, A., Ramabhadran, B., Wang, G., and Moreno, P. 2021. Injecting text in self-supervised speech pretraining. In Injecting text in self-supervised speech pretraining. arXiv:2108.12226\nChuang, Y., Liu, C., Lee, H., and Lee, L. 2019. Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. In Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. arXiv:1910.11559\nChung, J.S., Huh, J., and Mun, S. 2020. Delving into VoxCeleb: Environment invariant speaker recognition. In Proceedings of Odyssey. pp. 349--356\nChung, Y. and Glass, J. 2018. Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. In Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. arXiv:1803.08976\nChung, Y., Zhang, Y., Han, W., Chiu, C., Qin, J., Pang, R., and Wu, Y. 2021. W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. In W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. arXiv:2108.06209\nChung, Y., Zhu, C., and Zeng, M. 2021. SPLAT: Speech-language joint pre-training for spoken language understanding. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1897--1907 10.18653/v1/2021.naacl-main.152\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423\nMattia, A. D., Gangi, R., Cattoni, L., Bentivogli, M., Negri, M., and Turchi 2019. MuST-C: a Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2012--2017 10.18653/v1/N19-1202\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H. 2019. Unified language model pre-training for natural language understanding and generation. In Proceedings of the 33rd Conference on Neural Information Processing Systems. pp. 13063--13075\nGulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H., Bougares, F., Schwenk, H., and Bengio, Y. 2015. On using monolingual corpora in neural machine translation. In On using monolingual corpora in neural machine translation. arXiv:1503.03535\nHori, T., Watanabe, S., and Hershey, J. 2017. Joint CTC/attention decoding for end-to-end speech recognition. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 518--529 10.18653/v1/P17-1048\nHsu, W., Bolte, B., Tsai, Y.H., and Lakhotia, K. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. In Hubert: Self-supervised speech representation learning by masked prediction of hidden units. arXiv:2106.07447\nHuang, W., Hayashi, T., Wu, Y., Kameoka, H., and Toda, T. 2021. Pretraining techniques for sequence-to-sequence voice conversion. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 745--755 10.1109/TASLP.2021.3049336\nInaguma, H., Kiyono, S., Duh, K., Karita, S., NelsonEnrique, Soplin, Y., Hayashi, T., and Watanabe, S. 2020. Espnet-st: Allin-one speech translation toolkit. In Espnet-st: Allin-one speech translation toolkit. arXiv:2004.10234\nJensen, J. and Taal, C. H. 2016. An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers. In IEEE/ACM Transactions on Audio Speech and Language Processing. pp. 2009--2022 10.1109/TASLP.2016.2585878\nKameoka, H., Huang, W., Tanaka, K., Kaneko, T., Hojo, N., and Toda, T. 2021. Many-to-many voice transformer network. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 656--670 10.1109/TASLP.2020.3047262\nKharitonov, E., Lee, A., Polyak, A., Adi, Y., Copet, J., Lakhotia, K., Nguyen, T., Rivi\u00e8re, M., Mohamed, A., and Dupoux, E. 2021. Text-free prosody-aware generative spoken language modeling. In Text-free prosody-aware generative spoken language modeling. arXiv:2109.03264\nKim, M., Kim, G., Lee, S., and Ha, J. 2021. St-bert: Cross-modal language model pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7478--7482 10.1109/ICASSP39728.2021.9414558\nDiederik, P., Kingma, J., and Ba 2014. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization. arXiv:1412.6980\nKominek, J. and Black, A. W. 2004. The cmu arctic speech databases. In Proceedings of the Fifth ISCA workshop on speech synthesis.\nKong, J., Kim, J., and Bae, J. 2020. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 17022--17033\nLakhotia, K., Kharitonov, E., Hsu, W., Adi, Y., Polyak, A., Bolte, B., Nguyen, T., Copet, J., and Baevski, A. Generative spoken language modeling from raw audio. In Generative spoken language modeling from raw audio. arXiv:2102.01192\nLample, G. and Conneau, A. 2019. Crosslingual language model pretraining. In Crosslingual language model pretraining. arXiv:1901.07291\nLe, H., Pino, J., Wang, C., Gu, J., Schwab, D., and Besacier, L. 2021. Lightweight adapter tuning for multilingual speech translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 817--824 10.18653/v1/2021.acl-short.103\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, N., Liu, S., Liu, Y., Zhao, S., and Liu, M. 2019. Neural speech synthesis with transformer network. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 6706--6713 10.1609/aaai.v33i01.33016706\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nMittag, G. and M\u00f6ller, S. 2020. Deep learning based assessment of synthetic speech naturalness. In Proceedings of the 2020 Interspeech. pp. 1748--1752\nNagrani, A. 2017. Joon Son Chung, and Andrew Zisserman. In Voxceleb: A large-scale speaker identification dataset. arXiv:1706.08612\nVan Den Oord, A., Vinyals, O., and Kavukcuoglu, K. 2017. Neural discrete representation learning. In Neural discrete representation learning. arXiv:1711.00937\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In fairseq: A fast, extensible toolkit for sequence modeling. arXiv:1904.01038\nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. 2015. Librispeech: an asr corpus based on public domain audio books. In Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5206--5210 10.1109/ICASSP.2015.7178964\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/N18-1202\nQian, Y., Bianv, X., Shi, Y., Kanda, N., Shen, L., Xiao, Z., and Zeng, M. 2021. Speech-language pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7458--7462 10.1109/ICASSP39728.2021.9414900\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In OpenAI blog. pp. 9\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683\nChandan, K. A., Reddy, H., Dubey, V., Gopal, R., Cutler, S., and Braun Hannes Gamper, Robert Aichner, and Sriram Srinivasan. 2021. ICASSP 2021 deep noise suppression challenge. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6623--6627 10.1109/ICASSP39728.2021.9415105\nShuo Ren, L., Zhou, S., Liu, F., Wei, M., Zhou, S., and Ma 2021. Semface: Pre-training encoder and decoder with a semantic interface for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 4518--4527 10.18653/v1/2021.acl-long.348\nRix, A. W., Beerends, J. G., Hollier, M. P., and AP 2001. Perceptual evaluation of speech quality (PESQ) -A new method for speech quality assessment of telephone networks and codecs. In Proceedings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. pp. 749--752 10.1109/ICASSP.2001.941023\nSebastian, B. and Ivan, T. 2020. Data augmentation and loss normalization for deep noise suppression. In Proceedings of Speech and Computer. pp. 79--86\nShaw, P., Uszkoreit, J., and Vaswani, A. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 464--468 10.18653/v1/N18-2074\nShen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomvrgiannakis, Y., and Wu, Y. 2018. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4779--4783 10.1109/ICASSP.2018.8461368\nSnyder, D., Garcia-Romero, D., Sell, G., Povey, D., and Khudanpur, S. 2018. Xvectors: Robust DNN embeddings for speaker recognition. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5329--5333 10.1109/ICASSP.2018.8461375\nSong, X., Wang, G., Wu, Z., Huang, Y., Su, D., Yu, D., and Meng, H. 2019. Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. In Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. arXiv:1910.10387\nSynnaeve, G., Xu, Q., Kahn, J., Likhomanenko, T., Grave, E., Pratap, V., Sriram, A., Liptchinsky, V., and Collobert, R. 2020. End-to-end asr: from supervised to semi-supervised learning with modern architectures. In End-to-end asr: from supervised to semi-supervised learning with modern architectures. arXiv:1911.08460\nTachibana, H., Uenoyama, K., and Aihara, S. 2018. Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4784--4788 10.1109/ICASSP.2018.8461829\nTjandra, A., Sakti, S., and Nakamura, S. 2020. Machine speech chain. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 976--989 10.1109/TASLP.2020.2977776\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. 2017. Attention is all you need. In Proceedings of the 31st Conference on Neural Information Processing Systems. pp. 6000--6010 10.5555/3295222.3295349\nWang, C., Tang, Y., Ma, X., Wu, A., Okhonko, D., and Pino, J. 2020. Fairseq s2t: Fast speech-to-text modeling with fairseq. In Fairseq s2t: Fast speech-to-text modeling with fairseq. arXiv:2010.05171\nWang, C., Wu, Y., Qian, Y., Kumatani, K., Liu, S., Wei, F., Zeng, M., and Huang, X. 2021. Unispeech: Unified speech representation learning with labeled and unlabeled data. In Unispeech: Unified speech representation learning with labeled and unlabeled data. arXiv:2101.07597\nWatanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., NelsonEnrique, Soplin, Y., Heymann, J., Wiesner, M., and Chen, N. 2018. Espnet: Endto-end speech processing toolkit. In Espnet: Endto-end speech processing toolkit. arXiv:1804.00015\nWatanabe, S., Hori, T., Kim, S., Hershey, J. R., and Hayashi, T. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. In IEEE Journal of Selected Topics in Signal Processing. pp. 1240--1253 10.1109/JSTSP.2017.2763455\nWichern, G., Antognini, J., Flynn, M., Zhu, L. R., Mcquinn, E., Crow, D., Manilow, E., and Roux, J. L. 2019. WHAM!: Extending speech separation to noisy environments. In WHAM!: Extending speech separation to noisy environments. arXiv:1907.01160\nYamamoto, R., Song, E., and Kim, J. 2020. Parallel Wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6199--6203 10.1109/ICASSP40776.2020.9053795\nShu-Wen Yang, P., Chi, Y., Chuang, Cheng, -.J., Lai, K., Lakhotia, Yist, Y., Lin, A. T., Liu, J., Shi, X., Chang, G., and Lin 2021. Superb: Speech processing universal performance benchmark. In Superb: Speech processing universal performance benchmark. arXiv:2105.01051"
            }
        ]
    },
    {
        "filename": "paper_8.txt",
        "label": "Unsupported claim",
        "text": "Despite their success, it is still unknown whether or how well the instance difficulty",
        "start": 1604,
        "end": 1688,
        "spans_a": [
            {
                "filename": "paper_8.txt",
                "start": 1604,
                "end": 1690,
                "label": "Unsupported claim",
                "text": "Despite their success, it is still unknown whether or how well the instance difficulty",
                "full_text": "Introduction\n\nEarly exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem.\n\nMost existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy  (Xin et al., 2020;Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.\n\nAnother way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.\n\nDespite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006). The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier. Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets. Experimental results demonstrate that, modern neu-ral networks perform poorly on predicting instance difficulty. This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.\n\nGiven that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules? We hypothesis that the consistency between training and inference may play an important role. That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too. Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules. If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function. In particular, we use hash functions to assign each token to a fixed exiting layer. This hash-based early exiting method is named HASHEE.\n\nCompared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation. (c) The speed-up ratio can be easily tuned by modifying the hash function. (d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.\n\nWe conduct experiments on classification, regression, and generation tasks. Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks. Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT  while maintaining 97% ROUGE-1 score.\n\n References: \nSamuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/d15-1075\nDaniel, M., Cer, M. T., Diab, E., Agirre, I., Lopez-Gazpio, L., and Specia 2017. Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. In Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. abs/1708.00055\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. 2019. Universal transformers. In 7th International Conference on Learning Representations. ICLR 2019\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nWilliam, B., Dolan, C., and Brockett 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005.\nElbayad, M., Gu, J., Grave, E., and Auli, M. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations.\nGordon, M. A., Duh, K., and Andrews, N. 2020. Compressing BERT: studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online. pp. 143--155 10.18653/v1/2020.repl4nlp-1.18\nMoritz Hermann, K., Kocisk\u00fd, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. pp. 1693--1701\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A., and Weischedel, R. M. 2006. Ontonotes: The 90% solution. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings.\nHua, L., Wan, X., and Li, L. 2017. Overview of the nlpcc 2017 shared task: Single document summarization. In National CCF Conference on Natural Language Processing and Chinese Computing. pp. 942--947\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. 2020. Tinybert: Distilling BERT for natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020. pp. 4163--4174 10.18653/v1/2020.findings-emnlp.372\nKhot, T., Sabharwal, A., and Clark, P. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). pp. 5189--5197\nKim, B., Kim, H., and Kim, G. 2019. Abstractive summarization of reddit posts with multi-level memory networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 2519--2531 10.18653/v1/n19-1260\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations.\nLaverghetta, A., Mirzakhalov, J., and Licato, J. 2020. Towards a task-agnostic model of difficulty estimation for supervised learning tasks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021. pp. 16--23\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020.\n2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, L., Lin, Y., Chen, D., Ren, S., Li, P., Zhou, J., and Sun, X. 2021. Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade. In Findings of EMNLP.\nLi, X., Shao, Y., Sun, T., Yan, H., Qiu, X., and Huang, X. 2021. Accelerating BERT inference for sequence labeling via earlyexit. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 189--199 10.18653/v1/2021.acl-long.16\nLiao, K., Zhang, Y., Ren, X., and Su, Q. 2021. A global past-future early exit method for accelerating inference of pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 2013--2023 10.18653/v1/2021.naacl-main.162\nLiu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju, Q. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. pp. 6035--6044\nLiu, X., Sun, T., He, J., Wu, L., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. In Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. abs/2110.07038\nLiu, X., Lai, H., Wong, D. F., and Chao, L. S. 2020. Norm-based curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 427--436 10.18653/v1/2020.acl-main.41\nLiu, Y., Meng, F., Zhou, J., Chen, Y., and Xu, J. 2021. Faster depth-adaptive transformers. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. pp. 13424--13432\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. pp. 142--150\nMichel, P., Levy, O., and Neubig, G. 2019. Are sixteen heads really better than one?. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 14014--14024\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. 2020. Pre-trained models for natural language processing: A survey. In SCIENCE CHINA Technological Sciences. 10.1007/s11431-020-1647-3\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67\nReimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 3980--3990 10.18653/v1/D19-1410\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. 2021. Hash layers for large sparse models. In Hash layers for large sparse models.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. abs/1910.01108\nAdriaan, M. J., Schakel, B. J., and Wilson 2015. Measuring word significance using distributed representations of words. In Measuring word significance using distributed representations of words. abs/1508.02297\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. 2020. The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th.\n2020. Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 6640--6651\nShao, Y., Geng, Z., Liu, Y., Dai, J., Yang, F., Zhe, L., Bao, H., and Qiu, X. 2021. CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. In CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. abs/2109.05729\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. 2020. Q-BERT: hessian based ultra low precision quantization of BERT. In The Thirty-Fourth AAAI Conference on Artificial Intelligence. pp. 8815--8821\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642\nSun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 4322--4331 10.18653/v1/D19-1441\nSun, T., Zhou, Y., Liu, X., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. In Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. abs/2105.13792\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008\nWang, R., Cheng, M., Chen, X., Tang, X., and Hsieh, C. 2021. Rethinking architecture selection in differentiable NAS. In 9th International Conference on Learning Representations.\nXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. 2020. Deebert: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2246--2251\nXin, J., Tang, R., Yu, Y., and Lin, J. 2021. Berxit: Early exiting for BERT with better finetuning and extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main. pp. 91--104\nXu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 7859--7869 10.18653/v1/2020.emnlp-main.633\nXu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. 2020. CLUE: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4762--4772 10.18653/v1/2020.coling-main.419\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 5754--5764\nZhang, C., Yu, M., Wang, W., and Yan, F. 2019. Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving. In 2019 USENIX Annual Technical Conference, USENIX ATC 2019. pp. 1049--1062\nZhou, W., Xu, C., Ge, T., Mcauley, J. J., Xu, K., and Wei, F. 2020. BERT loses patience: Fast and robust inference with early exit. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.\nZhu, W. 2021. Leebert: Learned early exit for BERT with cross-level optimization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 2968--2980 10.18653/v1/2021.acl-long.231\nZiegel, E. R. 2003. The elements of statistical learning. In Technometrics. pp. 267--268 10.1198/tech.2003.s770"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_8.txt",
                "start": 1585,
                "end": 1688,
                "label": "Unsupported claim",
                "text": "Despite their success, it is still unknown whether or how well the instance difficulty can be learned.",
                "full_text": "Introduction\n\nEarly exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem.\n\nMost existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy  (Xin et al., 2020;Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.\n\nAnother way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.\n\nDespite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006). The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier. Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets. Experimental results demonstrate that, modern neu-ral networks perform poorly on predicting instance difficulty. This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.\n\nGiven that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules? We hypothesis that the consistency between training and inference may play an important role. That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too. Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules. If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function. In particular, we use hash functions to assign each token to a fixed exiting layer. This hash-based early exiting method is named HASHEE.\n\nCompared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation. (c) The speed-up ratio can be easily tuned by modifying the hash function. (d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.\n\nWe conduct experiments on classification, regression, and generation tasks. Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks. Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT  while maintaining 97% ROUGE-1 score.\n\n References: \nSamuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/d15-1075\nDaniel, M., Cer, M. T., Diab, E., Agirre, I., Lopez-Gazpio, L., and Specia 2017. Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. In Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. abs/1708.00055\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. 2019. Universal transformers. In 7th International Conference on Learning Representations. ICLR 2019\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nWilliam, B., Dolan, C., and Brockett 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005.\nElbayad, M., Gu, J., Grave, E., and Auli, M. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations.\nGordon, M. A., Duh, K., and Andrews, N. 2020. Compressing BERT: studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online. pp. 143--155 10.18653/v1/2020.repl4nlp-1.18\nMoritz Hermann, K., Kocisk\u00fd, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. pp. 1693--1701\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A., and Weischedel, R. M. 2006. Ontonotes: The 90% solution. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings.\nHua, L., Wan, X., and Li, L. 2017. Overview of the nlpcc 2017 shared task: Single document summarization. In National CCF Conference on Natural Language Processing and Chinese Computing. pp. 942--947\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. 2020. Tinybert: Distilling BERT for natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020. pp. 4163--4174 10.18653/v1/2020.findings-emnlp.372\nKhot, T., Sabharwal, A., and Clark, P. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). pp. 5189--5197\nKim, B., Kim, H., and Kim, G. 2019. Abstractive summarization of reddit posts with multi-level memory networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 2519--2531 10.18653/v1/n19-1260\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations.\nLaverghetta, A., Mirzakhalov, J., and Licato, J. 2020. Towards a task-agnostic model of difficulty estimation for supervised learning tasks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021. pp. 16--23\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020.\n2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, L., Lin, Y., Chen, D., Ren, S., Li, P., Zhou, J., and Sun, X. 2021. Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade. In Findings of EMNLP.\nLi, X., Shao, Y., Sun, T., Yan, H., Qiu, X., and Huang, X. 2021. Accelerating BERT inference for sequence labeling via earlyexit. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 189--199 10.18653/v1/2021.acl-long.16\nLiao, K., Zhang, Y., Ren, X., and Su, Q. 2021. A global past-future early exit method for accelerating inference of pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 2013--2023 10.18653/v1/2021.naacl-main.162\nLiu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju, Q. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. pp. 6035--6044\nLiu, X., Sun, T., He, J., Wu, L., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. In Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. abs/2110.07038\nLiu, X., Lai, H., Wong, D. F., and Chao, L. S. 2020. Norm-based curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 427--436 10.18653/v1/2020.acl-main.41\nLiu, Y., Meng, F., Zhou, J., Chen, Y., and Xu, J. 2021. Faster depth-adaptive transformers. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. pp. 13424--13432\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. pp. 142--150\nMichel, P., Levy, O., and Neubig, G. 2019. Are sixteen heads really better than one?. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 14014--14024\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. 2020. Pre-trained models for natural language processing: A survey. In SCIENCE CHINA Technological Sciences. 10.1007/s11431-020-1647-3\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67\nReimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 3980--3990 10.18653/v1/D19-1410\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. 2021. Hash layers for large sparse models. In Hash layers for large sparse models.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. abs/1910.01108\nAdriaan, M. J., Schakel, B. J., and Wilson 2015. Measuring word significance using distributed representations of words. In Measuring word significance using distributed representations of words. abs/1508.02297\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. 2020. The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th.\n2020. Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 6640--6651\nShao, Y., Geng, Z., Liu, Y., Dai, J., Yang, F., Zhe, L., Bao, H., and Qiu, X. 2021. CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. In CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. abs/2109.05729\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. 2020. Q-BERT: hessian based ultra low precision quantization of BERT. In The Thirty-Fourth AAAI Conference on Artificial Intelligence. pp. 8815--8821\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642\nSun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 4322--4331 10.18653/v1/D19-1441\nSun, T., Zhou, Y., Liu, X., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. In Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. abs/2105.13792\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008\nWang, R., Cheng, M., Chen, X., Tang, X., and Hsieh, C. 2021. Rethinking architecture selection in differentiable NAS. In 9th International Conference on Learning Representations.\nXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. 2020. Deebert: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2246--2251\nXin, J., Tang, R., Yu, Y., and Lin, J. 2021. Berxit: Early exiting for BERT with better finetuning and extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main. pp. 91--104\nXu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 7859--7869 10.18653/v1/2020.emnlp-main.633\nXu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. 2020. CLUE: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4762--4772 10.18653/v1/2020.coling-main.419\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 5754--5764\nZhang, C., Yu, M., Wang, W., and Yan, F. 2019. Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving. In 2019 USENIX Annual Technical Conference, USENIX ATC 2019. pp. 1049--1062\nZhou, W., Xu, C., Ge, T., Mcauley, J. J., Xu, K., and Wei, F. 2020. BERT loses patience: Fast and robust inference with early exit. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.\nZhu, W. 2021. Leebert: Learned early exit for BERT with cross-level optimization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 2968--2980 10.18653/v1/2021.acl-long.231\nZiegel, E. R. 2003. The elements of statistical learning. In Technometrics. pp. 267--268 10.1198/tech.2003.s770"
            }
        ]
    },
    {
        "filename": "paper_10.txt",
        "label": "Unsupported claim",
        "text": "This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way.",
        "start": 1785,
        "end": 1992,
        "spans_a": [
            {
                "filename": "paper_10.txt",
                "start": 1785,
                "end": 1992,
                "label": "Unsupported claim",
                "text": "This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way.",
                "full_text": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students\u2019 performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions\u2019 reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Ko\u02c7cisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models\u2019 reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n References: \nAlonzo, J., Basaraba, D., Tindal, G., and Carriveau, R. S. 2009. They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. In They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. pp. 34--44\nBajgar, O. and Kadlec, R. 2016. Embracing data abundance: Booktest dataset for reading comprehension. In Embracing data abundance: Booktest dataset for reading comprehension. arXiv:1610.00956\nBrahman, F., Huang, M., Tafjord, O., Zhao, C., Sachan, M., and Chaturvedi, S. 2021. let your characters tell their story. In A dataset for character-centric narrative understanding. arXiv:2109.05438\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. In Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457\nDalvi, B., Huang, L., Tandon, N., Wen-Tau, Y., and Clark, P. 2018. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1595--1604\nDas, B. and Majumder, M. 2021. Automatic question generation and answer assessment: a survey. In Technology Enhanced Learning. pp. 1--15\nCarolyn A Denton, M., Enos, M. J., York, D. J., Francis, M. A., Barnes, P. A., Kulesz, Jack, M., Fletcher, S., and Carter 2015. Text-processing differences in adolescent adequate and poor comprehenders reading accessible and challenging narrative and informational text. In Reading Research Quarterly. pp. 393--416\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805\nDavid, J., Francis, Jack, M., Fletcher, H. W., Catts, and Bruce Tomblin 2005. Dimensions affecting the assessment of reading comprehension. In Children's reading comprehension and assessment. pp. 387--412\nAnna, S., Gellert, C., and Elbro 2013. Cloze tests may be quick, but are they dirty? development and preliminary validation of a cloze test of reading comprehension. In Journal of Psychoeducational Assessment. pp. 16--28\nGoldie, P. 2003. One's remembered past: Narrative thinking, emotion, and the external perspective. In Philosophical Papers. pp. 301--319\nMartha, H., Head, J. E., Readence, R. R., and Buss 1989. An examination of summary writing as a measure of reading comprehension. In Literacy Research and Instruction. pp. 1--11\nKim, Y.G. 2017. Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. In Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. pp. 310--333\nKlufa, J. 2015. Multiple choice question testsadvantages and disadvantages. In 3rd International Conference on Education and Modern Educational Technologies (EMET). pp. 39--42\nKo\u010disk\u1ef3, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. 2018. The narrativeqa reading comprehension challenge. In Transactions of the Association for Computational Linguistics. pp. 317--328\nKry\u015bci\u0144ski, W. and Rajani, N. 2021. Booksum: A collection of datasets for longform narrative summarization. In Booksum: A collection of datasets for longform narrative summarization. arXiv:2105.08209\nKurdi, G., Leo, J., Parsia, B., Sattler, U., and Al-Emari, S. 2020. A systematic review of automatic question generation for educational purposes. In International Journal of Artificial Intelligence in Education. pp. 121--204\nLadhak, F., Li, B., Al-Onaizan, Y., and Mckeown, K. 2020. Exploring content selection in summarization of novel chapters. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5043--5054\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. 2017. Race: Large-scale reading comprehension dataset from examinations. In Race: Large-scale reading comprehension dataset from examinations. arXiv:1704.04683\nKumar Lal, Y., Chambers, N., Mooney, R., and Balasubramanian, N. 2021. Tellmewhy: A dataset for answering why-questions in narratives. In Tellmewhy: A dataset for answering why-questions in narratives. arXiv:2106.06132\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461\nMeghan, D. and Liebfreund 2021. Cognitive and motivational predictors of narrative and informational text comprehension. In Reading Psychology. pp. 177--196\nJulie, S., Lynch, P., Van Den, Broek, K. E., Kremer, P., Kendeou, M. J., White, E. P., and Lorch 2008. The development of narrative comprehension and its relation to other early reading skills. In Reading Psychology. pp. 327--365\nNancy, A., Martin, R., and Brownell 2011. Expressive one-word picture vocabulary test-4 (EOWPVT. In Expressive one-word picture vocabulary test-4 (EOWPVT.\nMostafazadeh, N., Chambers, N., He, X., Parikh, D., Batra, D., Vanderwende, L., Kohli, P., and Allen, J. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. In A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv:1604.01696\nMou, X., Yang, C., Yu, M., Yao, B., Guo, X., Potdar, S., and Su, H. 2021. Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. In Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. arXiv:2106.03826\nEzgi \u00c7etinkaya \u00d6zdemir, H. and Akyol 2019. The development of a reading comprehension test. In Universal Journal of Educational Research. pp. 563--570\nAlison, H., Paris, G., and Paris 2003. Assessing narrative comprehension in young children. In Reading Research Quarterly. pp. 36--76\nTaffy, E. and Raphael 1986. Teaching question answer relationships, revisited. The reading teacher. In Teaching question answer relationships, revisited. The reading teacher. pp. 516--522\nRoberts, P. and Priest, H. 2006. Reliability and validity in research. In Nursing standard. pp. 41--46\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv:1910.01108\nSims, M., Park, J. H., and Bamman, D. 2019. Literary event detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3623--3634\nXie, Q., Lai, G., Dai, Z., and Hovy, E. 2017. Large-scale cloze test dataset created by teachers. In Large-scale cloze test dataset created by teachers. arXiv:1711.03225\nYao, B., Wang, D., Wu, T., Hoang, T., Sun, B., Li, T.J., Yu, M., and Xu, Y. 2021. It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. In It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. arXiv:2109.03423"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_10.txt",
                "start": 1785,
                "end": 1992,
                "label": "Unsupported claim",
                "text": "This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way.",
                "full_text": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students\u2019 performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions\u2019 reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Ko\u02c7cisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models\u2019 reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n References: \nAlonzo, J., Basaraba, D., Tindal, G., and Carriveau, R. S. 2009. They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. In They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. pp. 34--44\nBajgar, O. and Kadlec, R. 2016. Embracing data abundance: Booktest dataset for reading comprehension. In Embracing data abundance: Booktest dataset for reading comprehension. arXiv:1610.00956\nBrahman, F., Huang, M., Tafjord, O., Zhao, C., Sachan, M., and Chaturvedi, S. 2021. let your characters tell their story. In A dataset for character-centric narrative understanding. arXiv:2109.05438\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. In Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457\nDalvi, B., Huang, L., Tandon, N., Wen-Tau, Y., and Clark, P. 2018. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1595--1604\nDas, B. and Majumder, M. 2021. Automatic question generation and answer assessment: a survey. In Technology Enhanced Learning. pp. 1--15\nCarolyn A Denton, M., Enos, M. J., York, D. J., Francis, M. A., Barnes, P. A., Kulesz, Jack, M., Fletcher, S., and Carter 2015. Text-processing differences in adolescent adequate and poor comprehenders reading accessible and challenging narrative and informational text. In Reading Research Quarterly. pp. 393--416\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805\nDavid, J., Francis, Jack, M., Fletcher, H. W., Catts, and Bruce Tomblin 2005. Dimensions affecting the assessment of reading comprehension. In Children's reading comprehension and assessment. pp. 387--412\nAnna, S., Gellert, C., and Elbro 2013. Cloze tests may be quick, but are they dirty? development and preliminary validation of a cloze test of reading comprehension. In Journal of Psychoeducational Assessment. pp. 16--28\nGoldie, P. 2003. One's remembered past: Narrative thinking, emotion, and the external perspective. In Philosophical Papers. pp. 301--319\nMartha, H., Head, J. E., Readence, R. R., and Buss 1989. An examination of summary writing as a measure of reading comprehension. In Literacy Research and Instruction. pp. 1--11\nKim, Y.G. 2017. Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. In Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. pp. 310--333\nKlufa, J. 2015. Multiple choice question testsadvantages and disadvantages. In 3rd International Conference on Education and Modern Educational Technologies (EMET). pp. 39--42\nKo\u010disk\u1ef3, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. 2018. The narrativeqa reading comprehension challenge. In Transactions of the Association for Computational Linguistics. pp. 317--328\nKry\u015bci\u0144ski, W. and Rajani, N. 2021. Booksum: A collection of datasets for longform narrative summarization. In Booksum: A collection of datasets for longform narrative summarization. arXiv:2105.08209\nKurdi, G., Leo, J., Parsia, B., Sattler, U., and Al-Emari, S. 2020. A systematic review of automatic question generation for educational purposes. In International Journal of Artificial Intelligence in Education. pp. 121--204\nLadhak, F., Li, B., Al-Onaizan, Y., and Mckeown, K. 2020. Exploring content selection in summarization of novel chapters. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5043--5054\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. 2017. Race: Large-scale reading comprehension dataset from examinations. In Race: Large-scale reading comprehension dataset from examinations. arXiv:1704.04683\nKumar Lal, Y., Chambers, N., Mooney, R., and Balasubramanian, N. 2021. Tellmewhy: A dataset for answering why-questions in narratives. In Tellmewhy: A dataset for answering why-questions in narratives. arXiv:2106.06132\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461\nMeghan, D. and Liebfreund 2021. Cognitive and motivational predictors of narrative and informational text comprehension. In Reading Psychology. pp. 177--196\nJulie, S., Lynch, P., Van Den, Broek, K. E., Kremer, P., Kendeou, M. J., White, E. P., and Lorch 2008. The development of narrative comprehension and its relation to other early reading skills. In Reading Psychology. pp. 327--365\nNancy, A., Martin, R., and Brownell 2011. Expressive one-word picture vocabulary test-4 (EOWPVT. In Expressive one-word picture vocabulary test-4 (EOWPVT.\nMostafazadeh, N., Chambers, N., He, X., Parikh, D., Batra, D., Vanderwende, L., Kohli, P., and Allen, J. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. In A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv:1604.01696\nMou, X., Yang, C., Yu, M., Yao, B., Guo, X., Potdar, S., and Su, H. 2021. Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. In Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. arXiv:2106.03826\nEzgi \u00c7etinkaya \u00d6zdemir, H. and Akyol 2019. The development of a reading comprehension test. In Universal Journal of Educational Research. pp. 563--570\nAlison, H., Paris, G., and Paris 2003. Assessing narrative comprehension in young children. In Reading Research Quarterly. pp. 36--76\nTaffy, E. and Raphael 1986. Teaching question answer relationships, revisited. The reading teacher. In Teaching question answer relationships, revisited. The reading teacher. pp. 516--522\nRoberts, P. and Priest, H. 2006. Reliability and validity in research. In Nursing standard. pp. 41--46\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv:1910.01108\nSims, M., Park, J. H., and Bamman, D. 2019. Literary event detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3623--3634\nXie, Q., Lai, G., Dai, Z., and Hovy, E. 2017. Large-scale cloze test dataset created by teachers. In Large-scale cloze test dataset created by teachers. arXiv:1711.03225\nYao, B., Wang, D., Wu, T., Hoang, T., Sun, B., Li, T.J., Yu, M., and Xu, Y. 2021. It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. In It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. arXiv:2109.03423"
            }
        ]
    }
]