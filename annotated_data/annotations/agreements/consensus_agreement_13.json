[
    {
        "filename": "paper_3.txt",
        "label": "Unsupported claim",
        "text": "most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks.",
        "start": 1017,
        "end": 1194,
        "spans_a": [
            {
                "filename": "paper_3.txt",
                "start": 1017,
                "end": 1194,
                "label": "Unsupported claim",
                "text": "most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks.",
                "full_text": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n\u2022 To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n\u2022 We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n\u2022 Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n References: \nBaevski, A., Auli, M., and Mohamed, A. 2019. Effectiveness of self-supervised pretraining for speech recognition. In Effectiveness of self-supervised pretraining for speech recognition. arXiv:1911.03912\nBaevski, A., Zhou, Y., Mohamed, A., and Auli, M. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 12449--12460\nChen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., and Wei, F. 2021. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. In Wavlm: Large-scale self-supervised pre-training for full stack speech processing. arXiv:2110.13900\nChen, Y., Chi, P., Yang, S., Chang, K., Lin, J., Huang, S., Liu, D., and Liu, C. Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. In Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. arXiv:2105.03070\nChen, Z., Zhang, Y., Rosenberg, A., Ramabhadran, B., Wang, G., and Moreno, P. 2021. Injecting text in self-supervised speech pretraining. In Injecting text in self-supervised speech pretraining. arXiv:2108.12226\nChuang, Y., Liu, C., Lee, H., and Lee, L. 2019. Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. In Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. arXiv:1910.11559\nChung, J.S., Huh, J., and Mun, S. 2020. Delving into VoxCeleb: Environment invariant speaker recognition. In Proceedings of Odyssey. pp. 349--356\nChung, Y. and Glass, J. 2018. Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. In Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. arXiv:1803.08976\nChung, Y., Zhang, Y., Han, W., Chiu, C., Qin, J., Pang, R., and Wu, Y. 2021. W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. In W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. arXiv:2108.06209\nChung, Y., Zhu, C., and Zeng, M. 2021. SPLAT: Speech-language joint pre-training for spoken language understanding. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1897--1907 10.18653/v1/2021.naacl-main.152\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423\nMattia, A. D., Gangi, R., Cattoni, L., Bentivogli, M., Negri, M., and Turchi 2019. MuST-C: a Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2012--2017 10.18653/v1/N19-1202\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H. 2019. Unified language model pre-training for natural language understanding and generation. In Proceedings of the 33rd Conference on Neural Information Processing Systems. pp. 13063--13075\nGulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H., Bougares, F., Schwenk, H., and Bengio, Y. 2015. On using monolingual corpora in neural machine translation. In On using monolingual corpora in neural machine translation. arXiv:1503.03535\nHori, T., Watanabe, S., and Hershey, J. 2017. Joint CTC/attention decoding for end-to-end speech recognition. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 518--529 10.18653/v1/P17-1048\nHsu, W., Bolte, B., Tsai, Y.H., and Lakhotia, K. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. In Hubert: Self-supervised speech representation learning by masked prediction of hidden units. arXiv:2106.07447\nHuang, W., Hayashi, T., Wu, Y., Kameoka, H., and Toda, T. 2021. Pretraining techniques for sequence-to-sequence voice conversion. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 745--755 10.1109/TASLP.2021.3049336\nInaguma, H., Kiyono, S., Duh, K., Karita, S., NelsonEnrique, Soplin, Y., Hayashi, T., and Watanabe, S. 2020. Espnet-st: Allin-one speech translation toolkit. In Espnet-st: Allin-one speech translation toolkit. arXiv:2004.10234\nJensen, J. and Taal, C. H. 2016. An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers. In IEEE/ACM Transactions on Audio Speech and Language Processing. pp. 2009--2022 10.1109/TASLP.2016.2585878\nKameoka, H., Huang, W., Tanaka, K., Kaneko, T., Hojo, N., and Toda, T. 2021. Many-to-many voice transformer network. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 656--670 10.1109/TASLP.2020.3047262\nKharitonov, E., Lee, A., Polyak, A., Adi, Y., Copet, J., Lakhotia, K., Nguyen, T., Rivi\u00e8re, M., Mohamed, A., and Dupoux, E. 2021. Text-free prosody-aware generative spoken language modeling. In Text-free prosody-aware generative spoken language modeling. arXiv:2109.03264\nKim, M., Kim, G., Lee, S., and Ha, J. 2021. St-bert: Cross-modal language model pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7478--7482 10.1109/ICASSP39728.2021.9414558\nDiederik, P., Kingma, J., and Ba 2014. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization. arXiv:1412.6980\nKominek, J. and Black, A. W. 2004. The cmu arctic speech databases. In Proceedings of the Fifth ISCA workshop on speech synthesis.\nKong, J., Kim, J., and Bae, J. 2020. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 17022--17033\nLakhotia, K., Kharitonov, E., Hsu, W., Adi, Y., Polyak, A., Bolte, B., Nguyen, T., Copet, J., and Baevski, A. Generative spoken language modeling from raw audio. In Generative spoken language modeling from raw audio. arXiv:2102.01192\nLample, G. and Conneau, A. 2019. Crosslingual language model pretraining. In Crosslingual language model pretraining. arXiv:1901.07291\nLe, H., Pino, J., Wang, C., Gu, J., Schwab, D., and Besacier, L. 2021. Lightweight adapter tuning for multilingual speech translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 817--824 10.18653/v1/2021.acl-short.103\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, N., Liu, S., Liu, Y., Zhao, S., and Liu, M. 2019. Neural speech synthesis with transformer network. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 6706--6713 10.1609/aaai.v33i01.33016706\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nMittag, G. and M\u00f6ller, S. 2020. Deep learning based assessment of synthetic speech naturalness. In Proceedings of the 2020 Interspeech. pp. 1748--1752\nNagrani, A. 2017. Joon Son Chung, and Andrew Zisserman. In Voxceleb: A large-scale speaker identification dataset. arXiv:1706.08612\nVan Den Oord, A., Vinyals, O., and Kavukcuoglu, K. 2017. Neural discrete representation learning. In Neural discrete representation learning. arXiv:1711.00937\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In fairseq: A fast, extensible toolkit for sequence modeling. arXiv:1904.01038\nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. 2015. Librispeech: an asr corpus based on public domain audio books. In Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5206--5210 10.1109/ICASSP.2015.7178964\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/N18-1202\nQian, Y., Bianv, X., Shi, Y., Kanda, N., Shen, L., Xiao, Z., and Zeng, M. 2021. Speech-language pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7458--7462 10.1109/ICASSP39728.2021.9414900\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In OpenAI blog. pp. 9\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683\nChandan, K. A., Reddy, H., Dubey, V., Gopal, R., Cutler, S., and Braun Hannes Gamper, Robert Aichner, and Sriram Srinivasan. 2021. ICASSP 2021 deep noise suppression challenge. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6623--6627 10.1109/ICASSP39728.2021.9415105\nShuo Ren, L., Zhou, S., Liu, F., Wei, M., Zhou, S., and Ma 2021. Semface: Pre-training encoder and decoder with a semantic interface for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 4518--4527 10.18653/v1/2021.acl-long.348\nRix, A. W., Beerends, J. G., Hollier, M. P., and AP 2001. Perceptual evaluation of speech quality (PESQ) -A new method for speech quality assessment of telephone networks and codecs. In Proceedings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. pp. 749--752 10.1109/ICASSP.2001.941023\nSebastian, B. and Ivan, T. 2020. Data augmentation and loss normalization for deep noise suppression. In Proceedings of Speech and Computer. pp. 79--86\nShaw, P., Uszkoreit, J., and Vaswani, A. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 464--468 10.18653/v1/N18-2074\nShen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomvrgiannakis, Y., and Wu, Y. 2018. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4779--4783 10.1109/ICASSP.2018.8461368\nSnyder, D., Garcia-Romero, D., Sell, G., Povey, D., and Khudanpur, S. 2018. Xvectors: Robust DNN embeddings for speaker recognition. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5329--5333 10.1109/ICASSP.2018.8461375\nSong, X., Wang, G., Wu, Z., Huang, Y., Su, D., Yu, D., and Meng, H. 2019. Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. In Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. arXiv:1910.10387\nSynnaeve, G., Xu, Q., Kahn, J., Likhomanenko, T., Grave, E., Pratap, V., Sriram, A., Liptchinsky, V., and Collobert, R. 2020. End-to-end asr: from supervised to semi-supervised learning with modern architectures. In End-to-end asr: from supervised to semi-supervised learning with modern architectures. arXiv:1911.08460\nTachibana, H., Uenoyama, K., and Aihara, S. 2018. Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4784--4788 10.1109/ICASSP.2018.8461829\nTjandra, A., Sakti, S., and Nakamura, S. 2020. Machine speech chain. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 976--989 10.1109/TASLP.2020.2977776\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. 2017. Attention is all you need. In Proceedings of the 31st Conference on Neural Information Processing Systems. pp. 6000--6010 10.5555/3295222.3295349\nWang, C., Tang, Y., Ma, X., Wu, A., Okhonko, D., and Pino, J. 2020. Fairseq s2t: Fast speech-to-text modeling with fairseq. In Fairseq s2t: Fast speech-to-text modeling with fairseq. arXiv:2010.05171\nWang, C., Wu, Y., Qian, Y., Kumatani, K., Liu, S., Wei, F., Zeng, M., and Huang, X. 2021. Unispeech: Unified speech representation learning with labeled and unlabeled data. In Unispeech: Unified speech representation learning with labeled and unlabeled data. arXiv:2101.07597\nWatanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., NelsonEnrique, Soplin, Y., Heymann, J., Wiesner, M., and Chen, N. 2018. Espnet: Endto-end speech processing toolkit. In Espnet: Endto-end speech processing toolkit. arXiv:1804.00015\nWatanabe, S., Hori, T., Kim, S., Hershey, J. R., and Hayashi, T. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. In IEEE Journal of Selected Topics in Signal Processing. pp. 1240--1253 10.1109/JSTSP.2017.2763455\nWichern, G., Antognini, J., Flynn, M., Zhu, L. R., Mcquinn, E., Crow, D., Manilow, E., and Roux, J. L. 2019. WHAM!: Extending speech separation to noisy environments. In WHAM!: Extending speech separation to noisy environments. arXiv:1907.01160\nYamamoto, R., Song, E., and Kim, J. 2020. Parallel Wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6199--6203 10.1109/ICASSP40776.2020.9053795\nShu-Wen Yang, P., Chi, Y., Chuang, Cheng, -.J., Lai, K., Lakhotia, Yist, Y., Lin, A. T., Liu, J., Shi, X., Chang, G., and Lin 2021. Superb: Speech processing universal performance benchmark. In Superb: Speech processing universal performance benchmark. arXiv:2105.01051"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_3.txt",
                "start": 715,
                "end": 1194,
                "label": "Unsupported claim",
                "text": "However, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks.",
                "full_text": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n\u2022 To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n\u2022 We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n\u2022 Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n References: \nBaevski, A., Auli, M., and Mohamed, A. 2019. Effectiveness of self-supervised pretraining for speech recognition. In Effectiveness of self-supervised pretraining for speech recognition. arXiv:1911.03912\nBaevski, A., Zhou, Y., Mohamed, A., and Auli, M. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 12449--12460\nChen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., and Wei, F. 2021. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. In Wavlm: Large-scale self-supervised pre-training for full stack speech processing. arXiv:2110.13900\nChen, Y., Chi, P., Yang, S., Chang, K., Lin, J., Huang, S., Liu, D., and Liu, C. Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. In Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. arXiv:2105.03070\nChen, Z., Zhang, Y., Rosenberg, A., Ramabhadran, B., Wang, G., and Moreno, P. 2021. Injecting text in self-supervised speech pretraining. In Injecting text in self-supervised speech pretraining. arXiv:2108.12226\nChuang, Y., Liu, C., Lee, H., and Lee, L. 2019. Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. In Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. arXiv:1910.11559\nChung, J.S., Huh, J., and Mun, S. 2020. Delving into VoxCeleb: Environment invariant speaker recognition. In Proceedings of Odyssey. pp. 349--356\nChung, Y. and Glass, J. 2018. Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. In Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. arXiv:1803.08976\nChung, Y., Zhang, Y., Han, W., Chiu, C., Qin, J., Pang, R., and Wu, Y. 2021. W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. In W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. arXiv:2108.06209\nChung, Y., Zhu, C., and Zeng, M. 2021. SPLAT: Speech-language joint pre-training for spoken language understanding. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1897--1907 10.18653/v1/2021.naacl-main.152\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423\nMattia, A. D., Gangi, R., Cattoni, L., Bentivogli, M., Negri, M., and Turchi 2019. MuST-C: a Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2012--2017 10.18653/v1/N19-1202\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H. 2019. Unified language model pre-training for natural language understanding and generation. In Proceedings of the 33rd Conference on Neural Information Processing Systems. pp. 13063--13075\nGulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H., Bougares, F., Schwenk, H., and Bengio, Y. 2015. On using monolingual corpora in neural machine translation. In On using monolingual corpora in neural machine translation. arXiv:1503.03535\nHori, T., Watanabe, S., and Hershey, J. 2017. Joint CTC/attention decoding for end-to-end speech recognition. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 518--529 10.18653/v1/P17-1048\nHsu, W., Bolte, B., Tsai, Y.H., and Lakhotia, K. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. In Hubert: Self-supervised speech representation learning by masked prediction of hidden units. arXiv:2106.07447\nHuang, W., Hayashi, T., Wu, Y., Kameoka, H., and Toda, T. 2021. Pretraining techniques for sequence-to-sequence voice conversion. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 745--755 10.1109/TASLP.2021.3049336\nInaguma, H., Kiyono, S., Duh, K., Karita, S., NelsonEnrique, Soplin, Y., Hayashi, T., and Watanabe, S. 2020. Espnet-st: Allin-one speech translation toolkit. In Espnet-st: Allin-one speech translation toolkit. arXiv:2004.10234\nJensen, J. and Taal, C. H. 2016. An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers. In IEEE/ACM Transactions on Audio Speech and Language Processing. pp. 2009--2022 10.1109/TASLP.2016.2585878\nKameoka, H., Huang, W., Tanaka, K., Kaneko, T., Hojo, N., and Toda, T. 2021. Many-to-many voice transformer network. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 656--670 10.1109/TASLP.2020.3047262\nKharitonov, E., Lee, A., Polyak, A., Adi, Y., Copet, J., Lakhotia, K., Nguyen, T., Rivi\u00e8re, M., Mohamed, A., and Dupoux, E. 2021. Text-free prosody-aware generative spoken language modeling. In Text-free prosody-aware generative spoken language modeling. arXiv:2109.03264\nKim, M., Kim, G., Lee, S., and Ha, J. 2021. St-bert: Cross-modal language model pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7478--7482 10.1109/ICASSP39728.2021.9414558\nDiederik, P., Kingma, J., and Ba 2014. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization. arXiv:1412.6980\nKominek, J. and Black, A. W. 2004. The cmu arctic speech databases. In Proceedings of the Fifth ISCA workshop on speech synthesis.\nKong, J., Kim, J., and Bae, J. 2020. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 17022--17033\nLakhotia, K., Kharitonov, E., Hsu, W., Adi, Y., Polyak, A., Bolte, B., Nguyen, T., Copet, J., and Baevski, A. Generative spoken language modeling from raw audio. In Generative spoken language modeling from raw audio. arXiv:2102.01192\nLample, G. and Conneau, A. 2019. Crosslingual language model pretraining. In Crosslingual language model pretraining. arXiv:1901.07291\nLe, H., Pino, J., Wang, C., Gu, J., Schwab, D., and Besacier, L. 2021. Lightweight adapter tuning for multilingual speech translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 817--824 10.18653/v1/2021.acl-short.103\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, N., Liu, S., Liu, Y., Zhao, S., and Liu, M. 2019. Neural speech synthesis with transformer network. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 6706--6713 10.1609/aaai.v33i01.33016706\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nMittag, G. and M\u00f6ller, S. 2020. Deep learning based assessment of synthetic speech naturalness. In Proceedings of the 2020 Interspeech. pp. 1748--1752\nNagrani, A. 2017. Joon Son Chung, and Andrew Zisserman. In Voxceleb: A large-scale speaker identification dataset. arXiv:1706.08612\nVan Den Oord, A., Vinyals, O., and Kavukcuoglu, K. 2017. Neural discrete representation learning. In Neural discrete representation learning. arXiv:1711.00937\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In fairseq: A fast, extensible toolkit for sequence modeling. arXiv:1904.01038\nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. 2015. Librispeech: an asr corpus based on public domain audio books. In Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5206--5210 10.1109/ICASSP.2015.7178964\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/N18-1202\nQian, Y., Bianv, X., Shi, Y., Kanda, N., Shen, L., Xiao, Z., and Zeng, M. 2021. Speech-language pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7458--7462 10.1109/ICASSP39728.2021.9414900\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In OpenAI blog. pp. 9\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683\nChandan, K. A., Reddy, H., Dubey, V., Gopal, R., Cutler, S., and Braun Hannes Gamper, Robert Aichner, and Sriram Srinivasan. 2021. ICASSP 2021 deep noise suppression challenge. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6623--6627 10.1109/ICASSP39728.2021.9415105\nShuo Ren, L., Zhou, S., Liu, F., Wei, M., Zhou, S., and Ma 2021. Semface: Pre-training encoder and decoder with a semantic interface for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 4518--4527 10.18653/v1/2021.acl-long.348\nRix, A. W., Beerends, J. G., Hollier, M. P., and AP 2001. Perceptual evaluation of speech quality (PESQ) -A new method for speech quality assessment of telephone networks and codecs. In Proceedings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. pp. 749--752 10.1109/ICASSP.2001.941023\nSebastian, B. and Ivan, T. 2020. Data augmentation and loss normalization for deep noise suppression. In Proceedings of Speech and Computer. pp. 79--86\nShaw, P., Uszkoreit, J., and Vaswani, A. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 464--468 10.18653/v1/N18-2074\nShen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomvrgiannakis, Y., and Wu, Y. 2018. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4779--4783 10.1109/ICASSP.2018.8461368\nSnyder, D., Garcia-Romero, D., Sell, G., Povey, D., and Khudanpur, S. 2018. Xvectors: Robust DNN embeddings for speaker recognition. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5329--5333 10.1109/ICASSP.2018.8461375\nSong, X., Wang, G., Wu, Z., Huang, Y., Su, D., Yu, D., and Meng, H. 2019. Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. In Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. arXiv:1910.10387\nSynnaeve, G., Xu, Q., Kahn, J., Likhomanenko, T., Grave, E., Pratap, V., Sriram, A., Liptchinsky, V., and Collobert, R. 2020. End-to-end asr: from supervised to semi-supervised learning with modern architectures. In End-to-end asr: from supervised to semi-supervised learning with modern architectures. arXiv:1911.08460\nTachibana, H., Uenoyama, K., and Aihara, S. 2018. Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4784--4788 10.1109/ICASSP.2018.8461829\nTjandra, A., Sakti, S., and Nakamura, S. 2020. Machine speech chain. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 976--989 10.1109/TASLP.2020.2977776\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. 2017. Attention is all you need. In Proceedings of the 31st Conference on Neural Information Processing Systems. pp. 6000--6010 10.5555/3295222.3295349\nWang, C., Tang, Y., Ma, X., Wu, A., Okhonko, D., and Pino, J. 2020. Fairseq s2t: Fast speech-to-text modeling with fairseq. In Fairseq s2t: Fast speech-to-text modeling with fairseq. arXiv:2010.05171\nWang, C., Wu, Y., Qian, Y., Kumatani, K., Liu, S., Wei, F., Zeng, M., and Huang, X. 2021. Unispeech: Unified speech representation learning with labeled and unlabeled data. In Unispeech: Unified speech representation learning with labeled and unlabeled data. arXiv:2101.07597\nWatanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., NelsonEnrique, Soplin, Y., Heymann, J., Wiesner, M., and Chen, N. 2018. Espnet: Endto-end speech processing toolkit. In Espnet: Endto-end speech processing toolkit. arXiv:1804.00015\nWatanabe, S., Hori, T., Kim, S., Hershey, J. R., and Hayashi, T. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. In IEEE Journal of Selected Topics in Signal Processing. pp. 1240--1253 10.1109/JSTSP.2017.2763455\nWichern, G., Antognini, J., Flynn, M., Zhu, L. R., Mcquinn, E., Crow, D., Manilow, E., and Roux, J. L. 2019. WHAM!: Extending speech separation to noisy environments. In WHAM!: Extending speech separation to noisy environments. arXiv:1907.01160\nYamamoto, R., Song, E., and Kim, J. 2020. Parallel Wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6199--6203 10.1109/ICASSP40776.2020.9053795\nShu-Wen Yang, P., Chi, Y., Chuang, Cheng, -.J., Lai, K., Lakhotia, Yist, Y., Lin, A. T., Liu, J., Shi, X., Chang, G., and Lin 2021. Superb: Speech processing universal performance benchmark. In Superb: Speech processing universal performance benchmark. arXiv:2105.01051"
            }
        ]
    },
    {
        "filename": "paper_10.txt",
        "label": "Unsupported claim",
        "text": "Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning.",
        "start": 98,
        "end": 293,
        "spans_a": [
            {
                "filename": "paper_10.txt",
                "start": 98,
                "end": 293,
                "label": "Unsupported claim",
                "text": "Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning.",
                "full_text": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students\u2019 performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions\u2019 reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Ko\u02c7cisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models\u2019 reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n References: \nAlonzo, J., Basaraba, D., Tindal, G., and Carriveau, R. S. 2009. They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. In They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. pp. 34--44\nBajgar, O. and Kadlec, R. 2016. Embracing data abundance: Booktest dataset for reading comprehension. In Embracing data abundance: Booktest dataset for reading comprehension. arXiv:1610.00956\nBrahman, F., Huang, M., Tafjord, O., Zhao, C., Sachan, M., and Chaturvedi, S. 2021. let your characters tell their story. In A dataset for character-centric narrative understanding. arXiv:2109.05438\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. In Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457\nDalvi, B., Huang, L., Tandon, N., Wen-Tau, Y., and Clark, P. 2018. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1595--1604\nDas, B. and Majumder, M. 2021. Automatic question generation and answer assessment: a survey. In Technology Enhanced Learning. pp. 1--15\nCarolyn A Denton, M., Enos, M. J., York, D. J., Francis, M. A., Barnes, P. A., Kulesz, Jack, M., Fletcher, S., and Carter 2015. Text-processing differences in adolescent adequate and poor comprehenders reading accessible and challenging narrative and informational text. In Reading Research Quarterly. pp. 393--416\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805\nDavid, J., Francis, Jack, M., Fletcher, H. W., Catts, and Bruce Tomblin 2005. Dimensions affecting the assessment of reading comprehension. In Children's reading comprehension and assessment. pp. 387--412\nAnna, S., Gellert, C., and Elbro 2013. Cloze tests may be quick, but are they dirty? development and preliminary validation of a cloze test of reading comprehension. In Journal of Psychoeducational Assessment. pp. 16--28\nGoldie, P. 2003. One's remembered past: Narrative thinking, emotion, and the external perspective. In Philosophical Papers. pp. 301--319\nMartha, H., Head, J. E., Readence, R. R., and Buss 1989. An examination of summary writing as a measure of reading comprehension. In Literacy Research and Instruction. pp. 1--11\nKim, Y.G. 2017. Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. In Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. pp. 310--333\nKlufa, J. 2015. Multiple choice question testsadvantages and disadvantages. In 3rd International Conference on Education and Modern Educational Technologies (EMET). pp. 39--42\nKo\u010disk\u1ef3, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. 2018. The narrativeqa reading comprehension challenge. In Transactions of the Association for Computational Linguistics. pp. 317--328\nKry\u015bci\u0144ski, W. and Rajani, N. 2021. Booksum: A collection of datasets for longform narrative summarization. In Booksum: A collection of datasets for longform narrative summarization. arXiv:2105.08209\nKurdi, G., Leo, J., Parsia, B., Sattler, U., and Al-Emari, S. 2020. A systematic review of automatic question generation for educational purposes. In International Journal of Artificial Intelligence in Education. pp. 121--204\nLadhak, F., Li, B., Al-Onaizan, Y., and Mckeown, K. 2020. Exploring content selection in summarization of novel chapters. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5043--5054\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. 2017. Race: Large-scale reading comprehension dataset from examinations. In Race: Large-scale reading comprehension dataset from examinations. arXiv:1704.04683\nKumar Lal, Y., Chambers, N., Mooney, R., and Balasubramanian, N. 2021. Tellmewhy: A dataset for answering why-questions in narratives. In Tellmewhy: A dataset for answering why-questions in narratives. arXiv:2106.06132\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461\nMeghan, D. and Liebfreund 2021. Cognitive and motivational predictors of narrative and informational text comprehension. In Reading Psychology. pp. 177--196\nJulie, S., Lynch, P., Van Den, Broek, K. E., Kremer, P., Kendeou, M. J., White, E. P., and Lorch 2008. The development of narrative comprehension and its relation to other early reading skills. In Reading Psychology. pp. 327--365\nNancy, A., Martin, R., and Brownell 2011. Expressive one-word picture vocabulary test-4 (EOWPVT. In Expressive one-word picture vocabulary test-4 (EOWPVT.\nMostafazadeh, N., Chambers, N., He, X., Parikh, D., Batra, D., Vanderwende, L., Kohli, P., and Allen, J. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. In A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv:1604.01696\nMou, X., Yang, C., Yu, M., Yao, B., Guo, X., Potdar, S., and Su, H. 2021. Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. In Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. arXiv:2106.03826\nEzgi \u00c7etinkaya \u00d6zdemir, H. and Akyol 2019. The development of a reading comprehension test. In Universal Journal of Educational Research. pp. 563--570\nAlison, H., Paris, G., and Paris 2003. Assessing narrative comprehension in young children. In Reading Research Quarterly. pp. 36--76\nTaffy, E. and Raphael 1986. Teaching question answer relationships, revisited. The reading teacher. In Teaching question answer relationships, revisited. The reading teacher. pp. 516--522\nRoberts, P. and Priest, H. 2006. Reliability and validity in research. In Nursing standard. pp. 41--46\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv:1910.01108\nSims, M., Park, J. H., and Bamman, D. 2019. Literary event detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3623--3634\nXie, Q., Lai, G., Dai, Z., and Hovy, E. 2017. Large-scale cloze test dataset created by teachers. In Large-scale cloze test dataset created by teachers. arXiv:1711.03225\nYao, B., Wang, D., Wu, T., Hoang, T., Sun, B., Li, T.J., Yu, M., and Xu, Y. 2021. It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. In It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. arXiv:2109.03423"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_10.txt",
                "start": 98,
                "end": 293,
                "label": "Unsupported claim",
                "text": "Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning.",
                "full_text": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students\u2019 performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions\u2019 reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Ko\u02c7cisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models\u2019 reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n References: \nAlonzo, J., Basaraba, D., Tindal, G., and Carriveau, R. S. 2009. They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. In They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. pp. 34--44\nBajgar, O. and Kadlec, R. 2016. Embracing data abundance: Booktest dataset for reading comprehension. In Embracing data abundance: Booktest dataset for reading comprehension. arXiv:1610.00956\nBrahman, F., Huang, M., Tafjord, O., Zhao, C., Sachan, M., and Chaturvedi, S. 2021. let your characters tell their story. In A dataset for character-centric narrative understanding. arXiv:2109.05438\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. In Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457\nDalvi, B., Huang, L., Tandon, N., Wen-Tau, Y., and Clark, P. 2018. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1595--1604\nDas, B. and Majumder, M. 2021. Automatic question generation and answer assessment: a survey. In Technology Enhanced Learning. pp. 1--15\nCarolyn A Denton, M., Enos, M. J., York, D. J., Francis, M. A., Barnes, P. A., Kulesz, Jack, M., Fletcher, S., and Carter 2015. Text-processing differences in adolescent adequate and poor comprehenders reading accessible and challenging narrative and informational text. In Reading Research Quarterly. pp. 393--416\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805\nDavid, J., Francis, Jack, M., Fletcher, H. W., Catts, and Bruce Tomblin 2005. Dimensions affecting the assessment of reading comprehension. In Children's reading comprehension and assessment. pp. 387--412\nAnna, S., Gellert, C., and Elbro 2013. Cloze tests may be quick, but are they dirty? development and preliminary validation of a cloze test of reading comprehension. In Journal of Psychoeducational Assessment. pp. 16--28\nGoldie, P. 2003. One's remembered past: Narrative thinking, emotion, and the external perspective. In Philosophical Papers. pp. 301--319\nMartha, H., Head, J. E., Readence, R. R., and Buss 1989. An examination of summary writing as a measure of reading comprehension. In Literacy Research and Instruction. pp. 1--11\nKim, Y.G. 2017. Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. In Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. pp. 310--333\nKlufa, J. 2015. Multiple choice question testsadvantages and disadvantages. In 3rd International Conference on Education and Modern Educational Technologies (EMET). pp. 39--42\nKo\u010disk\u1ef3, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. 2018. The narrativeqa reading comprehension challenge. In Transactions of the Association for Computational Linguistics. pp. 317--328\nKry\u015bci\u0144ski, W. and Rajani, N. 2021. Booksum: A collection of datasets for longform narrative summarization. In Booksum: A collection of datasets for longform narrative summarization. arXiv:2105.08209\nKurdi, G., Leo, J., Parsia, B., Sattler, U., and Al-Emari, S. 2020. A systematic review of automatic question generation for educational purposes. In International Journal of Artificial Intelligence in Education. pp. 121--204\nLadhak, F., Li, B., Al-Onaizan, Y., and Mckeown, K. 2020. Exploring content selection in summarization of novel chapters. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5043--5054\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. 2017. Race: Large-scale reading comprehension dataset from examinations. In Race: Large-scale reading comprehension dataset from examinations. arXiv:1704.04683\nKumar Lal, Y., Chambers, N., Mooney, R., and Balasubramanian, N. 2021. Tellmewhy: A dataset for answering why-questions in narratives. In Tellmewhy: A dataset for answering why-questions in narratives. arXiv:2106.06132\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461\nMeghan, D. and Liebfreund 2021. Cognitive and motivational predictors of narrative and informational text comprehension. In Reading Psychology. pp. 177--196\nJulie, S., Lynch, P., Van Den, Broek, K. E., Kremer, P., Kendeou, M. J., White, E. P., and Lorch 2008. The development of narrative comprehension and its relation to other early reading skills. In Reading Psychology. pp. 327--365\nNancy, A., Martin, R., and Brownell 2011. Expressive one-word picture vocabulary test-4 (EOWPVT. In Expressive one-word picture vocabulary test-4 (EOWPVT.\nMostafazadeh, N., Chambers, N., He, X., Parikh, D., Batra, D., Vanderwende, L., Kohli, P., and Allen, J. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. In A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv:1604.01696\nMou, X., Yang, C., Yu, M., Yao, B., Guo, X., Potdar, S., and Su, H. 2021. Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. In Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. arXiv:2106.03826\nEzgi \u00c7etinkaya \u00d6zdemir, H. and Akyol 2019. The development of a reading comprehension test. In Universal Journal of Educational Research. pp. 563--570\nAlison, H., Paris, G., and Paris 2003. Assessing narrative comprehension in young children. In Reading Research Quarterly. pp. 36--76\nTaffy, E. and Raphael 1986. Teaching question answer relationships, revisited. The reading teacher. In Teaching question answer relationships, revisited. The reading teacher. pp. 516--522\nRoberts, P. and Priest, H. 2006. Reliability and validity in research. In Nursing standard. pp. 41--46\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv:1910.01108\nSims, M., Park, J. H., and Bamman, D. 2019. Literary event detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3623--3634\nXie, Q., Lai, G., Dai, Z., and Hovy, E. 2017. Large-scale cloze test dataset created by teachers. In Large-scale cloze test dataset created by teachers. arXiv:1711.03225\nYao, B., Wang, D., Wu, T., Hoang, T., Sun, B., Li, T.J., Yu, M., and Xu, Y. 2021. It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. In It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. arXiv:2109.03423"
            }
        ]
    },
    {
        "filename": "paper_10.txt",
        "label": "Unsupported claim",
        "text": "This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way.",
        "start": 1785,
        "end": 1992,
        "spans_a": [
            {
                "filename": "paper_10.txt",
                "start": 1785,
                "end": 1992,
                "label": "Unsupported claim",
                "text": "This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way.",
                "full_text": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students\u2019 performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions\u2019 reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Ko\u02c7cisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models\u2019 reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n References: \nAlonzo, J., Basaraba, D., Tindal, G., and Carriveau, R. S. 2009. They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. In They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. pp. 34--44\nBajgar, O. and Kadlec, R. 2016. Embracing data abundance: Booktest dataset for reading comprehension. In Embracing data abundance: Booktest dataset for reading comprehension. arXiv:1610.00956\nBrahman, F., Huang, M., Tafjord, O., Zhao, C., Sachan, M., and Chaturvedi, S. 2021. let your characters tell their story. In A dataset for character-centric narrative understanding. arXiv:2109.05438\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. In Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457\nDalvi, B., Huang, L., Tandon, N., Wen-Tau, Y., and Clark, P. 2018. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1595--1604\nDas, B. and Majumder, M. 2021. Automatic question generation and answer assessment: a survey. In Technology Enhanced Learning. pp. 1--15\nCarolyn A Denton, M., Enos, M. J., York, D. J., Francis, M. A., Barnes, P. A., Kulesz, Jack, M., Fletcher, S., and Carter 2015. Text-processing differences in adolescent adequate and poor comprehenders reading accessible and challenging narrative and informational text. In Reading Research Quarterly. pp. 393--416\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805\nDavid, J., Francis, Jack, M., Fletcher, H. W., Catts, and Bruce Tomblin 2005. Dimensions affecting the assessment of reading comprehension. In Children's reading comprehension and assessment. pp. 387--412\nAnna, S., Gellert, C., and Elbro 2013. Cloze tests may be quick, but are they dirty? development and preliminary validation of a cloze test of reading comprehension. In Journal of Psychoeducational Assessment. pp. 16--28\nGoldie, P. 2003. One's remembered past: Narrative thinking, emotion, and the external perspective. In Philosophical Papers. pp. 301--319\nMartha, H., Head, J. E., Readence, R. R., and Buss 1989. An examination of summary writing as a measure of reading comprehension. In Literacy Research and Instruction. pp. 1--11\nKim, Y.G. 2017. Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. In Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. pp. 310--333\nKlufa, J. 2015. Multiple choice question testsadvantages and disadvantages. In 3rd International Conference on Education and Modern Educational Technologies (EMET). pp. 39--42\nKo\u010disk\u1ef3, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. 2018. The narrativeqa reading comprehension challenge. In Transactions of the Association for Computational Linguistics. pp. 317--328\nKry\u015bci\u0144ski, W. and Rajani, N. 2021. Booksum: A collection of datasets for longform narrative summarization. In Booksum: A collection of datasets for longform narrative summarization. arXiv:2105.08209\nKurdi, G., Leo, J., Parsia, B., Sattler, U., and Al-Emari, S. 2020. A systematic review of automatic question generation for educational purposes. In International Journal of Artificial Intelligence in Education. pp. 121--204\nLadhak, F., Li, B., Al-Onaizan, Y., and Mckeown, K. 2020. Exploring content selection in summarization of novel chapters. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5043--5054\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. 2017. Race: Large-scale reading comprehension dataset from examinations. In Race: Large-scale reading comprehension dataset from examinations. arXiv:1704.04683\nKumar Lal, Y., Chambers, N., Mooney, R., and Balasubramanian, N. 2021. Tellmewhy: A dataset for answering why-questions in narratives. In Tellmewhy: A dataset for answering why-questions in narratives. arXiv:2106.06132\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461\nMeghan, D. and Liebfreund 2021. Cognitive and motivational predictors of narrative and informational text comprehension. In Reading Psychology. pp. 177--196\nJulie, S., Lynch, P., Van Den, Broek, K. E., Kremer, P., Kendeou, M. J., White, E. P., and Lorch 2008. The development of narrative comprehension and its relation to other early reading skills. In Reading Psychology. pp. 327--365\nNancy, A., Martin, R., and Brownell 2011. Expressive one-word picture vocabulary test-4 (EOWPVT. In Expressive one-word picture vocabulary test-4 (EOWPVT.\nMostafazadeh, N., Chambers, N., He, X., Parikh, D., Batra, D., Vanderwende, L., Kohli, P., and Allen, J. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. In A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv:1604.01696\nMou, X., Yang, C., Yu, M., Yao, B., Guo, X., Potdar, S., and Su, H. 2021. Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. In Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. arXiv:2106.03826\nEzgi \u00c7etinkaya \u00d6zdemir, H. and Akyol 2019. The development of a reading comprehension test. In Universal Journal of Educational Research. pp. 563--570\nAlison, H., Paris, G., and Paris 2003. Assessing narrative comprehension in young children. In Reading Research Quarterly. pp. 36--76\nTaffy, E. and Raphael 1986. Teaching question answer relationships, revisited. The reading teacher. In Teaching question answer relationships, revisited. The reading teacher. pp. 516--522\nRoberts, P. and Priest, H. 2006. Reliability and validity in research. In Nursing standard. pp. 41--46\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv:1910.01108\nSims, M., Park, J. H., and Bamman, D. 2019. Literary event detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3623--3634\nXie, Q., Lai, G., Dai, Z., and Hovy, E. 2017. Large-scale cloze test dataset created by teachers. In Large-scale cloze test dataset created by teachers. arXiv:1711.03225\nYao, B., Wang, D., Wu, T., Hoang, T., Sun, B., Li, T.J., Yu, M., and Xu, Y. 2021. It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. In It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. arXiv:2109.03423"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_10.txt",
                "start": 1785,
                "end": 1992,
                "label": "Unsupported claim",
                "text": "This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way.",
                "full_text": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students\u2019 performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions\u2019 reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Ko\u02c7cisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models\u2019 reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n References: \nAlonzo, J., Basaraba, D., Tindal, G., and Carriveau, R. S. 2009. They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. In They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. pp. 34--44\nBajgar, O. and Kadlec, R. 2016. Embracing data abundance: Booktest dataset for reading comprehension. In Embracing data abundance: Booktest dataset for reading comprehension. arXiv:1610.00956\nBrahman, F., Huang, M., Tafjord, O., Zhao, C., Sachan, M., and Chaturvedi, S. 2021. let your characters tell their story. In A dataset for character-centric narrative understanding. arXiv:2109.05438\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. In Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457\nDalvi, B., Huang, L., Tandon, N., Wen-Tau, Y., and Clark, P. 2018. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1595--1604\nDas, B. and Majumder, M. 2021. Automatic question generation and answer assessment: a survey. In Technology Enhanced Learning. pp. 1--15\nCarolyn A Denton, M., Enos, M. J., York, D. J., Francis, M. A., Barnes, P. A., Kulesz, Jack, M., Fletcher, S., and Carter 2015. Text-processing differences in adolescent adequate and poor comprehenders reading accessible and challenging narrative and informational text. In Reading Research Quarterly. pp. 393--416\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805\nDavid, J., Francis, Jack, M., Fletcher, H. W., Catts, and Bruce Tomblin 2005. Dimensions affecting the assessment of reading comprehension. In Children's reading comprehension and assessment. pp. 387--412\nAnna, S., Gellert, C., and Elbro 2013. Cloze tests may be quick, but are they dirty? development and preliminary validation of a cloze test of reading comprehension. In Journal of Psychoeducational Assessment. pp. 16--28\nGoldie, P. 2003. One's remembered past: Narrative thinking, emotion, and the external perspective. In Philosophical Papers. pp. 301--319\nMartha, H., Head, J. E., Readence, R. R., and Buss 1989. An examination of summary writing as a measure of reading comprehension. In Literacy Research and Instruction. pp. 1--11\nKim, Y.G. 2017. Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. In Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. pp. 310--333\nKlufa, J. 2015. Multiple choice question testsadvantages and disadvantages. In 3rd International Conference on Education and Modern Educational Technologies (EMET). pp. 39--42\nKo\u010disk\u1ef3, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. 2018. The narrativeqa reading comprehension challenge. In Transactions of the Association for Computational Linguistics. pp. 317--328\nKry\u015bci\u0144ski, W. and Rajani, N. 2021. Booksum: A collection of datasets for longform narrative summarization. In Booksum: A collection of datasets for longform narrative summarization. arXiv:2105.08209\nKurdi, G., Leo, J., Parsia, B., Sattler, U., and Al-Emari, S. 2020. A systematic review of automatic question generation for educational purposes. In International Journal of Artificial Intelligence in Education. pp. 121--204\nLadhak, F., Li, B., Al-Onaizan, Y., and Mckeown, K. 2020. Exploring content selection in summarization of novel chapters. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5043--5054\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. 2017. Race: Large-scale reading comprehension dataset from examinations. In Race: Large-scale reading comprehension dataset from examinations. arXiv:1704.04683\nKumar Lal, Y., Chambers, N., Mooney, R., and Balasubramanian, N. 2021. Tellmewhy: A dataset for answering why-questions in narratives. In Tellmewhy: A dataset for answering why-questions in narratives. arXiv:2106.06132\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461\nMeghan, D. and Liebfreund 2021. Cognitive and motivational predictors of narrative and informational text comprehension. In Reading Psychology. pp. 177--196\nJulie, S., Lynch, P., Van Den, Broek, K. E., Kremer, P., Kendeou, M. J., White, E. P., and Lorch 2008. The development of narrative comprehension and its relation to other early reading skills. In Reading Psychology. pp. 327--365\nNancy, A., Martin, R., and Brownell 2011. Expressive one-word picture vocabulary test-4 (EOWPVT. In Expressive one-word picture vocabulary test-4 (EOWPVT.\nMostafazadeh, N., Chambers, N., He, X., Parikh, D., Batra, D., Vanderwende, L., Kohli, P., and Allen, J. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. In A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv:1604.01696\nMou, X., Yang, C., Yu, M., Yao, B., Guo, X., Potdar, S., and Su, H. 2021. Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. In Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. arXiv:2106.03826\nEzgi \u00c7etinkaya \u00d6zdemir, H. and Akyol 2019. The development of a reading comprehension test. In Universal Journal of Educational Research. pp. 563--570\nAlison, H., Paris, G., and Paris 2003. Assessing narrative comprehension in young children. In Reading Research Quarterly. pp. 36--76\nTaffy, E. and Raphael 1986. Teaching question answer relationships, revisited. The reading teacher. In Teaching question answer relationships, revisited. The reading teacher. pp. 516--522\nRoberts, P. and Priest, H. 2006. Reliability and validity in research. In Nursing standard. pp. 41--46\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv:1910.01108\nSims, M., Park, J. H., and Bamman, D. 2019. Literary event detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3623--3634\nXie, Q., Lai, G., Dai, Z., and Hovy, E. 2017. Large-scale cloze test dataset created by teachers. In Large-scale cloze test dataset created by teachers. arXiv:1711.03225\nYao, B., Wang, D., Wu, T., Hoang, T., Sun, B., Li, T.J., Yu, M., and Xu, Y. 2021. It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. In It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. arXiv:2109.03423"
            }
        ]
    }
]